- en: Chapter 3. Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章。线性回归
- en: We've learned from previous chapters that regression problems *involve predicting
    a numerical output*. The simplest but most common type of regression is linear
    regression. In this chapter, we'll explore why linear regression is so commonly
    used, its limitations, and extensions, and then touch on *polynomial regression*,
    which you may consider when a linear relationship isn't a best fit for your circumstances.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前几章了解到，回归问题*涉及预测数值输出*。最简单但最常见的一种回归是线性回归。在本章中，我们将探讨为什么线性回归如此常用，其局限性以及扩展，然后简要介绍*多项式回归*，当线性关系不适合你的情况时，你可能需要考虑它。
- en: Introduction to linear regression
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归简介
- en: 'In **linear regression**, the output variable is predicted by a linearly weighted
    combination of input features. Here is an example of a simple linear model:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在**线性回归**中，输出变量是通过输入特征的线性加权组合来预测的。以下是一个简单线性模型的例子：
- en: '![Introduction to linear regression](img/00029.jpeg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归简介](img/00029.jpeg)'
- en: 'The preceding model essentially says that we are estimating one output, and
    this is a linear function of a single predictor variable (that is, a feature)
    denoted by the letter *x*. The terms involving the Greek letter *β* are the parameters
    of the model and are known as **regression coefficients**. Once we train the model
    and settle on values for these parameters, we can make a prediction on the output
    variable for any value of *x* by a simple substitution in our equation. Another
    example of a linear model, this time with three features and with values assigned
    to the regression coefficients, is given by the following equation:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的模型本质上表示我们正在估计一个输出，这是一个单一预测变量（即特征）的线性函数，用字母 *x* 表示。涉及希腊字母 *β* 的项是模型的参数，被称为**回归系数**。一旦我们训练了模型并确定了这些参数的值，我们就可以通过在我们的方程中进行简单的替换来对任何
    *x* 值的输出变量进行预测。另一个线性模型的例子，这次有三个特征并分配了回归系数的值，如下方程所示：
- en: '![Introduction to linear regression](img/00030.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归简介](img/00030.jpeg)'
- en: In this equation, just as with the previous one, we can observe that we have
    one more coefficient than the number of features. This additional coefficient,
    *β[0]*, is known as the **intercept** and is the expected value of the model when
    the value of all input features is zero. The other *β* coefficients can be interpreted
    as the expected change in the value of the output per unit increase of a feature.
    For example, in the preceding equation, if the value of the feature *x[1]* rises
    by one unit, the expected value of the output will rise by 1.91 units. Similarly,
    a unit increase in the feature *x[3]* results in a decrease in the output by 7.56
    units. In a simple one-dimensional regression problem, we can plot the output
    on the *y* axis of a graph and the input feature on the *x* axis. In this case,
    the model predicts a straight-line relationship between these two, where *β[0]*
    represents the point at which the straight line crosses or intercepts the *y*
    axis and *β[1]* represents the slope of the line. We often refer to the case of
    a single feature (hence, two regression coefficients) as **simple linear regression**
    and the case of two or more features as **multiple linear regression**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，就像前一个方程一样，我们可以观察到我们有一个比特征数量多的系数。这个额外的系数，*β[0]*，被称为**截距**，是当所有输入特征值为零时模型的期望值。其他
    *β* 系数可以解释为输出值随特征单位增加的期望变化。例如，在前面的方程中，如果特征 *x[1]* 的值增加一个单位，输出值的期望值将增加 1.91 个单位。同样，特征
    *x[3]* 的单位增加会导致输出值减少 7.56 个单位。在一个简单的一维回归问题中，我们可以在图的 *y* 轴上绘制输出，在 *x* 轴上绘制输入特征。在这种情况下，模型预测这两个变量之间存在直线关系，其中
    *β[0]* 代表直线与 *y* 轴相交或截取的点，而 *β[1]* 代表直线的斜率。我们通常将只有一个特征（因此有两个回归系数）的情况称为**简单线性回归**，而将有两个或更多特征的情况称为**多元线性回归**。
- en: Assumptions of linear regression
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归的假设
- en: 'Before we delve into the details of how to train a linear regression model
    and how it performs, we''ll look at model assumptions. Model assumptions essentially
    describe what the model believes about the output variable *y* that we are trying
    to predict. Specifically, linear regression models assume that the output variable
    is a weighted linear function of a set of feature variables. Additionally, the
    model assumes that for fixed values of the feature variables, the output is normally
    distributed with a constant variance. This is the same as saying that the model
    assumes that the true output variable *y* can be represented by an equation such
    as the following one, shown for two input features:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨如何训练线性回归模型及其表现之前，我们将查看模型假设。模型假设本质上描述了模型对我们试图预测的输出变量 *y* 的信念。具体来说，线性回归模型假设输出变量是一组特征变量的加权线性函数。此外，模型假设对于特征变量的固定值，输出是正态分布且方差恒定。这等同于说模型假设真实输出变量
    *y* 可以用一个如下的方程表示，这里展示了两个输入特征：
- en: '![Assumptions of linear regression](img/00031.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的假设](img/00031.jpeg)'
- en: 'Here, *ε* represents an error term, which is normally distributed with a zero
    mean and a constant variance *σ²*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*ε* 代表一个误差项，它服从均值为零、方差为常数 *σ²* 的正态分布：
- en: '![Assumptions of linear regression](img/00032.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的假设](img/00032.jpeg)'
- en: 'We might come across the term **homoscedasticity** as a more formal way of
    describing the notion of constant variance. By homoscedasticity or constant variance,
    we are referring to the fact that the variance in the error component does not
    vary with the values or levels of the input features. In the following plot, we
    are visualizing a hypothetical example of a linear relationship with **heteroskedastic**
    errors, which are errors that do not have a constant variance. The data points
    lie close to the line at low values of the input feature, because the variance
    is low in this region of the plot, but lie farther away from the line at higher
    values of the input feature because of the higher variance:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会遇到**同方差性**这个术语，作为描述恒定方差概念的一种更正式的方式。通过同方差性或恒定方差，我们指的是误差成分的方差不会随着输入特征的值或水平而变化。在下面的图中，我们可视化了一个具有**异方差**误差的假设线性关系示例，这些误差没有恒定的方差。在输入特征的低值处，数据点靠近线，因为在这个图的这个区域方差较低，但在输入特征的高值处，由于方差较高，数据点远离线：
- en: '![Assumptions of linear regression](img/00033.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的假设](img/00033.jpeg)'
- en: The *ε* term is an irreducible error component of the true function *y* and
    can be used to represent random errors, such as measurement errors in the feature
    values. When training a linear regression model, we always expect to observe some
    amount of error in our estimate of the output, even if we have all the right features,
    enough data, and the system being modeled really is linear.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*ε* 项是真实函数 *y* 的不可约误差组成部分，可以用来表示随机误差，例如特征值中的测量误差。在训练线性回归模型时，我们总是期望在我们的输出估计中观察到一定量的误差，即使我们拥有所有正确的特征、足够的数据，并且所建模的系统确实是线性的。'
- en: Put differently, even with a true function that is linear, we still expect that
    once we find a line of best fit through our training examples, our line will not
    go through all, or even any, of our data points because of this inherent variance
    exhibited by the error component. The critical thing to remember, though, is that
    in this ideal scenario, because our error component has zero mean and constant
    variance, our training criterion will allow us to come close to the true values
    of the regression coefficients given a sufficiently large sample, as the errors
    will cancel out.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，即使有一个真正的线性函数，我们仍然期望一旦我们通过训练示例找到一个最佳拟合线，由于误差成分所表现出的这种固有方差，我们的线不会穿过所有，甚至任何，我们的数据点。然而，关键要记住的是，在这个理想场景中，因为我们的误差成分具有零均值和恒定方差，我们的训练标准将允许我们在足够大的样本下接近回归系数的真实值，因为误差将会相互抵消。
- en: 'Another important assumption relates to the independence of the error terms.
    This means that we do not expect the **residual** or error term associated with
    one particular observation to be somehow correlated with that of another observation.
    This assumption can be violated if observations are functions of each other, which
    is typically the result of an error in the measurement. If we were to take a portion
    of our training data, double all the values of the features and outputs, and add
    these new data points to our training data, we could create the illusion of having
    a larger dataset; however, there will be pairs of observations whose error terms
    will depend on each other as a result, and hence our model assumption would be
    violated. Incidentally, artificially growing our dataset in such a manner is never
    acceptable for any model. Similarly, correlated error terms may occur if observations
    are related in some way by an unmeasured variable. For example, if we are measuring
    the malfunction rate of parts from an assembly line, then parts from the same
    factory might have a correlation in the error: for example, due to different standards
    and protocols used in the assembly process. Therefore, if we don''t use the factory
    as a feature, we may see correlated errors in our sample among observations that
    correspond to parts from the same factory. The study of **experimental design**
    is concerned with identifying and reducing correlations in error terms, but this
    is beyond the scope of this book.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的假设与误差项的独立性相关。这意味着我们并不期望与一个特定观测值相关的**残差**或误差项以某种方式与另一个观测值相关联。如果观测值彼此是函数关系，这一假设可能会被违反，这通常是测量误差的结果。如果我们从我们的训练数据中取出一部分，将所有特征和输出的值加倍，并将这些新的数据点添加到我们的训练数据中，我们可以创造出拥有更大数据集的假象；然而，由此将会有成对的观测值，它们的误差项将相互依赖，因此我们的模型假设将被违反。顺便提一下，以这种方式人工增加数据集在任何模型中都是不可接受的。同样，如果观测值通过一个未测量的变量以某种方式相关联，也可能出现相关的误差项。例如，如果我们正在测量装配线零部件的故障率，那么来自同一工厂的零部件可能存在误差的相关性：例如，由于装配过程中使用了不同的标准和协议。因此，如果我们不将工厂作为特征，我们可能会在我们的样本中观察到来自同一工厂的零部件之间的相关误差。**实验设计**的研究涉及识别和减少误差项中的相关性，但这超出了本书的范围。
- en: 'Finally, another important assumption concerns the notion that the features
    themselves are statistically independent of each other. It is worth clarifying
    here that in linear models, although the input features must be linearly weighted,
    they themselves may be the output of another function. To illustrate this, one
    might be surprised to see that the following is a linear model of three features,
    *sin(z[1]**)*, *ln(z[2]* *)*, and *exp(z*[3]*)*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个重要的假设涉及特征本身在统计上相互独立的概念。在这里值得澄清的是，在线性模型中，尽管输入特征必须是线性加权的，但它们自身可能是另一个函数的输出。为了说明这一点，人们可能会惊讶地看到以下是一个包含三个特征*sin(z[1]**)*、*ln(z[2]*
    *)*和*exp(z*[3]*)*的线性模型：
- en: '![Assumptions of linear regression](img/00034.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的假设](img/00034.jpeg)'
- en: 'We can see that this is a linear model by making a few transformations on the
    input features and then making the replacements in our model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对输入特征进行一些变换，然后在我们的模型中进行替换，来看到这是一个线性模型：
- en: '![Assumptions of linear regression](img/00035.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的假设](img/00035.jpeg)'
- en: 'Now, we have an equation that is more recognizable as a linear regression model.
    If the previous example made us believe that nearly everything could be transformed
    into a linear model, then the following two examples will emphatically convince
    us that this is not in fact the case:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个更易被识别为线性回归模型的方程。如果先前的例子让我们相信几乎所有东西都可以转化为线性模型，那么接下来的两个例子将明确地让我们相信这实际上并非如此：
- en: '![Assumptions of linear regression](img/00036.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的假设](img/00036.jpeg)'
- en: Both models are not linear models because of the first regression coefficient
    (*β[1]*). The first model is not a linear model because *β[1]* is acting as the
    exponent of the first input feature. In the second model, *β[1]* is inside a *sine*
    function. The important lesson to take away from these examples is that there
    are cases where we can apply transformations on our input features in order to
    fit our data to a linear model; however, we need to be careful that our regression
    coefficients are always the linear weights of the resulting new features.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第一个回归系数（*β[1]*），这两个模型都不是线性模型。第一个模型不是线性模型，因为*β[1]*充当第一个输入特征的指数。在第二个模型中，*β[1]*位于一个*正弦*函数内部。从这些例子中可以吸取的重要教训是，在某些情况下，我们可以对我们的输入特征应用变换，以便将数据拟合到线性模型中；然而，我们需要小心，确保我们的回归系数始终是结果新特征的线性权重。
- en: Simple linear regression
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'Here is the code for the simple linear regression model:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是简单线性回归模型的代码：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For our input feature, we randomly sample points from a uniform distribution.
    We used a uniform distribution to get a good spread of data points. Note that
    our final `df` data frame is meant to simulate a data frame that we would obtain
    in practice; as a result, we do not include the error terms, as these would be
    unavailable to us in a real-world setting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的输入特征，我们从均匀分布中随机采样点。我们使用均匀分布来获得数据点的良好分布。请注意，我们的最终`df`数据框旨在模拟我们在实践中获得的数据框；因此，我们不包含误差项，因为在现实世界的设置中这些误差项对我们是不可用的。
- en: When we train a linear model using some data such as the data in our data frame,
    we are essentially hoping to produce a linear model with the same coefficients
    as the ones from the underlying model of the data. Put differently, the original
    coefficients define a **population regression line**. In this case, the population
    regression line represents the true underlying model of the data. In general,
    we will find ourselves attempting to model a function that is not necessarily
    linear. In this case, we can still define the population regression line as the
    best possible linear regression line, but a linear regression model will obviously
    not perform equally well.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用某些数据（如我们的数据框中的数据）训练线性模型时，我们实际上希望产生一个与数据的潜在模型具有相同系数的线性模型。换句话说，原始系数定义了一个**总体回归线**。在这种情况下，总体回归线代表了数据的真实潜在模型。一般来说，我们会发现自己试图模拟一个不一定是线性的函数。在这种情况下，我们仍然可以将总体回归线定义为最佳可能的线性回归线，但线性回归模型显然不会表现同样好。
- en: Estimating the regression coefficients
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计回归系数
- en: For our simple linear regression model, the process of training the model amounts
    to an estimation of our two regression coefficients from our dataset. As we can
    see from our previously constructed data frame, our data is effectively a series
    of observations, each of which is a pair of values (*x[i]*, *y[i]*) where the
    first element of the pair is the input feature value and the second element of
    the pair is its output label. It turns out that for the case of simple linear
    regression, it is possible to write down two equations that can be used to compute
    our two regression coefficients. Instead of merely presenting these equations,
    we'll first take a brief moment to review some very basic statistical quantities
    that the reader has most likely encountered previously, as they will be featured
    very shortly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的简单线性回归模型，训练模型的过程相当于从我们的数据集中估计两个回归系数。正如我们可以从我们之前构建的数据框中看到的那样，我们的数据实际上是一系列观察结果，每个观察结果是一对值（*x[i]*，*y[i]*），其中这对值的第一个元素是输入特征值，第二个元素是对应的输出标签。结果证明，对于简单线性回归的情况，我们可以写出两个方程来计算我们的两个回归系数。我们不会仅仅展示这些方程，而是首先简要回顾一些读者可能之前已经遇到的一些非常基本的统计量，因为它们将在不久的将来被介绍。
- en: 'The **mean** of a set of values is just the average of these values and is
    often described as a measure of location, giving a sense of where the values are
    centered on the scale in which they are measured. In statistical literature, the
    average value of a random variable is often known as the **expectation**, so we
    often find that the mean of a random variable *X* is denoted as *E(X)*. Another
    notation that is commonly used is bar notation, where we can represent the notion
    of taking the average of a variable by placing a bar over that variable. To illustrate
    this, the following two equations show the mean of the output variable *y* and
    input feature *x*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一组值的**均值**就是这些值的平均值，通常被描述为位置度量，给出值在其测量尺度上的中心位置。在统计文献中，随机变量的平均值通常被称为**期望**，所以我们经常发现随机变量
    *X* 的均值表示为 *E(X)*。另一种常用的表示法是横线表示法，其中我们可以通过在该变量上方放置横线来表示取该变量的平均值。为了说明这一点，以下两个方程显示了输出变量
    *y* 和输入特征 *x* 的均值：
- en: '![Estimating the regression coefficients](img/00038.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![估计回归系数](img/00038.jpeg)'
- en: 'A second very common quantity, which should also be familiar, is the **variance**
    of a variable. The variance measures the average square distance that individual
    values have from the mean. In this way, it is a measure of dispersion, so that
    a low variance implies that most of the values are bunched up close to the mean,
    whereas a higher variance results in values that are spread out. Note that the
    definition of variance involves the definition of the mean, and for this reason
    we''ll see the use of the *x* variable with a bar on it in the following equation,
    which shows the variance of our input feature *x*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个非常常见的量，你也应该很熟悉，是变量的**方差**。方差衡量各个值与均值之间的平均平方距离。因此，它是一个分散度度量，所以低方差意味着大多数值都聚集在均值附近，而高方差会导致值分布得更广。请注意，方差的定义涉及到均值的定义，因此我们将在以下方程中看到带有横线的
    *x* 变量的使用，该方程显示了我们的输入特征 *x* 的方差：
- en: '![Estimating the regression coefficients](img/00039.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![估计回归系数](img/00039.jpeg)'
- en: 'Finally, we''ll define the **covariance** between two random variables, *x*
    and *y*, using the following equation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用以下方程定义两个随机变量 *x* 和 *y* 之间的**协方差**：
- en: '![Estimating the regression coefficients](img/00040.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![估计回归系数](img/00040.jpeg)'
- en: From the previous equation, it should be clear that the variance, which we just
    defined previously, is actually a special case of the covariance where the two
    variables are the same. The covariance measures how strongly two variables are
    correlated with each other and can be positive or negative. A positive covariance
    implies a positive correlation; that is, when one variable increases, the other
    will increase as well. A negative covariance suggests the opposite; when one variable
    increases, the other will tend to decrease. When two variables are statistically
    independent of each other and hence uncorrelated, their covariance will be zero
    (although it should be noted that a zero covariance does not necessarily imply
    statistical independence).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程中，应该很明显，我们之前定义的方差实际上是一个特殊的情况，即协方差中的两个变量是相同的。协方差衡量两个变量之间相互关联的强度，可以是正的也可以是负的。正协方差意味着正相关；也就是说，当一个变量增加时，另一个变量也会增加。负协方差表示相反的情况；当一个变量增加时，另一个变量往往会减少。当两个变量在统计上相互独立且因此不相关时，它们的协方差将为零（尽管应该注意的是，零协方差并不一定意味着统计独立性）。
- en: 'Armed with these basic concepts, we can now present equations for the estimates
    of the two regression coefficients for the case of simple linear regression:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些基本概念后，我们现在可以给出简单线性回归中两个回归系数估计的方程：
- en: '![Estimating the regression coefficients](img/00041.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![估计回归系数](img/00041.jpeg)'
- en: The first regression coefficient can be computed as the ratio of the covariance
    between the output and the input feature, and the variance of the input feature.
    Note that if the output feature were to be independent of the input feature, the
    covariance would be zero and therefore, our linear model would consist of a horizontal
    line with no slope. In practice, it should be noted that even when two variables
    are statistically independent, we will still typically see a small degree of covariance
    due to the random nature of the errors; thus, if we were to train a linear regression
    model to describe their relationship, our first regression coefficient would be
    nonzero in general. Later, we'll see how significance tests can be used to detect
    features we should not include in our models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个回归系数可以计算为输出特征与输入特征之间的协方差与输入特征方差的比率。请注意，如果输出特征与输入特征独立，协方差将为零，因此，我们的线性模型将是一条没有斜率的水平线。在实践中，应注意，即使两个变量在统计上独立，我们通常也会看到由于误差的随机性质而存在一定程度的协方差；因此，如果我们训练一个线性回归模型来描述它们之间的关系，我们的第一个回归系数通常不会为零。稍后，我们将看到如何使用显著性测试来检测我们不应包括在模型中的特征。
- en: 'To implement linear regression in R, it is not necessary to perform these calculations
    as R provides us with the `lm()` function, which builds a linear regression model
    for us. The following code sample uses the `df` data frame we created previously
    and calculates the regression coefficients:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中实现线性回归时，没有必要执行这些计算，因为R为我们提供了`lm()`函数，该函数为我们构建线性回归模型。以下代码示例使用我们之前创建的`df`数据框并计算回归系数：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the first line, we see that the usage of the `lm()` function involves first
    specifying a formula and then following up with the `data` parameter, which in
    our case is our data frame. In the case of simple linear regression, the syntax
    of the formula that we specify for the `lm()` function is the name of the output
    variable, followed by a tilde (*~*) and then by the name of the single input feature.
    We''ll see how to specify more complex formulas when we look at multiple linear
    regression further along in this chapter. Finally, the output shows us the values
    for the two regression coefficients. Note that the *β[0]* coefficient is labeled
    as the intercept, and the *β[1]* coefficient is labeled by the name of the corresponding
    feature (in this case, `x[1]`) in the equation of the linear model:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，我们看到使用`lm()`函数的用法包括首先指定一个公式，然后是`data`参数，在我们的情况下是数据框。在简单线性回归的情况下，我们为`lm()`函数指定的公式的语法是输出变量的名称，后跟波浪号(*~*)，然后是单个输入特征的名称。当我们在本章后面进一步研究多元线性回归时，我们将看到如何指定更复杂的公式。最后，输出显示了两个回归系数的值。请注意，*β[0]*系数被标记为截距，而*β[1]*系数被标记为对应特征（在这种情况下，`x[1]`）的名称，在线性模型的方程中：
- en: 'The following graph shows the population line and the estimated line on the
    same plot:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了人口线和估计线在同一图上的情况：
- en: '![Estimating the regression coefficients](img/00042.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![估计回归系数](img/00042.jpeg)'
- en: As we can see, the two lines are so close to each other that they are barely
    distinguishable, showing that the model has estimated the true population line
    very closely. From [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*,
    we know that we can formalize how closely our model matches our dataset, as well
    as how closely it would match an analogous test set using the mean square error.
    We'll examine this as well as several other metrics of model performance and quality
    in this chapter, but first we'll generalize our regression model to deal with
    more than one input feature.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这两条线非常接近，几乎无法区分，这表明模型非常接近地估计了真实的人口线。从[第一章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第一章. 准备预测建模")《准备预测建模》中，我们知道我们可以形式化我们的模型与数据集匹配的紧密程度，以及它将如何使用均方误差与类似的测试集匹配。在本章中，我们将检查这一点以及几个其他模型性能和质量的指标，但首先我们将泛化我们的回归模型以处理多个输入特征。
- en: Multiple linear regression
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: 'Whenever we have more than one input feature and want to build a linear regression
    model, we are in the realm of multiple linear regression. The general equation
    for a multiple linear regression model with *k* input features is:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们有多个输入特征并想要构建线性回归模型时，我们就处于多元线性回归的领域。具有*k*个输入特征的多元线性回归模型的一般方程是：
- en: '![Multiple linear regression](img/00043.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![多重线性回归](img/00043.jpeg)'
- en: Our assumptions about the model and about the error component *ε* remain the
    same as with simple linear regression, remembering that, as we now have more than
    one input feature, we assume that these are independent of each other. Instead
    of using simulated data to demonstrate multiple linear regression, we will analyze
    two real-world datasets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型和误差分量*ε*的假设与简单线性回归相同，记住我们现在有多个输入特征，我们假设它们是相互独立的。我们不会使用模拟数据来展示多重线性回归，而是将分析两个真实世界的数据集。
- en: Predicting CPU performance
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测CPU性能
- en: 'Our first real-world dataset was presented by the researchers *Dennis F. Kibler*,
    *David W. Aha*, and *Marc K. Albert* in a 1989 paper entitled *Instance-based
    prediction of real-valued attributes* and published in the *Journal of Computational
    Intelligence*. The data contains the characteristics of different CPU models,
    such as the cycle time and the amount of cache memory. When deciding between processors,
    we would like to take all of these things into account, but ideally, we''d like
    to compare processors on a single numerical scale. For this reason, we often develop
    programs to benchmark the relative performance of a CPU. Our dataset also comes
    with the published relative performance of our CPUs and our objective will be
    to use the available CPU characteristics as features to predict this. The dataset
    can be obtained online from the UCI Machine Learning Repository via this link:
    [http://archive.ics.uci.edu/ml/datasets/Computer+Hardware](http://archive.ics.uci.edu/ml/datasets/Computer+Hardware).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个真实世界的数据集是由研究人员*Dennis F. Kibler*、*David W. Aha*和*Marc K. Albert*在1989年发表的一篇题为*Instance-based
    prediction of real-valued attributes*的论文中提出的，该论文发表在*Journal of Computational Intelligence*杂志上。数据包含了不同CPU模型的特征，例如周期时间和缓存内存的大小。在决定处理器之间选择时，我们希望考虑所有这些因素，但理想情况下，我们希望在一个单一的数值尺度上比较处理器。因此，我们经常开发程序来基准测试CPU的相对性能。我们的数据集还包含了我们CPU的已发布相对性能，我们的目标将是使用可用的CPU特征来预测这一点。该数据集可以通过以下链接从UCI机器学习仓库在线获取：[http://archive.ics.uci.edu/ml/datasets/Computer+Hardware](http://archive.ics.uci.edu/ml/datasets/Computer+Hardware)。
- en: Tip
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The UCI Machine Learning Repository is a wonderful online resource that hosts
    a large number of datasets, many of which are often cited by authors of books
    and tutorials. It is well worth the effort to familiarize yourself with this website
    and its datasets. A very good way to learn predictive analytics is to practice
    using the techniques you learn in this book on different datasets, and the UCI
    repository provides many of these for exactly this purpose.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习仓库是一个优秀的在线资源，它托管了大量的数据集，其中许多数据集经常被书籍和教程的作者引用。熟悉这个网站及其数据集是值得努力的。学习预测分析的一个非常好的方法是练习使用你在本书中学到的技术在不同数据集上进行分析，而UCI仓库提供了许多这样的数据集，正好用于这个目的。
- en: 'The `machine.data` file contains all our data in a comma-separated format,
    with one line per CPU model. We''ll import this in R and label all the columns.
    Note that there are 10 columns in total, but we don''t need the first two for
    our analysis, as these are just the brand and model name of the CPU. Similarly,
    the final column is a predicted estimate of the relative performance that was
    produced by the researchers themselves; our actual output variable, PRP, is in
    column *9*. We''ll store the data that we need in a data frame called `machine`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`machine.data`文件包含了我们所有的数据，以逗号分隔的格式，每行对应一个CPU模型。我们将使用R导入这些数据并标记所有列。请注意，总共有10列，但我们的分析不需要前两列，因为这些只是CPU的品牌和型号名称。同样，最后一列是研究人员自己产生的相对性能预测估计；我们的实际输出变量PRP在第9列。我们将需要的所有数据存储在一个名为`machine`的数据框中：'
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The dataset also comes with definitions of the data columns:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集还包含了数据列的定义：
- en: '| Column name | Definition |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 定义 |'
- en: '| --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `MYCT` | The machine cycle time in nanoseconds |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `MYCT` | 机器周期时间（纳秒） |'
- en: '| `MMIN` | The minimum main memory in kilobytes |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `MMIN` | 最小主内存（千字节） |'
- en: '| `MMAX` | The maximum main memory in kilobytes |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `MMAX` | 最大主内存（千字节） |'
- en: '| `CACH` | The cache memory in kilobytes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `CACH` | 缓存内存（千字节） |'
- en: '| `CHMIN` | The minimum channels in units |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `CHMIN` | 最小通道数（单位） |'
- en: '| `CHMAX` | The maximum channels in units |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `CHMAX` | 最大通道数（单位） |'
- en: '| `PRP` | The published relative performance (our output variable) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `PRP` | 已发布的相对性能（我们的输出变量） |'
- en: 'The dataset contains no missing values, so no observations need to be removed
    or modified. One thing that we''ll notice is that we only have roughly 200 data
    points, which is generally considered a very small sample. Nonetheless, we will
    proceed with splitting our data into a training set and a test set, with an 85-15
    split, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集不包含缺失值，因此不需要移除或修改任何观测值。我们会注意到，我们只有大约200个数据点，这在一般情况下被认为是一个非常小的样本。尽管如此，我们将继续将我们的数据分为训练集和测试集，比例为85-15，如下所示：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have our data set up-and-running, we''ll usually want to investigate
    further and check whether some of our assumptions for linear regression are valid.
    For example, we would like to know whether we have any highly correlated features.
    To do this, we can construct a correlation matrix with the `cor()`function and
    use the `findCorrelation()` function from the `caret` package to get suggestions
    for which features to remove:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了数据集并开始运行，我们通常会想进一步调查并检查我们的一些线性回归假设是否有效。例如，我们想知道我们是否有高度相关的特征。为此，我们可以使用`cor()`函数构建一个相关矩阵，并使用`caret`包中的`findCorrelation()`函数来获取关于哪些特征需要移除的建议：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using the default cutoff of `0.9` for a high degree of correlation, we found
    that none of our features should be removed. When we reduce this cutoff to `0.75`,
    we see that `caret` recommends that we remove the third feature (MMAX). As the
    final line of the preceding code shows, the degree of correlation between this
    feature and MMIN is `0.768`. While the value is not very high, it is still high
    enough to cause us a certain degree of concern that this will affect our model.
    Intuitively, of course, if we look at the definitions of our input features, we
    will certainly tend to expect that a model with a relatively high value for the
    minimum main memory will also be likely to have a relatively high value for the
    maximum main memory. Linear regression can sometimes still give us a good model
    with correlated variables, but we would expect to get better results if our variables
    were uncorrelated. For now, we've decided to keep all our features for this dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认的`0.9`截止值以获得高度相关性，我们发现我们没有任何特征应该被移除。当我们把这个截止值降低到`0.75`时，我们看到`caret`建议我们移除第三个特征（MMAX）。正如前面代码的最后一行所示，这个特征和MMIN之间的相关性为`0.768`。虽然这个值不是很高，但它仍然足够高，让我们对它可能会影响我们的模型产生一定的担忧。直观上，当然，如果我们查看我们输入特征的定义，我们肯定会倾向于期望一个最小主存值相对较高的模型也可能会有一个相对较高的最大主存值。线性回归有时仍然可以用相关变量给出一个好的模型，但我们预计如果我们的变量不相关，我们会得到更好的结果。目前，我们决定保留这个数据集的所有特征。
- en: Predicting the price of used cars
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测二手车价格
- en: 'Our second dataset is in the cars data frame included in the `caret` package
    and was collected by *Shonda Kuiper* in 2008 from the *Kelly Blue Book* website,
    [www.kbb.com](http://www.kbb.com). This is an online resource to obtain reliable
    prices for used cars. The dataset comprises 804 GM cars, all with the model year
    2005\. It includes a number of car attributes, such as the mileage and engine
    size as well as the suggested selling price. Many features are binary indicator
    variables, such as the Buick feature, which represents whether a particular car''s
    make is Buick. The cars were all in excellent condition and less than one year
    old when priced, so the car condition is not included as a feature. Our objective
    for this dataset is to build a model that will predict the selling price of a
    car using the values of these attributes. The definitions of the features are
    as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个数据集包含在`caret`包中的`cars`数据框中，由*Shonda Kuiper*在2008年从*Kelly Blue Book*网站收集，[www.kbb.com](http://www.kbb.com)。这是一个在线资源，可以获取可靠的二手车价格。数据集包括804辆通用汽车，所有车辆都是2005年的型号。它包括许多汽车属性，如里程和发动机尺寸以及建议的售价。许多特征是二元指示变量，例如Buick特征，表示一辆特定汽车的制造商是否为Buick。当定价时，这些汽车都处于极好状态且不到一年，因此汽车状况没有被包括为特征。我们的目标是构建一个模型，使用这些属性的值来预测汽车的售价。特征的定义如下：
- en: '| Column name | Definition |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 定义 |'
- en: '| --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Price` | The suggested retail price in USD (our output variable) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `价格` | 美元（我们的输出变量）的建议零售价 |'
- en: '| `Mileage` | The number of miles the car has been driven |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `里程` | 汽车行驶的英里数 |'
- en: '| `Cylinder` | The number of cylinders in the car''s engine |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `汽缸数` | 汽车发动机的汽缸数 |'
- en: '| `Doors` | The number of doors |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `车门数` | 车门数量 |'
- en: '| `Cruise` | The indicator variable representing whether the car has cruise
    control |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `Cruise` | 表示汽车是否具有定速巡航的指示变量 |'
- en: '| `Sound` | The indicator variable representing whether the car has upgraded
    speakers |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `Sound` | 表示汽车是否升级了扬声器的指示变量 |'
- en: '| `Leather` | The indicator variable representing whether the car has leather
    seats |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `Leather` | 表示汽车是否配备了真皮座椅的指示变量 |'
- en: '| `Buick` | The indicator variable representing whether the make of the car
    is Buick |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `Buick` | 表示汽车品牌是否为别克 的指示变量 |'
- en: '| `Cadillac` | The indicator variable representing whether the make of the
    car is Cadillac |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `Cadillac` | 表示汽车品牌是否为凯迪拉克的指示变量 |'
- en: '| `Chevy` | The indicator variable representing whether the make of the car
    is Chevy |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `Chevy` | 表示汽车品牌是否为雪佛兰的指示变量 |'
- en: '| `Pontiac` | The indicator variable representing whether the make of the car
    is Pontiac |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `Pontiac` | 表示汽车品牌是否为庞蒂亚克的指示变量 |'
- en: '| `Saab` | The indicator variable representing whether the make of the car
    is Saab |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `Saab` | 表示汽车品牌是否为萨博的指示变量 |'
- en: '| `Saturn` | The indicator variable representing whether the make of the car
    is Saturn |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `Saturn` | 表示汽车品牌是否为土星的指示变量 |'
- en: '| `convertible` | The indicator variable representing whether the type of the
    car is a convertible |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `convertible` | 表示汽车类型是否为敞篷车的指示变量 |'
- en: '| `coupe` | The indicator variable representing whether the type of the car
    is a coupe |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `coupe` | 表示汽车类型是否为敞篷车的指示变量 |'
- en: '| `hatchback` | The indicator variable representing whether the type of the
    car is a hatchback |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `hatchback` | 表示汽车类型是否为掀背车的指示变量 |'
- en: '| `sedan` | The indicator variable representing whether the type of the car
    is a sedan |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `sedan` | 表示汽车类型是否为轿车的指示变量 |'
- en: '| `wagon` | The indicator variable representing whether the type of the car
    is a wagon |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `wagon` | 表示汽车类型是否为旅行车的指示变量 |'
- en: 'As with the machine dataset, we should investigate the correlation between
    the input features:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器数据集一样，我们应该调查输入特征之间的相关性：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Just as with the machine dataset, we have a correlation that shows up when we
    set `cutoff` to `0.75` in the `findCorrelation()` function of `caret`. By directly
    examining the correlation matrix, we found that there is a relatively high degree
    of correlation between the `Doors` feature and the `coupe` feature. By cross-tabulating
    these two, we can see why this is the case. If we know that the type of a car
    is a coupe, then the number of doors is always two. If the car is not a coupe,
    then it most likely has four doors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就像机器数据集一样，当我们把`caret`包中的`findCorrelation()`函数的`cutoff`设置为`0.75`时，会出现相关性。通过直接检查相关矩阵，我们发现`Doors`特征和`coupe`特征之间存在相对较高的相关性。通过交叉表分析这两个特征，我们可以看到为什么会出现这种情况。如果我们知道一辆车的类型是敞篷车，那么车门数量总是两个。如果汽车不是敞篷车，那么它很可能有四个车门。
- en: 'Another problematic aspect of the cars data is that some features are exact
    linear combinations of other features. This is discovered using the `findLinearCombos()`
    function in the caret package:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车数据中另一个问题方面是，一些特征是其他特征的精确线性组合。这是通过使用`caret`包中的`findLinearCombos()`函数发现的：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we are advised to drop the `coupe` and `wagon` columns, which are the
    15^(th) and 18^(th) features, respectively, because they are exact linear combinations
    of other features. We will remove both of these from our data frame, thus also
    eliminating the correlation problem we saw previously.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们被建议删除`coupe`和`wagon`列，它们分别是第15和第18个特征，因为它们是其他特征的精确线性组合。我们将从我们的数据框中删除这两列，从而消除我们之前看到的关联问题。
- en: 'Next, we''ll split our data into training and test sets:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分为训练集和测试集：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have our data ready, we'll build some models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，我们将构建一些模型。
- en: Assessing linear regression models
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估线性回归模型
- en: 'We''ll once again use the `lm()` function to fit linear regression models to
    our data. For both of our datasets, we''ll want to use all the input features
    that remain in our respective data frames. R provides us with a shorthand to write
    formulas that include all the columns of a data frame as features, excluding the
    one chosen as the output. This is done using a single period, as the following
    code snippets show:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用`lm()`函数将线性回归模型拟合到我们的数据上。对于我们的两个数据集，我们希望使用各自数据框中剩余的所有输入特征。R为我们提供了一个简写，可以编写包含数据框所有列作为特征的公式，排除选定的输出列。这可以通过单个点来完成，如下面的代码片段所示：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Training a linear regression model may be a one-line affair once we have all
    our data prepared, but the important work comes straight after, when we study
    our model in order to determine how well we did. Fortunately, we can instantly
    obtain some important information about our model using the `summary()` function.
    The output of this function for our CPU dataset is shown here:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了所有数据，训练线性回归模型可能只是一行代码的事情，但重要的工作紧接着就开始了，我们需要研究模型以确定我们做得如何。幸运的是，我们可以通过使用`summary()`函数立即获取有关模型的一些重要信息。此函数对我们CPU数据集的输出如下所示：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Following a repeat of the call we made to the `lm()` function itself, the information
    provided by the `summary()` function is organized into three distinct sections.
    The first section is a summary of the model residuals, which are the errors that
    our model makes on the observations in the data on which it was trained. The second
    section is a table containing the predicted values of the model coefficients as
    well as the results of their significance tests. The final few lines display overall
    performance metrics for our model. If we repeat the same process on our cars dataset,
    we will notice the following line in our model summary:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在重复调用`lm()`函数本身之后，`summary()`函数提供的信息被组织成三个不同的部分。第一部分是模型残差的总结，这些残差是我们模型在训练数据上观测到的误差。第二部分是一个表格，包含模型系数的预测值以及它们显著性测试的结果。最后几行显示了模型的总体性能指标。如果我们对汽车数据集重复同样的过程，我们会在模型总结中注意到以下这一行：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This occurs because we still have a feature whose effect on the output is indiscernible
    from other features due to underlying dependencies. This phenomenon is known as
    **aliasing**. The `alias()` command shows the features we need to remove from
    the model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况发生是因为我们仍然有一个特征，其影响输出效果与其他特征不可区分，这是由于潜在的依赖关系。这种现象被称为**混叠**。`alias()`命令显示了我们需要从模型中移除的特征：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we can see, the problematic feature is the `Saturn` feature, so we will
    remove this feature and retrain the model. To exclude a feature from a linear
    regression model, we include it in the formula after the period and prefix it
    with a minus sign:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，问题特征是`Saturn`，因此我们将移除这个特征并重新训练模型。要排除线性回归模型中的一个特征，我们在公式中包含它，并在其后加上一个减号：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Residual analysis
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差分析
- en: 'A residual is simply the error our model makes for a particular observation.
    Put differently, it is the difference between the actual value of the output and
    our prediction:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 残差简单来说就是我们的模型对特定观测值产生的误差。换句话说，它是实际输出值与我们的预测值之间的差异：
- en: '![Residual analysis](img/00044.jpeg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![残差分析](img/00044.jpeg)'
- en: 'Analyzing residuals is very important when building a good regression model,
    as residuals reveal various aspects of our model, from violated assumptions and
    the quality of the fit to other problems, such as outliers. To understand the
    metrics in the residual summary, imagine ordering residuals from the smallest
    to the largest. Besides the minimum and maximum values that occur at the extremes
    of this sequence, the summary shows the first and third quartiles, which are the
    values a quarter along the way in this sequence and three quarters, respectively.
    The **median** is the value in the middle of the sequence. The **interquartile
    range** is the portion of the sequence between the first and third quartiles,
    and by definition contains half of the data. Looking first at the residual summary
    from our CPU model, it is interesting to note that the first and third quartiles
    are quite small in value compared to the minimum and maximum value. This is a
    first indication that there might be a few points that have a large residual error.
    In an ideal scenario, our residuals will have a median of zero and will have small
    values for the quartiles. We can reproduce the residuals summary from the summary
    function by noting that the model produced by the `lm()` function has a `residuals`
    attribute:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建一个好的回归模型时，分析残差非常重要，因为残差揭示了我们的模型的各种方面，从违反的假设和拟合质量到其他问题，例如异常值。为了理解残差摘要中的指标，想象一下将残差从小到大排序。除了出现在序列两端的极值最小值和最大值之外，摘要还显示了第一和第三四分位数，它们分别代表在这个序列中四分之一和三分之四处的值。**中位数**是序列中间的值。**四分位距**是第一和第三四分位数之间的序列部分，并且根据定义包含了一半的数据。首先看看我们CPU模型的残差摘要，有趣的是，与最小值和最大值相比，第一和第三四分位数的值相当小。这是第一个迹象，表明可能有一些点具有较大的残差误差。在理想情况下，我们的残差将有一个中位数为零，并且四分位数将具有较小的值。我们可以通过注意由`lm()`函数生成的模型具有`residuals`属性来重现残差摘要：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that, in the preceding example for our cars model, we need to compare the
    value of the residuals against the average value of the output variable, in order
    to get a sense of whether the residuals are large or not. Thus, the previous results
    show that the average selling price of a car in our training data is around $21
    k, and 50% of our predictions are roughly within ± $1.6 k of the correct value,
    which seems fairly reasonable. Obviously, the residuals for our CPU model are
    all much smaller in the absolute value because the values of the output variable
    for that model, namely the published relative performance, are much smaller than
    the values for `Price` in the cars model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们之前的cars模型示例中，我们需要将残差值与输出变量的平均值进行比较，以便了解残差是否很大。因此，之前的结果表明，我们训练数据中汽车的平均售价约为21千美元，并且50%的预测值大致在正确值的±1.6千美元范围内，这似乎相当合理。显然，我们CPU模型的残差绝对值都小得多，因为该模型的输出变量值，即发布的相对性能，比cars模型中的`Price`值小得多。
- en: In linear regression, we assume that the irreducible errors in our model are
    randomly distributed with a normal distribution. A diagnostic plot, known as the
    **Quantile-Quantile plot** (**Q-Q plot**), is useful in helping us visually gauge
    the extent to which this assumption holds. The key idea behind this plot is that
    we can compare two distributions by comparing the values at their **quantiles**.
    The quantiles of a distribution are essentially evenly spaced intervals of a random
    variable, such that each interval has the same probability; for example, quartiles
    are four-quantiles because they split up a distribution into four equally probable
    parts. If the two distributions are the same, then the graph should be a plot
    of the line *y = x*. To check whether our residuals are normally distributed,
    we can compare their distribution against a normal distribution and see how close
    to the *y = x* line we land.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们假设模型中的不可减少误差是随机分布的，服从正态分布。一种称为**分位数-分位数图**（**Q-Q图**）的诊断图有助于我们直观地评估这种假设的成立程度。这种图背后的关键思想是，我们可以通过比较两个分布的**分位数**来比较这两个分布。分布的分位数实际上是随机变量的等间距区间，每个区间具有相同的概率；例如，四分位数是四分位数，因为它们将分布分成四个等可能的四部分。如果两个分布相同，那么图表应该是一条线*y
    = x*的图。为了检查我们的残差是否服从正态分布，我们可以将它们的分布与正态分布进行比较，并看看我们离*y = x*线有多近。
- en: Tip
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are many other ways to check whether the model residuals are normally
    distributed. A good place to look is the `nortest` R package, which implements
    a number of well-known tests for normality, including the Anderson-Darling test
    and the Lilliefors test. In addition, the `stats` package contains the `shapiro.test()`
    function for performing the Shapiro-Wilk normality test.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他方法可以检查模型残差是否呈正态分布。一个好的地方是查看`nortest` R包，它实现了许多著名的正态性测试，包括安德森-达尔林测试和Lilliefors测试。此外，`stats`包包含用于执行Shapiro-Wilk正态性测试的`shapiro.test()`函数。
- en: 'The following code generates Q-Q plots for our two datasets:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成我们的两个数据集的Q-Q图：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following diagram displays the Q-Q plots:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了Q-Q图：
- en: '![Residual analysis](img/00045.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![残差分析](img/00045.jpeg)'
- en: The residuals from both models seem to lie reasonably close to the theoretical
    quantiles of a normal distribution, although the fit isn't perfect, as is typical
    with most real-world data. A second very useful diagnostic plot for a linear regression
    is the so-called **residual plot**. This is a plot of residuals against corresponding
    fitted values for the observations in the training data. In other words, this
    is a plot of the pairs (*i*, *e[i]*). There are two important properties of the
    residual plot that interest us in particular. Firstly, we would like to confirm
    our assumption of constant variance by checking whether the residuals are not
    larger on average for a range of fitted values and smaller in a different range.
    Secondly, we should verify that there isn't some sort of pattern in the residuals.
    If a pattern is observed, however, it may be an indication that the underlying
    model is nonlinear in terms of the features involved or that there are additional
    features missing from our model that we have not included. In fact, one way of
    discovering new features that might be useful for our model is to look for new
    features that are correlated with our model's residuals.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的残差似乎合理地接近正态分布的理论分位数，尽管拟合并不完美，这在大多数现实世界的数据中是典型的。对于线性回归来说，第二个非常有用的诊断图是所谓的**残差图**。这是训练数据中观测值的残差与对应拟合值的图。换句话说，这是(*i*,
    *e[i]*)对的图。残差图有两个重要的特性特别引起我们的兴趣。首先，我们希望通过检查残差是否在拟合值的不同范围内平均上不是更大，而是更小，来确认我们的常数方差假设。其次，我们应该验证残差中是否存在某种模式。然而，如果观察到模式，这可能表明基础模型在涉及的特征方面是非线性的，或者我们的模型中缺少一些我们没有包括的额外特征。实际上，发现可能对我们模型有用的新特征的一种方法是寻找与我们模型残差相关的特征。
- en: '![Residual analysis](img/00046.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![残差分析](img/00046.jpeg)'
- en: Both plots show a slight pattern of decreasing residuals in the left part of
    the graph. Slightly more worrying is the fact that the variance of the residuals
    seems to be a little higher for higher values of both output variables, which
    could indicate that the errors are not homoscedastic. This is more pronounced
    in the second plot for the cars datasets. In the preceding two residual plots,
    we have also labeled some of the larger residuals (in absolute magnitude). We'll
    see shortly that these are potential candidates for outliers. Another way to obtain
    a residual plot is to use the `plot()` function on the model produced by the `lm()`
    function itself. This generates four diagnostic plots, including the residual
    plot and the Q-Q plot.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 两个图都显示了图形左侧残差略微减少的模式。更令人担忧的是，残差的方差似乎对于两个输出变量的较高值都要高一些，这可能表明误差不是同方差。这在第二个关于汽车数据集的图中更为明显。在前两个残差图中，我们还标记了一些较大的残差（以绝对值计）。我们很快就会看到这些是潜在的异常值候选者。另一种获得残差图的方法是使用`lm()`函数生成的模型上的`plot()`函数。这生成了四个诊断图，包括残差图和Q-Q图。
- en: Significance tests for linear regression
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归的显著性测试
- en: After scrutinizing the residual summaries, the next thing we should focus on
    is the table of coefficients that our models have produced. Here, every estimated
    coefficient is accompanied by an additional set of numbers, as well as a number
    of stars or a dot at the end. At first, this may seem confusing because of the
    barrage of numbers, but there is a good reason why all this information is included.
    When we collect measurements on some data and specify a set of features to build
    a linear regression model, it is often the case that one or more of these features
    are not actually related to the output we are trying to predict. Of course, this
    is something we are generally not aware of beforehand when we are collecting data.
    Ideally, we would want our model to not only find the best values for the coefficients
    that correspond to the features that our output does actually depend on, but also
    to tell us which of the features we don't need.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在仔细审查残差摘要之后，我们接下来应该关注的是我们模型产生的系数表。在这里，每个估计系数都伴随着一组额外的数字，以及一个或多个星号或点在末尾。起初，由于数字的冲击，这可能会让人感到困惑，但所有这些信息都包含在内是有很好的理由的。当我们对某些数据进行测量并指定一组特征来构建线性回归模型时，通常情况下，这些特征中的一个或多个实际上与我们要预测的输出无关。当然，在我们收集数据之前，我们通常不会意识到这一点。理想情况下，我们希望我们的模型不仅找到与我们的输出实际依赖的特征相对应的系数的最佳值，而且还告诉我们哪些特征我们不需要。
- en: One possible approach for determining whether a particular feature is needed
    in our model is to train two models instead of one. The second model will have
    all the features of the first model, excluding the specific feature whose significance
    we are trying to ascertain. We can then test whether the two models are different
    by looking at their distributions of residuals. This is actually what R does for
    all of the features that we have specified in each model. For each coefficient,
    a **confidence interval** is constructed for the null hypothesis that its corresponding
    feature is unrelated to the output variable. Specifically, for each coefficient,
    we consider a linear model with all the other features included, except the feature
    that corresponds to this coefficient. Then, we test whether adding this particular
    feature to the model significantly changes the distribution of residual errors,
    which would be evidence of a linear relationship between this feature and the
    output and that its coefficient should be nonzero. R's `lm()` function automatically
    runs these tests for us.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 确定我们模型中某个特定特征是否需要的可能方法之一是训练两个模型而不是一个。第二个模型将包含第一个模型的所有特征，但不包括我们试图确定其重要性的特定特征。然后，我们可以通过查看它们的残差分布来测试这两个模型是否不同。这正是R为我们每个模型中指定的所有特征所做的事情。对于每个系数，都会为对应特征与输出变量无关的零假设构建一个**置信区间**。具体来说，对于每个系数，我们考虑一个包含所有其他特征的线性模型，除了与该系数对应的特征。然后，我们测试是否将这个特定特征添加到模型中会显著改变残差误差的分布，这将作为该特征与输出之间存在线性关系的证据，并且其系数不应为零。R的`lm()`函数会自动为我们运行这些测试。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In statistics, a confidence interval combines a point estimate with the precision
    of that estimate. This is done by specifying an interval in which the true value
    of the parameter that is being estimated is expected to lie under a certain degree
    of confidence. A 95% globally confidence interval for a parameter essentially
    tells us that, if we were to collect 100 samples of data from the same experiment
    and construct a 95 percent confidence interval for the estimated parameter in
    each sample, the real value of the target parameter would lie within its corresponding
    confidence interval for 95 of these data samples. Confidence intervals that are
    constructed for point estimates with high variance, such as when the estimate
    is being made with very few data points, will tend to define a wider interval
    for the same degree of confidence than estimates made with low variance.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，置信区间结合了点估计的精度。这是通过指定一个区间来完成的，在该区间内，估计的参数的真实值预计将在一定程度的置信度下。一个参数的95%全局置信区间基本上告诉我们，如果我们从同一实验中收集100个数据样本，并为每个样本中估计的参数构建一个95%的置信区间，那么目标参数的真实值将位于其对应的置信区间内95个数据样本。对于具有高方差的点估计构建的置信区间，例如当使用非常少的数据点进行估计时，往往会定义一个更宽的区间，以相同的置信度来定义，比使用低方差进行的估计。
- en: 'Let''s look at a snapshot of the summary output for the CPU model, which shows
    the coefficient for the intercept and the MYCT feature in the CPU model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下CPU模型的摘要输出的快照，它显示了CPU模型中截距和MYCT特征的系数：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Focusing on the MYCT feature for the moment, the first number in its row is
    the estimate of its coefficient, and this number is roughly 0.05 (*5.210×10^(-2)*).
    The **standard error** is the standard deviation of this estimate, and this is
    given next as 0.01885\. We can gauge our confidence as to whether the value of
    our coefficient is really zero (indicating no linear relationship for this feature)
    by counting the number of standard errors between zero and our coefficient estimate.
    To do this, we can divide our coefficient estimate by our standard error, and
    this is precisely the definition of the **t-value**, the third value in our row:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目前专注于MYCT特征，其所在行中的第一个数字是它系数的估计值，这个数字大约是0.05（*5.210×10^(-2)*）。**标准误差**是这个估计值的标准差，这个值接下来给出为0.01885。我们可以通过计算零和我们的系数估计值之间的标准误差数量来衡量我们对系数值是否真正为零（表示此特征的线性关系不存在）的信心。为此，我们可以将我们的系数估计值除以我们的标准误差，这正是**t值**的定义，我们行中的第三个值：
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, our MYCT coefficient is almost three standard errors away from zero, which
    is a fairly good indicator that this coefficient is not likely to be zero. The
    higher the t-value, the more likely we should be including our feature in our
    linear model with a nonzero coefficient. We can turn this absolute value into
    a probability that tells us how likely it is that our coefficient should really
    be zero. This probability is obtained from Student''s t-distribution and is known
    as the **p-value**. For the MYCT feature, this probability is 0.006335, which
    is small. We can obtain this value for ourselves using the `pt()` function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的MYCT系数几乎有3个标准误差远离零，这是一个相当好的指标，表明这个系数不太可能是零。t值越高，我们越有可能在我们的线性模型中包含我们的特征，并且系数不为零。我们可以将这个绝对值转换成一个概率，告诉我们系数真正为零的可能性有多大。这个概率是从Student's
    t分布获得的，称为**p值**。对于MYCT特征，这个概率是0.006335，这个值很小。我们可以使用`pt()`函数获得这个值：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `pt()` function is the distribution function for the t-distribution, which
    is symmetric. To understand why our p-value is computed this way, note that we
    are interested in the probability of the absolute value of the t-value being larger
    than the value we computed. To obtain this, we first obtain the probability of
    the upper or right tail of the t-distribution and multiply this by 2 in order
    to include the lower tail as well. Working with basic distribution functions is
    a very important skill in R, and we have included examples in our online tutorial
    chapter on R if this example seems overly difficult. The t-distribution is parameterized
    by the degrees of freedom.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`pt()`函数是t分布的分布函数，它是对称的。为了理解为什么我们的p值是这样计算的，请注意，我们感兴趣的是t值的绝对值大于我们计算出的值的概率。为了获得这个值，我们首先获得t分布的上尾或右尾的概率，然后乘以2，以便包括下尾。在R中使用基本分布函数是一个非常重要的技能，如果这个例子看起来过于困难，我们已经在我们的在线教程章节中包含了示例。t分布由自由度参数化。'
- en: Note
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The number of degrees of freedom is essentially the number of variables that
    we can freely change when calculating a particular statistic, such as a coefficient
    estimate. In our linear regression context, this amounts to the number of observations
    in our training data minus the number of parameters in the model (the number of
    regression coefficients). For our CPU model, this number is *179 – 7 = 172*. For
    the cars model where we have more data points, this number is 664\. The name comes
    from its relation to the number of independent dimensions or pieces of information
    that are applied as input to a system, and hence reflects the extent to which
    the system can be freely configured without violating any constraints on the input.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自由度基本上是我们计算特定统计量（如系数估计）时可以自由改变的变量的数量。在我们的线性回归背景下，这相当于我们的训练数据中的观测数减去模型中的参数数（回归系数的数量）。对于我们的CPU模型，这个数字是*179
    – 7 = 172*。对于数据点更多的汽车模型，这个数字是664。这个名字来源于它与作为系统输入应用的独立维度或信息数量的关系，因此反映了系统在不违反任何输入约束的情况下可以自由配置的程度。
- en: As a general rule of thumb, we would like our p-values to be less than 0.05,
    which is the same as saying that we would like to have 95 percent confidence intervals
    for our coefficient estimates that do not include zero. The number of stars next
    to each coefficient provides us with a quick visual aid for what the confidence
    level is, and a single star corresponds to our 95 percent rule of thumb while
    two stars represent a 99 percent confidence interval. Consequently, every coefficient
    in our model summary that does not have any stars corresponds to a feature that,
    we are not confident we should include in our model using our rule of thumb. In
    the CPU model, the CHMIN feature is the only feature that is suspect, with the
    other p-values being very small. The situation is different with the cars model.
    Here, we have four features that are suspect as well as the intercept.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们希望我们的p值小于0.05，这意味着我们希望我们的系数估计的95%置信区间不包括零。每个系数旁边的星号数量为我们提供了一个快速的可视辅助工具，以了解置信水平，一个星号对应我们的95%经验法则，而两个星号则代表99%的置信区间。因此，我们模型总结中没有任何星号的每个系数都对应一个特征，我们不太确定是否应该使用我们的经验法则将其包含在模型中。在CPU模型中，CHMIN特征是唯一一个可疑的特征，其他特征的p值都非常小。在cars模型中，情况则不同。这里，我们有四个可疑的特征，包括截距项。
- en: It is important to properly understand the interpretation of p-values in the
    context of our linear regression model. Firstly, we cannot and should not compare
    p-values against each other in order to gauge which feature is the most important.
    Secondly, a high p-value does not necessarily indicate that there is no linear
    relationship between a feature and the output; it only suggests that in the presence
    of all the other model features, this feature does not provide any new information
    about the output variable. Finally, we should always remember that the 95 percent
    rule of thumb is not infallible and is only really useful when the number of features
    and hence coefficients is not very large. Under 95 percent confidence, if we have
    1,000 features in our model, we can expect to get the wrong result for 50 coefficients
    on average. Consequently, linear regression coefficient significance tests aren't
    as useful for problems in high dimensions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的线性回归模型背景下，正确理解p值的解释非常重要。首先，我们不能也不应该将p值相互比较，以判断哪个特征最重要。其次，高p值并不一定意味着特征与输出之间没有线性关系；它仅仅表明，在所有其他模型特征存在的情况下，这个特征不会为输出变量提供任何新的信息。最后，我们应始终记住，95%的经验法则并非完美无缺，并且只有在特征和系数数量不是很大时才真正有用。在95%的置信水平下，如果我们模型中有1,000个特征，我们平均可以期望有50个系数的结果是错误的。因此，线性回归系数显著性检验在处理高维问题时不那么有用。
- en: The final test of significance actually appears at the very bottom of the summary
    of the `lm()` output and is on the last line. This line provides us with the **F
    statistic**, which gets its name from the F test, which checks whether there is
    a statistical significance between the variances of two (ideally normal) distributions.
    The F statistic in this case tries to assess whether the variance of the residuals
    from a model in which all coefficients are zero is significantly different from
    the variance of the residuals from our trained model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性检验的最终结果实际上出现在`lm()`输出摘要的底部，位于最后一行。这一行提供了**F统计量**，这个名字来源于F检验，该检验检查两个（理想情况下为正态分布）分布的方差之间是否存在统计显著性。在这个情况下，F统计量试图评估所有系数为零的模型的残差方差与训练模型的残差方差之间是否存在显著的差异。
- en: 'Put differently, the F test will tell us whether the trained model explains
    some of the variance in the output, and hence we know that at least one of the
    coefficients must be nonzero. While not as useful when we have many coefficients,
    this tests the significance of coefficients together and doesn''t suffer from
    the same problem as the t-test on the individual coefficients. The summary shows
    a tiny p-value for this, so we know that at least one of our coefficients is nonzero.
    We can reproduce the F test that was run using the `anova()` function, which stands
    for **analysis of variance**. This test compares the **null model**, which is
    the model built with just an intercept and none of the features, with our trained
    model. We''ll show this here for the CPU dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，F检验将告诉我们训练的模型是否解释了输出中的部分方差，因此我们知道至少有一个系数不为零。虽然当我们有许多系数时并不那么有用，但这个测试一起测试了系数的显著性，并且不会像对单个系数的t检验那样出现问题。总结显示了一个极小的p值，因此我们知道至少有一个系数不为零。我们可以使用`anova()`函数重现所运行的F检验，该函数代表**方差分析**。这个测试比较了**零模型**，即仅包含截距而没有特征构建的模型，与我们的训练模型。我们将在这里展示CPU数据集的示例：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that the formula of the null model is `PRP ~ 1`, where the 1 represents
    the intercept.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，零模型公式为 `PRP ~ 1`，其中1代表截距。
- en: Performance metrics for linear regression
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归的性能指标
- en: 'The final details in our summary are concerned with the performance of the
    model as a whole and the degree to which the linear model fits the data. To understand
    how we assess a linear regression fit, we should first point out that the training
    criterion of the linear regression model is to minimize the MSE on the data. In
    other words, fitting a linear model to a set of data points amounts to finding
    a line whose slope and position minimize the sum (or average) of the squared distances
    from these points. As we refer to the error between a data point and its predicted
    value on the line as the residual, we can define the **Residual Sum of Squares**
    (**RSS**) as the sum of all the squared residuals:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结中的最后细节涉及模型的整体性能以及线性模型拟合数据的程度。为了理解我们如何评估线性回归拟合，我们首先应该指出，线性回归模型的训练标准是最小化数据上的MSE。换句话说，将线性模型拟合到一组数据点相当于找到一个斜率和位置，使得这些点到直线的平方距离之和（或平均值）最小。由于我们将数据点与其在直线上的预测值之间的误差称为残差，我们可以将**残差平方和**（**RSS**）定义为所有平方残差的和：
- en: '![Performance metrics for linear regression](img/00047.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00047.jpeg)'
- en: 'In other words, RSS is just the **Sum of Squared Errors** (**SSE**), so we
    can relate to the MSE with which we are familiar via this simple equation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，RSS仅仅是**平方误差之和**（**SSE**），因此我们可以通过这个简单的方程与我们所熟悉的MSE（均方误差）联系起来：
- en: '![Performance metrics for linear regression](img/00048.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00048.jpeg)'
- en: Beyond certain historic reasons, RSS is an important metric to be aware of because
    it is related to another important metric, known as the RSE, which we will talk
    about next. For this, we'll need to first build up an intuition about what happens
    when we train linear regression models. If we run our simple linear regression
    experiment with artificial data a number of times, each time changing the random
    seed so that we get a different random sample, we'll see that we will get a number
    of regression lines that are likely to be very close to the true population line,
    just as our single run showed us. This illustrates the fact that linear models
    are characterized by low variance in general. Of course, the unknown function
    we are trying to approximate may very well be nonlinear and as a result, even
    the population regression line is not likely to be a good fit to the data for
    nonlinear functions. This is because the linearity assumption is very strict,
    and consequently, linear regression is a method with high bias.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除去某些历史原因之外，RSS是一个需要关注的 重要指标，因为它与另一个重要的指标RSE（残差平方和）相关，我们将在下一节中讨论。为此，我们首先需要建立起对训练线性回归模型时会发生什么的感觉。如果我们多次运行我们的简单线性回归实验，每次改变随机种子以获得不同的随机样本，我们会看到我们会得到多条回归线，这些回归线很可能非常接近真实总体线，正如我们的单次运行所显示的那样。这说明了线性模型通常具有低方差的特点。当然，我们试图逼近的未知函数可能非常非线性，因此，即使是总体回归线也不太可能很好地拟合非线性函数的数据。这是因为线性假设非常严格，因此，线性回归是一种具有高偏差的方法。
- en: 'We define a metric known as the **Residual Standard Error** (**RSE**), which
    estimates the standard deviation of our model compared to the target function.
    That is to say, it measures roughly how far away from the population regression
    line on average our model will be. This is measured in the units of the output
    variable and is an absolute value. Consequently, it needs to be compared against
    the values of *y* in order to gauge whether it is high or not for a particular
    sample. The general RSE for a model with *k* input features is computed as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个称为**残差标准误差**（**RSE**）的度量，它估计了我们的模型与目标函数的标准差。也就是说，它大致衡量了我们的模型平均偏离总体回归线的距离。这是以输出变量的单位来衡量的，是一个绝对值。因此，它需要与*y*的值进行比较，以判断对于特定的样本它是否很高。具有*k*个输入特征的模型的通用RSE计算如下：
- en: '![Performance metrics for linear regression](img/00049.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00049.jpeg)'
- en: 'For simple linear regression, this is just with *k = 1*:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单线性回归，这仅仅是*k = 1*时的情况：
- en: '![Performance metrics for linear regression](img/00050.jpeg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00050.jpeg)'
- en: 'We can compute the RSE for our two models using the preceding formula, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前面的公式来计算我们两个模型的RSE，如下所示：
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To interpret the RSE values for our two models, we need to compare them with
    the mean of our output variables:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释我们两个模型的RSE值，我们需要将它们与我们的输出变量均值进行比较：
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that, in the car model, the RSE of 61.3 is quite small compared to the
    RSE of the cars model, which is roughly 2,947\. When we look at these numbers
    in terms of how close they are to the means of their respective output variables,
    however, we learn that actually it is the cars model RSE that shows a better fit.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在汽车模型中，61.3的RSE与汽车模型的RSE相比非常小，后者大约为2,947。然而，当我们从这些数字与各自输出变量均值接近的程度来看时，我们了解到实际上汽车模型的RSE显示出更好的拟合。
- en: 'Now, although the RSE is useful as an absolute value in that one can compare
    it to the mean of the output variable, we often want a relative value that we
    can use to compare across different training scenarios. To this end, when evaluating
    the fit of linear regression models, we often also look at the **R2 statistic**.
    In the summary, this is denoted as multiple R-squared. Before we provide the equation,
    we''ll first present the notion of the **Total Sum of Squares** (**TSS**). The
    total sum of squares is proportional to the total variance in the output variable,
    and is designed to measure the amount of variability intrinsic to this variable
    before we perform our regression. The formula for TSS is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尽管RSE作为一个绝对值是有用的，因为可以将其与输出变量的均值进行比较，但我们经常想要一个相对值，我们可以用它来比较不同的训练场景。为此，在评估线性回归模型的拟合度时，我们通常也会查看**R²统计量**。在总结中，这表示为多重R平方。在我们提供方程之前，我们首先介绍**总平方和**（**TSS**）的概念。总平方和与输出变量的总方差成比例，旨在衡量我们在进行回归之前该变量的内在变异性。TSS的公式是：
- en: '![Performance metrics for linear regression](img/00051.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00051.jpeg)'
- en: 'The idea behind the R² statistic is that if a linear regression model is a
    close fit to the true population model, it should be able to completely capture
    all the variance in the output. In fact, we often refer to the R² statistic as
    the relative amount that shows us what proportion of the output variance is explained
    by the regression. When we apply our regression model to obtain an estimate of
    the output variable, we see that the errors in our observations are called residuals
    and the RSS is essentially proportional to the variance that is left between our
    prediction and the true values of the output function. Consequently, we can define
    the R² statistic, which is the amount of variance in our output *y* that our linear
    regression model explains, as the difference between our starting variance (TSS)
    and our ending variance (RSS) relative to our starting variance (TSS). As a formula,
    this is just:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: R²统计量的背后的思想是，如果一个线性回归模型与真实总体模型非常接近，它应该能够完全捕捉输出中的所有方差。实际上，我们经常将R²统计量称为相对量，它显示了输出方差中有多少比例是由回归解释的。当我们应用我们的回归模型来获得输出变量的估计时，我们看到我们的观察误差被称为残差，而RSS基本上与我们的预测和输出函数真实值之间剩余的方差成比例。因此，我们可以定义R²统计量，即我们的线性回归模型解释的输出*y*中的方差量，作为起始方差（TSS）和结束方差（RSS）相对于起始方差（TSS）的差值。作为一个公式，这仅仅是：
- en: '![Performance metrics for linear regression](img/00052.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00052.jpeg)'
- en: 'From this equation, we can see that R² ranges between 0 and 1\. A value close
    to 1 is indicative of a good fit as it means that most of the variance in the
    output variable has been explained by the regression model. A low value, on the
    other hand, indicates that there is still significant variance in the errors in
    the model, indicating that our model is not a good fit. Let''s see how the R2
    statistic can be computed manually for our two models:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程中，我们可以看到R²的范围在0到1之间。一个接近1的值表明拟合良好，因为它意味着输出变量的大部分方差已经被回归模型解释。另一方面，一个低值表明模型中仍然存在显著的误差方差，这表明我们的模型不是一个好的拟合。让我们看看如何手动计算我们两个模型的R²统计量：
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We used the `fitted.values` attribute of the model trained by `lm()`, which
    is the predictions the model makes on the training data. Both values are quite
    high, with the cars model again indicating a slightly better fit. We've now seen
    two important metrics to assess a linear regression model, namely RSE and the
    R² statistic. At this point, we might consider whether there is a more general
    measure of the linear relationship between two variables that we could also apply
    to our case. From statistics, we might recall that the notion of correlation describes
    exactly that.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了由`lm()`训练的模型的`fitted.values`属性，这是模型在训练数据上做出的预测。这两个值都相当高，汽车模型再次显示出略微更好的拟合。我们现在已经看到了两个评估线性回归模型的重要指标，即RSE和R²统计量。在这个时候，我们可能考虑是否存在一个更通用的度量两个变量之间线性关系的指标，我们也可以将其应用于我们的案例。从统计学中，我们可能会回忆起相关性的概念正是描述这一点。
- en: 'The **correlation** between two random variables, *X* and *Y*, is given by:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 两个随机变量*X*和*Y*之间的**相关性**由以下公式给出：
- en: '![Performance metrics for linear regression](img/00053.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归的性能指标](img/00053.jpeg)'
- en: It turns out that in the case of simple regression, the square of the correlation
    between the output variable and the input feature is the same as the R2 statistic,
    a result that further bolsters the importance of the latter as a useful metric.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在简单回归的情况下，输出变量和输入特征之间的相关性的平方与R²统计量相同，这一结果进一步强调了后者作为一个有用指标的重要性。
- en: Comparing different regression models
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较不同的回归模型
- en: When we want to compare two different regression models that have been trained
    on the same set of input features, the R² statistic can be very useful. Often,
    however, we want to compare two models that don't have the same number of input
    features. For example, during the process of feature selection, we may want to
    know whether including a particular feature in our model is a good idea. One of
    the limitations of the R² statistic is that it tends to be higher for models with
    more input parameters.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要比较在相同输入特征集上训练的两个不同的回归模型时，R²统计量非常有用。然而，通常我们想要比较两个没有相同数量输入特征的模型。例如，在特征选择的过程中，我们可能想知道是否在我们的模型中包含某个特定特征是一个好主意。R²统计量的一个局限性是它往往对于具有更多输入参数的模型更高。
- en: 'The **adjusted R²** attempts to correct the fact that R² always tends to be
    higher for models with more input features and hence is susceptible to overfitting.
    The adjusted R² is generally lower than R² itself, as we can verify by checking
    the values in our model summaries. The formula for the adjusted R² is:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整R²**试图纠正R²总是对于具有更多输入特征的模型更高的现象，因此容易过拟合。调整R²通常低于R²本身，正如我们可以通过检查我们的模型摘要中的值来验证的那样。调整R²的公式是：'
- en: '![Comparing different regression models](img/00054.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![比较不同的回归模型](img/00054.jpeg)'
- en: 'The definitions of *n* and *k* are the same as those for the R² statistic.
    Now, let''s implement this function in R and compute the adjusted R² for our two
    models:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*和*k*的定义与R²统计量的定义相同。现在，让我们在R中实现这个函数并计算我们两个模型的调整R²：'
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There are several other commonly used performance metrics designed to compare
    models with a different number of features. The **Akaike Information Criterion**
    (**AIC**) uses an information theoretic approach to assess the relative quality
    of a model by balancing model complexity and accuracy. For linear regression models
    trained by minimizing the squared error, this is proportional to another well-known
    statistic, **Mallow's Cp**, so these can be used interchangeably. A third metric
    is the **Bayesian Information Criterion** (**BIC**). This tends to penalize models
    with more variables more heavily, compared to the previous metrics.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种其他常用的性能指标，旨在比较具有不同特征数量的模型。**赤池信息量准则（AIC**）使用信息论方法，通过平衡模型复杂度和准确性来评估模型的相对质量。对于通过最小化平方误差训练的线性回归模型，这与其他已知的统计量**马尔可夫Cp（Mallow's
    Cp**）成比例，因此它们可以互换使用。第三个指标是**贝叶斯信息量准则（BIC**）。与之前的指标相比，它倾向于对具有更多变量的模型进行更重的惩罚。
- en: Test set performance
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试集性能
- en: 'So far, we''ve looked at the performance of our models in terms of the training
    data. This is important in order to gauge whether a linear model can fit the data
    well, but doesn''t give us a good sense of predictive accuracy over unseen data.
    For this, we turn to our test datasets. To use our model to make predictions,
    we can use the `predict()` function. This is a general function in R that many
    packages extend. With models trained with `lm()`, we simply need to provide the
    model and a data frame with the observations that we want to predict:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从训练数据的角度来评估我们模型的性能。这对于判断一个线性模型是否能够很好地拟合数据非常重要，但它并不能给我们一个关于未见数据预测准确性的良好感觉。为此，我们转向我们的测试数据集。为了使用我们的模型进行预测，我们可以使用`predict()`函数。这是R中一个通用的函数，许多包都对其进行了扩展。对于使用`lm()`训练的模型，我们只需要提供模型和一个包含我们想要预测的观测值的数据框：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we''ll define our own function for computing the MSE:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们自己的函数来计算均方误差（MSE）：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For each model, we've used our `compute_mse()` function to return the training
    and test MSE. It happens that in this case both test MSE values are smaller than
    the train MSE values. Whether the test MSE is slightly larger or smaller than
    the train MSE is not particularly important. The important issue is that the test
    MSE is not significantly larger than the train MSE as this would indicate that
    our model is overfitting the data. Note that, especially for the CPU model, the
    number of observations in the original data set is very small and this has resulted
    in a test set size that is also very small. Consequently, we should be conservative
    with our confidence in the accuracy of these estimates for the predictive performance
    of our models on unseen data, because predictions made using a small test set
    size will have a higher variance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，我们已经使用我们的`compute_mse()`函数来返回训练和测试的MSE。在这种情况下，两个测试MSE值都小于训练MSE值。测试MSE是略微大于还是小于训练MSE并不特别重要。重要的问题是测试MSE并没有显著大于训练MSE，因为这会表明我们的模型正在过度拟合数据。请注意，特别是对于CPU模型，原始数据集中的观测数非常少，这导致了测试集的大小也非常小。因此，我们应该对我们的模型在未见数据上的预测性能的准确性保持谨慎，因为使用小测试集大小做出的预测将具有更高的方差。
- en: Problems with linear regression
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归问题
- en: In this chapter, we've already seen some examples where trying to build a linear
    regression model might run into problems. One big class of problems that we've
    talked about is related to our model assumptions of linearity, feature independence,
    and the homoscedasticity and normality of errors. In particular we saw methods
    of diagnosing these problems either via plots, such as the residual plot, or by
    using functions that identify dependent components. In this section, we'll investigate
    a few more issues that can arise with linear regression.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到了一些例子，说明尝试构建线性回归模型可能会遇到问题。我们讨论的一个大类别的问题与我们的模型假设——线性、特征独立性和误差的同方差性和正态性有关。特别是，我们看到了通过绘图（如残差图）或使用识别相关成分的函数来诊断这些问题的方法。在本节中，我们将探讨一些可能出现在线性回归中的更多问题。
- en: Multicollinearity
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多重共线性
- en: As part of our preprocessing steps, we were diligent in removing features that
    were linearly related to each other. In doing this, we were looking for an exact
    linear relationship and this is an example of **perfect collinearity**. **Collinearity**
    is the property that describes when two features are approximately in a linear
    relationship. This creates a problem for linear regression as we are trying to
    assign separate coefficients to variables that are almost linear functions of
    each other. This can result in a situation where the coefficients of two highly
    collinear features have high p-values that indicate they are not related to the
    output variable, but if we remove one of these and retrain a model, the one left
    in has a low p-value. Another classic indication of collinearity is an unusual
    sign on one of the coefficients; for example, a negative coefficient on educational
    background for a linear model that predicts income. Collinearity between two features
    can be detected through pairwise correlation. One way to deal with collinearity
    is to combine two features into a new one (for example, by averaging); another
    is by simply discarding one of the features.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预处理步骤的一部分，我们勤奋地移除了彼此之间线性相关的特征。在这个过程中，我们寻找的是精确的线性关系，这是一个**完全共线性**的例子。**共线性**是描述两个特征大约处于线性关系时的属性。这给线性回归带来了问题，因为我们试图为几乎相互为线性函数的变量分配单独的系数。这可能导致两个高度共线性特征具有高p值，表明它们与输出变量无关，但如果我们移除其中一个并重新训练模型，剩下的那个将具有低p值。共线性的另一个经典迹象是其中一个系数的异常符号；例如，对于一个预测收入的线性模型，教育背景的系数为负。可以通过成对相关性检测两个特征之间的共线性。处理共线性的方法之一是将两个特征合并成一个新的特征（例如，通过平均）；另一种方法简单地丢弃其中一个特征。
- en: '**Multicollinearity** occurs when the linear relationship involves more than
    two features. A standard method for detecting this is to calculate the **variance
    inflation factor** (**VIF**) for every input feature in a linear model. In a nutshell,
    the VIF tries to estimate the increase in variance that is observed in the estimate
    of a particular coefficient that is a direct result of that feature being collinear
    with other features. This is typically done by fitting a linear regression model
    in which we treat one of the features as the output feature and the remaining
    features as regular input features. We then compute the R2 statistic for this
    linear model and from this, the VIF for our chosen feature using the formula *1
    / (1 – R²)*. In R, the `car` package contains the `vif()` function, which conveniently
    calculates the VIF value for every feature in a linear regression model. A rule
    of thumb here is that a VIF score of 4 or more for a feature is suspect, and a
    score in excess of 10 indicates a strong likelihood of multicollinearity. Since
    we saw that our cars data had linearly dependent features that we had to remove,
    let''s investigate whether we have multicollinearity in those that remain:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当线性关系涉及超过两个特征时，就会发生**多重共线性**。检测这种关系的一个标准方法是计算线性模型中每个输入特征的**方差膨胀因子**（**VIF**）。简而言之，VIF试图估计由于该特征与其他特征共线性而导致的特定系数估计中观察到的方差增加。这通常是通过拟合一个线性回归模型来完成的，我们将其中一个特征作为输出特征，将剩余的特征作为常规输入特征。然后我们计算这个线性模型的R²统计量，并据此使用公式
    *1 / (1 – R²)* 计算我们选择特征的VIF。在R中，`car`包包含`vif()`函数，它可以方便地计算线性回归模型中每个特征的VIF值。这里的一个经验法则是，如果一个特征的VIF得分超过4，那么它是可疑的，而得分超过10则表明存在多重共线性的可能性很大。鉴于我们注意到我们的汽车数据具有线性相关的特征，我们必须移除它们，让我们调查剩下的那些是否有多重共线性：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We see three values here that are slightly above `4` but no values above that.
    As an example, the following code shows how the VIF value for `sedan` was calculated:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到三个略高于`4`的值，但没有超过这个值的值。例如，以下代码展示了如何计算`sedan`的VIF值：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Outliers
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值
- en: When we looked at the residuals of our two models, we saw that there were certain
    observations that had a significantly higher residual than others. For example,
    referring to the residual plot for the CPU model, we can see that the observation
    200 has a very high residual. This is an example of an **outlier**, an observation
    whose predicted value is very far from its actual value. Due to the squaring of
    residuals, outliers tend to have a significant impact on the RSS, giving us a
    sense that we don't have a good model fit. Outliers can occur due to measurement
    errors and detecting them may be important, as they may denote data that is inaccurate
    or invalid.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看两个模型的残差时，我们发现某些观测的残差明显高于其他观测。例如，参考CPU模型的残差图，我们可以看到观测200有一个非常高的残差。这是一个**异常值**的例子，其预测值与实际值相差甚远。由于残差的平方，异常值往往会显著影响RSS，让我们有一种感觉，即我们没有好的模型拟合。异常值可能由于测量误差而产生，检测它们可能很重要，因为它们可能表示不准确或不有效的数据。
- en: On the other hand, outliers may simply be the result of not having the right
    features or building the wrong kind of model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，异常值可能仅仅是由于没有正确的特征或构建了错误类型的模型。
- en: As we generally won't know whether an outlier is an error or a genuine observation
    during data collection, handling outliers can be very tricky. Sometimes, especially
    when we have very few outliers, a common recourse is to remove them, because including
    them frequently has the effect of changing the predicted model coefficients significantly.
    We say that outliers are often points with high **influence**.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在数据收集过程中通常无法知道一个异常值是错误还是真实的观测，处理异常值可能非常棘手。有时，尤其是当我们只有很少的异常值时，一种常见的做法是移除它们，因为包括它们通常会显著改变预测模型的系数。我们说异常值通常是具有高**影响力**的点。
- en: Note
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Outliers are not the only observations that can have high influence. **High
    leverage points** are observations that have an extreme value for at least one
    of their features and thus, lie far away from most other observations. **Cook's
    distance** is a typical metric that combines the notions of outlier and high leverage
    to identify points that have high influence on the data. For a more thorough exploration
    of linear regression diagnostics, a wonderful reference is *An R Companion to
    Applied Regression*, *John Fox*, *Sage Publications*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值并不是唯一可能具有高影响力的观测。**高杠杆点**是指至少有一个特征具有极端值的观测，因此它们与其他大多数观测点相距甚远。**库克距离**是一个典型的指标，它结合了异常值和高杠杆的概念，以识别对数据具有高影响力的点。对于线性回归诊断的更深入探索，一本非常好的参考书是《应用回归的R伴侣》，作者约翰·福克斯，由Sage
    Publications出版。
- en: 'To illustrate the effect of removing an outlier, we will create a new CPU model
    by using our training data without observation number 200\. Then, we will see
    whether our model has an improved fit on the training data. Here, we''ve shown
    the steps taken and a truncated model summary with only the final three lines:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明移除异常值的效果，我们将使用不包含观测编号200的训练数据创建一个新的CPU模型。然后，我们将看看我们的模型在训练数据上是否有更好的拟合。在这里，我们展示了所采取的步骤和截断后的模型摘要，只包含最后三行：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As we can see from the reduced RSE and improved R2, we have a better fit on
    our training data. Of course, the real measure of model accuracy is the performance
    on the test data, and there are no guarantees that our decision to label observation
    200 as a spurious outlier was the right one.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，RSE降低和R2提高，表明我们在训练数据上有了更好的拟合。当然，模型准确性的真正衡量标准是测试数据的性能，而且我们无法保证将观测200标记为虚假异常值是正确的决定。
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We have a lower test MSE than before, which is usually a good sign that we made
    the right choice. Again, because we have a small test set, we cannot be certain
    of this fact despite the positive indication from the MSE.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的测试均方误差（MSE）比之前低，这通常是一个好兆头，表明我们做出了正确的选择。然而，由于我们的测试集很小，尽管MSE给出了积极的指示，我们仍然不能确定这一点。
- en: Feature selection
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: Our CPU model only came with six features. Often, we encounter real-world datasets
    that have a very large number of features arising from a diverse array of measurements.
    Alternatively, we may have to come up with a large number of features when we
    aren't really sure what features will be important in influencing our output variable.
    Moreover, we might have categorical variables with many possible levels from which
    we are forced to create a large number of new indicator variables, as we saw in
    [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7 "Chapter 1. Gearing
    Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*. When our scenario
    involves a large number of features, we often find that our output only depends
    on a subset of these. Given *k* input features, there are *2^k* distinct subsets
    that we can form, so for even a moderate number of features the space of subsets
    is too large for us to fully explore by fitting a model on each subset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 CPU 模型只包含六个特征。通常，我们会遇到来自各种测量的具有大量特征的现实世界数据集。或者，当我们不确定哪些特征会对影响我们的输出变量产生重要影响时，我们可能需要提出大量特征。此外，我们可能还有许多可能级别的分类变量，我们必须从中创建大量新的指示变量，正如我们在
    [第 1 章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7 "第 1 章。为预测建模做准备")
    中所见到的，*为预测建模做准备*。当我们的场景涉及大量特征时，我们通常发现我们的输出只依赖于这些特征的一个子集。给定 *k* 个输入特征，我们可以形成 *2^k*
    个不同的子集，因此对于特征数量适中的情况，子集空间太大，我们无法通过在每个子集上拟合模型来完全探索。
- en: Tip
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'One easy way to understand why there are *2^k* possible feature subsets is
    this: we can assign a unique identifying code to every subset as a string of binary
    digits of length *k*, where the digit at a certain position *i* is 1 if we chose
    to include the *i^(th)* feature (features can be ordered arbitrarily) in the subset.
    For example, if we have three features, the string 101 corresponds to the subset
    that only includes the first and third features. In this way, we have formed all
    possible binary strings from a string of *k* zeros to a string of *k* ones; thus
    we have all the numbers from 0 to *2^(k-1)* and *2^k* total subsets.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么存在 *2^k* 个可能的特征子集的一个简单方法是这样的：我们可以为每个子集分配一个唯一的识别码，这个识别码是一个长度为 *k* 的二进制数字字符串，其中某个位置
    *i* 的数字为 1 表示我们选择了包含第 *i* 个特征（特征可以任意排序）的子集。例如，如果我们有三个特征，字符串 101 对应的子集只包含第一个和第三个特征。通过这种方式，我们从长度为
    *k* 的零字符串形成所有可能的二进制字符串，直到长度为 *k* 的全一字符串；因此我们得到了从 0 到 *2^(k-1)* 和 *2^k* 个总子集的所有数字。
- en: '**Feature selection** refers to the process by which a subset of features in
    a model is chosen in order to form a new model with fewer features. This removes
    features that we deem unrelated to the output variable and consequently results
    in a simpler model, which is easier to train as well as interpret. There are a
    number of methods designed to do this, and they generally do not involve exhaustively
    searching the space of possible subsets, but perform a guided search through this
    space instead.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择** 指的是在模型中选择特征子集的过程，以便形成一个具有较少特征的新模型。这移除了我们认为与输出变量无关的特征，从而得到一个更简单的模型，这个模型更容易训练和解释。有许多方法旨在完成这项任务，它们通常不涉及对可能子集空间的全面搜索，而是通过这个空间进行有指导的搜索。'
- en: One such method is **forward selection**, which is an example of **stepwise
    regression** that performs feature selection in a series of steps. With forward
    selection, the idea is to start out with an empty model that has no features selected.
    We then perform *k* simple linear regressions (one for every feature that we have)
    and pick the best one. Here, we are comparing models that have the same number
    of features so that we can use the R² statistic to guide our choice, although
    we can use metrics such as AIC as well. Once we have chosen our first feature
    to add, we then pick another feature to add from the remaining *k-1* features.
    Therefore, we now run *k-1* multiple regressions for every possible pair of features,
    where one of the features in the pair is the feature that we picked in the first
    step. We continue adding in features like this until we have evaluated the model
    with all the features included and stop. Note that, in every step, we make a hard
    choice about which feature to include for all future steps.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一种这样的方法是**前向选择**，它是一个**逐步回归**的例子，通过一系列步骤进行特征选择。使用前向选择时，我们的想法是从一个没有任何特征选择的空模型开始。然后我们进行
    *k* 个简单的线性回归（每个特征一个），并选择最好的一个。在这里，我们比较具有相同数量特征的模型，这样我们就可以使用 R² 统计量来指导我们的选择，尽管我们也可以使用
    AIC 等指标。一旦我们选择了要添加的第一个特征，我们就从剩余的 *k-1* 个特征中选择另一个特征来添加。因此，我们现在为每个可能的特征对运行 *k-1*
    个多重回归，其中一对特征中的一个是我们第一步中选择的特征。我们继续以这种方式添加特征，直到我们评估了包含所有特征的模型并停止。请注意，在每一步中，我们都要做出一个关于要包含哪个特征以供所有后续步骤使用的艰难选择。
- en: 'For example, models that have more than one feature in them and do not include
    the feature we chose in the first step of this process are never considered. Therefore,
    we do not exhaustively search our space. In fact, if we take into account that
    we also assess the null model, we can compute the total number of models we perform
    a linear regression on as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，具有一个以上特征且不包含我们在此过程的第一步中选择的特征的模型永远不会被考虑。因此，我们不会彻底搜索我们的空间。实际上，如果我们考虑到我们还要评估空模型，我们可以计算出我们对多少个模型进行了线性回归的总数如下：
- en: '![Feature selection](img/00055.jpeg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择](img/00055.jpeg)'
- en: 'The order of magnitude of this computation is on the scale of *k²*, which for
    even small values of *k* is already considerably less than *2^k*. At the end of
    the forward selection process, we have to choose between *k+1* models, corresponding
    to the subsets we obtained at the end of every step of the process. As the final
    part of the process involves comparing models with different numbers of features,
    we usually use a criterion such as the AIC or the adjusted R2 to make our final
    choice of model. We can demonstrate this process for our CPU dataset by running
    the following commands:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算的量级在 *k²* 的范围内，对于 *k* 的较小值来说，这已经比 *2^k* 小得多。在前向选择过程结束时，我们必须在 *k+1* 个模型之间进行选择，这些模型对应于过程每一步结束时获得的子集。由于过程的最后部分涉及比较具有不同数量特征的模型，我们通常使用
    AIC 或调整后的 R² 等标准来做出最终模型选择。我们可以通过运行以下命令来演示我们的 CPU 数据集的过程：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `step()` function implements the process of forward selection. We first
    provide it with the null model obtained by fitting a linear model with no features
    on our training data. For the `scope` parameter, we specify that we want our algorithm
    to step through from the null model all the way to our full model consisting of
    all six features. The effect of issuing these commands in R is an output that
    demonstrates which feature subset is specified at every step of the iteration.
    To conserve space, we present the results in the following table, along with the
    value of the AIC for each model. Note that the lower the AIC value, the better
    the model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()` 函数实现了前向选择的过程。我们首先向它提供通过在我们的训练数据上拟合没有特征的线性模型得到的空模型。对于 `scope` 参数，我们指定我们希望我们的算法从空模型逐步过渡到包含所有六个特征的完整模型。在
    R 中发出这些命令的效果是输出一个演示迭代每一步中指定的特征子集的输出。为了节省空间，我们将结果以及每个模型的 AIC 值以以下表格的形式呈现。请注意，AIC
    值越低，模型越好。'
- en: '| Step | Features in subset | AIC value |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | 子集特征 | AIC 值 |'
- en: '| --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | `{}` | 1839.13 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 0 | `{}` | 1839.13 |'
- en: '| 1 | `{MMAX}` | 1583.38 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `{MMAX}` | 1583.38 |'
- en: '| 2 | `{MMAX, CACH}` | 1547.21 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `{MMAX, CACH}` | 1547.21 |'
- en: '| 3 | `{MMAX, CACH, MMIN}` | 1522.06 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `{MMAX, CACH, MMIN}` | 1522.06 |'
- en: '| 4 | `{MMAX, CACH, MMIN, CHMAX}` | 1484.14 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 4 | `{MMAX, CACH, MMIN, CHMAX}` | 1484.14 |'
- en: '| 5 | `{MMAX, CACH, MMIN, CHMAX, MYCT}` | 1478.36 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 5 | `{MMAX, CACH, MMIN, CHMAX, MYCT}` | 1478.36 |'
- en: The `step()` function uses an alternative specification for forward selection,
    which is to terminate when there is no feature from those remaining that can be
    added to the current feature subset and would improve our score. For our dataset,
    only one feature was left out from the final model, as adding it did not improve
    the overall score. It is interesting and somewhat reassuring that this feature
    was CHMIN, which was the only variable whose relatively high p-value indicated
    that we weren't confident that our output variable is related to this feature
    in the presence of the other features.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()` 函数使用了一种替代的前向选择规范，即在没有任何剩余特征可以添加到当前特征子集并且会提高我们的分数时终止。对于我们的数据集，最终模型中只遗漏了一个特征，因为添加它并没有提高整体分数。有趣且多少有些令人放心的是，这个特征是
    CHMIN，它是唯一一个相对高 p 值的变量，这表明在其他特征存在的情况下，我们并不确定我们的输出变量与这个特征相关。'
- en: 'One might wonder whether we could perform variable selection in the opposite
    direction by starting off with a full model and removing features one by one based
    on which feature, when removed, will make the biggest improvement in the model
    score. This is indeed possible, and the process is known either as **backward
    selection** or **backward elimination**. This can be done in R with the `step()`
    function by specifying `backward` as the direction and starting from the full
    model. We''ll show this on our cars dataset and save the result into a new cars
    model:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会想知道我们是否可以通过从完整模型开始，逐个删除特征，根据哪个特征被删除时会使模型分数提高最大来进行变量选择。这确实可能，这个过程被称为**向后选择**或**向后消除**。在
    R 中，可以通过指定 `backward` 作为方向并从完整模型开始，使用 `step()` 函数来完成此操作。我们将在我们的汽车数据集上展示这一点，并将结果保存到一个新的汽车模型中：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The formula for the final linear regression model on the cars dataset is:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车数据集上最终线性回归模型的公式是：
- en: '[PRE31]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As we can see, the final model has thrown away the `Cruise`, `Sound`, and `Chevy`
    features. Looking at our previous model summary, we can see that these three features
    had high p-values. The previous two approaches are examples of a **greedy algorithm**.
    This is to say that, once a choice about whether to include a variable has been
    made, it becomes final and cannot be undone later. To remedy this, a third method
    of variable selection known as **mixed selection** or **bidirectional elimination**
    starts as forward selection with forward steps to add variables, but also includes
    backward steps when these can improve the AIC. Predictably, the `step()` function
    does this when the `direction` is specified as `both`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，最终模型已经丢弃了 `Cruise`、`Sound` 和 `Chevy` 特征。查看我们之前的模型摘要，我们可以看到这三个特征具有高 p 值。前两种方法是**贪婪算法**的例子。这意味着一旦关于是否包含变量的选择被做出，它就变得最终且不能在以后撤销。为了解决这个问题，一种称为**混合选择**或**双向消除**的变量选择方法开始时是前向选择，使用前向步骤添加变量，但在这些步骤可以提高
    AIC 时也包括向后步骤。可预测的是，当 `direction` 被指定为 `both` 时，`step()` 函数会这样做。
- en: 'Now that we have two new models, we can see how they perform on the test sets:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个新的模型，我们可以看到它们在测试集上的表现：
- en: '[PRE32]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: For the CPU model, we perform marginally better on the test set than our original
    model. A suitable next step might be to investigate whether this reduced set of
    features works better in combination with the removal of our outlier; this is
    left as an exercise for the reader. In contrast, for the cars model, we see that
    the test MSE has increased slightly as a result of removing all these features.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CPU 模型，我们在测试集上的表现略好于我们的原始模型。一个合适的下一步可能是调查这个特征集是否与移除我们的异常值结合使用效果更好；这留给读者作为练习。相比之下，对于汽车模型，我们看到由于移除了所有这些特征，测试
    MSE 略有增加。
- en: Regularization
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: Variable selection is an important process, as it tries to make models simpler
    to interpret, easier to train, and free of spurious associations by eliminating
    variables unrelated to the output. This is one possible approach to dealing with
    the problem of overfitting. In general, we don't expect a model to completely
    fit our training data; in fact, the problem of overfitting often means that it
    may be detrimental to our predictive model's accuracy on unseen data if we fit
    our training data too well. In this section on **regularization**, we'll study
    an alternative to reducing the number of variables in order to deal with overfitting.
    Regularization is essentially the process of introducing an intentional bias or
    constraint in our training procedure that prevents our coefficients from taking
    large values. As this is a process that tries to shrink the coefficients, the
    methods we'll look at are also known as **shrinkage methods**.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 变量选择是一个重要的过程，因为它试图通过消除与输出无关的变量，使模型更容易解释、更容易训练，并且没有虚假关联。这是处理过拟合问题的一种可能方法。一般来说，我们不期望模型完全拟合我们的训练数据；事实上，过拟合的问题通常意味着如果我们对训练数据拟合得太好，可能会损害我们的预测模型在未见数据上的准确性。在本节关于**正则化**的内容中，我们将研究一种减少变量数量的替代方法，以处理过拟合问题。正则化本质上是在我们的训练过程中引入一个有意的偏差或约束，以防止我们的系数取大值。由于这是一个试图缩小系数的过程，因此我们将会探讨的方法也被称为**收缩方法**。
- en: Ridge regression
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 岭回归
- en: 'When the number of parameters is very large, particularly compared to the number
    of available observations, linear regression tends to exhibit very high variance.
    This is to say that small changes in a few of the observations will cause the
    coefficients to change substantially. **Ridge regression** is a method that introduces
    bias through its constraint but is effective at reducing the model''s variance.
    Ridge regression tries to minimize the sum of the residual sum of squares and
    uses a term that involves the sum of the squares of the coefficients multiplied
    by a constant for which we''ll use the Greek letter *λ*. For a model with *k*
    parameters, not counting the constant term *β[0]*, and a dataset with *n* observations,
    ridge regression minimizes the following quantity:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当参数数量非常大，尤其是与可用观测值的数量相比时，线性回归往往会表现出非常高的方差。这意味着观测值中的一些微小变化会导致系数发生显著变化。**岭回归**是一种通过其约束引入偏差的方法，但能有效减少模型的方差。岭回归试图最小化残差平方和的总和，并使用一个涉及系数平方和乘以一个常数的项，我们将使用希腊字母*λ*来表示这个常数。对于一个有*k*个参数的模型（不包括常数项*β[0]*），以及一个有*n*个观测值的数据库，岭回归最小化以下量：
- en: '![Ridge regression](img/00056.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![岭回归](img/00056.jpeg)'
- en: We are still minimizing the RSS but the second term is the penalty term, which
    is high when any of the coefficients is high. Thus, when minimizing, we are effectively
    pushing the coefficients to smaller values. The *λ* parameter is known as a **meta
    parameter**, which we need to select or tune. A very large value of *λ* will mask
    the RSS term and just push the coefficients to zero. An overly small value of
    *λ* will not be as effective against overfitting and a *λ* parameter of 0 just
    performs regular linear regression.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍在最小化均方误差（RSS），但第二项是惩罚项，当任何系数较高时，该惩罚项会增大。因此，在最小化过程中，我们实际上是在将系数推向更小的值。*λ*参数被称为**元参数**，我们需要选择或调整它。*λ*的值非常大时，会掩盖RSS项，并将系数推向零。*λ*的值过小则对防止过拟合的效果不佳，而*λ*参数为0则仅执行普通线性回归。
- en: When performing ridge regression, we often want to scale by dividing the values
    of all our features by their variance. This was not the case with regular linear
    regression because, if one feature is scaled by a factor of 10, then the coefficient
    will simply be scaled by a factor of a tenth to compensate. With ridge regression,
    the scale of a feature affects the computation of all other features through the
    penalty term.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行岭回归时，我们通常希望通过将所有特征值除以它们的方差来进行缩放。这与普通线性回归不同，因为如果某个特征值乘以10倍，那么系数将简单地乘以十分之一来补偿。在岭回归中，一个特征的缩放会通过惩罚项影响其他所有特征的计算。
- en: Least absolute shrinkage and selection operator (lasso)
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小绝对收缩和选择算子（lasso）
- en: The **lasso** is an alternative regularization method to ridge regression. The
    difference appears only in the penalty term, which involves minimizing the sum
    of the absolute values of the coefficients.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**lasso**是岭回归的一种替代正则化方法。差异仅出现在惩罚项上，该惩罚项涉及最小化系数绝对值的总和。'
- en: '![Least absolute shrinkage and selection operator (lasso)](img/00057.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![最小绝对收缩和选择算子 (lasso)](img/00057.jpeg)'
- en: It turns out that this difference in the penalty term is very significant, as
    the lasso combines both shrinkage and selection because it shrinks some coefficients
    to exactly zero, which is not the case with ridge regression. Despite this, there
    is no clear winner between these two. Models that depend on a subset of the input
    features will tend to perform better with lasso; models that have a large spread
    in coefficients across many different variables will tend to perform better with
    ridge regression. It is usually worth trying both.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，惩罚项的差异非常显著，因为lasso结合了收缩和选择，它将一些系数收缩到正好为零，而岭回归则不是这样。尽管如此，这两种方法之间并没有明确的胜者。依赖于输入特征子集的模型倾向于使用lasso表现更好；具有许多不同变量系数分布广泛的模型倾向于使用岭回归表现更好。通常尝试这两种方法都是值得的。
- en: Note
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The penalty in ridge regression is often referred to as an *l[2]* penalty,
    whereas the penalty term in lasso is known as an *l[1]* penalty. This arises from
    the mathematical notion of a **norm** of a vector. A norm of a vector is a function
    that assigns a positive number to that vector to represent its length or size.
    There are many different types of norms. Both the *l[1]* and *l[2]* norms are
    examples of a family of norms known as **p-norms** and have the following general
    form for a vector *v* with *n* components:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归中的惩罚通常被称为 *l[2]* 惩罚，而lasso中的惩罚项则被称为 *l[1]* 惩罚。这源于向量 **范数** 的数学概念。向量的范数是一个函数，它将一个正数分配给该向量以表示其长度或大小。有许多不同类型的范数。*l[1]*
    和 *l[2]* 范数是称为 **p-范数** 的范数族中的例子，对于具有 *n* 个分量的向量 *v*，它们具有以下一般形式：
- en: '![Least absolute shrinkage and selection operator (lasso)](img/00058.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![最小绝对收缩和选择算子 (lasso)](img/00058.jpeg)'
- en: Implementing regularization in R
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在R中实现正则化
- en: There are a number of different functions and packages that implement ridge
    regression, such as `lm.ridge()` from the `MASS` package and `ridge()` from the
    `genridge` package. For the lasso there is also the `lars` package. In this chapter,
    we are going to work with the `glmnet()` function from the `glmnet` package due
    to its consistent and friendly interface. The key to working with regularization
    is to determine an appropriate value of *λ* to use. The approach that the `glmnet()`
    function uses is to use a grid of different *λ* values and train a regression
    model for each value. Then, one can either pick a value manually or use a technique
    to estimate the best lambda. We can specify the sequence of *λ* values to try
    via the `lambda` parameter; otherwise, a default sequence with 100 values will
    be used. The first parameter to the `glmnet()` function must be a matrix of features,
    which we can build using the `model.matrix()` function.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的函数和包实现了岭回归，例如来自 `MASS` 包的 `lm.ridge()` 和来自 `genridge` 包的 `ridge()`。对于lasso，也有
    `lars` 包。在本章中，我们将使用来自 `glmnet` 包的 `glmnet()` 函数，因为它具有一致且友好的界面。使用正则化的关键是确定一个合适的
    *λ* 值。`glmnet()` 函数使用的方法是使用不同 *λ* 值的网格，并为每个值训练一个回归模型。然后，可以选择手动选择一个值或使用一种技术来估计最佳的lambda。我们可以通过
    `lambda` 参数指定要尝试的 *λ* 值序列；否则，将使用默认的包含100个值的序列。`glmnet()` 函数的第一个参数必须是一个特征矩阵，我们可以使用
    `model.matrix()` 函数构建它。
- en: 'The second parameter is a vector with the output variable. Finally, the `alpha`
    parameter is a switch between ridge regression (0) and lasso (1). We''re now ready
    to train some models on the cars dataset:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是一个包含输出变量的向量。最后，`alpha` 参数是在岭回归（0）和lasso（1）之间的开关。我们现在已经准备好在cars数据集上训练一些模型：
- en: '[PRE33]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As we provided a sequence of 250 *λ* values, we''ve actually trained 250 ridge
    regression models and another 250 lasso models. We can see the value of *λ* from
    the `lambda` attribute of the object that is produced by `glmnet()` and apply
    the `coef()` function on this object to retrieve the corresponding coefficients
    for the 100th model, as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们提供了一系列250个 *λ* 值，我们实际上训练了250个岭回归模型和另外250个lasso模型。我们可以从 `glmnet()` 函数生成的对象的
    `lambda` 属性中看到 *λ* 的值，然后应用 `coef()` 函数来检索第100个模型的相应系数，如下所示：
- en: '[PRE34]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can use the `plot()` function to obtain a plot showing how the values of
    the coefficients change as the logarithm of *λ* changes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `plot()` 函数来获得一个图表，显示系数的值如何随着 *λ* 的对数变化而变化。
- en: 'As shown below, it is very helpful to show the corresponding plot for ridge
    regression and lasso side by side:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，同时展示岭回归和lasso的对应图表非常有帮助：
- en: '[PRE35]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Implementing regularization in R](img/00059.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![在R中实现正则化](img/00059.jpeg)'
- en: The key difference between these two graphs is that lasso forces many coefficients
    to fall to zero exactly, whereas in ridge regression they tend to drop off smoothly
    and only become zero altogether at extreme values of *λ*. This is further evident
    by reading the values of the numbers on the top horizontal axis of both graphs,
    which show the number of non-zero coefficients as *λ* varies. In this way, lasso
    has a significant advantage in that it can often be used to perform feature selection
    (because a feature with a zero coefficient is essentially not included in the
    model) as well as providing regularization to minimize the issue of overfitting.
    We can obtain other useful plots by changing the value supplied to the `xvar`
    parameter. The value `norm` plots the *l[1]* norm of the coefficients on the x-axis
    and `dev` plots the percentage deviance explained. We will learn about deviance
    in the next chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个图的关键区别在于，lasso强制许多系数恰好降到零，而岭回归中它们倾向于平滑下降，只有在λ的极端值时才整体变为零。这一点通过阅读两个图顶部水平轴上的数值可以进一步证实，这些数值显示了随着λ的变化非零系数的数量。这样，lasso在特征选择（因为零系数的特征实际上不包括在模型中）以及提供正则化以最小化过拟合问题方面具有显著优势。我们可以通过更改`xvar`参数提供的值来获得其他有用的图。值`norm`在x轴上绘制系数的l[1]范数，而`dev`绘制解释的偏差百分比。我们将在下一章学习偏差。
- en: 'To deal with the issue of finding a good value for *λ*, the `glmnet()` package
    offers the `cv.glmnet()` function. This uses a technique known as cross-validation
    (we''ll study this in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*) on the training data
    to find an appropriate *λ* that minimizes the MSE:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决寻找合适的*λ*值的问题，`glmnet()`包提供了`cv.glmnet()`函数。这个函数使用一种称为交叉验证的技术（我们将在[第五章](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "第五章. 神经网络")中学习），*支持向量机*）在训练数据上找到合适的*λ*值，以最小化均方误差（MSE）：
- en: '[PRE36]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we plot the result produced by the `cv.glmnet()` function, we can see how
    the MSE changes over the different values of lambda:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制`cv.glmnet()`函数产生的结果，我们可以看到均方误差如何随λ的不同值而变化：
- en: '![Implementing regularization in R](img/00060.jpeg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![在R中实现正则化](img/00060.jpeg)'
- en: The bars shown above and below each dot are the error bars showing one standard
    deviation above and below the estimate of the MSE for each plotted value of lambda.
    The plots also show two vertical dotted lines. The first vertical line shown corresponds
    to the value of `lambda.min`, which is the optimal value proposed by cross-validation.
    The second vertical line to the right is the value in the attribute `lambda.1se`.
    This corresponds to a value that is one standard error away from `lambda.min`
    and produces a more regularized model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 每个点上方和下方的条形是误差条，表示每个绘制的λ值估计的MSE上方和下方的一个标准差。这些图还显示了两条垂直虚线。第一条垂直线对应于`lambda.min`的值，这是交叉验证提出的最佳值。第二条垂直线在右侧的值是`lambda.1se`属性中的值。这对应于比`lambda.min`大一个标准误差的值，并产生一个更正则化的模型。
- en: With the `glmnet` package, the `predict()` function now operates in a variety
    of contexts. We can, for example, obtain the coefficients of a model for a lambda
    value that was not in our original list.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`glmnet`包，`predict()`函数现在可以在各种上下文中运行。例如，我们可以获取一个不在我们原始列表中的λ值的模型系数。
- en: 'For example, we have this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们有以下内容：
- en: '[PRE37]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Note that it seems that lasso has not forced any coefficients to zero in this
    case, indicating that, based on the MSE, it is not suggesting removing any of
    them for the cars dataset. Finally, using the `predict()` function again, we can
    make predictions with a regularized model using the `newx` parameter to provide
    a matrix of features for observations on which we want to make predictions:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，lasso似乎没有强制任何系数为零，这表明根据MSE，它不建议从cars数据集中删除任何系数。最后，再次使用`predict()`函数，我们可以使用`newx`参数提供特征矩阵来对观察值进行预测，从而使用正则化模型进行预测：
- en: '[PRE38]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The lasso model performs best and, unlike ridge regression, in this case also
    slightly outperforms the regular model on the test data.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso模型表现最佳，并且与岭回归不同，在这种情况下，在测试数据上也略微优于常规模型。
- en: Polynomial regression
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归
- en: Polynomial regression is a *kind* of linear regression.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是一种*类型的*线性回归。
- en: While linear regression is when both the predictor and the response are each
    continuous and linearly-related, causing the response to increase or decrease
    at a constant ratio to the predictor (that is, in a straight line), with polynomial
    regression, *different powers* of the predictor are successively added to see
    if they adjust the response significantly. As these increases are added to the
    equation, the line of data points will change its shape, turning the linear regression
    model from a best fitted line into a best fitted curve.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测变量和响应变量都是连续的并且线性相关时，这就是线性回归，响应变量会以恒定的比率相对于预测变量增加或减少（即，呈直线），而在多项式回归中，会依次添加预测变量的*不同幂次*，以查看它们是否能显著调整响应。随着这些增加被添加到方程中，数据点的线条将改变其形状，将线性回归模型从最佳拟合线转变为最佳拟合曲线。
- en: 'So, why should you bother with polynomial regression? The generally accepted
    answer or thought process is: when a linear model doesn''t seem to be the best
    model for your data.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么你应该费心去考虑多项式回归呢？普遍接受的答案或思维过程是：当线性模型似乎不是你数据的最佳模型时。
- en: 'There are three main conditions that indicate a linear relationship may not
    be a good model for a use:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个主要条件表明线性关系可能不是一个好的模型：
- en: There will be some variable relationships in your data that you *assume* are
    curvilinear
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的数据中，将会有一些变量关系，你*假设*它们是曲线关系。
- en: During visual inspection of your variables, you *establish* (using a scatter
    plot is the most common method for this) a curvilinear relationship
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在检查你的变量时，你*建立*（使用散点图是最常见的方法）一个曲线关系。
- en: After you have actually created a linear regression model, an examination of
    residuals (using a scatterplot) shows numerous positive residual values in the
    middle, but patches of negative residual values at either end (or vice versa)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你实际上创建了线性回归模型之后，通过残差（使用散点图）的检查显示，中间有许多正残差值，但在两端（或反之）有负残差值的块状区域。
- en: Note
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: In curvilinear relationships, values increase together up to a certain
    level (like a positive relationship) and then, as one value increases, the other
    decreases (negative relationship) or vice versa.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在曲线关系中，值会一起增加到一定水平（如正相关关系），然后，随着一个值的增加，另一个值减少（负相关关系）或反之。
- en: So, here we consider an example like the one just mentioned involving user cars.
    In our vehicle data, we have information listing many attributes, including the
    number of options each vehicle has. Suppose we are interested in the relationship
    between the number of options a car has (such as air conditioning or heated seats)
    and the resell price.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这里，我们考虑一个类似于刚才提到的涉及用户汽车的一个例子。在我们的车辆数据中，我们有许多属性的信息，包括每辆车的选项数量。假设我们感兴趣的是汽车拥有的选项数量（如空调或加热座椅）与二手价格之间的关系。
- en: One might assume that the more options a vehicle has, the higher the price the
    vehicle would sell for.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会认为，一辆车拥有的选项越多，其售价就越高。
- en: 'However, upon closer analysis of this data, we see that is not the case:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在更仔细地分析这些数据后，我们发现情况并非如此：
- en: '![Polynomial regression](img/00061.jpeg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/00061.jpeg)'
- en: 'If we plot the data, we can see perhaps a text book example of a scenario that
    would benefit from a polynomial regression:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制数据，我们可以看到可能是一个受益于多项式回归的情景的教科书示例：
- en: '![Polynomial regression](img/00062.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/00062.jpeg)'
- en: In this hypothetical scenario, the relationship between the independent variable
    *x* (the percentage increase in the resell price over the blue book value) and
    the dependent variable *y* (the number of options a vehicle has) can be modeled
    as an *n^(th)* degree polynomial in *x*.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个假设场景中，自变量*x*（二手价格相对于蓝皮书价值的百分比增加）和因变量*y*（车辆拥有的选项数量）之间的关系可以用*x*的*n*次幂多项式来建模。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied linear regression, a method that allows us to fit
    a linear model in a supervised learning setting where we have a number of input
    features and a single numeric output. Simple linear regression is the name given
    to the scenario where we have only one input feature, and multiple linear regression
    describes the case where we have multiple input features. Linear regression is
    very commonly used as a first approach to solving a regression problem. It assumes
    that the output is a linear weighted combination of the input features in the
    presence of an irreducible error component that is normally distributed and has
    zero mean and constant variance. The model also assumes that the features are
    independent. The performance of linear regression can be assessed by a number
    of different metrics from the more standard MSE to others, such as the R² statistic.
    We explored several model diagnostics and significance tests designed to detect
    problems from violated assumptions to outliers. Finally, we also discussed how
    to perform feature selection with stepwise regression and perform regularization
    using ridge regression and lasso.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了线性回归，这是一种允许我们在有多个输入特征和单个数值输出的监督学习环境中拟合线性模型的方法。简单线性回归是指我们只有一个输入特征的情况，而多重线性回归描述的是我们拥有多个输入特征的情况。线性回归是非常常用的回归问题解决方案的第一步。它假设输出是输入特征的线性加权组合，存在一个不可减少的误差成分，该误差成分服从正态分布，均值为零，方差恒定。该模型还假设特征是独立的。线性回归的性能可以通过多种不同的指标来评估，从更标准的均方误差（MSE）到其他指标，如R²统计量。我们探讨了几个模型诊断和显著性测试，旨在检测违反假设的问题和异常值。最后，我们还讨论了如何使用逐步回归进行特征选择，以及如何使用岭回归和Lasso进行正则化。
- en: Linear regression is a model with several advantages, which include fast and
    cheap parameter computation and a model that, by virtue of its simple form, is
    very easy to interpret and draw inferences from. There is a plethora of tests
    available to diagnose problems with the model fit and perform hypothesis testing
    to check the significance of the coefficients. In general, as a method, it is
    considered to be low variance because it is robust to small errors in the data.
    On the negative side, because it makes very strict assumptions, notably that the
    output function must be linear in the model parameters, it introduces a high degree
    of bias, and for general functions that are complex or highly nonlinear this approach
    tends to fare poorly. In addition, we saw that we cannot really rely on significance
    testing for coefficients when we move to a high number of input features. This
    fact, coupled with the independence assumption between features, renders linear
    regression a relatively poor choice to make when working in a higher dimensional
    feature space.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一个具有多个优点的模型，包括快速且成本低廉的参数计算，以及由于其简单形式，非常容易解释和从中得出推论。有大量的测试可用于诊断模型拟合问题，并执行假设检验以检查系数的显著性。总的来说，作为一种方法，它被认为是低方差，因为它对数据中的小误差具有鲁棒性。然而，从负面来看，因为它做出了非常严格的假设，特别是输出函数在模型参数中必须是线性的，这引入了很高的偏差，对于复杂或高度非线性的通用函数，这种方法往往表现不佳。此外，我们看到了当我们转向大量输入特征时，我们实际上不能真正依赖于系数的显著性测试。这一事实，加上特征之间的独立性假设，使得线性回归在处理高维特征空间时成为一个相对较差的选择。
- en: We also mentioned polynomial regression as an option for fitting data once a
    linear regression falls short, based upon the relationship of your data point
    values or when residuals from a linear regression model show certain positive-negative
    relationships.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提到了多项式回归作为线性回归不足以拟合数据时的一个选项，这基于你的数据点值之间的关系，或者当线性回归模型的残差显示出某些正负关系时。
- en: In the next chapter, we will study logistic regression, which is an important
    method used in classification problems.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究逻辑回归，这是在分类问题中使用的 重要方法。
