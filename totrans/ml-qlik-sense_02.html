<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer017">
<h1 class="chapter-number" id="_idParaDest-37"><a id="_idTextAnchor037"/>2</h1>
<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Machine Learning Algorithms and Models with Qlik</h1>
<p>Machine learning algorithms have become an integral part of our lives, from the personalization of online ads to the recommendation systems on streaming platforms. These algorithms are responsible for making intelligent decisions based on data, without being <span class="No-Break">explicitly programmed.</span></p>
<p>Machine learning algorithms refer to a set of mathematical models and techniques that enable software to learn patterns and relationships from data, allowing them to make predictions and decisions. These algorithms can be broadly categorized into supervised, unsupervised, semi-supervised, and reinforcement learning algorithms. Each type of algorithm has its own unique characteristics and applications, suiting them to a wide range <span class="No-Break">of tasks.</span></p>
<p>In this chapter, we will provide an overview of machine learning algorithms and their applications, focusing on algorithms used in Qlik tools. Here is what you will learn as a part of <span class="No-Break">this chapter:</span></p>
<ul>
<li>Understand regression models and how to <span class="No-Break">use these</span></li>
<li>Understand different clustering algorithms and <span class="No-Break">decision trees</span></li>
<li>Understand the basics of boosting algorithms, especially the one used in <span class="No-Break">Qlik AutoML</span></li>
<li>Understand the basics of neural networks and other advanced machine <span class="No-Break">learning models</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Qlik AutoML was using the following algorithms at the time of writing <span class="No-Break">this book:</span></p>
<p class="callout"><strong class="bold">Binary and multiclass </strong><span class="No-Break"><strong class="bold">classification problems:</strong></span></p>
<p class="callout">CatBoost Classification, Elastic Net Regression, Gaussian Naive Bayes, Lasso Regression, LightGBM Classification, Logistic Regression, Random Forest Classification, <span class="No-Break">XGBoost Classification</span></p>
<p class="callout"><span class="No-Break"><strong class="bold">Regression problems:</strong></span></p>
<p class="callout">CatBoost Regression, LightGBM Regression, Linear Regression, Random Forest Regression, SGD Regression, <span class="No-Break">XGBoost Regression</span></p>
<p class="callout">Some of these algorithms will be covered in more detail in the <span class="No-Break">coming sections.</span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Regression models</h1>
<p><strong class="bold">Regression models</strong> are a type <a id="_idIndexMarker136"/>of supervised machine-learning model used to predict continuous numerical values for a target variable based on one or more input variables. In other words, regression models are used to estimate the relationships between the input variables and the <span class="No-Break">output variable.</span></p>
<p>There are various types of regression models used in machine learning, some of which include <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Linear Regression</strong>: This is a type<a id="_idIndexMarker137"/> of regression model that <a id="_idIndexMarker138"/>assumes a linear relationship between the input variables and the <span class="No-Break">output variable.</span></li>
<li><strong class="bold">Polynomial Regression</strong>: This is a type <a id="_idIndexMarker139"/>of regression model that <a id="_idIndexMarker140"/>assumes a polynomial relationship between the input variables <a id="_idIndexMarker141"/>and the <span class="No-Break">output variable.</span></li>
<li><strong class="bold">Logistic Regression</strong>: This is a<a id="_idIndexMarker142"/> type of regression model used to <a id="_idIndexMarker143"/>predict binary or categorical outcomes. It estimates the probability of an event occurring based on the <span class="No-Break">input variables.</span></li>
<li><strong class="bold">Ridge Regression</strong>: This is a type <a id="_idIndexMarker144"/>of linear regression model that uses regularization to prevent overfitting <a id="_idIndexMarker145"/>of <span class="No-Break">the model.</span></li>
<li><strong class="bold">Lasso Regression</strong>: This is <a id="_idIndexMarker146"/>another type of linear regression model that uses regularization to prevent overfitting of the model. It is<a id="_idIndexMarker147"/> particularly useful when dealing with datasets that have a large number <span class="No-Break">of features.</span></li>
</ul>
<p>In the next section, we are going to take a closer look at some of the regression models in the <span class="No-Break">preceding list.</span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Linear regression</h2>
<p>Linear regression is used for modeling the relationship between a dependent variable (also known as the target or response variable) and<a id="_idIndexMarker148"/> one or more independent variables (also known as the explanatory or predictor <a id="_idIndexMarker149"/>variables). The goal of linear regression is to find the best-fit line (or hyperplane) that can predict the dependent variable based on the values of the independent variables. In other words, linear regression tries to find a linear equation that relates the input variables to the output variable. The equation takes the <span class="No-Break">following form:</span></p>
<p><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span></p>
<p>where <em class="italic">Y</em> is the dependent variable, <em class="italic">X</em> is the independent variable, <em class="italic">m</em> is the slope of the line, <em class="italic">b</em> is the intercept, and <em class="italic">e</em> represents the error of the model. The goal of linear regression is to find the best values of <em class="italic">m</em> and <em class="italic">b</em> that minimize the difference between the predicted values and the actual values of the dependent variable. In simple linear regression, we have one independent variable and in multiple linear regression, we will have multiple <span class="No-Break">independent variables.</span></p>
<h3>Example</h3>
<p>We want to investigate the<a id="_idIndexMarker150"/> relationship between the number of hours a student studies and their exam score. We collect data from 10 students, recording the number of hours they studied and their corresponding exam scores. The data is <span class="No-Break">shown here:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Hours </strong><span class="No-Break"><strong class="bold">studied (X)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Exam </strong><span class="No-Break"><strong class="bold">score (Y)</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">60</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">70</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">85</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">90</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">95</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">100</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">105</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">10</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">110</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">11</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">115</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1: Exam score data</p>
<p>We can use a simple linear<a id="_idIndexMarker151"/> regression model to model the relationship between the two variables, with the number of hours studied as the independent variable (<em class="italic">X</em>) and the exam score as the dependent variable (<em class="italic">Y</em>). The model takes the <span class="No-Break">following form:</span></p>
<p><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span></p>
<p>where <em class="italic">b</em> is the intercept (the value of <em class="italic">Y</em> when <em class="italic">X=0</em>) and <em class="italic">m</em> is the slope (the rate at which <em class="italic">Y</em> changes with respect to <em class="italic">X</em>). To estimate the values of <em class="italic">b</em> and <em class="italic">m</em>, we can use the least squares method. Solving for the regression equation, we get <span class="No-Break">the following:</span></p>
<p><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">55</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">e</span></p>
<p>This equation tells us that, on average, for every additional hour studied, a student can expect to score 5 points higher on the exam. The intercept of 55 tells us that a student who studies 0 hours can expect to score 55 points (which may not be realistic, but it’s just the mathematical extrapolation from <span class="No-Break">the model).</span></p>
<p>We can use this model to make predictions about exam scores based on the number of hours studied. For example, if a student studies for 7 hours, we can estimate their exam score to be <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">7</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">55</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">90</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">e</span></p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>Logistic regression</h2>
<p>Logistic regression is a statistical method used for binary classification problems where the outcome variable is categorical and has only two possible values, such as “yes” or “no,” “pass” or “fail,” or “spam” or “not spam.” It is a type of regression analysis that models the relationship<a id="_idIndexMarker152"/> between the independent variables and the dependent variable by estimating the probability of the <span class="No-Break">binary outcome.</span></p>
<p>The logistic regression model uses a logistic<a id="_idIndexMarker153"/> function, also known as the sigmoid function, to model the relationship between the input variables and the binary outcome. The sigmoid function transforms the input values into a range between 0 and 1, which represents the probability of the <span class="No-Break">binary outcome.</span></p>
<p>The logistic regression model can be trained using a maximum likelihood estimation method to find the parameters that maximize the likelihood of the observed data given the model. These parameters can then be used to predict the probability of the binary outcome for new <span class="No-Break">input data.</span></p>
<p>Logistic regression is widely used in medical diagnosis, credit scoring, and marketing analysis. It is a popular algorithm due to its simplicity <span class="No-Break">and interpretability.</span></p>
<h3>Example</h3>
<p>Let’s say we want to predict <a id="_idIndexMarker154"/>whether or not a customer will purchase a product based on their age and income. We have a dataset of 90 customers, where each row represents a customer and the columns represent their age, income, and whether or not they purchased the product (0 for not purchased, 1 <span class="No-Break">for purchased).</span></p>
<p>We can use logistic regression to model the probability of a customer purchasing the product based on their age and income. The logistic regression model can be expressed as <span class="No-Break">the following:</span></p>
<p><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">h</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">_____________________________</span><span class="_-----MathTools-_Math_Base">   </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>where <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">0</span>, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">1</span>, and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">2</span> are the parameters of the model that we need to estimate from <span class="No-Break">the data.</span></p>
<p>We can estimate these parameters using maximum likelihood estimation. Once we have estimated the parameters, we can use the model to predict the probability of a customer purchasing the product for new customers based on their age <span class="No-Break">and income.</span></p>
<p>For example, if a new customer is 35 years old and has an income of $50,000, we can use the model to predict their probability of purchasing the product <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">h</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">  </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">___________________________</span><span class="_-----MathTools-_Math_Base">   </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">35</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">50000</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span></p>
<p>We can then use a decision<a id="_idIndexMarker155"/> threshold, such as 0.5, to determine whether to classify the customer as a purchaser or non-purchaser. Note that a choice of threshold can affect the trade-off between precision <span class="No-Break">and recall.</span></p>
<p>We can solve the above problem with R and Python using the corresponding libraries. Let’s take a look at how to do that. In the following examples, we are going to use a sample dataset <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">customer_data.csv</strong></span><span class="No-Break">.</span></p>
<p>Here is an overview of <span class="No-Break">the datafile:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Age</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Income</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">purchased</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">22</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">20000</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">35</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80000</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">42</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">50000</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">27</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">30000</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">48</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">70000</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">38</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">60000</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">41</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">45000</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">29</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">35000</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">33</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40000</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.2: Customer data</p>
<h3>Example solution with R</h3>
<p>The following code <a id="_idIndexMarker156"/>reads customer data from a CSV file, builds a logistic regression model to predict the probability of a customer making a purchase based on their age and income, and then predicts the probability for a new customer and provides a corresponding prediction message based on the <span class="No-Break">probability value:</span></p>
<pre class="source-code">
data &lt;- read.csv('customer_data.csv')
model &lt;- glm(purchased ~ age + income, data = data, family = binomial())
new_customer &lt;- data.frame(age = 35, income = 50000)
prob_purchase &lt;- predict(model, new_customer, type = "response")
if (prob_purchase &gt;= 0.5) {
  print("The customer is predicted to purchase the product.")
} else {
  print("The customer is predicted not to purchase the product.")
}</pre> <h3>Example solution with Python</h3>
<p>This code reads customer data from a CSV file into a pandas DataFrame, uses scikit-learn’s <strong class="source-inline">LogisticRegression</strong> class to<a id="_idIndexMarker157"/> build a logistic regression model to predict purchase probabilities based on age and income, and then predicts the probability for a new customer and provides a corresponding prediction message based on the <span class="No-Break">probability value:</span></p>
<pre class="source-code">
import pandas as pd
data = pd.read_csv("customer_data.csv")
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(data[['age', 'income']], data['purchased'])
new_customer = pd.DataFrame({'age': [35], 'income': [50000]})
prob_purchase = model.predict_proba(new_customer)[:, 1]
if prob_purchase &gt;= 0.5:
    print("The customer is predicted to purchase the product.")
else:
    print("The customer is predicted not to purchase the product.")</pre> <p>The example will print “<strong class="bold">The </strong><strong class="bold"><a id="_idIndexMarker158"/></strong><strong class="bold">customer is predicted to purchase the product</strong>” with a probability <span class="No-Break">of 0.81.</span></p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>Lasso regression</h2>
<p>Lasso regression or least <a id="_idIndexMarker159"/>absolute shrinkage and selection operator (also known as L1 regularization) is a type <a id="_idIndexMarker160"/>of linear regression method used for variable selection and regularization. It is a regression technique that adds a penalty term to the sum of squared errors, which includes the absolute values of the <span class="No-Break">regression coefficients.</span></p>
<p>The lasso regression algorithm aims to minimize the residual sum of squares subject to the constraint that the sum of absolute values of the coefficients is less than or equal to a constant value. This constraint causes some coefficients to be shrunk toward zero, resulting in sparse models, whereas some features have zero coefficients, effectively excluding them from <span class="No-Break">the model.</span></p>
<p>Lasso regression is particularly useful when dealing with high-dimensional datasets, where the number of features (or predictors) is much larger than the number of observations. It can also help to overcome problems such as overfitting in linear regression models, where the model becomes too complex and fits the training data too well but fails to generalize well to <span class="No-Break">new data.</span></p>
<h3>Example</h3>
<p>We have a dataset that <a id="_idIndexMarker161"/>contains information about houses and their sale prices, including features such as the number of bedrooms, the size of the lot, the age of the house, and the location. We want to build a model that can predict the sale price of a house based on <span class="No-Break">these features.</span></p>
<p>To build this model, we can use Lasso regression. We start by splitting our dataset into a training set and a test set. We then use the training set to fit a Lasso regression model with a specific value of the regularization parameter (alpha). We can tune this parameter using cross-validation to find the best value that results in the lowest error on the <span class="No-Break">test set.</span></p>
<p>Once we have trained our model, we can use it to make predictions on new data by inputting the values of the features and computing the corresponding sale price. The Lasso regression model will automatically select the most important features for prediction by shrinking the coefficients of less important features <span class="No-Break">toward zero.</span></p>
<p>For example, let’s say our Lasso <a id="_idIndexMarker162"/>regression model selected the number of bedrooms and the location as the most important features for predicting the sale price and shrunk the coefficients of the other features to zero. We can use this information to inform our decision-making when buying or <span class="No-Break">selling houses.</span></p>
<p>Let’s take a look at how a sample solution would work in both R and Python. Both examples use the California Housing dataset, split the data into training and testing sets, fit a Lasso regression model, predict on the test set, and evaluate the model’s performance on the testing set using the <span class="No-Break">RMSE metric.</span></p>
<h3>Example solution with R</h3>
<p>The code performs a linear regression using the<a id="_idIndexMarker163"/> Lasso regularization (L1 penalty) to predict the median house values based on a housing dataset. The dataset is loaded from a specific URL, and after preprocessing, it is split into training and testing sets. The <strong class="source-inline">glmnet</strong><strong class="bold"> </strong>library is used to build the model, and the <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) is calculated t<a id="_idIndexMarker164"/>o evaluate the <span class="No-Break">model’s performance:</span></p>
<pre class="source-code">
url &lt;- "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv"
housing &lt;- read.csv(url)
housing &lt;- na.omit(housing)
set.seed(123)
train_index &lt;- sample(nrow(housing), nrow(housing) * 0.8)
train &lt;- housing[train_index, ]
test &lt;- housing[-train_index, ]
library(glmnet)
x &lt;- model.matrix(median_house_value ~ ., train)[,-1]
y &lt;- train$median_house_value
model &lt;- cv.glmnet(x, y, alpha = 1)
x_test &lt;- model.matrix(median_house_value ~ ., test)[,-1]
y_test &lt;- test$median_house_value
predictions &lt;- predict(model, newx = x_test)
rmse &lt;- sqrt(mean((predictions - y_test)^2))
print(paste0("RMSE: ", rmse))</pre> <h3>Example solution with Python</h3>
<p>The following code begins by<a id="_idIndexMarker165"/> importing pandas and assigning it the alias <strong class="source-inline">pd</strong> for convenience. Next, it reads data from a CSV file hosted at a specific URL and creates a DataFrame named “housing” to hold the dataset. To handle categorical data effectively, the code performs one-hot encoding on the <strong class="source-inline">ocean_proximity</strong> column, converting it into multiple <span class="No-Break">binary columns.</span></p>
<p>Data cleanliness is vital for reliable models, so the script takes care of missing values by removing any rows containing NaN entries from the DataFrame. The dataset is then split into training and testing sets using the <strong class="source-inline">train_test_split</strong> function from scikit-learn, where 80% of the data is used for training and the remaining 20% <span class="No-Break">for testing.</span></p>
<p>Now comes the machine-learning part. The script imports the <strong class="source-inline">LassoCV</strong> class from scikit-learn, which is a linear regression model with L1 regularization (Lasso). <strong class="source-inline">LassoCV</strong> performs cross-validation to find the optimal regularization strength (alpha) from a predefined set of values. The model is then trained on the training data using the “<span class="No-Break"><strong class="source-inline">fit</strong></span><span class="No-Break">” method.</span></p>
<p>After training, the model is put to the test. Predictions are made on the testing data using the trained <strong class="source-inline">LassoCV</strong> model, and the performance of the model is assessed using the RMSE metric. The RMSE represents the deviation between the predicted <strong class="source-inline">median_house_value</strong> and the actual target values in the testing data. A lower RMSE indicates better <span class="No-Break">predictive accuracy.</span></p>
<p>Finally, the script concludes by displaying the calculated RMSE value, providing insight into how well the <strong class="source-inline">LassoCV</strong> model performs in predicting <strong class="source-inline">median_house_value</strong> on <span class="No-Break">unseen data:</span></p>
<pre class="source-code">
import pandas as pd
url = "https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv"
housing = pd.read_csv(url)
housing = pd.get_dummies(housing, columns=['ocean_proximity'])
housing.dropna(inplace=True)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(housing.drop(columns=['median_house_value']), housing['median_house_value'], test_size=0.2, random_state=123)
from sklearn.linear_model import LassoCV
model = LassoCV(alphas=[0.001, 0.01, 0.1, 1], cv=5)
model.fit(X_train, y_train)
from sklearn.metrics import mean_squared_error
predictions = model.predict(X_test)
rmse = mean_squared_error(y_test, predictions, squared=False)
print(f"RMSE: {rmse}")</pre> <p>In the Python example, we have to <a id="_idIndexMarker166"/>one-hot encode the <strong class="source-inline">ocean_proximity</strong> feature before splitting the data into training and testing sets, which will allow the Lasso regression model to use the feature in the model. Both models will give us predictions and print RMSE of around 67,000 to 68,000 depending on the version of the <span class="No-Break">libraries used.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In this section, we took a closer look at Lasso regression (L1 regularization). L2 regularization is used with Ridge regression. Lasso and Ridge regression differ mainly in the type of regularization they apply, their impact on feature selection, and their handling of multicollinearity. Lasso tends to produce sparse models with feature selection, while Ridge maintains all features but with smaller coefficients, making it more suitable when multicollinearity is a concern or if you want to control the magnitude of coefficients. The choice between them depends on your specific modeling goals and the nature of your data. We are not going to dive into the details of Ridge regression in <span class="No-Break">this chapter.</span></p>
<p>In this chapter, we have investigated various linear regression models and how to implement these using R and Python. Linear regression models are an essential part of machine learning and understanding <a id="_idIndexMarker167"/>the principles is an important skill. In the next section, we will take a closer look into clustering algorithms, decision trees, and <span class="No-Break">random forests.</span></p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor043"/>Clustering algorithms, decision trees, and random forests</h1>
<p><strong class="bold">Clustering algorithms</strong> are used for unsupervised learning tasks, which means they are used to find patterns in data <a id="_idIndexMarker168"/>without any predefined labels or categories. The goal of clustering algorithms is to group similar data points together in clusters, while keeping dissimilar data <span class="No-Break">points separate.</span></p>
<p>There are several types of clustering algorithms, including K-means, hierarchical clustering, and density-based clustering. K-means is a popular clustering algorithm that works by dividing a dataset into K clusters, where K is a predefined number of clusters. Hierarchical clustering is another clustering algorithm that creates a hierarchy of clusters based on the similarity between data points. Density-based clustering algorithms, such as DBSCAN, group together data points that are closely packed together in <span class="No-Break">high-density regions.</span></p>
<p><strong class="bold">Decision trees</strong>, on the other <a id="_idIndexMarker169"/>hand, are used for supervised learning tasks, which means they are used to make predictions or decisions based on input data with predefined labels or categories. A decision tree is a tree-like structure that consists of nodes and branches, where each node represents a feature or attribute, and each branch represents a decision based on that feature. The goal of a decision tree is to create a model that can accurately predict the label or category of a new input based on <span class="No-Break">its features.</span></p>
<p>There are several types of decision trees, including ID3, C4.5, and CART. The ID3 algorithm is a popular decision tree algorithm that works by choosing the attribute with the highest information gain as<a id="_idIndexMarker170"/> the root node, and recursively building the tree by selecting attributes that maximize information gain at each level. The C4.5 algorithm is an improved version of ID3 that can handle continuous and discrete data, and CART is another decision tree algorithm that can handle both classification and <span class="No-Break">regression tasks.</span></p>
<p><strong class="bold">Random forests</strong> combine multiple decision trees to create a more accurate and robust model. A random forest <a id="_idIndexMarker171"/>consists of a large number of decision trees, each trained on a different subset of the data and using a random subset of the available features. This helps to reduce overfitting and increase the generalization ability of <span class="No-Break">the model.</span></p>
<p>The random subset of features used for each tree is selected randomly from the available features, with a new subset selected for each tree. This ensures that each tree in the forest is different and provides a diverse set of predictions. During training, each tree in the forest is grown to its full depth, and predictions are made by aggregating the predictions of all the trees in <span class="No-Break">the forest.</span></p>
<p>The aggregation process can vary depending on the task at hand. For classification tasks, the most common method is to use a majority vote, where the final prediction is the class that is predicted by the most trees in the forest. For regression tasks, the most common method is to use the average prediction of all the trees in <span class="No-Break">the forest.</span></p>
<p>Random forests have several advantages over a single decision tree, including improved accuracy, reduced overfitting, and robustness to noise and outliers in the data. They are also relatively easy to use and can handle a wide range of input features and data types. However, they can be computationally expensive to train and can be difficult to interpret and visualize, especially when dealing with a large number <span class="No-Break">of trees.</span></p>
<p>In the next sections, we will take a closer look at some of the clustering and decision <span class="No-Break">tree algorithms.</span></p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>K-means clustering</h2>
<p>K-means clustering is a <a id="_idIndexMarker172"/>popular algorithm used to partition a set of data points into K clusters, where K is a predefined number. The algorithm works by iteratively assigning data points to the nearest cluster centroid and updating the cluster centroids based on the <span class="No-Break">new assignments.</span></p>
<p>Here’s a simple step-by-step overview of how the K-means <span class="No-Break">algorithm works:</span></p>
<ol>
<li>Initialize K centroids randomly from the <span class="No-Break">data points.</span></li>
<li>Assign each data point to the nearest centroid based on the Euclidean distance between the data point and <span class="No-Break">the centroid.</span></li>
<li>Update the centroids of each cluster by computing the mean of all the data points assigned to <span class="No-Break">that cluster.</span></li>
<li>Repeat steps 2 and 3 until the centroids no longer move significantly or a maximum number of iterations <span class="No-Break">is reached.</span></li>
</ol>
<p>The goal of the K-means algorithm is to minimize the sum of squared distances between each data point and its assigned cluster<a id="_idIndexMarker173"/> centroid, also known as the “inertia.” The algorithm can be sensitive to the initial random selection of centroids, so it’s often a good idea to run the algorithm multiple times with different initializations and select the solution with the <span class="No-Break">lowest inertia.</span></p>
<p>K-means is a fast and effective algorithm for clustering data, but it does have some limitations. It assumes that the clusters are spherical and of equal size, and it can be sensitive to outliers and noise in the data. Additionally, determining the optimal number of clusters, K, can be challenging and may require some domain knowledge or trial <span class="No-Break">and error.</span></p>
<h3>Example</h3>
<p>The iris dataset<a id="_idIndexMarker174"/> contains measurements of four features (sepal length, sepal width, petal length, and petal width) for 150 iris flowers, with 50 flowers from each of three species (setosa, versicolor, and virginica). We can use K-means clustering to group these flowers into distinct clusters based on their <span class="No-Break">feature values.</span></p>
<p>To do this, we first select the features we want to cluster on and preprocess the data by scaling the features to have zero mean and unit variance. Scaling is done to ensure that we have equal influence of features. We then apply the K-means algorithm to the preprocessed data, specifying the number of clusters K we want to create. In this case, we might choose K=3 to correspond to the three <span class="No-Break">iris species.</span></p>
<p>The K-means algorithm then partitions the flowers into K clusters based on their feature values, with each cluster represented by<a id="_idIndexMarker175"/> its centroid (the mean feature values of the flowers assigned to the cluster). We can examine the resulting clusters and their centroids to gain insights into the structure of the <span class="No-Break">iris dataset.</span></p>
<p>For example, we might find that one cluster contains flowers with smaller sepal and petal dimensions, which could correspond to the setosa species. Another cluster might contain flowers with larger petal dimensions and intermediate sepal dimensions, which could correspond to the versicolor species. The third cluster might contain flowers with larger sepal dimensions and larger petal dimensions, which could correspond to the <span class="No-Break">virginica species.</span></p>
<p>By using K-means clustering to group the iris flowers based on their feature values, we can gain a deeper understanding of the structure of the dataset and potentially identify patterns and relationships in data. Let’s see how the above example would look in R <span class="No-Break">and Python.</span></p>
<h3>Example with R</h3>
<p>The following R code uses the “iris” dataset, a popular <a id="_idIndexMarker176"/>dataset in the machine-learning community. It performs K-means clustering on the dataset’s four numeric attributes: <strong class="source-inline">Sepal.Length</strong>, <strong class="source-inline">Sepal.Width</strong>, <strong class="source-inline">Petal.Length</strong>, and <strong class="source-inline">Petal.Width</strong>. The code sets a random seed for reproducibility and applies the K-means algorithm with three cluster centers. After the clustering is performed, the code displays the cluster assignments for each data point, indicating which cluster each observation belongs to (represented by values 1, 2, or 3). K-means clustering aims to group similar data points into clusters and is a common technique for unsupervised machine <span class="No-Break">learning tasks:</span></p>
<pre class="source-code">
data(iris)
iris_cluster &lt;- iris[, c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")]
set.seed(123)
kmeans_results &lt;- kmeans(iris_cluster, centers = 3)
kmeans_results$cluster</pre> <h3>Example with Python</h3>
<p>The following Python <a id="_idIndexMarker177"/>code uses the scikit-learn library to perform K-means clustering on the Iris dataset. The Iris dataset is loaded using <strong class="source-inline">load_iris()</strong> from scikit-learn, containing measurements of iris flowers’ sepal length, sepal width, petal length, and petal width, along with their corresponding <span class="No-Break">species labels.</span></p>
<p>The script extracts the four feature columns for clustering and stores them in the <strong class="source-inline">iris_cluster</strong> variable. Then, it imports the <strong class="source-inline">KMeans</strong> class from scikit-learn’s <span class="No-Break"><strong class="source-inline">sklearn.cluster</strong></span><span class="No-Break"> module.</span></p>
<p>The K-means algorithm is applied to the feature data (<strong class="source-inline">iris_cluster</strong>) with the number of clusters (<strong class="source-inline">n_clusters</strong>) set to <strong class="source-inline">3</strong>. The <strong class="source-inline">random_state</strong> parameter is set to <strong class="source-inline">123</strong> to ensure the reproducibility of <span class="No-Break">the results.</span></p>
<p>After clustering, the code prints the cluster assignments for each data point in the Iris dataset. Each data point is assigned a cluster label (0, 1, or 2), indicating the group it belongs to according to the <span class="No-Break">K-means clustering:</span></p>
<pre class="source-code">
from sklearn.datasets import load_iris
iris = load_iris()
iris_cluster = iris.data[:, [0, 1, 2, 3]]
from sklearn.cluster import KMeans
kmeans_results = KMeans(n_clusters=3, random_state=123).fit(iris_cluster)
print(kmeans_results.labels_)</pre> <p>Both of the code examples will print the cluster assignments for the iris dataset. The result looks similar to <span class="No-Break">the following:</span></p>
<pre class="console">
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 2]</pre> <p class="callout-heading">Note</p>
<p class="callout">This is a simple example. In practice, you would likely spend more time exploring and visualizing the data, tuning the number of clusters, and interpreting the results of the <span class="No-Break">clustering algorithm.</span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>ID3 decision tree</h2>
<p><strong class="bold">ID3</strong> (<strong class="bold">Iterative Dichotomiser 3</strong>) is a popular<a id="_idIndexMarker178"/> algorithm for building decision trees. The ID3 algorithm was developed by Ross Quinlan in 1986 and is based on the concept of <span class="No-Break">information entropy.</span></p>
<p>The goal of a decision tree is to create a model that can be used to make predictions or classify new instances based on their characteristics. A decision tree consists of a set of nodes and edges, where each node represents a decision or a test on one or more input variables, and each edge represents the possible outcome of <span class="No-Break">that decision.</span></p>
<p>The ID3 algorithm works by recursively partitioning the data based on the input variables with the highest information gain. Information gain is a measure of the reduction in entropy or impurity that results from splitting the data on a particular input variable. The algorithm selects the input variable that maximizes information gain at each step, until all instances in a given partition belong to the same class or a stopping criterion <span class="No-Break">is met.</span></p>
<p>The ID3 algorithm has several advantages, including its simplicity and efficiency in handling large datasets with categorical variables. However, it has limitations in handling continuous variables and overfitting, which can be addressed by using modified algorithms such as C4.5 <span class="No-Break">and CART.</span></p>
<p>In the next example, we can see how ID3 works <span class="No-Break">in practice.</span></p>
<h3>Example</h3>
<p>In this example, we are using the following <span class="No-Break">animal-related dataset:</span></p>
<table class="No-Table-Style" id="table003">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Animal</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Has fur?</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Has feathers?</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Eats meat?</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Classification</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Dog</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Mammal</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Cat</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Mammal</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Parrot</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Bird</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Eagle</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Bird</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Snake</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">No</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Yes</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Reptile</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.3: Animal characteristics data</p>
<p>Our goal is to build a decision tree that can classify animals based on <span class="No-Break">their features.</span></p>
<p>First, let’s calculate the entropy of the <a id="_idIndexMarker179"/>entire dataset. Entropy measures the impurity of a dataset. A dataset with all the same class labels has an entropy of 0, while a dataset with an equal number of examples from each class has an entropy <span class="No-Break">of 1.</span></p>
<p>A general equation to calculate entropy can be represented in the the <span class="No-Break">following way:</span></p>
<p><span class="_-----MathTools-_Math_Variable_v-normal">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span></p>
<p>In this equation, <em class="italic">E(S)</em> is the entropy of a set <em class="italic">S</em>, <em class="italic">n</em> is the number of classes in <em class="italic">S</em>, and <em class="italic">pi</em> is the proportion of the number of elements in <em class="italic">S</em> that belong to <span class="No-Break">class </span><span class="No-Break"><em class="italic">i</em></span><span class="No-Break">.</span></p>
<p>In this example, we have <em class="italic">n=3</em> classes: Mammal, Bird, and Reptile. The number of animals in each class is <span class="No-Break">as follows:</span></p>
<ul>
<li>Mammal: 2 (<span class="No-Break">Dog, Cat)</span></li>
<li>Bird: 2 (<span class="No-Break">Parrot, Eagle)</span></li>
<li>Reptile: <span class="No-Break">1 (Snake)</span></li>
</ul>
<p>Therefore, the probabilities are <span class="No-Break">as follows:</span></p>
<ul>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Mammal</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.4</span></span></li>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Bird</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.4</span></span></li>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Reptile</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.2</span></span></li>
</ul>
<p>Substituting these values into the entropy formula, we get <span class="No-Break">the following:</span></p>
<p><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0.4</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Number">4</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.4</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Number">4</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.2</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1.52193</span></span></p>
<p>So the entropy of the “Classification” attribute is <span class="No-Break">approximately 1.52.</span></p>
<p>Next, let’s calculate the information gain of the “Has fur?” attribute. Information gain is a measure of how much a given attribute<a id="_idIndexMarker180"/> or feature in a dataset contributes to reducing the uncertainty in the classification of the data. In decision trees, information gain is used to select the best attribute to use for splitting the data at each node of <span class="No-Break">the tree.</span></p>
<p>The information gain formula is <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Text">IG</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Text">Values</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">E</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">S</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">v</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p>where <em class="italic">A</em> is the attribute (in this case, “Has fur?”), <em class="italic">v</em> is a possible value of the attribute, <em class="italic">Values(A)</em> is the set of possible values, <em class="italic">|Sv|</em> is the number of animals in the dataset that have attribute <em class="italic">A=v</em>, and <em class="italic">E(Sv)</em> is the entropy of the subset of animals that have <span class="No-Break">attribute </span><span class="No-Break"><em class="italic">A=v</em></span><span class="No-Break">.</span></p>
<p>We can split the data based on whether the animals have fur or not. The subsets are <span class="No-Break">as follows:</span></p>
<ul>
<li>Has fur: <span class="No-Break">Dog, Cat</span></li>
<li>No fur: Parrot, <span class="No-Break">Eagle, Snake</span></li>
</ul>
<p>The proportion of animals in each subset is <span class="No-Break">as follows:</span></p>
<ul>
<li><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Has</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span></li>
<li><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">No</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span></li>
</ul>
<p>To calculate <span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Has</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur</span><span class="_-----MathTools-_Math_Base">)</span>, we need to count the number of animals in each class that <span class="No-Break">have fur:</span></p>
<ul>
<li>Mammal: 2 (<span class="No-Break">Dog, Cat)</span></li>
<li><span class="No-Break">Bird: 0</span></li>
<li><span class="No-Break">Reptile: 0</span></li>
</ul>
<p>Therefore, the probabilities are <span class="No-Break">as follows:</span></p>
<ul>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Mammal</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span></li>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Bird</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></li>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Reptile</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></li>
</ul>
<p>Substituting these values into the entropy formula, we get the <span class="No-Break">following results:</span></p>
<p><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Has</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></p>
<p>Entropy of 0 means that the set is perfectly classified: in this case, all animals with fur <span class="No-Break">are mammals.</span></p>
<p>To calculate <span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">No</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur</span><span class="_-----MathTools-_Math_Base">)</span>, we need to <a id="_idIndexMarker181"/>count the number of animals in each class that don’t <span class="No-Break">have fur:</span></p>
<ul>
<li><span class="No-Break">Mammal: 0</span></li>
<li>Bird: 2 (<span class="No-Break">Parrot, Eagle)</span></li>
<li>Reptile: <span class="No-Break">1 (Snake)</span></li>
</ul>
<p>Therefore, the probabilities are <span class="No-Break">as follows:</span></p>
<ul>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Mammal</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></li>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Bird</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.67</span></span></li>
<li><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Reptile</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.33</span></span></li>
</ul>
<p>Substituting these values into the entropy formula, we get the <span class="No-Break">following result:</span></p>
<p><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">No</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.67</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Number">67</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.33</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Number">33</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.9183</span></span></p>
<p>Now we can substitute these values into the information <span class="No-Break">gain formula:</span></p>
<p><span class="_-----MathTools-_Math_Text">IG</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Text">Has</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur?</span><span class="_-----MathTools-_Math_Text">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Text">Values</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Text">Has</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Text">fur?</span><span class="_-----MathTools-_Math_Text">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">E</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">S</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">v</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1.52193</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.9183</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.971</span></span></p>
<p>Therefore, the information gained for the “<strong class="source-inline">Has</strong> <strong class="source-inline">fur</strong>?” attribute is <span class="No-Break">approximately 0.971.</span></p>
<p>We could also calculate the information gained for the “<strong class="source-inline">Has</strong> <strong class="source-inline">feathers</strong>?” and “<strong class="source-inline">Eats</strong> <strong class="source-inline">meat</strong>?” attributes by using the same formula and splitting the data based on whether the animals have feathers or not or whether they eat meat or not. The attribute with the highest information gain would <a id="_idIndexMarker182"/>be selected to split the data at the root of the <span class="No-Break">decision tree.</span></p>
<p>Information gain for “Has feathers?” is also 0.971, and for “Eats meat?” it is 0.322. In this case, we will select “Has fur?” for our <span class="No-Break">root node.</span></p>
<p>Let’s take a look at the same example with R and Python. Both code snippets will load the animal dataset, create the decision tree model, visualize it, and test the tree with new data. At the end, we can see that the new animal is classified <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">Mammal</strong></span><span class="No-Break">.</span></p>
<p>The final decision tree looks like the one in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 2.1: Final decision tree (printed using the following R code)" height="537" src="image/B19863_02_01.jpg" width="768"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: Final decision tree (printed using the following R code)</p>
<p>Here is the <a id="_idIndexMarker183"/>example <span class="No-Break">with R:</span></p>
<pre class="source-code">
data &lt;- data.frame(
  Animal = c("Dog", "Cat", "Parrot", "Eagle", "Snake"),
  Has_fur = c("Yes", "Yes", "No", "No", "No"),
  Has_feathers = c("No", "No", "Yes", "Yes", "No"),
  Eats_meat = c("Yes", "Yes", "No", "Yes", "Yes"),
  Classification = c("Mammal", "Mammal", "Bird", "Bird", "Reptile")
)
library(rpart)
library(rpart.plot)
tree &lt;- rpart(Classification ~ Has_fur + Has_feathers + Eats_meat, data = data, method = "class", control = rpart.control(minsplit = 1))
rpart.plot(tree, type=5)
new_data &lt;- data.frame(
  Has_fur = "Yes",
  Has_feathers = "No",
  Eats_meat = "Yes"
)
predicted &lt;- predict(tree, new_data, type = "class")
print(predicted)</pre> <p>The Python example is similar to the<a id="_idIndexMarker184"/> preceding R example, but in Python, we also need to convert categorical features to numerical using <span class="No-Break">one-hot encoding:</span></p>
<pre class="source-code">
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
data = pd.DataFrame({
    'Animal': ['Dog', 'Cat', 'Parrot', 'Eagle', 'Snake'],
    'Has_fur': ['Yes', 'Yes', 'No', 'No', 'No'],
    'Has_feathers': ['No', 'No', 'Yes', 'Yes', 'No'],
    'Eats_meat': ['Yes', 'Yes', 'No', 'Yes', 'Yes'],
    'Classification': ['Mammal', 'Mammal', 'Bird', 'Bird', 'Reptile']
})
data_encoded = pd.get_dummies(data[['Has_fur', 'Has_feathers', 'Eats_meat']])
clf = DecisionTreeClassifier(criterion='entropy', min_samples_split=2)
clf.fit(data_encoded, data['Classification'])
plt.figure(figsize=(8, 6))
plot_tree(clf, feature_names=data_encoded.columns, class_names=np.unique(data['Classification']), filled=True)
plt.show()
new_data = pd.DataFrame({
    'Has_fur_No': [0],
    'Has_fur_Yes': [1],
    'Has_feathers_No': [1],
    'Has_feathers_Yes': [0],
    'Eats_meat_No': [0],
    'Eats_meat_Yes': [1]
})
predicted = clf.predict(new_data)
print(predicted)</pre> <p class="callout-heading">Note</p>
<p class="callout">This is a really simple example of a decision tree using the ID3 algorithm. In real-world examples, we would most likely have a lot of data and therefore multiple nodes and branches in our <span class="No-Break">final tree.</span></p>
<p>We have now learned how clustering<a id="_idIndexMarker185"/> algorithms, decision trees, and random forests work. These algorithms are an important part of machine learning and are commonly used for classification. In the next section, we will take a closer look into boosting algorithms and <span class="No-Break">Naive Bayes.</span></p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>Boosting algorithms and Naive Bayes</h1>
<p><strong class="bold">Boosting</strong> is a machine learning technique that involves creating an ensemble of weak learners to form a strong<a id="_idIndexMarker186"/> learner. The idea behind boosting algorithms is to iteratively train models on the data, where each new model attempts to correct the errors of the previous model. Boosting algorithms are widely used in supervised learning tasks, such as classification <span class="No-Break">and regression.</span></p>
<p>There are several key types of <span class="No-Break">boosting algorithms:</span></p>
<ul>
<li><strong class="bold">AdaBoost (Adaptive Boosting):</strong> AdaBoost is one of the earliest and most popular boosting algorithms. It starts by training a base classifier on the entire dataset and then sequentially<a id="_idIndexMarker187"/> trains additional <a id="_idIndexMarker188"/>classifiers on the samples that the previous classifiers got wrong. The final prediction is made by taking a weighted sum of the predictions of all <span class="No-Break">the classifiers.</span></li>
<li><strong class="bold">Gradient Boosting:</strong> Gradient <a id="_idIndexMarker189"/>Boosting is another popular boosting algorithm that works by iteratively adding new models to the ensemble, each trained to minimize the error of the previous models. Gradient Boosting is used in regression and<a id="_idIndexMarker190"/> classification problems and has been shown to achieve state-of-the-art results in <span class="No-Break">many applications.</span></li>
<li><strong class="bold">XGBoost: </strong>XGBoost (Extreme Gradient Boosting) is a popular and highly optimized implementation <a id="_idIndexMarker191"/>of the Gradient<a id="_idIndexMarker192"/> Boosting algorithm. XGBoost uses a regularized objective function and a variety of techniques to reduce overfitting and <span class="No-Break">improve accuracy.</span></li>
</ul>
<p>Boosting algorithms are known for their ability to improve the accuracy of machine-learning models by reducing bias <span class="No-Break">and variance.</span></p>
<p><strong class="bold">Naive Bayes</strong> is a simple but <a id="_idIndexMarker193"/>effective algorithm used for classification tasks. It is based on Bayes’ theorem, which states that the probability of a hypothesis (in this case, a class label) is updated in the light of new evidence (in this case, the feature values of a new data point). The algorithm assumes that the features are independent of each other, which is why it is <span class="No-Break">called “naïve.”</span></p>
<p>In Naive Bayes, the probability of a data point belonging to a particular class is calculated by multiplying the prior probability of that class by the likelihood of the data point given that class. The algorithm then selects the class with the highest probability as the predicted class for that <span class="No-Break">data point.</span></p>
<p>There are several variants of the Naive <span class="No-Break">Bayes algorithm:</span></p>
<ul>
<li><strong class="bold">Gaussian Naive Bayes</strong>: Used when the <a id="_idIndexMarker194"/>features are continuous and assumed to be <span class="No-Break">normally distributed</span></li>
<li><strong class="bold">Multinomial Naive Bayes:</strong> Used when the features <a id="_idIndexMarker195"/>are discrete and represent counts or frequencies (such as in <span class="No-Break">text classification)</span></li>
<li><strong class="bold">Bernoulli Naive Bayes:</strong> A variant of Multinomial<a id="_idIndexMarker196"/> Naive Bayes, used when the features are binary (such as in <span class="No-Break">spam filtering)</span></li>
</ul>
<p>Naive Bayes is a simple and efficient algorithm that works well on high-dimensional datasets with sparse features. It is widely used in natural-language processing, spam filtering, sentiment analysis, and other classification tasks. However, the assumption of feature independence may not hold true in all cases, and the algorithm may not perform well if the data violates <span class="No-Break">this assumption.</span></p>
<p>In the next sections, we will take a closer look into some of the boosting and Naive <span class="No-Break">Bayes algorithms.</span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>XGBoost</h2>
<p><strong class="bold">XGBoost</strong> (<strong class="bold">eXtreme Gradient Boosting</strong>) is an open source machine learning library that is designed to be <a id="_idIndexMarker197"/>highly efficient, flexible, and scalable. It is an implementation of gradient-boosting algorithms that can be used for both classification and <span class="No-Break">regression problems.</span></p>
<p>It is based on the gradient-boosting framework and uses an ensemble of decision trees to make predictions. XGBoost is designed to handle large-scale and high-dimensional data and provides various techniques to prevent overfitting, such as regularization and <span class="No-Break">early stopping.</span></p>
<p>Let’s take a look into simple examples of XGBoost with R and Python. In these examples, we will use the iris dataset that we have already used in earlier chapters of this book. We will split the data into train and test <a id="_idIndexMarker198"/>datasets and then train our XGBoost model to predict the species. At the end, we will test the model with our test data and evaluate the <span class="No-Break">model performance.</span></p>
<p>Here’s the example <span class="No-Break">with R:</span></p>
<pre class="source-code">
library(xgboost)
library(caret)
data(iris)
set.seed(123)
trainIndex &lt;- createDataPartition(iris$Species, p = 0.8, list = FALSE)
train &lt;- iris[trainIndex, ]
test &lt;- iris[-trainIndex, ]
train$Species &lt;- as.factor(train$Species)
test$Species &lt;- as.factor(test$Species)
train$label &lt;- as.integer(train$Species) - 1
test$label &lt;- as.integer(test$Species) - 1
xgb_model &lt;- xgboost(data = as.matrix(train[, 1:4]),
                     label = train$label,
                     nrounds = 10,
                     objective = "multi:softmax",
                     num_class = 3,
                     eval_metric = "mlogloss")
predictions &lt;- predict(xgb_model, as.matrix(test[, 1:4]))
predictions &lt;- factor(predictions, levels = 0:2, labels = levels(iris$Species))
confusionMatrix(predictions, test$Species)</pre> <p>Here is the example <span class="No-Break">with Python:</span></p>
<pre class="source-code">
import xgboost as xgb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=123)
xgb_model = xgb.XGBClassifier(objective="multi:softmax", 
      n_estimators=10, seed=123)
xgb_model.fit(X_train, y_train)
predictions = xgb_model.predict(X_test)
print(classification_report(y_test, predictions))</pre> <h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>Gaussian Naive Bayes</h2>
<p>Gaussian Naive Bayes (GNB) is a variant<a id="_idIndexMarker199"/> of the Naive Bayes algorithm that assumes a Gaussian (normal) distribution for the input variables. In GNB, the probability distribution of each input variable is estimated separately for each class using the <span class="No-Break">training data.</span></p>
<p>During the testing phase, the model calculates the likelihood of the input features belonging to each class based on the Gaussian distribution parameters estimated during training. Then, the model applies Bayes’ theorem to calculate the posterior probability of each class, given the input features. The class with the highest posterior probability is then assigned to <span class="No-Break">the input.</span></p>
<p>GNB is called “naïve” because it makes a strong assumption that the input features are conditionally independent, given the class label. This assumption simplifies the model and makes the computation of the posterior probabilities tractable. However, this assumption may not hold in practice for some datasets, and thus, the model may not perform well. Nonetheless, GNB can be a fast and accurate classifier for datasets where the independence <span class="No-Break">assumption holds.</span></p>
<p>Let’s take a look into a similar classification <a id="_idIndexMarker200"/>problem, as we did with XGBoost, using Gaussian Naive Bayes. Here is the example code with both R and Python. Once again, we are using the iris dataset to classify the <span class="No-Break">different species.</span></p>
<p>Here is the example <span class="No-Break">with R:</span></p>
<pre class="source-code">
library(e1071)
data(iris)
set.seed(123)
trainIndex &lt;- sample(nrow(iris), 0.7 * nrow(iris))
train &lt;- iris[trainIndex, ]
test &lt;- iris[-trainIndex, ]
model &lt;- naiveBayes(Species ~ ., data = train)
predictions &lt;- predict(model, test)
cfm &lt;- table(predictions, test$Species)
print(cfm)</pre> <p>Here is the <a id="_idIndexMarker201"/>example <span class="No-Break">with Python:</span></p>
<pre class="source-code">
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, 
      iris.target, test_size=0.3, random_state=123)
model = GaussianNB()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
print(classification_report(y_test, predictions))</pre> <p>Now we have familiarized ourselves with the concepts of boosting algorithms and Naive Bayes. These methods are used widely in Qlik AutoML, and understanding the concepts of these algorithms is an essential skill to work with machine-learning problems. In our next section, we will take a<a id="_idIndexMarker202"/> closer look at some of the advanced machine-learning algorithms, including neural networks, deep learning, and <span class="No-Break">natural-language models.</span></p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor049"/>Neural networks, deep learning, and natural-language models</h1>
<p><strong class="bold">Neural networks</strong> are a type of machine-learning algorithm that is inspired by the structure and function of the <a id="_idIndexMarker203"/>human brain. They are composed of layers of interconnected nodes or artificial neurons that process and <span class="No-Break">transmit information.</span></p>
<p>In a neural network, the input data is fed into the first layer of nodes, which applies a set of mathematical transformations to the data and produces an output. The output of the first layer is then fed into the second layer, which applies another set of transformations to produce another output, and so on until the final output <span class="No-Break">is produced.</span></p>
<p>The connections between the nodes in the neural network have weights that are adjusted during the learning process to optimize the network’s ability to make accurate predictions or classifications. This is typically achieved using an optimization algorithm such as stochastic gradient descent. An example of the structure of a neural network is visualized in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 2.2: High-level architecture of a neural network" height="582" src="image/B19863_02_02.jpg" width="1105"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: High-level architecture of a neural network</p>
<p>Neural networks have been used to solve a wide range of machine-learning problems, including image and speech recognition, natural language processing, and predictive modeling. They have been shown to be effective in many applications due to their ability to learn complex, non-linear relationships between input and <span class="No-Break">output data.</span></p>
<p><strong class="bold">Deep learning</strong> involves the use <a id="_idIndexMarker204"/>of neural networks with multiple layers. It has achieved significant success in a wide range of applications, including computer vision, speech recognition, natural-language processing, and game playing. Some notable examples include the use of deep learning in image recognition tasks, such as identifying objects in photos, and in natural language processing tasks, such as language translation and <span class="No-Break">sentiment analysis.</span></p>
<p>One of the key advantages of deep learning is its ability to automatically learn features from data without the need for manual feature engineering. This makes it possible to train models on large datasets with many features, which can be computationally challenging or even impossible with traditional machine <span class="No-Break">learning methods.</span></p>
<p>However, training deep neural networks can also be challenging due to the large number of parameters and the risk of overfitting. To address these challenges, researchers have developed a variety of techniques, including regularization methods, dropout, and batch normalization, that can improve the performance and stability of deep <span class="No-Break">neural networks.</span></p>
<p><strong class="bold">Natural-language models</strong> are a type of machine-learning model that can process and understand human language. These<a id="_idIndexMarker205"/> models are trained on large amounts of text data, such as books, articles, and social media posts, and learn to generate coherent and semantically meaningful responses to natural <span class="No-Break">language input.</span></p>
<p>One common type of natural-language model is the language model, which is trained to predict the probability of a sequence of words given a context. For example, a language model might be trained to predict the probability of the word “pizza,” given the context “I am in the mood for something to eat that is typically round and covered <span class="No-Break">in toppings.”</span></p>
<p>Another type of natural-language model is the text-generation model, which can be used to generate natural-language text, such as news articles, stories, and chatbot responses. These models can be trained to generate text that is similar to a particular style or genre, or even to imitate the writing style of a <span class="No-Break">particular author.</span></p>
<p>Natural-language models have a wide range of applications, including language translation, sentiment analysis, chatbots and virtual assistants, and text summarization. Recent advances in deep learning and natural language processing have led to the development of powerful language models such as GPT-3, which can generate coherent and human-like text on a wide range of topics. Qlik Insight Advisor is one example of a product that has integrated the <span class="No-Break">natural-language model.</span></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor050"/>Summary</h1>
<p>In this chapter, we have gained an overview of different machine-learning algorithms. We have discovered how different algorithms can be used to solve problems and how they function. We started this chapter by getting familiar with some of the most common regression algorithms and gained knowledge on how to use these in R and Python. We discovered how to utilize clustering, decision trees, and random forests with <span class="No-Break">practical examples.</span></p>
<p>In the later part of this chapter, we moved on to more complex algorithms and learned how different boosting algorithms, neural networks, and other advanced models function. These models are utilized in Qlik AutoML, and it’s important to know how each model is structured. After reading this chapter, you now have a basic understanding of the models and are prepared to utilize these with Qlik tools. We will use most of these algorithms in the later parts of <span class="No-Break">this book.</span></p>
<p>In the next chapter, we will focus on data literacy in a machine-learning context. Data literacy is a hot topic, and it is also an important concept in the world of machine learning. To be able to create a well-functioning model and interpret the results from it, we must be able to understand the data. This is where data literacy comes <span class="No-Break">into play.</span></p>
</div>
</div></body></html>