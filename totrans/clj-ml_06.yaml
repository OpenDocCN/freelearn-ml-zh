- en: Chapter 6. Building Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。构建支持向量机
- en: In this chapter, we will explore **Support Vector Machines** (**SVMs**). We
    will study several SVM implementations in Clojure that can be used to build and
    train an SVM using some given training data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨**支持向量机**（**SVMs**）。我们将研究Clojure中的一些SVM实现，这些实现可以用来使用一些给定的训练数据构建和训练SVM。
- en: SVMs are supervised learning models that are used for both regression and classification.
    In this chapter, however, we will focus on the problem of classification within
    the context of SVMs. SVMs find applications in text mining, chemical classification,
    and image and handwriting recognition. Of course, we should not overlook the fact
    that the overall performance of a machine learning model mostly depends on the
    amount and nature of the training data and is also affected by which machine learning
    model we use to model the available data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs是用于回归和分类的监督学习模型。然而，在本章中，我们将专注于SVMs中的分类问题。SVMs在文本挖掘、化学分类、图像和手写识别中都有应用。当然，我们不应忽视这样一个事实，即机器学习模型的整体性能主要取决于训练数据量和性质，并且也受我们用于建模可用数据的机器学习模型的影响。
- en: In the simplest form, an SVM separates and predicts two classes of data by estimating
    the optimal vector plane or **hyperplane** between these two classes represented
    in vector space. A **hyperplane** can be simply defined as a plane that has one
    less dimension than the ambient space. For a three-dimensional space, we would
    obtain a two-dimensional hyperplane.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，SVM通过估计在向量空间中表示的两个类别的最佳向量平面或**超平面**来分离和预测两个类别的数据。一个**超平面**可以简单地定义为比环境空间少一个维度的平面。对于三维空间，我们会得到一个二维超平面。
- en: A basic SVM is a non-probabilistic binary classifier that uses linear classification.
    In addition to linear classification, SVMs can also be used to perform nonlinear
    classification over several classes. An interesting aspect of SVMs is that the
    estimated vector plane will have a substantially large and distinct gap between
    the classes of input values. Due to this, SVMs often have a good generalization
    performance and also implement a kind of automatic complexity control to avoid
    overfitting. Hence, SVMs are also called **large margin classifiers**. In this
    chapter, we will also study how SVMs achieve this large margin between classes
    of input data, when compared to other classifiers. Another interesting fact about
    SVMs is that they scale very well with the number of features being modeled and
    thus, SVMs are often used in machine learning problems that deal with a large
    number of features.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的支持向量机（SVM）是一种非概率的二分类器，它使用线性分类。除了线性分类之外，SVMs还可以用于对多个类别进行非线性分类。SVMs的一个有趣方面是，估计的向量平面将在输入值的类别之间具有相当大且独特的间隙。正因为如此，SVMs通常具有很好的泛化性能，并且实现了一种自动复杂度控制来避免过拟合。因此，SVMs也被称为**大间隔分类器**。在本章中，我们还将研究SVMs如何与其他分类器相比，在输入数据的类别之间实现这种大间隔。关于SVMs的另一个有趣的事实是，它们与被建模的特征数量非常匹配，因此SVMs通常用于处理大量特征的机器学习问题。
- en: Understanding large margin classification
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解大间隔分类
- en: As we previously mentioned, SVMs classify input data across large margins. Let's
    examine how this is achieved. We use our definition of a logistic classification
    model, which we previously described in [Chapter 3](ch03.html "Chapter 3. Categorizing
    Data"), *Categorizing Data*, as a basis for reasoning about SVMs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，SVMs通过大间隔对输入数据进行分类。让我们来看看这是如何实现的。我们使用我们之前在[第3章](ch03.html "第3章。数据分类")中描述的逻辑分类模型定义，作为对SVMs进行推理的基础。
- en: 'We can use the logistic or *sigmoid* function to separate two classes of input
    values, as we described in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"),
    *Categorizing Data*. This function can be formally defined as a function of an
    input variable *X* as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用逻辑或*sigmoid*函数来分离两个类别的输入值，正如我们在[第3章](ch03.html "第3章。数据分类")中描述的，*数据分类*。这个函数可以正式定义为输入变量*X*的函数，如下所示：
- en: '![Understanding large margin classification](img/4351OS_06_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_01.jpg)'
- en: 'In the preceding equation, the output variable ![Understanding large margin
    classification](img/4351OS_06_02.jpg) depends not only on the variable ![Understanding
    large margin classification](img/4351OS_06_03.jpg), but also on the coefficient
    ![Understanding large margin classification](img/4351OS_06_04.jpg). The variable
    ![Understanding large margin classification](img/4351OS_06_03.jpg) is analogous
    to the vector of input values in our model, and the term ![Understanding large
    margin classification](img/4351OS_06_04.jpg) is the parameter vector of the model.
    For binary classification, the value of *Y* must exist in the range of 0 and 1\.
    Also, the class of a set of input values is determined by whether the output variable
    ![Understanding large margin classification](img/4351OS_06_02.jpg) is closer to
    0 or 1\. For these values of *Y*, the term ![Understanding large margin classification](img/4351OS_06_05.jpg)
    is either much greater than or much less than 0\. This can be formally expressed
    as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，输出变量 ![理解大间隔分类](img/4351OS_06_02.jpg) 不仅依赖于变量 ![理解大间隔分类](img/4351OS_06_03.jpg)，还依赖于系数
    ![理解大间隔分类](img/4351OS_06_04.jpg)。变量 ![理解大间隔分类](img/4351OS_06_03.jpg) 类似于我们模型中的输入值向量，而项
    ![理解大间隔分类](img/4351OS_06_04.jpg) 是模型的参数向量。对于二元分类，*Y* 的值必须在 0 和 1 的范围内。此外，一组输入值的类别由输出变量
    ![理解大间隔分类](img/4351OS_06_02.jpg) 是更接近 0 还是 1 来决定。对于这些 *Y* 的值，项 ![理解大间隔分类](img/4351OS_06_05.jpg)
    要么远大于 0，要么远小于 0。这可以形式化地表达如下：
- en: '![Understanding large margin classification](img/4351OS_06_06.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_06.jpg)'
- en: 'For ![Understanding large margin classification](img/4351OS_06_07.jpg) sample
    with input values ![Understanding large margin classification](img/4351OS_06_08.jpg)
    and output values ![Understanding large margin classification](img/4351OS_06_09.jpg),
    we define the cost function ![Understanding large margin classification](img/4351OS_06_10.jpg)
    as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有输入值 ![理解大间隔分类](img/4351OS_06_08.jpg) 和输出值 ![理解大间隔分类](img/4351OS_06_09.jpg)
    的 ![理解大间隔分类](img/4351OS_06_07.jpg) 个样本，我们定义成本函数 ![理解大间隔分类](img/4351OS_06_10.jpg)
    如下：
- en: '![Understanding large margin classification](img/4351OS_06_12.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_12.jpg)'
- en: Note
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the term ![Understanding large margin classification](img/4351OS_06_11.jpg)
    represents the output variable calculated from the estimated model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，项 ![理解大间隔分类](img/4351OS_06_11.jpg) 代表从估计模型计算得到的输出变量。
- en: 'For a logistic classification model, ![Understanding large margin classification](img/4351OS_06_11.jpg)
    is the value of logistic function when applied to a set of input values ![Understanding
    large margin classification](img/4351OS_06_08.jpg). We can simplify and expand
    the summation term ![Understanding large margin classification](img/4351OS_06_13.jpg)
    in the cost function defined in the preceding equation, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归分类模型，![理解大间隔分类](img/4351OS_06_11.jpg) 是将逻辑函数应用于一组输入值 ![理解大间隔分类](img/4351OS_06_08.jpg)
    时的值。我们可以简化并展开前面方程定义的成本函数中的求和项 ![理解大间隔分类](img/4351OS_06_13.jpg)，如下所示：
- en: '![Understanding large margin classification](img/4351OS_06_14.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_14.jpg)'
- en: 'It''s obvious that the cost function shown in the preceding expression depends
    on the two logarithmic terms in the expression. Thus, we can represent the cost
    function as a function of these two logarithmic terms, represented by the terms
    ![Understanding large margin classification](img/4351OS_06_15.jpg) and ![Understanding
    large margin classification](img/4351OS_06_16.jpg). Now, let''s assume the two
    terms as shown the following equation:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，前面表达式中显示的成本函数取决于表达式中的两个对数项。因此，我们可以将成本函数表示为这两个对数项的函数，分别用项 ![理解大间隔分类](img/4351OS_06_15.jpg)
    和 ![理解大间隔分类](img/4351OS_06_16.jpg) 表示。现在，让我们假设以下方程中的两个项：
- en: '![Understanding large margin classification](img/4351OS_06_17.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_17.jpg)'
- en: 'Both the functions ![Understanding large margin classification](img/4351OS_06_15.jpg)
    and ![Understanding large margin classification](img/4351OS_06_16.jpg) are composed
    using the logistic function. A classifier that models the logistic function must
    be trained such that these two functions are minimized over all possible values
    of the parameter vector ![Understanding large margin classification](img/4351OS_06_04.jpg).
    We can use the **hinge-loss** function to approximate the desired behavior of
    a linear classifier that uses the logistic function (for more information, refer
    to "Are Loss Functions All the Same?"). We will now study the hinge-loss function
    by comparing it to the logistic function. The following diagram depicts how the
    ![Understanding large margin classification](img/4351OS_06_15.jpg) function must
    vary with respect to the term ![Understanding large margin classification](img/4351OS_06_18.jpg)
    and how it can be modeled using the logistic and hinge-loss functions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 函数![理解大间隔分类](img/4351OS_06_15.jpg)和![理解大间隔分类](img/4351OS_06_16.jpg)都是使用逻辑函数组成的。一个模拟逻辑函数的分类器必须经过训练，使得这两个函数在参数向量![理解大间隔分类](img/4351OS_06_04.jpg)的所有可能值上都被最小化。我们可以使用**铰链损失**函数来近似使用逻辑函数的线性分类器的期望行为（更多信息，请参阅“损失函数都一样吗？”）。现在，我们将通过将其与逻辑函数进行比较来研究铰链损失函数。以下图表描述了![理解大间隔分类](img/4351OS_06_15.jpg)函数必须如何随![理解大间隔分类](img/4351OS_06_18.jpg)项变化，以及它如何可以使用逻辑函数和铰链损失函数来建模：
- en: '![Understanding large margin classification](img/image1.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/image1.jpg)'
- en: 'In the plot shown in the preceding diagram, the logistic function is represented
    as a smooth curve. The function is seen to decrease rapidly before a given point
    and then decreases at a lower rate. In this example, the point at which this change
    of rate of the logistic function occurs is found to be *x = 0*. The hinge-loss
    function approximates this by using two line segments that meet at the point *x
    = 0*. Interestingly, both these functions model a behavior that changes at a rate
    that is inversely proportional to the input value *x*. Similarly, we can approximate
    the effect of the ![Understanding large margin classification](img/4351OS_06_16.jpg)
    function using the hinge-loss function as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一张图中所示的图中，逻辑函数被表示为一条平滑的曲线。可以看到，在某个给定点之前，该函数迅速下降，然后以更低的速率下降。在这个例子中，逻辑函数速率变化发生的点是*x
    = 0*。铰链损失函数通过使用两个在*x = 0*点交汇的线段来近似这一点。有趣的是，这两个函数都模拟了一种随输入值*x*成反比变化的速率的行为。同样，我们可以使用铰链损失函数来近似![理解大间隔分类](img/4351OS_06_16.jpg)函数的效果，如下所示：
- en: '![Understanding large margin classification](img/image2.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/image2.jpg)'
- en: Note that the ![Understanding large margin classification](img/4351OS_06_16.jpg)
    function is directly proportional to the term ![Understanding large margin classification](img/4351OS_06_18.jpg).
    Thus, we can achieve the classification ability of the logistic function by modelling
    the hinge-loss function and a classifier built using the hinge-loss function will
    perform equally well as a classifier using the logistic function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，![理解大间隔分类](img/4351OS_06_16.jpg)函数与![理解大间隔分类](img/4351OS_06_18.jpg)项成正比。因此，我们可以通过模拟铰链损失函数来实现逻辑函数的分类能力，而使用铰链损失函数构建的分类器将表现得与使用逻辑函数的分类器一样好。
- en: As seen in the preceding diagram, the hinge-loss function only changes its value
    at the point ![Understanding large margin classification](img/4351OS_06_21.jpg).
    This applies to both the functions ![Understanding large margin classification](img/4351OS_06_15.jpg)
    and ![Understanding large margin classification](img/4351OS_06_16.jpg). Thus,
    we can use the hinge loss function to separate two classes of data depending on
    whether the value of ![Understanding large margin classification](img/4351OS_06_23.jpg)
    is greater or less than 0\. In this case, there's virtually no margin of separation
    between these two classes. To improve the margin of classification, we can modify
    the hinge-loss function such that its value is greater than 0 only when ![Understanding
    large margin classification](img/4351OS_06_24.jpg) or ![Understanding large margin
    classification](img/4351OS_06_25.jpg).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，hinge损失函数仅在![理解大间隔分类](img/4351OS_06_21.jpg)这一点上改变其值。这适用于![理解大间隔分类](img/4351OS_06_15.jpg)和![理解大间隔分类](img/4351OS_06_16.jpg)这两个函数。因此，我们可以使用hinge损失函数根据![理解大间隔分类](img/4351OS_06_23.jpg)的值是大于还是小于0来分离两类数据。在这种情况下，这两类数据之间几乎没有分离间隔。为了提高分类间隔，我们可以修改hinge损失函数，使其仅在![理解大间隔分类](img/4351OS_06_24.jpg)或![理解大间隔分类](img/4351OS_06_25.jpg)时值为大于0。
- en: 'The modified hinge-loss functions can be plotted for the two classes of data
    as follows. The following plot describes the case where ![Understanding large
    margin classification](img/4351OS_06_25.jpg):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的hinge损失函数可以如下绘制两类数据。以下图表描述了![理解大间隔分类](img/4351OS_06_25.jpg)的情况：
- en: '![Understanding large margin classification](img/image3.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/image3.jpg)'
- en: 'Similarly, the modified hinge-loss function for the case ![Understanding large
    margin classification](img/4351OS_06_24.jpg) can be illustrated by the following
    plot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于![理解大间隔分类](img/4351OS_06_24.jpg)情况修改后的hinge损失函数可以通过以下图表进行说明：
- en: '![Understanding large margin classification](img/image4.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/image4.jpg)'
- en: Note that the *hinge* occurs at *-1* in the case of ![Understanding large margin
    classification](img/4351OS_06_24.jpg).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在![理解大间隔分类](img/4351OS_06_24.jpg)的情况下，*hinge*发生在*-1*处。
- en: 'If we substitute the hinge-loss functions in place of the ![Understanding large
    margin classification](img/4351OS_06_15.jpg) and ![Understanding large margin
    classification](img/4351OS_06_16.jpg) functions, we arrive at an optimization
    problem of SVMs (for more information, refer to "Support-vector networks"), which
    can be formally written as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将![理解大间隔分类](img/4351OS_06_15.jpg)和![理解大间隔分类](img/4351OS_06_16.jpg)函数替换为hinge损失函数，我们就会得到SVMs（支持向量机）的优化问题（更多信息，请参阅“支持向量网络”），其形式化表达如下：
- en: '![Understanding large margin classification](img/4351OS_06_28.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_28.jpg)'
- en: 'In the preceding equation, the term ![Understanding large margin classification](img/4351OS_06_29.jpg)
    is the regularization parameter. Also, when ![Understanding large margin classification](img/4351OS_06_30.jpg),
    the behavior of the SVM is affected more by the ![Understanding large margin classification](img/4351OS_06_15.jpg)
    function than the ![Understanding large margin classification](img/4351OS_06_16.jpg)
    function, and vice versa when ![Understanding large margin classification](img/4351OS_06_31.jpg).
    In some contexts, the regularization parameter ![Understanding large margin classification](img/4351OS_06_29.jpg)
    of the model is added to the optimization problem as a constant *C*, where *C*
    is analogous to ![Understanding large margin classification](img/4351OS_06_32.jpg).
    This representation of the optimization problem can be formally expressed as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，项![理解大间隔分类](img/4351OS_06_29.jpg)是正则化参数。此外，当![理解大间隔分类](img/4351OS_06_30.jpg)时，SVM的行为更多地受到![理解大间隔分类](img/4351OS_06_15.jpg)函数的影响，反之亦然当![理解大间隔分类](img/4351OS_06_31.jpg)。在某些情况下，模型的正则化参数![理解大间隔分类](img/4351OS_06_29.jpg)作为常数*C*添加到优化问题中，其中*C*类似于![理解大间隔分类](img/4351OS_06_32.jpg)。这种优化问题的表示可以形式化地表达如下：
- en: '![Understanding large margin classification](img/4351OS_06_33.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_33.jpg)'
- en: 'As we only deal with two classes of data in which ![Understanding large margin
    classification](img/4351OS_06_09.jpg) is either 0 or 1, we can rewrite the optimization
    problem described previously, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只处理两类数据，其中![理解大间隔分类](img/4351OS_06_09.jpg)要么是0要么是1，我们可以将之前描述的优化问题重写如下：
- en: '![Understanding large margin classification](img/4351OS_06_34.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_34.jpg)'
- en: 'Let''s try to visualize the behavior of an SVM on some training data. Suppose
    we have two input variables ![Understanding large margin classification](img/4351OS_06_35.jpg)
    and ![Understanding large margin classification](img/4351OS_06_36.jpg) in our
    training data. The input values and their classes can represented by the following
    plot diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试可视化SVM在训练数据上的行为。假设我们在训练数据中有两个输入变量![理解大间隔分类](img/4351OS_06_35.jpg)和![理解大间隔分类](img/4351OS_06_36.jpg)。输入值及其类别可以用以下图示表示：
- en: '![Understanding large margin classification](img/image5.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/image5.jpg)'
- en: In the preceding plot diagram, the two classes in the training data are represented
    as circles and squares. A linear classifier will attempt to partition these sample
    values into two distinct classes and will produce a decision boundary that can
    be represented by any one of the lines in the preceding plot diagram. Of course,
    the classifier should strive to minimize the overall error of the formulated model,
    while also finding a model that generalizes the data well. An SVM will also attempt
    to partition the sample data into two classes just as any other classification
    model. However, the SVM manages to determine a hyperplane of separation that is
    observed to have the largest possible margin between the two classes of input
    data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图示中，训练数据中的两类被表示为圆圈和正方形。线性分类器将尝试将这些样本值划分为两个不同的类别，并产生一个决策边界，该边界可以由前述图示中的任意一条线表示。当然，分类器应努力最小化所构建模型的总体误差，同时找到一个很好地泛化数据的模型。SVM也会像其他分类模型一样尝试将样本数据划分为两个类别。然而，SVM设法确定了一个分离超平面，该超平面在输入数据的两个类别之间观察到具有最大的可能间隔。
- en: 'This behavior of an SVM can be illustrated using the following plot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的这种行为可以用以下图示来展示：
- en: '![Understanding large margin classification](img/4351OS_06_38_a.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_38_a.jpg)'
- en: 'As shown in the preceding plot diagram, an SVM will determine the optimal hyperplane
    that separates the two classes of data with the maximum possible margin between
    these two classes. From the optimization problem of an SVM, which we previously
    described, we can prove that the equation of the hyperplane of separation estimated
    by the SVM is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述图示所示，SVM将确定一个最优的超平面，该超平面在两类数据之间具有最大的可能间隔来分离这两类数据。从我们之前描述的SVM优化问题中，我们可以证明SVM估计的分离超平面的方程如下：
- en: '![Understanding large margin classification](img/4351OS_06_40.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_40.jpg)'
- en: Note
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in the preceding equation, the constant ![Understanding large margin
    classification](img/4351OS_06_39.jpg) is simply the y-intercept of the hyperplane.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前述方程中，常数![理解大间隔分类](img/4351OS_06_39.jpg)仅仅是超平面的y截距。
- en: 'To understand more about how an SVM achieves this large margin of separation,
    we need to use some elementary vector arithmetic. Firstly, we can define the length
    of a given vector as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解SVM如何实现这种大的间隔分离，我们需要使用一些基本的向量代数。首先，我们可以定义一个给定向量的长度如下：
- en: '![Understanding large margin classification](img/4351OS_06_41.jpg)![Understanding
    large margin classification](img/4351OS_06_42.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_41.jpg)![理解大间隔分类](img/4351OS_06_42.jpg)'
- en: 'Another operation that is often used to describe SVMs is the inner product
    of the two vectors. The inner product of two given vectors can be formally defined
    as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用来描述支持向量机（SVMs）的操作是两个向量的内积。两个给定向量的内积可以形式化定义为如下：
- en: '![Understanding large margin classification](img/4351OS_06_43.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_43.jpg)'
- en: Note
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the inner product of two vectors only exists if the two vectors are
    of the same length.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只有当两个向量长度相同时，两个向量的内积才存在。
- en: 'As shown in the preceding equation, the inner product ![Understanding large
    margin classification](img/4351OS_06_44.jpg) of the two vectors ![Understanding
    large margin classification](img/4351OS_06_45.jpg) and ![Understanding large margin
    classification](img/4351OS_06_46.jpg) is equal to the dot product of the transpose
    of ![Understanding large margin classification](img/4351OS_06_45.jpg) and the
    vector ![Understanding large margin classification](img/4351OS_06_46.jpg). Another
    way to represent the inner product of two vectors is in terms of the projection
    of one vector onto another, which is given as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述方程所示，两个向量![理解大间隔分类](img/4351OS_06_44.jpg)和![理解大间隔分类](img/4351OS_06_45.jpg)的内积等于![理解大间隔分类](img/4351OS_06_45.jpg)的转置与向量![理解大间隔分类](img/4351OS_06_46.jpg)的点积。另一种表示两个向量内积的方法是利用一个向量在另一个向量上的投影，如下所示：
- en: '![Understanding large margin classification](img/4351OS_06_47.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_47.jpg)'
- en: 'Note that the term ![Understanding large margin classification](img/4351OS_06_48.jpg)
    is equivalent to the vector product ![Understanding large margin classification](img/4351OS_06_49.jpg)
    of the vector V and the transpose of the vector U. Since the expression ![Understanding
    large margin classification](img/4351OS_06_50.jpg) is equivalent to the product
    ![Understanding large margin classification](img/4351OS_06_51.jpg) of the vectors,
    we can rewrite the optimization problem, which we described earlier in terms of
    the projection of the input variables onto the output variable. This can be formally
    expressed as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，项![理解大间隔分类](img/4351OS_06_48.jpg)等同于向量V与向量U转置的向量积![理解大间隔分类](img/4351OS_06_49.jpg)。由于表达式![理解大间隔分类](img/4351OS_06_50.jpg)等同于向量的乘积![理解大间隔分类](img/4351OS_06_51.jpg)，我们可以将我们之前用输入变量投影到输出变量描述的优化问题重新写为以下形式：
- en: '![Understanding large margin classification](img/4351OS_06_52.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_52.jpg)'
- en: 'Hence, an SVM attempts to minimize the squared sum of the elements in the parameter
    vector ![Understanding large margin classification](img/4351OS_06_04.jpg) while
    ensuring that the optimal hyperplane that separates the two classes of data is
    present in between the two planes and ![Understanding large margin classification](img/4351OS_06_53.jpg)
    and ![Understanding large margin classification](img/4351OS_06_54.jpg). These
    two planes are called the **support vectors** of the SVM. Since we must minimize
    the values of the elements in the parameter vector ![Understanding large margin
    classification](img/4351OS_06_04.jpg), the projection ![Understanding large margin
    classification](img/4351OS_06_55.jpg) must be large enough to ensure that ![Understanding
    large margin classification](img/4351OS_06_56.jpg) and ![Understanding large margin
    classification](img/4351OS_06_57.jpg):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SVM试图最小化参数向量![理解大间隔分类](img/4351OS_06_04.jpg)中元素的平方和，同时确保将两个数据类别分开的最佳超平面位于两个平面之间以及![理解大间隔分类](img/4351OS_06_53.jpg)和![理解大间隔分类](img/4351OS_06_54.jpg)。这两个平面被称为SVM的**支持向量**。由于我们必须最小化参数向量![理解大间隔分类](img/4351OS_06_04.jpg)中元素的值，因此投影![理解大间隔分类](img/4351OS_06_55.jpg)必须足够大，以确保![理解大间隔分类](img/4351OS_06_56.jpg)和![理解大间隔分类](img/4351OS_06_57.jpg)：
- en: '![Understanding large margin classification](img/4351OS_06_58.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![理解大间隔分类](img/4351OS_06_58.jpg)'
- en: Thus, the SVM will ensure that the projection of the input variable ![Understanding
    large margin classification](img/4351OS_06_08.jpg) onto the output variable ![Understanding
    large margin classification](img/4351OS_06_09.jpg) is as large as possible. This
    implies that the SVM will find the largest possible margin between the two classes
    of input values in the training data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SVM将确保输入变量![理解大间隔分类](img/4351OS_06_08.jpg)投影到输出变量![理解大间隔分类](img/4351OS_06_09.jpg)的投影尽可能大。这意味着SVM将在训练数据中找到两个输入值类别之间可能的最大间隔。
- en: Alternative forms of SVMs
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVM的替代形式
- en: We will now describe a couple of alternative forms to represent an SVM. The
    remainder of this section can be safely skipped, but the reader is advised to
    know these forms as they also widely used notations of SVMs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将描述几种替代形式来表示SVM。本节的其余部分可以安全地跳过，但建议读者了解这些形式，因为它们也是SVM广泛使用的符号。
- en: 'If ![Alternative forms of SVMs](img/4351OS_06_59.jpg) is the normal to hyperplane
    estimated by an SVM, we can represent this hyperplane of separation using the
    following equation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果![SVMs的替代形式](img/4351OS_06_59.jpg)是SVM估计的超平面的法线，我们可以用以下方程表示这个分离超平面：
- en: '![Alternative forms of SVMs](img/4351OS_06_61.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_61.jpg)'
- en: Note
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that in the preceding equation, the term ![Alternative forms of SVMs](img/4351OS_06_60.jpg)
    is the y-intercept of the hyperplane and is analogous to the term ![Alternative
    forms of SVMs](img/4351OS_06_39.jpg) in the equation of the hyperplane that we
    previously described.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的方程中，项![SVMs的替代形式](img/4351OS_06_60.jpg)是超平面的y截距，与我们之前描述的超平面方程中的项![SVMs的替代形式](img/4351OS_06_39.jpg)类似。
- en: 'The two peripheral support vectors of this hyperplane have the following equations:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个超平面的两个外围支持向量具有以下方程：
- en: '![Alternative forms of SVMs](img/4351OS_06_62.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_62.jpg)'
- en: 'We can use the expression ![Alternative forms of SVMs](img/4351OS_06_63.jpg)
    to determine the class of a given set of input values. If the value of this expression
    is less than or equal to -1, then we can say that the input values belong to one
    of the two classes of data. Similarly, if the value of the expression ![Alternative
    forms of SVMs](img/4351OS_06_63.jpg) is greater than or equal to 1, the input
    values are predicted to belong to the second class. This can be formally expressed
    as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用表达式![SVMs的替代形式](img/4351OS_06_63.jpg)来确定给定输入值集的类别。如果这个表达式的值小于或等于-1，那么我们可以说输入值属于两个数据类别之一。同样，如果表达式![SVMs的替代形式](img/4351OS_06_63.jpg)的值大于或等于1，预测输入值属于第二个类别。这可以正式表示如下：
- en: '![Alternative forms of SVMs](img/4351OS_06_64.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_64.jpg)'
- en: 'The two inequalities described in the preceding equation can be combined into
    a single inequality, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 前面方程中描述的两个不等式可以合并成一个不等式，如下所示：
- en: '![Alternative forms of SVMs](img/4351OS_06_65.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_65.jpg)'
- en: 'Thus, we can concisely rewrite the optimization problem of SVMs as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以简洁地重写SVMs的优化问题如下：
- en: '![Alternative forms of SVMs](img/4351OS_06_66.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_66.jpg)'
- en: 'In the constrained problem defined in the preceding equation, we use the normal
    ![Alternative forms of SVMs](img/4351OS_06_59.jpg) instead of the parameter vector
    ![Alternative forms of SVMs](img/4351OS_06_04.jpg) to parameterize the optimization
    problem. By using Lagrange multipliers ![Alternative forms of SVMs](img/4351OS_06_67.jpg),
    we can express the optimization problem as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面方程定义的受约束问题中，我们使用法线![SVMs的替代形式](img/4351OS_06_59.jpg)而不是参数向量![SVMs的替代形式](img/4351OS_06_04.jpg)来参数化优化问题。通过使用拉格朗日乘数![SVMs的替代形式](img/4351OS_06_67.jpg)，我们可以将优化问题表示如下：
- en: '![Alternative forms of SVMs](img/4351OS_06_68.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_68.jpg)'
- en: 'This form of the optimization problem of an SVM is known as the **primal form**.
    Note that in practice, only a few of the Lagrange multipliers will have a value
    greater than 0\. Also, this solution can be expressed as a linear combination
    of the input vectors ![Alternative forms of SVMs](img/4351OS_06_08.jpg) and the
    output variable ![Alternative forms of SVMs](img/4351OS_06_09.jpg), as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种SVM优化问题的形式被称为**原始形式**。请注意，在实践中，只有少数拉格朗日乘数将具有大于0的值。此外，这个解可以表示为输入向量![SVMs的替代形式](img/4351OS_06_08.jpg)和输出变量![SVMs的替代形式](img/4351OS_06_09.jpg)的线性组合，如下所示：
- en: '![Alternative forms of SVMs](img/4351OS_06_69.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_69.jpg)'
- en: 'We can also express the optimization problem of an SVM in the *dual form*,
    which is a constrained representation that can be described as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将SVM的优化问题表示为**对偶形式**，这是一种受约束的表示，可以描述如下：
- en: '![Alternative forms of SVMs](img/4351OS_06_70.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![SVMs的替代形式](img/4351OS_06_70.jpg)'
- en: In the constrained problem described in the preceding equation, the function
    ![Alternative forms of SVMs](img/4351OS_06_71.jpg) is called the **kernel function**
    and we will discuss more about the role of this function in SVMs in the later
    sections of this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面方程中描述的受约束问题中，函数![SVMs的替代形式](img/4351OS_06_71.jpg)被称为**核函数**，我们将在本章后面的部分讨论这个函数在SVMs中的作用。
- en: Linear classification using SVMs
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM进行线性分类
- en: As we previously described, SVMs can be used to perform linear classification
    over two distinct classes. An SVM will attempt to find a hyperplane that separates
    the two classes such that the estimated hyperplane describes the maximum achievable
    margin of separation between the two classes in our model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所描述的，SVMs 可以用于在两个不同的类别上执行线性分类。SVM 将尝试找到一个超平面来分隔这两个类别，使得估计的超平面描述了我们在模型中两个类别之间可达到的最大分离间隔。
- en: 'For example, an estimated hyperplane between two classes of data can be visualized
    using the following plot diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以使用以下图表来可视化两个数据类别的估计超平面：
- en: '![Linear classification using SVMs](img/4351OS_06_72.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![使用 SVMs 进行线性分类](img/4351OS_06_72.jpg)'
- en: As depicted in the graph shown in the preceding plot diagram, the circles and
    crosses are used to represent the two classes of input values in the sample data.
    The line represents the estimated hyperplane of an SVM.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述图表所示，圆圈和交叉用于表示样本数据中的两个类别的输入值。线代表 SVM 的估计超平面。
- en: In practice, it's often more efficient to use an implemented SVM rather than
    implement our own SVM. There are several libraries that implement SVMs that have
    been ported to multiple programming languages. One such library is **LibLinear**
    ([http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)),
    which implements a linear classifier using an SVM. The Clojure wrapper for LibLinear
    is `clj-liblinear` ([https://github.com/lynaghk/clj-liblinear](https://github.com/lynaghk/clj-liblinear))
    and we will now explore how we can use this library to easily build a linear classifier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用已实现的 SVM 而不是自己实现 SVM 通常更有效。有几个库实现了 SVM，并且已经移植到多种编程语言中。其中一个这样的库是 **LibLinear**
    ([http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/))，它使用
    SVM 实现了一个线性分类器。LibLinear 的 Clojure 封装是 `clj-liblinear` ([https://github.com/lynaghk/clj-liblinear](https://github.com/lynaghk/clj-liblinear))，我们现在将探讨如何使用这个库轻松构建一个线性分类器。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `clj-liblinear` library can be added to a Leiningen project by adding the
    following dependency to the `project.clj` file:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在 `project.clj` 文件中添加以下依赖项将 `clj-liblinear` 库添加到 Leiningen 项目中：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，命名空间声明应类似于以下声明：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Firstly, let''s generate some training data, such that we have two classes
    of input values. For this example, we will model two input variables, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们生成一些训练数据，以便我们有两个类别的输入值。在这个例子中，我们将模拟两个输入变量，如下所示：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the `repeatedly` function as shown in the preceding code, we generate
    two sequences of maps. Each map in these two sequences contains the keys `:class`
    and `:data`. The value of the `:class` key represents the class of category of
    the input values and the value of the `:data` key is itself another map with the
    keys `:x` and `:y`. The values of the keys `:x` and `:y` represent the two input
    variables in our training data. These values for the input variables are randomly
    generated using the `rand` function. The training data is generated such that
    the class of a set of input values is `0` if both the input values are positive,
    and the class of a set of input values is `1` if both the input values are negative.
    As shown in the preceding code, a total of a 1,000 samples are generated for two
    classes as two sequences using the `repeatedly` function, and then combined into
    a single sequence using the `concat` function. We can inspect some of these input
    values in the REPL, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面代码中显示的 `repeatedly` 函数，我们生成了两个映射序列。这两个序列中的每个映射都包含键 `:class` 和 `:data`。`:class`
    键的值表示输入值的类别，而 `:data` 键的值是另一个包含键 `:x` 和 `:y` 的映射。`:x` 和 `:y` 键的值代表我们训练数据中的两个输入变量。这些输入变量的值是通过使用
    `rand` 函数随机生成的。训练数据是生成的，使得一组输入值的类别为 `0`，如果两个输入值都是正数，而如果两个输入值都是负数，则一组输入值的类别为 `1`。如前所述的代码所示，使用
    `repeatedly` 函数生成了总共 1,000 个样本，分为两个类别，作为两个序列，然后使用 `concat` 函数合并成一个序列。我们可以在 REPL
    中检查这些输入值，如下所示：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can create and train an SVM using the training data we''ve generated. To
    do this, we use the `train` function. The `train` function accepts two arguments,
    which include a sequence of input values and a sequence of output values. Both
    sequences are assumed to be in the same order. For the purpose of classification,
    the output variable can be set to the class of a given set of input values as
    shown in the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们生成的训练数据创建和训练一个SVM。为此，我们使用`train`函数。`train`函数接受两个参数，包括输入值序列和输出值序列。这两个序列都假定是相同顺序的。对于分类的目的，输出变量可以设置为给定一组输入值的类别，如下所示：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `train-svm` function defined in the preceding code will instantiate and
    train an SVM with the `training-data` sequence. Now, we can use the trained SVM
    to perform classification using the `predict` function, as shown in the following
    code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中定义的`train-svm`函数将使用`training-data`序列实例化和训练一个SVM。现在，我们可以使用训练好的SVM通过`predict`函数进行分类，如下所示：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `predict` function requires two parameters, which are an instance of an
    SVM and a set of input values.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`函数需要两个参数，一个是SVM的实例，以及一组输入值。'
- en: As shown in the preceding code, we use the `svm` variable to represent a trained
    SVM. We then pass the `svm` variable to the `predict` function, along with a new
    set of input values whose class we intend to predict. It's observed that the output
    of the `predict` function agrees with the training data. Interestingly, the classifier
    predicts the class of any set of input values as `0` as long as the input value
    `:y` is positive, and conversely the class of a set of input values whose `:y`
    feature is negative is predicted as `1`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们使用`svm`变量来表示一个训练好的SVM。然后我们将`svm`变量传递给`predict`函数，同时传递一组新的输入值，这些输入值的类别是我们想要预测的。观察到`predict`函数的输出与训练数据一致。有趣的是，只要输入值`:y`为正，分类器就会预测任何一组输入值的类别为`0`；相反，如果一组输入值的`:y`特征为负，则预测为`1`。
- en: In the previous example, we used an SVM to perform classification. However,
    the output variable of the trained SVM was always a number. Thus, we could also
    use the `clj-liblinear` library in the same way as described in the preceding
    code to train a regression model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个例子中，我们使用SVM进行分类。然而，训练好的SVM的输出变量始终是一个数字。因此，我们也可以像前述代码中描述的那样使用`clj-liblinear`库来训练一个回归模型。
- en: 'The `clj-liblinear` library also supports more complex types for the features
    of an SVM, such as vectors, maps, and sets. We will now demonstrate how we can
    train a classifier that uses sets as input variables, instead of plain numbers
    as shown in the previous example. Suppose we have a stream of tweets from a given
    user''s Twitter feed. Assume that the user will manually classify these tweets
    into a specific category, which is selected from a set of predefined categories.
    This processed sequence of tweets can be represented as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`clj-liblinear`库也支持更复杂的SVM特征类型，如向量、映射和集合。现在，我们将演示如何训练一个使用集合作为输入变量的分类器，而不是像前一个例子中那样使用纯数字。假设我们有一个来自特定用户Twitter动态的推文流。假设用户将手动将这些推文分类到预定义类别中的一个。这个处理过的推文序列可以表示如下：'
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The tweets vector defined in the preceding code contains several maps, each
    of which have the keys `:class` and `:text`. The `:text` key contains the text
    of a tweet, and we will train an SVM using the value contained by the `:text`
    keyword. But we can''t use the text in verbatim, since some words might be repeated
    in a tweet. Also, we need some way of dealing with the case of the letters in
    this text. Let''s define a function to convert this text into a set as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码中定义的推文向量包含几个映射，每个映射都有`:class`和`:text`键。`:text`键包含推文文本，我们将使用`:text`键中的值来训练SVM。但是我们不能直接使用文本，因为推文中可能会有重复的单词。此外，我们还需要处理这个文本中字母的情况。让我们定义一个函数将这个文本转换为集合，如下所示：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `extract-words` function defined in the preceding code will convert any
    string, represented by the parameter `text`, into a set of words that are all
    in lower case. To create a set, we use the `(into #{})` form. By definition, this
    set will not contain any duplicate values. Note the use of the `->>` threading
    macro in the definition of the `extract-words` function.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '前述代码中定义的`extract-words`函数会将任何字符串（由参数`text`表示）转换为一系列单词，这些单词全部为小写。为了创建一个集合，我们使用`(into
    #{})`形式。根据定义，这个集合将不包含任何重复的值。注意在`extract-words`函数定义中使用了`->>`线程宏。'
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'In the `extract-words` function, the `->>` form can be equivalently written
    as `(into #{} (map lower-case (split text #" ")))`.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在`extract-words`函数中，`->>`形式可以等价地写成`(into #{} (map lower-case (split text #"
    ")))`。'
- en: 'We can inspect the behavior of the `extract-words` function in the REPL, as
    follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在REPL中检查`extract-words`函数的行为，如下所示：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using the `extract-words` function, we can effectively train an SVM with a
    set of strings as a feature variable. As we mentioned earlier, this can be done
    using the `train` function, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`extract-words`函数，我们可以有效地使用一组字符串作为特征变量来训练SVM。如我们之前提到的，这可以通过`train`函数来完成，如下所示：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `train-svm` function defined in the preceding code will create and train
    an SVM with the processed training data in the tweets variable using the `train`
    and `extract-word`s functions. We now need to compose the `predict` and `extract-words`
    functions in the following code so that we can predict the class of a given tweet:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`train-svm`函数将使用`train`和`extract-word`s函数创建并训练一个SVM，该SVM使用推文变量中的处理后的训练数据。我们现在需要在以下代码中组合`predict`和`extract-words`函数，以便我们可以预测给定推文的类别：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `predict-svm` function defined in the preceding code can be used to classify
    a given tweet. We can verify the predicted classes of the SVM for some arbitrary
    tweets in the REPL, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中定义的`predict-svm`函数可以用来对给定的推文进行分类。我们可以在REPL中验证SVM对一些任意推文的预测类别，如下所示：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In conclusion, the `clj-liblinear` library allows us to easily build and train
    an SVM with most Clojure data types. The only restriction that is imposed by this
    library is that the training data must be linearly separable into the classes
    of our model. We will study how we can build more complex classifiers in the following
    sections of this chapter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`clj-liblinear`库允许我们轻松地使用大多数Clojure数据类型构建和训练SVM。该库施加的唯一限制是训练数据必须能够线性分离成我们模型中的类别。我们将在本章的后续部分研究如何构建更复杂的分类器。
- en: Using kernel SVMs
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核SVM
- en: 'In some cases, the available training data is not linearly separable and we
    would not be able to model the data using linear classification. Thus, we need
    to use different models to fit nonlinear data. As described in [Chapter 4](ch04.html
    "Chapter 4. Building Neural Networks"), *Building Neural Networks*, ANNs can be
    used to model this kind of data. In this section, we will describe how we can
    fit an SVM on nonlinear data using kernel functions. An SVM that incorporates
    kernel function is termed as a **kernel support vector machine**. Note that, in
    this section, the terms SVM and kernel SVM are used interchangeably. A kernel
    SVM will classify data based on a nonlinear decision boundary, and the nature
    of the decision boundary depends on the kernel function that is used by the SVM.
    To illustrate this behavior, a kernel SVM will classify the training data into
    two classes as described by the following plot diagram:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，可用的训练数据不是线性可分的，我们无法使用线性分类来建模数据。因此，我们需要使用不同的模型来拟合非线性数据。如[第4章](ch04.html
    "第4章。构建神经网络")《构建神经网络》中所述，人工神经网络（ANNs）可以用来建模这类数据。在本节中，我们将描述如何使用核函数将SVM拟合到非线性数据上。包含核函数的SVM被称为**核支持向量机**。请注意，在本节中，术语SVM和核SVM是互换使用的。核SVM将根据非线性决策边界对数据进行分类，决策边界的性质取决于SVM使用的核函数。为了说明这种行为，核SVM将按照以下图示将训练数据分为两类：
- en: '![Using kernel SVMs](img/4351OS_06_73.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![使用核SVM](img/4351OS_06_73.jpg)'
- en: The concept of using kernel functions in SVMs is actually based on mathematical
    transformation. The role of the kernel function in an SVM is to transform the
    input variables in the training data such that the transformed features are linearly
    separable. Since an SVM linearly partitions the input data based on a large margin,
    this large gap of separation between the two classes of data will also be observable
    in a nonlinear space.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持向量机（SVMs）中使用核函数的概念实际上是基于数学变换的。核函数在SVM中的作用是将训练数据中的输入变量进行变换，使得变换后的特征是线性可分的。由于SVM基于大间隔线性划分输入数据，因此两个数据类别之间的这种大间隔分离在非线性空间中也将是可观察的。
- en: 'The kernel function is written as ![Using kernel SVMs](img/4351OS_06_71.jpg),
    where ![Using kernel SVMs](img/4351OS_06_08.jpg) is a vector of input values from
    the training data and ![Using kernel SVMs](img/4351OS_06_74.jpg) is the transformed
    vector of ![Using kernel SVMs](img/4351OS_06_75.jpg). The function ![Using kernel
    SVMs](img/4351OS_06_71.jpg) represents the similarity of these two vectors and
    is equivalent to the inner product of these two vectors in the transformed space.
    If the input vector ![Using kernel SVMs](img/4351OS_06_75.jpg) has a given class,
    then the class of the vector ![Using kernel SVMs](img/4351OS_06_74.jpg) is the
    same as that of the vector ![Using kernel SVMs](img/4351OS_06_75.jpg) when the
    kernel function of these two vectors has a value close to 1, that is, when ![Using
    kernel SVMs](img/4351OS_06_76.jpg). A kernel function can be mathematically expressed
    as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数表示为![使用核支持向量机](img/4351OS_06_71.jpg)，其中![使用核支持向量机](img/4351OS_06_08.jpg)是从训练数据中得到的输入值向量，![使用核支持向量机](img/4351OS_06_74.jpg)是![使用核支持向量机](img/4351OS_06_75.jpg)的转换向量。函数![使用核支持向量机](img/4351OS_06_71.jpg)表示这两个向量的相似性，并且等同于转换空间中这两个向量的内积。如果输入向量![使用核支持向量机](img/4351OS_06_75.jpg)具有给定的类别，那么当这两个向量的核函数值接近1时，即![使用核支持向量机](img/4351OS_06_76.jpg)时，向量![使用核支持向量机](img/4351OS_06_74.jpg)的类别与向量![使用核支持向量机](img/4351OS_06_75.jpg)的类别相同。核函数可以用以下数学表达式表示：
- en: '![Using kernel SVMs](img/4351OS_06_77.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![使用核支持向量机](img/4351OS_06_77.jpg)'
- en: In the preceding equation, the function ![Using kernel SVMs](img/4351OS_06_78.jpg)
    performs the transformation from a nonlinear space ![Using kernel SVMs](img/4351OS_06_79.jpg)
    into a linear space ![Using kernel SVMs](img/4351OS_06_46.jpg). Note that the
    explicit representation of ![Using kernel SVMs](img/4351OS_06_78.jpg) is not required,
    and it's enough to know that ![Using kernel SVMs](img/4351OS_06_46.jpg) is an
    inner product space. Although we are free to choose any arbitrary kernel function
    to model the given training data, we must strive to reduce the problem of minimizing
    the cost function of the formulated SVM model. Thus, the kernel function is generally
    selected such that calculating the SVM's decision boundary only requires determining
    the dot products of vectors in the transformed feature space ![Using kernel SVMs](img/4351OS_06_46.jpg).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，函数![使用核支持向量机](img/4351OS_06_78.jpg)执行从非线性空间![使用核支持向量机](img/4351OS_06_79.jpg)到线性空间![使用核支持向量机](img/4351OS_06_46.jpg)的转换。请注意，![使用核支持向量机](img/4351OS_06_78.jpg)的显式表示不是必需的，只需知道![使用核支持向量机](img/4351OS_06_46.jpg)是一个内积空间即可。虽然我们可以自由选择任何任意的核函数来建模给定的训练数据，但我们必须努力减少最小化所构建SVM模型成本函数的问题。因此，核函数通常被选择，使得计算SVM的决策边界只需要确定转换特征空间![使用核支持向量机](img/4351OS_06_46.jpg)中向量的点积。
- en: A common choice for the kernel function of an SVM is the **polynomial kernel
    function**, also called the **polynomic kernel function**, which models the training
    data as polynomials of the original feature variables. As the reader may recall
    from [Chapter 5](ch05.html "Chapter 5. Selecting and Evaluating Data"), *Selecting
    and Evaluating Data*, we have discussed how polynomial features can greatly improve
    the performance of a given machine learning model. The polynomial kernel function
    can be thought of as an extension of this concept that applies to SVMs. This function
    can be formally expressed as follows.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的核函数的一个常见选择是**多项式核函数**，也称为**多项式核函数**，它将训练数据建模为原始特征变量的多项式。读者可能还记得[第5章](ch05.html
    "第5章. 选择和评估数据")中关于*选择和评估数据*的讨论，我们讨论了多项式特征如何极大地提高给定机器学习模型的性能。多项式核函数可以被视为这一概念的扩展，适用于SVM。该函数可以形式化地表示如下。
- en: '![Using kernel SVMs](img/4351OS_06_80.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![使用核支持向量机](img/4351OS_06_80.jpg)'
- en: In the preceding equation, the term ![Using kernel SVMs](img/4351OS_06_81.jpg)
    represents the highest degree of the polynomial features. Also, when (the constant)
    ![Using kernel SVMs](img/4351OS_06_82.jpg), the kernel is termed to be **homogenous**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，术语![使用核支持向量机](img/4351OS_06_81.jpg)代表多项式特征的最高次数。此外，当（常数）![使用核支持向量机](img/4351OS_06_82.jpg)时，核被称作**同质的**。
- en: Another widely used kernel function is the **Gaussian kernel function**. Most
    readers who are adept in linear algebra will need no introduction to the Gaussian
    function. It's important to know that this function represents a normal distribution
    of data in which the data points are closer to the mean of the data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的核函数是**高斯核函数**。大多数熟悉线性代数的读者对高斯函数都不陌生。重要的是要知道，这个函数表示数据点的正态分布，其中数据点更接近数据的均值。
- en: 'In the context of SVMs, the Gaussian kernel function can be used to represent
    a model in which one of the two classes in the training data has values for the
    input variables that are close to an arbitrary mean value. The Gaussian kernel
    function can be formally expressed as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM的背景下，高斯核函数可以用来表示一个模型，其中训练数据中的两个类别之一在输入变量上的值接近任意均值。高斯核函数可以形式地表示如下：
- en: '![Using kernel SVMs](img/4351OS_06_83.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![使用核SVM](img/4351OS_06_83.jpg)'
- en: In the Gaussian kernel function defined in the preceding equation, the term
    ![Using kernel SVMs](img/4351OS_06_84.jpg) represents the variance of the training
    data and represents the *width* of the Gaussian kernel.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面方程定义的高斯核函数中，术语![使用核SVM](img/4351OS_06_84.jpg)表示训练数据的方差，并代表高斯核的**宽度**。
- en: Another popular choice for the kernel function is the **string kernel function**
    that operates on string values. By the term *string*, we mean a finite sequence
    of symbols. The string kernel function essentially measures the similarity between
    two given strings. If both the strings passed to the string kernel function are
    the same, the value returned by this function will be `1`. Thus, the string kernel
    function is useful in modeling data where the features are represented as strings.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数的另一个流行选择是**字符串核函数**，它作用于字符串值。术语“字符串”指的是符号的有限序列。字符串核函数本质上衡量两个给定字符串之间的相似度。如果传递给字符串核函数的两个字符串相同，该函数返回的值将是`1`。因此，字符串核函数在将特征表示为字符串的数据建模中非常有用。
- en: Sequential minimal optimization
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列最小优化
- en: 'The optimization problem of an SVM can be solved using **Sequential Minimal
    Optimization** (**SMO**). The optimization problem of an SVM is the numerical
    optimization of the cost function across several dimensions in order to reduce
    the overall error of the trained SVM. In practice, this must be done through numerical
    optimization techniques. A complete discussion of the SMO algorithm is beyond
    the scope of this book. However, we must note that this algorithm solves the optimization
    problem by a *divide-and-conquer* technique. Essentially, SMO divides the optimization
    problem of multiple dimensions into several smaller two-dimensional problems that
    can be solved analytically (for more information, refer to *Sequential Minimal
    Optimization: A Fast Algorithm for Training Support Vector Machines*).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的优化问题可以使用**序列最小优化**（**SMO**）来解决。SVM的优化问题是跨多个维度的成本函数的数值优化，以减少训练SVM的整体误差。在实践中，这必须通过数值优化技术来完成。SMO算法的完整讨论超出了本书的范围。然而，我们必须注意，该算法通过一种*分而治之*的技术来解决优化问题。本质上，SMO将多个维度的优化问题分解为几个可以解析解决的较小的二维问题（更多信息，请参阅*序列最小优化：训练支持向量机的一种快速算法*）。
- en: '**LibSVM** is a popular library that implements SMO to train an SVM. The `svm-clj`
    library is a Clojure wrapper for LibSVM and we will now explore how we can use
    this library to formulate an SVM model.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**LibSVM**是一个流行的库，它实现了SMO来训练SVM。`svm-clj`库是LibSVM的Clojure包装器，我们将现在探讨如何使用这个库来构建SVM模型。'
- en: Note
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `svm-clj` library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`project.clj`文件中添加以下依赖项将`svm-clj`库添加到Leiningen项目中：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，命名空间声明应类似于以下声明：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This example will use a simplified version of the **SPECT Heart** dataset ([http://archive.ics.uci.edu/ml/datasets/SPECT+Heart](http://archive.ics.uci.edu/ml/datasets/SPECT+Heart)).
    This dataset describes the diagnosis of several heart disease patients using **Single
    Proton Emission Computed Tomography** (**SPECT**) images. The original dataset
    contains a total of 267 samples, in which each sample has 23 features. The output
    variable of the dataset describes a positive or negative diagnosis of a given
    patient, which is represented using either +1 or -1, respectively.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例将使用**SPECT Heart**数据集的简化版本（[http://archive.ics.uci.edu/ml/datasets/SPECT+Heart](http://archive.ics.uci.edu/ml/datasets/SPECT+Heart)）。此数据集描述了使用**单光子发射计算机断层扫描**（**SPECT**）图像对几位心脏病患者的诊断。原始数据集包含总共267个样本，其中每个样本有23个特征。数据集的输出变量描述了给定患者的阳性或阴性诊断，分别用+1或-1表示。
- en: 'For this example, the training data is stored in a file named `features.dat`.
    This file must be placed in the `resources/` directory of the Leiningen project
    to make it available for use. This file contains several input features and the
    class of these input values. Let''s have a look at one of the following sample
    values in this file:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此示例，训练数据存储在一个名为`features.dat`的文件中。此文件必须放置在Leiningen项目的`resources/`目录中，以便可供使用。此文件包含几个输入特征和这些输入值的类别。让我们看一下文件中的以下样本值之一：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As shown in the preceding line of code, the first value `+1` denotes the class
    of the sample and the other values represent the input variables. Note that the
    indexes of the input variables are also given. Also, the value of the first feature
    in the preceding sample is `0`, as it is not mentioned using a `1:` key. From
    the preceding line, it's clear that each sample will have a maximum of 12 features.
    All sample values must conform to this format, as dictated by LibSVM.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码行所示，第一个值`+1`表示样本的类别，其他值表示输入变量。请注意，输入变量的索引也给出了。此外，前述样本中第一个特征的值是`0`，因为它没有使用`1:`键提及。从前述行中可以清楚地看出，每个样本将最多有12个特征。所有样本值都必须符合LibSVM规定的此格式。
- en: 'We can train an SVM using this sample data. To do this, we use the `train-model`
    function from the `svm-clj` library. Also, since we must first load the sample
    data from the file, we will need to first call the `read-dataset` function as
    well using the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些样本数据训练一个SVM。为此，我们使用`svm-clj`库中的`train-model`函数。此外，由于我们必须首先从文件中加载样本数据，我们还需要首先使用以下代码调用`read-dataset`函数：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The trained SVM represented by the model variable as defined in the preceding
    code can now be used to predict the class of a set of input values. The `predict`
    function can be used for this purpose. For simplicity, we will use a sample value
    from the dataset variable itself as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码中定义的模型变量所表示的训练好的SVM现在可以用来预测一组输入值的类别。`predict`函数可用于此目的。为了简单起见，我们将使用数据集变量本身的样本值如下：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As shown in the REPL output in the preceding code, `dataset` can be treated
    as a sequence of maps. Each map contains a single key that represents the value
    of the output variable in the sample. The value of this key in the `dataset` map
    is another map that represents the input variables of the given sample. Since
    the `feature` variable represents a map, we can call it as a function, as shown
    by the `(feature 1)` call in the preceding code.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码中的REPL输出所示，`dataset`可以被视为一系列的映射。每个映射包含一个代表样本中输出变量值的单个键。`dataset`映射中此键的值是另一个映射，它表示给定样本的输入变量。由于`feature`变量代表一个映射，我们可以将其作为函数调用，如前述代码中的`(feature
    1)`调用所示。
- en: The predicted value agrees with the actual value of the output variable, or
    the class, of a given set of input values. In conclusion, the `svm-clj` library
    provides us with a simple and concise implementation of an SVM.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值与给定一组输入值的输出变量实际值或类别相一致。总之，`svm-clj`库为我们提供了一个简单且简洁的SVM实现。
- en: Using kernel functions
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用核函数
- en: As we have mentioned earlier, we can choose a kernel function for an SVM when
    we need to fit some nonlinear data. We will now demonstrate how this is achieved
    in practice using the `clj-ml` library. Since this library has already been discussed
    in the previous chapters, we will not focus on the complete training of an SVM,
    but rather on how we can create an SVM that uses kernel functions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，当我们需要拟合一些非线性数据时，我们可以为SVM选择一个核函数。现在，我们将通过使用`clj-ml`库来展示如何在实践中实现这一点。由于这个库已经在之前的章节中讨论过，我们将不会关注SVM的完整训练过程，而是关注如何创建使用核函数的SVM。
- en: Note
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的示例，命名空间声明应类似于以下声明：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The function, `make-kernel-function`, from the `clj-ml.kernel-functions` namespace
    is used to create kernel functions that can be used for SVMs. For example, we
    can create a polynomial kernel function by passing the :`polynomic` keyword to
    this function, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`clj-ml.kernel-functions`命名空间的`make-kernel-function`函数用于创建可用于SVMs的核函数。例如，我们可以通过将`:polynomic`关键字传递给此函数来创建一个多项式核函数，如下所示：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As shown in the preceding line, the polynomial kernel function defined by the
    variable `K` has a polynomial degree of `3`. Similarly, we can also create a string
    kernel function using the `:string` keyword, as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一行所示，由变量`K`定义的多项式核函数具有多项式次数`3`。同样，我们也可以使用`:string`关键字创建一个字符串核函数，如下所示：
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'There are several such kernel functions available in the `clj-ml` library and
    the reader is encouraged to explore more kernel functions in this library. The
    documentation for this namespace is available at [http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html](http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html).
    We can create an SVM using the `make-classifier` function by specifying the `:support-vector-machine`
    and `:smo` keywords; and the kernel function with the keyword option `:kernel-function`,
    as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在`clj-ml`库中存在几种这样的核函数，鼓励读者探索这个库中更多的核函数。该命名空间文档可在[http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html](http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html)找到。我们可以通过指定`:support-vector-machine`和`:smo`关键字以及使用`:kernel-function`关键字选项来创建一个SVM，如下所示：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can now train the SVM represented by the variable classifier as we have done
    in the previous chapters. The `clj-ml` library, thus, allows us to create SVMs
    that exhibit a given kernel function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以像之前章节中做的那样，训练由变量classifier表示的SVM。因此，`clj-ml`库允许我们创建具有给定核函数的SVM。
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have explored SVMs and how they can be used to fit both
    linear and nonlinear data. The following are the other topics that we have covered:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了SVMs及其如何用于拟合线性和非线性数据。以下是我们已经涵盖的其他主题：
- en: We have examined how SVMs are capable of large margin classification and the
    various forms of the optimization problem of SVMs
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经探讨了支持向量机（SVMs）如何实现大间隔分类以及SVMs的优化问题各种形式
- en: We have discussed how we can use kernel functions and SMO to train an SVM with
    nonlinear sample data
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何使用核函数和SMO来训练非线性样本数据的SVM
- en: We have also demonstrated how we can use several Clojure libraries to build
    and train SVMs
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还展示了如何使用几个Clojure库来构建和训练SVMs
- en: We will shift our focus to unsupervised learning in the next chapter and we
    will explore clustering techniques to model these types of machine learning problems.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把重点转向无监督学习，并探讨聚类技术来模拟这些类型的机器学习问题。
