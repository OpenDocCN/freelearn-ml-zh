- en: Chapter 6. Building Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore **Support Vector Machines** (**SVMs**). We
    will study several SVM implementations in Clojure that can be used to build and
    train an SVM using some given training data.
  prefs: []
  type: TYPE_NORMAL
- en: SVMs are supervised learning models that are used for both regression and classification.
    In this chapter, however, we will focus on the problem of classification within
    the context of SVMs. SVMs find applications in text mining, chemical classification,
    and image and handwriting recognition. Of course, we should not overlook the fact
    that the overall performance of a machine learning model mostly depends on the
    amount and nature of the training data and is also affected by which machine learning
    model we use to model the available data.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest form, an SVM separates and predicts two classes of data by estimating
    the optimal vector plane or **hyperplane** between these two classes represented
    in vector space. A **hyperplane** can be simply defined as a plane that has one
    less dimension than the ambient space. For a three-dimensional space, we would
    obtain a two-dimensional hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: A basic SVM is a non-probabilistic binary classifier that uses linear classification.
    In addition to linear classification, SVMs can also be used to perform nonlinear
    classification over several classes. An interesting aspect of SVMs is that the
    estimated vector plane will have a substantially large and distinct gap between
    the classes of input values. Due to this, SVMs often have a good generalization
    performance and also implement a kind of automatic complexity control to avoid
    overfitting. Hence, SVMs are also called **large margin classifiers**. In this
    chapter, we will also study how SVMs achieve this large margin between classes
    of input data, when compared to other classifiers. Another interesting fact about
    SVMs is that they scale very well with the number of features being modeled and
    thus, SVMs are often used in machine learning problems that deal with a large
    number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding large margin classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we previously mentioned, SVMs classify input data across large margins. Let's
    examine how this is achieved. We use our definition of a logistic classification
    model, which we previously described in [Chapter 3](ch03.html "Chapter 3. Categorizing
    Data"), *Categorizing Data*, as a basis for reasoning about SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the logistic or *sigmoid* function to separate two classes of input
    values, as we described in [Chapter 3](ch03.html "Chapter 3. Categorizing Data"),
    *Categorizing Data*. This function can be formally defined as a function of an
    input variable *X* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the output variable ![Understanding large margin
    classification](img/4351OS_06_02.jpg) depends not only on the variable ![Understanding
    large margin classification](img/4351OS_06_03.jpg), but also on the coefficient
    ![Understanding large margin classification](img/4351OS_06_04.jpg). The variable
    ![Understanding large margin classification](img/4351OS_06_03.jpg) is analogous
    to the vector of input values in our model, and the term ![Understanding large
    margin classification](img/4351OS_06_04.jpg) is the parameter vector of the model.
    For binary classification, the value of *Y* must exist in the range of 0 and 1\.
    Also, the class of a set of input values is determined by whether the output variable
    ![Understanding large margin classification](img/4351OS_06_02.jpg) is closer to
    0 or 1\. For these values of *Y*, the term ![Understanding large margin classification](img/4351OS_06_05.jpg)
    is either much greater than or much less than 0\. This can be formally expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For ![Understanding large margin classification](img/4351OS_06_07.jpg) sample
    with input values ![Understanding large margin classification](img/4351OS_06_08.jpg)
    and output values ![Understanding large margin classification](img/4351OS_06_09.jpg),
    we define the cost function ![Understanding large margin classification](img/4351OS_06_10.jpg)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the term ![Understanding large margin classification](img/4351OS_06_11.jpg)
    represents the output variable calculated from the estimated model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a logistic classification model, ![Understanding large margin classification](img/4351OS_06_11.jpg)
    is the value of logistic function when applied to a set of input values ![Understanding
    large margin classification](img/4351OS_06_08.jpg). We can simplify and expand
    the summation term ![Understanding large margin classification](img/4351OS_06_13.jpg)
    in the cost function defined in the preceding equation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s obvious that the cost function shown in the preceding expression depends
    on the two logarithmic terms in the expression. Thus, we can represent the cost
    function as a function of these two logarithmic terms, represented by the terms
    ![Understanding large margin classification](img/4351OS_06_15.jpg) and ![Understanding
    large margin classification](img/4351OS_06_16.jpg). Now, let''s assume the two
    terms as shown the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Both the functions ![Understanding large margin classification](img/4351OS_06_15.jpg)
    and ![Understanding large margin classification](img/4351OS_06_16.jpg) are composed
    using the logistic function. A classifier that models the logistic function must
    be trained such that these two functions are minimized over all possible values
    of the parameter vector ![Understanding large margin classification](img/4351OS_06_04.jpg).
    We can use the **hinge-loss** function to approximate the desired behavior of
    a linear classifier that uses the logistic function (for more information, refer
    to "Are Loss Functions All the Same?"). We will now study the hinge-loss function
    by comparing it to the logistic function. The following diagram depicts how the
    ![Understanding large margin classification](img/4351OS_06_15.jpg) function must
    vary with respect to the term ![Understanding large margin classification](img/4351OS_06_18.jpg)
    and how it can be modeled using the logistic and hinge-loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/image1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the plot shown in the preceding diagram, the logistic function is represented
    as a smooth curve. The function is seen to decrease rapidly before a given point
    and then decreases at a lower rate. In this example, the point at which this change
    of rate of the logistic function occurs is found to be *x = 0*. The hinge-loss
    function approximates this by using two line segments that meet at the point *x
    = 0*. Interestingly, both these functions model a behavior that changes at a rate
    that is inversely proportional to the input value *x*. Similarly, we can approximate
    the effect of the ![Understanding large margin classification](img/4351OS_06_16.jpg)
    function using the hinge-loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/image2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the ![Understanding large margin classification](img/4351OS_06_16.jpg)
    function is directly proportional to the term ![Understanding large margin classification](img/4351OS_06_18.jpg).
    Thus, we can achieve the classification ability of the logistic function by modelling
    the hinge-loss function and a classifier built using the hinge-loss function will
    perform equally well as a classifier using the logistic function.
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding diagram, the hinge-loss function only changes its value
    at the point ![Understanding large margin classification](img/4351OS_06_21.jpg).
    This applies to both the functions ![Understanding large margin classification](img/4351OS_06_15.jpg)
    and ![Understanding large margin classification](img/4351OS_06_16.jpg). Thus,
    we can use the hinge loss function to separate two classes of data depending on
    whether the value of ![Understanding large margin classification](img/4351OS_06_23.jpg)
    is greater or less than 0\. In this case, there's virtually no margin of separation
    between these two classes. To improve the margin of classification, we can modify
    the hinge-loss function such that its value is greater than 0 only when ![Understanding
    large margin classification](img/4351OS_06_24.jpg) or ![Understanding large margin
    classification](img/4351OS_06_25.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified hinge-loss functions can be plotted for the two classes of data
    as follows. The following plot describes the case where ![Understanding large
    margin classification](img/4351OS_06_25.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/image3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the modified hinge-loss function for the case ![Understanding large
    margin classification](img/4351OS_06_24.jpg) can be illustrated by the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/image4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the *hinge* occurs at *-1* in the case of ![Understanding large margin
    classification](img/4351OS_06_24.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we substitute the hinge-loss functions in place of the ![Understanding large
    margin classification](img/4351OS_06_15.jpg) and ![Understanding large margin
    classification](img/4351OS_06_16.jpg) functions, we arrive at an optimization
    problem of SVMs (for more information, refer to "Support-vector networks"), which
    can be formally written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the term ![Understanding large margin classification](img/4351OS_06_29.jpg)
    is the regularization parameter. Also, when ![Understanding large margin classification](img/4351OS_06_30.jpg),
    the behavior of the SVM is affected more by the ![Understanding large margin classification](img/4351OS_06_15.jpg)
    function than the ![Understanding large margin classification](img/4351OS_06_16.jpg)
    function, and vice versa when ![Understanding large margin classification](img/4351OS_06_31.jpg).
    In some contexts, the regularization parameter ![Understanding large margin classification](img/4351OS_06_29.jpg)
    of the model is added to the optimization problem as a constant *C*, where *C*
    is analogous to ![Understanding large margin classification](img/4351OS_06_32.jpg).
    This representation of the optimization problem can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we only deal with two classes of data in which ![Understanding large margin
    classification](img/4351OS_06_09.jpg) is either 0 or 1, we can rewrite the optimization
    problem described previously, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to visualize the behavior of an SVM on some training data. Suppose
    we have two input variables ![Understanding large margin classification](img/4351OS_06_35.jpg)
    and ![Understanding large margin classification](img/4351OS_06_36.jpg) in our
    training data. The input values and their classes can represented by the following
    plot diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/image5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding plot diagram, the two classes in the training data are represented
    as circles and squares. A linear classifier will attempt to partition these sample
    values into two distinct classes and will produce a decision boundary that can
    be represented by any one of the lines in the preceding plot diagram. Of course,
    the classifier should strive to minimize the overall error of the formulated model,
    while also finding a model that generalizes the data well. An SVM will also attempt
    to partition the sample data into two classes just as any other classification
    model. However, the SVM manages to determine a hyperplane of separation that is
    observed to have the largest possible margin between the two classes of input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This behavior of an SVM can be illustrated using the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_38_a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding plot diagram, an SVM will determine the optimal hyperplane
    that separates the two classes of data with the maximum possible margin between
    these two classes. From the optimization problem of an SVM, which we previously
    described, we can prove that the equation of the hyperplane of separation estimated
    by the SVM is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that in the preceding equation, the constant ![Understanding large margin
    classification](img/4351OS_06_39.jpg) is simply the y-intercept of the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand more about how an SVM achieves this large margin of separation,
    we need to use some elementary vector arithmetic. Firstly, we can define the length
    of a given vector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_41.jpg)![Understanding
    large margin classification](img/4351OS_06_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another operation that is often used to describe SVMs is the inner product
    of the two vectors. The inner product of two given vectors can be formally defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the inner product of two vectors only exists if the two vectors are
    of the same length.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding equation, the inner product ![Understanding large
    margin classification](img/4351OS_06_44.jpg) of the two vectors ![Understanding
    large margin classification](img/4351OS_06_45.jpg) and ![Understanding large margin
    classification](img/4351OS_06_46.jpg) is equal to the dot product of the transpose
    of ![Understanding large margin classification](img/4351OS_06_45.jpg) and the
    vector ![Understanding large margin classification](img/4351OS_06_46.jpg). Another
    way to represent the inner product of two vectors is in terms of the projection
    of one vector onto another, which is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the term ![Understanding large margin classification](img/4351OS_06_48.jpg)
    is equivalent to the vector product ![Understanding large margin classification](img/4351OS_06_49.jpg)
    of the vector V and the transpose of the vector U. Since the expression ![Understanding
    large margin classification](img/4351OS_06_50.jpg) is equivalent to the product
    ![Understanding large margin classification](img/4351OS_06_51.jpg) of the vectors,
    we can rewrite the optimization problem, which we described earlier in terms of
    the projection of the input variables onto the output variable. This can be formally
    expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, an SVM attempts to minimize the squared sum of the elements in the parameter
    vector ![Understanding large margin classification](img/4351OS_06_04.jpg) while
    ensuring that the optimal hyperplane that separates the two classes of data is
    present in between the two planes and ![Understanding large margin classification](img/4351OS_06_53.jpg)
    and ![Understanding large margin classification](img/4351OS_06_54.jpg). These
    two planes are called the **support vectors** of the SVM. Since we must minimize
    the values of the elements in the parameter vector ![Understanding large margin
    classification](img/4351OS_06_04.jpg), the projection ![Understanding large margin
    classification](img/4351OS_06_55.jpg) must be large enough to ensure that ![Understanding
    large margin classification](img/4351OS_06_56.jpg) and ![Understanding large margin
    classification](img/4351OS_06_57.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding large margin classification](img/4351OS_06_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the SVM will ensure that the projection of the input variable ![Understanding
    large margin classification](img/4351OS_06_08.jpg) onto the output variable ![Understanding
    large margin classification](img/4351OS_06_09.jpg) is as large as possible. This
    implies that the SVM will find the largest possible margin between the two classes
    of input values in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative forms of SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now describe a couple of alternative forms to represent an SVM. The
    remainder of this section can be safely skipped, but the reader is advised to
    know these forms as they also widely used notations of SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![Alternative forms of SVMs](img/4351OS_06_59.jpg) is the normal to hyperplane
    estimated by an SVM, we can represent this hyperplane of separation using the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that in the preceding equation, the term ![Alternative forms of SVMs](img/4351OS_06_60.jpg)
    is the y-intercept of the hyperplane and is analogous to the term ![Alternative
    forms of SVMs](img/4351OS_06_39.jpg) in the equation of the hyperplane that we
    previously described.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two peripheral support vectors of this hyperplane have the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the expression ![Alternative forms of SVMs](img/4351OS_06_63.jpg)
    to determine the class of a given set of input values. If the value of this expression
    is less than or equal to -1, then we can say that the input values belong to one
    of the two classes of data. Similarly, if the value of the expression ![Alternative
    forms of SVMs](img/4351OS_06_63.jpg) is greater than or equal to 1, the input
    values are predicted to belong to the second class. This can be formally expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_64.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The two inequalities described in the preceding equation can be combined into
    a single inequality, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_65.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can concisely rewrite the optimization problem of SVMs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_66.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the constrained problem defined in the preceding equation, we use the normal
    ![Alternative forms of SVMs](img/4351OS_06_59.jpg) instead of the parameter vector
    ![Alternative forms of SVMs](img/4351OS_06_04.jpg) to parameterize the optimization
    problem. By using Lagrange multipliers ![Alternative forms of SVMs](img/4351OS_06_67.jpg),
    we can express the optimization problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_68.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This form of the optimization problem of an SVM is known as the **primal form**.
    Note that in practice, only a few of the Lagrange multipliers will have a value
    greater than 0\. Also, this solution can be expressed as a linear combination
    of the input vectors ![Alternative forms of SVMs](img/4351OS_06_08.jpg) and the
    output variable ![Alternative forms of SVMs](img/4351OS_06_09.jpg), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also express the optimization problem of an SVM in the *dual form*,
    which is a constrained representation that can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alternative forms of SVMs](img/4351OS_06_70.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the constrained problem described in the preceding equation, the function
    ![Alternative forms of SVMs](img/4351OS_06_71.jpg) is called the **kernel function**
    and we will discuss more about the role of this function in SVMs in the later
    sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification using SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we previously described, SVMs can be used to perform linear classification
    over two distinct classes. An SVM will attempt to find a hyperplane that separates
    the two classes such that the estimated hyperplane describes the maximum achievable
    margin of separation between the two classes in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an estimated hyperplane between two classes of data can be visualized
    using the following plot diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear classification using SVMs](img/4351OS_06_72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As depicted in the graph shown in the preceding plot diagram, the circles and
    crosses are used to represent the two classes of input values in the sample data.
    The line represents the estimated hyperplane of an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it's often more efficient to use an implemented SVM rather than
    implement our own SVM. There are several libraries that implement SVMs that have
    been ported to multiple programming languages. One such library is **LibLinear**
    ([http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)),
    which implements a linear classifier using an SVM. The Clojure wrapper for LibLinear
    is `clj-liblinear` ([https://github.com/lynaghk/clj-liblinear](https://github.com/lynaghk/clj-liblinear))
    and we will now explore how we can use this library to easily build a linear classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `clj-liblinear` library can be added to a Leiningen project by adding the
    following dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Firstly, let''s generate some training data, such that we have two classes
    of input values. For this example, we will model two input variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `repeatedly` function as shown in the preceding code, we generate
    two sequences of maps. Each map in these two sequences contains the keys `:class`
    and `:data`. The value of the `:class` key represents the class of category of
    the input values and the value of the `:data` key is itself another map with the
    keys `:x` and `:y`. The values of the keys `:x` and `:y` represent the two input
    variables in our training data. These values for the input variables are randomly
    generated using the `rand` function. The training data is generated such that
    the class of a set of input values is `0` if both the input values are positive,
    and the class of a set of input values is `1` if both the input values are negative.
    As shown in the preceding code, a total of a 1,000 samples are generated for two
    classes as two sequences using the `repeatedly` function, and then combined into
    a single sequence using the `concat` function. We can inspect some of these input
    values in the REPL, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create and train an SVM using the training data we''ve generated. To
    do this, we use the `train` function. The `train` function accepts two arguments,
    which include a sequence of input values and a sequence of output values. Both
    sequences are assumed to be in the same order. For the purpose of classification,
    the output variable can be set to the class of a given set of input values as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train-svm` function defined in the preceding code will instantiate and
    train an SVM with the `training-data` sequence. Now, we can use the trained SVM
    to perform classification using the `predict` function, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `predict` function requires two parameters, which are an instance of an
    SVM and a set of input values.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding code, we use the `svm` variable to represent a trained
    SVM. We then pass the `svm` variable to the `predict` function, along with a new
    set of input values whose class we intend to predict. It's observed that the output
    of the `predict` function agrees with the training data. Interestingly, the classifier
    predicts the class of any set of input values as `0` as long as the input value
    `:y` is positive, and conversely the class of a set of input values whose `:y`
    feature is negative is predicted as `1`.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we used an SVM to perform classification. However,
    the output variable of the trained SVM was always a number. Thus, we could also
    use the `clj-liblinear` library in the same way as described in the preceding
    code to train a regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `clj-liblinear` library also supports more complex types for the features
    of an SVM, such as vectors, maps, and sets. We will now demonstrate how we can
    train a classifier that uses sets as input variables, instead of plain numbers
    as shown in the previous example. Suppose we have a stream of tweets from a given
    user''s Twitter feed. Assume that the user will manually classify these tweets
    into a specific category, which is selected from a set of predefined categories.
    This processed sequence of tweets can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The tweets vector defined in the preceding code contains several maps, each
    of which have the keys `:class` and `:text`. The `:text` key contains the text
    of a tweet, and we will train an SVM using the value contained by the `:text`
    keyword. But we can''t use the text in verbatim, since some words might be repeated
    in a tweet. Also, we need some way of dealing with the case of the letters in
    this text. Let''s define a function to convert this text into a set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `extract-words` function defined in the preceding code will convert any
    string, represented by the parameter `text`, into a set of words that are all
    in lower case. To create a set, we use the `(into #{})` form. By definition, this
    set will not contain any duplicate values. Note the use of the `->>` threading
    macro in the definition of the `extract-words` function.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the `extract-words` function, the `->>` form can be equivalently written
    as `(into #{} (map lower-case (split text #" ")))`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can inspect the behavior of the `extract-words` function in the REPL, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `extract-words` function, we can effectively train an SVM with a
    set of strings as a feature variable. As we mentioned earlier, this can be done
    using the `train` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train-svm` function defined in the preceding code will create and train
    an SVM with the processed training data in the tweets variable using the `train`
    and `extract-word`s functions. We now need to compose the `predict` and `extract-words`
    functions in the following code so that we can predict the class of a given tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict-svm` function defined in the preceding code can be used to classify
    a given tweet. We can verify the predicted classes of the SVM for some arbitrary
    tweets in the REPL, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, the `clj-liblinear` library allows us to easily build and train
    an SVM with most Clojure data types. The only restriction that is imposed by this
    library is that the training data must be linearly separable into the classes
    of our model. We will study how we can build more complex classifiers in the following
    sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using kernel SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In some cases, the available training data is not linearly separable and we
    would not be able to model the data using linear classification. Thus, we need
    to use different models to fit nonlinear data. As described in [Chapter 4](ch04.html
    "Chapter 4. Building Neural Networks"), *Building Neural Networks*, ANNs can be
    used to model this kind of data. In this section, we will describe how we can
    fit an SVM on nonlinear data using kernel functions. An SVM that incorporates
    kernel function is termed as a **kernel support vector machine**. Note that, in
    this section, the terms SVM and kernel SVM are used interchangeably. A kernel
    SVM will classify data based on a nonlinear decision boundary, and the nature
    of the decision boundary depends on the kernel function that is used by the SVM.
    To illustrate this behavior, a kernel SVM will classify the training data into
    two classes as described by the following plot diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernel SVMs](img/4351OS_06_73.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The concept of using kernel functions in SVMs is actually based on mathematical
    transformation. The role of the kernel function in an SVM is to transform the
    input variables in the training data such that the transformed features are linearly
    separable. Since an SVM linearly partitions the input data based on a large margin,
    this large gap of separation between the two classes of data will also be observable
    in a nonlinear space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel function is written as ![Using kernel SVMs](img/4351OS_06_71.jpg),
    where ![Using kernel SVMs](img/4351OS_06_08.jpg) is a vector of input values from
    the training data and ![Using kernel SVMs](img/4351OS_06_74.jpg) is the transformed
    vector of ![Using kernel SVMs](img/4351OS_06_75.jpg). The function ![Using kernel
    SVMs](img/4351OS_06_71.jpg) represents the similarity of these two vectors and
    is equivalent to the inner product of these two vectors in the transformed space.
    If the input vector ![Using kernel SVMs](img/4351OS_06_75.jpg) has a given class,
    then the class of the vector ![Using kernel SVMs](img/4351OS_06_74.jpg) is the
    same as that of the vector ![Using kernel SVMs](img/4351OS_06_75.jpg) when the
    kernel function of these two vectors has a value close to 1, that is, when ![Using
    kernel SVMs](img/4351OS_06_76.jpg). A kernel function can be mathematically expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernel SVMs](img/4351OS_06_77.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the function ![Using kernel SVMs](img/4351OS_06_78.jpg)
    performs the transformation from a nonlinear space ![Using kernel SVMs](img/4351OS_06_79.jpg)
    into a linear space ![Using kernel SVMs](img/4351OS_06_46.jpg). Note that the
    explicit representation of ![Using kernel SVMs](img/4351OS_06_78.jpg) is not required,
    and it's enough to know that ![Using kernel SVMs](img/4351OS_06_46.jpg) is an
    inner product space. Although we are free to choose any arbitrary kernel function
    to model the given training data, we must strive to reduce the problem of minimizing
    the cost function of the formulated SVM model. Thus, the kernel function is generally
    selected such that calculating the SVM's decision boundary only requires determining
    the dot products of vectors in the transformed feature space ![Using kernel SVMs](img/4351OS_06_46.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: A common choice for the kernel function of an SVM is the **polynomial kernel
    function**, also called the **polynomic kernel function**, which models the training
    data as polynomials of the original feature variables. As the reader may recall
    from [Chapter 5](ch05.html "Chapter 5. Selecting and Evaluating Data"), *Selecting
    and Evaluating Data*, we have discussed how polynomial features can greatly improve
    the performance of a given machine learning model. The polynomial kernel function
    can be thought of as an extension of this concept that applies to SVMs. This function
    can be formally expressed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernel SVMs](img/4351OS_06_80.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the term ![Using kernel SVMs](img/4351OS_06_81.jpg)
    represents the highest degree of the polynomial features. Also, when (the constant)
    ![Using kernel SVMs](img/4351OS_06_82.jpg), the kernel is termed to be **homogenous**.
  prefs: []
  type: TYPE_NORMAL
- en: Another widely used kernel function is the **Gaussian kernel function**. Most
    readers who are adept in linear algebra will need no introduction to the Gaussian
    function. It's important to know that this function represents a normal distribution
    of data in which the data points are closer to the mean of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of SVMs, the Gaussian kernel function can be used to represent
    a model in which one of the two classes in the training data has values for the
    input variables that are close to an arbitrary mean value. The Gaussian kernel
    function can be formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kernel SVMs](img/4351OS_06_83.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the Gaussian kernel function defined in the preceding equation, the term
    ![Using kernel SVMs](img/4351OS_06_84.jpg) represents the variance of the training
    data and represents the *width* of the Gaussian kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular choice for the kernel function is the **string kernel function**
    that operates on string values. By the term *string*, we mean a finite sequence
    of symbols. The string kernel function essentially measures the similarity between
    two given strings. If both the strings passed to the string kernel function are
    the same, the value returned by this function will be `1`. Thus, the string kernel
    function is useful in modeling data where the features are represented as strings.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential minimal optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The optimization problem of an SVM can be solved using **Sequential Minimal
    Optimization** (**SMO**). The optimization problem of an SVM is the numerical
    optimization of the cost function across several dimensions in order to reduce
    the overall error of the trained SVM. In practice, this must be done through numerical
    optimization techniques. A complete discussion of the SMO algorithm is beyond
    the scope of this book. However, we must note that this algorithm solves the optimization
    problem by a *divide-and-conquer* technique. Essentially, SMO divides the optimization
    problem of multiple dimensions into several smaller two-dimensional problems that
    can be solved analytically (for more information, refer to *Sequential Minimal
    Optimization: A Fast Algorithm for Training Support Vector Machines*).'
  prefs: []
  type: TYPE_NORMAL
- en: '**LibSVM** is a popular library that implements SMO to train an SVM. The `svm-clj`
    library is a Clojure wrapper for LibSVM and we will now explore how we can use
    this library to formulate an SVM model.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `svm-clj` library can be added to a Leiningen project by adding the following
    dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This example will use a simplified version of the **SPECT Heart** dataset ([http://archive.ics.uci.edu/ml/datasets/SPECT+Heart](http://archive.ics.uci.edu/ml/datasets/SPECT+Heart)).
    This dataset describes the diagnosis of several heart disease patients using **Single
    Proton Emission Computed Tomography** (**SPECT**) images. The original dataset
    contains a total of 267 samples, in which each sample has 23 features. The output
    variable of the dataset describes a positive or negative diagnosis of a given
    patient, which is represented using either +1 or -1, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, the training data is stored in a file named `features.dat`.
    This file must be placed in the `resources/` directory of the Leiningen project
    to make it available for use. This file contains several input features and the
    class of these input values. Let''s have a look at one of the following sample
    values in this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding line of code, the first value `+1` denotes the class
    of the sample and the other values represent the input variables. Note that the
    indexes of the input variables are also given. Also, the value of the first feature
    in the preceding sample is `0`, as it is not mentioned using a `1:` key. From
    the preceding line, it's clear that each sample will have a maximum of 12 features.
    All sample values must conform to this format, as dictated by LibSVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train an SVM using this sample data. To do this, we use the `train-model`
    function from the `svm-clj` library. Also, since we must first load the sample
    data from the file, we will need to first call the `read-dataset` function as
    well using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The trained SVM represented by the model variable as defined in the preceding
    code can now be used to predict the class of a set of input values. The `predict`
    function can be used for this purpose. For simplicity, we will use a sample value
    from the dataset variable itself as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the REPL output in the preceding code, `dataset` can be treated
    as a sequence of maps. Each map contains a single key that represents the value
    of the output variable in the sample. The value of this key in the `dataset` map
    is another map that represents the input variables of the given sample. Since
    the `feature` variable represents a map, we can call it as a function, as shown
    by the `(feature 1)` call in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: The predicted value agrees with the actual value of the output variable, or
    the class, of a given set of input values. In conclusion, the `svm-clj` library
    provides us with a simple and concise implementation of an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Using kernel functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have mentioned earlier, we can choose a kernel function for an SVM when
    we need to fit some nonlinear data. We will now demonstrate how this is achieved
    in practice using the `clj-ml` library. Since this library has already been discussed
    in the previous chapters, we will not focus on the complete training of an SVM,
    but rather on how we can create an SVM that uses kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the example that will follow, the namespace declaration should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The function, `make-kernel-function`, from the `clj-ml.kernel-functions` namespace
    is used to create kernel functions that can be used for SVMs. For example, we
    can create a polynomial kernel function by passing the :`polynomic` keyword to
    this function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding line, the polynomial kernel function defined by the
    variable `K` has a polynomial degree of `3`. Similarly, we can also create a string
    kernel function using the `:string` keyword, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several such kernel functions available in the `clj-ml` library and
    the reader is encouraged to explore more kernel functions in this library. The
    documentation for this namespace is available at [http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html](http://antoniogarrote.github.io/clj-ml/clj-ml.kernel-functions-api.html).
    We can create an SVM using the `make-classifier` function by specifying the `:support-vector-machine`
    and `:smo` keywords; and the kernel function with the keyword option `:kernel-function`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can now train the SVM represented by the variable classifier as we have done
    in the previous chapters. The `clj-ml` library, thus, allows us to create SVMs
    that exhibit a given kernel function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have explored SVMs and how they can be used to fit both
    linear and nonlinear data. The following are the other topics that we have covered:'
  prefs: []
  type: TYPE_NORMAL
- en: We have examined how SVMs are capable of large margin classification and the
    various forms of the optimization problem of SVMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have discussed how we can use kernel functions and SMO to train an SVM with
    nonlinear sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have also demonstrated how we can use several Clojure libraries to build
    and train SVMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will shift our focus to unsupervised learning in the next chapter and we
    will explore clustering techniques to model these types of machine learning problems.
  prefs: []
  type: TYPE_NORMAL
