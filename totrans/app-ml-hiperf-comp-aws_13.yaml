- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autonomous Vehicles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, almost every car company is advancing technology in their cars using
    **Autonomous Vehicle** (**AV**) systems and **Advanced Driver Assistance Systems**
    (**ADAS**). This covers everything from cruise control to several safety features
    and fully autonomous driving that you are all probably familiar with. If you are
    not familiar with these concepts, we encourage you to take the following crash
    course – test drive a car with fully autonomous capabilities to appreciate the
    technology and sophistication involved in building these kinds of systems. Companies
    that are currently heavily investing in AV and ADAS systems require heavy computational
    resources to test, simulate, and develop related technologies before deploying
    them in their cars. Many companies are turning to the cloud when there is a need
    for on-demand, elastic compute for these large-scale applications. The previous
    chapters have covered storage, network, and computing, and introduced ML in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will broadly cover what AV systems and ADAS are, and how
    AWS compute and ML services help with the design and deployment of AV/ADAS architectures.
    Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing AV systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS services supporting AV systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing an architecture for AV systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML applied to AV systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have learned about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The technology used in AVs at a high level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS services that can be used to create and test software related to AVs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How machine learning is used in the development of AVs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Familiarity with AWS and its basic usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web browser. (For the best experience, it is recommended that you use a Chrome
    or Firefox browser.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An AWS account. (If you are unfamiliar with how to get started with an AWS
    account, you can go to this link: [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/).)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing AV systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, several automotive companies are already implementing
    ADAS and AV systems in their vehicles. As such, there is a large amount of research
    and development happening in the space, but this section will introduce you to
    key terms and concepts so we can proceed further and explore how machine learning
    is involved here.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us discuss ADAS and AV at a high level. There are several threads
    of questions from people being introduced to this field leading to confusion,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Are AV systems and ADAS one and the same thing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are ADAS contained within AV systems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does a company usually develop ADAS first and then AV systems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there different levels of automation within AV systems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we can answer these questions, let’s drill down even further. In order
    to automate any part of the driving experience, whether it is for a car or a container
    truck, innovation in the following components becomes necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: Driver assistance or self-driving hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver assistance or self-driving software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data and compute services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first step in adding these technologies is adding the right hardware. Usually,
    this comprises a combination of RADAR, LiDAR, and **Camera sensors**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Radio Detection and Ranging**, or **RADAR**, uses radio waves to estimate
    the distance and velocity of objects around it. RADARs are classified based on
    their range as short-range, mid-range, and long-range. Shorter-range RADARs are
    used for functions such as parking distance assistance and blind spot monitoring.
    Longer-range RADARs are used for lane following, automatic braking, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Light Detection and Ranging**, or **LiDAR**, is similar to RADAR but uses
    the reflection of light from surfaces to determine the distance to objects. High-resolution
    LiDAR can also be used to determine the shape of objects along with **Deep Learning**
    (**DL**) algorithms, as we will learn later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, cameras are placed around the vehicles for low-level tasks such as parking,
    as well as high-level ones such as fully autonomous driving (when the right DL
    algorithms are used along with a full camera array). Elon Musk, the CEO of Tesla,
    a self-driving car company with a large number of cars currently sold with self-driving
    capabilities, famously preferred Tesla cars to be designed with camera systems
    only – no RADAR or LiDAR – since he said, humans only depend on vision to drive.
    The following figure shows an older systems architecture for Tesla, which now
    heavily depends on computer vision-based systems rather than LiDAR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 13.1 – Camera, RADAR, and LiDAR sensors mounted on a self-driving\
    \ vehicle. \uFEFF](img/B18493_13_001.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Camera, RADAR, and LiDAR sensors mounted on a self-driving vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: On a typical car chassis, you can imagine these sensors mounted, as seen in
    *Figure 13**.1*. As we can see, there are several cameras, RADAR, and LiDAR sensors
    that are used to achieve various levels of autonomous driving for vehicles. *Now,
    what are these levels of* *autonomous driving?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with some of the hardware (sensors) defined previously, a complex software
    stack is required to build and maintain the features required for AV development.
    These features are categorized by the **Society of Automotive Engineers** (**SAE**)
    and are widely adopted. The SAE defined (in 2014 and later revised) five levels
    of autonomous driving (see here: [https://www.sae.org/blog/sae-j3016-update](https://www.sae.org/blog/sae-j3016-update)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Level 0**: Features that provide warnings and limited assistance, for example,
    automatic emergency braking, blind spot warnings, and lane departure warnings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 1 – Basic driver assistance**: The driver can take their feet off the
    pedals but needs to keep their hands on the wheel to take over, for example, lane
    centering or cruise control. Note that cruise control can be *adaptive* as well,
    maintaining a safe distance from the vehicle in front of you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 2 – Limited automation**: The system controls steering, braking, and
    driving, but in limited scenarios. However, the driver must be ready to take over
    as needed to maintain safety.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 3 – Low-level automation**: The system can navigate in a greater number
    of circumstances, such as driving in traffic in addition to highway driving. Drivers
    are still required to take over driving in certain situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 4 – High-level automation**: The system controls the car in all situations,
    and drivers are only required to take over rarely when unknown situations are
    encountered. The technology is aimed to be used in driverless taxis and trucks,
    with the driver still present, to reduce workload and fatigue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Level 5 – Full automation**: The system can handle all situations of driving,
    and a driver is not required to be present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To clarify one of the questions posed at the beginning of this chapter, ADAS
    may be part of a larger AV system or used standalone. They are primarily focused
    on lower-level automation tasks and driver aids such as adaptive cruise control
    or driver alertness warning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the SAE definitions for levels of automation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – SAE levels of automation. Source: https://www.sae.org/blog/sae-j3016-update](img/B18493_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2 – SAE levels of automation. Source: [https://www.sae.org/blog/sae-j3016-update](https://www.sae.org/blog/sae-j3016-update)'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have introduced basic concepts around AV systems at a high
    level, including hardware and software components required in the development
    of these systems. In the next section, we will take a look at AWS services that
    support the development of AV systems.
  prefs: []
  type: TYPE_NORMAL
- en: AWS services supporting AV systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The development and testing of AV systems and ADAS require a cloud platform
    with highly scalable compute, storage, and networking. Being HPC applications,
    these components were covered in detail in previous chapters. As a recap, we have
    covered the following topics that are still relevant in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B18493_03.xhtml#_idTextAnchor050), *Compute and Networking* (see
    topics on architectural patterns and compute instances)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B18493_04.xhtml#_idTextAnchor074), *Data Storage* (see topics
    on Amazon S3 and FSx for Lustre)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis and Preprocessing*
    (see topics on large-scale data processing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapters 6* to *9* (covering distributed training and deployment on the cloud
    and at the edge)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will highlight some more services, including the ones that
    we discussed in the context of AV and ADAS development. A single autonomous vehicle
    can generate several TB of data per day. This data is used across the AV development
    workflow that is discussed at the following link ([https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/))
    and includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition and ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing and analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model and algorithm development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verification and validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment and orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With ever-expanding data sizes, customers look for scalable, virtually unlimited-capacity
    data stores at the center of all the preceding activities. For example, Lyft Level
    5 manages **Petabytes** (**PB**) of storage data from car sensors in **Amazon
    S3**. Amazon S3 is also central to the concept of building a **data lake** for
    applications in the AV space, which will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '*But how do customers get this data into Amazon S3 in the* *first place?*'
  prefs: []
  type: TYPE_NORMAL
- en: Customers have several options for this. In the AV space, the customer uses
    the **Snow** family of devices (**SnowBall**, **SnowCone**, and **SnowMobile**)
    to transfer up to PB of data to Amazon S3\. Customers with on-prem systems can
    also use **Amazon Outposts** to temporarily host and process this data with APIs
    similar to the cloud and also use **Amazon Direct Connect** to securely transfer
    data to Amazon S3 using a dedicated network connection. Several use cases highlighted
    here are from public references of companies using AWS for actual AV development
    (see the *References* section for more information).
  prefs: []
  type: TYPE_NORMAL
- en: Customers can also use **AWS IoT FleetWise**, a service that, at the time of
    writing, is still under preview, to easily collect and transfer data from vehicles
    to the cloud in near real time. With IoT FleetWise, customers first model the
    vehicle sensors using the FleetWise designer, then they install the **IoT FleetWise
    Edge Agent** onto the compatible edge devices that are running on the vehicle,
    define data schemas and conditions to collect the data, and stream this data to
    **Amazon Timestream** or Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: The data collected can be raw or processed sensor data, image, audio, video,
    RADAR, or LiDAR data. Once this data is on Amazon S3, it can be processed, analyzed,
    and labeled before using it in downstream tasks. Several customers, such as TuSimple,
    use Amazon **Elastic Compute Cloud** (**EC2**) to do various AV-related processing
    tasks. Customers trying to optimize the cost of processing with this scale of
    data use EC2 Spot Instances extensively. In 2020, Lyft Level 5 reported that more
    than 75% of their computing fleet was on EC2 Spot Instances to reduce the cost
    of operation (see the *References* section for a link to the use case).
  prefs: []
  type: TYPE_NORMAL
- en: For image- and video-based preprocessing workloads, several pre-trained ML models
    can be used but require access to GPU-based instances. Toyota Research Institute,
    for example, extensively uses P3 and P4 instances for highly scalable and performant
    cloud applications. Companies such as Momenta also use Amazon EMR to create analytics
    services such as safe driving assistance decision services.
  prefs: []
  type: TYPE_NORMAL
- en: For labeling data (primarily image, video, and 3D LiDAR data), customers use
    **Amazon SageMaker Ground Truth** on AWS, which provides specialized templates
    for these labeling use cases, access to private, vendor, or public labeling worker
    pools, and several assistive labeling capabilities to speed up these time-intensive
    tasks and reduce costs.
  prefs: []
  type: TYPE_NORMAL
- en: Customers also use pipelines to orchestrate end-to-end preprocessing, training,
    or post-processing workflows. A service that can help create, manage, and run
    these pipelines at scale is Amazon **Managed Workflows for Apache Airflow** or
    **MWAA**. MWAA is a managed orchestration service for Apache Airflow that makes
    it very simple to set up and operate end-to-end data pipelines in the cloud at
    scale. Alternatives to using MWAA include AWS services such as **Step Functions**
    and **Amazon** **SageMaker Pipelines**.
  prefs: []
  type: TYPE_NORMAL
- en: Model training, simulation, model compilation, verification, and validation
    workflows can make use of **Amazon EC2**, or one of the following managed services
    – **Amazon SageMaker**, **Amazon Elastic Kubernetes Service** (**EKS**), **Amazon
    Elastic Container Service** (**ECS**), and/or **AWS Batch**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a reference architecture for AV development
    on AWS that brings together many of these services.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an architecture for AV systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be discussing a reference architecture published by
    AWS called the *Autonomous Driving Data Lake Reference Architecture*, a link to
    which can be found in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete architecture is replicated in *Figure 13**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Autonomous Driving Data Lake Reference Architecture](img/B18493_13_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Autonomous Driving Data Lake Reference Architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will zoom into parts of this architecture to discuss it
    in further detail. Let’s start with data ingestion:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.4* shows how cars may be installed with a data logger or some
    removable storage media that stores data from sensors. Custom hardware or AWS
    Outposts can be used to process data that is stored from one or more trips. For
    near real time, **AWS IoT core** can be used along with **Amazon Kinesis Firehose**
    to deliver data to Amazon S3\. Customers can also use Amazon Direct Connect, as
    mentioned earlier in this chapter, for secure and fast data transfer to Amazon
    S3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Data ingestion – steps 1 and 2](img/B18493_13_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Data ingestion – steps 1 and 2
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.5* shows how **Amazon EMR** can be used to filter incoming raw
    data. For example, the quality of data can be assessed using custom PySpark code
    and dropped into different buckets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Initial data processing – step 3](img/B18493_13_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Initial data processing – step 3
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.6* shows *step 5* and *step 6*, where data is further cleaned
    and enriched (for example, with location-specific or weather data). Given the
    large amount of data being collected, another processing step in Amazon EMR can
    be used to identify interesting scenes for downstream steps such as ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Data enrichment – steps 5 and 6](img/B18493_13_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Data enrichment – steps 5 and 6
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.7* shows how Amazon MWAA can be used to orchestrate these end-to-end
    data processing workflows (*step 4*). Data generated in many intermediate steps
    can be cataloged in **AWS Glue Data Catalog**, and the lineage of this data can
    be stored as a graph in **Amazon Neptune** (*step 7*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, data can be preprocessed for visualization tools in Fargate tasks,
    with **AWS AppSync** and **Amazon QuickSight** providing visualization and KPI
    reporting capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Workflow Orchestration, Data Catalog and Visualization Tools
    – steps 4, 7, and 10, respectively](img/B18493_13_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Workflow Orchestration, Data Catalog and Visualization Tools –
    steps 4, 7, and 10, respectively
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.8* shows how **Amazon Fargate** tasks can be used to extract images
    or sequences of images from videos, with **AWS Lambda** functions used with pre-trained
    models or with the help of **Amazon Rekognition** to blur and anonymize parts
    of images such as faces or license plates. For AV customers, further pre-labeling
    can be done where pre-trained models available in open source can be used to identify
    pedestrians, cars, trucks, road signs, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This helps the labeling process go faster since most of the labeling effort
    is only to adjust existing labels, compared to creating all labels (such as bounding
    boxes) from scratch. This high-quality labeled data is the most important step
    when creating ML-powered ADAS and AV systems, which can include multiple models.
    More details on ML for AV systems will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Labeling and ML on labeled data – steps 8 and 9](img/B18493_13_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Labeling and ML on labeled data – steps 8 and 9
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how ML is applied to AV systems and use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: ML applied to AV systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing highly sophisticated **Deep Neural Networks** (**DNNs**) with the
    ability to safely operate an AV is a highly complex technical challenge. Practitioners
    require PB of real-world sensor data, hundreds of thousands, if not millions,
    of **virtual Central Processing Unit** (**vCPU**) hours, and thousands of accelerator
    chips or **Graphics Processing Unit** (**GPU**) hours to train these DNNs (also
    called *models* or *algorithms*). The end goal is to ensure these models can operate
    a vehicle autonomously safer than a human driver.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll talk about what is involved in developing models relevant
    to end-to-end AV/ADAS development workflows on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Model development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AVs typically operate through five key processes, each of which may involve
    ML to various degrees:'
  prefs: []
  type: TYPE_NORMAL
- en: Localization and mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the steps also requires different supporting data and infrastructure
    to efficiently produce a functional model or models. For example, while the perception
    stack is built on top of large computer vision models requiring distributed compute
    infrastructure to support DL training, the control step consumes a mix of general-purpose
    GPU and large memory GPU cards optimized for DL, in an online or offline **Reinforcement
    Learning** (**RL**) workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore some of the challenges that a cloud-based
    ML environment can overcome for successful AV development by leveraging AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three main challenges in building DL-based AV models:'
  prefs: []
  type: TYPE_NORMAL
- en: Feeding TB or more of training data to ML frameworks running on large-scale,
    high-performance computing infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticity to linearly scale compute infrastructure to thousands of accelerators
    leveraging high bandwidth networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestration of training the ML frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large amounts of data also means a large number of resources needed for labeling,
    so let us discuss this challenge next.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling large amounts of data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vehicle sensor and simulation data contain streams of images and videos, point
    clouds from RADAR and LiDAR, and time series data from inertia measuring sensors,
    GPS, and the vehicle **Controller Area Network** (**CAN**) bus. In a given test
    drive of 8 hours, an AV may collect more than 40 TB of data across these sensors.
    Depending on the company, a test fleet can range anywhere from a handful of vehicles
    to nearly 1,000\. At such a scale, AV data lakes grow at PB annually. Altogether,
    this data is indexed by time and stored as scenes or scenarios leveraged for model
    training. For the purposes of building models and agents that ultimately drive
    the vehicle, the data needs to get processed and labeled.
  prefs: []
  type: TYPE_NORMAL
- en: The first challenge is to make these labeled datasets available to the ML framework
    during training as fast as it can process a data batch. The amount of data required
    to train one model in one specific task alone can be in excess of hundreds of
    TB, making pre-fetching and loading data into memory unfeasible. The combination
    of ML framework and compute hardware accelerator dictates the speed at which a
    batch of such data gets read from the source for a specific model task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, you can use several built-in labeling templates on Amazon SageMaker
    Ground Truth for labeling images, video, and LiDAR data. For a description of
    AWS services that can be used to preprocess and label high-resolution video files
    recorded, visit the linked blog in the *References* section. Specifically for
    LiDAR use cases, you can use data that has both LiDAR as well as image data captured
    in sync with the LiDAR using multiple onboard cameras. Amazon SageMaker Ground
    Truth can synchronize a frame containing 3D point cloud data with up to eight
    camera sources. Once the raw data manifest is ready, you can use SageMaker Ground
    Truth for 3D object detection, object tracking, and semantic segmentation of 3D
    point clouds. As with standard SageMaker Ground Truth labeling jobs, you can use
    a fully private workforce or a trusted vendor to complete your labeling tasks.
    *Figure 13**.9* is an example of a car being labeled using a 3D bounding box along
    with three projected side views and images from cameras corresponding to that
    timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Amazon SageMaker Ground Truth labeling jobs with LiDAR and
    camera data for AV workloads](img/B18493_13_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Amazon SageMaker Ground Truth labeling jobs with LiDAR and camera
    data for AV workloads
  prefs: []
  type: TYPE_NORMAL
- en: For more information on using Ground Truth to label 3D point cloud data, see
    the link in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: Training with large amounts of data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically, DL tasks in the context of AV can be related to *perception* (finding
    information about the environment or obstacles) and *localization* (finding your
    position in the world with high accuracy). Several state-of-the-art DL models
    are being developed for detecting other vehicles, roads, pedestrians, signs, and
    objects and also to describe the position of these 2D objects around the vehicle
    in the 3D world. The *KITTI* *Benchmark* is often used to test new algorithms
    and approaches to looking at use cases related to autonomous vehicles, such as
    semantic segmentation and object detection. Access to the KITTI dataset (and other
    similar datasets such as the *Audi A2D2 Autonomous driving dataset*) can be found
    on the Registry of Open Data on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Training large semantic segmentation and object detection models on large open
    source datasets, such as **Mask R-CNN** on the **Common Objects in Context** (**COCO**)
    dataset, can achieve throughputs of 60 images per second – approximately 35 MB/s
    – on a single multi-GPU instance. For simpler architectures, training throughput
    can reach thousands of images per second due to the smaller scale of the network
    being trained on more straightforward tasks. That is true in the case of image
    classification using *ResNet* models that make up the backbone of larger models
    such as *Mask R-CNN*. More recently, models such as *DeepManta* have been used
    to obtain high scores on other related tasks such as vehicle pose estimation –
    links to these methods and papers can be found in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: At lower rates of data transfer, the training job can retrieve data objects
    directly from Amazon S3\. Amazon S3 is a foundational data service for AV development
    on AWS, as discussed in this blog post on *Building an Autonomous Driving Data
    Lake* (see the *References* section).
  prefs: []
  type: TYPE_NORMAL
- en: Some data loaders provide connectivity directly to Amazon S3, such as TensorFlow,
    for TFRecord-style datasets. However, it requires optimizing the size of each
    data file as well as the number of worker processes to maximize Amazon S3 throughput,
    which can make the data loading pipeline complex and less scalable. It is possible
    to achieve hundreds of GB/s total throughputs reading directly from S3 when horizontally
    scaling the data reader, but there is a compromise on CPU utilization for the
    training process itself. This blog ([https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/))
    explains how to optimize I/O for GPU performance. Mobileye explained their use
    of TFRecord datasets and Pipe mode on Amazon SageMaker for training their large
    DNNs resulting in faster training and a 10x improvement in development time in
    their reinvent video , which is included in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: For a more straightforward architecture, which leverages the ML framework’s
    native support of *POSIX-compliant* file system interface, AWS offers **FSx for
    Lustre** (more information about FSx and POSIX compliance is linked to in the
    *References* section). FSx for Lustre is a high-throughput, low-latency distributed
    file system that can be provisioned from existing S3 data, making the whole dataset
    available to the DNN training workers as files. These files can be iterated over
    using any of the major ML framework data readers, such as PyTorch dataset or DataLoader.
  prefs: []
  type: TYPE_NORMAL
- en: FSx for Lustre can scale its baseline aggregate bandwidth to 200 GB/s for a
    1 PB training dataset, with burst speeds of 1.3 GB/s per TiB of training data.
    The larger the provisioned FSx for Lustre deployment, the higher the aggregate
    bandwidth, enabling a PB-scale network fabric. FSx for Lustre is hydrated with
    a subset of the data from the Autonomous Driving data lake and synchronized back
    using data repository tasks in case model artifacts or data transformations are
    generated and recorded during training. For a real-world example of how the Amazon
    ML Solutions Lab helped Hyundai train a model using SageMaker’s distributed training
    library and FSx for Lustre 10x faster with only 5x the number of instances, take
    a look at the link in the *References* section to this use case.
  prefs: []
  type: TYPE_NORMAL
- en: The need for a PB-scale data repository also comes from the need to scale the
    number of compute workers processing this data. At the 60 images per second rate,
    a single worker would take more than 6.5 hours to train over the 118,000 images
    in the *COCO* dataset, considering a dozen epochs to achieve reasonable accuracy.
    Scaling the number of images per training iteration is key for achieving reasonable
    training times. Even more so given the experimental and iterative nature of building
    DL based models, requiring multiple training runs for a single model to be built.
    Large-scale training generally translates to high costs of running training experiments.
    Amazon SageMaker provides cost-saving features for both training as well as deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alex Bain, Lead for ML Systems at Lyft Level 5, said: “*By using Amazon SageMaker
    distributed training, we reduced our model training time from days to a couple
    of hours. By running our ML workloads on AWS, we streamlined our development cycles
    and reduced costs, ultimately accelerating our mission to deliver self-driving
    capabilities to* *our customers.*”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the following blog post for more examples of use cases around cost savings:
    [https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/).'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker provides connections to common file systems that store data,
    such as Amazon S3, EFS, and FSx for Lustre. When running long training jobs, the
    choice of an appropriate storage service can speed up training times overall.
    If training data is already in EFS, it is common to continue preprocessing data
    on EFS and training the model by pointing SageMaker to EFS. When data is in Amazon
    S3, customers can decide to use this data directly from S3 to make use of features
    on SageMaker such as Fast File mode, Pipe mode, data shuffling, or sharding by
    S3 key for distributed training (more information about these modes is included
    in the *References* section). Customers can also use FSx for Lustre since it automatically
    makes data available to SageMaker training instances and avoids any repetitive
    copying of data. When multiple epochs use slightly different subsets of data that
    fit into instance memory, or in the case of distributed training, FSx for Lustre
    provides extremely fast and consistent access to datasets by mounting a volume
    with the data accessible to your training code.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With distributed training strategies, many compute nodes within a cluster read
    batches of data, train over them, and synchronize the model parameters as the
    training goes on. The unit of compute for these clusters is not the individual
    compute instance, sometimes called a *node*, but the individual GPUs. This is
    because the DNNs require hardware acceleration for training. So, distribution
    occurs within and across multi-GPU compute instances.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon’s EC2 service provides the broadest compute platform in the cloud with
    17 distinct compute instance families. Each family is designed for a few specific
    workloads and consists of a given ratio of vCPU, GPU (for certain instances),
    memory, storage, and networking. For full, end-to-end AV development, companies
    largely rely on the C, M, R, G, and P instance families.
  prefs: []
  type: TYPE_NORMAL
- en: For ML model training, companies leverage the **Deep Learning Amazon Machine
    Images** (**DLAMI**) to launch NVIDIA GPU-based EC2 instances in the *P family*.
    Each EC2 P family instance generation integrates the latest NVIDIA technology,
    including the p2 instances (Tesla K80) and the p3 instances (Volta V100), and
    the recently released p4d (with Ampere A100 GPUs).
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS and NVIDIA continue to collaborate on achieving state-of-the-art model
    training times for data-parallel and model-parallel training (see the blog link
    in the *References* section). SageMaker distributed training includes libraries
    for distributed data-parallel and model-parallel modes of training. These libraries
    for data and model-parallel training extend SageMaker’s training capabilities,
    so you can train large-scale models with small code changes to your training scripts.
    For readers interested in this topic, the following video from the AWS Deep Engines
    team is on an AWS library that is useful for both data- and model-parallel training:
    [https://youtu.be/nz1EwsS5OiA](https://youtu.be/nz1EwsS5OiA).'
  prefs: []
  type: TYPE_NORMAL
- en: At the single GPU level, optimizing the memory consumption helps increase the
    throughput. For model training on data that can be batched, this means increasing
    the number of images per iteration before running out of GPU memory. Therefore,
    the higher the GPU memory, the greater the achievable training throughput, which
    favors large memory GPU nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Across GPUs, fast communication within and between instances enables faster
    synchronization of gradients during training. Networking is, therefore, a key
    aspect of scalability in enhancing the speed of each iteration step. This type
    of infrastructure is analogous to a tightly coupled HPC infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: AWS offers EC2 instances that support HPC and accelerated computing on the cloud.
    AWS has demonstrated the fastest training times for models such as Mask R-CNN
    and near linear scalability of large-scale GPU instances using the EC2 **p3dn.24xlarge**
    instance. This instance has 8 NVIDIA V100 GPUs, with 32 GB of memory each, and
    can make use of the **AWS Elastic Fabric Adapter** (**EFA**) network interface.
    EFA is a custom-built OS bypass hardware interface that enhances the performance
    of inter-instance communication, achieving 100 gigabits per second bandwidth per
    card and natively integrating with communication libraries such as MPI and **NVIDIA
    Collective Communication Library** (**NCCL**) used on ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: AWS introduced the latest generation (in 2020) of NVIDIA GPU hardware **General
    Availability** (**GA**) with EC2 p4d instances. This instance takes ML training
    in the cloud to the next level and includes 8 NVIDIA A100 GPUs with 40 GB of memory
    per GPU and an improved networking stack. Instead of a single network interface
    card, the p4d instance has 4 EFA cards for a total of 400 gigabits per second
    bandwidth. Within the instance, the p4d family also increases GPU-to-GPU communication
    bandwidth with an NVlink mesh topology for ML frameworks using NCCL. The new p4d
    instance design provides up to 3.8x the training throughput compared to the **p3dn.24xlarge**
    on backbone models for the major computer vision tasks, such as semantic segmentation,
    used in the data labeling phase of the AV development process. For more information
    on the p4d design and benchmark results, refer to this deep dive blog.
  prefs: []
  type: TYPE_NORMAL
- en: During the December 2020 AWS re:Invent conference, AWS announced plans to make
    Intel Habana Gaudi accelerators and an in-house built training chip, which will
    offer more **Terraflops** (**TFLOPS**) than any compute instance in the cloud,
    available. In 2020, AWS collaborated with NVIDIA to bring down training times
    of Mask R-CNN on the cloud to 6 minutes and 45 seconds on PyTorch and 6 minutes
    and 12 seconds with TensorFlow (see the link in the *References* section). For
    more information on data-parallel and model-parallel training with minimal code
    changes on Amazon SageMaker, see the documentation on SageMaker distributed training
    linked to in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final challenge in training AV DNNs is managing and orchestrating the tightly
    coupled HPC infrastructure at scale. AWS provides a suite of services and solutions
    for HPC that you can leverage to build and manage a large-scale DNN training cluster
    for AV including tools such as Amazon EKS, Amazon ECS, AWS Batch, and AWS Parallel
    Cluster. These topics have been discussed in the preceding chapters in detail
    and will not be repeated here.
  prefs: []
  type: TYPE_NORMAL
- en: The infrastructure management challenge also includes the ability to integrate
    upstream and downstream tasks from the AV stack throughout the development of
    the models. As an example, the validation of a perception model stack may include
    online driving simulations. When integrating a simulation environment into a model-building
    deployment, the requirements for distributed compute change from tightly coupled
    high-performance computing to highly parallelized, embarrassingly parallel batch
    simulations and client-server architectures. Efficiently integrating services
    becomes critically important to bringing AV systems development from a research
    exercise to a scalable, production-ready pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'These options give you the flexibility to build a scalable and diverse ML platform
    for your data scientist team with the quickest velocity and most robust infrastructure
    in the world. Regardless of whether you use a managed platform for ML such as
    Amazon SageMaker, or manage your own platform on Kubernetes, ML deployment and
    orchestration specifically for AV needs to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous training and re-training functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about MLOps on AWS can be found at [https://aws.amazon.com/sagemaker/mlops/](https://aws.amazon.com/sagemaker/mlops/).
  prefs: []
  type: TYPE_NORMAL
- en: 'For AV specifically, here is what a typical ML workflow spanning a few weeks
    may look like:'
  prefs: []
  type: TYPE_NORMAL
- en: New or updated datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset curation and scene selection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-labeling and data curation, where pre-trained models are used to provide
    coarse quality labeled data to human labelers, and in some cases where sensitive
    information is obfuscated from image and video datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Labeling and active learning (more information at [https://aws.amazon.com/sagemaker/data-labeling/](https://aws.amazon.com/sagemaker/data-labeling/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed training for various tasks (see [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Software-in-the-loop testing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware-in-the-loop testing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: On-road testing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset collection, and go back to *step 1.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more information about these steps, you may be interested in reading about
    how an actual customer of AWS, Aurora, achieves self-driving capabilities using
    their Aurora Driver platform, at [https://www.cnet.com/roadshow/news/aurora-drive-aws-autonomous-vehicle-development/](https://www.cnet.com/roadshow/news/aurora-drive-aws-autonomous-vehicle-development/)
    and [https://www.youtube.com/watch?v=WAELZY_TJ04](https://www.youtube.com/watch?v=WAELZY_TJ04).
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18493_06.xhtml#_idTextAnchor116), *Distributed Training of Machine
    Learning Models*, discussed distributed training, and [*Chapter 8*](B18493_08.xhtml#_idTextAnchor161),
    *Optimizing and Managing Machine Learning Models for Edge Deployment*, discussed
    model deployment at the edge; these are both very relevant to the topics listed
    in the previous steps (*steps 5–9*). Within model testing, software-in-the-loop
    testing can be done using tools on AWS, and this will be discussed in the next
    section using a hands-on example that you can follow along on your AWS account.'
  prefs: []
  type: TYPE_NORMAL
- en: Software-in-the-loop (SITL) simulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we will be discussing one very specific type of **Software-in-the-Loop**
    (**SITL**) testing that is useful for AV customers. Note that this is not the
    only type of simulation that is being run by AV customers around the world today.
    Some may involve perception tasks, planning or mapping tasks, and also end-to-end
    software tasks before moving on to **Hardware-in-the-Loop** (**HITL**) or on-road
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will walk through how you can set up a high-fidelity simulation
    of a driving environment and even test out some DL models for AV within the simulation
    environment! To do this, you have to follow two high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a container with your simulation environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use RoboMaker to run your simulation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you set this up, you can interactively work with your simulation, use the
    environment as part of a RL loop, or even generate synthetic data for your future
    ML experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we walk through the steps, here are some basics:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is AWS RoboMaker?* **RoboMaker** is a cloud-based simulation service
    where you can run your simulations without managing any infrastructure. More information
    can be found at [https://aws.amazon.com/robomaker/](https://aws.amazon.com/robomaker/).
    RoboMaker provides GPU-based compute for high-fidelity simulators discussed in
    the following point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific to AV and manufacturing-related simulations, RoboMaker lets you use
    simulators such as CARLA (which we will be using in this section), AirSim, Aslan,
    Summit, DeepDrive or *Ignition*, Drake or NVIDIA Isaac Sim, and even custom simulations
    that you create using high-fidelity rendering engines such as Unity or Unreal
    Engine. Links to these simulators and tools can be found in the *References* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CARLA** is an open source project that is commonly used in AV studies for
    simulating vehicles in environments and testing out DL or RL models for AV. CARLA
    exposes an API that lets users control all aspects of the simulation, such as
    driving, environment, traffic, and pedestrians, and also lets users configure
    sensors on vehicles, such as LiDARs, cameras, and GPS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Great, let us now get started with the steps required to run CARLA on RoboMaker!
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – build and push the CARLA container to Amazon ECR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build our custom simulation container, we need two files:'
  prefs: []
  type: TYPE_NORMAL
- en: A Dockerfile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A shell script to build and push the customer’s Docker container to ECR (you
    can use your own pipeline to do this step as well)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we will be building the container on a SageMaker notebook instance, but
    you may use the same script from your local laptop or an EC2 instance provided
    you have the right permissions set up. To demonstrate this step, we will assume
    that you’re aware of SageMaker notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what the Dockerfile looks like for CARLA when viewed from a notebook
    instance. The `writefile` command writes the following code into a new file named
    `Dockerfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we start off with the CARLA base image and install some additional
    dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The second file that is required is a script to build and push the container.
    First, we define the arguments (the name of the container) and some inputs to
    the script, such as region, account, and the full name of the Docker container
    to be built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the repository and log into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build and push the container to ECR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This shell script takes the name of your container as an argument, builds the
    container based on a local Dockerfile, and pushes the container to a repository
    in ECR. Once you have these two scripts on a notebook instance or anywhere with
    Docker installed, you can run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the location of the container you just built, similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Copy this output, as you will need it in *Step 2 – configure and run CARLA*
    *on RoboMaker*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – configure and run CARLA on RoboMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To configure and run CARLA simulations on RoboMaker, go through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **AWS RoboMaker** console on AWS – [https://aws.amazon.com/robomaker/](https://aws.amazon.com/robomaker/)
    (sign in to your AWS account if asked).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, look for `carsimrole`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Create a simulation job on AWS RoboMaker](img/B18493_13_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Create a simulation job on AWS RoboMaker
  prefs: []
  type: TYPE_NORMAL
- en: 'On the same screen, scroll down to the compute options and make sure you select
    CPU and GPU, with sliders corresponding to `s3://carlasim-bucket/output`; make
    sure you browse to a bucket that you have access to and not the one shown in this
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Configure compute options and output location for your simulation
    job](img/B18493_13_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Configure compute options and output location for your simulation
    job
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Specify robot application** page, select **None** for robot application
    (see *Figure 13**.12*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Select None for robot application](img/B18493_13_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Select None for robot application
  prefs: []
  type: TYPE_NORMAL
- en: 'Move on to the **Specify Simulation application** page, as shown in *Figure
    13**.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Create a new simulation application](img/B18493_13_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Create a new simulation application
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **Create new application**, and add the ECR repository link you copied
    from *step 1*. If you already have a simulation application, you can choose it
    from the drop-down menu, as shown in *Figure 13**.14*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.14 – Configure Simulation application by selecting it from the
    drop-down menu](img/B18493_13_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.14 – Configure Simulation application by selecting it from the drop-down
    menu
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to the **Simulation application configuration** section, enter
    the following as your launch command, as shown in *Figure 13**.15*, and remember
    to check the option for running with a streaming session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 13.15 – Enter the launch command in the Simulation application configuration
    section](img/B18493_13_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Enter the launch command in the Simulation application configuration
    section
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Simulation application tools** section, create two terminals with
    the options highlighted in *Figure 13**.16*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.16 – Add two custom tools for terminal access](img/B18493_13_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.16 – Add two custom tools for terminal access
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, click **Next** to go to the summary screen, and then click **Create**.
    This process of creating your simulation environment will take a few minutes.
    Once it is created, you should see your simulation job was created, as seen in
    *Figure 13**.17*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.17 – Wait for the simulation job to be created](img/B18493_13_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.17 – Wait for the simulation job to be created
  prefs: []
  type: TYPE_NORMAL
- en: Once the simulation is created and the **Status** field says **Running**, click
    the **Connect** button in the **Simulation application** section, as shown in
    *Figure 13**.18*. You can also access the terminal for running scripts or monitoring
    the simulation environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.18 – Connect to the main simulation application or the terminals
    created](img/B18493_13_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.18 – Connect to the main simulation application or the terminals created
  prefs: []
  type: TYPE_NORMAL
- en: Click `PythonAPI` folder, as shown in *Figure 13**.19*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.19 – CARLA Python examples inside the simulation job](img/B18493_13_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.19 – CARLA Python examples inside the simulation job
  prefs: []
  type: TYPE_NORMAL
- en: Visit the getting started guide to understand the Python API as well as included
    examples ([https://carla.readthedocs.io/en/0.9.2/getting_started/](https://carla.readthedocs.io/en/0.9.2/getting_started/)).
    Some examples are provided in *Figure 13**.20* to *Figure 13**.22*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13.20* shows a sample application where you can manually drive a Tesla
    Cybertruck in the CARLA simulation with 264 other vehicles:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.20 – Driving a Tesla Cybertruck manually in a CARLA simulation](img/B18493_13_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.20 – Driving a Tesla Cybertruck manually in a CARLA simulation
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13.21* shows a sample application that uses a Python program through
    one of the terminal applications we created to spawn traffic:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.21 – Spawning traffic in the CARLA simulation world using Python
    code](img/B18493_13_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.21 – Spawning traffic in the CARLA simulation world using Python code
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13.22* shows an application that simulates LiDAR data around a vehicle
    in the simulation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 13.22 – Simulated LiDAR data around the car](img/B18493_13_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.22 – Simulated LiDAR data around the car
  prefs: []
  type: TYPE_NORMAL
- en: 'As a next step, read how you can use RL models to control your car for self-driving
    use cases in this tutorial: [https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/](https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize all that we’ve learned so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed AV and ADAS systems at a high level, along with
    a reference architecture to build some of these systems on AWS. We also discussed
    the three main challenges practitioners face when training AV-related ML models
    in the cloud: feeding TB or more of training data to ML frameworks running on
    a large-scale, high-performance computing infrastructure, elasticity to linearly
    scale compute infrastructure to thousands of accelerators leveraging high bandwidth
    networking, and orchestrating the ML framework training.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we walked you through examples of how you can make use of tools on AWS
    to run SITL simulations for testing your ML models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on solving numerical optimization problems
    on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about topics discussed in this chapter, visit the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Autonomous Vehicle and ADAS development on AWS Part 1: Achieving* *Scale*:
    [https://aws.amazon.com/blogs/industries/autonomous-vehicle-and-adas-development-on-aws-part-1-achieving-scale/](https://aws.amazon.com/blogs/industries/autonomous-vehicle-and-adas-development-on-aws-part-1-achieving-scale/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Building an Autonomous Driving and ADAS Data Lake on* *AWS*: [https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing Hardware-in-the-Loop for Autonomous Driving Development on* *AWS*:
    [https://aws.amazon.com/blogs/architecture/field-notes-implementing-hardware-in-the-loop-for-autonomous-driving-development-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-implementing-hardware-in-the-loop-for-autonomous-driving-development-on-aws/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advanced Driver Assistance Systems (**ADAS)*: [https://www.gartner.com/en/information-technology/glossary/advanced-driver-assistance-systems-adass](https://www.gartner.com/en/information-technology/glossary/advanced-driver-assistance-systems-adass)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CARLA* *Documentation*: [https://carla.readthedocs.io/en/0.9.2/getting_started/](https://carla.readthedocs.io/en/0.9.2/getting_started/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Highly Automated and Autonomous Vehicle Development with Amazon Web* *Services*:
    [https://pages.awscloud.com/rs/112-TZM-766/images/Autonomous_Vehicle_Development_with_AWS.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Autonomous_Vehicle_Development_with_AWS.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS IoT* *FleetWise*: [https://aws.amazon.com/iot-fleetwise/](https://aws.amazon.com/iot-fleetwise/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lyft Increases Simulation Capacity, Lowers Costs Using Amazon EC2 Spot* *Instances*:
    [https://aws.amazon.com/solutions/case-studies/Lyft-level-5-spot/](https://aws.amazon.com/solutions/case-studies/Lyft-level-5-spot/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Autonomous Driving Data Lake Reference* *Architecture*: [https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/autonomous-driving-data-lake-ra.pdf?did=wp_card&trk=wp_card](https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/autonomous-driving-data-lake-ra.pdf?did=wp_card&trk=wp_card)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automating Data Ingestion and Labeling for Autonomous Vehicle* *Development*:
    [https://aws.amazon.com/blogs/architecture/field-notes-automating-data-ingestion-and-labeling-for-autonomous-vehicle-development/](https://aws.amazon.com/blogs/architecture/field-notes-automating-data-ingestion-and-labeling-for-autonomous-vehicle-development/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use Ground Truth to Label 3D Point* *Clouds*: [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mask RCNN* *paper*: [https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*COCO* *dataset*: [https://cocodataset.org](https://cocodataset.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*KITTI dataset*: [https://registry.opendata.aws/kitti/](https://registry.opendata.aws/kitti/)
    and [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A2D2* *dataset*: [https://registry.opendata.aws/aev-a2d2/](https://registry.opendata.aws/aev-a2d2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ResNet*: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DeepManta*: [https://arxiv.org/abs/1703.07570](https://arxiv.org/abs/1703.07570)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vehicle Pose Estimation on KITTI Cars* *Hard*: [https://paperswithcode.com/sota/vehicle-pose-estimation-on-kitti-cars-hard](https://paperswithcode.com/sota/vehicle-pose-estimation-on-kitti-cars-hard).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Building an Autonomous Driving and ADAS Data Lake on* *AWS*: [https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TFRecord* *dataset*: [https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance Design Patterns for Amazon* *S3*: [https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moving our Machine Learning to the Cloud Inspired* *Innovation*: [https://www.mobileye.com/blog/moving-our-machine-learning-to-the-cloud-inspired-innovation/](https://www.mobileye.com/blog/moving-our-machine-learning-to-the-cloud-inspired-innovation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lustre User* *Guide*: [https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html](https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pytorch* *DataLoader*: [https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exporting changes to the data* *repository*: [https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-changed-data-meta-dra.html](https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-changed-data-meta-dra.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hyundai reduces ML model training time for autonomous driving models using
    Amazon* *SageMaker*: [https://aws.amazon.com/de/blogs/machine-learning/hyundai-reduces-training-time-for-autonomous-driving-models-using-amazon-sagemaker/](https://aws.amazon.com/de/blogs/machine-learning/hyundai-reduces-training-time-for-autonomous-driving-models-using-amazon-sagemaker/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Access Training* *Data*: [https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon EC2 P4* *Instances*: [https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS and NVIDIA achieve the fastest training times for Mask R-CNN and* *T5-3B*:
    [https://aws.amazon.com/blogs/machine-learning/aws-and-nvidia-achieve-the-fastest-training-times-for-mask-r-cnn-and-t5-3b/](https://aws.amazon.com/blogs/machine-learning/aws-and-nvidia-achieve-the-fastest-training-times-for-mask-r-cnn-and-t5-3b/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVLink and* *NVSwitch*: [https://www.nvidia.com/en-us/data-center/nvlink/](https://www.nvidia.com/en-us/data-center/nvlink/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA Collective Communications Library (**NCCL)*: [https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS* *re:Invent*: [https://reinvent.awsevents.com/](https://reinvent.awsevents.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon EC2 DL1* *Instances*: [https://aws.amazon.com/ec2/instance-types/habana-gaudi/](https://aws.amazon.com/ec2/instance-types/habana-gaudi/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distributed training* *libraries*: [https://aws.amazon.com/sagemaker/distributed-training/](https://aws.amazon.com/sagemaker/distributed-training/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CARLA*: [http://carla.org/](http://carla.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AirSim*: [https://github.com/microsoft/AirSim](https://github.com/microsoft/AirSim)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Project* *Aslan*: [https://github.com/project-aslan/Aslan](https://github.com/project-aslan/Aslan)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SUMMIT* *Simulator*: [https://github.com/AdaCompNUS/summit](https://github.com/AdaCompNUS/summit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deepdrive*: [https://github.com/deepdrive/deepdrive](https://github.com/deepdrive/deepdrive)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gazebo*: [https://gazebosim.org/home](https://gazebosim.org/home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Drake*: [https://drake.mit.edu/](https://drake.mit.edu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA Isaac* *Sim*: [https://developer.nvidia.com/isaac-sim](https://developer.nvidia.com/isaac-sim)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unity*: [https://unity.com/](https://unity.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unreal* *Engine*: [https://www.unrealengine.com/](https://www.unrealengine.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
