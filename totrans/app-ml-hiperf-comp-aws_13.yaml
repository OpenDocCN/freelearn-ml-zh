- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Autonomous Vehicles
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: Today, almost every car company is advancing technology in their cars using
    **Autonomous Vehicle** (**AV**) systems and **Advanced Driver Assistance Systems**
    (**ADAS**). This covers everything from cruise control to several safety features
    and fully autonomous driving that you are all probably familiar with. If you are
    not familiar with these concepts, we encourage you to take the following crash
    course – test drive a car with fully autonomous capabilities to appreciate the
    technology and sophistication involved in building these kinds of systems. Companies
    that are currently heavily investing in AV and ADAS systems require heavy computational
    resources to test, simulate, and develop related technologies before deploying
    them in their cars. Many companies are turning to the cloud when there is a need
    for on-demand, elastic compute for these large-scale applications. The previous
    chapters have covered storage, network, and computing, and introduced ML in general.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，几乎每家汽车公司都在使用**自动驾驶汽车**（**AV**）系统和**高级驾驶辅助系统**（**ADAS**）来推进他们汽车中的技术。这包括从定速巡航到多个安全特性，以及你们可能都熟悉的完全自动驾驶。如果您不熟悉这些概念，我们鼓励您参加以下速成课程——试驾一辆具有完全自动驾驶能力的汽车，以欣赏构建这些系统所涉及的技术和复杂性。目前，在AV和ADAS系统中大量投资的公司需要大量的计算资源来测试、模拟和开发相关技术，然后再将其部署到他们的汽车中。许多公司正在转向云服务，以满足这些大规模应用对按需、弹性计算的需求。前几章已经涵盖了存储、网络和计算，并介绍了机器学习。
- en: 'In this chapter, we will broadly cover what AV systems and ADAS are, and how
    AWS compute and ML services help with the design and deployment of AV/ADAS architectures.
    Specifically, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将广泛介绍AV系统和ADAS是什么，以及AWS计算和ML服务如何帮助设计AV/ADAS架构。具体来说，我们将涵盖以下主题：
- en: Introducing AV systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍AV系统
- en: AWS services supporting AV systems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持AV系统的AWS服务
- en: Designing an architecture for AV systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计AV系统的架构
- en: ML applied to AV systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习在AV系统中的应用
- en: 'By the end of this chapter, you will have learned about the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解以下内容：
- en: The technology used in AVs at a high level
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AV系统中的高级技术
- en: AWS services that can be used to create and test software related to AVs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用于创建和测试与AV相关的软件的AWS服务
- en: How machine learning is used in the development of AVs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习在AV开发中的应用
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，你应该具备以下先决条件：
- en: Familiarity with AWS and its basic usage.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉AWS及其基本使用。
- en: A web browser. (For the best experience, it is recommended that you use a Chrome
    or Firefox browser.)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器。（为了获得最佳体验，建议您使用Chrome或Firefox浏览器。）
- en: 'An AWS account. (If you are unfamiliar with how to get started with an AWS
    account, you can go to this link: [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/).)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个AWS账户。（如果您不熟悉如何开始AWS账户，您可以访问此链接：[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)。）
- en: Introducing AV systems
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍AV系统
- en: As we mentioned earlier, several automotive companies are already implementing
    ADAS and AV systems in their vehicles. As such, there is a large amount of research
    and development happening in the space, but this section will introduce you to
    key terms and concepts so we can proceed further and explore how machine learning
    is involved here.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，几家汽车公司已经在他们的车辆中实施ADAS和AV系统。因此，在这个领域正在进行大量的研究和开发，但本节将向你介绍关键术语和概念，以便我们进一步探讨机器学习在这里的涉及情况。
- en: 'First, let us discuss ADAS and AV at a high level. There are several threads
    of questions from people being introduced to this field leading to confusion,
    such as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从高层次上讨论ADAS和AV。对于刚开始接触这个领域的人来说，有几个问题线索会导致混淆，如下所示：
- en: Are AV systems and ADAS one and the same thing?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AV系统和ADAS是否是同一件事？
- en: Are ADAS contained within AV systems?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ADAS是否包含在AV系统中？
- en: Does a company usually develop ADAS first and then AV systems?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司通常会先开发ADAS系统，然后再开发AV系统吗？
- en: Are there different levels of automation within AV systems?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AV系统中是否存在不同的自动化级别？
- en: 'Before we can answer these questions, let’s drill down even further. In order
    to automate any part of the driving experience, whether it is for a car or a container
    truck, innovation in the following components becomes necessary:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够回答这些问题之前，让我们进一步深入探讨。为了自动化驾驶体验的任何一部分，无论是汽车还是集装箱卡车，以下组件的创新变得必要：
- en: Driver assistance or self-driving hardware
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶辅助或自动驾驶硬件
- en: Driver assistance or self-driving software
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驾驶辅助或自动驾驶软件
- en: Data and compute services
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和计算服务
- en: The first step in adding these technologies is adding the right hardware. Usually,
    this comprises a combination of RADAR, LiDAR, and **Camera sensors**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加这些技术时，第一步是添加正确的硬件。通常，这包括雷达、激光雷达和**摄像头传感器**的组合。
- en: '**Radio Detection and Ranging**, or **RADAR**, uses radio waves to estimate
    the distance and velocity of objects around it. RADARs are classified based on
    their range as short-range, mid-range, and long-range. Shorter-range RADARs are
    used for functions such as parking distance assistance and blind spot monitoring.
    Longer-range RADARs are used for lane following, automatic braking, and so on.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无线电探测与测距**，或称为**雷达**，使用无线电波来估计周围物体的距离和速度。雷达根据其范围分为短程、中程和长程。短程雷达用于停车距离辅助和盲点监控等功能。长程雷达用于车道保持、自动制动等。'
- en: '**Light Detection and Ranging**, or **LiDAR**, is similar to RADAR but uses
    the reflection of light from surfaces to determine the distance to objects. High-resolution
    LiDAR can also be used to determine the shape of objects along with **Deep Learning**
    (**DL**) algorithms, as we will learn later in this chapter.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**光探测与测距**，或称为**激光雷达**，与雷达类似，但使用从表面反射的光来确定物体距离。高分辨率激光雷达还可以与**深度学习**（**DL**）算法一起使用，以确定物体的形状，正如我们将在本章后面学到的那样。'
- en: Lastly, cameras are placed around the vehicles for low-level tasks such as parking,
    as well as high-level ones such as fully autonomous driving (when the right DL
    algorithms are used along with a full camera array). Elon Musk, the CEO of Tesla,
    a self-driving car company with a large number of cars currently sold with self-driving
    capabilities, famously preferred Tesla cars to be designed with camera systems
    only – no RADAR or LiDAR – since he said, humans only depend on vision to drive.
    The following figure shows an older systems architecture for Tesla, which now
    heavily depends on computer vision-based systems rather than LiDAR.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，车辆周围安装了摄像头，用于低级任务，如停车，以及高级任务，如完全自动驾驶（当使用完整的摄像头阵列和正确的深度学习算法时）。特斯拉公司的首席执行官埃隆·马斯克，该公司是一家拥有大量已售出具有自动驾驶功能的汽车的公司，著名地更喜欢特斯拉汽车只设计有摄像头系统——没有雷达或激光雷达——因为他表示，人类只依赖视觉来驾驶。以下图显示了特斯拉较老的系统架构，现在它严重依赖于基于计算机视觉的系统，而不是激光雷达。
- en: "![Figure 13.1 – Camera, RADAR, and LiDAR sensors mounted on a self-driving\
    \ vehicle. \uFEFF](img/B18493_13_001.jpg)"
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图13.1 – 安装在自动驾驶车辆上的摄像头、雷达和激光雷达传感器。](img/B18493_13_001.jpg)'
- en: Figure 13.1 – Camera, RADAR, and LiDAR sensors mounted on a self-driving vehicle.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 – 安装在自动驾驶车辆上的摄像头、雷达和激光雷达传感器。
- en: On a typical car chassis, you can imagine these sensors mounted, as seen in
    *Figure 13**.1*. As we can see, there are several cameras, RADAR, and LiDAR sensors
    that are used to achieve various levels of autonomous driving for vehicles. *Now,
    what are these levels of* *autonomous driving?*
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的汽车底盘上，你可以想象这些传感器被安装的情况，如*图13.1*所示。正如我们所见，有几个摄像头、雷达和激光雷达传感器，它们被用来实现车辆的多种自动驾驶级别。*那么，这些自动驾驶级别是什么？*
- en: 'Along with some of the hardware (sensors) defined previously, a complex software
    stack is required to build and maintain the features required for AV development.
    These features are categorized by the **Society of Automotive Engineers** (**SAE**)
    and are widely adopted. The SAE defined (in 2014 and later revised) five levels
    of autonomous driving (see here: [https://www.sae.org/blog/sae-j3016-update](https://www.sae.org/blog/sae-j3016-update)):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前定义的一些硬件（传感器）之外，还需要一个复杂的软件堆栈来构建和维护自动驾驶车辆开发所需的功能。这些功能被**汽车工程师协会**（**SAE**）分类，并且被广泛采用。SAE定义了（2014年及以后修订）五个级别的自动驾驶（见此处：[https://www.sae.org/blog/sae-j3016-update](https://www.sae.org/blog/sae-j3016-update)）：
- en: '**Level 0**: Features that provide warnings and limited assistance, for example,
    automatic emergency braking, blind spot warnings, and lane departure warnings.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**级别0**：提供警告和有限辅助的功能，例如，自动紧急制动、盲点警告和车道偏离警告。'
- en: '**Level 1 – Basic driver assistance**: The driver can take their feet off the
    pedals but needs to keep their hands on the wheel to take over, for example, lane
    centering or cruise control. Note that cruise control can be *adaptive* as well,
    maintaining a safe distance from the vehicle in front of you.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一级 – 基本驾驶辅助**：驾驶员可以抬起脚离开踏板，但需要保持手在方向盘上以接管，例如车道保持或定速巡航。请注意，定速巡航也可以是*自适应*的，与前方车辆保持安全距离。'
- en: '**Level 2 – Limited automation**: The system controls steering, braking, and
    driving, but in limited scenarios. However, the driver must be ready to take over
    as needed to maintain safety.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二级 – 有限自动化**：系统控制转向、制动和驾驶，但仅限于有限场景。然而，驾驶员必须随时准备接管，以确保安全。'
- en: '**Level 3 – Low-level automation**: The system can navigate in a greater number
    of circumstances, such as driving in traffic in addition to highway driving. Drivers
    are still required to take over driving in certain situations.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三级 – 低级自动化**：系统可以在更多情况下导航，例如在高速公路驾驶外，还可以在交通中驾驶。驾驶员仍需要在某些情况下接管驾驶。'
- en: '**Level 4 – High-level automation**: The system controls the car in all situations,
    and drivers are only required to take over rarely when unknown situations are
    encountered. The technology is aimed to be used in driverless taxis and trucks,
    with the driver still present, to reduce workload and fatigue.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四级 – 高级自动化**：系统在所有情况下控制车辆，驾驶员仅在遇到未知情况时很少需要接管。这项技术旨在用于无人出租车和卡车，尽管驾驶员仍然在场，以减少工作量和疲劳。'
- en: '**Level 5 – Full automation**: The system can handle all situations of driving,
    and a driver is not required to be present.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第五级 – 完全自动化**：系统可以处理所有驾驶情况，无需驾驶员在场。'
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To clarify one of the questions posed at the beginning of this chapter, ADAS
    may be part of a larger AV system or used standalone. They are primarily focused
    on lower-level automation tasks and driver aids such as adaptive cruise control
    or driver alertness warning systems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清本章开头提出的一个问题，ADAS可能是更大AV系统的一部分或独立使用。它们主要关注低级自动化任务和驾驶员辅助，如自适应定速巡航或驾驶员警觉性警告系统。
- en: 'The following figure shows the SAE definitions for levels of automation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了SAE对自动化水平的定义：
- en: '![Figure 13.2 – SAE levels of automation. Source: https://www.sae.org/blog/sae-j3016-update](img/B18493_13_002.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图13.2 – 自动化水平SAE。来源：https://www.sae.org/blog/sae-j3016-update](img/B18493_13_002.jpg)'
- en: 'Figure 13.2 – SAE levels of automation. Source: [https://www.sae.org/blog/sae-j3016-update](https://www.sae.org/blog/sae-j3016-update)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 自动化水平SAE。来源：[https://www.sae.org/blog/sae-j3016-update](https://www.sae.org/blog/sae-j3016-update)
- en: In this section, we have introduced basic concepts around AV systems at a high
    level, including hardware and software components required in the development
    of these systems. In the next section, we will take a look at AWS services that
    support the development of AV systems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们以高级别介绍了AV系统相关的基本概念，包括这些系统开发所需的硬件和软件组件。在下一节中，我们将探讨支持AV系统开发的AWS服务。
- en: AWS services supporting AV systems
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持AV系统的AWS服务
- en: 'The development and testing of AV systems and ADAS require a cloud platform
    with highly scalable compute, storage, and networking. Being HPC applications,
    these components were covered in detail in previous chapters. As a recap, we have
    covered the following topics that are still relevant in this chapter:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: AV系统和ADAS的开发和测试需要一个具有高度可扩展计算、存储和网络功能的云平台。作为高性能计算应用，这些组件在之前的章节中已有详细阐述。作为回顾，本章仍涉及以下相关主题：
- en: '[*Chapter 3*](B18493_03.xhtml#_idTextAnchor050), *Compute and Networking* (see
    topics on architectural patterns and compute instances)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第三章*](B18493_03.xhtml#_idTextAnchor050)，*计算和网络*（参见关于架构模式和计算实例的主题）'
- en: '[*Chapter 4*](B18493_04.xhtml#_idTextAnchor074), *Data Storage* (see topics
    on Amazon S3 and FSx for Lustre)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第四章*](B18493_04.xhtml#_idTextAnchor074)，*数据存储*（参见关于Amazon S3和FSx for Lustre的主题）'
- en: '[*Chapter 5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis and Preprocessing*
    (see topics on large-scale data processing)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第五章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析与预处理*（参见关于大规模数据处理的主题）'
- en: '*Chapters 6* to *9* (covering distributed training and deployment on the cloud
    and at the edge)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第六章*至*第九章*（涵盖在云和边缘的分布式训练和部署）'
- en: 'In this section, we will highlight some more services, including the ones that
    we discussed in the context of AV and ADAS development. A single autonomous vehicle
    can generate several TB of data per day. This data is used across the AV development
    workflow that is discussed at the following link ([https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/))
    and includes the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将突出介绍一些更多服务，包括我们在自动驾驶和ADAS开发背景下讨论的服务。一辆单一的自驾驶汽车每天可以生成数TB的数据。这些数据被用于以下链接中讨论的自动驾驶开发工作流程（[https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/)），并包括以下内容：
- en: Data acquisition and ingestion
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采集和摄取
- en: Data processing and analytics
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理和分析
- en: Labeling
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记
- en: Map development
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地图开发
- en: Model and algorithm development
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型和算法开发
- en: Simulation
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟
- en: Verification and validation
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证和验证
- en: Deployment and orchestration
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署和编排
- en: With ever-expanding data sizes, customers look for scalable, virtually unlimited-capacity
    data stores at the center of all the preceding activities. For example, Lyft Level
    5 manages **Petabytes** (**PB**) of storage data from car sensors in **Amazon
    S3**. Amazon S3 is also central to the concept of building a **data lake** for
    applications in the AV space, which will be discussed in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据规模的不断扩展，客户寻求在所有上述活动的中心寻找可扩展的、几乎无限容量的数据存储。例如，Lyft Level 5管理着来自汽车传感器的**PB**（**PB**）存储数据，存储在**Amazon
    S3**中。Amazon S3也是构建自动驾驶领域应用**数据湖**概念的核心，这将在下一节中讨论。
- en: '*But how do customers get this data into Amazon S3 in the* *first place?*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*但客户最初是如何将数据放入Amazon S3的* *呢*？'
- en: Customers have several options for this. In the AV space, the customer uses
    the **Snow** family of devices (**SnowBall**, **SnowCone**, and **SnowMobile**)
    to transfer up to PB of data to Amazon S3\. Customers with on-prem systems can
    also use **Amazon Outposts** to temporarily host and process this data with APIs
    similar to the cloud and also use **Amazon Direct Connect** to securely transfer
    data to Amazon S3 using a dedicated network connection. Several use cases highlighted
    here are from public references of companies using AWS for actual AV development
    (see the *References* section for more information).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一点，客户有多种选择。在自动驾驶领域，客户使用**Snow**系列设备（**SnowBall**、**SnowCone**和**SnowMobile**）将高达PB的数据传输到Amazon
    S3。拥有本地系统的客户也可以使用**Amazon Outposts**来临时托管和处理这些数据，并使用与云类似的API，同时还可以使用**Amazon Direct
    Connect**通过专用网络连接安全地将数据传输到Amazon S3。这里突出的一些用例来自使用AWS进行实际自动驾驶开发的公司公开引用（有关更多信息，请参阅*参考文献*部分）。
- en: Customers can also use **AWS IoT FleetWise**, a service that, at the time of
    writing, is still under preview, to easily collect and transfer data from vehicles
    to the cloud in near real time. With IoT FleetWise, customers first model the
    vehicle sensors using the FleetWise designer, then they install the **IoT FleetWise
    Edge Agent** onto the compatible edge devices that are running on the vehicle,
    define data schemas and conditions to collect the data, and stream this data to
    **Amazon Timestream** or Amazon S3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 客户还可以使用**AWS IoT FleetWise**，这是一种在撰写本文时仍在预览中的服务，可以轻松地从车辆收集和传输数据到云中，几乎实时。使用IoT
    FleetWise，客户首先使用FleetWise设计器对车辆传感器进行建模，然后在车辆上运行的兼容边缘设备上安装**IoT FleetWise Edge
    Agent**，定义数据模式和条件以收集数据，并将这些数据流式传输到**Amazon Timestream**或Amazon S3。
- en: The data collected can be raw or processed sensor data, image, audio, video,
    RADAR, or LiDAR data. Once this data is on Amazon S3, it can be processed, analyzed,
    and labeled before using it in downstream tasks. Several customers, such as TuSimple,
    use Amazon **Elastic Compute Cloud** (**EC2**) to do various AV-related processing
    tasks. Customers trying to optimize the cost of processing with this scale of
    data use EC2 Spot Instances extensively. In 2020, Lyft Level 5 reported that more
    than 75% of their computing fleet was on EC2 Spot Instances to reduce the cost
    of operation (see the *References* section for a link to the use case).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的数据可以是原始或处理过的传感器数据、图像、音频、视频、雷达或激光雷达数据。一旦这些数据存储在Amazon S3上，就可以在用于下游任务之前进行处理、分析和标记。例如，TuSimple等几家客户使用Amazon
    **弹性计算云**（**EC2**）来完成各种与自动驾驶相关的处理任务。试图优化处理成本并使用如此规模数据的客户广泛使用EC2 Spot Instances。2020年，Lyft
    Level 5报告称，他们超过75%的计算车队都在使用EC2 Spot Instances以降低运营成本（有关用例的链接，请参阅*参考文献*部分）。
- en: For image- and video-based preprocessing workloads, several pre-trained ML models
    can be used but require access to GPU-based instances. Toyota Research Institute,
    for example, extensively uses P3 and P4 instances for highly scalable and performant
    cloud applications. Companies such as Momenta also use Amazon EMR to create analytics
    services such as safe driving assistance decision services.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于图像和视频的预处理工作负载，可以使用几个预训练的机器学习模型，但需要访问基于GPU的实例。例如，丰田研究院广泛使用P3和P4实例来构建高度可扩展和性能优异的云应用。像Momenta这样的公司也使用Amazon
    EMR来创建如安全驾驶辅助决策服务之类的分析服务。
- en: For labeling data (primarily image, video, and 3D LiDAR data), customers use
    **Amazon SageMaker Ground Truth** on AWS, which provides specialized templates
    for these labeling use cases, access to private, vendor, or public labeling worker
    pools, and several assistive labeling capabilities to speed up these time-intensive
    tasks and reduce costs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据标注（主要是图像、视频和3D激光雷达数据），客户使用AWS上的**Amazon SageMaker Ground Truth**，它为这些标注用例提供专门的模板，访问私有、供应商或公共标注工作者池，以及几种辅助标注能力，以加快这些耗时任务并降低成本。
- en: Customers also use pipelines to orchestrate end-to-end preprocessing, training,
    or post-processing workflows. A service that can help create, manage, and run
    these pipelines at scale is Amazon **Managed Workflows for Apache Airflow** or
    **MWAA**. MWAA is a managed orchestration service for Apache Airflow that makes
    it very simple to set up and operate end-to-end data pipelines in the cloud at
    scale. Alternatives to using MWAA include AWS services such as **Step Functions**
    and **Amazon** **SageMaker Pipelines**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 客户还使用管道来编排端到端预处理、训练或后处理工作流程。一个可以帮助创建、管理和大规模运行这些管道的服务是Amazon **Managed Workflows
    for Apache Airflow**或**MWAA**。MWAA是Apache Airflow的托管编排服务，使得在云中大规模设置和操作端到端数据管道变得非常简单。使用MWAA的替代方案包括AWS服务，如**Step
    Functions**和**Amazon SageMaker Pipelines**。
- en: Model training, simulation, model compilation, verification, and validation
    workflows can make use of **Amazon EC2**, or one of the following managed services
    – **Amazon SageMaker**, **Amazon Elastic Kubernetes Service** (**EKS**), **Amazon
    Elastic Container Service** (**ECS**), and/or **AWS Batch**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练、模拟、模型编译、验证和验证工作流程可以利用**Amazon EC2**或以下托管服务之一 – **Amazon SageMaker**、**Amazon
    Elastic Kubernetes Service**（**EKS**）、**Amazon Elastic Container Service**（**ECS**）和/或**AWS
    Batch**。
- en: In the next section, we will discuss a reference architecture for AV development
    on AWS that brings together many of these services.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论AWS上自动驾驶开发的一个参考架构，它汇集了许多这些服务。
- en: Designing an architecture for AV systems
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计自动驾驶系统架构
- en: In this section, we will be discussing a reference architecture published by
    AWS called the *Autonomous Driving Data Lake Reference Architecture*, a link to
    which can be found in the *References* section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论AWS发布的名为*自动驾驶数据湖参考架构*的参考架构，其链接可在*参考文献*部分找到。
- en: 'The complete architecture is replicated in *Figure 13**.3*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的架构在*图 13.3*中得到了复制：
- en: '![Figure 13.3 – Autonomous Driving Data Lake Reference Architecture](img/B18493_13_003.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3 – 自动驾驶数据湖参考架构](img/B18493_13_003.jpg)'
- en: Figure 13.3 – Autonomous Driving Data Lake Reference Architecture
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 自动驾驶数据湖参考架构
- en: 'In this section, we will zoom into parts of this architecture to discuss it
    in further detail. Let’s start with data ingestion:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨该架构的某些部分，以进一步详细讨论。让我们从数据采集开始：
- en: '*Figure 13**.4* shows how cars may be installed with a data logger or some
    removable storage media that stores data from sensors. Custom hardware or AWS
    Outposts can be used to process data that is stored from one or more trips. For
    near real time, **AWS IoT core** can be used along with **Amazon Kinesis Firehose**
    to deliver data to Amazon S3\. Customers can also use Amazon Direct Connect, as
    mentioned earlier in this chapter, for secure and fast data transfer to Amazon
    S3.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图 13.4*展示了汽车可能安装有数据记录器或某些可移除的存储介质，用于存储传感器的数据。可以使用定制硬件或AWS Outposts来处理存储在一个或多个行程中的数据。对于接近实时，可以使用**AWS
    IoT core**以及**Amazon Kinesis Firehose**将数据传输到Amazon S3。客户还可以使用在本章前面提到的Amazon Direct
    Connect，以安全快速地将数据传输到Amazon S3。'
- en: '![Figure 13.4 – Data ingestion – steps 1 and 2](img/B18493_13_004.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.4 – 数据采集 – 步骤 1 和 2](img/B18493_13_004.jpg)'
- en: Figure 13.4 – Data ingestion – steps 1 and 2
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – 数据采集 – 步骤 1 和 2
- en: '*Figure 13**.5* shows how **Amazon EMR** can be used to filter incoming raw
    data. For example, the quality of data can be assessed using custom PySpark code
    and dropped into different buckets.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图 13.5* 展示了如何使用 **Amazon EMR** 来过滤传入的原始数据。例如，可以使用自定义 PySpark 代码评估数据质量，并将其丢弃到不同的桶中。'
- en: '![Figure 13.5 – Initial data processing – step 3](img/B18493_13_005.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 初始数据处理 – 步骤 3](img/B18493_13_005.jpg)'
- en: Figure 13.5 – Initial data processing – step 3
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 初始数据处理 – 步骤 3
- en: '*Figure 13**.6* shows *step 5* and *step 6*, where data is further cleaned
    and enriched (for example, with location-specific or weather data). Given the
    large amount of data being collected, another processing step in Amazon EMR can
    be used to identify interesting scenes for downstream steps such as ML.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图 13.6* 展示了 *步骤 5* 和 *步骤 6*，其中数据进一步清理和丰富（例如，使用特定位置或天气数据）。鉴于收集到的数据量很大，可以在 Amazon
    EMR 中使用另一个处理步骤来识别对下游步骤（如机器学习）有趣的场景。'
- en: '![Figure 13.6 – Data enrichment – steps 5 and 6](img/B18493_13_006.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6 – 数据丰富 – 步骤 5 和 6](img/B18493_13_006.jpg)'
- en: Figure 13.6 – Data enrichment – steps 5 and 6
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 – 数据丰富 – 步骤 5 和 6
- en: '*Figure 13**.7* shows how Amazon MWAA can be used to orchestrate these end-to-end
    data processing workflows (*step 4*). Data generated in many intermediate steps
    can be cataloged in **AWS Glue Data Catalog**, and the lineage of this data can
    be stored as a graph in **Amazon Neptune** (*step 7*).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图 13.7* 展示了如何使用 Amazon MWAA 来编排这些端到端的数据处理工作流程（*步骤 4*）。在许多中间步骤中生成的数据可以存储在 **AWS
    Glue 数据目录** 中，并且这些数据的历史可以存储在 **Amazon Neptune** 中作为图（*步骤 7*）。'
- en: Finally, data can be preprocessed for visualization tools in Fargate tasks,
    with **AWS AppSync** and **Amazon QuickSight** providing visualization and KPI
    reporting capabilities.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，数据可以预先处理以供 Fargate 任务中的可视化工具使用，**AWS AppSync** 和 **Amazon QuickSight** 提供可视化和
    KPI 报告功能。
- en: '![Figure 13.7 – Workflow Orchestration, Data Catalog and Visualization Tools
    – steps 4, 7, and 10, respectively](img/B18493_13_007.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.7 – 工作流程编排、数据目录和可视化工具 – 分别为步骤 4、7 和 10](img/B18493_13_007.jpg)'
- en: Figure 13.7 – Workflow Orchestration, Data Catalog and Visualization Tools –
    steps 4, 7, and 10, respectively
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 – 工作流程编排、数据目录和可视化工具 – 分别为步骤 4、7 和 10
- en: '*Figure 13**.8* shows how **Amazon Fargate** tasks can be used to extract images
    or sequences of images from videos, with **AWS Lambda** functions used with pre-trained
    models or with the help of **Amazon Rekognition** to blur and anonymize parts
    of images such as faces or license plates. For AV customers, further pre-labeling
    can be done where pre-trained models available in open source can be used to identify
    pedestrians, cars, trucks, road signs, and so on.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图 13.8* 展示了如何使用 **Amazon Fargate** 任务从视频中提取图像或图像序列，使用预训练模型或借助 **Amazon Rekognition**
    来模糊和匿名化图像的某些部分（如面部或车牌）。对于自动驾驶系统客户，还可以进行进一步的预标注，其中可以使用开源中可用的预训练模型来识别行人、汽车、卡车、路标等。'
- en: This helps the labeling process go faster since most of the labeling effort
    is only to adjust existing labels, compared to creating all labels (such as bounding
    boxes) from scratch. This high-quality labeled data is the most important step
    when creating ML-powered ADAS and AV systems, which can include multiple models.
    More details on ML for AV systems will be discussed in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于加快标注过程，因为大部分的标注工作只是调整现有的标签，而不是从头创建所有标签（如边界框）。这种高质量的标注数据是创建机器学习驱动的 ADAS 和自动驾驶系统时最重要的步骤，这些系统可能包含多个模型。关于自动驾驶系统中的机器学习的更多细节将在下一节讨论。
- en: '![Figure 13.8 – Labeling and ML on labeled data – steps 8 and 9](img/B18493_13_008.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.8 – 标注和标注数据上的机器学习 – 步骤 8 和 9](img/B18493_13_008.jpg)'
- en: Figure 13.8 – Labeling and ML on labeled data – steps 8 and 9
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – 标注和标注数据上的机器学习 – 步骤 8 和 9
- en: In the next section, we will discuss how ML is applied to AV systems and use
    cases.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论机器学习在自动驾驶系统中的应用和用例。
- en: ML applied to AV systems
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用到自动驾驶系统中的机器学习
- en: Developing highly sophisticated **Deep Neural Networks** (**DNNs**) with the
    ability to safely operate an AV is a highly complex technical challenge. Practitioners
    require PB of real-world sensor data, hundreds of thousands, if not millions,
    of **virtual Central Processing Unit** (**vCPU**) hours, and thousands of accelerator
    chips or **Graphics Processing Unit** (**GPU**) hours to train these DNNs (also
    called *models* or *algorithms*). The end goal is to ensure these models can operate
    a vehicle autonomously safer than a human driver.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 开发能够安全操作自动驾驶汽车的**深度神经网络**（**DNNs**）是一项高度复杂的技术挑战。从业者需要PB级的真实世界传感器数据，数百万甚至数百万的**虚拟中央处理器**（**vCPU**）小时，以及数千个加速器芯片或**图形处理单元**（**GPU**）小时来训练这些DNNs（也称为*模型*或*算法*）。最终目标是确保这些模型可以比人类驾驶员更安全地自主驾驶车辆。
- en: In this section, we’ll talk about what is involved in developing models relevant
    to end-to-end AV/ADAS development workflows on AWS.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在AWS上开发与端到端AV/ADAS开发工作流程相关的模型所涉及的内容。
- en: Model development
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型开发
- en: 'AVs typically operate through five key processes, each of which may involve
    ML to various degrees:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车（AVs）通常通过五个关键过程来运行，每个过程可能涉及机器学习（ML）的不同程度：
- en: Localization and mapping
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定位和地图构建
- en: Perception
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知
- en: Prediction
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Planning
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划
- en: Control
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制
- en: Each of the steps also requires different supporting data and infrastructure
    to efficiently produce a functional model or models. For example, while the perception
    stack is built on top of large computer vision models requiring distributed compute
    infrastructure to support DL training, the control step consumes a mix of general-purpose
    GPU and large memory GPU cards optimized for DL, in an online or offline **Reinforcement
    Learning** (**RL**) workflow.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤都需要不同的支持数据和基础设施来高效地生成一个或多个功能模型。例如，虽然感知堆栈建立在需要分布式计算基础设施来支持深度学习训练的大型计算机视觉模型之上，但控制步骤消耗了通用GPU和针对深度学习优化的、具有大内存的GPU卡，在一个在线或离线的**强化学习**（**RL**）工作流程中。
- en: In the next section, we will explore some of the challenges that a cloud-based
    ML environment can overcome for successful AV development by leveraging AWS.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨基于云的ML环境如何利用AWS克服一些挑战，以实现自动驾驶汽车（AV）的成功开发。
- en: Challenges
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 挑战
- en: 'There are three main challenges in building DL-based AV models:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建基于深度学习（DL）的AV模型方面有三个主要挑战：
- en: Feeding TB or more of training data to ML frameworks running on large-scale,
    high-performance computing infrastructure
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将TB或更多的训练数据输入到运行在大型高性能计算基础设施上的ML框架中
- en: Elasticity to linearly scale compute infrastructure to thousands of accelerators
    leveraging high bandwidth networking
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性扩展计算基础设施，以线性方式扩展到数千个加速器，利用高带宽网络
- en: Orchestration of training the ML frameworks
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML框架训练的编排
- en: Large amounts of data also means a large number of resources needed for labeling,
    so let us discuss this challenge next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的数据也意味着需要大量的资源来进行标注，因此让我们接下来讨论这个挑战。
- en: Labeling large amounts of data
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标注大量数据
- en: Vehicle sensor and simulation data contain streams of images and videos, point
    clouds from RADAR and LiDAR, and time series data from inertia measuring sensors,
    GPS, and the vehicle **Controller Area Network** (**CAN**) bus. In a given test
    drive of 8 hours, an AV may collect more than 40 TB of data across these sensors.
    Depending on the company, a test fleet can range anywhere from a handful of vehicles
    to nearly 1,000\. At such a scale, AV data lakes grow at PB annually. Altogether,
    this data is indexed by time and stored as scenes or scenarios leveraged for model
    training. For the purposes of building models and agents that ultimately drive
    the vehicle, the data needs to get processed and labeled.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆传感器和仿真数据包含图像和视频流、来自雷达和激光雷达的点云，以及来自惯性测量传感器、GPS和车辆**控制器区域网络**（**CAN**）总线的时间序列数据。在8小时的特定测试驾驶中，AV可能通过这些传感器收集超过40
    TB的数据。根据公司规模，测试车队可能从几辆车到近1000辆车不等。在这样的规模下，AV数据湖每年增长PB级。总的来说，这些数据按时间索引并存储为用于模型训练的场景或情景。为了构建最终能够驾驶车辆的模型和代理，这些数据需要得到处理和标注。
- en: The first challenge is to make these labeled datasets available to the ML framework
    during training as fast as it can process a data batch. The amount of data required
    to train one model in one specific task alone can be in excess of hundreds of
    TB, making pre-fetching and loading data into memory unfeasible. The combination
    of ML framework and compute hardware accelerator dictates the speed at which a
    batch of such data gets read from the source for a specific model task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个挑战是在训练过程中尽可能快地将这些标注数据集提供给机器学习（ML）框架，以便它能够处理数据批次。仅在一个特定任务中训练一个模型所需的数据量就可能超过数百TB，这使得预先获取并将数据加载到内存中变得不可行。机器学习框架和计算硬件加速器的组合决定了从源读取此类数据批次的速度，以适应特定模型任务。
- en: 'Today, you can use several built-in labeling templates on Amazon SageMaker
    Ground Truth for labeling images, video, and LiDAR data. For a description of
    AWS services that can be used to preprocess and label high-resolution video files
    recorded, visit the linked blog in the *References* section. Specifically for
    LiDAR use cases, you can use data that has both LiDAR as well as image data captured
    in sync with the LiDAR using multiple onboard cameras. Amazon SageMaker Ground
    Truth can synchronize a frame containing 3D point cloud data with up to eight
    camera sources. Once the raw data manifest is ready, you can use SageMaker Ground
    Truth for 3D object detection, object tracking, and semantic segmentation of 3D
    point clouds. As with standard SageMaker Ground Truth labeling jobs, you can use
    a fully private workforce or a trusted vendor to complete your labeling tasks.
    *Figure 13**.9* is an example of a car being labeled using a 3D bounding box along
    with three projected side views and images from cameras corresponding to that
    timestamp:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，您可以在Amazon SageMaker Ground Truth上使用几个内置的标签模板来标注图像、视频和LiDAR数据。有关可以使用AWS服务预处理和标注高分辨率视频文件的描述，请访问*参考文献*部分中的链接。特别是对于LiDAR用例，您可以使用同时使用多个机载摄像头捕获的LiDAR以及图像数据的数据。Amazon
    SageMaker Ground Truth可以将包含3D点云数据的帧与多达八个摄像头源同步。一旦原始数据清单准备就绪，您就可以使用SageMaker Ground
    Truth进行3D目标检测、目标跟踪和3D点云的语义分割。与标准的SageMaker Ground Truth标注工作一样，您可以使用完全私有的劳动力或受信任的供应商来完成您的标注任务。*图13.9*.9是一个使用3D边界框以及三个投影侧视图和对应时间戳的摄像头图像对车辆进行标注的示例：
- en: '![Figure 13.9 – Amazon SageMaker Ground Truth labeling jobs with LiDAR and
    camera data for AV workloads](img/B18493_13_009.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图13.9 – Amazon SageMaker Ground Truth标注作业，用于自动驾驶工作负载的LiDAR和摄像头数据](img/B18493_13_009.jpg)'
- en: Figure 13.9 – Amazon SageMaker Ground Truth labeling jobs with LiDAR and camera
    data for AV workloads
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9 – Amazon SageMaker Ground Truth标注作业，用于自动驾驶工作负载的LiDAR和摄像头数据
- en: For more information on using Ground Truth to label 3D point cloud data, see
    the link in the *References* section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有关使用Ground Truth标注3D点云数据的更多信息，请参阅*参考文献*部分中的链接。
- en: Training with large amounts of data
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用大量数据进行训练
- en: Typically, DL tasks in the context of AV can be related to *perception* (finding
    information about the environment or obstacles) and *localization* (finding your
    position in the world with high accuracy). Several state-of-the-art DL models
    are being developed for detecting other vehicles, roads, pedestrians, signs, and
    objects and also to describe the position of these 2D objects around the vehicle
    in the 3D world. The *KITTI* *Benchmark* is often used to test new algorithms
    and approaches to looking at use cases related to autonomous vehicles, such as
    semantic segmentation and object detection. Access to the KITTI dataset (and other
    similar datasets such as the *Audi A2D2 Autonomous driving dataset*) can be found
    on the Registry of Open Data on AWS.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在自动驾驶车辆（AV）的背景下，深度学习（DL）任务可以与*感知*（寻找有关环境或障碍物的信息）和*定位*（以高精度确定你在世界中的位置）相关联。正在开发一些最先进的深度学习模型，用于检测其他车辆、道路、行人、标志和物体，并描述这些二维物体在3D世界中的位置。*KITTI*
    *基准测试*通常用于测试新的算法和针对自动驾驶车辆相关用例的方法，例如语义分割和目标检测。可以在AWS开放数据注册表中找到对KITTI数据集（以及其他类似的数据集，如*奥迪A2D2自动驾驶数据集*）的访问。
- en: Training large semantic segmentation and object detection models on large open
    source datasets, such as **Mask R-CNN** on the **Common Objects in Context** (**COCO**)
    dataset, can achieve throughputs of 60 images per second – approximately 35 MB/s
    – on a single multi-GPU instance. For simpler architectures, training throughput
    can reach thousands of images per second due to the smaller scale of the network
    being trained on more straightforward tasks. That is true in the case of image
    classification using *ResNet* models that make up the backbone of larger models
    such as *Mask R-CNN*. More recently, models such as *DeepManta* have been used
    to obtain high scores on other related tasks such as vehicle pose estimation –
    links to these methods and papers can be found in the *References* section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型开源数据集上训练大型语义分割和目标检测模型，例如在 **Common Objects in Context**（**COCO**）数据集上的 **Mask
    R-CNN**，可以在单个多 GPU 实例上实现每秒 60 张图像的吞吐量——大约 35 MB/s。对于更简单的架构，由于训练网络的规模较小，训练吞吐量可以达到每秒数千张图像。这在使用构成更大模型（如
    *Mask R-CNN*）骨干的 *ResNet* 模型进行图像分类的情况下是正确的。最近，一些模型如 *DeepManta* 已被用于在车辆姿态估计等其他相关任务上获得高分——这些方法和论文的链接可以在
    *参考文献* 部分找到。
- en: At lower rates of data transfer, the training job can retrieve data objects
    directly from Amazon S3\. Amazon S3 is a foundational data service for AV development
    on AWS, as discussed in this blog post on *Building an Autonomous Driving Data
    Lake* (see the *References* section).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在较低的数据传输速率下，训练作业可以直接从 Amazon S3 获取数据对象。Amazon S3 是 AWS 上自动驾驶汽车开发的基础数据服务，如本博客文章
    *构建自动驾驶数据湖* 中所述（请参阅 *参考文献* 部分）。
- en: Some data loaders provide connectivity directly to Amazon S3, such as TensorFlow,
    for TFRecord-style datasets. However, it requires optimizing the size of each
    data file as well as the number of worker processes to maximize Amazon S3 throughput,
    which can make the data loading pipeline complex and less scalable. It is possible
    to achieve hundreds of GB/s total throughputs reading directly from S3 when horizontally
    scaling the data reader, but there is a compromise on CPU utilization for the
    training process itself. This blog ([https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/))
    explains how to optimize I/O for GPU performance. Mobileye explained their use
    of TFRecord datasets and Pipe mode on Amazon SageMaker for training their large
    DNNs resulting in faster training and a 10x improvement in development time in
    their reinvent video , which is included in the *References* section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据加载器直接提供对 Amazon S3 的连接，例如 TensorFlow，用于 TFRecord 风格的数据集。然而，这需要优化每个数据文件的大小以及工作进程的数量，以最大化
    Amazon S3 的吞吐量，这可能会使数据加载管道复杂且可扩展性降低。当水平扩展数据读取器时，可以直接从 S3 读取以实现数百 GB/s 的总吞吐量，但会在训练过程的
    CPU 利用率上做出妥协。本博客 ([https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/))
    解释了如何优化 I/O 以提高 GPU 性能。Mobileye 在他们的 reinvent 视频中解释了他们在 Amazon SageMaker 上使用 TFRecord
    数据集和 Pipe 模式来训练大型 DNN，从而实现了更快的训练和开发时间提高 10 倍，该视频包含在 *参考文献* 部分。
- en: For a more straightforward architecture, which leverages the ML framework’s
    native support of *POSIX-compliant* file system interface, AWS offers **FSx for
    Lustre** (more information about FSx and POSIX compliance is linked to in the
    *References* section). FSx for Lustre is a high-throughput, low-latency distributed
    file system that can be provisioned from existing S3 data, making the whole dataset
    available to the DNN training workers as files. These files can be iterated over
    using any of the major ML framework data readers, such as PyTorch dataset or DataLoader.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更直接的架构，AWS 提供了利用 ML 框架原生支持 *POSIX 兼容* 文件系统接口的 **FSx for Lustre**（有关 FSx
    和 POSIX 兼容的更多信息，请参阅 *参考文献* 部分）。FSx for Lustre 是一个高吞吐量、低延迟的分布式文件系统，可以从现有的 S3 数据中配置，使整个数据集作为文件对
    DNN 训练工作员可用。这些文件可以使用任何主要的 ML 框架数据读取器进行迭代，例如 PyTorch 数据集或 DataLoader。
- en: FSx for Lustre can scale its baseline aggregate bandwidth to 200 GB/s for a
    1 PB training dataset, with burst speeds of 1.3 GB/s per TiB of training data.
    The larger the provisioned FSx for Lustre deployment, the higher the aggregate
    bandwidth, enabling a PB-scale network fabric. FSx for Lustre is hydrated with
    a subset of the data from the Autonomous Driving data lake and synchronized back
    using data repository tasks in case model artifacts or data transformations are
    generated and recorded during training. For a real-world example of how the Amazon
    ML Solutions Lab helped Hyundai train a model using SageMaker’s distributed training
    library and FSx for Lustre 10x faster with only 5x the number of instances, take
    a look at the link in the *References* section to this use case.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FSx for Lustre可以将其基线聚合带宽扩展到200 GB/s，用于1 PB的训练数据集，每个TiB的训练数据有1.3 GB/s的突发速度。提供的FSx
    for Lustre部署越大，聚合带宽就越高，从而实现PB级网络布线。FSx for Lustre使用来自自动驾驶数据湖的数据子集进行初始化，并在训练过程中生成和记录模型工件或数据转换时，使用数据存储库任务进行同步。关于Amazon
    ML解决方案实验室如何帮助现代汽车使用SageMaker的分布式训练库和FSx for Lustre，以10倍的速度仅用5倍的实例数量训练模型的真实世界示例，请参阅*参考文献*部分的使用案例链接。
- en: The need for a PB-scale data repository also comes from the need to scale the
    number of compute workers processing this data. At the 60 images per second rate,
    a single worker would take more than 6.5 hours to train over the 118,000 images
    in the *COCO* dataset, considering a dozen epochs to achieve reasonable accuracy.
    Scaling the number of images per training iteration is key for achieving reasonable
    training times. Even more so given the experimental and iterative nature of building
    DL based models, requiring multiple training runs for a single model to be built.
    Large-scale training generally translates to high costs of running training experiments.
    Amazon SageMaker provides cost-saving features for both training as well as deployment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个PB级数据存储库的需求也来自于需要扩展处理这些数据的计算工作者的数量。以每秒60张图片的速度，单个工作者需要超过6.5小时才能在*COCO*数据集的118,000张图片上完成训练，考虑到需要十二个epoch才能达到合理的准确率。在每次训练迭代中扩展图片数量对于实现合理的训练时间至关重要。考虑到基于深度学习的模型构建的实验性和迭代性，需要多次训练运行才能构建单个模型。大规模训练通常意味着运行训练实验的高成本。Amazon
    SageMaker为训练和部署都提供了节省成本的功能。
- en: 'Alex Bain, Lead for ML Systems at Lyft Level 5, said: “*By using Amazon SageMaker
    distributed training, we reduced our model training time from days to a couple
    of hours. By running our ML workloads on AWS, we streamlined our development cycles
    and reduced costs, ultimately accelerating our mission to deliver self-driving
    capabilities to* *our customers.*”'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Lyft Level 5的ML系统负责人Alex Bain表示：“通过使用Amazon SageMaker分布式训练，我们将模型训练时间从几天缩短到几个小时。通过在AWS上运行我们的机器学习工作负载，我们简化了我们的开发周期并降低了成本，最终加速了我们将自动驾驶能力交付给客户的使命。”
- en: 'Visit the following blog post for more examples of use cases around cost savings:
    [https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于成本节约的使用案例，请访问以下博客文章：[https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/)。
- en: Amazon SageMaker provides connections to common file systems that store data,
    such as Amazon S3, EFS, and FSx for Lustre. When running long training jobs, the
    choice of an appropriate storage service can speed up training times overall.
    If training data is already in EFS, it is common to continue preprocessing data
    on EFS and training the model by pointing SageMaker to EFS. When data is in Amazon
    S3, customers can decide to use this data directly from S3 to make use of features
    on SageMaker such as Fast File mode, Pipe mode, data shuffling, or sharding by
    S3 key for distributed training (more information about these modes is included
    in the *References* section). Customers can also use FSx for Lustre since it automatically
    makes data available to SageMaker training instances and avoids any repetitive
    copying of data. When multiple epochs use slightly different subsets of data that
    fit into instance memory, or in the case of distributed training, FSx for Lustre
    provides extremely fast and consistent access to datasets by mounting a volume
    with the data accessible to your training code.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker提供了连接到存储数据的常见文件系统的连接，例如Amazon S3、EFS和FSx for Lustre。当运行长时间的训练作业时，选择合适的存储服务可以加快整体训练时间。如果训练数据已经在EFS中，通常会在EFS上继续预处理数据，并通过将SageMaker指向EFS来训练模型。当数据在Amazon
    S3中时，客户可以选择直接从S3使用这些数据，以利用SageMaker上的功能，如快速文件模式、管道模式、数据洗牌或通过S3键进行分片以进行分布式训练（有关这些模式的更多信息请参阅*参考*部分）。客户还可以使用FSx
    for Lustre，因为它会自动使数据可供SageMaker训练实例使用，并避免任何重复的数据复制。当多个epoch使用略微不同的数据子集，这些子集适合实例内存，或者在分布式训练的情况下，FSx
    for Lustre通过挂载一个数据可供训练代码访问的卷，提供了对数据集的极快和一致访问。
- en: Scaling
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 规模化
- en: With distributed training strategies, many compute nodes within a cluster read
    batches of data, train over them, and synchronize the model parameters as the
    training goes on. The unit of compute for these clusters is not the individual
    compute instance, sometimes called a *node*, but the individual GPUs. This is
    because the DNNs require hardware acceleration for training. So, distribution
    occurs within and across multi-GPU compute instances.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布式训练策略时，集群内的许多计算节点读取数据批次，对它们进行训练，并在训练过程中同步模型参数。这些集群的计算单位不是单个计算实例，有时称为*节点*，而是单个GPU。这是因为深度神经网络（DNNs）需要硬件加速进行训练。因此，分布式发生在多GPU计算实例内部和之间。
- en: Amazon’s EC2 service provides the broadest compute platform in the cloud with
    17 distinct compute instance families. Each family is designed for a few specific
    workloads and consists of a given ratio of vCPU, GPU (for certain instances),
    memory, storage, and networking. For full, end-to-end AV development, companies
    largely rely on the C, M, R, G, and P instance families.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊的EC2服务提供了云中最广泛的计算平台，拥有17种不同的计算实例系列。每个系列都是为了少数几个特定的工作负载而设计的，并包含一定比例的vCPU、GPU（对于某些实例）、内存、存储和网络。对于完整的端到端AV开发，公司主要依赖于C、M、R、G和P实例系列。
- en: For ML model training, companies leverage the **Deep Learning Amazon Machine
    Images** (**DLAMI**) to launch NVIDIA GPU-based EC2 instances in the *P family*.
    Each EC2 P family instance generation integrates the latest NVIDIA technology,
    including the p2 instances (Tesla K80) and the p3 instances (Volta V100), and
    the recently released p4d (with Ampere A100 GPUs).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习模型的训练，公司利用**深度学习亚马逊机器镜像**（**DLAMI**）在*P系列*中启动基于NVIDIA GPU的EC2实例。每个EC2
    P系列实例代都集成了最新的NVIDIA技术，包括p2实例（Tesla K80）和p3实例（Volta V100），以及最近发布的p4d（配备A100 GPU）。
- en: 'AWS and NVIDIA continue to collaborate on achieving state-of-the-art model
    training times for data-parallel and model-parallel training (see the blog link
    in the *References* section). SageMaker distributed training includes libraries
    for distributed data-parallel and model-parallel modes of training. These libraries
    for data and model-parallel training extend SageMaker’s training capabilities,
    so you can train large-scale models with small code changes to your training scripts.
    For readers interested in this topic, the following video from the AWS Deep Engines
    team is on an AWS library that is useful for both data- and model-parallel training:
    [https://youtu.be/nz1EwsS5OiA](https://youtu.be/nz1EwsS5OiA).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: AWS和NVIDIA持续合作，以实现数据并行和模型并行训练的最先进模型训练时间（参见*参考文献*部分的博客链接）。SageMaker分布式训练包括用于分布式数据并行和模型并行训练模式的库。这些用于数据和模型并行训练的库扩展了SageMaker的训练能力，因此您可以通过对训练脚本进行少量代码更改来训练大规模模型。对于对此主题感兴趣的读者，AWS深度引擎团队以下视频介绍了AWS库，该库对数据和模型并行训练都很有用：[https://youtu.be/nz1EwsS5OiA](https://youtu.be/nz1EwsS5OiA)。
- en: At the single GPU level, optimizing the memory consumption helps increase the
    throughput. For model training on data that can be batched, this means increasing
    the number of images per iteration before running out of GPU memory. Therefore,
    the higher the GPU memory, the greater the achievable training throughput, which
    favors large memory GPU nodes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个GPU级别上，优化内存消耗有助于提高吞吐量。对于可以批处理的数据进行模型训练，这意味着在GPU内存耗尽之前，每轮迭代可以处理更多的图像。因此，GPU内存越高，可实现的训练吞吐量就越大，这有利于使用大内存GPU节点。
- en: Across GPUs, fast communication within and between instances enables faster
    synchronization of gradients during training. Networking is, therefore, a key
    aspect of scalability in enhancing the speed of each iteration step. This type
    of infrastructure is analogous to a tightly coupled HPC infrastructure.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个GPU之间，实例内部和实例之间的快速通信使得在训练过程中梯度同步更快。因此，网络是增强每个迭代步骤速度的可扩展性的关键方面。此类基础设施类似于紧密耦合的高性能计算（HPC）基础设施。
- en: AWS offers EC2 instances that support HPC and accelerated computing on the cloud.
    AWS has demonstrated the fastest training times for models such as Mask R-CNN
    and near linear scalability of large-scale GPU instances using the EC2 **p3dn.24xlarge**
    instance. This instance has 8 NVIDIA V100 GPUs, with 32 GB of memory each, and
    can make use of the **AWS Elastic Fabric Adapter** (**EFA**) network interface.
    EFA is a custom-built OS bypass hardware interface that enhances the performance
    of inter-instance communication, achieving 100 gigabits per second bandwidth per
    card and natively integrating with communication libraries such as MPI and **NVIDIA
    Collective Communication Library** (**NCCL**) used on ML applications.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供支持云上HPC和加速计算的EC2实例。AWS已经展示了Mask R-CNN等模型的最快训练时间，以及使用EC2 **p3dn.24xlarge**
    实例的大规模GPU实例的近线性可扩展性。该实例具有8个NVIDIA V100 GPU，每个GPU有32 GB的内存，并且可以利用**AWS弹性网络适配器**（**EFA**）网络接口。EFA是一个定制的操作系统旁路硬件接口，它增强了实例间通信的性能，每张卡达到100千兆比特每秒的带宽，并且与ML应用中使用的通信库（如MPI和**NVIDIA集体通信库**（**NCCL**））原生集成。
- en: AWS introduced the latest generation (in 2020) of NVIDIA GPU hardware **General
    Availability** (**GA**) with EC2 p4d instances. This instance takes ML training
    in the cloud to the next level and includes 8 NVIDIA A100 GPUs with 40 GB of memory
    per GPU and an improved networking stack. Instead of a single network interface
    card, the p4d instance has 4 EFA cards for a total of 400 gigabits per second
    bandwidth. Within the instance, the p4d family also increases GPU-to-GPU communication
    bandwidth with an NVlink mesh topology for ML frameworks using NCCL. The new p4d
    instance design provides up to 3.8x the training throughput compared to the **p3dn.24xlarge**
    on backbone models for the major computer vision tasks, such as semantic segmentation,
    used in the data labeling phase of the AV development process. For more information
    on the p4d design and benchmark results, refer to this deep dive blog.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: AWS推出了最新一代（2020年）的NVIDIA GPU硬件**通用可用性**（**GA**）与EC2 p4d实例。该实例将云中的机器学习训练提升到了新的水平，包括8个配备40
    GB内存的NVIDIA A100 GPU以及改进的网络堆栈。p4d实例不再只有一个网络接口卡，而是拥有4个EFA卡，总带宽达到每秒400 Gbps。在实例内部，p4d系列还通过NVlink网格拓扑结构增加了GPU到GPU的通信带宽，适用于使用NCCL的机器学习框架。新的p4d实例设计相比**p3dn.24xlarge**在主干模型上的训练吞吐量提高了高达3.8倍，这对于在自动驾驶车辆开发过程中的数据标注阶段使用的计算机视觉任务，如语义分割，至关重要。有关p4d设计和基准测试结果的更多信息，请参阅这篇深度分析博客。
- en: During the December 2020 AWS re:Invent conference, AWS announced plans to make
    Intel Habana Gaudi accelerators and an in-house built training chip, which will
    offer more **Terraflops** (**TFLOPS**) than any compute instance in the cloud,
    available. In 2020, AWS collaborated with NVIDIA to bring down training times
    of Mask R-CNN on the cloud to 6 minutes and 45 seconds on PyTorch and 6 minutes
    and 12 seconds with TensorFlow (see the link in the *References* section). For
    more information on data-parallel and model-parallel training with minimal code
    changes on Amazon SageMaker, see the documentation on SageMaker distributed training
    linked to in the *References* section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年12月的AWS re:Invent大会上，AWS宣布了计划使Intel Habana Gaudi加速器和自建的训练芯片可用，这些芯片将提供比云中任何计算实例更多的**万亿次浮点运算**（**TFLOPS**）。2020年，AWS与NVIDIA合作，将Mask
    R-CNN在云上的训练时间缩短到PyTorch上的6分钟45秒和TensorFlow上的6分钟12秒（见*参考文献*部分链接）。有关在Amazon SageMaker上使用最小代码更改进行数据并行和模型并行训练的更多信息，请参阅*参考文献*部分链接的SageMaker分布式训练文档。
- en: Orchestration
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编排
- en: The final challenge in training AV DNNs is managing and orchestrating the tightly
    coupled HPC infrastructure at scale. AWS provides a suite of services and solutions
    for HPC that you can leverage to build and manage a large-scale DNN training cluster
    for AV including tools such as Amazon EKS, Amazon ECS, AWS Batch, and AWS Parallel
    Cluster. These topics have been discussed in the preceding chapters in detail
    and will not be repeated here.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自动驾驶深度神经网络（DNN）的最终挑战是在大规模上管理和编排紧密耦合的高性能计算（HPC）基础设施。AWS提供了一套服务解决方案，您可以利用这些服务构建和管理大规模的自动驾驶DNN训练集群，包括Amazon
    EKS、Amazon ECS、AWS Batch和AWS Parallel Cluster等工具。这些主题在前面的章节中已经详细讨论过，此处不再重复。
- en: The infrastructure management challenge also includes the ability to integrate
    upstream and downstream tasks from the AV stack throughout the development of
    the models. As an example, the validation of a perception model stack may include
    online driving simulations. When integrating a simulation environment into a model-building
    deployment, the requirements for distributed compute change from tightly coupled
    high-performance computing to highly parallelized, embarrassingly parallel batch
    simulations and client-server architectures. Efficiently integrating services
    becomes critically important to bringing AV systems development from a research
    exercise to a scalable, production-ready pipeline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施管理挑战还包括在整个模型开发过程中集成AV堆栈的上游和下游任务的能力。例如，感知模型堆栈的验证可能包括在线驾驶模拟。当将模拟环境集成到模型构建部署中时，分布式计算的要求从紧密耦合的高性能计算转变为高度并行化的、尴尬的并行批处理模拟和客户端-服务器架构。有效地集成服务对于将自动驾驶系统开发从研究练习转变为可扩展、生产就绪的管道变得至关重要。
- en: 'These options give you the flexibility to build a scalable and diverse ML platform
    for your data scientist team with the quickest velocity and most robust infrastructure
    in the world. Regardless of whether you use a managed platform for ML such as
    Amazon SageMaker, or manage your own platform on Kubernetes, ML deployment and
    orchestration specifically for AV needs to have the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Continuous training and re-training functionality
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous deployment
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous monitoring
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information about MLOps on AWS can be found at [https://aws.amazon.com/sagemaker/mlops/](https://aws.amazon.com/sagemaker/mlops/).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'For AV specifically, here is what a typical ML workflow spanning a few weeks
    may look like:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: New or updated datasets.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset curation and scene selection.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-labeling and data curation, where pre-trained models are used to provide
    coarse quality labeled data to human labelers, and in some cases where sensitive
    information is obfuscated from image and video datasets.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Labeling and active learning (more information at [https://aws.amazon.com/sagemaker/data-labeling/](https://aws.amazon.com/sagemaker/data-labeling/)).
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed training for various tasks (see [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*).
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model testing:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Software-in-the-loop testing
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware-in-the-loop testing
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: On-road testing
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset collection, and go back to *step 1.*
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more information about these steps, you may be interested in reading about
    how an actual customer of AWS, Aurora, achieves self-driving capabilities using
    their Aurora Driver platform, at [https://www.cnet.com/roadshow/news/aurora-drive-aws-autonomous-vehicle-development/](https://www.cnet.com/roadshow/news/aurora-drive-aws-autonomous-vehicle-development/)
    and [https://www.youtube.com/watch?v=WAELZY_TJ04](https://www.youtube.com/watch?v=WAELZY_TJ04).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18493_06.xhtml#_idTextAnchor116), *Distributed Training of Machine
    Learning Models*, discussed distributed training, and [*Chapter 8*](B18493_08.xhtml#_idTextAnchor161),
    *Optimizing and Managing Machine Learning Models for Edge Deployment*, discussed
    model deployment at the edge; these are both very relevant to the topics listed
    in the previous steps (*steps 5–9*). Within model testing, software-in-the-loop
    testing can be done using tools on AWS, and this will be discussed in the next
    section using a hands-on example that you can follow along on your AWS account.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Software-in-the-loop (SITL) simulation
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we will be discussing one very specific type of **Software-in-the-Loop**
    (**SITL**) testing that is useful for AV customers. Note that this is not the
    only type of simulation that is being run by AV customers around the world today.
    Some may involve perception tasks, planning or mapping tasks, and also end-to-end
    software tasks before moving on to **Hardware-in-the-Loop** (**HITL**) or on-road
    testing.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will walk through how you can set up a high-fidelity simulation
    of a driving environment and even test out some DL models for AV within the simulation
    environment! To do this, you have to follow two high-level steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何设置一个高保真驾驶环境模拟，甚至可以在模拟环境中测试一些AV的DL模型！为此，您必须遵循两个高级步骤：
- en: Create a container with your simulation environment.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含您的模拟环境的容器。
- en: Use RoboMaker to run your simulation.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用RoboMaker运行您的模拟。
- en: Once you set this up, you can interactively work with your simulation, use the
    environment as part of a RL loop, or even generate synthetic data for your future
    ML experiments.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，您可以交互式地与您的模拟进行工作，将环境作为RL循环的一部分使用，甚至为未来的ML实验生成合成数据。
- en: 'Before we walk through the steps, here are some basics:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍步骤之前，这里有一些基础知识：
- en: '*What is AWS RoboMaker?* **RoboMaker** is a cloud-based simulation service
    where you can run your simulations without managing any infrastructure. More information
    can be found at [https://aws.amazon.com/robomaker/](https://aws.amazon.com/robomaker/).
    RoboMaker provides GPU-based compute for high-fidelity simulators discussed in
    the following point.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是AWS RoboMaker？* **RoboMaker**是一个基于云的模拟服务，您可以在其中运行模拟而无需管理任何基础设施。更多信息可以在[https://aws.amazon.com/robomaker/](https://aws.amazon.com/robomaker/)找到。RoboMaker为以下要点中讨论的高保真模拟器提供基于GPU的计算。'
- en: Specific to AV and manufacturing-related simulations, RoboMaker lets you use
    simulators such as CARLA (which we will be using in this section), AirSim, Aslan,
    Summit, DeepDrive or *Ignition*, Drake or NVIDIA Isaac Sim, and even custom simulations
    that you create using high-fidelity rendering engines such as Unity or Unreal
    Engine. Links to these simulators and tools can be found in the *References* section.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对AV和制造相关的模拟，RoboMaker允许您使用模拟器，例如CARLA（我们将在本节中使用），AirSim，Aslan，Summit，DeepDrive或*Ignition*，Drake或NVIDIA
    Isaac Sim，甚至您使用高保真渲染引擎（如Unity或Unreal Engine）创建的定制模拟。这些模拟器和工具的链接可以在*参考文献*部分找到。
- en: '**CARLA** is an open source project that is commonly used in AV studies for
    simulating vehicles in environments and testing out DL or RL models for AV. CARLA
    exposes an API that lets users control all aspects of the simulation, such as
    driving, environment, traffic, and pedestrians, and also lets users configure
    sensors on vehicles, such as LiDARs, cameras, and GPS.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CARLA**是一个开源项目，常用于AV研究中模拟环境中的车辆，并测试AV的DL或RL模型。CARLA提供了一个API，允许用户控制模拟的所有方面，如驾驶、环境、交通和行人，并允许用户在车辆上配置传感器，如激光雷达、摄像头和GPS。'
- en: Great, let us now get started with the steps required to run CARLA on RoboMaker!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在让我们开始运行CARLA在RoboMaker上的步骤！
- en: Step 1 – build and push the CARLA container to Amazon ECR
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 - 构建并将CARLA容器推送到Amazon ECR
- en: 'To build our custom simulation container, we need two files:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的定制模拟容器，我们需要两个文件：
- en: A Dockerfile
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Dockerfile
- en: A shell script to build and push the customer’s Docker container to ECR (you
    can use your own pipeline to do this step as well)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个shell脚本，用于构建并将客户的Docker容器推送到ECR（您也可以使用自己的管道来完成此步骤）
- en: Here, we will be building the container on a SageMaker notebook instance, but
    you may use the same script from your local laptop or an EC2 instance provided
    you have the right permissions set up. To demonstrate this step, we will assume
    that you’re aware of SageMaker notebooks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将在SageMaker笔记本实例上构建容器，但您也可以使用来自本地笔记本电脑或EC2实例的相同脚本，前提是您已设置正确的权限。为了演示这一步骤，我们假设您已经熟悉SageMaker笔记本。
- en: 'Here is what the Dockerfile looks like for CARLA when viewed from a notebook
    instance. The `writefile` command writes the following code into a new file named
    `Dockerfile`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是查看CARLA Dockerfile时的样子。`writefile`命令将以下代码写入一个名为`Dockerfile`的新文件中：
- en: '[PRE0]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, we start off with the CARLA base image and install some additional
    dependencies:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们首先使用CARLA基础镜像并安装一些额外的依赖项：
- en: '[PRE1]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The second file that is required is a script to build and push the container.
    First, we define the arguments (the name of the container) and some inputs to
    the script, such as region, account, and the full name of the Docker container
    to be built:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个必需的文件是一个构建并推送容器的脚本。首先，我们定义脚本的参数（容器名称）和一些输入，例如区域、账户以及要构建的Docker容器的全名：
- en: '[PRE2]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we create the repository and log into it:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建仓库并登录：
- en: '[PRE3]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we build and push the container to ECR:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将构建并将容器推送到ECR：
- en: '[PRE4]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This shell script takes the name of your container as an argument, builds the
    container based on a local Dockerfile, and pushes the container to a repository
    in ECR. Once you have these two scripts on a notebook instance or anywhere with
    Docker installed, you can run the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will output the location of the container you just built, similar to the
    following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Copy this output, as you will need it in *Step 2 – configure and run CARLA*
    *on RoboMaker*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – configure and run CARLA on RoboMaker
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To configure and run CARLA simulations on RoboMaker, go through the following
    steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **AWS RoboMaker** console on AWS – [https://aws.amazon.com/robomaker/](https://aws.amazon.com/robomaker/)
    (sign in to your AWS account if asked).
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, look for `carsimrole`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Create a simulation job on AWS RoboMaker](img/B18493_13_010.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Create a simulation job on AWS RoboMaker
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'On the same screen, scroll down to the compute options and make sure you select
    CPU and GPU, with sliders corresponding to `s3://carlasim-bucket/output`; make
    sure you browse to a bucket that you have access to and not the one shown in this
    example:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Configure compute options and output location for your simulation
    job](img/B18493_13_011.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Configure compute options and output location for your simulation
    job
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Specify robot application** page, select **None** for robot application
    (see *Figure 13**.12*):'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Select None for robot application](img/B18493_13_012.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Select None for robot application
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Move on to the **Specify Simulation application** page, as shown in *Figure
    13**.13*:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Create a new simulation application](img/B18493_13_013.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Create a new simulation application
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **Create new application**, and add the ECR repository link you copied
    from *step 1*. If you already have a simulation application, you can choose it
    from the drop-down menu, as shown in *Figure 13**.14*:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.14 – Configure Simulation application by selecting it from the
    drop-down menu](img/B18493_13_014.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: Figure 13.14 – Configure Simulation application by selecting it from the drop-down
    menu
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to the **Simulation application configuration** section, enter
    the following as your launch command, as shown in *Figure 13**.15*, and remember
    to check the option for running with a streaming session:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 13.15 – Enter the launch command in the Simulation application configuration
    section](img/B18493_13_015.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Enter the launch command in the Simulation application configuration
    section
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Simulation application tools** section, create two terminals with
    the options highlighted in *Figure 13**.16*:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.16 – Add two custom tools for terminal access](img/B18493_13_016.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Figure 13.16 – Add two custom tools for terminal access
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, click **Next** to go to the summary screen, and then click **Create**.
    This process of creating your simulation environment will take a few minutes.
    Once it is created, you should see your simulation job was created, as seen in
    *Figure 13**.17*:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，点击**下一步**进入摘要屏幕，然后点击**创建**。创建您的模拟环境的过程将需要几分钟。一旦创建完成，您应该会看到您的模拟作业已创建，如图*图13**.17*所示：
- en: '![Figure 13.17 – Wait for the simulation job to be created](img/B18493_13_017.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图13.17 – 等待创建模拟作业](img/B18493_13_017.jpg)'
- en: Figure 13.17 – Wait for the simulation job to be created
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.17 – 等待创建模拟作业
- en: Once the simulation is created and the **Status** field says **Running**, click
    the **Connect** button in the **Simulation application** section, as shown in
    *Figure 13**.18*. You can also access the terminal for running scripts or monitoring
    the simulation environment.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模拟创建完成，并且**状态**字段显示为**运行中**，点击**模拟应用程序**部分中的**连接**按钮，如图*图13**.18*所示。您还可以访问终端来运行脚本或监控模拟环境。
- en: '![Figure 13.18 – Connect to the main simulation application or the terminals
    created](img/B18493_13_018.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图13.18 – 连接到主模拟应用程序或创建的终端](img/B18493_13_018.jpg)'
- en: Figure 13.18 – Connect to the main simulation application or the terminals created
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.18 – 连接到主模拟应用程序或创建的终端
- en: Click `PythonAPI` folder, as shown in *Figure 13**.19*.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**PythonAPI**文件夹，如图*图13**.19*所示。
- en: '![Figure 13.19 – CARLA Python examples inside the simulation job](img/B18493_13_019.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图13.19 – 模拟作业内的CARLA Python示例](img/B18493_13_019.jpg)'
- en: Figure 13.19 – CARLA Python examples inside the simulation job
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.19 – 模拟作业内的CARLA Python示例
- en: Visit the getting started guide to understand the Python API as well as included
    examples ([https://carla.readthedocs.io/en/0.9.2/getting_started/](https://carla.readthedocs.io/en/0.9.2/getting_started/)).
    Some examples are provided in *Figure 13**.20* to *Figure 13**.22*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 访问入门指南以了解Python API以及包含的示例（[https://carla.readthedocs.io/en/0.9.2/getting_started/](https://carla.readthedocs.io/en/0.9.2/getting_started/))。*图13**.20*到*图13**.22*提供了示例。
- en: '*Figure 13.20* shows a sample application where you can manually drive a Tesla
    Cybertruck in the CARLA simulation with 264 other vehicles:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图13.20*展示了一个示例应用，您可以在CARLA模拟中手动驾驶特斯拉赛博卡车，与其他264辆车一起：'
- en: '![Figure 13.20 – Driving a Tesla Cybertruck manually in a CARLA simulation](img/B18493_13_020.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图13.20 – 在CARLA模拟中手动驾驶特斯拉赛博卡车](img/B18493_13_020.jpg)'
- en: Figure 13.20 – Driving a Tesla Cybertruck manually in a CARLA simulation
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.20 – 在CARLA模拟中手动驾驶特斯拉赛博卡车
- en: '*Figure 13.21* shows a sample application that uses a Python program through
    one of the terminal applications we created to spawn traffic:'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图13.21*展示了一个示例应用，它通过我们创建的一个终端应用程序使用Python程序生成交通：'
- en: '![Figure 13.21 – Spawning traffic in the CARLA simulation world using Python
    code](img/B18493_13_021.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图13.21 – 使用Python代码在CARLA模拟世界中生成交通](img/B18493_13_021.jpg)'
- en: Figure 13.21 – Spawning traffic in the CARLA simulation world using Python code
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.21 – 使用Python代码在CARLA模拟世界中生成交通
- en: '*Figure 13.22* shows an application that simulates LiDAR data around a vehicle
    in the simulation:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图13.22*展示了一个模拟应用程序，它模拟了模拟中车辆周围的激光雷达数据：'
- en: '![Figure 13.22 – Simulated LiDAR data around the car](img/B18493_13_022.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图13.22 – 汽车周围的模拟激光雷达数据](img/B18493_13_022.jpg)'
- en: Figure 13.22 – Simulated LiDAR data around the car
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.22 – 模拟汽车周围的激光雷达数据
- en: 'As a next step, read how you can use RL models to control your car for self-driving
    use cases in this tutorial: [https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/](https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，阅读如何在教程中了解如何使用RL模型来控制您的汽车以用于自动驾驶用例：[https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/](https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/)
- en: Let’s summarize all that we’ve learned so far in this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章到目前为止所学到的所有内容。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed AV and ADAS systems at a high level, along with
    a reference architecture to build some of these systems on AWS. We also discussed
    the three main challenges practitioners face when training AV-related ML models
    in the cloud: feeding TB or more of training data to ML frameworks running on
    a large-scale, high-performance computing infrastructure, elasticity to linearly
    scale compute infrastructure to thousands of accelerators leveraging high bandwidth
    networking, and orchestrating the ML framework training.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we walked you through examples of how you can make use of tools on AWS
    to run SITL simulations for testing your ML models.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on solving numerical optimization problems
    on AWS.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about topics discussed in this chapter, visit the following
    links:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '*Autonomous Vehicle and ADAS development on AWS Part 1: Achieving* *Scale*:
    [https://aws.amazon.com/blogs/industries/autonomous-vehicle-and-adas-development-on-aws-part-1-achieving-scale/](https://aws.amazon.com/blogs/industries/autonomous-vehicle-and-adas-development-on-aws-part-1-achieving-scale/)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Building an Autonomous Driving and ADAS Data Lake on* *AWS*: [https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Implementing Hardware-in-the-Loop for Autonomous Driving Development on* *AWS*:
    [https://aws.amazon.com/blogs/architecture/field-notes-implementing-hardware-in-the-loop-for-autonomous-driving-development-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-implementing-hardware-in-the-loop-for-autonomous-driving-development-on-aws/)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advanced Driver Assistance Systems (**ADAS)*: [https://www.gartner.com/en/information-technology/glossary/advanced-driver-assistance-systems-adass](https://www.gartner.com/en/information-technology/glossary/advanced-driver-assistance-systems-adass)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CARLA* *Documentation*: [https://carla.readthedocs.io/en/0.9.2/getting_started/](https://carla.readthedocs.io/en/0.9.2/getting_started/)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Highly Automated and Autonomous Vehicle Development with Amazon Web* *Services*:
    [https://pages.awscloud.com/rs/112-TZM-766/images/Autonomous_Vehicle_Development_with_AWS.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Autonomous_Vehicle_Development_with_AWS.pdf))'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS IoT* *FleetWise*: [https://aws.amazon.com/iot-fleetwise/](https://aws.amazon.com/iot-fleetwise/)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lyft Increases Simulation Capacity, Lowers Costs Using Amazon EC2 Spot* *Instances*:
    [https://aws.amazon.com/solutions/case-studies/Lyft-level-5-spot/](https://aws.amazon.com/solutions/case-studies/Lyft-level-5-spot/)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Autonomous Driving Data Lake Reference* *Architecture*: [https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/autonomous-driving-data-lake-ra.pdf?did=wp_card&trk=wp_card](https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/autonomous-driving-data-lake-ra.pdf?did=wp_card&trk=wp_card)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automating Data Ingestion and Labeling for Autonomous Vehicle* *Development*:
    [https://aws.amazon.com/blogs/architecture/field-notes-automating-data-ingestion-and-labeling-for-autonomous-vehicle-development/](https://aws.amazon.com/blogs/architecture/field-notes-automating-data-ingestion-and-labeling-for-autonomous-vehicle-development/)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use Ground Truth to Label 3D Point* *Clouds*: [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mask RCNN* *paper*: [https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*COCO* *dataset*: [https://cocodataset.org](https://cocodataset.org)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*KITTI dataset*: [https://registry.opendata.aws/kitti/](https://registry.opendata.aws/kitti/)
    and [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A2D2* *dataset*: [https://registry.opendata.aws/aev-a2d2/](https://registry.opendata.aws/aev-a2d2/)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ResNet*: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DeepManta*: [https://arxiv.org/abs/1703.07570](https://arxiv.org/abs/1703.07570)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vehicle Pose Estimation on KITTI Cars* *Hard*: [https://paperswithcode.com/sota/vehicle-pose-estimation-on-kitti-cars-hard](https://paperswithcode.com/sota/vehicle-pose-estimation-on-kitti-cars-hard).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Building an Autonomous Driving and ADAS Data Lake on* *AWS*: [https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/](https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TFRecord* *dataset*: [https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance Design Patterns for Amazon* *S3*: [https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moving our Machine Learning to the Cloud Inspired* *Innovation*: [https://www.mobileye.com/blog/moving-our-machine-learning-to-the-cloud-inspired-innovation/](https://www.mobileye.com/blog/moving-our-machine-learning-to-the-cloud-inspired-innovation/)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lustre User* *Guide*: [https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html](https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pytorch* *DataLoader*: [https://pytorch.org/docs/stable/data.html](https://pytorch.org/docs/stable/data.html)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exporting changes to the data* *repository*: [https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-changed-data-meta-dra.html](https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-changed-data-meta-dra.html)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hyundai reduces ML model training time for autonomous driving models using
    Amazon* *SageMaker*: [https://aws.amazon.com/de/blogs/machine-learning/hyundai-reduces-training-time-for-autonomous-driving-models-using-amazon-sagemaker/](https://aws.amazon.com/de/blogs/machine-learning/hyundai-reduces-training-time-for-autonomous-driving-models-using-amazon-sagemaker/)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Access Training* *Data*: [https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html](https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon EC2 P4* *Instances*: [https://aws.amazon.com/ec2/instance-types/p4/](https://aws.amazon.com/ec2/instance-types/p4/)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS and NVIDIA achieve the fastest training times for Mask R-CNN and* *T5-3B*:
    [https://aws.amazon.com/blogs/machine-learning/aws-and-nvidia-achieve-the-fastest-training-times-for-mask-r-cnn-and-t5-3b/](https://aws.amazon.com/blogs/machine-learning/aws-and-nvidia-achieve-the-fastest-training-times-for-mask-r-cnn-and-t5-3b/)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVLink and* *NVSwitch*: [https://www.nvidia.com/en-us/data-center/nvlink/](https://www.nvidia.com/en-us/data-center/nvlink/)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA Collective Communications Library (**NCCL)*: [https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AWS* *re:Invent*: [https://reinvent.awsevents.com/](https://reinvent.awsevents.com/)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon EC2 DL1* *Instances*: [https://aws.amazon.com/ec2/instance-types/habana-gaudi/](https://aws.amazon.com/ec2/instance-types/habana-gaudi/)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distributed training* *libraries*: [https://aws.amazon.com/sagemaker/distributed-training/](https://aws.amazon.com/sagemaker/distributed-training/)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CARLA*: [http://carla.org/](http://carla.org/)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AirSim*: [https://github.com/microsoft/AirSim](https://github.com/microsoft/AirSim)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Project* *Aslan*: [https://github.com/project-aslan/Aslan](https://github.com/project-aslan/Aslan)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SUMMIT* *Simulator*: [https://github.com/AdaCompNUS/summit](https://github.com/AdaCompNUS/summit)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deepdrive*: [https://github.com/deepdrive/deepdrive](https://github.com/deepdrive/deepdrive)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gazebo*: [https://gazebosim.org/home](https://gazebosim.org/home)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Drake*: [https://drake.mit.edu/](https://drake.mit.edu/)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*NVIDIA Isaac* *Sim*: [https://developer.nvidia.com/isaac-sim](https://developer.nvidia.com/isaac-sim)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unity*: [https://unity.com/](https://unity.com/)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unreal* *Engine*: [https://www.unrealengine.com/](https://www.unrealengine.com/)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
