<html><head></head><body>
        

                            
                    <h1 class="header-title">Deep Learning with OpenCV</h1>
                
            
            
                
<p>Deep learning is a state-of-the-art form of machine learning that is reaching its best accuracy in image classification and speech recognition. Deep learning is also used in other fields, such as robotics and artificial intelligence with reinforcement learning. This is the main reason OpenCV is making significant efforts to include deep learning at its core. We are going to learn the basic use of OpenCV deep learning interfaces and look at using them in two use cases: object detection and face detection.</p>
<p class="mce-root">In this chapter, we are going to learn the basics of deep learning and see how to use it in OpenCV. To reach our objective, we are going to learn object detection and classification using the <strong>you only look once </strong>(<strong>YOLO</strong>) algorithm.</p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li class="mce-root">What is deep learning?</li>
<li class="mce-root">How OpenCV works with deep learning and implementing deep learning <strong>neural networks</strong>(<strong>NN</strong>s)</li>
<li class="mce-root">YOLO – a very fast deep learning object detection algorithm</li>
<li>Face detection using Single Shot Detector</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>To follow the chapter with ease, it is required that you install OpenCV with the deep learning module compiled. If you do not have this module, you will not be able to compile and run the sample codes.</p>
<p>It's very useful to have an NVIDIA GPU with CUDA support. You can enable CUDA on OpenCV to improve the speed of training and detection.</p>
<p class="mce-root">Finally, you can download the code used in this chapter from <a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12</a>.</p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2SmbWf7">http://bit.ly/2SmbWf7</a></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction to deep learning</h1>
                
            
            
                
<p class="mce-root">Deep learning is most commonly written about in scientific papers nowadays with regards to image classification and speech recognition. This is a subfield of machine learning, based on traditional neural networks and inspired by the structure of the brain. To understand this technology, it is very important to understand what a neural network is and how it works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is a neural network and how can we learn from data?</h1>
                
            
            
                
<p class="mce-root">The neural network is inspired by the structure of the brain, in which multiple neurons are interconnected, creating a network. Each neuron has multiple inputs and multiple outputs, like a biological neuron.</p>
<p class="mce-root">This network is distributed in layers, and each layer contains a number of neurons that are connected to all the previous layer's neurons. This always has an input layer, which normally consists of the features that describe the input image or data, and an output layer, which normally consists of the result of our classification. The other middle layers are called <strong>hidden layers</strong>. The following diagram shows a basic three-layer neural network in which the input layer contains three neurons, the output layer contains two neurons, and a hidden layer contains four neurons:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/f5ed96eb-8a0a-4957-adf4-b31b8e65075d.png" style="width:23.25em;height:28.00em;"/></p>
<p class="mce-root">The neuron is the basic element of a neural network and it uses a simple mathematical formula that we can see in the following diagram:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/16287cff-6b21-429b-b6af-74a22e2719db.png" style="width:28.92em;height:19.08em;"/></p>
<p class="mce-root">As we can see, for each neuron, <strong>i</strong>, we mathematically add all the previous neuron's output, which is the input of neuron <strong>i</strong> (<strong>x1</strong>, <strong>x2</strong>...), by a weight (<strong>wi1</strong>, <strong>wi2</strong>...) plus a bias value, and the result is the argument of an activation function, <strong>f</strong>. The final result is the output of <strong>i</strong> neuron:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/2d237c49-7c71-4b51-8d9f-e550d882ccca.png" style="width:33.17em;height:1.50em;"/></p>
<p class="mce-root">The most common activation functions (<strong>f</strong>) on classical neural networks are the sigmoid function or linear functions. The sigmoid function is used most often, and it looks as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/ad7a4ce4-bf8b-410f-beb6-d196b485d28b.png" style="width:30.00em;height:16.75em;"/></p>
<p class="mce-root">But how can we learn a neural network with this formula and these connections? How do we classify input data? The learn algorithm of neural networks can be called <strong>supervised</strong> if we know the desired output; while learning, the input pattern is given to the net's input layer. Initially, we set up all weights as random numbers and send the input features into the network, checking the output result. If this is wrong, we have to adjust all the weights of the network to get the correct output. This algorithm is called <strong>backpropagation</strong>. If you want to read more about how a neural network learns, check out <a href="http://neuralnetworksanddeeplearning.com/chap2.html">http://neuralnetworksanddeeplearning.com/chap2.html</a> and <a href="https://youtu.be/IHZwWFHWa-w">https://youtu.be/IHZwWFHWa-w</a>.</p>
<p class="mce-root">Now that we have a brief introduction to what a neural network is and the internal architecture of NN, we are going to explore the differences between NN and deep learning.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Convolutional neural networks</h1>
                
            
            
                
<p class="mce-root">Deep learning neural networks have the same background as the classical neural network. However, in the case of image analysis, the main difference is the input layer. In a classical machine learning algorithm, the researcher has to identify the best features that define the image target to classify. For example, if we want to classify numbers, we could extract the borders and lines of numbers in each image, measure the area of an object in an image, and all of these features are the input of the neural network, or any other machine learning algorithm. However, in deep learning, you don't have to explore what the features are; instead, you use whole image as an input of the neural network directly. Deep learning can learn what the most important features are and <strong>deep neural networks</strong> (<strong>DNN</strong>) are able to detect an image or input and recognize it.</p>
<p class="mce-root">To learn what these features are, we use one of the most important layers in deep learning and neural networks: the <strong>convolutional layer</strong>. A convolutional layer works like a convolutional operator, where a kernel filter is applied to the whole previous layer, giving us a new filtered image, like a sobel operator:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/98fbb3b3-3557-40ed-9dd4-0805262af590.png"/></p>
<p class="mce-root">However, in a convolutional layer we can define different parameters, and one of them is the number of filters and the sizes we want to apply to the previous layer or image. These filters are calculated in the learning step, just like the weights on a classical neural network. This is the magic of deep learning: it can extract the most significant features from labeled images.</p>
<p class="mce-root">However, these convolutional layers are the main reason behind the name <strong>deep</strong>, and we are going to see why in the following basic example. Imagine we have a 100 x 100 image. In a classical neural network, we will extract the most relevant features we can imagine from the input image. This will normally approximately 1,000 features, and with each hidden layer we can increase or decrease this number, but the number of neurons to calculate its weights is reasonable to compute in a normal computer. However, in deep learning, we normally start applying a convolutional layer – with a 64 filter kernels of 3 x 3 size. This will generate a new layer of 100 x 100 x 64 neurons with 3 x 3 x 64 weights to calculate. If we continue adding more and more layers, these numbers quickly increase and require huge computing power to learn the good weights and parameters of our deep learning architecture.</p>
<p class="mce-root">Convolutional layers are one of the most important aspects of the deep learning architecture, but there are also other important layers, such as <strong>Pooling</strong>, <strong>Dropout</strong>, <strong>Flatten</strong>, and <strong>Softmax</strong>. In the following diagram, we can see a basic deep learning architecture in which some convolutional and pooling layers are stacked:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/4aaa0318-f894-4bfb-8b1f-f52c66642517.png" style="width:43.75em;height:17.00em;"/></p>
<p class="mce-root">However, there is one more very important thing that makes deep learning get the best results: the amount of labeled data. If you have a small dataset, a deep learning algorithm will not help you in your classification because there is not enough data to learn the features (the weights and parameters of your deep learning architecture). However, if you have tons of data, you will get very good results. But take care, you will need a lot of time to compute and learn the weights and parameters of your architecture. This is why deep learning was not used early in the process, because computing requires a lot of time. However, thanks to new parallel architectures, such as NVIDIA GPUs, we can optimize the learning backpropagation and speed up the learning tasks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deep learning in OpenCV</h1>
                
            
            
                
<p class="mce-root">The deep learning module was introduced to OpenCV in version 3.1 as a contribute module. This was moved to part of OpenCV in 3.3, but it was not widely adopted by developers until versions 3.4.3 and 4.</p>
<p class="mce-root">OpenCV implements deep learning only for inference, which means that you cannot create your own deep learning architecture and train in OpenCV; you can only import a pre-trained model, execute it under OpenCV library, and use it as <strong>feedforward</strong> (inference) to obtain the results.</p>
<p class="mce-root">The most important reason to implement the feedforward method is to optimize OpenCV to speed up computing time and performance in inference. Another reason to not implement backward methods is to avoid wasting time developing something that other libraries, such as TensorFlow or Caffe, are specialized in. OpenCV then created importers for the most important deep learning libraries and frameworks to make it possible to import pre-trained models.</p>
<p class="mce-root">Then if you wish to create a new deep learning model to use in OpenCV, you first have to create and train it using the TensorFlow, Caffe, Torch, or DarkNet frameworks or a framework that you can use to export your model in an <strong>Open Neural Network Exchange</strong> (<strong>ONNX</strong>) format. Creating a model with this framework can be easy or complex depending on the framework you use, but essentially you have to stack multiple layers like we did in the previous diagram, setting the parameters and the function required by the DNN. Nowadays there are other tools to help you to create your models without coding, such as <a href="https://www.tensoreditor.com">https://www.tensoreditor.com</a> or <a href="https://lobe.ai/">lobe.ai</a>. TensorEditor allows you to download the TensorFlow code generated from a visual design architecture to train in your computer or in the cloud. In the following screenshot, we can see TensorEditor:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/df035e2c-2f4a-45dc-8a76-e0d649a005e8.png"/></p>
<p class="mce-root">When you have your model trained and you are comfortable with the results, you can import it to OpenCV directly to predict new input images. In the next section, you will see how to import and use deep learning models in OpenCV.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">YOLO – real-time object detection</h1>
                
            
            
                
<p class="mce-root">To learn how to use deep learning in OpenCV, we are going to present an example of object detection and classification based on the YOLO algorithm. This is one of the fastest object detection and recognition algorithms, which can run at around 30 fps in an NVIDIA Titan X.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">YOLO v3 deep learning model architecture</h1>
                
            
            
                
<p class="mce-root">Common object detection in classical computer vision uses a sliding window to detect objects, scanning a whole image with different window sizes and scales. The main problem here is the huge time consumption in scanning the image several times to find objects.</p>
<p class="mce-root">YOLO uses a different approach by dividing the diagram into an S x S grid. For each grid, YOLO checks for B bounding boxes, and then the deep learning model extracts the bounding boxes for each patch, the confidence to contain a possible object, and the confidence of each category in the training dataset per each box. The following screenshot shows the S x S grid:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/d372b7ab-1d10-4ca4-b591-82d73bdc8d8a.png" style="width:44.42em;height:26.42em;"/></p>
<p class="mce-root">YOLO is trained with a grid of 19 and 5 bounding boxes per grid using 80 categories. Then, the output result is 19 x 19 x 425, where 425 comes from the data of bounding box (x, y, width, height), the object confidence, and the 80 classes, confidence multiplied by the number of boxes per grid; <em>5_bounding boxes</em>*(<em>x</em>,<em>y</em>,<em>w</em>,<em>h</em>,<em>object</em>_<em>confidence</em>, <em>classify</em>_<em>confidence</em>[<em>80</em>])=<em>5</em>*(<em>4</em> + <em>1</em> + <em>80</em>):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/5b1e7274-d759-46a3-a7ec-b09c9cffdc9f.png" style="width:31.00em;height:40.25em;"/></p>
<p class="mce-root">The YOLO v3 architecture is based on DarkNet, which contains 53 layer networks, and YOLO adds 53 more layers for a total of 106 network layers. If you want a faster architecture, you can check version 2 or TinyYOLO versions, which use fewer layers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The YOLO dataset, vocabulary, and model</h1>
                
            
            
                
<p class="mce-root">Before we start to import the model into our OpenCV code, we have to obtain it through the YOLO website: <a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a>. This provides pre-trained model files based on the <strong>COCO</strong> dataset, which contains 80 object categories, such as person, umbrella, bike, motorcycle, car, apple, banana, computer, and chair.</p>
<p class="mce-root">To get all the names of categories and uses for visualization, check out <a href="https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true">https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true</a>.</p>
<p class="mce-root">The names are in the same order as the results of deep learning model confidences. If you want to see some images of the COCO dataset by category, you can explore the dataset at <a href="http://cocodataset.org/#explore">http://cocodataset.org/#explore</a>, and download some of them to test our sample application.</p>
<p class="mce-root">To get the model configuration and pre-trained weights, you have to download the following files:</p>
<ul>
<li class="mce-root"><a href="https://pjreddie.com/media/files/yolov3.weights">https://pjreddie.com/media/files/yolov3.weights</a></li>
<li class="mce-root"><a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true">https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true</a></li>
</ul>
<p class="mce-root">Now we are ready to start to import the models into OpenCV.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Importing YOLO into OpenCV</h1>
                
            
            
                
<p class="mce-root">The deep learning OpenCV module is found under the <kbd>opencv2/dnn.hpp</kbd> header, which we have to include in our source header and in <kbd>cv::dnn namespace</kbd>.</p>
<p class="mce-root">Then our header for OpenCV must look like this:</p>
<pre class="mce-root">...<br/>#include &lt;opencv2/core.hpp&gt;<br/>#include &lt;opencv2/dnn.hpp&gt;<br/>#include &lt;opencv2/imgproc.hpp&gt;<br/>#include &lt;opencv2/highgui.hpp&gt;<br/>using namespace cv;<br/>using namespace dnn;<br/>...</pre>
<p class="mce-root">The first thing we have to do is import the COCO name's vocabulary, which is in the <kbd>coco.names</kbd> file. This file is a plaintext file that contains one class category per line, and is ordered in the same way as the confidence results. Then we are going to read each line of this file and store it in a vector of strings, called classes:</p>
<pre class="mce-root">...<br/> int main(int argc, char** argv)<br/> {<br/>     // Load names of classes<br/>     string classesFile = "coco.names";<br/>     ifstream ifs(classesFile.c_str());<br/>     string line;<br/>     while (getline(ifs, line)) classes.push_back(line);<br/>     ...<br/> </pre>
<p class="mce-root">Now we are going to import the deep learning model into OpenCV. OpenCV implements the most common readers/importers for deep learning frameworks, such as TensorFlow and DarkNet, and all of them have a similar syntax. In our case, we are going to import a DarkNet model using the weights, and the model using the <kbd>readNetFromDarknet</kbd> OpenCV function:</p>
<pre class="mce-root">...<br/> // Give the configuration and weight files for the model<br/> String modelConfiguration = "yolov3.cfg";<br/> String modelWeights = "yolov3.weights";<br/>// Load the network<br/>Net net = readNetFromDarknet(modelConfiguration, modelWeights);<br/>...</pre>
<p class="mce-root">Now we are in a position to read an image and send the deep neural network to inference. First we have to read an image with the <kbd>imread</kbd> function and convert it into a tensor/blob data that can read the <strong>DotNetNuke</strong> (<strong>DNN</strong>). To create the blob from an image, we are going to use the <kbd>blobFromImage</kbd> function by passing the image. This function accepts the following parameters:</p>
<ul>
<li class="mce-root"><strong>image</strong>: Input image (with 1, 3, or 4 channels).</li>
<li class="mce-root"><strong>blob</strong>: Output <kbd>mat</kbd>.</li>
<li class="mce-root"><strong>scalefactor</strong>: Multiplier for image values.</li>
<li class="mce-root"><strong>size</strong>: Spatial size for output blob required as input of DNN.</li>
<li class="mce-root"><strong>mean</strong>: Scalar with mean values that are subtracted from channels. Values are intended to be in (mean-R, mean-G, and mean-B) order if the image has BGR ordering and <kbd>swapRB</kbd> is true.</li>
<li class="mce-root"><strong>swapRB</strong>: A flag that indicates to swap the first and last channels in a 3-channel image is necessary.</li>
<li class="mce-root"><strong>crop</strong>: A flag that indicates whether the image will be cropped after resize.</li>
</ul>
<p>You can read the full code on how to read and convert an image into a blob in the following snippet:</p>
<pre class="mce-root">...<br/>input= imread(argv[1]);<br/>// Stop the program if reached end of video<br/>if (input.empty()) {<br/>    cout &lt;&lt; "No input image" &lt;&lt; endl;<br/>    return 0;<br/>}<br/>// Create a 4D blob from a frame.<br/>blobFromImage(input, blob, 1/255.0, Size(inpWidth, inpHeight), Scalar(0,0,0), true, false);<br/>...<br/> </pre>
<p class="mce-root">Finally, we have to feed the blob into Deep Net and call the inference with the <kbd>forward</kbd> function, which requires two parameters: the out <kbd>mat</kbd> results, and the names of the layers that the output needs to retrieve:</p>
<pre class="mce-root">...<br/>//Sets the input to the network<br/>net.setInput(blob);<br/> <br/>// Runs the forward pass to get output of the output layers<br/>vector&lt;Mat&gt; outs;<br/>net.forward(outs, getOutputsNames(net));<br/>// Remove the bounding boxes with low confidence<br/>postprocess(input, outs);<br/>...</pre>
<p class="mce-root">In the <kbd>mat</kbd> output vector, we have all bounding boxes detected by the neural network and we have to post-process the output to get only the results that have a confidence greater than a threshold, normally 0.5, and finally apply non-maximum suppression to eliminate redundant overlapping boxes. You can get the full post-process code on GitHub.</p>
<p class="mce-root">The final result of our example is multiple-object detection and classification in deep learning that shows a window similar to the following:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/e8f14417-bfe1-45c4-8f1b-2b5d5502b3c3.jpg"/></p>
<p>Now we are going to learn another commonly-used object detection function customized for face detection.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Face detection with SSD</h1>
                
            
            
                
<p><strong>Single Shot Detection</strong> (<strong>SSD</strong>) is another fast and accurate deep learning object-detection method with a similar concept to YOLO, in which the object and bounding box are predicted in the same architecture.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">SSD model architecture</h1>
                
            
            
                
<p>The SSD algorithm is called single shot because it predicts the bounding box and the class simultaneously as it processes the image in the same deep learning model. Basically, the architecture is summarized in the following steps:</p>
<ol>
<li>A 300 x 300 image is input into the architecture.</li>
<li>The input image is passed through multiple convolutional layers, obtaining different features at different scales.</li>
<li>For each feature map obtained in 2, we use a 3 x 3 convolutional filter to evaluate small set of default bounding boxes.</li>
<li>For each default box evaluated, the bounding box offsets and class probabilities are predicted.</li>
</ol>
<p>The model architecture looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/35723e83-ec4e-46a3-99d6-bd32b4b67a98.png"/></p>
<p>SSD is used for predicting multiple classes similar to that in YOLO, but it can be modified to detect a single object, changing the last layer and training for only one class – this is what we used in our example, a re-trained model for face detection, where only one class is predicted.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Importing SSD face detection into OpenCV</h1>
                
            
            
                
<p>To work with deep learning in our code, we have to import the corresponding headers:</p>
<pre>#include &lt;opencv2/dnn.hpp&gt;<br/>#include &lt;opencv2/imgproc.hpp&gt;<br/>#include &lt;opencv2/highgui.hpp&gt;</pre>
<p>After that, we will import the required namespaces:</p>
<pre>using namespace cv;<br/>using namespace std;<br/>using namespace cv::dnn;</pre>
<p>Now we are going to define the input image size and constant that we are going to use in our code:</p>
<pre>const size_t inWidth = 300;<br/>const size_t inHeight = 300;<br/>const double inScaleFactor = 1.0;<br/>const Scalar meanVal(104.0, 177.0, 123.0);</pre>
<p>In this example, we need a few parameters as input, such as the model configuration and pre-trained model, if we are going to process camera or video input. We also need the minimum confidence to accept a prediction as correct or not:</p>
<pre>const char* params<br/>= "{ help | false | print usage }"<br/>"{ proto | | model configuration (deploy.prototxt) }"<br/>"{ model | | model weights (res10_300x300_ssd_iter_140000.caffemodel) }"<br/>"{ camera_device | 0 | camera device number }"<br/>"{ video | | video or image for detection }"<br/>"{ opencl | false | enable OpenCL }"<br/>"{ min_confidence | 0.5 | min confidence }";</pre>
<p>Now, we are going to start with the <kbd>main</kbd> function, where we are going to parse the arguments with the <kbd>CommandLineParser</kbd> function:</p>
<pre>int main(int argc, char** argv)<br/>{<br/> CommandLineParser parser(argc, argv, params);<br/><br/> if (parser.get&lt;bool&gt;("help"))<br/> {<br/> cout &lt;&lt; about &lt;&lt; endl;<br/> parser.printMessage();<br/> return 0;<br/> }</pre>
<p>We are also going to load the model architecture and pre-trained model files, and load the model in a deep learning network:</p>
<pre> String modelConfiguration = parser.get&lt;string&gt;("proto");<br/> String modelBinary = parser.get&lt;string&gt;("model");<br/><br/> //! [Initialize network]<br/> dnn::Net net = readNetFromCaffe(modelConfiguration, modelBinary);<br/> //! [Initialize network]</pre>
<p>It's very important to check that we have imported the network correctly. We must also check whether the model is imported, using the <kbd>empty</kbd> function, as follows:</p>
<pre>if (net.empty())<br/> {<br/> cerr &lt;&lt; "Can't load network by using the following files" &lt;&lt; endl;<br/> exit(-1);<br/> }</pre>
<p>After loading our network, we are going to initialize our input source, a camera or video file, and load into <kbd>VideoCapture</kbd>, as follows:</p>
<pre> VideoCapture cap;<br/> if (parser.get&lt;String&gt;("video").empty())<br/> {<br/> int cameraDevice = parser.get&lt;int&gt;("camera_device");<br/> cap = VideoCapture(cameraDevice);<br/> if(!cap.isOpened())<br/> {<br/> cout &lt;&lt; "Couldn't find camera: " &lt;&lt; cameraDevice &lt;&lt; endl;<br/> return -1;<br/> }<br/> }<br/> else<br/> {<br/> cap.open(parser.get&lt;String&gt;("video"));<br/> if(!cap.isOpened())<br/> {<br/> cout &lt;&lt; "Couldn't open image or video: " &lt;&lt; parser.get&lt;String&gt;("video") &lt;&lt; endl;<br/> return -1;<br/> }<br/> }</pre>
<p>Now we are prepared to start capturing frames and processing each one into the deep neural network to find faces.</p>
<p>First of all, we have to capture each frame in a loop:</p>
<pre>for(;;)<br/> {<br/> Mat frame;<br/> cap &gt;&gt; frame; // get a new frame from camera/video or read image<br/><br/> if (frame.empty())<br/> {<br/> waitKey();<br/> break;<br/> }</pre>
<p>Next, we will put the input frame into a <kbd>Mat</kbd> blob structure that can manage the deep neural network. We have to send the image with the proper size of SSD, which is 300 x 300 (we will have initialized the <kbd>inWidth</kbd> and <kbd>inHeight</kbd> constant variables already) and we subtract from the input image a mean value, which is required in the SSD using the defined <kbd>meanVal</kbd> constant variable:</p>
<pre>Mat inputBlob = blobFromImage(frame, inScaleFactor, Size(inWidth, inHeight), meanVal, false, false); </pre>
<p>Now we are ready to set the data into the network and get the predictions/detections using the <kbd>net.setInput</kbd> and <kbd>net.forward</kbd> functions, respectively. This converts the detection results into a detection <kbd>mat</kbd> that we can read, where <kbd>detection.size[2]</kbd> is the number of detected objects and <kbd>detection.size[3]</kbd> is the number of results per detection (bounding box data and confidence):</p>
<pre> net.setInput(inputBlob, "data"); //set the network input<br/> Mat detection = net.forward("detection_out"); //compute output<br/> Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr&lt;float&gt;());<br/> </pre>
<p>The <kbd>Mat</kbd> detection contains the following per each row:</p>
<ul>
<li>
<p><strong>Column 0</strong>: Confidence of object being present</p>
</li>
<li>
<p><strong>Column 1</strong>: Confidence of bounding box</p>
</li>
<li>
<p><strong>Column 2</strong>: Confidence of face detected</p>
</li>
<li>
<p><strong>Column 3</strong>: X bottom-left bounding box</p>
</li>
<li>
<p><strong>Column 4</strong>: Y bottom-left bounding box</p>
</li>
<li>
<p><strong>Column 5</strong>: X top-right bounding box</p>
</li>
<li>
<p><strong>Column 6</strong>: Y top-right bounding box</p>
</li>
</ul>
<p>The bounding box is relative (zero to one) to the image size.</p>
<p>Now we have to apply the threshold to get only the desired detections based on the defined input threshold:</p>
<pre>float confidenceThreshold = parser.get&lt;float&gt;("min_confidence");<br/> for(int i = 0; i &lt; detectionMat.rows; i++)<br/> {<br/> float confidence = detectionMat.at&lt;float&gt;(i, 2);<br/><br/> if(confidence &gt; confidenceThreshold)<br/> {</pre>
<p>Now we are going to extract the bounding box, draw a rectangle over each detected face, and show it as follows:</p>
<pre> int xLeftBottom = static_cast&lt;int&gt;(detectionMat.at&lt;float&gt;(i, 3) * frame.cols);<br/> int yLeftBottom = static_cast&lt;int&gt;(detectionMat.at&lt;float&gt;(i, 4) * frame.rows);<br/> int xRightTop = static_cast&lt;int&gt;(detectionMat.at&lt;float&gt;(i, 5) * frame.cols);<br/> int yRightTop = static_cast&lt;int&gt;(detectionMat.at&lt;float&gt;(i, 6) * frame.rows);<br/><br/> Rect object((int)xLeftBottom, (int)yLeftBottom, (int)(xRightTop - xLeftBottom), (int)(yRightTop - yLeftBottom));<br/><br/> rectangle(frame, object, Scalar(0, 255, 0));<br/> }<br/> }<br/> imshow("detections", frame);<br/> if (waitKey(1) &gt;= 0) break;<br/>}</pre>
<p>The final result looks like this:</p>
<div><img src="img/6abd2d7b-110b-44a2-9507-9d7c07d1e330.png" style="width:39.83em;height:31.67em;"/></div>
<p>In this section, you learned a new deep learning architecture, SSD, and how to use it for face detection.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we learned what deep learning is and how to use it on OpenCV with object detection and classification. This chapter is a foundation for working with other models and deep neural networks for any purpose.</p>
<p>This book taught you how to obtain and compile OpenCV, how to use the basic image and <kbd>mat</kbd> operations, and how to create your own graphical user interfaces. You used basic filters and applied all of them in an industrial inspection example. We looked at how to use OpenCV for face detection and how to manipulate it to add masks. Finally, we introduced you to very complex use cases of object tracking, text segmentation, and recognition. Now you are ready to create your own applications in OpenCV, thanks to these use cases, which show you how to apply each technique or algorithm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p class="mce-root">To learn more about deep learning in OpenCV, check out <em>Object Detection and Recognition Using Deep Learning in OpenCV</em> by <em>Packt Publishing</em>.</p>


            

            
        
    </body></html>