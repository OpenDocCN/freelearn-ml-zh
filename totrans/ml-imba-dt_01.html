<html><head></head><body>
		<div id="_idContainer017">
			<h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Introduction to Data Imbalance in Machine Learning</h1>
			<p>Machine learning algorithms have helped solve real-world problems as diverse as disease prediction and online shopping. However, many problems we would like to address with machine learning involve imbalanced datasets. In this chapter, we will discuss and define imbalanced datasets, explaining how they differ from other types of datasets. The ubiquity of imbalanced data will be demonstrated with examples of common problems and scenarios. We will also go through the basics of machine learning and cover the essentials, such as loss functions, regularization, and feature engineering. We will also learn about common evaluation metrics, particularly those that can be very helpful for imbalanced datasets. We will then introduce the <span class="No-Break"><strong class="source-inline">imbalanced-learn</strong></span><span class="No-Break"> library.</span></p>
			<p>In particular, we will learn about the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introduction to <span class="No-Break">imbalanced datasets</span></li>
				<li>Machine <span class="No-Break">learning 101</span></li>
				<li>Types of datasets <span class="No-Break">and splits</span></li>
				<li>Common <span class="No-Break">evaluation metrics</span></li>
				<li>Challenges and considerations when dealing with <span class="No-Break">imbalanced data</span></li>
				<li>When can we have an imbalance <span class="No-Break">in datasets?</span></li>
				<li>Why can imbalanced data be <span class="No-Break">a challenge?</span></li>
				<li>When to not worry about <span class="No-Break">data imbalance</span></li>
				<li>Introduction to the <span class="No-Break"><strong class="source-inline">imbalanced-learn</strong></span><span class="No-Break"> library</span></li>
				<li>General rules <span class="No-Break">to follow</span></li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Technical requirements</h1>
			<p>In this chapter, we will utilize common libraries such as <strong class="source-inline">numpy</strong> and <strong class="source-inline">scikit-learn</strong> and introduce the <strong class="source-inline">imbalanced-learn</strong> library. The code and notebooks for this chapter are available on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01</a>. You can fire up the GitHub notebook using Google Colab by clicking on the <strong class="bold">Open in Colab</strong> icon at the top of this chapter’s notebook or by launching it from <a href="https://colab.research.google.com">https://colab.research.google.com</a> using the GitHub URL of <span class="No-Break">the notebook.</span></p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Introduction to imbalanced datasets</h1>
			<p>Machine learning algorithms learn from collections <a id="_idIndexMarker000"/>of examples that we call <strong class="bold">datasets</strong>. <a id="_idIndexMarker001"/>These datasets contain multiple data samples or points, which we may refer to as examples, samples, or instances interchangeably throughout <span class="No-Break">this book.</span></p>
			<p>A dataset can be said to have a balanced distribution when all the target classes have a similar number of examples, as shown in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer005" class="IMG---Figure">
					<img src="image/B17259_01_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Balanced distribution with an almost equal number of examples for each class</p>
			<p>Imbalanced datasets or skewed datasets are those that have some target classes (also called labels) that outnumber the rest of the classes (<span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>). Though this generally applies to classification problems (for example, fraud detection) in machine learning, they inevitably occur in regression problems (for example, house price <span class="No-Break">prediction) too:</span></p>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B17259_01_02.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – An imbalanced dataset with five classes and a varying number of samples</p>
			<p>We label the <a id="_idIndexMarker002"/>class with more instances as the “majority” or “negative” class and the one with fewer instances as the “minority” or “positive” class. Most of the time, our main interest lies in the minority class, which is why we often refer to the minority class as the “positive” class and to the majority class as the “<span class="No-Break">negative” class:</span></p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B17259_01_03.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – A visual guide to common terminology used in imbalanced classification</p>
			<p>This can be scaled to more than two classes, and such classification problems are called multi-class <a id="_idIndexMarker003"/>classification. In the first half of this book, we will focus our attention only on binary class classification to keep the material easier to grasp. It’s relatively easy to extend the concepts to <span class="No-Break">multi-class classification.</span></p>
			<p>Let’s look at a few examples of <span class="No-Break">imbalanced datasets:</span></p>
			<ul>
				<li><strong class="bold">Fraud detection</strong> is where <a id="_idIndexMarker004"/>fraudulent transactions need to be detected out of several transactions. This problem is often encountered and widely used in finance, healthcare, and <span class="No-Break">e-commerce industries.</span></li>
				<li><strong class="bold">Network intrusion detection</strong> using <a id="_idIndexMarker005"/>machine learning involves analyzing large volumes of network traffic data to detect and prevent instances of unauthorized access and misuse of <span class="No-Break">computer systems.</span></li>
				<li><strong class="bold">Cancer detection</strong>. Cancer is not rare, but we still may want to use machine learning <a id="_idIndexMarker006"/>to analyze medical data to identify potential cases of cancer earlier and improve <span class="No-Break">treatment outcomes.</span></li>
			</ul>
			<p>In this book, we would like to focus on the class imbalance problem in general and look at various solutions where we see that class imbalance is affecting the performance of our model. A typical problem is that models perform quite poorly on the minority classes for which the model has seen a very low number of examples during <span class="No-Break">model training.</span></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Machine learning 101</h1>
			<p>Let’s do a <a id="_idIndexMarker007"/>quick overview of machine learning and its <span class="No-Break">related fields:</span></p>
			<ul>
				<li><strong class="bold">Artificial intelligence</strong> is the superset of all intelligence-related problems. Classical <a id="_idIndexMarker008"/>machine learning encompasses <a id="_idIndexMarker009"/>problems that can be solved by training traditional classical models (such as decision trees or logistic regression) and predicting the target values. They typically work on tabular data, require extensive feature engineering (manual development of features), and are less effective on text and image data. Deep learning tends to do far better on image, text, speech, and video data, wherein, typically, no manual feature engineering is needed, and various layers in the neural network automatically do feature engineering <span class="No-Break">for us.</span></li>
				<li>In <strong class="bold">supervised learning</strong>, we have both inputs and outputs (labels) in the dataset, and <a id="_idIndexMarker010"/>the model learns to predict the <a id="_idIndexMarker011"/>output during the training. Each input can be represented as a list of features. The output or labels can be a finite set of classes (classification), a real number (regression), or something more complex. A classic example of supervised learning in classification is the Iris flowers classification. In this case, the dataset includes features such as petal length, petal width, sepal length, and sepal width, and the labels are the species of the <a id="_idIndexMarker012"/>Iris flowers (setosa, versicolor, or virginica). A model can <a id="_idIndexMarker013"/>be trained on this dataset and then be used to classify new, unseen Iris flowers as one of <span class="No-Break">these species.</span></li>
				<li>In <strong class="bold">unsupervised learning</strong>, models either don’t have access to the labels or don’t use <a id="_idIndexMarker014"/>the labels and then try to make <a id="_idIndexMarker015"/>some predictions – for example, clustering the examples in the dataset into <span class="No-Break">different groups.</span></li>
				<li>In <strong class="bold">reinforcement learning</strong>, the model tries to learn by making mistakes and optimizing <a id="_idIndexMarker016"/>a goal or profit variable. An example would <a id="_idIndexMarker017"/>be training a model to play chess and adjusting its strategy based on feedback received through rewards <span class="No-Break">and penalties.</span></li>
			</ul>
			<p>In supervised learning (which is the focus of this book), there are two main types of problems: classification and regression. Classification problems involve categorizing data into predefined classes or labels, such as “fraud” or “non-fraud” and “spam” or “non-spam.” On the other hand, regression problems aim to predict a continuous variable, such as the price of <span class="No-Break">a house.</span></p>
			<p>While data <a id="_idIndexMarker018"/>imbalance can also affect regression problems, this book will concentrate solely on classification problems. This focus is due to several factors, such as the limited scope of this book and the well-established techniques available for classification. In some cases, you might even be able to reframe a regression problem as a classification problem, making the methods discussed in this book <span class="No-Break">still relevant.</span></p>
			<p>When it comes to various kinds of models that are popular for classification problems, we have quite a few categories of classical supervised machine <span class="No-Break">learning models:</span></p>
			<ul>
				<li><strong class="bold">Logistic regression</strong>: This <a id="_idIndexMarker019"/>is a supervised <a id="_idIndexMarker020"/>machine learning algorithm that’s used for binary classification problems. It predicts the probability of a binary target variable based on a set of predictor variables (features) by fitting a logistic function to the data, which outputs a value between 0 <span class="No-Break">and 1.</span></li>
				<li><strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVMs</strong>): These are supervised machine learning algorithms <a id="_idIndexMarker021"/>that are mainly used for classification and can be extended to regression <a id="_idIndexMarker022"/>problems. SVMs classify data by finding the optimal hyperplane that maximally separates the different classes in the input data, thus making it a powerful tool for binary and multiclass <span class="No-Break">classification tasks.</span></li>
				<li><strong class="bold">K-Nearest Neighbors</strong> (<strong class="bold">KNN</strong>): This is a supervised machine learning algorithm that’s <a id="_idIndexMarker023"/>used for classification and regression analysis. It predicts the target variable based <a id="_idIndexMarker024"/>on the <em class="italic">k</em>-nearest neighbors in the training dataset. The value of <em class="italic">k</em> determines the number of neighbors to consider when making a prediction, and it can be tuned to optimize the <span class="No-Break">model’s performance.</span></li>
				<li><strong class="bold">Tree models</strong>: These are a type of supervised machine learning algorithm that’s used for <a id="_idIndexMarker025"/>classification and regression analysis. They recursively split the data into smaller subsets based on <a id="_idIndexMarker026"/>the most important features to create a decision tree that predicts the target variable based on the <span class="No-Break">input features.</span></li>
				<li><strong class="bold">Ensemble models</strong>: These combine multiple individual models to improve predictive <a id="_idIndexMarker027"/>accuracy and reduce overfitting (explained later in this chapter). Ensemble techniques <a id="_idIndexMarker028"/>include bagging (for example, random forest), boosting (for example, XGBoost), and stacking. They are commonly used for classification as well as <span class="No-Break">regression analysis.</span></li>
				<li><strong class="bold">Neural networks</strong>: These <a id="_idIndexMarker029"/>models are <a id="_idIndexMarker030"/>inspired by the human brain, consist of multiple layers with numerous neurons in each, and are capable of learning complex functions. We will discuss these in more detail in <a href="B17259_06.xhtml#_idTextAnchor185"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Data Imbalance in </em><span class="No-Break"><em class="italic">Deep Learning</em></span><span class="No-Break">.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em> displays the decision boundaries of various classifiers we have reviewed so far. It shows that <a id="_idIndexMarker031"/>logistic regression has a linear decision boundary, while tree-based models such as decision trees, random forests, and XGBoost work by dividing examples into axis-parallel rectangles to form their decision boundary. SVM, on the other hand, transforms the data to a different space so that it can plot its non-linear decision boundary. Neural networks have a non-linear <span class="No-Break">decision boundary:</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B17259_01_04.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – The decision boundaries of popular machine learning algorithms on an imbalanced dataset</p>
			<p>Next, we’ll <a id="_idIndexMarker032"/>delve into the principles underlying the process of <span class="No-Break">model training.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>What happens during model training?</h2>
			<p>In the training phase of a machine learning model, we provide a dataset consisting of examples, each with <a id="_idIndexMarker033"/>input features and a corresponding label, to the model. Let <span class="_-----MathTools-_Math_Variable">X</span> represent the list of features used for training, and <span class="_-----MathTools-_Math_Variable">y</span> be the list of labels in the training dataset. The goal of the model is to learn a function, <span class="_-----MathTools-_Math_Variable">f</span>, such that <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break">.</span></p>
			<p>The model has adjustable parameters, denoted as <span class="_-----MathTools-_Math_Variable">θ</span>, which are fine-tuned during the training process. The error <a id="_idIndexMarker034"/>function, commonly referred to as the <strong class="bold">loss function</strong>, is defined as <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Operator">;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">)</span>. This error function needs to be minimized by a learning algorithm, which finds the optimal setting of these <span class="No-Break">parameters, </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">θ</span></span><span class="No-Break">.</span></p>
			<p>In classification problems, our typical loss functions are cross-entropy loss (also called the <span class="No-Break">log loss</span><span class="No-Break">):</span></p>
			<p><span class="_-----MathTools-_Math_Variable">CrossEntropyLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space">               </span><span class="_-----MathTools-_Math_Variable">if</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space">     </span><span class="_-----MathTools-_Math_Variable">otherwise</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>Here, <span class="_-----MathTools-_Math_Variable">p</span> is the predicted probability from the model when <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span><span class="No-Break">.</span></p>
			<p>When the model’s prediction closely agrees with the target label, the loss function will approach zero. However, when the prediction deviates significantly from the target, the loss can become arbitrarily large, indicating a poor <span class="No-Break">model fit.</span></p>
			<p>As training progresses, the training loss keeps going down (<span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B17259_01_05.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Rate of change of the loss function as training progresses</p>
			<p>This brings us to the concept of the fit of <span class="No-Break">a model:</span></p>
			<ul>
				<li>A model is said to <strong class="bold">underfit</strong> if it is too <a id="_idIndexMarker035"/>simple and can’t capture the data’s complexity. It performs poorly on both training and <span class="No-Break">new data.</span></li>
				<li>A model is of <strong class="bold">right fit</strong> if it accurately <a id="_idIndexMarker036"/>captures data patterns without learning noise. It performs well on both training and <span class="No-Break">new data.</span></li>
				<li>An <strong class="bold">overfit</strong> model is <a id="_idIndexMarker037"/>too complex and learns noise along with data patterns. It performs well on training data but poorly on new data (<span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">):</span></li>
			</ul>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B17259_01_06.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Underfit, right fit, and overfit models for classification task</p>
			<p>Next, let’s briefly <a id="_idIndexMarker038"/>try to learn about two important concepts in <span class="No-Break">machine learning:</span></p>
			<ul>
				<li><strong class="bold">Regularization</strong> is a set of techniques that are used to prevent the overfitting of a model <a id="_idIndexMarker039"/>to the training data. One type of regularization (namely L1 or L2) adds a penalty term to the loss function, which encourages the model to have smaller weights and reduces its complexity. This helps prevent the model from fitting too closely to the training data and generalizes better to <span class="No-Break">unseen data.</span></li>
				<li><strong class="bold">Feature engineering</strong> is the process of selecting and transforming the input features of <a id="_idIndexMarker040"/>a model to improve its performance. Feature engineering involves selecting the most relevant features for the problem, transforming them to make them more informative, and creating new features from the existing ones. Good feature engineering can make a huge difference in the performance of a model and can often be more important than the choice of algorithm <span class="No-Break">or hyperparameters.</span></li>
			</ul>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Types of dataset and splits</h1>
			<p>Typically, we train our model on the training set and test the model on an independent unseen dataset called the test set. We do this to do a fair evaluation of the model. If we don’t do this <a id="_idIndexMarker041"/>and train the model on the full dataset and evaluate the model <a id="_idIndexMarker042"/>on the same dataset, we don’t know how good the model would do on unseen data, plus the model will likely <span class="No-Break">be overfitted.</span></p>
			<p>We may encounter three kinds of datasets in <span class="No-Break">machine learning:</span></p>
			<ul>
				<li><strong class="bold">Training set</strong>: A dataset <a id="_idIndexMarker043"/>on which the model <span class="No-Break">is trained.</span></li>
				<li><strong class="bold">Validation set</strong>: A dataset <a id="_idIndexMarker044"/>used for tuning the hyperparameters of the model. A validation set is often referred to as a <span class="No-Break">development set.</span></li>
				<li><strong class="bold">Evaluation set or test set</strong>: A dataset <a id="_idIndexMarker045"/>used for evaluating <a id="_idIndexMarker046"/>the performance of <span class="No-Break">the model.</span></li>
			</ul>
			<p>When working with small example datasets, it’s common to allocate 80% of the data for the training set, 10% for the validation set, and 10% for the test set. However, the specific ratio between training and test sets is not as important as ensuring that the test set is large enough to provide statistically meaningful evaluation results. In the context of big data, a split of 98%, 1%, and 1% for training, validation, and test sets, respectively, could <span class="No-Break">be appropriate.</span></p>
			<p>Often, people don’t have a dedicated validation set for hyperparameter tuning and refer to the test set as an evaluation set. This can happen when the hyperparameter tuning is not performed as a part of the regular training cycle and is a <span class="No-Break">one-off activity.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Cross-validation</h2>
			<p><strong class="bold">Cross-validation</strong> can be a <a id="_idIndexMarker047"/>confusing term to guess its meaning. Breaking it down: cross + validation, so it’s some sort of validation on an extended (cross) something. <em class="italic">Something</em> here is the test set <span class="No-Break">for us.</span></p>
			<p>Let’s see what <span class="No-Break">cross-validation is:</span></p>
			<ul>
				<li>Cross-validation is a technique that’s used to estimate how accurately a model will perform <span class="No-Break">in practice</span></li>
				<li>It is typically used to detect overfitting – that is, failing to generalize patterns in data, particularly when the amount of data may <span class="No-Break">be limited</span></li>
			</ul>
			<p>Let’s look at the <a id="_idIndexMarker048"/>different types <span class="No-Break">of cross-validation:</span></p>
			<ul>
				<li><strong class="bold">Holdout</strong>: In the <a id="_idIndexMarker049"/>holdout method, we randomly assign data points to two sets, usually called the training set and the test set, respectively. We then train (build a model) on the <em class="italic">training set</em> and test (evaluate its performance) on the <span class="No-Break"><em class="italic">test set</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">k-fold</strong>: This works <a id="_idIndexMarker050"/><span class="No-Break">as follows:</span><ul><li>We randomly shuffle <span class="No-Break">the data.</span></li><li>We divide <a id="_idIndexMarker051"/>all the data into <em class="italic">k</em> parts, also known as folds. We train the model on <em class="italic">k</em>-1 folds and evaluate it on the remaining fold. We record the performance of this model using our chosen model evaluation metric, then discard <span class="No-Break">this model.</span></li><li>We repeat this process <em class="italic">k</em> times, each time holding out a different subset for testing. We take an average of the evaluation metric values (for example, accuracy) from all the previous models. This average represents the overall performance measure of <span class="No-Break">the model.</span></li></ul></li>
			</ul>
			<p><strong class="bold">k-fold cross-validation</strong> is mainly <a id="_idIndexMarker052"/>used when you have limited data points, say 100 points. Using 5 or 10 folds is the most common when <span class="No-Break">doing cross-validation.</span></p>
			<p>Let’s look at the common evaluation metrics in machine learning, with a special focus on the ones relevant to problems with <span class="No-Break">imbalanced data.</span></p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor023"/>Common evaluation metrics</h1>
			<p>Several machine <a id="_idIndexMarker053"/>learning and deep learning metrics are used for evaluating the performance of <span class="No-Break">classification models.</span></p>
			<p>Let’s look at some of the helpful metrics that can help evaluate the performance of our model on the <span class="No-Break">test set.</span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Confusion matrix</h2>
			<p>Given a model <a id="_idIndexMarker054"/>that tries to classify an example <a id="_idIndexMarker055"/>as belonging to the positive or negative class, there are <span class="No-Break">four possibilities:</span></p>
			<ul>
				<li><strong class="bold">True Positive (TP)</strong>: This occurs when the model correctly predicts a sample as part of the positive class, which is its <span class="No-Break">actual classification</span></li>
				<li><strong class="bold">False Negative (FN)</strong>: This happens when the model incorrectly classifies a sample from the positive class as belonging to the <span class="No-Break">negative class</span></li>
				<li><strong class="bold">True Negative (TN)</strong>: This refers to instances where the model correctly identifies a sample as part of the negative class, which is its <span class="No-Break">actual classification</span></li>
				<li><strong class="bold">False Positive (FP)</strong>: This occurs when the model incorrectly predicts a sample from the negative class as belonging to the <span class="No-Break">positive class</span></li>
			</ul>
			<p><em class="italic">Table 1.1</em> shows in what ways the model can get “confused” when making predictions, aptly called the <strong class="bold">confusion matrix</strong>. The confusion matrix forms the basis of many common metrics in <span class="No-Break">machine learning:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Predicted Positive</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Predicted Negative</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Actually Positive</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>True <span class="No-Break">Positive (TP)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>False <span class="No-Break">Negative (FN)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Actually Negative</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>False <span class="No-Break">Positive (FP)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>True <span class="No-Break">Negative (TN)</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – Confusion matrix</p>
			<p>Let’s look <a id="_idIndexMarker056"/>at some of the most common metrics <a id="_idIndexMarker057"/>in <span class="No-Break">machine learning:</span></p>
			<ul>
				<li><strong class="bold">True Positive Rate</strong> (<strong class="bold">TPR</strong>) measures the proportion of actual positive examples correctly classified by <span class="No-Break">the model:</span><p class="list-inset">TPR = <span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Positives</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">classified</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">correctly</span><span class="_-----MathTools-_Math_Variable_v-normal">  </span><span class="_-----MathTools-_Math_Base">______________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Total</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">positives</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Space"> </span>= <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">FN</span></span></p></li>
				<li><strong class="bold">False Positive Rate</strong> (<strong class="bold">FPR</strong>) measures the proportion of actual negative examples that are incorrectly identified as positives by <span class="No-Break">the model:</span><p class="list-inset"><span class="_-----MathTools-_Math_Variable_v-normal">FPR</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Negatives</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">classified</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">incorrectly</span><span class="_-----MathTools-_Math_Variable_v-normal">  </span><span class="_-----MathTools-_Math_Base">_______________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable_v-normal">Total</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">negatives</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span> = <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">TN</span></span></p></li>
				<li><strong class="bold">Accuracy</strong>: Accuracy is the fraction of correct predictions made out of all the predictions that the model makes. This is mathematically equal to <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TN</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">___________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TN</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FN</span><span class="_-----MathTools-_Math_Base">)</span>. This functionality is available in the <strong class="source-inline">sklearn</strong> library <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">sklearn.metrics.accuracy_score</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Precision</strong>: Precision, as an evaluation metric, measures the proportion of true positives over the number of items that the model predicts as belonging to the positive class, which is equal to the sum of the true positives and false positives. A high precision score indicates that the model has a low rate of false positives, meaning that when it predicts a positive result, it is usually correct. You can find this functionality in the <strong class="source-inline">sklearn</strong> library under the name <strong class="source-inline">sklearn.metrics.precision_score</strong>. <span class="_-----MathTools-_Math_Variable_v-normal">Precision</span><span class="_-----MathTools-_Math_Space"> </span>= <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">FP</span></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Recall (or sensitivity, or true positive rate)</strong>: Recall measures the proportion of true positives over the number of items that belong to the positive class. The number of items that belong to the positive class is equal to the sum of true positives and false negatives. A high recall score indicates that the model has a low rate of false negatives, meaning that it correctly identifies most of the positive instances. It is especially important to measure recall when the cost of mislabeling a positive example as negative (false negatives) <span class="No-Break">is high.</span><p class="list-inset">Recall measures <a id="_idIndexMarker058"/>the model’s ability to correctly <a id="_idIndexMarker059"/>detect all the positive instances. Recall can be considered to be the accuracy of the positive class in binary classification. You can find this functionality in the <strong class="source-inline">sklearn</strong> library under the name <strong class="source-inline">sklearn.metrics.recall_score</strong>. <span class="_-----MathTools-_Math_Variable_v-normal">Recal</span><span class="_-----MathTools-_Math_Variable_v-normal">l</span><span class="_-----MathTools-_Math_Space"> </span>= <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">FN</span></span><span class="No-Break">.</span></p><p class="list-inset"><em class="italic">Table 1.2</em> summarizes the <a id="_idIndexMarker060"/>differences between precision <a id="_idIndexMarker061"/><span class="No-Break">and recall:</span></p></li>
			</ul>
			<table id="table002" class="Basic-Table _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="Basic-Table">
						<td class="Basic-Table"/>
						<td class="Basic-Table">
							<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
						</td>
						<td class="Basic-Table">
							<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
						</td>
					</tr>
					<tr class="Basic-Table">
						<td class="Basic-Table">
							<p><span class="No-Break">Definition</span></p>
						</td>
						<td class="Basic-Table">
							<p>Precision is a measure <span class="No-Break">of trustworthiness</span></p>
						</td>
						<td class="Basic-Table">
							<p>Recall is a measure <span class="No-Break">of completeness</span></p>
						</td>
					</tr>
					<tr class="Basic-Table">
						<td class="Basic-Table">
							<p>Question <span class="No-Break">to ask</span></p>
						</td>
						<td class="Basic-Table">
							<p>When the model says something is positive, how often is <span class="No-Break">it right?</span></p>
						</td>
						<td class="Basic-Table">
							<p>Out of all the positive instances, how many did the model <span class="No-Break">correctly identify?</span></p>
						</td>
					</tr>
					<tr class="Basic-Table">
						<td class="Basic-Table">
							<p>Example (using an <span class="No-Break">email filter)</span></p>
						</td>
						<td class="Basic-Table">
							<p>Precision measures how many of the emails the model flags as spam are actually spam, as a percentage of all the <span class="No-Break">flagged emails</span></p>
						</td>
						<td class="Basic-Table">
							<p>Recall measures how many of the actual spam emails the model catches, as a percentage of all the spam emails in <span class="No-Break">the dataset</span></p>
						</td>
					</tr>
					<tr class="Basic-Table">
						<td class="Basic-Table">
							<p><span class="No-Break">Formula</span></p>
						</td>
						<td class="Basic-Table">
							<p><span class="_-----MathTools-_Math_Variable_v-normal">Precision</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span></p>
						</td>
						<td class="Basic-Table">
							<p><span class="_-----MathTools-_Math_Variable_v-normal">Recall</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FN</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.2 – Precision versus recall</p>
			<p class="callout-heading">Why can accuracy be a bad metric for imbalanced datasets?</p>
			<p class="callout">Let’s assume <a id="_idIndexMarker062"/>we have an imbalanced dataset with 1,000 examples, with 100 labels belonging to class 1 (the minority class) and 900 belonging to class 0 (the <span class="No-Break">majority class).</span></p>
			<p class="callout">Let’s say <a id="_idIndexMarker063"/>we have a model that always predicts 0 for all examples. The model’s accuracy for the minority class is <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">900</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">900</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">100</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">)</span> = <span class="No-Break">90%.</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B17259_01_07.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – A comic showing accuracy may not always be the right metric</p>
			<p>This brings <a id="_idIndexMarker064"/>us to the <strong class="bold">precision-recall trade-off</strong> in machine learning. Usually, precision and recall are inversely correlated – that is, when recall increases, precision most often decreases. Why? Note that recall <span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FN</span><span class="_-----MathTools-_Math_Space">  </span>and for recall to increase, FN should decrease. This means the model needs to classify more items as positive. However, if the model classifies more items as positive, some of these will likely be incorrect classifications, leading to an increase in the number of <strong class="bold">false positives</strong> (<strong class="bold">FPs</strong>). As the number of FPs increases, precision, defined as <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FP</span>, will decrease. With similar logic, you can <a id="_idIndexMarker065"/>argue that when recall decreases, precision <span class="No-Break">often increases.</span></p>
			<p>Next, let’s try to <a id="_idIndexMarker066"/>understand some of the precision and recall-based metrics that can help measure the performance of models trained on <span class="No-Break">imbalanced data:</span></p>
			<ul>
				<li><strong class="bold">F1 score</strong>: The F1 score (also called F-measure) is <a id="_idIndexMarker067"/>the harmonic mean of precision and recall. It combines precision and recall into a single metric. The F1 score varies between 0 and 1 and is most useful when we want to give equal priority to precision and recall (more on this later). This is available in the <strong class="source-inline">sklearn</strong> library <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">sklearn.metrics.f1_score</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">F-beta score or F-measure</strong>: The F-beta score is a generalization of the F1 score. It is a <a id="_idIndexMarker068"/>weighted harmonic mean of precision and recall, where the beta parameter controls the relative importance of precision <span class="No-Break">and recall.</span><p class="list-inset">The formula for the F-beta (<span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Space"> </span>) score is <span class="No-Break">as follow</span><span class="No-Break">s:</span></p><p class="list-inset"><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">precision</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">recall</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">____________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">precision</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">recall</span><span class="_-----MathTools-_Math_Variable"> </span></p><p class="list-inset">Here, beta (<span class="_-----MathTools-_Math_Variable">β</span>) is a positive parameter that determines the weight given to precision in the calculation of the score. When beta (<span class="_-----MathTools-_Math_Variable">β</span>) is set to 1, the F1 score is obtained, which is the harmonic mean of precision and recall. The F-beta score is a useful metric for imbalanced datasets, where one class may be more important than the other. By adjusting the beta parameter, we can control the relative importance of precision and recall for a particular class. For example, if we want to prioritize precision over recall for the minority class, we can set beta &lt; 1. To see why that’s the case, set <span class="_-----MathTools-_Math_Variable">β</span> = 0 in the <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Space"> </span>formula, which implies <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">precision</span></span><span class="No-Break">.</span></p><p class="list-inset">Conversely, if we want to prioritize recall over precision for the minority class, we can set beta &gt; 1 (we can set <span class="_-----MathTools-_Math_Variable">β</span> = <span class="_-----MathTools-_Math_Symbol">∞</span> in the <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Space"> </span>formula to see it reduce <span class="No-Break">to recall).</span></p><p class="list-inset">In practice, the choice of beta parameter depends on the specific problem and the desired <a id="_idIndexMarker069"/>trade-off between precision and recall. In general, higher values of beta result in more emphasis on recall, while lower values of beta result in more emphasis on precision. This is available in the <strong class="source-inline">sklearn</strong> library <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">sklearn.metrics.fbeta_score</strong></span><span class="No-Break">.</span></p></li>
				<li><strong class="bold">Balanced accuracy score</strong>: The balanced accuracy score is defined as the average <a id="_idIndexMarker070"/>of the recall obtained in each class. This metric is commonly used in both binary and multiclass classification scenarios to address imbalanced datasets. This is available in the <strong class="source-inline">sklearn</strong> library <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">sklearn.metrics.balanced_accuracy_score</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Specificity (SPE)</strong>: Specificity is a measure of the model’s ability to correctly identify the <a id="_idIndexMarker071"/>negative samples. In binary classification, it is calculated as the ratio of true negative predictions to the total number of negative samples. High specificity indicates that the model is good at identifying the negative class, while low specificity indicates that the model is biased toward the <span class="No-Break">positive class.</span></li>
				<li><strong class="bold">Support</strong>: Support <a id="_idIndexMarker072"/>refers to the number of samples in each class. Support is one of the values returned by the <strong class="source-inline">sklearn.metrics.precision_recall_fscore_support</strong> and <span class="No-Break"><strong class="source-inline">imblearn.metrics.classification_report_imbalanced</strong></span><span class="No-Break"> APIs.</span></li>
				<li><strong class="bold">Geometric mean</strong>: The <a id="_idIndexMarker073"/>geometric mean is a measure of the overall performance of the model on imbalanced datasets. In <strong class="source-inline">imbalanced-learn</strong>, <strong class="source-inline">geometric_mean_score()</strong> is defined by the geometric mean of “accuracy on positive class examples” (recall or sensitivity or TPR) and “accuracy on negative class examples” (specificity or TNR). So, even if one class is <a id="_idIndexMarker074"/>heavily outnumbered by the other class, the metric will still be representative of the model’s <span class="No-Break">overall performance.</span></li>
				<li><strong class="bold">Index Balanced Accuracy (IBA)</strong>: The IBA [1] is a measure of the overall accuracy of the model on imbalanced datasets. It takes into account both the sensitivity <a id="_idIndexMarker075"/>and specificity of the model and is calculated as the mean of the sensitivity and specificity, weighted by the imbalance ratio of each class. The IBA metric is useful for evaluating the overall performance of the model on imbalanced datasets and can be used to compare the performance of different models. IBA is one of the several values returned <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">imblearn.metrics.classification_report_imbalanced</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p><em class="italic">Table 1.3</em> shows the associated metrics and their formulas as an extension of the <span class="No-Break">confusion matrix:</span></p>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Predicted Positive</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Predicted Negative</strong></span></p>
						</td>
						<td class="No-Table-Style"/>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Actually </strong><span class="No-Break"><strong class="bold">Positive</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>True <span class="No-Break">positive (TP)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>False <span class="No-Break">negative (FN)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><em class="italic">Recall = Sensitivity = True positive </em><em class="italic">rate (TPR) = </em><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">FN</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Actually </strong><span class="No-Break"><strong class="bold">Negative</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>False <span class="No-Break">positive (FP)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>True <span class="No-Break">negative (TN)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="_-----MathTools-_Math_Variable">Specificity</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TN</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TN</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>Precision = <span class="No-Break">TP/(TP+FP)</span></p>
							<p>FPR = <span class="No-Break">FP/(FP+TN)</span></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="_-----MathTools-_Math_Variable">Accuracy</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TN</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">______________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TN</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FN</span><span class="_-----MathTools-_Math_Variable"> </span></p>
							<p><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">score</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">Precision</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">Recall</span><span class="_-----MathTools-_Math_Variable">  </span><span class="_-----MathTools-_Math_Base">_______________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">Precision</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">Recall</span><span class="_-----MathTools-_Math_Variable"> </span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.3 – Confusion matrix with various metrics and their definitions</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>ROC</h2>
			<p><strong class="bold">Receiver Operating Characteristics</strong>, commonly known as <strong class="bold">ROC</strong> curves, are plots that <a id="_idIndexMarker076"/>display the <strong class="bold">TPR</strong> on the <em class="italic">y</em>-axis <a id="_idIndexMarker077"/>against the <strong class="bold">FPR</strong> on the <em class="italic">x</em>-axis for <a id="_idIndexMarker078"/>various <span class="No-Break">threshold values:</span></p>
			<ul>
				<li>The ROC curve essentially represents the proportion of correctly predicted positive <a id="_idIndexMarker079"/>instances on the <em class="italic">y</em>-axis, contrasted with the proportion of incorrectly predicted negative instances on <span class="No-Break">the </span><span class="No-Break"><em class="italic">x</em></span><span class="No-Break">-axis.</span></li>
				<li>In classification tasks, a threshold is a cut-off value that’s used to determine the class of an example. For instance, if a model classifies an example as “positive,” a threshold of 0.5 might be set to decide whether the instance should be labeled as belonging to the “positive” or “negative” class. The ROC curve can be used to identify the optimal threshold for a model. This topic will be discussed in detail in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">.</span></li>
				<li>To create the ROC curve, we calculate the TPR and FPR for many various threshold values of the model’s predicted probabilities. For each threshold, the corresponding TPR value is plotted on the <em class="italic">y</em>-axis, and the FPR value is plotted on the <em class="italic">x</em>-axis, creating a single point. By connecting these points, we generate the ROC curve (<span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">):</span></li>
			</ul>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B17259_01_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – The ROC curve as a plot of TPR versus FPR (the dotted line shows a model with no skill)</p>
			<p>Some properties of the ROC curve are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <strong class="bold">Area Under Curve</strong> (<strong class="bold">AUC</strong>) of a ROC curve (also called <strong class="bold">AUC-ROC</strong>) serves a specific purpose: it provides a single numerical value that represents the model’s performance across all possible <span class="No-Break">classification thresholds:</span><ul><li>AUC-ROC represents the degree of separability of the classes. This means that the higher the AUC-ROC, the more the model can distinguish between the classes and predict a positive class example as positive and a negative class example as negative. A poor model with an AUC near 0 essentially predicts a positive class as a negative class and <span class="No-Break">vice versa.</span></li><li>The AUC-ROC of a random classifier is 0.5 and is the diagonal joining the points (0,0) and (1,0) on the <span class="No-Break">ROC curve.</span></li><li>The AUC-ROC has a probabilistic interpretation: an AUC of 0.9 indicates a 90% likelihood that the model will assign a higher score to a randomly chosen positive class example than to a negative class example. That is, AUC-ROC can be depicted <span class="No-Break">as follows:</span></li></ul></li>
			</ul>
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">score</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">score</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">−</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p class="list-inset">Here, 𝑥+ denotes the positive (minority) class, and 𝑥− denotes the negative (<span class="No-Break">majority) class.</span></p>
			<ul>
				<li>In the context of evaluating model performance, it’s crucial to use a test set that reflects the distribution of the data the model will encounter in real-world scenarios. This is particularly relevant when considering metrics such as the ROC curve, which remains consistent regardless of changes in class imbalance within the test data. Whether we have 1:1, 1:10, or 1:100 as the minority_class: majority_class distribution in the test set, the ROC curve remains the same [2]. The reason for this is that both of these rates are independent of the class distribution in the test data because they are calculated only based on the correctly and incorrectly classified instances of each class, not the total number of instances of each class. This is not to be confused with the change in imbalance in the training data, which can adversely impact the model’s performance and would be reflected in the <span class="No-Break">ROC curve.</span></li>
			</ul>
			<p>Now, let’s look <a id="_idIndexMarker080"/>at some of the problems in using ROC for <span class="No-Break">imbalanced datasets:</span></p>
			<ul>
				<li>ROC does not distinguish between the various classes – that is, it does not emphasize one class more over the other. This can be a problem for imbalanced datasets where, often, the minority class is more important to detect than the majority class. Because of this, it may not reflect the minority class well. For example, we may want better recall <span class="No-Break">over precision.</span></li>
				<li>While ROC curves can be useful for comparing the performance of models across a full range of FPRs, they may not be as relevant for specific applications that require a very low FPR, such as fraud detection in financial transactions or banking applications. The reason the FPR needs to be very low is that such applications usually require limited manual intervention. The number of transactions that can be manually checked may be as low as 1% or even 0.1% of all the data, which means the FPR can’t be higher than 0.001. In these cases, anything to <a id="_idIndexMarker081"/>the right of an FPR equal to 0.001 on the ROC curve becomes irrelevant [3]. To further understand this point, let’s consider <span class="No-Break">an example:</span><ul><li>Let’s say that for a test set, we have a total of 10,000 examples and only 100 examples of the positive class, making up 1% of the examples. So, any FPR higher than 1% - that is, 0.01 – is going to raise too many alerts to be handled manually <span class="No-Break">by investigators.</span></li><li>The performance on the far left-hand side of the ROC curve becomes crucial in most real-world problems, which are often dominated by a large number of negative instances. As a result, most of the ROC curve becomes irrelevant for applications that need to maintain a very <span class="No-Break">low FPR.</span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Precision-Recall curve</h2>
			<p>Similar to ROC curves, <strong class="bold">Precision-Recall</strong> (<strong class="bold">PR</strong>) curves plot a pair of metrics for different threshold values. But unlike ROC curves, which plot TPR and FPR, PR curves plot precision <a id="_idIndexMarker082"/>and recall. To demonstrate <a id="_idIndexMarker083"/>the difference between the two curves, let’s say we compare the performance of two models – Model 1 and Model 2 – on a particular handcrafted <span class="No-Break">imbalanced dataset:</span></p>
			<ul>
				<li>In <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.9 (a)</em>, the ROC curves for both models appear to be close to the top-left corner (point (0, 1)), which might lead you to conclude that both models are performing well. However, this can be misleading, especially in the context of <span class="No-Break">imbalanced datasets.</span></li>
				<li>When we turn our attention to the PR curves in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.9 (b)</em>, a different story unfolds. Model 2 comes closer to the ideal top-right corner (point (1, 1)) of the plot, indicating that its performance is much better than Model 1 when precision and recall <span class="No-Break">are considered.</span></li>
				<li>The PR curve reveals that Model 2 has an advantage over <span class="No-Break">Model 1.</span></li>
			</ul>
			<p>This discrepancy between the ROC and PR curves also underscores the importance of using multiple <a id="_idIndexMarker084"/>metrics for model evaluation, particularly <a id="_idIndexMarker085"/>when dealing with <span class="No-Break">imbalanced data:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B17259_01_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – The PR curve can show obvious differences between models compared to the ROC curve</p>
			<p>Let’s try to understand these observations in detail. While the ROC curve shows very little difference between the performance of the two models, the PR curve shows a much bigger gap. The reason for this is that the ROC curve uses FPR, which is FP/(FP+TN). Usually, TN is really high for an imbalanced dataset, and hence even if FP changes by a decent amount, FPR’s overall value is overshadowed by TN. Hence, ROC doesn’t change by a <span class="No-Break">whole lot.</span></p>
			<p>The conclusion of which classifier is superior can change with the distribution of classes in the test set. In the case of skewed datasets, the PR curve can more clearly show that the model did not work well compared to the ROC curve, as shown in the <span class="No-Break">preceding figure.</span></p>
			<p>The <strong class="bold">average precision</strong> is a single <a id="_idIndexMarker086"/>number that’s used to summarize a PR curve, and the corresponding API in <strong class="source-inline">sklearn</strong> <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">sklearn.metrics.average_precision_score</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Relation between the ROC curve and PR curve</h2>
			<p>The primary distinction between the ROC curve and the PR curve lies in the fact that while <a id="_idIndexMarker087"/>ROC assesses how well the model can “calculate” both positive and negative classes, PR solely <a id="_idIndexMarker088"/>focuses on the positive class. Therefore, when dealing with a balanced dataset scenario and you are concerned with both the positive and negative classes, ROC AUC works exceptionally well. In contrast, when dealing with an imbalanced situation, PR AUC is more suitable. However, it’s important to keep in mind that PR AUC only evaluates the model’s ability to “calculate” the positive class. <em class="italic">Because PR curves are more sensitive to the positive (minority) class, we will be using PR curves throughout the first half of </em><span class="No-Break"><em class="italic">this book.</em></span></p>
			<p>We can reimagine the PR curve with precision on the <em class="italic">x</em>-axis and TPR, also known as recall, on the <em class="italic">y</em>-axis. The key difference between the two curves is that while the ROC curve uses FPR, the PR curve <span class="No-Break">uses precision.</span></p>
			<p>As discussed earlier, FPR tends to be very low when dealing with imbalanced datasets. This aspect of having low FPR values is crucial in certain applications such as fraud detection, where the capacity for manual investigations is inherently limited. Consequently, this perspective can alter the perceived performance of classifiers. As shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.9</em>, it’s also possible that the performances of the two models seem reversed when compared using average precision (0.69 versus 0.90) instead of AUC-ROC (0.97 <span class="No-Break">and 0.95).</span></p>
			<p>Let’s <span class="No-Break">summarize this:</span></p>
			<ul>
				<li>The AUC-ROC is the area under the curve plotted with TPR on the <em class="italic">y</em>-axis and FPR on <span class="No-Break">the </span><span class="No-Break"><em class="italic">x</em></span><span class="No-Break">-axis.</span></li>
				<li>The AUC-PR is the area under the curve plotted with precision on the <em class="italic">y</em>-axis and recall on <span class="No-Break">the </span><span class="No-Break"><em class="italic">x</em></span><span class="No-Break">-axis.</span></li>
			</ul>
			<p>As TPR equals recall, the two plots only differ in what recall is compared to – either precision or FPR. Additionally, the plots are rotated by 90 degrees relative to <span class="No-Break">each other:</span></p>
			<table id="table004" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">AUC-ROC</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">AUC-PR</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">General formula</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">AUC</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">TPR</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">FPR</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">AUC</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">Precision</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">Recall</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Expanded formula</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="_-----MathTools-_Math_Variable_v-normal">AUC</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FN</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">TN</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="_-----MathTools-_Math_Variable_v-normal">AUC</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">FP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">FN</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Equivalence</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">AUC</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">Recall</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">FPR</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">AUC</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">Precision</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">Recall</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.4 – Comparing the ROC and PR curves</p>
			<p>In <a id="_idIndexMarker089"/>the next <a id="_idIndexMarker090"/>few sections, we’ll explore the circumstances that lead to imbalances in datasets, the challenges these imbalances can pose, and the situations where data imbalance might not be <span class="No-Break">a concern.</span></p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Challenges and considerations when dealing with imbalanced data</h1>
			<p>In certain instances, directly using data for machine learning without worrying about data imbalance <a id="_idIndexMarker091"/>can yield usable results suitable for a given business scenario. Yet, there are situations where a more dedicated effort is needed to <a id="_idIndexMarker092"/>manage the effects of <span class="No-Break">imbalanced data.</span></p>
			<p>Broad statements claiming that you must always or never adjust for imbalanced classes tend to be misleading. The truth is that the need to address class imbalance is contingent on the specific characteristics of the data, the problem at hand, and the definition of an acceptable solution. Therefore, the approach to dealing with class imbalance should be tailored according to <span class="No-Break">these factors.</span></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>When can we have an imbalance in datasets?</h1>
			<p>In this <a id="_idIndexMarker093"/>section, we’ll explore various situations and causes leading to an imbalance in datasets, such as rare event occurrences or skewed data <span class="No-Break">collection processes:</span></p>
			<ul>
				<li><strong class="bold">Inherent in the problem</strong>: Sometimes, the task we need to solve involves detecting outliers in datasets – for example, patients with a certain disease or fraud cases in a set of transactions. In such cases, the dataset is inherently imbalanced because the target events are rare to <span class="No-Break">begin with.</span></li>
				<li><strong class="bold">High cost of data collection while bootstrapping a machine learning solution</strong>: The cost of collecting data might be too high for certain classes. For example, collecting data on COVID-19 patients incurs high costs due to the need for specialized medical tests, protective equipment, and the ethical and logistical challenges of obtaining informed consent in a high-stress <span class="No-Break">healthcare environment.</span></li>
				<li><strong class="bold">Noisy labels for certain classes</strong>: This may happen when a lot of noise is introduced into the labels of the dataset for certain classes during <span class="No-Break">data collection.</span></li>
				<li><strong class="bold">Labeling errors</strong>: Errors in labeling can also contribute to data imbalance. For example, if some samples are mistakenly labeled as negative when they are positive, this can result in an imbalance in the dataset. Additionally, if a class is already inherently rare, human annotators might be biased and overlook the few examples of that rare class that <span class="No-Break">do exist.</span></li>
				<li><strong class="bold">Sampling bias</strong>: Data collection methods can sometimes introduce bias in the dataset. For example, if a survey is conducted in a specific geographical area or among a specific group of people, the resulting dataset may not be representative of the <span class="No-Break">entire population.</span></li>
				<li><strong class="bold">Data cleaning</strong>: During the data cleaning or filtering process, some classes or samples may be removed due to incomplete or missing data. This can result in an imbalance in the <span class="No-Break">remaining dataset.</span></li>
			</ul>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Why can imbalanced data be a challenge?</h1>
			<p>Let’s delve <a id="_idIndexMarker094"/>into the difficulties posed by imbalanced data on model predictions and their impact on <span class="No-Break">model performance:</span></p>
			<ul>
				<li><strong class="bold">Failure of metrics such as accuracy</strong>: As we discussed previously, conventional metrics such as accuracy can be misleading in the context of imbalanced data (a 99% imbalanced dataset would still achieve 99% accuracy). Threshold-invariant metrics such as the PR curve or ROC curve attempt to expose the performance of the model over a wide range of thresholds. The real challenge lies in the disproportionate influence of the “true negative” cell in the confusion matrix. Metrics that focus less on “true negatives,” such as precision, recall, or F1 score, are more appropriate for evaluating model performance. It’s important to note that these metrics have a hidden hyperparameter – the classification threshold – that should not be ignored but optimized for real-world applications (refer to <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, to learn more about <span class="No-Break">threshold tuning).</span></li>
				<li><strong class="bold">Imbalanced data can be a challenge for a model’s loss function</strong>: This may happen because the loss function is typically designed to minimize the errors between the predicted outputs and the true labels of the training data. When the data is imbalanced, there are more instances of one class than another, and the model may become biased toward the majority class. We will discuss solutions to this issue in more detail in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, and <a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep </em><span class="No-Break"><em class="italic">Learning Techniques</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Different misclassification costs for different classes</strong>: Often, it may be more expensive to misclassify positive examples than to misclassify negative examples. We may have false positives that are more expensive than false negatives. For example, usually, the cost of misclassifying a patient with cancer as healthy (false negative) will be much higher than misclassifying a healthy patient as having cancer (false positive). Why? Because it’s much cheaper to go through some extra tests to revalidate the test results in the second case instead of detecting it much later in the first case. This is called the cost of misclassification, which could be different for the majority and minority classes, making things complicated for imbalanced datasets. We will discuss more about this in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Constraints on computational resources</strong>: In sectors such as finance, healthcare, and retail, handling big data is a common challenge. Training on these large datasets is not only time-consuming but also costly due to the computational power needed. In such scenarios, downsampling or undersampling the majority class becomes essential, as will be discussed in <a href="B17259_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Undersampling Methods</em>. Additionally, acquiring more samples for the minority class can further increase dataset size and computational costs. Memory limitations may also restrict the amount of data that can <span class="No-Break">be processed.</span></li>
				<li><strong class="bold">Not enough variation in the minority class examples to sufficiently represent its distribution</strong>: Often, an absolute number of samples of the minority class is not as big of a problem as the <strong class="bold">variation</strong> in the samples of the minority class. The dataset might look large, but there might not be many variations <a id="_idIndexMarker095"/>or varieties in the samples that adequately represent the distribution of minority classes. This can lead to the model not being able to learn the classification boundary properly, which would lead to poor performance of the model (<span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.10</em>). This can often happen in computer vision problems, such as object detection, where we may have very few samples of certain classes. In such cases, data augmentation techniques (discussed in <a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep Learning Methods</em>) can <span class="No-Break">help significantly:</span></li>
			</ul>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B17259_01_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Change in decision boundary with a different distribution of minority class examples – the crosses denote the majority class, and the circles denote the minority class</p>
			<ul>
				<li><strong class="bold">Poor performance of uncalibrated models</strong>: Imbalanced data can be a challenge for <a id="_idIndexMarker096"/>uncalibrated models. Uncalibrated models are models that do not output well-calibrated probabilities, which means that the predicted probabilities may not reflect the true likelihood of the <span class="No-Break">predicted classes:</span><ul><li>When dealing with imbalanced data, uncalibrated models can be particularly susceptible to producing biased predictions toward the majority class as they may not be able to effectively differentiate between the minority and majority classes. This can lead to poor performance in the minority class, where the model may produce overly confident predictions or predictions that are <span class="No-Break">too conservative.</span></li><li>For example, an uncalibrated model that is trained on imbalanced data may incorrectly classify instances that belong to the minority class as majority class examples, often with high confidence. This is because the model may not have learned to adjust its predictions based on the imbalance in the data and may not have a good understanding of the minority <span class="No-Break">class examples.</span></li><li>To address this challenge, it is important to use well-calibrated models [4] that can output probabilities that reflect the true likelihood of the predicted classes. This can be achieved through techniques such as Platt scaling or isotonic regression, which can calibrate the predicted probabilities of an uncalibrated model to produce more accurate and reliable probabilities. Model calibration will be discussed in detail in <a href="B17259_10.xhtml#_idTextAnchor279"><em class="italic">Chapter 10</em></a>, <span class="No-Break"><em class="italic">Model Calibration</em></span><span class="No-Break">.</span></li></ul></li>
				<li><strong class="bold">Poor performance of models because of non-adjusted thresholds</strong>: It’s important to use intelligent thresholding when making predictions using models trained <a id="_idIndexMarker097"/>on imbalanced datasets. Simply predicting 1 when the model probability is over 0.5 may not always be the best approach. Instead, we should consider other thresholds that may be more effective. This can be achieved by examining the PR curve of the model rather than relying solely on its success rate with a default probability threshold of 0.5. Threshold adjustment can be quite important, even for models trained on naturally or artificially balanced datasets. We will discuss threshold adjustment in detail in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">.</span></li>
			</ul>
			<p>Next, let’s try to see when we shouldn’t do anything about <span class="No-Break">data imbalance.</span></p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>When to not worry about data imbalance</h1>
			<p>Class imbalance may not always negatively impact performance, and using imbalance-specific methods can sometimes worsen results [5]. Therefore, it’s crucial to accurately assess whether <a id="_idIndexMarker098"/>a task is genuinely affected by class imbalance before applying any specialized techniques. One such strategy can be as simple as setting up a baseline model without worrying about class imbalance and observing the model’s performance on various classes using various <span class="No-Break">performance metrics.</span></p>
			<p>Let’s explore scenarios where data imbalance may not be a concern and no corrective measures may <span class="No-Break">be needed:</span></p>
			<ul>
				<li><strong class="bold">When the imbalance is small</strong>: If the imbalance in the dataset is relatively small, with the ratio of the minority class to the majority class being only slightly skewed (say 4:5 or 2:3), the impact on the model’s performance may be minimal. In such cases, the model may still perform reasonably well without requiring any special techniques to handle <span class="No-Break">the imbalance.</span></li>
				<li><strong class="bold">When the goal is to predict the majority class</strong>: In some cases, the focus may be on predicting the majority class accurately, and the minority class may not be of particular interest. For example, in online ad placement, the focus can be on targeting users (majority class) likely to click on ads to maximize click-through rates <a id="_idIndexMarker099"/>and immediate revenue, while less attention is given to users (minority class) who may find <span class="No-Break">ads annoying.</span></li>
				<li><strong class="bold">When the cost of misclassification is nearly equal for both classes</strong>: In some applications, the cost of misclassifying a positive class example is not high (that is, false negative). An example is classifying emails as spam or non-spam. It’s totally fine to miss a spam email once in a while and misclassify it as non-spam. In such cases, the impact of misclassification on the performance metrics may be negligible, and the imbalance may not be <span class="No-Break">a concern.</span></li>
				<li><strong class="bold">When the dataset is sufficiently large</strong>: Even if the ratio of minority to majority class samples is very low, such as 1:100, and if the dataset is sufficiently large, with a large number of samples in both the minority and majority classes, the impact of data imbalance on the model’s performance may be reduced. With a larger dataset, the model may be able to learn the patterns in the minority class more effectively. However, it would still be advisable to compare the baseline model’s performance with the performance of models that take the data imbalance into account. For example, compare a baseline model to models with threshold adjustment, oversampling, and undersampling (<a href="B17259_02.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Oversampling Methods</em>, and <a href="B17259_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Undersampling Methods</em>), and algorithm-based techniques such as cost-sensitive learning (<a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">).</span></li>
			</ul>
			<p>In the next section, we will become familiar with a library that can be very useful when dealing with imbalanced data. We will train a model on an imbalanced toy dataset and look at some metrics to evaluate the performance of the <span class="No-Break">trained model.</span></p>
			<h1 id="_idParaDest-33">Introduction to the imbalanced-lear<a id="_idTextAnchor032"/>n library</h1>
			<p><strong class="source-inline">imbalanced-learn</strong> (imported as <strong class="source-inline">imblearn</strong>) is a Python package that offers several techniques <a id="_idIndexMarker100"/>to deal with data imbalance. In the first half of this book, we will rely heavily on this library. Let’s install the <span class="No-Break"><strong class="source-inline">imbalanced-learn</strong></span><span class="No-Break"> library:</span></p>
			<pre class="console">
pip3 install imbalanced-learn==0.11.0</pre>			<p>We can use <strong class="source-inline">imbalanced-learn</strong> to create a synthetic dataset for <span class="No-Break">our analysis:</span></p>
			<pre class="source-code">
from sklearn.datasets import make_classification
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
def make_data(sep):
    X, y = make_classification(n_samples=50000,
        n_features=2, n_redundant=0,
        n_clusters_per_class=1, weights=[0.995],
        class_sep=sep, random_state=1)
    X = pd.DataFrame(X, columns=['feature_1', 'feature_2'])
    y = pd.Series(y)
    r<a id="_idTextAnchor033"/>eturn X, y</pre>			<p>Let’s <a id="_idIndexMarker101"/>analyze the <span class="No-Break">generated dataset:</span></p>
			<pre class="source-code">
from collections import Counter
X, y = make_data(sep=2)
print(y.value_counts())
sns.scatterplot(data=X, x="feature_1", y="feature_2", hue=y)
plt.title('Separation: {}'.format(separation))
plt.show()</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
0     49498
1       502</pre>			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B17259_01_11.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – 2 class dataset with tw<a id="_idTextAnchor034"/>o features</p>
			<p>Let’s split <a id="_idIndexMarker102"/>this dataset into training and <span class="No-Break">test sets:</span></p>
			<pre class="source-code">
From sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = \
    y, test_size=0.2, random_state=42)
print('train data: ', Counter(y_train))
print('test data: ', Counter(y_test))</pre>			<p>Here’s <span class="No-Break">the output:</span></p>
			<pre class="source-code">
train data:  Counter({0: 39598, 1: 402})
test data:  Counter({0: 990<a id="_idTextAnchor035"/>0, 1: 100})</pre>			<p>Note the usage of <strong class="source-inline">stratify</strong> in the <strong class="source-inline">train_test_split</strong> API of <strong class="source-inline">sklearn</strong>. Specifying <strong class="source-inline">stratify=y</strong> ensures we maintain the same ratio of majority and minority classes in both the training set and the test set. Let’s understand stratification in <span class="No-Break">more detail.</span></p>
			<p><strong class="bold">Stratified sampling</strong> is a way to split the dataset into various subgroups (called “strata”) based on certain characteristics they share. It can be highly valuable when dealing with <a id="_idIndexMarker103"/>imbalanced datasets because it ensures that the train and test datasets have the same proportions of class labels as the <span class="No-Break">original dataset.</span></p>
			<p>In an imbalanced dataset, the minority class constitutes a small fraction of the total data. If we perform a simple random split without any stratification, there’s a risk that the minority class may not be adequately represented in the training set or could be entirely left out from the test set, which may lead to poor performance and unreliable <span class="No-Break">evaluation metrics.</span></p>
			<p>With stratified sampling, the proportion of each class in the overall dataset is preserved in both training and test sets, ensuring representative sampling and a better chance for the model to learn from the minority class. This leads to a more robust model and a more reliable evaluation of the <span class="No-Break">model’s performance.</span></p>
			<p class="callout-heading">The scikit-learn APIs for stratification</p>
			<p class="callout">The <strong class="source-inline">scikit-learn</strong> APIs, such as <strong class="source-inline">RepeatedStratifiedKFold</strong> and <strong class="source-inline">StratifiedKFold</strong>, employ the concept of stratification to evaluate model performance through cross-validation, especially when working with <span class="No-Break">imbalanced datasets.</span></p>
			<p>Now, let’s <a id="_idIndexMarker104"/>train a logistic regression model on <span class="No-Break">training data:</span></p>
			<pre class="source-code">
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(random_state=0, max_iter=2000)
lr.fit(X_train, y_train)
y_pred = lr.pr<a id="_idTextAnchor036"/>edict(X_test)</pre>			<p>Let’s get the report metrics from the <span class="No-Break"><strong class="source-inline">sklearn</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))</pre>			<p>This <a id="_idIndexMarker105"/>outputs <span class="No-Break">the following:</span></p>
			<pre class="source-code">
          precision     recall      f1-score    support
0         0.99          1.00        1.00        9900
1         0.94          0.17        0.29        100
accuracy                                0.99      10000
macro avg       0.97        0.58        0.64      10000
weighted avg    0.99        0.99        0<a id="_idTextAnchor037"/>.99      10000</pre>			<p>Let’s get the report metrics <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">imblearn</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
from imblearn.metrics import classification_report_imbalanced
print(classification_report_imbalanced(y_test, y_pred))</pre>			<p>This outputs a lot <span class="No-Break">more columns:</span></p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B17259_01_12.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – Output of the classification report from imbalanced-learn</p>
			<p>Do you notice <a id="_idIndexMarker106"/>the extra metrics here compared to the API of <strong class="source-inline">sklearn</strong>? We got three additional metrics: <strong class="source-inline">spe</strong> for specificity, <strong class="source-inline">geo</strong> for geometric mean, and <strong class="source-inline">iba</strong> for index <span class="No-Break">balanced accuracy.</span></p>
			<p>The <strong class="source-inline">imblearn.metrics</strong> module has several such functions that can be helpful for imbalanced datasets. Apart from <strong class="source-inline">classification_report_imbalanced()</strong>, it offers APIs such as <strong class="source-inline">sensitivity_specificity_support()</strong>, <strong class="source-inline">geometric_mean_score()</strong>, <strong class="source-inline">sensitivity_score()</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">specificity_score()</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor038"/>General rules to follow</h1>
			<p>Usually, the first step in any machine learning pipeline should be to split the data into train/test/validation sets. We should avoid applying any techniques to handle the imbalance until after the data has been split. We should begin by splitting the data into training, testing, and validation sets and then proceed with any necessary adjustments to the training data. Applying techniques such as oversampling (see <a href="B17259_02.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Oversampling Methods</em>) before splitting the data can result in data leakage, overfitting, and <span class="No-Break">over-optimism [6].</span></p>
			<p>We should ensure that the validation data closely resembles the test data. Both validation data and test data should represent real-world scenarios on which the model will be used for prediction. Avoid applying any sampling techniques or modifications to the validation set. The only requirement is to include a sufficient number of samples from <span class="No-Break">all classes.</span></p>
			<p>Let’s switch to <a id="_idIndexMarker107"/>discussing a bit about using unsupervised learning <a id="_idIndexMarker108"/>algorithms. <strong class="bold">Anomaly detection</strong> or <strong class="bold">outlier detection</strong> is a class of problems that can be used for dealing with imbalanced data problems. Anomalies or outliers are data points that deviate significantly from the rest of the data. These anomalies often correspond to the minority class in an imbalanced dataset, making unsupervised methods <span class="No-Break">potentially useful.</span></p>
			<p>The term that’s <a id="_idIndexMarker109"/>often used for these kinds of problems is <strong class="bold">one-class classification</strong>. This technique is particularly beneficial when the positive (minority) cases are sparse or when gathering them before the training is not feasible. The model is trained exclusively on what is considered the “normal” or majority class. It then classifies new instances as “normal” or “anomalous,” effectively identifying what could be the minority class. This can be especially useful for binary imbalanced classification problems, where the majority class is deemed “normal,” and the minority class is considered <span class="No-Break">an anomaly.</span></p>
			<p>However, it does have a drawback: outliers or positive cases during training are discarded [7], which could lead to the potential loss of <span class="No-Break">valuable information.</span></p>
			<p>In summary, while unsupervised methods such as one-class classification offer an alternative for managing class imbalance, our discussion in this book will remain centered on supervised learning algorithms. Nevertheless, we recommend that you explore and experiment with such solutions when you find <span class="No-Break">them appropriate.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor039"/>Summary</h1>
			<p>Let’s summarize what we’ve learned so far. Imbalanced data is a common problem in machine learning, where there are significantly more instances of one class than another. Imbalanced datasets can arise from various situations, including rare event occurrences, high data collection costs, noisy labels, labeling errors, sampling bias, and data cleaning. This can be a challenge for machine learning models as they may be biased toward the <span class="No-Break">majority class.</span></p>
			<p>Several techniques can be used to deal with imbalanced data, such as oversampling, undersampling, and cost-sensitive learning. The best technique to use depends on the specific problem and <span class="No-Break">the data.</span></p>
			<p>In some cases, data imbalance may not be a concern. When the dataset is sufficiently large, the impact of data imbalance on the model’s performance may be reduced. However, it is still advisable to compare the baseline model’s performance with the performance of models that have been built using techniques that address data imbalance, such as threshold adjustment, data-based techniques (oversampling and undersampling), and <span class="No-Break">algorithm-based techniques.</span></p>
			<p>Traditional performance metrics such as accuracy can fail in imbalanced datasets. Some more useful metrics for imbalanced datasets are the ROC curve, the PR curve, precision, recall, and F1 score. While ROC curves are suitable for balanced datasets, PR curves are more suitable for imbalanced datasets when one class is more important than <span class="No-Break">the other.</span></p>
			<p>The <strong class="source-inline">imbalanced-learn</strong> library is a Python package that offers several techniques to deal with <span class="No-Break">data imbalance.</span></p>
			<p>There are some general rules to follow, such as splitting the data into train/test/validation sets before applying any techniques to handle the imbalance in the data, ensuring that the validation data closely resembles the test data and that test data represents the data on which the model will make final predictions, and avoiding applying any sampling techniques or modifications to the validation set and <span class="No-Break">test set.</span></p>
			<p>One-class classification or anomaly detection is another technique that can be used for dealing with unsupervised imbalanced data problems. In this book, we will focus our discussion on supervised learning <span class="No-Break">algorithms only.</span></p>
			<p>In the next chapter, we will look at one of the common ways to handle the data imbalance problem in datasets by applying <span class="No-Break">oversampling techniques.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor040"/>Questions</h1>
			<ol>
				<li>How does the choice of loss function when training a model affect the performance of the model on <span class="No-Break">imbalanced datasets?</span></li>
				<li>Can you explain why the PR curve is more informative than the ROC curve when dealing with highly <span class="No-Break">skewed datasets?</span></li>
				<li>What are some of the potential issues with using accuracy as a metric for model performance on <span class="No-Break">imbalanced datasets?</span></li>
				<li>How does the concept of “class imbalance” affect the process of feature engineering in <span class="No-Break">machine learning?</span></li>
				<li>In the context of imbalanced datasets, how does the choice of “k” in k-fold cross-validation affect the performance of the model? How would you fix <span class="No-Break">the issue?</span></li>
				<li>How does the distribution of classes in the test data affect the PR curve, and why? What about the <span class="No-Break">ROC curve?</span></li>
				<li>What are the implications of having a high AUC-ROC but a low AUC-PR in the context of an <span class="No-Break">imbalanced dataset?</span></li>
				<li>How does the concept of “sampling bias” contribute to the challenge of imbalanced datasets in <span class="No-Break">machine learning?</span></li>
				<li>How does the concept of “labeling errors” contribute to the challenge of imbalanced datasets in <span class="No-Break">machine learning?</span></li>
				<li>What are some of the real-world scenarios where dealing with imbalanced datasets is inherently part of <span class="No-Break">the problem?</span></li>
				<li><strong class="bold">Matthews Correlation Coefficient</strong> (<strong class="bold">MCC</strong>) is a metric that takes all the cells of the confusion matrix into account and is given by the <span class="No-Break">following formul</span><span class="No-Break">a:</span><p class="list-inset"><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">TN</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FN</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space">   </span><span class="_-----MathTools-_Math_Base">__________________________________</span><span class="_-----MathTools-_Math_Base">    </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">______________________________________</span><span class="_-----MathTools-_Math_Base">    </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FP</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">TP</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FN</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">TN</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FP</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">·</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">TN</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">FN</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span></p><ol><li class="Alphabets">What can be the minimum and maximum values of <span class="No-Break">the metric?</span></li><li class="Alphabets">Because it takes TN into account, its value may not change much when we are comparing different models, but it can tell us if the predictions for various classes are going well. Let’s illustrate this through an artificial example where we take a dummy model that always predicts 1 for an imbalanced test set made of 100 examples, with 90 of class 1 and 10 of class 0. Compute the various terms in the MCC formula and the value of MCC. Also, compute the values of accuracy, precision, recall, and <span class="No-Break">F1 score.</span></li><li class="Alphabets">What can you conclude about the model from the MCC value that you just computed in the <span class="No-Break">previous question?</span></li><li class="Alphabets">Create an imbalanced dataset using imblearn’s <strong class="source-inline">fetch_dataset</strong> API and then compute the values of MCC, accuracy, precision, recall, and F1 score. See if the MCC value can be a useful metric for <span class="No-Break">this dataset.</span></li></ol></li>
			</ol>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor041"/>References</h1>
			<ol>
				<li value="1">V. García, R. A. Mollineda, and J. S. Sánchez, <em class="italic">Index of Balanced Accuracy: A Performance Measure for Skewed Class Distributions</em>, in Pattern Recognition and Image Analysis, vol. 5524, H. Araujo, A. M. Mendonça, A. J. Pinho, and M. I. Torres, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009, pp. 441–448. Accessed: Mar. 18, 2023. [Online]. Available <span class="No-Break">at </span><a href="http://link.springer.com/10.1007/978-3-642-02172-5_57"><span class="No-Break">http://link.springer.com/10.1007/978-3-642-02172-5_57</span></a><span class="No-Break">.</span></li>
				<li>T. Fawcett, <em class="italic">An introduction to ROC analysis</em>, Pattern Recognition Letters, vol. 27, no. 8, pp. 861–874, Jun. 2006, <span class="No-Break">doi: 10.1016/j.patrec.2005.10.010.</span></li>
				<li>Y.-A. Le Borgne, W. Siblini, B. Lebichot, and G. Bontempi, <em class="italic">Reproducible Machine Learning for Credit Card Fraud Detection - Practical Handbook</em>. Université Libre de Bruxelles, 2022. [Online]. Available <span class="No-Break">at </span><a href="https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook"><span class="No-Break">https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook</span></a><span class="No-Break">.</span></li>
				<li>W. Siblini, J. Fréry, L. He-Guelton, F. Oblé, and Y.-Q. Wang, <em class="italic">Master your Metrics with Calibration</em>, vol. 12080, 2020, pp. 457–469. <span class="No-Break">doi: 10.1007/978-3-030-44584-3_36.</span></li>
				<li>Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou, <em class="italic">Exploratory Undersampling for Class-Imbalance Learning</em>, IEEE Trans. Syst., Man, Cybern. B, vol. 39, no. 2, pp. 539–550, Apr. 2009, <span class="No-Break">doi: </span><span class="No-Break">10.1109/TSMCB.2008.2007853</span><span class="No-Break">.</span></li>
				<li>M. S. Santos, J. P. Soares, P. H. Abreu, H. Araujo, and J. Santos, <em class="italic">Cross-Validation for Imbalanced Datasets: Avoiding Overoptimistic and Overfitting Approaches [Research Frontier]</em>, IEEE Comput. Intell. Mag., vol. 13, no. 4, pp. 59–76, Nov. 2018, <span class="No-Break">doi: 10.1109/MCI.2018.2866730.</span></li>
				<li>A. Fernández, S. García, M. Galar, R. Prati, B. Krawczyk, and F. Herrera, <em class="italic">Learning from Imbalanced Data Sets</em>. Springer International <span class="No-Break">Publishing, 2018</span></li>
			</ol>
		</div>
	</body></html>