<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Artificial Neural Networks</h1>
                </header>
            
            <article>
                
<p>Artificial neural networks try to mimic the way the human brain works. They are used to solve a number of difficult problems, such as understanding written or spoken language, identifying objects in an image, or driving a car.</p>
<p>You will learn the basics of how an artificial neural network works, look at the steps and mathematical calculations needed to train it, and have a general view of complex neural networks.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introducing the perceptron – the simplest type of neural network</li>
<li>Building a deep network</li>
<li>Understanding the backpropagation algorithm</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To complete this chapter, you will need to download the <kbd><span>transfusion.xlsx</span></kbd> <span>file</span> from the GitHub repository at <a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09">https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09</a><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09">.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing the perceptron – the simplest type of neural network</h1>
                </header>
            
            <article>
                
<p>Neural networks are inspired by the human brain' more specifically, by the neuron cells that compose it. Actually, since there have been major advances in neuroscience since the first artificial neuron was designed, it would be better to say that they are inspired by what was known about the brain some years ago.</p>
<p>The perceptron was the first attempt to build an artificial neural network (Frank Rosenblatt, 1959). It was actually a model of a single neuron, with multiple inputs and one output. The value at the output is calculated as the weighted sum of the inputs and these weights are adjusted iteratively. This simple implementation has many disadvantages and limitations, so it was later replaced by the multilayer perceptron. The most basic model of this artificial neural network has the structure shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6bdc8e1b-7880-4957-97dc-3eaa548f0d74.png" style="width:28.50em;height:18.75em;"/></p>
<p>The input and output layers are taken from the perceptron, but a hidden layer of nodes is now added. Each node in this layer acts in practice as a neuron. To understand how the inputs and outputs of each neuron work and how information is sent through the network, we need to know the details of how each neuron is built. A schematic view of an artificial neuron could be represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e343766d-3cc8-4b5f-b2bb-9f6ff1a0e948.png" style="width:18.00em;height:19.08em;"/></p>
<p>The combination function calculates the resulting input as the sum of the inputs weighted by <span class="packt_screen">w<sub>i</sub></span>. The activation function calculates the output using this input. The output range is usually limited to [0;1], using different functions. It is often the case that the neuron transmits the signal only if the input value is above a certain threshold.</p>
<p>How does an artificial neural network learn? A training dataset is used, for which the outputs are known. The input values are fed into the network, the predicted output is compared to the real output, and the <span class="packt_screen">w<sub>i</sub></span> weights are adjusted iteratively at each step. This means that a neural network is a supervised learning model.</p>
<p>The more complex the problem, the larger the number of training samples needed to adjust the weights. We will also see that the number of hidden layers and neurons are also adjusted depending on the problem. Fine tuning these parameters is a complex problem, almost a field of study in itself.</p>
<p>Artificial neural networks are useful since they can model any mathematical function. So, even if the relationship between the input values is unknown, we can use a network to reproduce it and make predictions.</p>
<p>Since the training process can be complicated and the number of parameters that are adjusted at training time is large, it is often difficult to understand why an artificial neural network correctly predicts a given value. The explainability of the artificial intelligence models, based on neural networks, is also an extensively studied problem.</p>
<p>Some applications of neural networks are as follows:</p>
<ul>
<li>Image analysis—faces, objects, colors, expressions, and gestures</li>
<li>Sound analysis—voices, speech to text, and sentiment</li>
<li>Text classification—email spam, fraud in document content, and sentiment</li>
<li>Hardware failures—predictive and/or diagnostic</li>
<li>Health risks and/or diagnostics</li>
<li>Customer or employee churn</li>
</ul>
<p>Let's see how training works in practice, following an example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a neural network</h1>
                </header>
            
            <article>
                
<p>We will use a public dataset from the Blood Transfusion Service Center in Hsin-Chu City, Taiwan (<em>Knowledge discovery on RFM model using Bernoulli sequence</em>, by <span>Yeh, I-Cheng, Yang, King-Jang, and Ting, Tao-Ming,</span> Expert Systems with Applications, 2008). The set contains information about blood donors, summarized in five variables:</p>
<ul>
<li>R (Recency – months since last donation)</li>
<li>F (Frequency – total number of donations)</li>
<li>M (Monetary – total blood donated in c.c.)</li>
<li>T (Time – months since first donation)</li>
<li>A binary variable representing whether they donated blood in March 2007 (one stands for donating blood; zero stands for not donating blood)</li>
</ul>
<p>We would like to prove how well an artificial neural network can learn from the first four of the preceding features, and predict the target, variable five. Follow these steps to reproduce and learn about the calculations already shown in the <kbd>transfusion.xlsx</kbd> file:</p>
<ol>
<li>Load the <span><kbd>transfusion.xlsx</kbd> file into Excel.</span></li>
</ol>
<ol start="2">
<li>In the worksheet named <kbd>transfusion</kbd><em>,</em> you will find the input data. It should look something like the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7967366a-93c2-410f-ad15-6d600228ff6d.png" style="width:55.42em;height:34.75em;"/></p>
<ol start="3">
<li>Since the data is not presented in any particular order, we can use the first 500 entries to train the neural network. Open a new worksheet and rename it <kbd>training1</kbd> (remember that we are repeating the steps to create the worksheets already present in the file, so that you can compare your results).</li>
</ol>
<ol start="4">
<li>Create a set of variables like the one you see in the following screenshot. If you use the same cells, it will be easier to follow the next steps:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/69450203-d090-42ce-92f0-2302834382e2.png" style="width:65.58em;height:31.58em;"/></p>
<p style="padding-left: 60px">If we build an artificial neural network with four inputs (the four features in the input data) and one hidden layer containing two neurons, we need eight weight parameters: <kbd>w<sub>11</sub></kbd>, <kbd><span>w</span><sub>12</sub></kbd><span>, <kbd>w<sub>13</sub></kbd>, and <kbd>w<sub>14</sub></kbd> for hidden neuron one, and <kbd>w<sub>2</sub><sub>1</sub></kbd>, <kbd>w<sub>21</sub></kbd>, <kbd>w<sub>23</sub></kbd> and <kbd>w<sub>24</sub></kbd> for hidden neuron two. The remaining parameters will be explained later.</span></p>
<ol start="5">
<li>From the worksheet named <kbd>transfusion</kbd><em>,</em> copy the first 500 data rows (excluding the header).</li>
<li>In the <kbd>training1</kbd> worksheet click on cell <em>B22.</em></li>
<li>Paste the copied cells.</li>
<li>You now have a table containing the input values, called x<sub>1</sub>, x<sub>2</sub>, <span>x</span><sub>3</sub>, <sub>and</sub> <span>x</span><sub>4</sub>, plus the output binary value, y. Column <em>#</em> in the table just shows the row number.</li>
</ol>
<p style="padding-left: 60px">The combination function of the hidden neuron <em>j</em> is the weighted sum of the inputs, as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/61e5a896-c544-4479-92ce-70b8a57ad7b9.png" style="width:11.33em;height:4.08em;"/></p>
<p style="padding-left: 60px">In our example, <em>N=4</em>, which gives us the next two expressions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d7b2f35d-7a34-4e15-84a2-f6aaf583b23f.png" style="width:26.25em;height:1.33em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2e2aefae-4e9f-4556-acbb-d2ec47513588.png" style="width:26.25em;height:1.33em;"/></p>
<ol start="9">
<li>Taking into account these expressions, write the following formula in cell <em>G22</em>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>=$E$3*B22+$E$4*C22+$E$5*D22+$E$6*E22</em></p>
<p style="padding-left: 60px">In cell <em>H22</em>, write the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>=$E$7*B22+$E$8*C22+$E$9*D22+$E$10*E22</em></p>
<ol start="10">
<li>Copy these expressions down to the rest of the cells in columns G and H. The simplest and most commonly used activation function is the following sigmoid function:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f5b8a1e-0a46-446d-a652-b3fb87820ca5.png" style="width:8.33em;height:2.83em;"/></p>
<p style="padding-left: 60px">In our example, x is the combination function calculated for each hidden neuron and each entry used for training:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/61804567-fd35-481b-8e62-d2f33bb64a3d.png" style="width:16.92em;height:2.92em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8d789415-0d7b-46c7-bc99-fbb0f31af1d0.png" style="width:17.92em;height:3.17em;"/></p>
<ol start="11">
<li>Define cell I22 as <em>=1/(1+EXP(-G22))</em> and cell J22 as <em>=1/(1+EXP(-H22)).</em></li>
<li>Copy these formulas down to the rest of the rows in columns I and J. The last calculation is the neural network output, which is a weighted sum of the outputs from the hidden neurons, plus a constant value that acts as a threshold; if the total input is less that this value, the output is zero and the network does not activate. This can be expressed by the following formula:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/342955f2-5ac0-4bcc-ac54-baca8e6f59a2.png" style="width:15.25em;height:1.25em;"/></p>
<ol start="13">
<li>You can then write the following in cell <em>K22</em>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>=$E$11+$E$12*I22+$E$13*J22</em></p>
<ol start="14">
<li>Copy the formula down to the rest of the cells in column K. Since <em>E11</em>, <em>E12</em>, and <em>E13</em> are the cells we have saved for theta 1, theta 2 and theta 3 respectively. We use all the defined weights and parameters in our calculations, but we don't have values for them. Training a neural network implies finding the values for these parameters that make the output as close as the target value as possible, for example, for each combination of <em><span>x</span><sub>1</sub></em><span>, <em>x</em></span><em><sub>2</sub></em><span>,</span> <em><span>x</span><sub>3</sub></em><span>,and <em>x<sub>4</sub></em>, the difference between the value of</span> <em>Output</em> <span>and the value of</span> <em>y</em> <span>should be the minimum possible. We need to calculate three values then: the output error (Output-y), the squared error (Error</span><sup>2</sup><span>), and the sum of the squared errors, which is the value to minimize.</span></li>
</ol>
<div class="packt_tip">The function we are minimizing, the sum of the squared errors, is only one possible <strong>loss function</strong><em>.</em> There are other functions that are used to compare the output of the neural network with the training value. Studying when to apply each function is shown in more advanced machine learning books.</div>
<ol start="15">
<li>Define cell L22 as <em>=K22-F22.</em></li>
<li>Copy the formula down to the rest of the rows in column L.</li>
<li>Define cell M22 as <em>=L22^2</em>.</li>
<li>Copy the formula down to the rest of the rows in column M.</li>
<li>Define cell E15 as <em>=SUM(M22:M521).</em> This is the sum of the squared errors.</li>
</ol>
<p>We can now use Excel's Solver to set values to <span>w</span><sub>11</sub><span>,</span> <span>w</span><sub>12</sub><span>, w<sub>13</sub>,w<sub>14</sub>, w<sub>2</sub><sub>1</sub>, w<sub>21</sub>, w<sub>23</sub>, w<sub>24</sub>, θ<sub>o</sub>, θ<sub>1</sub>, and θ<sub>2</sub>, while minimizing the sum of the squared errors:</span></p>
<ol>
<li>Navigate to <span class="packt_screen">Data.</span></li>
<li>Click on <span class="packt_screen">Solver.</span></li>
</ol>
<ol start="3">
<li>Fill in the details as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6c5c74e7-bfe4-4416-9cc2-d95d14f046be.png" style="width:41.50em;height:39.50em;"/></p>
<p style="padding-left: 60px">The objective is E15, where we store the sum of squared errors, and the variable cells E3 to E13.</p>
<ol start="4">
<li>Click on <span class="packt_screen">Solve.</span></li>
<li>The optimal result is shown in the following table:</li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Parameters</strong></p>
</td>
<td>
<p><strong>Values</strong></p>
</td>
</tr>
<tr>
<td>
<p>w11</p>
</td>
<td>
<p>-3.915205816</p>
</td>
</tr>
<tr>
<td>
<p>w12</p>
</td>
<td>
<p>0.055009315</p>
</td>
</tr>
<tr>
<td>
<p>w13</p>
</td>
<td>
<p>0.016855755</p>
</td>
</tr>
<tr>
<td>
<p>w14</p>
</td>
<td>
<p>-0.301397506</p>
</td>
</tr>
<tr>
<td>
<p>w21</p>
</td>
<td>
<p>-0.016701972</p>
</td>
</tr>
<tr>
<td>
<p>w22</p>
</td>
<td>
<p>0.451221978</p>
</td>
</tr>
<tr>
<td>
<p>w23</p>
</td>
<td>
<p>-0.001645853</p>
</td>
</tr>
<tr>
<td>
<p>w24</p>
</td>
<td>
<p>-0.011395209</p>
</td>
</tr>
<tr>
<td>
<p>theta0</p>
</td>
<td>
<p>-0.349977457</p>
</td>
</tr>
<tr>
<td>
<p>theta1</p>
</td>
<td>
<p>0.247932886</p>
</td>
</tr>
<tr>
<td>
<p>theta2</p>
</td>
<td>
<p>1.256803829</p>
</td>
</tr>
<tr>
<td>
<p><strong>Square error</strong></p>
</td>
<td>
<p><strong>77.02669809</strong></p>
</td>
</tr>
</tbody>
</table>
<div class="packt_tip">The results may vary depending on the type of regression used in Solver and on the initial values. The gradient descent algorithm <span>search</span> (explained in the <em>Understanding the backpropagation algorithm</em> section) might be trapped in a local minimum that has a larger value than the global minimum.</div>
<ol start="6">
<li>Define cell N22 as <em>=round(K22)</em> to convert the output of the neural network to binary values.</li>
<li>Comparing the predicted and linear values, you can build the confusion matrix:</li>
</ol>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td/>
<td>
<p>Real</p>
</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>Predicted</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>32</p>
</td>
<td>
<p>22</p>
</td>
</tr>
<tr>
<td/>
<td>
<p>0</p>
</td>
<td>
<p>86</p>
</td>
<td>
<p>360</p>
</td>
</tr>
</tbody>
</table>
<ol start="8"/>
<div class="packt_tip">Use the confusion matrix to measure the accuracy of the neural network training.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the neural network</h1>
                </header>
            
            <article>
                
<p>Once you are satisfied with the training, you can use the values obtained for the parameters to <strong>predict</strong> the y value for the rest of the data (which was never used in the training, and can then be used to test the network output).</p>
<p>Follow these steps to predict the target variable using the test dataset:</p>
<ol start="1">
<li>Make a copy of the worksheet named <kbd>training1</kbd>. Name the new worksheet <kbd>test1</kbd>.</li>
<li>Delete the range of cells <em>B22:F521.</em></li>
<li>Copy the last 248 rows in the worksheet named <kbd>transfusion</kbd> to the new worksheet, starting on cell B22.</li>
<li>All calculations should work and you should be able to see the results of using the test data as input.</li>
</ol>
<p>We have now developed a simple exercise that shows how an artificial neural network learns from input data. The calculations we made are the base of the <strong>backpropagation</strong> algorithm, which is explained in detail in the last section of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a deep network</h1>
                </header>
            
            <article>
                
<p>Our example of artificial neural network is very simple and only contains one hidden layer. Can we add more layers? Of course we can! The next step in complexity could be something similar to the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5c050b30-e1ed-4643-aa73-191c25058bc4.png" style="width:32.17em;height:24.58em;"/></p>
<p>We added a new hidden layer with two neurons, but we could add more layers and more neurons per layer. The architecture of a network depends on the specific use we give it. Multilayer artificial neural networks are often known as <strong>deep neural networks</strong><em>.</em></p>
<p>The output of a deep network is calculated in analogy with the single layer one, considering all inputs to each neuron, the activation function, and the addition of all the inputs to the output neuron. Looking at the preceding diagram, it is clear that each layer in the network is affected by the previous one. It is usually the case that, in order to solve complex problems, each layer learns a specific set of characteristics. For example, when identifying an image, the first layer could train on colors, the second on shapes, the third on objects, and so on, increasing in complexity as we advance toward the output.</p>
<p>As we add more neurons to the network, there are more parameters we need to adjust. The way this is done in practice will become clear in the following section, where the backpropagation algorithm is described.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the backpropagation algorithm</h1>
                </header>
            
            <article>
                
<p>There are two phases in the training process of a deep neural network: forward and back propagation. We have seen the forward phase in detail:</p>
<ol>
<li>Calculate the weighted sum of the inputs:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f2c41a91-087f-4c24-9267-6ffa5e890830.png" style="width:12.50em;height:4.50em;"/></p>
<ol start="2">
<li>Apply the activation function to the result:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16c6e136-e030-4cd2-a071-df9461aca7dc.png" style="width:16.58em;height:2.92em;"/></p>
<div class="packt_tip">Find different activation functions in the suggested reading at the end of the chapter. The sigmoid function is the most common and is easier to use, but not the only one.</div>
<ol start="3">
<li>Calculate the output by adding all the results from the last layer (N neurons):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d5f5294b-af9c-4713-85b2-41757d612f22.png" style="width:14.58em;height:4.58em;"/></p>
<p>After the forward phase, we calculate the error as the difference between the output and the known target value: <em>Error = (Output-y)<sup>2</sup>.</em></p>
<p>All weights are assigned random values at the beginning of the forward phase.</p>
<p>The output, and therefore the error, are functions of the weights <em>w<sub>i</sub></em> and <em>θ<sub>i</sub></em>. This means that we could go backward from the error and see how a small variation in each weight affects the result. This is expressed in mathematical terms as the derivative or gradient:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/08aca1b3-5c40-4fa9-b81c-6537eb05be9c.png" style="width:3.42em;height:2.33em;"/></p>
<p>This equation measures the change in the error every time we change w<sub>1</sub> by a small amount. We actually apply an activation function inside each neuron, so the change in error turns into the <span>following equation (known as the chain rule):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c02d2573-640e-4923-b633-c129c4ca12aa.png" style="width:11.33em;height:2.50em;"/></p>
<p>We want to change all weights values in the direction that decreases the error. This is the reason why the optimization method is called <strong>gradient descent</strong>. If we imagine the error as a function of two weights (there are more than two, of course, but we human beings have a hard time thinking beyond three dimensions!), we can picture this optimization as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a5ef8f3f-cfa8-4b61-aaf1-05b5f915d86f.png" style="width:31.50em;height:28.67em;"/></p>
<p class="mce-root">When are the weights adjusted? There are three methods:</p>
<ul>
<li><strong>Online</strong>: With each new training sample, all weights are recalculated. This is very time-consuming and could lead to problems if the dataset has too many outliers.</li>
<li><strong>Batch</strong>: The weights are calculated for the whole training dataset, calculating the accumulated error and using it to correct them.</li>
<li><strong>Stochastic</strong>: The batch mode is used taking small samples of the training data. This speeds up the whole process and makes the method more robust against local optimal values.</li>
</ul>
<p class="mce-root">We are now familiar with how artificial neural networks are built and how their output is calculated. It is generally impractical to perform these calculations as the size of the network grows, as often happens for all practical and useful implementations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We have studied the basic principles of how artificial neural networks are built and how they learn from the input data. Even if the actual method, in practice, for using neural networks is different than what we have done in our example, our approach is useful in order to understand the details and to go beyond the idea that neural networks are mysterious black boxes that magically solve problems.</p>
<p>In the next chapter, we will see how we can use pre-built machine learning models available in Azure, connecting them to Excel to solve the problems we have presented up to now.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Using the results of the perceptron test, build the confusion matrix and evaluate the quality of the prediction.</li>
<li>There is one important step that is missing in the binary classification problem that we solved with our artificial neural network, which might improve the result if we implement it. What did we miss? Hint: build an histogram of the binary variable that indicates whether there was a blood donation in March 2007.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><em>Brief Introduction to Neural Networks</em> by <span>David Kriesel</span><em>,</em> available online at <a href="http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf">http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf</a></li>
<li><em>Neural Networks and Deep Learning</em> by Michael A. Nielsen<em>,</em> available online at <a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a></li>
<li><span><em>Deep Learning: Using Algorithms to Make Machines Think</em>,</span> <a href="https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/">https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/</a>
<p> </p>
<p> </p>
</li>
</ul>


            </article>

            
        </section>
    </body></html>