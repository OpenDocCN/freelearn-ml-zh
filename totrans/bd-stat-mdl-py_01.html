<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer020">
<p><a id="_idTextAnchor014"/></p>
<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor015"/>1</h1>
<h1 id="_idParaDest-16">Sampling and Generalization<a id="_idTextAnchor016"/></h1>
<p><a id="_idTextAnchor017"/>In this chapter, we will describe the concept of populations and sampling from populations, including some common strategies for sampling. The discussion of <a id="_idIndexMarker000"/>sampling will lead to a section that will<a id="_idIndexMarker001"/> describe generalization. Generalization will be discussed as it relates to using samples to make conclusions about their respective populations. When modeling for statistical inference, it is necessary to ensure that samples can be generalized to populations. We will provide an in-depth overview of this bridge through the subjects in <span class="No-Break">this chapter<a id="_idTextAnchor018"/>.</span></p>
<p>We will cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Software and <span class="No-Break">environment setup</span></li>
<li>Population <span class="No-Break">versus sample</span></li>
<li>Population inference <span class="No-Break">from samples</span></li>
<li>Sampling strategies – random, systematic, <span class="No-Break">and stratifie<a id="_idTextAnchor019"/>d</span></li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor020"/>Software and environment setup</h1>
<p><strong class="bold">Python</strong> is one <a id="_idIndexMarker002"/>of the most popular programming languages for data science and machine learning thanks to the large open source community that has driven the development of these libraries. Python’s ease of use and flexible nature made it a prime candidate in the data science world, where experimentation and iteration are key features of the development cycle. While there are new languages in development for data science <a id="_idIndexMarker003"/>applications, such<a id="_idIndexMarker004"/> as <strong class="bold">Julia</strong>, Python currently remains the key language for data science due to its wide breadth of open source projects, supporting applications from statistical modeling to deep learning. We have chosen to use Python in this book due to its positioning as an important language for data science and its demand in the <span class="No-Break">job market.</span></p>
<p>Python is available for all major operating systems: Microsoft Windows, macOS, and Linux. Additionally, the installer<a id="_idIndexMarker005"/> and documentation can be found at the official <span class="No-Break">website: </span><a href="https://www.python.org/"><span class="No-Break">https://www.python.org/</span></a><span class="No-Break">.</span></p>
<p>This book is written for Python version 3.8 (or higher). It is recommended that you use whatever recent version of Python that is available. It is not likely that the code found in this book will be compatible with Python 2.7, and most active libraries have already started dropping support for Python 2.7 since official support ended <span class="No-Break">in 2020.</span></p>
<p>The libraries<a id="_idIndexMarker006"/> used in this book can be installed with the Python package manager, <strong class="source-inline">pip</strong>, which is<a id="_idIndexMarker007"/> part of the standard Python library in contemporary versions of Python. More information <a id="_idIndexMarker008"/>about <strong class="source-inline">pip</strong> can be found here: <a href="https://docs.python.org/3/installing/index.xhtml">https://docs.python.org/3/installing/index.xhtml</a>. After <strong class="source-inline">pip</strong> is installed, packages can be installed using <strong class="source-inline">pip</strong> on the command line. Here is basic usage at <span class="No-Break">a glance:</span></p>
<p>Install a new package using the <span class="No-Break">latest version:</span></p>
<pre class="source-code">
pip install SomePackage</pre>
<p>Install the package with a specific version, version <strong class="source-inline">2.1</strong> in <span class="No-Break">this example:</span></p>
<pre class="source-code">
pip install SomePackage==2.1</pre>
<p>A package that is already installed can be upgraded with the <strong class="source-inline">--</strong><span class="No-Break"><strong class="source-inline">upgrade</strong></span><span class="No-Break"> flag:</span></p>
<pre class="source-code">
pip install SomePackage –upgrade</pre>
<p>In general, it is recommended to use Python virtual environments between projects and to keep project dependencies separate from system directories. Python provides a virtual environment<a id="_idIndexMarker009"/> utility, <strong class="source-inline">venv</strong>, which, like <strong class="source-inline">pip</strong>, is part of the standard library in contemporary versions of Python. Virtual environments allow you to create individual binaries of Python, where each binary of Python has its own set of installed dependencies. Using virtual environments can prevent package version issues and conflict when working on multiple Python projects. Details on setting up and using virtual environments can be found <span class="No-Break">here: </span><a href="https://docs.python.org/3/library/venv.xhtml"><span class="No-Break">https://docs.python.org/3/library/venv.xhtml</span></a><span class="No-Break">.</span></p>
<p>While we recommend the use of Python and Python’s virtual environments for environment setups, a highly recommended <a id="_idIndexMarker010"/>alternative is <strong class="bold">Anaconda</strong>. Anaconda is a free (enterprise-ready) analytics-focused distribution of Python by Anaconda Inc. (previously Continuum Analytics). Anaconda <a id="_idIndexMarker011"/>distributions come with many of the core<a id="_idIndexMarker012"/> data science packages, common IDEs (such as <strong class="bold">Jupyter</strong> and <strong class="bold">Visual Studio Code</strong>), and a graphical user interface for managing environments. Anaconda can be installed using <a id="_idIndexMarker013"/>the installer found at the Anaconda website <span class="No-Break">here: </span><a href="https://www.anaconda.com/products/distribution"><span class="No-Break">https://www.anaconda.com/products/distribution</span></a><span class="No-Break">.</span></p>
<p>Anaconda comes with its own package manager, <strong class="source-inline">conda</strong>, which can be used to install new packages similarly <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">.</span></p>
<p>Install a new package using the <span class="No-Break">latest version:</span></p>
<pre class="source-code">
conda install SomePackage</pre>
<p>Upgrade a package that is <span class="No-Break">already installed:</span></p>
<pre class="source-code">
conda upgrade SomePackage</pre>
<p>Throughout<a id="_idIndexMarker014"/> this book, we will make use of several core libraries in the Python data science ecosystem, such as <strong class="source-inline">NumPy</strong> for array manipulations, <strong class="source-inline">pandas</strong> for higher-level data manipulations, and <strong class="source-inline">matplotlib</strong> for data visualization. The package versions used for this book are contained in the following list. Please ensure that the versions installed in your environment are equal to or greater than the versions listed. This will help ensure that the code examples <span class="No-Break">run correctly:</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">statsmodels 0.13.2</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">Matplotlib 3.5.2</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">NumPy 1.23.0</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">SciPy 1.8.1</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">scikit-learn 1.1.1</strong></span></li>
<li><span class="No-Break"><strong class="source-inline">pandas 1.4.3</strong></span></li>
</ul>
<p>The packages used for the code in this book are shown here in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em>. The <strong class="source-inline">__version__</strong> method can be used to print the package version <span class="No-Break">in code.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.1 – Package versions used in this book" height="866" src="image/B18945_01_001.jpg" width="1368"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Package versions used in this book</p>
<p>Having <a id="_idIndexMarker015"/>set up the technical environment for the book, let’s get into the statistics. In the next sections, we will discuss the concepts of population and sampling. We will demonstrate sampling strategies with <span class="No-Break">code implementations.</span></p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor021"/>Population versus sample</h1>
<p>In general, the<a id="_idIndexMarker016"/> goal of statistical modeling is to answer a question about a group by making an inference about that group. The group we are making an inference on could be machines in a production factory, people voting in an election, or plants on different plots of land. The entire group, every individual item or entity, is referred to as<a id="_idIndexMarker017"/> the <strong class="bold">population</strong>. In most cases, the population of interest is so large that it is not practical or even possible to collect data on every entity in the population. For instance, using the voting example, it would probably not be possible to poll every person that voted in an election. Even if it was possible to reach all the voters for the election of interest, many voters may not consent to polling, which would prevent collection on the entire population. An additional consideration would be the expense of polling such a large group. These factors make it practically impossible to collect population statistics in our example of vote polling. These types of prohibitive factors exist in many <a id="_idIndexMarker018"/>cases where we may want to assess a population-level attribute. Fortunately, we do not need to collect data on the entire population of interest. Inferences about a population can be made using a subset of the population. This subset of the population is <a id="_idIndexMarker019"/>called a <strong class="bold">sample</strong>. This is the main idea of statistical modeling. A model will be created using a sample and inferences will be made about <span class="No-Break">the population.</span></p>
<p>In order to make valid inferences about the population of interest using a sample, the sample must be <em class="italic">representative</em> of the population of interest, meaning that the sample should contain the variation found in the population. For example, if we were interested in making an inference about plants in a field, it is unlikely that samples from one corner of the field would be sufficient for inferences about the larger population. There would likely be variations in plant characteristics over the entire field. We could think of various reasons why there might be variation. For this example, we will consider some examples from <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 1.2 – Field of plants" height="1290" src="image/B18945_01_002.jpg" width="989"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Field of plants</p>
<p>The <a id="_idIndexMarker020"/>figure shows that <strong class="bold">Sample A</strong> is near a forest. This sample area may be affected by the presence of the forest; for example, some of the plants in that sample may receive less sunlight than plants in the other sample. <strong class="bold">Sample B</strong> is shown to be in between the main irrigation lines. It’s conceivable that this sample receives more water on average than the other two samples, which may have an effect on the plants in this sample. The final <strong class="bold">Sample C</strong> is near a road. This sample may see other effects that are not seen in <strong class="bold">Sample A</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="bold">B</strong></span><span class="No-Break">.</span></p>
<p>If samples were only taken from one of those sections, the inferences from those samples would be <em class="italic">biased</em> and would not provide valid references about the population. Thus, samples would need to be taken from across the entire field to create a sample that is more likely to be representative of the population of plants. When taking samples from populations, it is critical to ensure the sampling method is robust to possible issues, such as the influence of irrigation and shade in the previous example. Whenever taking a sample from a population, it’s important to identify and mitigate possible influences of bias because biases in data will affect your model and skew <span class="No-Break">your conclusions.</span></p>
<p>In the next section, various<a id="_idIndexMarker021"/> methods for sampling from a dataset will be discussed. An additional consideration is the sample size. The sample size impacts the type of statistical tools we can use, the distributional assumptions that can be made about the sample, and the confidence of inferences and predictions. The impact of sample size will be explored in depth in <a href="B18945_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Distributions of Data </em>and<em class="italic"> </em><a href="B18945_03.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, </em><span class="No-Break"><em class="italic">Hypothesis Testing</em></span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor022"/>Population inference from samples</h1>
<p>When using <a id="_idIndexMarker022"/>a statistical model to make inferential conclusions about a population from a sample subset of that population, the study design must account for similar degrees of uncertainty in its variables as those in the population. This is the variation mentioned earlier in this chapter. To appropriately draw inferential conclusions about a population, any statistical model must be structured around a chance mechanism. Studies structured around these chance mechanisms are called <strong class="bold">randomized experiments</strong> and provide an understanding of both correlation <span class="No-Break">and causation.</span></p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor023"/>Randomized experiments</h2>
<p>There are<a id="_idIndexMarker023"/> two primary characteristics <a id="_idIndexMarker024"/>of a <span class="No-Break">randomized experiment:</span></p>
<ul>
<li>Random sampling, colloquially referred to as <span class="No-Break">random selection</span></li>
<li>Random assignment of treatments, which is the nature of <span class="No-Break">the study</span></li>
</ul>
<h3>Random sampling</h3>
<p>Random<a id="_idIndexMarker025"/> sampling (also called random selection) is designed with the intent of creating a sample representative of the overall population so that statistical models generalize the population well enough to assign cause-and-effect outcomes. In order for random sampling to be successful, the population of interest must be well defined. All samples taken from the population must have a chance of being selected. In considering the example of polling voters, all voters must be willing to be polled. Once all voters are entered into a lottery, random sampling can be used to subset voters for modeling. Sampling from only voters who are willing to be polled introduces sampling bias into statistical modeling, which can lead to skewed results. The <a id="_idIndexMarker026"/>sampling method in the scenario where only some voters are willing to participate is <a id="_idIndexMarker027"/>called <strong class="bold">self-selection</strong>. Any information obtained and modeled from self-selected samples – or any non-random samples – cannot be used <span class="No-Break">for inference.</span></p>
<h3>Random assignment of treatments</h3>
<p>The<a id="_idIndexMarker028"/> random assignment of treatments refers to <span class="No-Break">two motivators:</span></p>
<ul>
<li>The first motivator is to gain an understanding of specific input variables and their influence on the response – for example, understanding whether assigning treatment A to a specific individual may produce more favorable outcomes than <span class="No-Break">a placebo.</span></li>
<li>The second motivator is to remove the impact of external variables on the outcomes of a study. These external variables, called <strong class="bold">confounding variables</strong> (or <strong class="bold">confounders</strong>), are<a id="_idIndexMarker029"/> important to remove as they often prove difficult to control. They may have unpredictable values or even be unknown to the researcher. The consequence of including confounders is that the outcomes of a study may not be replicable, which can be costly. While confounders can influence outcomes, they can also influence input variables, as well as the relationships between <span class="No-Break">those variables.</span></li>
</ul>
<p>Referring back to the example in the earlier section, <em class="italic">Population versus sample</em>, consider a farmer who decides to start using pesticides on his crops and wants to test two different brands. The farmer knows there are three distinct areas of the land; plot A, plot B, and plot C. To determine the success of the pesticides and prevent damage to the crops, the farmer randomly chooses 60 plants from<a id="_idIndexMarker030"/> each plot (this is called <strong class="bold">stratified random sampling</strong> where random sampling is stratified across each plot) for testing. This selection is representative of the population of plants. From this selection, the farmer labels his plants (labeling doesn’t need to be random). For each plot, the farmer shuffles the labels into a bag, to randomize them, and begins selecting 30 plants. The first 30 plants get one of two treatments and the other 30 are given the other treatment. This is a <em class="italic">random assignment of treatment</em>. Assuming the three separate plots represent a distinct <a id="_idIndexMarker031"/>set of confounding variables on crop yield, the farmer will have enough information to obtain an inference about the crop yield for each <span class="No-Break">pesticide brand.</span></p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor024"/>Observational study</h2>
<p>The other<a id="_idIndexMarker032"/> type of statistical study often<a id="_idIndexMarker033"/> performed is an <strong class="bold">observational study</strong>, in which the researcher seeks to learn through observing data that already exists. An observational study can aid in the understanding of input variables and their relationships to both the target and each other, but cannot provide cause-and-effect understanding as a randomized experiment can. An observational study may have one of the two components of a randomized experiment – either random sampling or random assignment of treatment – but without both components, will not directly yield inference. There are many reasons why an observational study may be performed versus a randomized experiment, such as <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker034"/></span><span class="No-Break"> following:</span></p>
<ul>
<li>A randomized experiment being <span class="No-Break">too costly</span></li>
<li>Ethical constraints for an experiment (for example, an experiment to determine the rate of birth defects caused by smoking <span class="No-Break">while pregnant)</span></li>
<li>Using data from prior randomized experiments, which thus removes the need for <span class="No-Break">another experiment</span></li>
</ul>
<p>One method for deriving some causality from an observational study is to perform random sampling and repeated analysis. Repeated random sampling and analysis can help minimize the impact of confounding variables over time. This concept plays a huge role in the usefulness of <em class="italic">big data</em> and <em class="italic">machine learning</em>, which has gained a lot of importance in many industries within this century. While almost any tool that can be used for observational analysis can also be used for a randomized experiment, this book focuses primarily on tools for observational analysis, as this is more common in <span class="No-Break">most industries.</span></p>
<p>It can be said that statistics is a science for helping make the best decisions when there are quantifiable uncertainties. All statistical tests contain a null hypothesis and an alternative hypothesis. That is to say, an assumption that there is no statistically significant difference between data (the null hypothesis) or that there is a statistically significant difference <a id="_idIndexMarker035"/>between data (the alternative hypothesis). The term statistically significant difference implies the existence of a benchmark – or threshold – beyond which a measure takes <a id="_idIndexMarker036"/>place and indicates significance. This benchmark is called the <span class="No-Break"><strong class="bold">critical value</strong></span><span class="No-Break">.</span></p>
<p>The measure that is<a id="_idIndexMarker037"/> applied against this critical value is called<a id="_idIndexMarker038"/> the <strong class="bold">test statistic</strong>. The critical value is a static value quantified based on behavior in the data, such as the average and variation, and is based on the hypothesis. If there are two possible routes by which a null hypothesis may be rejected – for example, we believe some output is either less than or more than the average – there will be two <a id="_idIndexMarker039"/>critical values (this test is called a <strong class="bold">two-tailed</strong> hypothesis test), but if there is only one argument against the null hypothesis, there will be only one<a id="_idIndexMarker040"/> critical value (this is called a <strong class="bold">one-tailed</strong> hypothesis test). Regardless of the number of critical values, there will always only be one test statistic measurement for each group within a given hypothesis test. If the test statistic exceeds the critical value, there is a statistically significant reason to support rejecting the null hypothesis and concluding there is a statistically significant difference in <span class="No-Break">the data.</span></p>
<p>It is useful to understand that a hypothesis test can test <span class="No-Break">the following:</span></p>
<ul>
<li>One variable against another (such as in <span class="No-Break">a t-test)</span></li>
<li>Multiple variables against one variable (for example, <span class="No-Break">linear regression)</span></li>
<li>Multiple variables against multiple variables (for <span class="No-Break">example, MANOVA)</span></li>
</ul>
<p>In the following figure, we can see visually the relationship between the test statistic and critical values in a two-tailed <span class="No-Break">hypothesis test.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 1.3 – Critical values versus a test statistic in a two-tailed hypothesis test" height="574" src="image/B18945_01_003.jpg" width="754"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Critical values versus a test statistic in a two-tailed hypothesis test</p>
<p>Based on <a id="_idIndexMarker041"/>the figure, we now have a visual idea of how a test statistic exceeding the critical value suggests rejecting the <span class="No-Break">null hypothesis.</span></p>
<p>One concern <a id="_idIndexMarker042"/>with using only the approach of measuring test statistics against critical values in the hypothesis, however, is that test statistics can be impractically large. This is likely to occur when there may be a wide range of results that are not considered to fall within the bounds of a treatment effect. It is uncertain whether a result as extreme as or more extreme than the test statistic is possible. To prevent misleadingly rejecting the null hypothesis, a <strong class="bold">p-value</strong> is used. The <a id="_idIndexMarker043"/>p-value represents the probability that chance alone resulted in a value as extreme as the one observed (the one that suggests rejecting the null hypothesis). If a p-value is low, relative to the level of significance, the null hypothesis can be rejected. Common levels of significance are 0.01, 0.05, and 0.10. It is beneficial to confirm prior to making a decision on a hypothesis to assess both the critical value’s relationship to the test statistic and the p-value. More will be discussed in <a href="B18945_03.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, Hypothesis Testing</em>, when we begin discussing <span class="No-Break">hypothesis testing.</span></p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor025"/>Sampling strategies – random, systematic, stratified, and clustering</h1>
<p>In this section, we <a id="_idIndexMarker044"/>will discuss the different sampling methods used in research. Broadly speaking, in the real world, it is not easy or possible to get the whole population data for many reasons. For instance, the costs of gathering data are expensive in terms of money and time. Collecting all the data is impractical in many cases and ethical issues are also considered. Taking samples from the population can help us overcome these problems and is a more efficient way to collect data. By collecting an appropriate sample for a study, we can draw statistical conclusions or statistical inferences about the population properties. Inferential statistical analysis is a fundamental aspect of statistical thinking. Different sampling methods from probability strategies to non-probability strategies used in research and industry will be discussed in <span class="No-Break">this section.</span></p>
<p>There are essentially two types of <span class="No-Break">sampling methods:</span></p>
<ul>
<li><span class="No-Break">Probability sampling</span></li>
<li><span class="No-Break">Non-probability sampling</span></li>
</ul>
<h2 id="_idParaDest-23"><a id="_idTextAnchor026"/>Probability sampling</h2>
<p>In <em class="italic">probability sampling</em>, a<a id="_idIndexMarker045"/> sample is chosen from a<a id="_idIndexMarker046"/> population based on the theory of probability, or it is chosen randomly using random selection. In <em class="italic">random selection</em>, the chance of each member in a population being selected is equal. For example, consider a game with 10 similar pieces of paper. We write numbers 1 through 10, with a separate piece of paper for each number. The numbers are then shuffled in a box. The game requires picking three of these ten pieces of paper randomly. Because the pieces of paper have been prepared using the same process, the chance of any piece of paper being selected (or the numbers one through ten) is equal for each piece. Collectively, the 10 pieces of paper are considered a population and the 3 selected pieces of paper constitute a random sample. This example is one approach to the probability sampling methods we will discuss in <span class="No-Break">this chapter.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 1.4 – A random sampling example" height="688" src="image/B18945_01_004.jpg" width="1011"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – A random sampling example</p>
<p>We <a id="_idIndexMarker047"/>can implement the sampling method described before (and shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em>) with <strong class="source-inline">numpy</strong>. We will use the <strong class="source-inline">choice</strong> method to <a id="_idIndexMarker048"/>select three samples from the given population. Notice that <strong class="source-inline">replace==False</strong> is used in the choice. This means that once a sample is chosen, it will not be considered again. Note that the random generator is used in the following code <span class="No-Break">for reproducibility:</span></p>
<pre class="source-code">
import numpy as np
# setup generator for reproducibility
random_generator = np.random.default_rng(2020)
population = np.arange(1, 10 + 1)
sample = random_generator.choice(
    population,    #sample from population
    size=3,        #number of samples to take
    replace=False  #only allow to sample individuals once
)
print(sample)
# array([1, 8, 5])</pre>
<p>The <a id="_idIndexMarker049"/>purpose of random selection is to avoid a <a id="_idIndexMarker050"/>biased result when some units of a population have a lower or higher probability of being chosen in a sample than others. Nowadays, a random selection process can be done by using computer <span class="No-Break">randomization programs.</span></p>
<p>Four main types of the probability sampling methods that will be discussed here are <span class="No-Break">as follows:</span></p>
<ul>
<li>Simple <span class="No-Break">random sampling</span></li>
<li><span class="No-Break">Systematic sampling</span></li>
<li><span class="No-Break">Stratified sampling</span></li>
<li><span class="No-Break">Cluster sampling</span></li>
</ul>
<p>Let’s look at each one <span class="No-Break">of them.</span></p>
<h3>Simple random sampling</h3>
<p>First, simple random sampling<a id="_idIndexMarker051"/> is a method <a id="_idIndexMarker052"/>to select a sample randomly from a population. Every member of the subset (or the sample) has an equal chance of being chosen through an unbiased selection method. This method is used when all members of a population have similar properties related to important variables (important features) and it is the most direct approach to probability sampling. The advantages of this method are to minimize bias and maximize representativeness. However, while this method helps limit a biased approach, there is a risk of errors with simple random sampling. This method also has some limitations. For instance, when the population is very large, there can be high costs and a lot of time required. Sampling errors need to be considered when a sample is not representative of the population and the study needs to perform this sampling process again. In addition, not every member of a population is willing to participate in the study voluntarily, which makes it a big challenge to obtain good information representative of a large population. The previous example of choosing 3 pieces of paper from 10 pieces of paper is a simple <span class="No-Break">random sample.</span></p>
<h3>Systematic sampling</h3>
<p>Here, members<a id="_idIndexMarker053"/> of a population are selected at a<a id="_idIndexMarker054"/> random starting point with a fixed sampling interval. We first choose a fixed sampling interval by dividing the number of members in a population by the number of members in a sample that the study conducts. Then, a random starting point between the number one and the number of members in the sampling interval is selected. Finally, we choose subsequent members by repeating this sampling process until enough samples have been collected. This method is faster and preferable than simple random sampling when cost and time are the main factors to be considered in the study. On the other hand, while in simple random sampling, each member of a population has an equal chance of being selected, in systematic sampling, a sampling interval rule is used to choose a member from a population in a sample for a study. It can be said that systematic sampling is less random than simple random sampling. Similarly, as in simple random sampling, member properties of a population are similarly related to important variables/features. Let us discuss how we perform systematic sampling through the following example. In a class at one high school in Dallas, there are 50 students but only 10 books to give to these students. The sampling interval is fixed by dividing the number of students in the class by the number of books (50/10 = 5). We also need to generate a random number between one and 50 as a random starting point. For example, take the number 18. Hence, the 10 students selected to get the books will be <span class="No-Break">as follows:</span></p>
<p>18, 23, 28, 33, 38, 43, 48, 3, <span class="No-Break">8, 13</span></p>
<p>The natural question arises as to whether the interval sampling is a fraction. For example, if we have 13 books, then the sampling interval will be 50/13 ~ 3.846. However, we cannot choose this fractional number as a sampling interval that represents the number of students. In this situation, we could choose number 3 or 4, alternatively, as the sampling intervals (we could also choose either 3 or 4 as the sampling interval). Let us assume that a random starting point generated is 17. Then, the 13 selected students <span class="No-Break">are these:</span></p>
<p>17, 20, 24, 27, 31, 34, 38, 41, 45, 48, 2, <span class="No-Break">5, 9</span></p>
<p>Observing the preceding series of numbers, after reaching the number 48, since adding 4 will produce a number greater than the count of students (50 students), the sequence restarts at 2 (48 + 4 = 52, but since 50 is the maximum, we restart at 2). Therefore, the last three numbers in<a id="_idIndexMarker055"/> the sequence are 2, 5, and 9, with the sampling intervals 4, 3, and 4, respectively (passing the number 50 and back to the number 1 until we have 13 selected students for the <span class="No-Break">systematic sample).</span></p>
<p>With <a id="_idIndexMarker056"/>systematic sampling, there is a biased risk when the list of members of a population is organized to match the sampling interval. For example, going back to the case of 50 students, researchers want to know how students feel about mathematics classes. However, if the best students in math correspond to numbers 2, 12, 22, 32, and 42, then the survey could be biased if conducted when the random starting point is 2 and the sampling interval <span class="No-Break">is 10.</span></p>
<h3>Stratified sampling</h3>
<p>It is a <a id="_idIndexMarker057"/>probability sampling method based on dividing <a id="_idIndexMarker058"/>a population into homogeneous subpopulations called <strong class="bold">strata</strong>. Each <a id="_idIndexMarker059"/>stratum splits based on distinctly different properties, such as gender, age, color, and so on. These subpopulations must be distinct so that every member in each stratum has an equal chance of being selected by using simple random sampling. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em> illustrates how stratified sampling is performed to select samples from two subpopulations (a set of numbers and a set <span class="No-Break">of letters):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 1.5 – A stratified sample example" height="768" src="image/B18945_01_005.jpg" width="1200"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – A stratified sample example</p>
<p>The <a id="_idIndexMarker060"/>following code sample shows how to implement<a id="_idIndexMarker061"/> stratified sampling with <strong class="source-inline">numpy</strong> using the example shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em>. First, the instances are split into the respective strata: numbers and letters. Then, we use <strong class="source-inline">numpy</strong> to take random samples from each stratum. Like in the previous code example, we utilize the <strong class="source-inline">choice</strong> method to take the random sample, but the sample size for each stratum is based on the total number of instances in each stratum rather than the total number of instances in the entire population; for example, sampling 50% of the numbers and 50% of <span class="No-Break">the letters:</span></p>
<pre class="source-code">
import numpy as np
# setup generator for reproducibility
random_generator = np.random.default_rng(2020)
population = [
  1, "A", 3, 4,
  5, 2, "D", 8,
  "C", 7, 6, "B"
]
# group strata
strata = {
    'number' : [],
    'string' : [],
}
for item in population:
    if isinstance(item, int):
        strata['number'].append(item)
    else:
        strata['string'].append(item)
# fraction of population to sample
sample_fraction = 0.5
# random sample from stata
sampled_strata = {}
for group in strata:
    sample_size = int(
        sample_fraction * len(strata[group])
    )
    sampled_strata[group] = random_generator.choice(
            strata[group],
            size=sample_size,
            replace=False
    )
print(sampled_strata)
#{'number': array([2, 8, 5, 1]), 'string': array(['D', 'C'], dtype='&lt;U1')}</pre>
<p>The <a id="_idIndexMarker062"/>main advantage of this method is that key population <a id="_idIndexMarker063"/>characteristics in a sample better represent the population that is studied and are also proportional to the overall population. This method helps to reduce sample selection bias. On the other hand, when classifying each member of a population into distinct subpopulations is not obvious, this method <span class="No-Break">becomes unusable.</span></p>
<h3>Cluster sampling</h3>
<p>Here, a <a id="_idIndexMarker064"/>population is divided into different subgroups<a id="_idIndexMarker065"/> called clusters. Each cluster has homogeneous characteristics. Instead of randomly selecting individual members in each cluster, entire clusters are randomly chosen and each of these clusters has an equal chance of being selected as part of a sample. If clusters are <a id="_idIndexMarker066"/>large, then we can conduct a <strong class="bold">multistage sampling</strong> by using one of the previous sampling methods to select individual members within each cluster. A cluster sampling example is discussed now. A local pizzeria plans to expand its business in the neighborhood. The owner wants to know how many people order pizzas from his pizzeria and what the preferred pizzas are. He then splits the neighborhood into different areas and selects clients randomly to form cluster samples. A survey is sent to the selected clients for his business study. Another example is related to multistage cluster sampling. A retail chain store conducts a study to see the performance of each store in the chain. The stores are divided into subgroups based on location, then <a id="_idIndexMarker067"/>samples are randomly selected to form clusters, and the sample cluster is used as a performance study of his stores. This<a id="_idIndexMarker068"/> method is easy and convenient. However, the sample clusters are not guaranteed to be representative of the <span class="No-Break">whole population.</span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor027"/>Non-probability sampling</h2>
<p>The <a id="_idIndexMarker069"/>other type of sampling method is non-probability sampling, where some or all members of a population do not have an equal chance <a id="_idIndexMarker070"/>of being selected as a sample to participate in the study. This method is used when random probability sampling is impossible to conduct and it is faster and easier to obtain data compared to the probability sampling method. One of the reasons to use this method is due to cost and time considerations. It allows us to collect data easily by using a non-random selection based on convenience or certain criteria. This method can lead to a higher-biased risk than the probability sampling method. The method is often used in exploratory and qualitative research. For example, if a group of researchers wants to understand clients’ opinions of a company related to one of its products, they send a survey to the clients who bought and used the product. It is a convenient way to get opinions, but these opinions are only from clients who already used the product. Therefore, the sample data is only representative of one group of clients and cannot be generalized as the opinions of all the clients of <span class="No-Break">the company.</span></p>
<p class="IMG---Figure"><img alt="Figure 1.6 – A survey study example" height="422" src="image/B18945_01_006.png" width="638"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – A survey study example</p>
<p>The previous example is one of two types of non-probability sampling methods that we want to discuss here. This<a id="_idIndexMarker071"/> method is <strong class="bold">convenience sampling</strong>. In convenience sampling, researchers<a id="_idIndexMarker072"/> choose members the most accessible to the researchers from a population to form a sample. This method is easy and inexpensive but generalizing the results obtained to the whole population <span class="No-Break">is questionable.</span></p>
<p><strong class="bold">Quota sampling</strong> is <a id="_idIndexMarker073"/>another type of non-probability sampling <a id="_idIndexMarker074"/>where a sample group is selected to be <a id="_idIndexMarker075"/>representative of a larger population in a non-random way. For example, recruiters with limited time can use the quota sampling method to search for potential candidates from professional social networks (LinkedIn, Indeed.com, etc.) and interview them. This method is cost-effective and saves time but presents bias during the <span class="No-Break">selection process.</span></p>
<p>In this section, we provided an overview of probability and non-probability sampling. Each strategy has advantages and disadvantages, but they help us to minimize risks, such as bias. A well-planned sampling strategy will also help reduce errors in <span class="No-Break">predictive modeling.</span></p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor028"/>Summary</h1>
<p>In this chapter, we discussed installing and setting up the Python environment to run the <strong class="source-inline">Statsmodels</strong> API and other requisite open-source packages. We also discussed populations versus samples and the requirements to gain inference from samples. Finally, we explained several different common sampling methods used in statistical and machine <span class="No-Break">learning models.</span></p>
<p>In the next chapter, we will begin a discussion on statistical distributions and their implications for building statistical models. In <a href="B18945_03.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, Hypothesis Testing</em>, we will begin discussing hypothesis testing in depth, expanding on the concepts discussed in the <em class="italic">Observational study</em> section of this chapter. We will also discuss power analysis, which is a useful tool for determining the sample size based on existing sample data parameters and the desired levels of <span class="No-Break">statistical significance.</span></p>
</div>
</div></body></html>