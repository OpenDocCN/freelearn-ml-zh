<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image Captioning with TensorFlow</h1>
                </header>
            
            <article>
                
<p>Primarily, this chapter will provide a brief overview of creating a detailed English language description of an image. Using the image captioning model based on TensorFlow, we will be able to replace a single word or compound words/phrases with detailed captions that perfectly describe the image. We will first use a pre-trained model for image captioning and then retrain the model from scratch to run on a set of images.</p>
<p><span>In this chapter, we will cover the following:</span></p>
<ul>
<li><span>Image captioning introduction</span></li>
<li><span>Google Brain im2txt captioning model</span></li>
<li><span>Running our captioning code in Jupyter</span></li>
<li><span>Retraining the model</span></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Along with knowledge of Python, the basics of image processing, and computer vision, we will need the following Python libraries:</p>
<ul>
<li>NumPy</li>
<li>Matplotlib</li>
</ul>
<p>The codes used in the chapter have been added to the following GitHub repository:<br/>
<a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3</a></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to image captioning</h1>
                </header>
            
            <article>
                
<p>Image captioning is a process in which textual description is generated based on an image. To better understand <span>image captioning, w</span>e need to first differentiate it from image classification.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Difference between image classification and image captioning</h1>
                </header>
            
            <article>
                
<p>Image classification is a relatively simple process that only tells us what is in an image. For example, if there is a boy on a bike, image classification will not give us a description; it will just provide the result as <strong>boy </strong>or <strong>bike</strong>. Image classification can tell us whether there is a woman or a dog in the image, or an action, such as snowboarding. This is not a desirable result as there is no description of what exactly is going on in the image.</p>
<p>The following is the result we get using image classification:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/3f7f3a53-c971-43ed-9e6b-52e3f449daee.png" style="width:19.83em;height:27.92em;" width="373" height="525"/></div>
<p>Comparatively, image captioning will provide a result with a description. For the preceding example, the result of image captioning would be <strong>a boy riding on a bike</strong> or <strong>a man is snowboarding</strong>. This could be useful for generating content for a book or maybe helping the hearing or visually impaired.</p>
<p>The following is the result we get using image captioning:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/f449b9bd-d21a-4c25-9193-9507bade3c26.png" style="width:23.25em;height:29.25em;" width="459" height="577"/></div>
<p>However, this is considerably more challenging as conventional neural networks are powerful, but they're not very compatible with sequential data. Sequential data is where we have data that's coming in an order and that order actually matters. In audio or video, we have words coming in a sequential order; jumbling the words might change the meaning of the sentence or just make it complete gibberish.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks with long short-term memory</h1>
                </header>
            
            <article>
                
<p>As powerful as <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) are, they don't handle sequential data so well; however, they are great for non-sequential tasks, such as image classification.</p>
<p>How CNNs work is shown in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/a8176ff9-a902-4bdf-b868-6fae3abe7c5e.png" style="width:40.75em;height:34.58em;" width="413" height="350"/></div>
<p><strong>Recurrent neural networks</strong> (<strong>RNNs</strong>), which really are state of the art, can handle sequential tasks. An RNN consists of CNNs where data is received in a sequence.</p>
<p>How RNNs work is shown in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/be749fa7-8b9b-4378-a461-f83bed83e489.png" style="width:19.58em;height:15.50em;" width="386" height="306"/><br/></div>
<p>Data coming in a sequence (<strong>x<sub>i</sub></strong>) goes through the neural network and we get an output (<strong>y<sub>i</sub></strong>). The output is then fed through to another iteration and forms a loop. This helps us remember the data coming from before and is helpful for sequential data tasks such as audio and speech recognition, language translation, video identification, and text generation.</p>
<p>Another concept that has been around for a while and is very helpful is <strong>long short-term memory</strong> (<strong>LSTM</strong>) with RNNs. It is a way to handle long-term memory and avoid just passing data from one iteration to the next. It handles the data from the iterations in a robust way and it allows us to effectively train RNNs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Google Brain im2txt captioning model</h1>
                </header>
            
            <article>
                
<p>Google Brain im2txt was used by Google in a paper <span><em>2015 MSCOCO Image Captioning Challenge,</em> and </span>will form the foundation of the image captioning code that we will implement in our project.</p>
<p>The Google's GitHub TensorFlow page can be found at <span class="URLPACKT"><a href="https://github.com/tensorflow/models/tree/master/research/im2txt">https://github.com/tensorflow/models/tree/master/research/im2txt</a><a href="https://github.com/tensorflow/models/tree/master/research/im2txt">.</a><a href="https://github.com/tensorflow/models/tree/master/research/im2txt"/></span></p>
<p>In the research directory, we will find the <kbd>im2txt</kbd> file, which was used by Google in the paper, <em>2015 MSCOCO Image Captioning Challenge</em>, which is available for free at <a href="https://arxiv.org/abs/1609.06647" target="_blank">https://arxiv.org/abs/1609.06647</a>. It covers RNNs, LSTM, and fundamental algorithms in detail.</p>
<p>We can check how CNNs are used for image classification and also learn how to use the LSTM RNNs for actually generating sequential caption outputs.</p>
<p class="mce-root"/>
<p>We can download the code from the GitHub link; however, it has not been set up to run easily as it does not include a pre-trained model, so we may face some challenges. We have provided you with a pre-trained model to avoid training an image classifier from scratch, since it is a time-consuming process. There have been some modifications made to the code that will make the code easy to run on a Jupyter Notebook or to incorporate in your own projects. The pre-trained model is very quick to learn using just a CPU. The same code without a pre-trained model might actually take weeks to learn, even on a good GPU.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running the captioning code on Jupyter</h1>
                </header>
            
            <article>
                
<p>Let's now run our own version of the code on a Jupyter Notebook. We can start up own own Jupyter Notebook and load the <kbd>Section_1-Tensorflow_Image_Captioning.ipynb</kbd> file from the GitHub repository (<a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb" target="_blank">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb</a>).</p>
<p>Once we load the file on a Jupyter Notebook, it will look something like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/7cf6b337-2a16-48f8-9c27-327a28fb278f.png" style="width:40.33em;height:25.50em;" width="740" height="469"/></div>
<p>In the first part, we are going to load some essential libraries, including <kbd>math</kbd>, <kbd>os</kbd>, and <kbd>tensorflow</kbd>. We will also use our handy utility function, <kbd>%pylab inline</kbd>, to easily read and display images within the Notebook.</p>
<p>Select the first code block:</p>
<pre># load essential libraries<br/>import math<br/>import os<br/><br/>import tensorflow as tf<br/><br/>%pylab inline</pre>
<p>When we hit <em>Ctrl</em> + <em>Enter</em> to execute the code in the cell, we will get the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/5279fc38-3cc4-4768-9c74-05fc8cbbb101.png" style="width:32.17em;height:10.25em;" width="512" height="163"/></div>
<p>We need to now load the TensorFlow/Google Brain base code, which we can get from <a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3</a>.</p>
<p>There are multiple utility functions, but we will be using and executing only a few of them in our example:</p>
<pre># load Tensorflow/Google Brain base code<br/># https://github.com/tensorflow/models/tree/master/research/im2txt<br/><br/>from im2txt import configuration<br/>from im2txt import inference_wrapper<br/>from im2txt.inference_utils import caption_generator<br/>from im2txt.inference_utils import vocabulary</pre>
<p>We need to tell our function where to find the trained model and vocabulary:</p>
<pre># tell our function where to find the trained model and vocabulary<br/>checkpoint_path = './model'<br/>vocab_file = './model/word_counts.txt'</pre>
<p>The code for the trained model and vocabulary have been added in the GitHub repository, and you can access it from this link:</p>
<p><a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3</a></p>
<p>The folder contains <kbd>checkpoint</kbd>, <kbd>word_counts.txt</kbd>, and the pre-trained model. We need to make sure that we use these files and avoid using other outdated files that might not be compatible with the latest version of TensorFlow. The <kbd>word_counts.txt</kbd> file contains a vocabulary list with the number of counts from our trained model, which our image caption generator is going to need.</p>
<p>Once these steps have been completed, we can look at our <kbd>main</kbd> function, which will generate the captions for us. The function can take an input as a string of input files (comma separated) or could be just one file that we want to process.<br/>
The verbosity is set to <kbd>tf.logging.FATAL</kbd> out of the different logging levels available, as it will tell us whether something has gone really wrong:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/30533d4c-b33a-4aa2-b3fb-062758390569.png" style="width:36.67em;height:14.08em;" width="637" height="245"/></div>
<p>In the initial part of the main code, we perform the following steps:</p>
<ol>
<li>Set the verbosity level to <kbd>tf.logging.FATAL</kbd>.</li>
<li>Load our pre-trained model.</li>
<li>Load the inference wrapper from our utility file provided by Google.</li>
<li>Load our pre-trained model from the <kbd>checkpoint</kbd> path that we established in the previous cell.</li>
<li>Run the <kbd>finalize</kbd> function:</li>
</ol>
<pre style="padding-left: 60px"># this is the function we'll call to produce our captions <br/># given input file name(s) -- separate file names by a,<br/># if more than one<br/><br/>def gen_caption(input_files):<br/>    # only print serious log messages<br/>    tf.logging.set_verbosity(tf.logging.FATAL)<br/>    # load our pretrained model<br/>    g = tf.Graph()<br/>    with g.as_default():<br/>        model = inference_wrapper.InferenceWrapper()<br/>        restore_fn = model.build_graph_from_config(configuration.ModelConfig(),<br/>                                                 checkpoint_path)<br/>    g.finalize()</pre>
<ol start="6">
<li>Load the vocabulary file again from the cell that we previously ran:</li>
</ol>
<pre style="padding-left: 60px">    # Create the vocabulary.<br/>    vocab = vocabulary.Vocabulary(vocab_file)</pre>
<ol start="7">
<li>Pre-process the filenames:</li>
</ol>
<pre style="padding-left: 60px">    filenames = []<br/>    for file_pattern in input_files.split(","):</pre>
<ol start="8">
<li>Perform the <kbd>Glob</kbd> action:</li>
</ol>
<pre style="padding-left: 60px">        filenames.extend(tf.gfile.Glob(file_pattern))</pre>
<ol start="9">
<li>Create a list of filenames so you can know on which file the image caption generator is running:</li>
</ol>
<pre style="padding-left: 60px">    tf.logging.info("Running caption generation on %d files matching %s",<br/>                    len(filenames), input_files)</pre>
<ol start="10">
<li>Create a session. We need to use the <kbd>restore</kbd> function since we are using a pre-trained model:</li>
</ol>
<pre style="padding-left: 60px">    with tf.Session(graph=g) as sess:<br/>        # Load the model from checkpoint.<br/>        restore_fn(sess)</pre>
<p>The code for these steps is included here:</p>
<pre># this is the function we'll call to produce our captions <br/># given input file name(s) -- separate file names by a,<br/># if more than one<br/><br/>def gen_caption(input_files):<br/>    # only print serious log messages<br/>    tf.logging.set_verbosity(tf.logging.FATAL)<br/>    # load our pretrained model<br/>    g = tf.Graph()<br/>    with g.as_default():<br/>        model = inference_wrapper.InferenceWrapper()<br/>        restore_fn = model.build_graph_from_config(configuration.ModelConfig(),<br/>                                                 checkpoint_path)<br/>    g.finalize()<br/><br/>    # Create the vocabulary.<br/>    vocab = vocabulary.Vocabulary(vocab_file)<br/><br/>    filenames = []<br/>    for file_pattern in input_files.split(","):<br/>        filenames.extend(tf.gfile.Glob(file_pattern))<br/>    tf.logging.info("Running caption generation on %d files matching %s",<br/>                    len(filenames), input_files)<br/><br/>    with tf.Session(graph=g) as sess:<br/>        # Load the model from checkpoint.<br/>        restore_fn(sess)</pre>
<p>We now move to the second half of the main code. Once the session has been restored, we perform the following steps:</p>
<ol>
<li>Load <kbd>caption_generator</kbd> from our model and the vocabulary stored in an object called <kbd>generator</kbd>:</li>
</ol>
<pre>        generator = caption_generator.CaptionGenerator(model, vocab)</pre>
<ol start="2">
<li>Make a caption list:</li>
</ol>
<pre>        captionlist = []</pre>
<ol start="3">
<li>Iterate the files and load them in the generator called <kbd>beam_search</kbd> to analyze the image:</li>
</ol>
<pre>        for filename in filenames:<br/>            with tf.gfile.GFile(filename, "rb") as f:<br/>                image = f.read()<br/>            captions = generator.beam_search(sess, image)</pre>
<ol start="4">
<li>Print the captions:</li>
</ol>
<pre>            print("Captions for image %s:" % os.path.basename(filename))</pre>
<ol start="5">
<li>Iterate to create multiple captions with the iteration already set for the model:</li>
</ol>
<pre>            for i, caption in enumerate(captions):<br/>                # Ignore begin and end words.<br/>                sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]<br/>                sentence = " ".join(sentence)<br/>                print(" %d) %s (p=%f)" % (i, sentence, math.exp(caption.logprob)))<br/>                captionlist.append(sentence)</pre>
<ol start="6">
<li>Return <kbd>captionlist</kbd>:</li>
</ol>
<pre>    return captionlist</pre>
<p style="padding-left: 60px">Run the code to generate the function.</p>
<p>See the following code block for the complete code:</p>
<pre>    # Prepare the caption generator. Here we are implicitly using the default<br/>    # beam search parameters. See caption_generator.py for a description of the<br/>    # available beam search parameters.<br/>        generator = caption_generator.CaptionGenerator(model, vocab)<br/>        <br/>        captionlist = []<br/><br/>        for filename in filenames:<br/>            with tf.gfile.GFile(filename, "rb") as f:<br/>                image = f.read()<br/>            captions = generator.beam_search(sess, image)<br/>            print("Captions for image %s:" % os.path.basename(filename))<br/>            for i, caption in enumerate(captions):<br/>                # Ignore begin and end words.<br/>                sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]<br/>                sentence = " ".join(sentence)<br/>                print(" %d) %s (p=%f)" % (i, sentence, math.exp(caption.logprob)))<br/>                captionlist.append(sentence)<br/>    return captionlist</pre>
<p>In the next code block, we will execute the code on sample stock photos from a <kbd>test</kbd> folder. The code will create a figure, show it, and then run the caption generator. We can then display the output using the <kbd>print</kbd> statement.</p>
<p class="mce-root"/>
<p>The following is the code we use to select the image for computation:</p>
<pre>testfile = 'test_images/dog.jpeg'<br/><br/>figure()<br/>imshow(imread(testfile))<br/><br/>capts = gen_caption(testfile)</pre>
<p>When we run our first test image, <kbd>dog.jpeg</kbd>, we get the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/b760c014-a429-4f3d-8387-50867828e243.png" style="width:37.67em;height:33.08em;" width="508" height="446"/></div>
<p>The result, <kbd>a woman and a dog are standing on the grass</kbd>, is a good caption for the image. Since all the three results are pretty similar, we can say that our model is working pretty well.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Analyzing the result captions</h1>
                </header>
            
            <article>
                
<p>Let's take a few examples to check our model. When we execute <kbd>football.jpeg</kbd>, we get the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/d9aaa6e6-3abc-4202-936d-33952a1612a8.png" style="width:36.83em;height:32.92em;" width="499" height="448"/></div>
<p>Here we clearly have American football going on in the image, and <kbd>a couple of men playing a game of football</kbd> is a very good result. However, the first result, <kbd>a couple of men playing a game of frisbee</kbd>, is not the desired output, nor is <kbd>a couple of men playing a game of soccer</kbd>. So, in this case, the second caption is generally going to be the best, but it is not always going to be perfect, depending on the log probability.</p>
<p>Let's try one more example, <kbd>giraffes.jpeg</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/edfbad17-b3e0-4342-9c32-190f28865bec.png" style="width:42.17em;height:30.25em;" width="621" height="446"/></div>
<p>Clearly, we have an image of giraffes, and the first caption, <kbd>a group of giraffe standing next to each other</kbd>, seems to be correct, except for the grammar issue. The other two results are <kbd>a group of giraffes are standing in a field</kbd> and <kbd>a group of giraffe standing next to each other on a field</kbd>.</p>
<p class="mce-root"/>
<p>Let's take a look at one more example, <kbd>headphones.jpeg</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/589ac694-b233-4dac-b54c-c5d32e45a5a5.png" style="width:36.67em;height:32.67em;" width="500" height="446"/></div>
<p>Here we selected <kbd>headphones.jpeg</kbd>, but the results did not include headphones as an output. The result was <kbd>a woman holding a cell phone in her hand</kbd>, which is a good result. The second result, <kbd>a woman holding a cell phone up to her ear</kbd>, is technically incorrect, but these are some good captions overall.</p>
<p>Let's take one last example, <kbd>ballons.jpeg</kbd>. When we run the image, we get the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/c60032d5-a1ea-45a8-9d20-fd35908a06b6.png" style="width:35.58em;height:32.33em;" width="490" height="446"/></div>
<p>The results we get for this image are <kbd>a woman standing on a beach flying a kite</kbd>, <kbd>a woman is flying a kite on the beach</kbd>, and <kbd>a young girl flying a kite on a beach</kbd>. So, the model got the <kbd>woman</kbd> or a <kbd>young girl</kbd>, but it got a <kbd>kite</kbd> instead of the balloon, even though "balloon" is in the vocabulary. So, we can infer that the model is not perfect, but it is impressive and can be included in your own application.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running the captioning code on Jupyter for multiple images</h1>
                </header>
            
            <article>
                
<p>Multiple images can also be added as an input string by separating the image path of the different images using commas. The execution time of a string of images will be greater than the times we've seen thus far.</p>
<p>The following is an example of multiple input files:</p>
<pre>input_files = 'test_images/ballons.jpeg,test_images/bike.jpeg,test_images/dog.jpeg,test_images/fireworks.jpeg,test_images/football.jpeg,test_images/giraffes.jpeg,test_images/headphones.jpeg,test_images/laughing.jpeg,test_images/objects.jpeg,test_images/snowboard.jpeg,test_images/surfing.jpeg'<br/><br/>capts = gen_caption(input_files)</pre>
<p>We will not be displaying the images, so the output will include only the results. We can see that some of the results are better than others:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/c759ff9d-66fb-4db3-b062-de2575419d27.png" width="994" height="776"/></div>
<p>This wraps up running the pre-trained image captioning model. We will now cover training our model from scratch and running it on captioned images.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Retraining the captioning model</h1>
                </header>
            
            <article>
                
<p>So, now that we have seen image captioning code in action, we are going to retrain the image captioner on our own desired data. However, we need to know that it will be very time consuming and will need over 100 GB of hard drive space for computations if we want it to process in a reasonable time. Even with a good GPU, it may take a few days or a week to complete the computation. Since we are inclined toward implementing it and have the resources, let's start retraining the model.</p>
<p>In the Notebook, the first step is to download the pre-trained Inception model. The <kbd>webbrowser</kbd> module will make it easy to open the URL and to download the file:</p>
<pre># First download pretrained Inception (v3) model<br/><br/>import webbrowser <br/>webbrowser.open("http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz")<br/><br/># Completely unzip tar.gz file to get inception_v3.ckpt,<br/># --recommend storing in im2txt/data directory</pre>
<p>The following will be the output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/726b3308-e90e-4581-ac71-9b977bd35bef.png" style="width:45.42em;height:9.33em;" width="799" height="165"/></div>
<p>When we select the code block and execute it, we might not be able to view the content on the web page, but we can click <span class="packt_screen">save</span> on the dialog box to download the file. Unzip the file to get the inception v3 checkpoint file. We can use any of the unzipping utility available, but it is preferable to use 7-zip to get the Inception v3 checkpoint file and store it in <kbd>im2txt/data</kbd> in the project directory.</p>
<p>The <kbd>cd</kbd> command is used to navigate to the <kbd>im2txt/data</kbd> directory, where all our files are present. The <kbd>run_build_mscoco_data.py</kbd> Python script will grab and process all the image data and the pre-made caption data. This process might take over 100 GB of space and take over an hour to complete its execution.</p>
<p>Once the computation is complete, we will see the three ZIP files in our project's directory. We can unzip these files to get the following directories:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/ec271f43-3ccd-485b-83a0-1d7bf5354776.png" style="width:38.33em;height:10.58em;" width="857" height="237"/></div>
<p>The training and validation JSON files are present in the <kbd>annotations</kbd> folder. The other directories have image training and validation data. Under the <kbd>train2014</kbd> directory, we will find a bunch of JPEG images corresponding to the training data. Similarly, the resources corresponding to the validation data will be present in the <kbd>val2014</kbd> folder. We can substitute our own images as well and edit the corresponding JSON file in the <kbd>annotations</kbd> folder. We will need many examples, as a handful examples will not provide effective results. There are over 80,000 images in the <kbd>train2014</kbd> directory and processing them will require intensive resources.</p>
<p>Once we execute the <kbd>run_build_mscoco_data.py</kbd> command, we need to load the required modules:</p>
<pre># Now gather and prepare the MSCOCO data<br/><br/># Comment out cd magic command if already in data directory<br/>%cd im2txt/data<br/># This command will take an hour or more to run typically.<br/># Note, you will need a lot of HD space (&gt;100 GB)!<br/>%run build_mscoco_data.py<br/><br/># At this point you have files in im2txt/data/mscoco/raw-data that you can train<br/># on, or you can substitute your own data<br/><br/>%cd ..<br/><br/># load needed modules<br/><br/>import tensorflow as tf<br/><br/>from im2txt import configuration<br/>from im2txt import show_and_tell_model</pre>
<p>We need to load <kbd>configuration</kbd> and <kbd>show_and_tell_model</kbd> in the <kbd>im2txt</kbd> folder along with TensorFlow. We can run the <kbd>cd ..</kbd> command to be in the right directory.</p>
<p>Now, we will be defining the following variables:</p>
<ul>
<li><kbd>input_file_pattern</kbd>: Defines the files pointing to the pre-trained Inception checkpoint, which will be generated from our model</li>
<li><kbd>train_dir</kbd>: Contains the path where the training data was stored after we downloaded and unzipped it</li>
<li><kbd>train_inception</kbd>: Set to<span> </span><kbd>false</kbd><span> </span>since we will not be training our Inception model for the initial run</li>
<li><kbd>number_of_steps</kbd>: One million steps for our function</li>
<li><kbd>log_every_n_steps</kbd>: Set <kbd>1</kbd> for our function</li>
</ul>
<p>Here is the code:</p>
<pre># Initial training<br/>input_file_pattern = 'im2txt/data/mscoco/train-?????-of-00256'<br/><br/># change these if you put your stuff somewhere else<br/>inception_checkpoint_file = 'im2txt/data/inception_v3.ckpt'<br/>train_dir = 'im2txt/model'<br/><br/># Don't train inception for initial run<br/>train_inception = False<br/>number_of_steps = 1000000<br/>log_every_n_steps = 1</pre>
<p>Now let's define our <kbd>train</kbd> function. The steps performed in the <kbd>train</kbd> function are as follows:</p>
<ol>
<li>Create the<span> </span><kbd>train</kbd><span> </span>directory</li>
<li>Create the graph file</li>
<li>Load the essential files</li>
<li>Add the required variables for TensorFlow to start training the model to get the learning rate with the number of batches per epoch delay step</li>
<li>Set up the layers</li>
<li>Set up the saver for saving and restoring the model checkpoint</li>
<li>Call TensorFlow and do the training</li>
</ol>
<p class="mce-root"/>
<p>The following is our <kbd>train</kbd> function:</p>
<ol>
<li>Define (but don't run yet) our captioning training function:</li>
</ol>
<pre style="padding-left: 60px">def train():<br/>    model_config = configuration.ModelConfig()<br/>    model_config.input_file_pattern = input_file_pattern<br/>    model_config.inception_checkpoint_file = inception_checkpoint_file<br/>    training_config = configuration.TrainingConfig()  </pre>
<ol start="2">
<li>Create the training directory:</li>
</ol>
<pre style="padding-left: 60px">    train_dir = train_dir<br/>    if not tf.gfile.IsDirectory(train_dir):<br/>        tf.logging.info("Creating training directory: %s", train_dir)<br/>        tf.gfile.MakeDirs(train_dir)<br/><br/></pre>
<ol start="3">
<li>Build the TensorFlow graph:</li>
</ol>
<pre style="padding-left: 60px">    g = tf.Graph()<br/>    with g.as_default():<br/>        </pre>
<ol start="4">
<li>Build the model:</li>
</ol>
<pre style="padding-left: 60px">        model = show_and_tell_model.ShowAndTellModel(<br/>                model_config, mode="train", train_inception=train_inception)<br/>        model.build()<br/><br/>        </pre>
<ol start="5">
<li>Set up the learning rate:</li>
</ol>
<pre style="padding-left: 60px">        learning_rate_decay_fn = None<br/>        if train_inception:<br/>            learning_rate = tf.constant(training_config.train_inception_learning_rate)<br/>        else:<br/>            learning_rate = tf.constant(training_config.initial_learning_rate)<br/>            if training_config.learning_rate_decay_factor &gt; 0:<br/>                num_batches_per_epoch = (training_config.num_examples_per_epoch /<br/>                                 model_config.batch_size)<br/>                decay_steps = int(num_batches_per_epoch *<br/>                          training_config.num_epochs_per_decay)<br/><br/>                def _learning_rate_decay_fn(learning_rate, global_step):<br/>                    return tf.train.exponential_decay(<br/>                                      learning_rate,<br/>                                      global_step,<br/>                                      decay_steps=decay_steps,<br/>                                      decay_rate=training_config.learning_rate_decay_factor,<br/>                                      staircase=True)<br/><br/>                learning_rate_decay_fn = _learning_rate_decay_fn<br/><br/>       </pre>
<ol start="6">
<li>Set up the training ops:</li>
</ol>
<pre style="padding-left: 60px">        train_op = tf.contrib.layers.optimize_loss(<br/>                                        loss=model.total_loss,<br/>                                        global_step=model.global_step,<br/>                                        learning_rate=learning_rate,<br/>                                        optimizer=training_config.optimizer,<br/>                                        clip_gradients=training_config.clip_gradients,<br/>                                        learning_rate_decay_fn=learning_rate_decay_fn)<br/><br/></pre>
<ol start="7">
<li>Set up the <kbd>Saver</kbd> for saving and restoring model checkpoints:</li>
</ol>
<pre style="padding-left: 60px">        saver = tf.train.Saver(max_to_keep=training_config.max_checkpoints_to_keep)<br/><br/>    # Run training.<br/>    tf.contrib.slim.learning.train(<br/>                                train_op,<br/>                                train_dir,<br/>                                log_every_n_steps=log_every_n_steps,<br/>                                graph=g,<br/>                                global_step=model.global_step,<br/>                                number_of_steps=number_of_steps,<br/>                                init_fn=model.init_fn,<br/>                                saver=saver)</pre>
<p>Hit <em>Ctrl</em> + <em>Enter</em> for this code cell, since we can execute this now. After that, we need to call the <kbd>train</kbd> function:</p>
<pre>train()</pre>
<p>This will take a long time to process, even on a good GPU, but if we have the resources and still want to refine the model, run the following code to fine-tuning our <kbd>inception</kbd> model:</p>
<pre># Fine tuning<br/>input_file_pattern = 'im2txt/data/mscoco/train-?????-of-00256'<br/><br/># change these if you put your stuff somewhere else<br/>inception_checkpoint_file = 'im2txt/data/inception_v3.ckpt'<br/>train_dir = 'im2txt/model'<br/><br/># This will refine our results<br/>train_inception = True<br/>number_of_steps = 3000000<br/>log_every_n_steps = 1<br/><br/># Now run the training (warning: takes even longer than initial training!!!)<br/>train()</pre>
<p>The model will run for three million steps. It actually continues from where the initial training completed its process and generate new checkpoints and refined models, before running the <kbd>train</kbd> function again. This will take even more time to process and provide a good result. We can do this in our Jupyter Notebook by correctly pointing our <kbd>checkpoint</kbd> path and the path for the vocabulary file:</p>
<pre># tell our function where to find the trained model and vocabulary<br/>checkpoint_path = './model'<br/>vocab_file = './model/word_counts.txt'</pre>
<p>After that, we can rerun code block 4 from the Jupyter Notebook file at <a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb" target="_blank">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb</a> to find <kbd>gen_caption</kbd>.</p>
<p>The last step is to run the following code, as we did before in the <em>Running the captioning code on Jupyter</em> section:</p>
<pre>testfile = 'test_images/ballons.jpeg'<br/><br/>figure()<br/>imshow(imread(testfile))<br/><br/>capts = gen_caption(testfile)</pre>
<p>Once the computation has been completed, we should get some good results. This wraps up image captioning with TensorFlow.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we were introduced to different image captioning methods. We learned about the Google Brain im2txt captioning model. While working on the project, we were able to run our pre-trained model on a Jupyter Notebook and analyze the model based on the results. In the last section of the chapter, we retrained our image captioning model from scratch. </p>
<p>In the next chapter, we will cover reading license plates with OpenCV.</p>


            </article>

            
        </section>
    </div>



  </body></html>