<html><head></head><body>
		<div id="_idContainer102">
			<h1 id="_idParaDest-81"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.1.1">Chapter 5: Understanding Machine Learning</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">Over the last few years, you have likely heard some of the many popular buzz words such as </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">AI</span></strong><span class="koboSpan" id="kobo.6.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">Machine Learning</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.9.1">ML</span></strong><span class="koboSpan" id="kobo.10.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">Deep Learning</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.13.1">DL</span></strong><span class="koboSpan" id="kobo.14.1">) that have rippled through most major industries. </span><span class="koboSpan" id="kobo.14.2">Although many of these phrases tend to be used interchangeably in company-wide all-staff and leadership meetings, each of these phrases does in fact refer to a distinct concept. </span><span class="koboSpan" id="kobo.14.3">So, let's take a look closer look at what these phrases actually refer to.</span></p>
			<p><span class="koboSpan" id="kobo.15.1">AI generally refers to the overarching domain of human-like intelligence demonstrated by software and machines. </span><span class="koboSpan" id="kobo.15.2">We can think of AI as the space that encompasses many of the topics we will discuss within the scope of this book. </span></p>
			<p><span class="koboSpan" id="kobo.16.1">Within the AI domain, there exists a sub-domain that we refer to as </span><strong class="bold"><span class="koboSpan" id="kobo.17.1">machine learning</span></strong><span class="koboSpan" id="kobo.18.1">. </span><span class="koboSpan" id="kobo.18.2">ML can be defined as </span><em class="italic"><span class="koboSpan" id="kobo.19.1">the study of algorithms in conjunction with data to develop predictive models</span></em><span class="koboSpan" id="kobo.20.1">.</span></p>
			<p><span class="koboSpan" id="kobo.21.1">Within the ML domain, there exists yet another sub-domain we refer to as </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">deep learning</span></strong><span class="koboSpan" id="kobo.23.1">. </span><span class="koboSpan" id="kobo.23.2">We will define </span><strong class="bold"><span class="koboSpan" id="kobo.24.1">DL</span></strong><span class="koboSpan" id="kobo.25.1"> as </span><em class="italic"><span class="koboSpan" id="kobo.26.1">the application of ML specifically through the use of artificial neural networks</span></em><span class="koboSpan" id="kobo.27.1">. </span></p>
			<p><span class="koboSpan" id="kobo.28.1">Now that we have gained a better sense of the differences between these terms, let's define the concept of ML in a little more detail. </span><span class="koboSpan" id="kobo.28.2">There are several different definitions of ML that you will encounter depending on who you ask. </span><span class="koboSpan" id="kobo.28.3">Physicists tend to link the definition to applications in </span><em class="italic"><span class="koboSpan" id="kobo.29.1">performance optimization</span></em><span class="koboSpan" id="kobo.30.1">, whereas mathematicians have a tendency to link the definition to </span><em class="italic"><span class="koboSpan" id="kobo.31.1">statistical probabilities</span></em><span class="koboSpan" id="kobo.32.1"> and, finally, computer scientists tend to link the definition to </span><em class="italic"><span class="koboSpan" id="kobo.33.1">algorithms</span></em><span class="koboSpan" id="kobo.34.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.35.1">code</span></em><span class="koboSpan" id="kobo.36.1">. </span><span class="koboSpan" id="kobo.36.2">To a certain extent, all three are technically correct. </span><span class="koboSpan" id="kobo.36.3">For the purposes of this book, we will define ML as a field of research concerning the development of mathematically optimized models using computer code, which </span><em class="italic"><span class="koboSpan" id="kobo.37.1">learn</span></em><span class="koboSpan" id="kobo.38.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.39.1">generalize</span></em><span class="koboSpan" id="kobo.40.1"> from historical data to unlock useful insights and make predictions. </span></p>
			<p><span class="koboSpan" id="kobo.41.1">Although this definition may seem straightforward, most experienced interview candidates still tend to struggle when defining this concept. </span><span class="koboSpan" id="kobo.41.2">Make note of the exact phrasing we used here, as it may prove to be useful in a future setting. </span></p>
			<p><span class="koboSpan" id="kobo.42.1">Over the course of this chapter, we will visit various aspects of ML, and we will review some of the most common steps a developer must take when it comes to developing a </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">predictive model</span></strong><span class="koboSpan" id="kobo.44.1">. </span></p>
			<p><span class="koboSpan" id="kobo.45.1">In this chapter, we will review the following main topics:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.46.1">Understanding ML</span></li>
				<li><span class="koboSpan" id="kobo.47.1">Overfitting and underfitting</span></li>
				<li><span class="koboSpan" id="kobo.48.1">Developing an ML model</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.49.1">With all this in mind, let's get started!</span></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.50.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.51.1">In this chapter, we will apply our understanding of </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">Python</span></strong><span class="koboSpan" id="kobo.53.1"> to demonstrate some of the concepts behind ML. </span><span class="koboSpan" id="kobo.53.2">We will take a close look at some of the main steps in the ML model development process in which we will utilize some familiar libraries, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.54.1">pandas</span></strong><span class="koboSpan" id="kobo.55.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.56.1">numpy</span></strong><span class="koboSpan" id="kobo.57.1">. </span><span class="koboSpan" id="kobo.57.2">In addition, we will also use some ML libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.58.1">sklearn</span></strong><span class="koboSpan" id="kobo.59.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.60.1">tensorflow</span></strong><span class="koboSpan" id="kobo.61.1">. </span><span class="koboSpan" id="kobo.61.2">Recall that the process of installing a new library can be done via the command line:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.62.1">$ pip install library-name</span></p>
			<p><span class="koboSpan" id="kobo.63.1">Let's begin!</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.64.1">Understanding ML</span></h1>
			<p><span class="koboSpan" id="kobo.65.1">In the</span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.66.1"> introduction, we broadly defined the concept of ML as it pertains to this book. </span><span class="koboSpan" id="kobo.66.2">With that definition in mind, let's now take a look at some examples to elaborate on our definition. </span><span class="koboSpan" id="kobo.66.3">In its broadest sense, ML can be divided into four areas: </span><strong class="bold"><span class="koboSpan" id="kobo.67.1">classification</span></strong><span class="koboSpan" id="kobo.68.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">regression</span></strong><span class="koboSpan" id="kobo.70.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">clustering</span></strong><span class="koboSpan" id="kobo.72.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.73.1">dimensionality reduction</span></strong><span class="koboSpan" id="kobo.74.1">. </span><span class="koboSpan" id="kobo.74.2">These four categories are often referred to as the field of </span><strong class="bold"><span class="koboSpan" id="kobo.75.1">data science</span></strong><span class="koboSpan" id="kobo.76.1">. </span><span class="koboSpan" id="kobo.76.2">Data science is a very broad term used to refer to various applications relating to data, as well as the field of AI and its subsets. </span><span class="koboSpan" id="kobo.76.3">We can visualize the relationships between these fields in </span><em class="italic"><span class="koboSpan" id="kobo.77.1">Figure 5.1</span></em><span class="koboSpan" id="kobo.78.1">:</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<span class="koboSpan" id="kobo.79.1"><img src="image/B17761_05_001.jpg" alt="Figure 5.1 – The domain of AI as it relates to other fields "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.80.1">Figure 5.1 – The domain of AI as it relates to other fields</span></p>
			<p><span class="koboSpan" id="kobo.81.1">With these </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.82.1">concepts in mind, let's discuss these four ML methods in more detail.</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.83.1">Classification</span></strong><span class="koboSpan" id="kobo.84.1"> is a</span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.85.1"> method of </span><em class="italic"><span class="koboSpan" id="kobo.86.1">pattern detection</span></em><span class="koboSpan" id="kobo.87.1"> in which our objective is to predict a </span><em class="italic"><span class="koboSpan" id="kobo.88.1">label</span></em><span class="koboSpan" id="kobo.89.1"> (or </span><em class="italic"><span class="koboSpan" id="kobo.90.1">category</span></em><span class="koboSpan" id="kobo.91.1">) from a finite set of possible options. </span><span class="koboSpan" id="kobo.91.2">For example, we can train a model to predict a protein's structure (for example, alpha helix or beta sheet), which can be referred</span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.92.1"> to as a </span><strong class="bold"><span class="koboSpan" id="kobo.93.1">binary classifier</span></strong><span class="koboSpan" id="kobo.94.1"> as there are only </span><em class="italic"><span class="koboSpan" id="kobo.95.1">two</span></em><span class="koboSpan" id="kobo.96.1"> possible outcomes or categories. </span><span class="koboSpan" id="kobo.96.2">We can break down classification models even further; however, we will visit this in greater detail in </span><a href="B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101"><em class="italic"><span class="koboSpan" id="kobo.97.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.98.1">,</span><em class="italic"><span class="koboSpan" id="kobo.99.1"> Supervised Machine Learning</span></em><span class="koboSpan" id="kobo.100.1">. </span><span class="koboSpan" id="kobo.100.2">For now, note that </span><em class="italic"><span class="koboSpan" id="kobo.101.1">classification</span></em><span class="koboSpan" id="kobo.102.1"> is a method to predict a label (or category). </span><span class="koboSpan" id="kobo.102.2">We begin with a dataset in which our input values (referred to as </span><strong class="source-inline"><span class="koboSpan" id="kobo.103.1">X</span></strong><span class="koboSpan" id="kobo.104.1">) and their subsequent output values (generally referred to as </span><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">ŷ</span></strong><span class="koboSpan" id="kobo.106.1">) are used to train a classifier. </span><span class="koboSpan" id="kobo.106.2">This classifier can then be used to make predictions on new and unseen data. </span><span class="koboSpan" id="kobo.106.3">We can represent this visually in </span><em class="italic"><span class="koboSpan" id="kobo.107.1">Figure 5.2</span></em><span class="koboSpan" id="kobo.108.1">:</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<span class="koboSpan" id="kobo.109.1"><img src="image/B17761_05_002.jpg" alt="Figure 5.2 – An example of a classification model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.110.1">Figure 5.2 – An example of a classification model</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.111.1">Clustering</span></strong><span class="koboSpan" id="kobo.112.1"> is similar to</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.113.1"> classification in the sense that the outcome of the model is a label (or category), but the difference here is that a clustering model is not trained on a list of predefined classes but is based on the similarities between objects. </span><span class="koboSpan" id="kobo.113.2">The clustering model then groups the data points together in </span><em class="italic"><span class="koboSpan" id="kobo.114.1">clusters</span></em><span class="koboSpan" id="kobo.115.1">. </span><span class="koboSpan" id="kobo.115.2">The total </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.116.1">number of clusters formed is not always known ahead of time, and this depends heavily on the parameters the model is trained on. </span><span class="koboSpan" id="kobo.116.2">In the following example, three clusters were formed using the original dataset:</span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<span class="koboSpan" id="kobo.117.1"><img src="image/B17761_05_003.jpg" alt="Figure 5.3 – An example of a clustering model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.118.1">Figure 5.3 – An example of a clustering model</span></p>
			<p><span class="koboSpan" id="kobo.119.1">On the other hand, when it comes to </span><strong class="bold"><span class="koboSpan" id="kobo.120.1">regression</span></strong><span class="koboSpan" id="kobo.121.1">, we are trying to predict a specific value, such </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.122.1">as an </span><strong class="bold"><span class="koboSpan" id="kobo.123.1">isoelectric point</span></strong><span class="koboSpan" id="kobo.124.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.125.1">pI</span></strong><span class="koboSpan" id="kobo.126.1">) in which the possible values are </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">continuous</span></strong><span class="koboSpan" id="kobo.128.1"> (pI = 5.59, 6.23, 7.12, and so on). </span><span class="koboSpan" id="kobo.128.2">Unlike classification or clustering, there are no labels or categories involved here, only a numerical value. </span><span class="koboSpan" id="kobo.128.3">We begin with a dataset in which our input values (</span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">X</span></strong><span class="koboSpan" id="kobo.130.1">) and their subsequent output values (</span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">ŷ</span></strong><span class="koboSpan" id="kobo.132.1">) are used to train a </span><em class="italic"><span class="koboSpan" id="kobo.133.1">regressor</span></em><span class="koboSpan" id="kobo.134.1">. </span><span class="koboSpan" id="kobo.134.2">This regressor can then be used to make predictions on new and unseen data:</span></p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<span class="koboSpan" id="kobo.135.1"><img src="image/B17761_05_004.jpg" alt="Figure 5.4 – An example of a regression model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.136.1">Figure 5.4 – An example of a regression model</span></p>
			<p><span class="koboSpan" id="kobo.137.1">Finally, when it comes</span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.138.1"> to </span><strong class="bold"><span class="koboSpan" id="kobo.139.1">dimensionality reduction</span></strong><span class="koboSpan" id="kobo.140.1">, ML can be applied not for the purposes of predicting a value, but in the sense of transforming data from a </span><em class="italic"><span class="koboSpan" id="kobo.141.1">high-dimensional</span></em><span class="koboSpan" id="kobo.142.1"> representation to a </span><em class="italic"><span class="koboSpan" id="kobo.143.1">low-dimensional</span></em><span class="koboSpan" id="kobo.144.1"> representation. </span><span class="koboSpan" id="kobo.144.2">Take, for example, the vast toxicity dataset we worked with in the previous chapters. </span><span class="koboSpan" id="kobo.144.3">We could apply a method such as </span><strong class="bold"><span class="koboSpan" id="kobo.145.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.146.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.147.1">PCA</span></strong><span class="koboSpan" id="kobo.148.1">) to</span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.149.1"> reduce the 10+ columns of features down to only two or three columns by </span><em class="italic"><span class="koboSpan" id="kobo.150.1">combining the importance</span></em><span class="koboSpan" id="kobo.151.1"> of </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.152.1">these features together. </span><span class="koboSpan" id="kobo.152.2">We will examine this in greater detail in </span><a href="B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101"><em class="italic"><span class="koboSpan" id="kobo.153.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.154.1">, </span><em class="italic"><span class="koboSpan" id="kobo.155.1">Understanding Supervised Machine Learning</span></em><span class="koboSpan" id="kobo.156.1">. </span><span class="koboSpan" id="kobo.156.2">We can see a visual representation of this in </span><em class="italic"><span class="koboSpan" id="kobo.157.1">Figure 5.5</span></em><span class="koboSpan" id="kobo.158.1">:</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<span class="koboSpan" id="kobo.159.1"><img src="image/B17761_05_005.jpg" alt="Figure 5.5 – An example of a dimensionality reduction model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.160.1">Figure 5.5 – An example of a dimensionality reduction model</span></p>
			<p><span class="koboSpan" id="kobo.161.1">The ML field is vast, complex, and extends well beyond the four basic examples we just touched on. </span><span class="koboSpan" id="kobo.161.2">However, the most common applications of ML models tend to focus on </span><em class="italic"><span class="koboSpan" id="kobo.162.1">predicting a category</span></em><span class="koboSpan" id="kobo.163.1">, </span><em class="italic"><span class="koboSpan" id="kobo.164.1">predicting a value</span></em><span class="koboSpan" id="kobo.165.1">, or </span><em class="italic"><span class="koboSpan" id="kobo.166.1">uncovering hidden insights</span></em><span class="koboSpan" id="kobo.167.1"> within data. </span></p>
			<p><span class="koboSpan" id="kobo.168.1">As scientists, we always want to organize our thoughts as best we can, and as it happens, the </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.169.1">concepts we just discussed can be placed into two main categories: </span><strong class="bold"><span class="koboSpan" id="kobo.170.1">Supervised Machine Learning</span></strong><span class="koboSpan" id="kobo.171.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.172.1">SML</span></strong><span class="koboSpan" id="kobo.173.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">Unsupervised Machine Learning</span></strong><span class="koboSpan" id="kobo.175.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.176.1">UML</span></strong><span class="koboSpan" id="kobo.177.1">). </span><span class="koboSpan" id="kobo.177.2">SML</span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.178.1"> encompasses all applications and models in which the datasets used to train the models contain both </span><em class="italic"><span class="koboSpan" id="kobo.179.1">features</span></em><span class="koboSpan" id="kobo.180.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.181.1">ground truth</span></em><span class="koboSpan" id="kobo.182.1"> output values. </span><span class="koboSpan" id="kobo.182.2">In other words, we know both the input (</span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">X</span></strong><span class="koboSpan" id="kobo.184.1">) and the output (</span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">ŷ</span></strong><span class="koboSpan" id="kobo.186.1">). </span><span class="koboSpan" id="kobo.186.2">We call this a </span><em class="italic"><span class="koboSpan" id="kobo.187.1">supervised</span></em><span class="koboSpan" id="kobo.188.1"> method because the model was taught (</span><em class="italic"><span class="koboSpan" id="kobo.189.1">supervised</span></em><span class="koboSpan" id="kobo.190.1">) which output label corresponds to which input value. </span><span class="koboSpan" id="kobo.190.2">On the other hand, UML encompasses ML models in which only the input (</span><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">X</span></strong><span class="koboSpan" id="kobo.192.1">) is known. </span><span class="koboSpan" id="kobo.192.2">Looking back to the four methods we discussed, we can divide them across both learning methods in the sense that </span><strong class="bold"><span class="koboSpan" id="kobo.193.1">classification</span></strong><span class="koboSpan" id="kobo.194.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.195.1">regression</span></strong><span class="koboSpan" id="kobo.196.1"> fall under SML, whereas </span><strong class="bold"><span class="koboSpan" id="kobo.197.1">clustering</span></strong><span class="koboSpan" id="kobo.198.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.199.1">dimensionality reduction</span></strong><span class="koboSpan" id="kobo.200.1"> fall under UML:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<span class="koboSpan" id="kobo.201.1"><img src="image/B17761_05_006.jpg" alt="Figure 5.6 – A representation of supervised and unsupervised machine learning "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.202.1">Figure 5.6 – A representation of supervised and unsupervised machine learning</span></p>
			<p><span class="koboSpan" id="kobo.203.1">Over the</span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.204.1"> course of the following chapters, we will explore many popular ML models and algorithms that fall under these four general categories. </span><span class="koboSpan" id="kobo.204.2">As you follow along, I encourage you to develop a mind map of your own, and further branch out each of the four categories to all the different models you will learn. </span><span class="koboSpan" id="kobo.204.3">For example, we will explore a </span><em class="italic"><span class="koboSpan" id="kobo.205.1">Naïve Bayes</span></em><span class="koboSpan" id="kobo.206.1"> model in the </span><em class="italic"><span class="koboSpan" id="kobo.207.1">Saving a model for deployment</span></em><span class="koboSpan" id="kobo.208.1"> section of this chapter, which could be added to the </span><strong class="bold"><span class="koboSpan" id="kobo.209.1">classification</span></strong><span class="koboSpan" id="kobo.210.1"> branch of </span><em class="italic"><span class="koboSpan" id="kobo.211.1">Figure 5.6</span></em><span class="koboSpan" id="kobo.212.1">. </span><span class="koboSpan" id="kobo.212.2">Perhaps you could branch out each of the models with some notes regarding the model itself. </span><span class="koboSpan" id="kobo.212.3">A map or visual aid may prove to be useful when preparing for a technical interview. </span></p>
			<p><span class="koboSpan" id="kobo.213.1">Throughout each of the models we develop, we will follow a particular set of steps to acquire our data, preprocess it, build a model, evaluate its performance, and finally, if the model is sufficient, deploy it to our end users or data engineers. </span><span class="koboSpan" id="kobo.213.2">Before we begin</span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.214.1"> developing our models, let's discuss the common dangers known as </span><em class="italic"><span class="koboSpan" id="kobo.215.1">overfitting</span></em><span class="koboSpan" id="kobo.216.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.217.1">underfitting</span></em><span class="koboSpan" id="kobo.218.1">.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.219.1">Overfitting and underfitting</span></h1>
			<p><span class="koboSpan" id="kobo.220.1">Within the</span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.221.1"> context of </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.222.1">SML, we will prepare our models by </span><em class="italic"><span class="koboSpan" id="kobo.223.1">fitting</span></em><span class="koboSpan" id="kobo.224.1"> them with historical data. </span><span class="koboSpan" id="kobo.224.2">The process of fitting a model generally outputs a measure of how well the model generalizes to data that is similar to the data on which the model was trained. </span><span class="koboSpan" id="kobo.224.3">Using this output, usually in the form of </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">precision</span></strong><span class="koboSpan" id="kobo.226.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.227.1">accuracy</span></strong><span class="koboSpan" id="kobo.228.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.229.1">recall</span></strong><span class="koboSpan" id="kobo.230.1">, we can determine whether the method we implemented or the parameters we changed had a positive impact on our model. </span><span class="koboSpan" id="kobo.230.2">If we revisit the definition of ML models that from earlier in this chapter, we specifically refer to them as models that </span><em class="italic"><span class="koboSpan" id="kobo.231.1">learn</span></em><span class="koboSpan" id="kobo.232.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.233.1">generalize</span></em><span class="koboSpan" id="kobo.234.1"> from historical data. </span><span class="koboSpan" id="kobo.234.2">Models that are able to learn from historical data are referred to as </span><em class="italic"><span class="koboSpan" id="kobo.235.1">well-fitted</span></em><span class="koboSpan" id="kobo.236.1"> models, in the sense that they are able to perform accurately on new and unseen data.</span></p>
			<p><span class="koboSpan" id="kobo.237.1">There are instances in which models are underfitted. </span><em class="italic"><span class="koboSpan" id="kobo.238.1">Underfitted</span></em><span class="koboSpan" id="kobo.239.1"> models generally perform poorly on datasets, which means they have not learned to generalize well. </span><span class="koboSpan" id="kobo.239.2">These cases are generally the result of an inappropriate model being selected for a given dataset or the inadequate setting of a parameter/hyperparameter for that model.</span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.240.1">Important note</span></p>
			<p class="callout"><strong class="bold"><span class="koboSpan" id="kobo.241.1">Parameters and hyperparameters</span></strong><span class="koboSpan" id="kobo.242.1">: Note </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.243.1">that while parameters</span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.244.1"> and hyperparameters are terms that are often used interchangeably, there is a difference between the two. </span><em class="italic"><span class="koboSpan" id="kobo.245.1">Hyperparameters</span></em><span class="koboSpan" id="kobo.246.1"> are parameters that are not learned by a model's estimator and must be manually tuned. </span></p>
			<p><span class="koboSpan" id="kobo.247.1">There are also instances in which models are overfitted. </span><em class="italic"><span class="koboSpan" id="kobo.248.1">Overfitted</span></em><span class="koboSpan" id="kobo.249.1"> models are models that </span><em class="italic"><span class="koboSpan" id="kobo.250.1">know</span></em><span class="koboSpan" id="kobo.251.1"> the dataset a little too well, and this means that they are no longer </span><em class="italic"><span class="koboSpan" id="kobo.252.1">learning</span></em><span class="koboSpan" id="kobo.253.1"> but </span><em class="italic"><span class="koboSpan" id="kobo.254.1">memorizing</span></em><span class="koboSpan" id="kobo.255.1">. </span><span class="koboSpan" id="kobo.255.2">Overfitting generally occurs when a model begins to learn from the noise within a dataset and is no longer able to generalize well on new data. </span><span class="koboSpan" id="kobo.255.3">The differences between well-fitted, overfitted, and underfitted models can be seen in </span><em class="italic"><span class="koboSpan" id="kobo.256.1">Figure 5.7</span></em><span class="koboSpan" id="kobo.257.1">:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<span class="koboSpan" id="kobo.258.1"><img src="image/B17761_05_007.jpg" alt="Figure 5.7 – A representation of overfitting and underfitting data "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.259.1">Figure 5.7 – A representation of overfitting and underfitting data</span></p>
			<p><span class="koboSpan" id="kobo.260.1">The</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.261.1"> objective</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.262.1"> of every data scientist is to develop a balanced model with optimal performance when it comes to your metrics of interest. </span><span class="koboSpan" id="kobo.262.2">One of the best ways to ensure that you are developing a balanced model that is not underfitting or overfitting is by splitting your dataset ahead of time and ensuring that the model is only ever trained on a subset of the data. </span><span class="koboSpan" id="kobo.262.3">We can split datasets into two categories: </span><em class="italic"><span class="koboSpan" id="kobo.263.1">training data</span></em><span class="koboSpan" id="kobo.264.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.265.1">testing data</span></em><span class="koboSpan" id="kobo.266.1"> (also often referred to as </span><em class="italic"><span class="koboSpan" id="kobo.267.1">validation data</span></em><span class="koboSpan" id="kobo.268.1">). </span><span class="koboSpan" id="kobo.268.2">We can use the training dataset to train the model, and we can use the testing dataset to test (or validate) the model. </span><span class="koboSpan" id="kobo.268.3">One of the most common classes to use for this purpose is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.270.1"> class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">sklearn</span></strong><span class="koboSpan" id="kobo.272.1">. </span><span class="koboSpan" id="kobo.272.2">If you think of your dataset with </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">X</span></strong><span class="koboSpan" id="kobo.274.1"> being your input variables and </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">ŷ</span></strong><span class="koboSpan" id="kobo.276.1"> being your output, you can split the dataset using the following code snippet. </span><span class="koboSpan" id="kobo.276.2">First, we import the data. </span><span class="koboSpan" id="kobo.276.3">Then, we isolate the features we are interested in and output their respective variables. </span><span class="koboSpan" id="kobo.276.4">Then, we implement the </span><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.278.1"> function to split the data accordingly:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.279.1">import pandas as pd</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.280.1">from sklearn.model_selection import train_test_split</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.281.1">df = pd.read_csv("../../datasets/dataset_wisc_sd.csv")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.282.1">X = df.drop(columns = ["id", "diagnosis"])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.283.1">y = df.diagnosis.values</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.284.1">X_train, X_test, y_train, y_test = train_test_split(X, y)</span></p>
			<p><span class="koboSpan" id="kobo.285.1">We can visualize the split dataset in </span><em class="italic"><span class="koboSpan" id="kobo.286.1">Figure 5.8</span></em><span class="koboSpan" id="kobo.287.1">: </span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<span class="koboSpan" id="kobo.288.1"><img src="image/B17761_05_008.jpg" alt="Figure 5.8 – A visual representation of data that has been split for training and testing "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.289.1">Figure 5.8 – A visual representation of data that has been split for training and testing</span></p>
			<p><span class="koboSpan" id="kobo.290.1">With the data split in this fashion, we can now use </span><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">X_train</span></strong><span class="koboSpan" id="kobo.292.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">y_train</span></strong><span class="koboSpan" id="kobo.294.1"> for the purposes of training our model, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">X_test</span></strong><span class="koboSpan" id="kobo.296.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">y_test</span></strong><span class="koboSpan" id="kobo.298.1"> for the purposes of testing (or validating) our model. </span><span class="koboSpan" id="kobo.298.2">The default splitting ratio is 75% training data to 25% testing data; however, we can pass the </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">test_size</span></strong><span class="koboSpan" id="kobo.300.1"> parameter to change this to any</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.301.1"> other ratio. </span><span class="koboSpan" id="kobo.301.2">We generally want to train on as much data as possible</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.302.1"> but still keep a meaningful amount of unseen data in reserve, and so </span><em class="italic"><span class="koboSpan" id="kobo.303.1">75/25</span></em><span class="koboSpan" id="kobo.304.1"> is a commonly accepted ratio in the industry. </span><span class="koboSpan" id="kobo.304.2">With this concept in mind, let's move on to developing a full ML model.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.305.1">Developing an ML model</span></h1>
			<p><span class="koboSpan" id="kobo.306.1">There are </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.307.1">numerous ML models that we interact with on a daily basis as end users, and we likely do not even realize it. </span><span class="koboSpan" id="kobo.307.2">Think back to all the activities you did today: scrolling through social media, checking your email, or perhaps you visited a store or a supermarket. </span><span class="koboSpan" id="kobo.307.3">In each of these settings, you likely interacted with an already deployed ML model. </span><span class="koboSpan" id="kobo.307.4">On social media, the posts that are presented on your feed are likely the output of a supervised </span><strong class="bold"><span class="koboSpan" id="kobo.308.1">recommendation</span></strong><span class="koboSpan" id="kobo.309.1"> model. </span><span class="koboSpan" id="kobo.309.2">The emails you opened were likely filtered for spam emails using a </span><strong class="bold"><span class="koboSpan" id="kobo.310.1">classification</span></strong><span class="koboSpan" id="kobo.311.1"> model. </span><span class="koboSpan" id="kobo.311.2">And, finally, the number of goods available within the grocery store was likely the output of a </span><strong class="bold"><span class="koboSpan" id="kobo.312.1">regression</span></strong><span class="koboSpan" id="kobo.313.1"> model, allowing them to predict today's demand. </span><span class="koboSpan" id="kobo.313.2">In each of these models, a great deal of time and effort was dedicated to ensuring they function and operate correctly. </span><span class="koboSpan" id="kobo.313.3">In these situations, while the development of the model is important, the most important thing is how the data is prepared ahead of time. </span><span class="koboSpan" id="kobo.313.4">As scientists, we always have a tendency to organize our thoughts and processes as best we can, so let's</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.314.1"> organize a workflow for the process of developing ML models:</span></p>
			<ol>
				<li><strong class="bold"><span class="koboSpan" id="kobo.315.1">Data acquisition</span></strong><span class="koboSpan" id="kobo.316.1">: Collecting data via SQL queries, local imports, or API requests</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.317.1">EDA and preprocessing</span></strong><span class="koboSpan" id="kobo.318.1">: Understanding and cleaning up the dataset</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.319.1">Model development and validation</span></strong><span class="koboSpan" id="kobo.320.1">: Training a model and verifying the results</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.321.1">Deployment</span></strong><span class="koboSpan" id="kobo.322.1">: Making your model available to end users </span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.323.1">With these steps in mind, let's go ahead and develop our first model.</span></p>
			<p><span class="koboSpan" id="kobo.324.1">We begin by importing our data. </span><span class="koboSpan" id="kobo.324.2">We will use a new dataset that we have not worked with yet known as the </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">Breast Cancer Wisconsin</span></strong><span class="koboSpan" id="kobo.326.1"> dataset. </span><span class="koboSpan" id="kobo.326.2">This is a </span><em class="italic"><span class="koboSpan" id="kobo.327.1">multivariate</span></em><span class="koboSpan" id="kobo.328.1"> dataset, published in 1995, containing several hundred instances of breast cancer masses. </span><span class="koboSpan" id="kobo.328.2">These masses are described in the form of measurements that we will use as </span><em class="italic"><span class="koboSpan" id="kobo.329.1">features</span></em><span class="koboSpan" id="kobo.330.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">X</span></strong><span class="koboSpan" id="kobo.332.1">). </span><span class="koboSpan" id="kobo.332.2">The dataset also includes information regarding the malignancy of each of the instances, which we will use for our output </span><em class="italic"><span class="koboSpan" id="kobo.333.1">label</span></em><span class="koboSpan" id="kobo.334.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">ŷ</span></strong><span class="koboSpan" id="kobo.336.1">). </span><span class="koboSpan" id="kobo.336.2">Given that we have both the input data and the output data, this calls for the use of a </span><em class="italic"><span class="koboSpan" id="kobo.337.1">classification</span></em><span class="koboSpan" id="kobo.338.1"> model. </span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.339.1">Data acquisition</span></h2>
			<p><span class="koboSpan" id="kobo.340.1">Let's</span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.341.1"> import our data and check its overall shape:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.342.1">import pandas as pd</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.343.1">import numpy as np</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.344.1">df = pd.read_csv("../../datasets/dataset_wisc_sd.csv")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.345.1">df.shape</span></p>
			<p><span class="koboSpan" id="kobo.346.1">We notice that there are 569 rows (which we</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.347.1"> generally call </span><em class="italic"><span class="koboSpan" id="kobo.348.1">observations</span></em><span class="koboSpan" id="kobo.349.1">) and 32 columns (which </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.350.1">we generally call </span><em class="italic"><span class="koboSpan" id="kobo.351.1">features</span></em><span class="koboSpan" id="kobo.352.1">) of data. </span><span class="koboSpan" id="kobo.352.2">We generally want our dataset to have many more </span><em class="italic"><span class="koboSpan" id="kobo.353.1">observations</span></em><span class="koboSpan" id="kobo.354.1"> than </span><em class="italic"><span class="koboSpan" id="kobo.355.1">features</span></em><span class="koboSpan" id="kobo.356.1">. </span><span class="koboSpan" id="kobo.356.2">There is no golden rule about the ideal ratio between the two, but you generally want to have at least 10x more observations than features. </span><span class="koboSpan" id="kobo.356.3">So, with 32 columns, you would want to have at least 320 observations – which we do in this case!</span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.357.1">Exploratory data analysis and preprocessing:</span></h2>
			<p><strong class="bold"><span class="koboSpan" id="kobo.358.1">Exploratory Data Analysis</span></strong><span class="koboSpan" id="kobo.359.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.360.1">EDA</span></strong><span class="koboSpan" id="kobo.361.1">) is</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.362.1"> arguably </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.363.1">one of the</span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.364.1"> most important and time-consuming steps in any given ML project. </span><span class="koboSpan" id="kobo.364.2">This step generally consists of many smaller steps whose objectives are as follows: </span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.365.1">Understand the data and its features.</span></li>
				<li><span class="koboSpan" id="kobo.366.1">Address any inconsistencies or missing values.</span></li>
				<li><span class="koboSpan" id="kobo.367.1">Check for any correlations between the features. </span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.368.1">Please note that the order in which we carry out these steps may be different depending on your dataset. </span><span class="koboSpan" id="kobo.368.2">With all this in mind, let's get started!</span></p>
			<h3><span class="koboSpan" id="kobo.369.1">Examining the dataset</span></h3>
			<p><span class="koboSpan" id="kobo.370.1">One of the</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.371.1"> first steps after importing your dataset is to quickly check the quality of the data. </span><span class="koboSpan" id="kobo.371.2">Recall that we can use square brackets (</span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">[]</span></strong><span class="koboSpan" id="kobo.373.1">) to specify the columns of interest, and we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">head()</span></strong><span class="koboSpan" id="kobo.375.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">tail()</span></strong><span class="koboSpan" id="kobo.377.1"> functions to see the first or last five rows of data:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.378.1">df[["id", "diagnosis", "radius_mean", "texture_mean", "concave points_worst"]].head()</span></p>
			<p><span class="koboSpan" id="kobo.379.1">We can see the results of this code in </span><em class="italic"><span class="koboSpan" id="kobo.380.1">Figure 5.9</span></em><span class="koboSpan" id="kobo.381.1">:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<span class="koboSpan" id="kobo.382.1"><img src="image/B17761_05_09.jpg" alt="Figure 5.9 – A sample of the Breast Cancer Wisconsin dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.383.1">Figure 5.9 – A sample of the Breast Cancer Wisconsin dataset</span></p>
			<p><span class="koboSpan" id="kobo.384.1">We can quickly get a sense of the fact that the data is very well organized, and from a first glance, it does not appear to have any problematic values, such as unusual characters or missing values. </span><span class="koboSpan" id="kobo.384.2">Looking over these select columns, we notice that there is a unique identifier in the beginning consisting of </span><em class="italic"><span class="koboSpan" id="kobo.385.1">integers</span></em><span class="koboSpan" id="kobo.386.1">, followed by the diagnosis (</span><strong class="bold"><span class="koboSpan" id="kobo.387.1">M</span></strong><span class="koboSpan" id="kobo.388.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.389.1">malignant</span></strong><span class="koboSpan" id="kobo.390.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.391.1">B</span></strong><span class="koboSpan" id="kobo.392.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.393.1">benign</span></strong><span class="koboSpan" id="kobo.394.1">) consisting of </span><em class="italic"><span class="koboSpan" id="kobo.395.1">strings</span></em><span class="koboSpan" id="kobo.396.1">. </span><span class="koboSpan" id="kobo.396.2">The rest of the columns are all features, and they all appear to be of the </span><em class="italic"><span class="koboSpan" id="kobo.397.1">float</span></em><span class="koboSpan" id="kobo.398.1"> (decimals) data type. </span><span class="koboSpan" id="kobo.398.2">I encourage you to expand the scope of the preceding table and explore all the other features within this dataset.</span></p>
			<p><span class="koboSpan" id="kobo.399.1">In addition to exploring the values, we can also explore some of the summary statistics provided by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">describe()</span></strong><span class="koboSpan" id="kobo.401.1"> function in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">pandas</span></strong><span class="koboSpan" id="kobo.403.1"> library. </span><span class="koboSpan" id="kobo.403.2">Using this function, we get a sense of the total count, as well as some descriptive statistics such as the mean, maximum, and minimum values:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.404.1">df[["id", "diagnosis", "radius_mean", "texture_mean", "perimeter_mean", "area_mean", "concave points_worst"]].describe()</span></p>
			<p><span class="koboSpan" id="kobo.405.1">The output of this function can be seen in the following screenshot:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<span class="koboSpan" id="kobo.406.1"><img src="image/B17761_05_10.jpg" alt="Figure 5.10 – A table of some summary statistics for a DataFrame "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.407.1">Figure 5.10 – A table of some summary statistics for a DataFrame</span></p>
			<p><span class="koboSpan" id="kobo.408.1">Looking over the code, we notice that we had requested the statistics for seven columns, however, only five appeared in the table. </span><span class="koboSpan" id="kobo.408.2">We can see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">id</span></strong><span class="koboSpan" id="kobo.410.1"> values (which are </span><em class="italic"><span class="koboSpan" id="kobo.411.1">primary keys</span></em><span class="koboSpan" id="kobo.412.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.413.1">unique identifiers</span></em><span class="koboSpan" id="kobo.414.1">) were summarized here. </span><span class="koboSpan" id="kobo.414.2">These values are meaningless, as the mean, maximum, and minimum of a set of primary keys tell us nothing. </span><span class="koboSpan" id="kobo.414.3">We can ignore this column for now. </span><span class="koboSpan" id="kobo.414.4">We also asked for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">diagnosis</span></strong><span class="koboSpan" id="kobo.416.1"> column; however, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">diagnosis</span></strong><span class="koboSpan" id="kobo.418.1"> column does not use a numerical value. </span><span class="koboSpan" id="kobo.418.2">Instead, it contains </span><em class="italic"><span class="koboSpan" id="kobo.419.1">strings</span></em><span class="koboSpan" id="kobo.420.1">. </span><span class="koboSpan" id="kobo.420.2">Finally, we see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">concave points_worst</span></strong><span class="koboSpan" id="kobo.422.1"> feature was also not included in this table, indicating that the data type is </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.423.1">not numerical for whatever reason. </span><span class="koboSpan" id="kobo.423.2">We will take a closer look at this in the next section when we clean the data.</span></p>
			<h3><span class="koboSpan" id="kobo.424.1">Cleaning up values</span></h3>
			<p><span class="koboSpan" id="kobo.425.1">Getting the </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.426.1">values within your dataset cleaned up is one of the most important steps when handling an ML project. </span><span class="koboSpan" id="kobo.426.2">A famous saying among data scientists when describing models is </span><em class="italic"><span class="koboSpan" id="kobo.427.1">garbage in, garbage out</span></em><span class="koboSpan" id="kobo.428.1">. </span><span class="koboSpan" id="kobo.428.2">If you want to have a strong predictive model, then ensuring the data that supports it is of good quality is an important first step. </span></p>
			<p><span class="koboSpan" id="kobo.429.1">To begin, let's take a closer look at the data types, given that there may be some inconsistencies here. </span><span class="koboSpan" id="kobo.429.2">We can get a sense of the data types for each of the 32 columns using the following code:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.430.1">df.dtypes</span></p>
			<p><span class="koboSpan" id="kobo.431.1">We can see the output of this code in </span><em class="italic"><span class="koboSpan" id="kobo.432.1">Figure 5.11</span></em><span class="koboSpan" id="kobo.433.1">, where the column names are shown with their respective data types:</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<span class="koboSpan" id="kobo.434.1"><img src="image/B17761_05_011.jpg" alt="Figure 5.11 – A list of all of the columns in a dataset with their respective data types "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.435.1">Figure 5.11 – A list of all of the columns in a dataset with their respective data types</span></p>
			<p><span class="koboSpan" id="kobo.436.1">Looking over </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.437.1">the listed data types, we see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">id</span></strong><span class="koboSpan" id="kobo.439.1"> column is listed as an integer and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.440.1">diagnosis</span></strong><span class="koboSpan" id="kobo.441.1"> column is listed as an object, which seems consistent with the fact that it appeared to be a single-letter string in </span><em class="italic"><span class="koboSpan" id="kobo.442.1">Figure 5.9</span></em><span class="koboSpan" id="kobo.443.1">. </span><span class="koboSpan" id="kobo.443.2">Looking over the features, they are all listed as floats, completely consistent with what we previously saw, with the exception of one feature: </span><strong class="source-inline"><span class="koboSpan" id="kobo.444.1">concave points_worst</span></strong><span class="koboSpan" id="kobo.445.1">. </span><span class="koboSpan" id="kobo.445.2">This feature is listed as an object, indicating that it might be a string. </span><span class="koboSpan" id="kobo.445.3">We noted earlier that this column consisted of float values, and so the column itself should be of the float type. </span><span class="koboSpan" id="kobo.445.4">Let's take a look at this inconsistency sooner rather than later. </span><span class="koboSpan" id="kobo.445.5">We can make an attempt to </span><em class="italic"><span class="koboSpan" id="kobo.446.1">cast</span></em><span class="koboSpan" id="kobo.447.1"> the column to be of the float type instead of using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.448.1">astype()</span></strong><span class="koboSpan" id="kobo.449.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.450.1">df['concave points_worst'] = df['concave points_worst'].astype(float)</span></p>
			<p><span class="koboSpan" id="kobo.451.1">However, you will find that this code will error out, indicating that there is a row in which the </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">\\n</span></strong><span class="koboSpan" id="kobo.453.1"> characters are present and it is unable to convert the string to a float. </span><span class="koboSpan" id="kobo.453.2">This is known </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.454.1">as a </span><em class="italic"><span class="koboSpan" id="kobo.455.1">newline character</span></em><span class="koboSpan" id="kobo.456.1"> and it is one of the most common items or </span><em class="italic"><span class="koboSpan" id="kobo.457.1">impurities</span></em><span class="koboSpan" id="kobo.458.1"> you will deal with when handling datasets. </span><span class="koboSpan" id="kobo.458.2">Let's move on and identify the lines in which this character is present and decide how to deal with it. </span><span class="koboSpan" id="kobo.458.3">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">contains()</span></strong><span class="koboSpan" id="kobo.460.1"> function to find all instances of a particular string:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.461.1">df[df['concave points_worst'].str.contains(r"\\n")]</span></p>
			<p><span class="koboSpan" id="kobo.462.1">The output of this function shows that only the row with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">146</span></strong><span class="koboSpan" id="kobo.464.1"> index contains this character. </span><span class="koboSpan" id="kobo.464.2">Let's take a closer look at the specific cell from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">146</span></strong><span class="koboSpan" id="kobo.466.1"> row:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.467.1">df["concave points_worst"].iloc[146]</span></p>
			<p><span class="koboSpan" id="kobo.468.1">We see that the cell contains the </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">0.1865\\n\\n</span></strong><span class="koboSpan" id="kobo.470.1"> string. </span><span class="koboSpan" id="kobo.470.2">It appears as though the character is printed twice and only in this row. </span><span class="koboSpan" id="kobo.470.3">We could easily open the CSV file and correct this value manually, given that it only occurred a single time. </span><span class="koboSpan" id="kobo.470.4">However, what</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.471.1"> if this </span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.472.1">string appeared 10 times, or 100 times? </span><span class="koboSpan" id="kobo.472.2">Luckily, we can use a </span><strong class="bold"><span class="koboSpan" id="kobo.473.1">Regular Expression</span></strong><span class="koboSpan" id="kobo.474.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.475.1">regex</span></strong><span class="koboSpan" id="kobo.476.1">) to </span><em class="italic"><span class="koboSpan" id="kobo.477.1">match</span></em><span class="koboSpan" id="kobo.478.1"> values, and we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">replace()</span></strong><span class="koboSpan" id="kobo.480.1"> function to </span><em class="italic"><span class="koboSpan" id="kobo.481.1">replace</span></em><span class="koboSpan" id="kobo.482.1"> them. </span><span class="koboSpan" id="kobo.482.2">We can specifically chain this function on </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">df</span></strong><span class="koboSpan" id="kobo.484.1"> instead of the single column to ensure that the function parses the full DataFrame:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.485.1">df = df.replace(r'\\n','', regex=True)</span></p>
			<p><span class="koboSpan" id="kobo.486.1">A regex is a powerful tool that you will often rely on for various text matching and cleaning tasks. </span><span class="koboSpan" id="kobo.486.2">You can remove spaces, numbers, characters, or unusual combinations of characters using regex functions. </span><span class="koboSpan" id="kobo.486.3">We can double-check the regex function's success by once again examining that specific cell's value:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.487.1">df["concave points_worst"].iloc[146]</span></p>
			<p><span class="koboSpan" id="kobo.488.1">The value is now only </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">0.1865</span></strong><span class="koboSpan" id="kobo.490.1">, indicating that the function was, in fact, successful. </span><span class="koboSpan" id="kobo.490.2">We can now cast the column's type to a float using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">astype()</span></strong><span class="koboSpan" id="kobo.492.1"> function and then also confirm the correct datatypes are listed using </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">df.dtypes</span></strong><span class="koboSpan" id="kobo.494.1">.</span></p>
			<p><span class="koboSpan" id="kobo.495.1">So far, we were able to address issues in which invalid characters made their way into the dataset. </span><span class="koboSpan" id="kobo.495.2">However, what about items that are missing? </span><span class="koboSpan" id="kobo.495.3">We can run a quick check on our dataset to determine if any values are missing using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">isna()</span></strong><span class="koboSpan" id="kobo.497.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.498.1">df.isna().values.sum()</span></p>
			<p><span class="koboSpan" id="kobo.499.1">The value returned shows that there are seven rows of data in which a value is missing. </span><span class="koboSpan" id="kobo.499.2">Recall that in </span><a href="B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066"><em class="italic"><span class="koboSpan" id="kobo.500.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.501.1">, </span><em class="italic"><span class="koboSpan" id="kobo.502.1">Visualizing Data with Python</span></em><span class="koboSpan" id="kobo.503.1">, we looked over a few methods to address missing values. </span><span class="koboSpan" id="kobo.503.2">Given that we have a sufficiently large dataset, it would be appropriate to simply eliminate these few rows using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">dropna()</span></strong><span class="koboSpan" id="kobo.505.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.506.1">df = df.dropna()</span></p>
			<p><span class="koboSpan" id="kobo.507.1">We can check the dataset's shape before and after implementing the function to ensure the proper number of rows was in fact dropped. </span></p>
			<p><span class="koboSpan" id="kobo.508.1">Taking some time to clean up your dataset ahead of time is always recommended, as</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.509.1"> it will help prevent problems and unusual errors down the line. </span><span class="koboSpan" id="kobo.509.2">It's always important to check the </span><em class="italic"><span class="koboSpan" id="kobo.510.1">data types</span></em><span class="koboSpan" id="kobo.511.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.512.1">missing values</span></em><span class="koboSpan" id="kobo.513.1">.</span></p>
			<h3><span class="koboSpan" id="kobo.514.1">Understanding the meaning of the data</span></h3>
			<p><span class="koboSpan" id="kobo.515.1">Let's now</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.516.1"> take a closer look at some of the data within this dataset, beginning with the output values in the second column. </span><span class="koboSpan" id="kobo.516.2">We know these values correspond to the labels as being </span><em class="italic"><span class="koboSpan" id="kobo.517.1">M</span></em><span class="koboSpan" id="kobo.518.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.519.1">malignant</span></em><span class="koboSpan" id="kobo.520.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.521.1">B</span></em><span class="koboSpan" id="kobo.522.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.523.1">benign</span></em><span class="koboSpan" id="kobo.524.1">. </span><span class="koboSpan" id="kobo.524.2">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">value_counts()</span></strong><span class="koboSpan" id="kobo.526.1"> function to determine the sum of each category:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.527.1">df['diagnosis'].value_counts()</span></p>
			<p><span class="koboSpan" id="kobo.528.1">The results show that there are 354 instances of benign masses and 208 instances of malignant masses. </span><span class="koboSpan" id="kobo.528.2">We can visualize this ratio using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">seaborn</span></strong><span class="koboSpan" id="kobo.530.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.531.1">sns.countplot(df['diagnosis'])</span></p>
			<p><span class="koboSpan" id="kobo.532.1">The output of this code can be seen as follows:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<span class="koboSpan" id="kobo.533.1"><img src="image/B17761_05_012.png.jpg" alt="Figure 5.12 – A bar plot showing the number of instances for each class "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.534.1">Figure 5.12 – A bar plot showing the number of instances for each class</span></p>
			<p><span class="koboSpan" id="kobo.535.1">In most ML models, we try to ensure that the output column is </span><em class="italic"><span class="koboSpan" id="kobo.536.1">well balanced</span></em><span class="koboSpan" id="kobo.537.1">, in the sense that the categories are </span><em class="italic"><span class="koboSpan" id="kobo.538.1">roughly equal</span></em><span class="koboSpan" id="kobo.539.1">. </span><span class="koboSpan" id="kobo.539.2">Training a model on an imbalanced dataset with, for example, 95 rows of malignant observations and 5 rows of benign observations would lead to an imbalanced model with poor performance. </span><span class="koboSpan" id="kobo.539.3">In</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.540.1"> addition to visualizing the diagnosis or output column, we can also visualize the features to get a sense of any trends or correlations using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.541.1">pairplot()</span></strong><span class="koboSpan" id="kobo.542.1"> function we reviewed in </span><a href="B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066"><em class="italic"><span class="koboSpan" id="kobo.543.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.544.1">, </span><em class="italic"><span class="koboSpan" id="kobo.545.1">Visualizing Data with Python</span></em><span class="koboSpan" id="kobo.546.1">. </span><span class="koboSpan" id="kobo.546.2">We can implement this with a handful of features:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.547.1">sns.pairplot(df[["diagnosis", "radius_mean", "concave points_mean", "texture_mean"]], hue = 'diagnosis')</span></p>
			<p><span class="koboSpan" id="kobo.548.1">The following graph shows the output of this:</span></p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<span class="koboSpan" id="kobo.549.1"><img src="image/B17761_05_013.png.jpg" alt="Figure 5.13 – A pair plot of selected features "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.550.1">Figure 5.13 – A pair plot of selected features</span></p>
			<p><span class="koboSpan" id="kobo.551.1">Looking over</span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.552.1"> these last few plots, we notice a distinguishable separation between the two clusters of data. </span><span class="koboSpan" id="kobo.552.2">The clusters appear to exhibit some of the characteristics of a </span><strong class="bold"><span class="koboSpan" id="kobo.553.1">normal distribution</span></strong><span class="koboSpan" id="kobo.554.1">, in the sense that most points are localized closer to the center, with fewer points further away. </span><span class="koboSpan" id="kobo.554.2">Given this nature, one of the first models we may try within this dataset is a </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">Naïve Bayes classifier</span></strong><span class="koboSpan" id="kobo.556.1">, which tends to work well for this type of data. </span><span class="koboSpan" id="kobo.556.2">However, we will discuss this model in greater detail later on in this chapter.</span></p>
			<p><span class="koboSpan" id="kobo.557.1">In each of these plots, we see some degree of overlap between the two classes, indicating that two columns alone are not enough to maintain a good degree of separation. </span><span class="koboSpan" id="kobo.557.2">So, we could ensure that our ML models utilize more columns or we could try to eliminate any potential outliers that may contribute to this overlap – or we could do both!</span></p>
			<p><span class="koboSpan" id="kobo.558.1">First, we can utilize some descriptive </span><em class="italic"><span class="koboSpan" id="kobo.559.1">statistics</span></em><span class="koboSpan" id="kobo.560.1">. </span><span class="koboSpan" id="kobo.560.2">Specifically, we can use the </span><strong class="bold"><span class="koboSpan" id="kobo.561.1">Interquartile Range</span></strong><span class="koboSpan" id="kobo.562.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.563.1">IQR</span></strong><span class="koboSpan" id="kobo.564.1">) to identify </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.565.1">any potential outliers. </span><span class="koboSpan" id="kobo.565.2">Let's examine this for the malignant masses. </span><span class="koboSpan" id="kobo.565.3">We begin by isolating the malignant observations in their own variable called </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">dfm</span></strong><span class="koboSpan" id="kobo.567.1">. </span><span class="koboSpan" id="kobo.567.2">We can then define the first quartile (</span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">Q1</span></strong><span class="koboSpan" id="kobo.569.1">) and third quartile (</span><strong class="source-inline"><span class="koboSpan" id="kobo.570.1">Q3</span></strong><span class="koboSpan" id="kobo.571.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">radius_mean</span></strong><span class="koboSpan" id="kobo.573.1"> feature:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.574.1">dfm = df[df["diagnosis"] == "M"]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.575.1">Q1 = dfm['radius_mean'].quantile(0.25)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.576.1">Q3 = dfm['radius_mean'].quantile(0.75)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.577.1">IQR = Q3 - Q1</span></p>
			<p><span class="koboSpan" id="kobo.578.1">We can then print the outputs of these variables to determine the IQR in conjunction with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.579.1">mean()</span></strong><span class="koboSpan" id="kobo.580.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.581.1">median()</span></strong><span class="koboSpan" id="kobo.582.1"> functions to get a sense of the distribution</span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.583.1"> of the data. </span><span class="koboSpan" id="kobo.583.2">We can visualize these metrics alongside the upper and lower ranges using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">boxplot()</span></strong><span class="koboSpan" id="kobo.585.1"> function provided in </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">seaborn</span></strong><span class="koboSpan" id="kobo.587.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.588.1">sns.boxplot(x='diagnosis', y='radius_mean', data=df)</span></p>
			<p><span class="koboSpan" id="kobo.589.1">This gives us </span><em class="italic"><span class="koboSpan" id="kobo.590.1">Figure 5.14</span></em><span class="koboSpan" id="kobo.591.1">:</span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<span class="koboSpan" id="kobo.592.1"><img src="image/B17761_05_014.png.jpg" alt="Figure 5.14 – A box-whisker plot of the radius_mean feature "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.593.1">Figure 5.14 – A box-whisker plot of the radius_mean feature</span></p>
			<p><span class="koboSpan" id="kobo.594.1">Using the upper and lower ranges, we can filter the DataFrame to exclude any data that falls outside of this scope using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">query()</span></strong><span class="koboSpan" id="kobo.596.1"> class within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">pandas</span></strong><span class="koboSpan" id="kobo.598.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.599.1">df = df.query('(@Q1 - 1.5 * @IQR) &lt;= radius_mean &lt;= (@Q3 + 1.5 * @IQR)')</span></p>
			<p><span class="koboSpan" id="kobo.600.1">With the code executed, we have successfully removed several outliers from our dataset. </span><span class="koboSpan" id="kobo.600.2">If we go ahead and replot the data using one of the preceding scatter plots, we will see that while some of the overlap was indeed reduced, there is still considerable overlap between the two classes, indicating that any future models we develop will need to take advantage of multiple columns to ensure adequate separation as we begin developing a robust classifier. </span><span class="koboSpan" id="kobo.600.3">Before we can start training </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.601.1">any classifiers, we will first need to address any potential </span><em class="italic"><span class="koboSpan" id="kobo.602.1">correlations</span></em><span class="koboSpan" id="kobo.603.1"> within the features.</span></p>
			<h3><span class="koboSpan" id="kobo.604.1">Finding correlations</span></h3>
			<p><span class="koboSpan" id="kobo.605.1">With the</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.606.1"> outliers filtered out, we are now ready to start taking a closer look at any correlations between the features in our dataset. </span><span class="koboSpan" id="kobo.606.2">Given that this dataset consists of 30 features, we can take advantage of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">corr()</span></strong><span class="koboSpan" id="kobo.608.1"> class we implemented in </span><a href="B17761_04_Final_JM_ePub.xhtml#_idTextAnchor066"><em class="italic"><span class="koboSpan" id="kobo.609.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.610.1">, </span><em class="italic"><span class="koboSpan" id="kobo.611.1">Visualizing Data with Python</span></em><span class="koboSpan" id="kobo.612.1">. </span><span class="koboSpan" id="kobo.612.2">We can </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.613.1">create a </span><strong class="bold"><span class="koboSpan" id="kobo.614.1">heat map</span></strong><span class="koboSpan" id="kobo.615.1"> visual by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.616.1">corr()</span></strong><span class="koboSpan" id="kobo.617.1"> function and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">heatmap()</span></strong><span class="koboSpan" id="kobo.619.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.620.1">seaborn</span></strong><span class="koboSpan" id="kobo.621.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.622.1">f, ax=plt.subplots( figsize = (20,15))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.623.1">sns.heatmap(df.corr(), annot= True, fmt = ".1f", ax=ax)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.624.1">plt.xticks(fontsize=18)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.625.1">plt.yticks(fontsize=18)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.626.1">plt.title('Breast Cancer Correlation Map', fontsize=18)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.627.1">plt.show()</span></p>
			<p><span class="koboSpan" id="kobo.628.1">The output of this code can be seen in </span><em class="italic"><span class="koboSpan" id="kobo.629.1">Figure 5.15</span></em><span class="koboSpan" id="kobo.630.1">, showing a heat map of the various features in which the most correlated features are shown in a lighter color and the least correlated features are shown in a darker color:</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<span class="koboSpan" id="kobo.631.1"><img src="image/B17761_05_015.png.jpg" alt="Figure 5.15 – A heat map showing the correlation of features "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.632.1">Figure 5.15 – A heat map showing the correlation of features</span></p>
			<p><span class="koboSpan" id="kobo.633.1">As we look </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.634.1">at this heat map, we see that there is a great deal of correlation between multiple features within this dataset. </span><span class="koboSpan" id="kobo.634.2">Take, for instance, the very strong correlation between the </span><strong class="source-inline"><span class="koboSpan" id="kobo.635.1">radius_worst</span></strong><span class="koboSpan" id="kobo.636.1"> feature and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">perimeter_mean</span></strong><span class="koboSpan" id="kobo.638.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">area_mean</span></strong><span class="koboSpan" id="kobo.640.1"> features. </span><span class="koboSpan" id="kobo.640.2">When there are strong correlations between independent variables or features within a dataset, this is</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.641.1"> known as </span><strong class="bold"><span class="koboSpan" id="kobo.642.1">multicollinearity</span></strong><span class="koboSpan" id="kobo.643.1">. </span><span class="koboSpan" id="kobo.643.2">From a statistical perspective, these correlations within an ML model can lead to less reliable statistical inferences, and therefore, less reliable results. </span><span class="koboSpan" id="kobo.643.3">To ensure that our dataset is purged of any potential problems of this nature, we simply drop the columns that present a very high degree of correlation with any others. </span><span class="koboSpan" id="kobo.643.4">We can identify the correlations using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">corr()</span></strong><span class="koboSpan" id="kobo.645.1"> function and create a matrix of these values. </span><span class="koboSpan" id="kobo.645.2">We can then select the upper triangle (half of the heat map) and then identify the features whose correlations are greater than </span><strong class="source-inline"><span class="koboSpan" id="kobo.646.1">0.90:</span></strong></p>
			<p class="source-code"><span class="koboSpan" id="kobo.647.1">corr_matrix = df.corr().abs()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.648.1">upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.649.1">to_drop = [column for column in upper.columns if any(upper[column] &gt; 0.90)]</span></p>
			<p><span class="koboSpan" id="kobo.650.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.651.1">to_drop</span></strong><span class="koboSpan" id="kobo.652.1"> variable </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.653.1">now represents a list of columns that should be dropped to ensure that any correlations above the threshold we set are effectively removed. </span><span class="koboSpan" id="kobo.653.2">Notice that we used </span><strong class="bold"><span class="koboSpan" id="kobo.654.1">list comprehension</span></strong><span class="koboSpan" id="kobo.655.1"> (a concept that we talked about in </span><a href="B17761_02_Final_JM_ePub.xhtml#_idTextAnchor023"><em class="italic"><span class="koboSpan" id="kobo.656.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.657.1">, </span><em class="italic"><span class="koboSpan" id="kobo.658.1">Introducing Python and the Command Line</span></em><span class="koboSpan" id="kobo.659.1">) to iterate through these values quickly and effectively. </span><span class="koboSpan" id="kobo.659.2">We can then go ahead and drop the columns from our dataset:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.660.1">df.drop(to_drop, axis=1, inplace=True)</span></p>
			<p><span class="koboSpan" id="kobo.661.1">We can once again plot the heat map to ensure that any potential collinearity is addressed:</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<span class="koboSpan" id="kobo.662.1"><img src="image/B17761_05_016.png.jpg" alt="Figure 5.16 – A heat map showing the correlation of features without multicollinearity "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.663.1">Figure 5.16 – A heat map showing the correlation of features without multicollinearity</span></p>
			<p><span class="koboSpan" id="kobo.664.1">Notice that </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.665.1">the groups of highly correlated features are no longer present. </span><span class="koboSpan" id="kobo.665.2">With the correlations now addressed, we have not only ensured that any potential models we created will not suffer from any performance problems relating to </span><em class="italic"><span class="koboSpan" id="kobo.666.1">multicollinearity</span></em><span class="koboSpan" id="kobo.667.1">, but we also inadvertently reduced the size of the dataset from 30 columns of features down to only 19, making it a little easier to handle and visualize! </span><span class="koboSpan" id="kobo.667.2">With the dataset now completely preprocessed, we are now ready to start training and preparing </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.668.1">some ML models.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.669.1">Developing and validating models</span></h2>
			<p><span class="koboSpan" id="kobo.670.1">Now</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.671.1"> that the data is ready to go, we can explore a </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.672.1">few models. </span><span class="koboSpan" id="kobo.672.2">Recall that our objective here is to develop a </span><em class="italic"><span class="koboSpan" id="kobo.673.1">classification</span></em><span class="koboSpan" id="kobo.674.1"> model. </span><span class="koboSpan" id="kobo.674.2">Therefore, our first step will be to separate our </span><strong class="source-inline"><span class="koboSpan" id="kobo.675.1">X</span></strong><span class="koboSpan" id="kobo.676.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.677.1">ŷ</span></em><span class="koboSpan" id="kobo.678.1"> values. </span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.679.1">We will create a variable, </span><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">X</span></strong><span class="koboSpan" id="kobo.681.1">, representing all of the features within the dataset (excluding the </span><strong class="source-inline"><span class="koboSpan" id="kobo.682.1">id</span></strong><span class="koboSpan" id="kobo.683.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">diagnosis</span></strong><span class="koboSpan" id="kobo.685.1"> columns, as these are not features). </span><span class="koboSpan" id="kobo.685.2">We will then create a variable, </span><strong class="source-inline"><span class="koboSpan" id="kobo.686.1">y</span></strong><span class="koboSpan" id="kobo.687.1">, representing the output column:</span><p class="source-code"><span class="koboSpan" id="kobo.688.1">X = df.drop(columns = ["id", "diagnosis"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.689.1">y = df.diagnosis.values</span></p><p><span class="koboSpan" id="kobo.690.1">Within most of the datasets we will work with, we will generally see a large difference in the </span><em class="italic"><span class="koboSpan" id="kobo.691.1">magnitude</span></em><span class="koboSpan" id="kobo.692.1"> of values, in the sense that one column could be on the order of 1,000, and another column could be on the order of 0.1. </span><span class="koboSpan" id="kobo.692.2">This means that features with far greater values will be perceived by the model to make far greater contributions to a prediction – which is not true. </span><span class="koboSpan" id="kobo.692.3">For example, think of a project in which we are trying to predict the lipophilicity of a molecule using 30 different features, with one of those being the molecular weight – a feature with a significantly large value but not that large a contribution. </span></p></li>
				<li><span class="koboSpan" id="kobo.693.1">To address this challenge, values within a dataset must be </span><strong class="bold"><span class="koboSpan" id="kobo.694.1">normalized</span></strong><span class="koboSpan" id="kobo.695.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.696.1">scaled</span></strong><span class="koboSpan" id="kobo.697.1">) through various functions – for example, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.698.1">StandardScaler()</span></strong><span class="koboSpan" id="kobo.699.1"> function from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">sklearn</span></strong><span class="koboSpan" id="kobo.701.1"> library:</span><p class="source-code"><span class="koboSpan" id="kobo.702.1">from sklearn.preprocessing import StandardScaler</span></p><p class="source-code"><span class="koboSpan" id="kobo.703.1">scaler = StandardScaler()</span></p><p class="source-code"><span class="koboSpan" id="kobo.704.1">X_scaled = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)</span></p></li>
				<li><span class="koboSpan" id="kobo.705.1">With the features now normalized, our next step is to split the data up into our </span><em class="italic"><span class="koboSpan" id="kobo.706.1">training</span></em><span class="koboSpan" id="kobo.707.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.708.1">testing</span></em><span class="koboSpan" id="kobo.709.1"> sets. </span><span class="koboSpan" id="kobo.709.2">Recall that the purpose of the training set is to train the model, and the testing set is to test the model. </span><span class="koboSpan" id="kobo.709.3">This is done to avoid any </span><em class="italic"><span class="koboSpan" id="kobo.710.1">overfitting</span></em><span class="koboSpan" id="kobo.711.1"> in the development process:</span><p class="source-code"><span class="koboSpan" id="kobo.712.1">from sklearn.model_selection import train_test_split</span></p><p class="source-code"><span class="koboSpan" id="kobo.713.1">X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=40)</span></p><p><span class="koboSpan" id="kobo.714.1">With the data now split up into four variables, we are now ready to train a few models, beginning with the </span><strong class="bold"><span class="koboSpan" id="kobo.715.1">Gaussian Naïve Bayes classifier</span></strong><span class="koboSpan" id="kobo.716.1">. </span><span class="koboSpan" id="kobo.716.2">This model is a supervised algorithm based on the application of Bayes' theorem. </span><span class="koboSpan" id="kobo.716.3">The model is called </span><em class="italic"><span class="koboSpan" id="kobo.717.1">naïve</span></em><span class="koboSpan" id="kobo.718.1"> because it makes the assumption that the </span><em class="italic"><span class="koboSpan" id="kobo.719.1">features</span></em><span class="koboSpan" id="kobo.720.1"> of each </span><em class="italic"><span class="koboSpan" id="kobo.721.1">observation</span></em><span class="koboSpan" id="kobo.722.1"> are independent of one another, which is rarely</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.723.1"> true. </span><span class="koboSpan" id="kobo.723.2">However, this model </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.724.1">tends to show strong performance anyway. </span><span class="koboSpan" id="kobo.724.2">The main idea behind the Gaussian Naïve Bayes classifier can be examined from a </span><em class="italic"><span class="koboSpan" id="kobo.725.1">probability</span></em><span class="koboSpan" id="kobo.726.1"> perspective. </span><span class="koboSpan" id="kobo.726.2">To explain what we mean by this, consider the following equation:</span></p><div id="_idContainer100" class="IMG---Figure"><span class="koboSpan" id="kobo.727.1"><img src="image/Formula_B17761_05_001.jpg" alt=""/></span></div><p><span class="koboSpan" id="kobo.728.1">This states that the probability of the label (given some data) is equal to the probability of the data (given a label – Gaussian, given the normal distribution) multiplied by the probability of the label (prior probability), all divided by the probability of the data (predictor prior probability). </span><span class="koboSpan" id="kobo.728.2">Given the simplicity of such a model, Naïve Bayes classifiers can be extremely fast to use in relation to more complex models. </span></p></li>
				<li><span class="koboSpan" id="kobo.729.1">Let's take a look at its implementation. </span><span class="koboSpan" id="kobo.729.2">We will begin by importing our libraries of interest: </span><p class="source-code"><span class="koboSpan" id="kobo.730.1">   from sklearn.naive_bayes import GaussianNB</span></p><p class="source-code"><span class="koboSpan" id="kobo.731.1">   from sklearn.metrics import accuracy_score</span></p></li>
				<li><span class="koboSpan" id="kobo.732.1">Next, we can create an instance of the actual model in the form of a variable we call </span><strong class="source-inline"><span class="koboSpan" id="kobo.733.1">gnb_clf</span></strong><span class="koboSpan" id="kobo.734.1">: </span><p class="source-code"><span class="koboSpan" id="kobo.735.1">   gnb_clf = GaussianNB()</span></p></li>
				<li><span class="koboSpan" id="kobo.736.1">We can then fit or train the model using the training dataset we split off earlier:</span><p class="source-code"><span class="koboSpan" id="kobo.737.1">   gnb_clf.fit(X_train, y_train)</span></p></li>
				<li><span class="koboSpan" id="kobo.738.1">Finally, we</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.739.1"> can use the trained model </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.740.1">to make predictions on the testing data and compare the results with the known values. </span><span class="koboSpan" id="kobo.740.2">We can use a simple accuracy score to test the model:</span><p class="source-code"><span class="koboSpan" id="kobo.741.1">gnb_pred = gnb_clf.predict(X_test)</span></p><p class="source-code"><span class="koboSpan" id="kobo.742.1">   print(accuracy_score(gnb_pred, y_test))</span></p><p class="source-code"><span class="koboSpan" id="kobo.743.1">  0.95035</span></p><p><span class="koboSpan" id="kobo.744.1">With that, we have successfully developed a model performing with roughly 95% accuracy – not a bad start for our first model! </span></p></li>
				<li><span class="koboSpan" id="kobo.745.1">While accuracy is always a fantastic metric, it is not the only metric we can use to assess the performance of a model. </span><span class="koboSpan" id="kobo.745.2">We can also use </span><strong class="bold"><span class="koboSpan" id="kobo.746.1">precision</span></strong><span class="koboSpan" id="kobo.747.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.748.1">recall</span></strong><span class="koboSpan" id="kobo.749.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.750.1">f1 scores</span></strong><span class="koboSpan" id="kobo.751.1">. </span><span class="koboSpan" id="kobo.751.2">We can quickly calculate these and get a better sense of the model's performance using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.752.1">classification_report()</span></strong><span class="koboSpan" id="kobo.753.1"> function provided by </span><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">sklearn</span></strong><span class="koboSpan" id="kobo.755.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.756.1">from sklearn.metrics import classification_report</span></p><p class="source-code"><span class="koboSpan" id="kobo.757.1">print(classification_report(gnb_pred, y_test))</span></p><p><span class="koboSpan" id="kobo.758.1">Looking at the following output, we can see our two classes of interest (B and M) listed with their respective metrics: </span><strong class="source-inline"><span class="koboSpan" id="kobo.759.1">precision</span></strong><span class="koboSpan" id="kobo.760.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.761.1">recall</span></strong><span class="koboSpan" id="kobo.762.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">f1-score</span></strong><span class="koboSpan" id="kobo.764.1">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<span class="koboSpan" id="kobo.765.1"><img src="image/B17761_05_017.jpg" alt="Figure 5.17 – The classification report of the Naïve Bayes classifier "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.766.1">Figure 5.17 – The classification report of the Naïve Bayes classifier</span></p>
			<p><span class="koboSpan" id="kobo.767.1">We will discuss</span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.768.1"> these metrics in much more </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.769.1">detail in </span><a href="B17761_07_Final_JM_ePub.xhtml#_idTextAnchor101"><em class="italic"><span class="koboSpan" id="kobo.770.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.771.1">, </span><em class="italic"><span class="koboSpan" id="kobo.772.1">Understanding Supervised Machine Learning</span></em><span class="koboSpan" id="kobo.773.1">. </span><span class="koboSpan" id="kobo.773.2">For now, we can see that all of these metrics are quite high, indicating that the model performed reasonably well.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.774.1">Saving a model for deployment</span></h2>
			<p><span class="koboSpan" id="kobo.775.1">When an ML</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.776.1"> model has been trained and is operating at a reasonable level of accuracy, we may wish to make this model available for others to use. </span><span class="koboSpan" id="kobo.776.2">However, we would not directly deliver the data or the code to data engineers to deploy the model into production. </span><span class="koboSpan" id="kobo.776.3">Instead, we would want to deliver a single trained model that they can take and deploy without having to worry about any moving pieces. </span><span class="koboSpan" id="kobo.776.4">Luckily for us, there is a great library known as </span><strong class="source-inline"><span class="koboSpan" id="kobo.777.1">pickle</span></strong><span class="koboSpan" id="kobo.778.1"> that can help us </span><em class="italic"><span class="koboSpan" id="kobo.779.1">gather</span></em><span class="koboSpan" id="kobo.780.1"> the model into a single entity, allowing us to </span><em class="italic"><span class="koboSpan" id="kobo.781.1">save</span></em><span class="koboSpan" id="kobo.782.1"> the model. </span><span class="koboSpan" id="kobo.782.2">Recall that we explored the </span><strong class="source-inline"><span class="koboSpan" id="kobo.783.1">pickle</span></strong><span class="koboSpan" id="kobo.784.1"> library in </span><a href="B17761_02_Final_JM_ePub.xhtml#_idTextAnchor023"><em class="italic"><span class="koboSpan" id="kobo.785.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.786.1">, </span><em class="italic"><span class="koboSpan" id="kobo.787.1">Getting Started with Python and the Command Line</span></em><span class="koboSpan" id="kobo.788.1">. </span><span class="koboSpan" id="kobo.788.2">We </span><em class="italic"><span class="koboSpan" id="kobo.789.1">pickle</span></em><span class="koboSpan" id="kobo.790.1"> a model, such as the model we named </span><strong class="source-inline"><span class="koboSpan" id="kobo.791.1">gnb_clf</span></strong><span class="koboSpan" id="kobo.792.1">, by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.793.1">dump()</span></strong><span class="koboSpan" id="kobo.794.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.795.1">import pickle</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.796.1">pickle.dump(gnb_clf, open("../../models/gnb_clf.pickle", 'wb'))</span></p>
			<p><span class="koboSpan" id="kobo.797.1">To prove that the model did in fact save correctly, we can load it using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.798.1">load()</span></strong><span class="koboSpan" id="kobo.799.1"> function, and once again, we can calculate the accuracy score:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.800.1">loaded_gnb_clf = pickle.load(open("../../models/gnb_clf.pickle", 'rb'))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.801.1">loaded_gnb_clf.score(X_test, y_test)</span></p>
			<p><span class="koboSpan" id="kobo.802.1">Notice that the output of this scoring calculation results in the same value (95%) as we saw </span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.803.1">earlier, indicating that the model did, in fact, save correctly!</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.804.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.805.1">In this chapter, we took an ambitious step toward understanding some of the most important and useful concepts in ML. </span><span class="koboSpan" id="kobo.805.2">We looked over the various terms used to describe the field as it relates to the domain of AI, examined the main areas of ML and the governing categories of </span><em class="italic"><span class="koboSpan" id="kobo.806.1">supervised</span></em><span class="koboSpan" id="kobo.807.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.808.1">unsupervised</span></em><span class="koboSpan" id="kobo.809.1"> learning, and then proceeded to explore the full process of developing an ML model for a given dataset.</span></p>
			<p><span class="koboSpan" id="kobo.810.1">While developing our model, we explored many useful steps. </span><span class="koboSpan" id="kobo.810.2">We explored and preprocessed the data to remove inconsistencies and missing values. </span><span class="koboSpan" id="kobo.810.3">We also examined the data in great detail, and we subsequently addressed issues relating to </span><em class="italic"><span class="koboSpan" id="kobo.811.1">multicollinearity</span></em><span class="koboSpan" id="kobo.812.1">. </span><span class="koboSpan" id="kobo.812.2">Next, we developed a </span><em class="italic"><span class="koboSpan" id="kobo.813.1">Gaussian Naïve Bayes</span></em><span class="koboSpan" id="kobo.814.1"> classification model, which operated with a robust 95% rate of accuracy – on our first try too! </span><span class="koboSpan" id="kobo.814.2">Finally, we looked at one of the most common ways data scientists hand over their fully trained models to data engineers to move ML models into production. </span></p>
			<p><span class="koboSpan" id="kobo.815.1">Although we took the time within this chapter to understand ML within the scope of a supervised classifier, in the following chapter, we will gain a much better understanding of the nuances and differences as we train several unsupervised models.</span></p>
		</div>
	</body></html>