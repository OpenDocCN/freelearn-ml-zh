<html><head></head><body>
		<div id="_idContainer101">
			<h1 id="_idParaDest-120"><a id="_idTextAnchor141"/><em class="italic">Chapter 9</em>: Deployment and Inference with MLflow</h1>
			<p>In this chapter, you will learn about an end-to-end deployment infrastructure for our <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) system including the inference component with the use of MLflow. We will then move to deploy our model in a cloud-native ML system (AWS SageMaker) and in a hybrid environment with Kubernetes. The main goal of the exposure to these different environments is to equip you with the skills to deploy an ML model under the varying environmental (cloud-native, and on-premises) constraints of different projects.</p>
			<p>The core of this chapter is to deploy the PsyStock model to predict the price of Bitcoin (BTC/USD) based on the previous 14 days of market behavior that you have been working on so far throughout the book. We will deploy this in multiple environments with the aid of a workflow.</p>
			<p>Specifically, we will look at the following sections in this chapter: </p>
			<ul>
				<li>Starting up a local model registry</li>
				<li>Setting up a batch inference job</li>
				<li>Creating an API process for inference</li>
				<li>Deploying your models for batch scoring in Kubernetes</li>
				<li>Making a cloud deployment with AWS SageMaker</li>
			</ul>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor142"/>Technical requirements</h1>
			<p>For this chapter, you will need the following prerequisites: </p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of <strong class="source-inline">docker-compose </strong>installed. Please follow the instructions at https://docs.docker.com/compose/install/.</li>
				<li>Access to Git in the command line, which can be installed as described at <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a Bash terminal (Linux or Windows). </li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>The latest version of your ML platform installed locally as described in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a><em class="italic">, Your Data Science Workbench</em>.</li>
				<li>An AWS account configured to run the MLflow model.</li>
			</ul>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor143"/>Starting up a local model registry </h1>
			<p>Before executing the following sections in this chapter, you will need to set up a centralized model <a id="_idIndexMarker313"/>registry and tracking server. We don't need the whole of the Data Science Workbench, so we can go directly to a lighter variant of the workbench built into the model that we will deploy in the following sections. You should be in the root folder of the code for this chapter, available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter09	.</p>
			<p>Next, move to the <strong class="source-inline">gradflow</strong> directory and start a light version of the environment to serve your model, as follows:</p>
			<p class="source-code">$ cd gradflow</p>
			<p class="source-code">$ export MLFLOW_TRACKING_URI=http://localhost:5000 </p>
			<p class="source-code">$ make gradflow-light</p>
			<p>After having set up our infrastructure for API deployment with MLflow with the model retrieved from the ML registry, we will next move on to the cases where we need to score some batch input data. We will prepare a batch inference job with MLflow for the prediction problem at hand.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor144"/>Setting up a batch inference job</h1>
			<p>The code required for this section is in the <strong class="source-inline">pystock-inference-api folder</strong>. The MLflow infrastructure <a id="_idIndexMarker314"/>is provided in the Docker image accompanying the code as shown in the following figure:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/image0015.jpg" alt="Figure 9.1 – Layout of a batch scoring deployment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Layout of a batch scoring deployment</p>
			<p>If you have direct access to the artifacts, you can do the following. The code is available under the <strong class="source-inline">pystock-inference-batch</strong> directory. In order to set up a batch inference job, we will follow these steps:</p>
			<ol>
				<li>Import the dependencies of your batch job; among the relevant dependencies we include <strong class="source-inline">pandas</strong>, <strong class="source-inline">mlflow,</strong> and <strong class="source-inline">xgboost</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">import mlflow</p><p class="source-code">import xgboost as xgb</p><p class="source-code">import mlflow.xgboost</p><p class="source-code">import mlflow.pyfunc</p></li>
				<li>We will next load <strong class="source-inline">start_run</strong> by calling <strong class="source-inline">mlflow.start_run</strong> and load the data from the <strong class="source-inline">input.csv</strong> scoring input file:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    with mlflow.start_run(run_name="batch_scoring") as run:</p><p class="source-code">        data=pd.read_csv("data/input.csv",header=None)</p></li>
				<li>Next, we load the model from the registry by specifying the <strong class="source-inline">model_uri</strong> value, based on the details of the model:<p class="source-code">        model_name = "training-model-psystock"</p><p class="source-code">        stage = 'Production'</p><p class="source-code">        model = mlflow.pyfunc.load_model(</p><p class="source-code">                model_uri=f"models:/{model_name}/{stage}"</p><p class="source-code">        )</p></li>
				<li>We are now ready to predict over the dataset that we just read by running <strong class="source-inline">model.predict</strong>:<p class="source-code">        y_probas=model.predict(data)</p></li>
				<li>Save the <a id="_idIndexMarker315"/>batch predictions. This basically involves mapping the probability target (of the market going up) in the <strong class="source-inline">y_preds</strong> variable to a value ranging from 0 to 1:<p class="source-code">    y_preds = [1 if  y_proba &gt; 0.5 else 0 for y_proba in y_probas]</p><p class="source-code">        </p><p class="source-code">    data[len(data.columns)] =y_preds</p><p class="source-code">        </p><p class="source-code">    result = data</p><p class="source-code">    result.to_csv("data/output.csv")</p></li>
				<li>We now need to package the job as a Docker image so we can run it in production easily:<p class="source-code">FROM continuumio/miniconda3</p><p class="source-code">WORKDIR /batch-scoring/</p><p class="source-code">RUN pip install mlflow==1.16.0</p><p class="source-code">RUN pip install pandas==1.2.4</p><p class="source-code">COPY batch_scoring.py   /batch-scoring/</p><p class="source-code">COPY MLproject          /batch-scoring/</p><p class="source-code">ENV MLFLOW_TRACKING_URI=http://localhost:5000</p><p class="source-code">ENTRYPOINT ["mlflow run . --no-conda"]</p></li>
				<li>Build your <a id="_idIndexMarker316"/>Docker image and tag it so you can reference it:<p class="source-code">docker build . -t pystock-inference-batch</p></li>
				<li>Run your Docker image by executing the following command:<p class="source-code">docker run -i pystock-inference-batch</p></li>
			</ol>
			<p>A Docker image in this case provides you with a mechanism to run your batch scoring job in any computing environment that supports Docker images in the cloud or on-premises.</p>
			<p>We will move now to illustrate the generation of a dockerized API inference environment for MLflow.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor145"/>Creating an API process for inference </h1>
			<p>The code required for this section is in the <strong class="source-inline">pystock-inference-api folder</strong>. The MLflow infrastructure <a id="_idIndexMarker317"/>is provided in the Docker image accompanying <a id="_idIndexMarker318"/>the code as shown in the following figure:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/image0026.jpg" alt="Figure 9.2 – The structure of the API job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – The structure of the API job</p>
			<p>Setting up an API system is quite easy by relying on the MLflow built-in REST API environment. We will rely on the artifact store on the local filesystem to test the APIs.</p>
			<p>With the following set of commands, which at its core consists of using the <strong class="source-inline">models serve</strong> command in the CLI, we can serve our models:</p>
			<p class="source-code">cd /gradflow/</p>
			<p class="source-code">export MLFLOW_TRACKING_URI=http://localhost:5000</p>
			<p class="source-code">mlflow models serve -m "models:/training-model-psystock/Production" -p 6000</p>
			<p>We next will package the preceding commands in a Docker image so it can be used on any environment for deployment. The steps to achieve this are the following:</p>
			<ol>
				<li value="1">Generate a Docker image specifying the work directory and the commands that need to be started as an <strong class="source-inline">entry point</strong>:<p class="source-code">FROM continuumio/miniconda3</p><p class="source-code">WORKDIR /batch-scoring/</p><p class="source-code">RUN pip install mlflow==1.16.0</p><p class="source-code">ENV MLFLOW_TRACKING_URI=http://localhost:5000</p><p class="source-code">ENTRYPOINT ["mlflow models serve -m "models:/training-model-psystock/Production" -p 6000"]</p></li>
				<li>Build your Docker image:<p class="source-code">docker build . -t pystock-inference-api</p></li>
				<li>Run your Docker image:<p class="source-code">docker run -i pystock-inference-api -p 6000:6000</p></li>
			</ol>
			<p>At this stage, you have <a id="_idIndexMarker319"/>dockerized the API infrastructure <a id="_idIndexMarker320"/>and can deploy it on a compute environment convenient to you.</p>
			<p>After having delved into the interaction of MLflow and a cloud-native deployment on the AWS platform, we will now look at a deployment that is independent of any provi<a id="_idTextAnchor146"/>der. </p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor147"/>Deploying your models for batch scoring in Kubernetes</h1>
			<p>We will use Kubernetes to deploy our batch scoring job. We will need to do some modifications <a id="_idIndexMarker321"/>to make it conform <a id="_idIndexMarker322"/>to the Docker format acceptable <a id="_idIndexMarker323"/>to the MLflow deployment in production through Kubernetes. The prerequisite of this section is that you have access to a Kubernetes cluster or can set up a local one. Guides for this can be found at <a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a> or <a href="https://minikube.sigs.k8s.io/docs/start/">https://minikube.sigs.k8s.io/docs/start/</a>. </p>
			<p>You will now execute the following steps to deploy your model from the registry in Kubernetes:</p>
			<ol>
				<li value="1">Prerequisite: Deploy and configure <strong class="source-inline">kubectl</strong> (<a href="https://kubernetes.io/docs/reference/kubectl/overview/">https://kubernetes.io/docs/reference/kubectl/overview/</a>) and link it to your Kubernetes cluster.</li>
				<li>Create a Kubernetes backend configuration file:<p class="source-code">{</p><p class="source-code">  "kube-context": "docker-for-desktop",</p><p class="source-code">  "repository-uri": "username/mlflow-kubernetes-example",</p><p class="source-code">  "kube-job-template-path": "/Users/username/path/to/kubernetes_job_template.yaml"</p><p class="source-code">}</p></li>
				<li>Load <a id="_idIndexMarker324"/>the input files and run the model:<p class="source-code">mlflow run . --backend kubernetes --backend-config kubernetes_config.json</p></li>
			</ol>
			<p>Having <a id="_idIndexMarker325"/>looked at deploying models <a id="_idIndexMarker326"/>in Kubernetes, we will now focus on deploying our model in a cloud-native ML platform.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor148"/>Making a cloud deployment with AWS SageMaker</h1>
			<p>In the last few years, services such as AWS SageMaker have been gaining ground as an engine to <a id="_idIndexMarker327"/>run ML workloads. MLflow <a id="_idIndexMarker328"/>provides integrations and easy-to-use commands to deploy your model into the SageMaker infrastructure. The execution of this section will take several minutes (5 to 10 minutes depending on your connection) due to the need to build large Docker images and push the images to the Docker Registry.</p>
			<p>The following is a list of some critical prerequisites for you to follow along:</p>
			<ul>
				<li>The AWS CLI configured locally with a default profile (for more details, you can look at <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html</a>).</li>
				<li>AWS access in the account to SageMaker and its dependencies.</li>
				<li>AWS access <a id="_idIndexMarker329"/>in the account to push to Amazon <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>) service.</li>
				<li>Your MLflow server needs to be running as mentioned in the first <em class="italic">Starting up a local model registry</em> section.</li>
			</ul>
			<p>To deploy <a id="_idIndexMarker330"/>the model from your local <a id="_idIndexMarker331"/>registry into AWS SageMaker, execute the following steps:</p>
			<ol>
				<li value="1">Build your <strong class="source-inline">mlflow-pyfunc</strong> image. This is the basic image that will be compatible with SageMaker.</li>
				<li>Build and push a container with an <strong class="source-inline">mlflow pyfunc</strong> message:<p class="source-code">mlflow sagemaker build-and-push-container</p><p>This command will build the MLflow default image and deploy it to the Amazon ECR container. </p><p>In order to confirm that this command was successful, you can check your ECR instance on the console:</p><div id="_idContainer099" class="IMG---Figure"><img src="image/image0036.jpg" alt="Figure 9.3 – SageMaker deployed image&#13;&#10;"/></div><p class="figure-caption">Figure 9.3 – SageMaker deployed image</p></li>
				<li>Run <a id="_idIndexMarker332"/>your model locally to test <a id="_idIndexMarker333"/>the SageMaker Docker image and export the tracking URI:<p class="source-code"><strong class="bold">$ export MLFLOW_TRACKING_URI=http://localhost:5000 </strong></p><p class="source-code"><strong class="bold">mlflow sagemaker run-local -m models:/training-model-psystock/Production  -p 7777</strong></p><p>This will basically load your model into the generic API for model inference and will allow you to run the model locally to test whether it works before deploying it in the cloud. It will run the model under port <strong class="source-inline">7777</strong>.</p><p>The output should look like the following excerpt and you should be able to test your model locally:</p><p class="source-code">Installing collected packages: mlflow</p><p class="source-code">  Attempting uninstall: mlflow</p><p class="source-code">    Found existing installation: mlflow 1.16.0</p><p class="source-code">    Uninstalling mlflow-1.16.0:</p><p class="source-code">      Successfully uninstalled mlflow-1.16.0</p><p class="source-code">Successfully installed mlflow-1.15.0</p><p class="source-code">pip 20.2.4 from /miniconda/lib/python3.8/site-packages/pip (python 3.8)</p><p class="source-code">Python 3.8.5</p><p class="source-code">1.15.0</p><p class="source-code">[2021-05-08 14:01:43 +0000] [354] [INFO] Starting gunicorn 20.1.0</p><p class="source-code">[2021-05-08 14:01:43 +0000] [354] [INFO] Listening at: http://127.0.0.1:8000 (354)</p><p>This will <a id="_idIndexMarker334"/>basically confirm <a id="_idIndexMarker335"/>that the image is working as expected and you should be able to run your API in SageMaker.</p></li>
				<li>Double-check your image through the AWS <strong class="source-inline">cli</strong>:<p class="source-code">aws ecr describe-images --repository-name mlflow-pyfunc </p><p>You should see your deployed image in the list of images and definitely ready to run.</p></li>
				<li>You need to configure a role in AWS as specified that allows SageMaker to create resources on your behalf (you can find more details at https://docs.databricks.com/administration-guide/cloud-configurations/aws/sagemaker.html#step-1-create-an-aws-iam-role-and-attach-sagemaker-permission-policy).</li>
				<li>Next, you need to export your region and roles to the <strong class="source-inline">$REGION</strong> and <strong class="source-inline">$ROLE</strong> environment variables with the following commands, specifying the actual values of your environment:<p class="source-code">export $REGION=your-aws-region</p><p class="source-code">export $ROLE=your sagemaker-enabled-role</p></li>
				<li>To deploy your model to SageMaker, run the following command:<p class="source-code">mlflow sagemaker deploy -a pystock-api -m models:/training-model-psystock/Production –region-name $REGION -- $ROLE</p><p>This command will load your model from your local registry into SageMaker as an internal representation and use the generated Docker image to serve the model in the AWS SageMaker infrastructure engine. It will take a few minutes to set up all the infrastructure. Upon success, you should see the following message:</p><p class="source-code">2021/05/08 21:09:12 INFO mlflow.sagemaker: The deployment operation completed successfully with message: "The SageMaker endpoint was created successfully."</p></li>
				<li>Verify <a id="_idIndexMarker336"/>your SageMaker endpoint:<p class="source-code">aws sagemaker list-endpoints</p><p>You can <a id="_idIndexMarker337"/>look at the following for an illustrative example of the type of output message:</p><p class="source-code">{</p><p class="source-code">    "Endpoints": [</p><p class="source-code">        {</p><p class="source-code">            "EndpointName": "pystock-api",</p><p class="source-code">            "EndpointArn": "arn:aws:sagemaker:eu-west-1:123456789:endpoint/pystock-api",</p><p class="source-code">            "CreationTime": "2021-05-08T21:01:13.130000+02:00",</p><p class="source-code">            "LastModifiedTime": "2021-05-08T21:09:08.947000+02:00",</p><p class="source-code">            "EndpointStatus": "InService"</p><p class="source-code">        }</p><p class="source-code">    ]</p><p class="source-code">}</p></li>
				<li>Next we <a id="_idIndexMarker338"/>need to consume <a id="_idIndexMarker339"/>our API with a simple script that basically the features, invokes the SageMaker endpoint using the Amazon Boto3 client, and prints the probablity of the market pricesgiven the feature vector:<p class="source-code">import pandas</p><p class="source-code">import boto3</p><p class="source-code">features = pd.DataFrame([[1,0,1,1,0,1,0,1,0,1,0,1,0,1]])</p><p class="source-code">payload = features.to_json(orient="split")</p><p class="source-code">result  = runtime.invoke_endpoint(</p><p class="source-code">            EndpointName='pystock-api', Body=payload, </p><p class="source-code">            ContentType='application/json')</p><p class="source-code">preds = result['Body'].read().decode("ascii")</p><p class="source-code">print(preds)</p><p>After running this previous script you should see the following output:</p><p class="source-code">'[0.04279635474085808]</p></li>
				<li>Explore the SageMaker endpoint interface. In its monitoring component, you can look at different metrics related to your deployment environment and model as shown in <em class="italic">Figure 9.4</em>:<div id="_idContainer100" class="IMG---Figure"><img src="image/image0046.jpg" alt="Figure 9.4 – SageMaker inference instance metrics&#13;&#10;"/></div><p class="figure-caption">Figure 9.4 – SageMaker inference instance metrics</p></li>
				<li>You can <a id="_idIndexMarker340"/>now easily tear <a id="_idIndexMarker341"/>down your deployed model, when in need to deploy the model or phase it out. All associated resources will be torn down:<p class="source-code">mlflow sagemaker delete -a pystock-api --region-name $REGION</p><p>Upon deletion, you should see a message similar to the one in the following excerpt:</p><p class="source-code">2021/05/08 23:49:46 INFO mlflow.sagemaker: The deletion operation completed successfully with message: "The SageMaker endpoint was deleted successfully."</p><p class="source-code">2021/05/08 23:49:46 INFO mlflow.sagemaker: Cleaning up unused resources...</p><p class="source-code">2021/05/08 23:49:47 INFO mlflow.sagemaker: Deleted associated endpoint configuration with arn: arn:aws:sagemaker:eu-west-1:123456789:endpoint-config/pystock-api-config-v-hznm3ttxwx-g8uavbzia</p><p class="source-code">2021/05/08 23:49:48 INFO mlflow.sagemaker: Deleted associated model with arn: arn:aws:sagemaker:eu-west-1:123456789:model/pystock-api-model-4nly3634reqomejx1owtdg</p></li>
			</ol>
			<p>With this <a id="_idIndexMarker342"/>section, we concluded <a id="_idIndexMarker343"/>the description of the features related to deploying an ML model with MLflow in production in different environments from your local machine, including Docker and <strong class="source-inline">docker-compose</strong>, public clouds, and the very flexible approach of using AWS SageMaker. </p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor149"/>Summary</h1>
			<p>In this chapter, we focused on production deployments of ML models, the concepts behind this, and the different features available for deploying in multiple environments with MLflow. </p>
			<p>We explained how to prepare Docker images ready for deployment. We also clarified how to interact with Kubernetes and AWS SageMaker to deploy models.</p>
			<p>In the next chapter and the following sections of the book, we will focus on using tools to help scale out our MLflow workloads to improve the performance of our models' infrastructure.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor150"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the documentation at the following links: </p>
			<ul>
				<li><a href="https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html">https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html</a></li>
				<li><a href="https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/</a></li>
			</ul>
		</div>
	</body></html>