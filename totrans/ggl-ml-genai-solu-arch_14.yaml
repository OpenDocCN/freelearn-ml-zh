- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias, Explainability, Fairness, and Lineage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned all of the steps required to build and deploy models
    in Google Cloud and to automate the entire **machine learning** (**ML**) model
    development life cycle, it’s time to dive into yet more advanced concepts that
    are fundamental to developing and maintaining high-quality models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to our models providing predictions that are as accurate as possible
    for a given use case, we need to ensure that the predictions provided by our models
    are as fair as possible and that they do not exhibit bias or prejudice against
    any individuals or demographic groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics of bias, fairness, and explainability are at the forefront of ML
    research today. This chapter discusses these concepts in detail and explains how
    to effectively incorporate these concepts into our ML workloads. Specifically,
    we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of bias, fairness, and explainability in **artificial** **intelligence**
    (**AI**)/ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect and mitigate bias in datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using explainability to understand ML models and reduce bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of lineage tracking in ML model development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining and describing the relevant concepts.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of bias, explainability, and fairness in AI/ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the terms “bias,” “explainability,” and “fairness” are not specific to
    ML, in this section, we will explore these terms as they apply to the development
    and use of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias in AI/ML refers to tendencies or prejudices in data and algorithms that
    can lead to unfair outcomes. One of the most common sources of bias in ML model
    development is when biases exist in the training data; for example, when the data
    points in the training data do not fairly represent the reality or the population
    that the model’s predictions will serve, which we refer to as **data bias**. For
    example, using a dataset in which the data points predominantly represent only
    one demographic group to train a model can result in poorer performance when that
    model is required to make predictions based on data points that represent other
    demographic groups. More specifically, this is an example of something called
    **sampling bias**. To tie this to a real-world scenario, let’s imagine that we’re
    training a model to perform facial recognition. If we train the model with images
    of people mainly from one specific demographic group, the model may not perform
    facial recognition tasks well when it is later presented with images of people
    from other demographic groups. There are a number of different ways in which we
    may encounter bias during the development and use of ML models, which we outline
    in the following paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: Collection or measurement bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A major cause of data bias is the manner in which the data is collected or measured.
    Sometimes, bias can occur due to the way we measure or frame a problem, which
    might not correctly represent the underlying concept that we’re trying to measure.
    For example, let’s imagine that we own a small company and we want to expand our
    product offerings to attract new customers. We may decide to use a survey to gather
    data to train ML models to predict what kinds of new products we should offer,
    based on how popular those new products are likely to be. There are a number of
    ways in which we could distribute this survey, such as via email or via physical
    mail. In the case of email, we might decide to send the survey to all of our current
    customers, because it’s likely that we would already have their email addresses
    in our database. In the case of physical email, we may decide to send it out to
    everybody in the same postal code area, city, state, or country in which our company
    is physically located.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, both of those methods may unexpectedly introduce bias into the
    datasets used to train our models. For example, perhaps we traditionally happened
    to appeal to only a specific type of customer in the past. If we use our current
    customer base as the survey group, we may not get good data points for products
    that would appeal to new customer demographics. Similarly, if we send out a survey
    to everybody in a specific geographical area, such as a postal code, city, state,
    or even country, the people living in that area may not represent diverse demographic
    groups, which could inadvertently introduce biases into the resulting dataset.
    This phenomenon is sometimes referred to as **response bias**, in which the people
    providing the required information may be biased in some way, and may therefore
    provide biased responses. To make this even more complicated, the way in which
    we phrase questions in the survey could accidentally bias the respondents.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we also need to be aware of **observer bias** or **subjectivity**,
    in which the people running the survey may be biased in some way. In fact, our
    example of deciding to email our current customers, or to send physical mail to
    people in a specific area, is a type of observer bias, in which we decided to
    survey specific groups of people based on factors such as currently available
    data (for example, our customers’ email addresses) or proximity to our company’s
    location.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-existing bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are generally biases that already exist in society. Such biases can stem
    from factors such as cultural, historical, and societal norms, consisting of ingrained
    beliefs and practices that shape the way individuals view the world.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common ways to train a model is to use historical data that
    has been recorded. However, data from the past might be tainted with biases that
    were inherent in those times — I think we can all agree that the societal norms
    of the 1950s or even the 1990s are quite different from today’s standards, especially
    in terms of fairness across different demographic groups. If AI/ML models are
    trained on such data without correction, they will likely perpetuate those biases.
    It’s important to understand that unaddressed pre-existing biases can not only
    perpetuate but sometimes even amplify stereotypes, leading to unfair or discriminatory
    outcomes. For example, a job recommendation system might be biased toward recommending
    higher-paying jobs to men if it is trained on historical data that exhibits this
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: Another common type of bias in this context is **confirmation bias**, in which
    people may subconsciously tend to select and interpret data in ways that confirm
    their pre-existing beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: If data collection processes do not check for pre-existing biases, they can
    lead to data that’s not truly representative of the reality or population it’s
    supposed to depict. Models trained on such data may learn and replicate those
    biases in their predictions or actions.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This refers to biases that are introduced by the design of the algorithms themselves,
    not just the data they are trained on. This is an increasingly important topic,
    considering that algorithms are used to implement an ever-expanding plethora of
    important decisions in modern society, such as credit approvals, recruitment processes,
    and medical diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic bias can be more subtle and difficult to detect. For example, such
    bias could simply stem from the algorithm developers themselves. If the team developing
    the algorithm lacks diversity, it might not foresee or recognize potential biases
    in how the algorithm is implemented. We also need to recognize the possibility
    of accidentally developing biased feedback loops in ML systems. For example, in
    systems where algorithms are continuously trained with new data, biased outcomes
    can reinforce the input biases, and over time, this can lead to increasingly skewed
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Just like the other types of biases discussed previously in this section, unaddressed
    algorithmic bias can lead to the perpetuation of stereotypes, misinformation,
    and unjust practices.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that in this section, we have discussed only some of
    the most common types of biases that can affect ML model development. This is
    an active area of research, and additional types of bias exist that we may not
    be explicitly aware of as we develop and use ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Figuratively speaking, the concept of bias represents one side of a coin, and
    on the other side of that coin is the concept of fairness, which we explore in
    more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fairness in AI and ML refers to the practice of developing algorithms in a way
    that prevents discrimination and promotes equity. While fairness is a rather straightforward
    concept to define, such as treating all people equally, in practice, it can be
    difficult to monitor and uphold. A little later in this chapter, we will look
    at mechanisms to monitor and enhance the fairness of ML models, but let’s first
    describe the concept in a bit more detail in the context of ML. Just as in the
    previous section related to bias, we will discuss a few different ways in which
    we can define and measure fairness with regard to ML models, which we outline
    in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Representational fairness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the first line of defense against bias in ML model development, especially
    with regard to data bias. Representational fairness aims to ensure that the data
    used in model training and validation contains a fair representation of each of
    the various demographic groups that the resulting model’s predictions will affect;
    for example, ensuring that genders and ethnic groups are represented fairly in
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Procedural fairness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This involves ensuring that the processes for data collection, data handling,
    and algorithm development are equitable and do not favor any particular group.
    In [*Chapter 13*](B18143_13.xhtml#_idTextAnchor328), we will discuss the concept
    of governance in great detail, especially in relation to data governance, and
    the important role it plays in helping to ensure that potential biases are addressed
    or mitigated in datasets. Many enterprises employ entire teams or organizations
    dedicated to outlining and upholding governance requirements. In the case of procedural
    fairness, the focus is not so much on the contents of the data, but rather on
    the fairness of the processes that lead to the development, deployment, and operation
    of ML models. Some key components of procedural fairness are represented by the
    **TAIC** framework, which consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transparency**, such as clear documentation of the methods, data sources,
    and decisions made throughout the model’s life cycle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability**, meaning that there should be clarity on who is responsible
    for various stages of model development, deployment, and monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impartiality**, meaning that procedures should not favor or prejudice any
    individuals or groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency**, meaning that the procedures used to develop a model should
    be reproducible in a consistent manner'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI provides mechanisms that help to audit procedural fairness,
    which we will explore in more detail in later sections of this chapter, as well
    as in the practical activities that accompany this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Outcome fairness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While representational fairness mainly focuses on the inputs used to train ML
    models (that is, the contents of the training data) and procedural fairness focuses
    on the procedures used to develop and deploy ML models, outcome fairness, as the
    name suggests, focuses on the results produced by ML models. Specifically, it
    aims to ensure that the outcomes produced by a model are equitable and do not
    disproportionately benefit or harm any group. Of course, one way to measure this
    is to monitor the predictions made by a model to determine whether any bias appears
    to be present. In this chapter, we will explore some mechanisms and metrics that
    can be used for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned when discussing bias in the previous section, the concepts of
    bias and fairness in ML are still very active and evolving areas of research.
    This is why fairness can be a complex topic in practice, and it’s important to
    understand that achieving one type of fairness can sometimes lead to a violation
    of another type. Moreover, what is considered “fair” can be context-dependent
    and might vary between communities, as well as over time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced the concepts that define the two-sided coin of bias
    and fairness, the next step will be to introduce a topic that is inherently linked
    to those two concepts, which is referred to as **explainability**.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explainability in ML focuses on the ability of a human to understand and explain
    how the outputs of an ML model were produced. This is often also referred to as
    the **interpretability** of a model. These concepts continue to grow in importance
    as ML models become increasingly complex over time. For example, it’s pretty easy
    to interpret and explain how the outputs of a simple linear regression model were
    produced, because we have well-defined mathematical formulae that describe that
    process, such as *y = a + bx*, where *y* (the output) is a linear transformation
    of *x* (the input). Similarly, for decision tree models, we can logically trace
    the decision path through the tree. However, when we’re dealing with large **neural
    networks** (**NNs**) containing millions or even billions of parameters, and incorporating
    different types of architectures and activation functions, we need some additional
    tools to help us understand how those models are making their decisions, which
    we will explore in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed procedural fairness in the previous section, we talked about
    the importance of transparency. Explainability links closely to the concept of
    transparency, as it seeks to ensure that people can understand the process by
    which predictions are produced from a given model. For example, if we own a bank
    and one of our models is responsible for granting or declining credit applications,
    we want to thoroughly understand how it makes those decisions.
  prefs: []
  type: TYPE_NORMAL
- en: We can talk about explainability in terms of **global explainability**, in which
    we try to understand the general logic the model applies to make predictions across
    all input instances, or **local explainability**, which involves explaining why
    a model made a specific decision for a particular instance (for example, understanding
    why a specific customer’s credit application was declined).
  prefs: []
  type: TYPE_NORMAL
- en: Overall, explainability is critical for building trust in AI systems, complying
    with legal requirements, and ensuring that humans can intervene effectively in
    decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced and explained the overall concepts, let’s start diving
    in deeper. Our first deep dive in this chapter will be on the topic of bias in
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How to detect and mitigate bias in datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore how to detect bias in our datasets, and there are
    various tools and methods we can use for this purpose. In fact, we’ve already
    covered some of them in previous chapters of this book, such as data exploration
    and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration and visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we explore our datasets using visualization, for example, charts such as
    histograms and scatter plots can help visualize disparities in data distribution
    for different demographic groups. Similarly, we’ve already explored descriptive
    statistics such as mean, median, mode, and variance to understand the contents
    of our datasets. If there are significant disparities in these statistics between
    subgroups, it may suggest the presence of bias in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Specific tools for detecting dependencies between features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We also want to test for potential correlations or dependence between features
    in our dataset in order to understand whether the values of some features are
    significantly influenced by the values of others. While this is something that
    we generally want to do as part of our regular data exploration and feature engineering
    anyway (that is, we need to understand underlying patterns in our data as much
    as possible), it becomes even more important in the context of bias and fairness,
    especially with regard to potential links between the target variable and protected
    attributes such as gender or ethnicity.
  prefs: []
  type: TYPE_NORMAL
- en: There are specific tools for detecting dependencies among features, such as
    **Pearson’s Correlation Coefficient**, which measures the linear relationship
    between numeric variables, or the **Chi-Squared Test for Independence**, which
    can be used to determine if there is a significant association between two categorical
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanisms incorporating model prediction results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the aforementioned methods, we can use mechanisms that go beyond
    just examining the dataset by also taking a model’s outputs into account, which
    can then help us to link any observed biases back to the training data and process.
    For example, we can use **disparate impact analysis** (**DIA**) to compare the
    ratio of favorable outcomes for different groups and measure whether any specific
    groups tend to get more favorable outcomes than any others. Let’s take a look
    at DIA in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: DIA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the case of DIA, we generally identify what is referred to as a **privileged
    group** and an **unprivileged group** (also referred to as a **protected group**),
    based on some protected characteristic such as gender or race, and we compare
    the outputs of a given model for both of those groups to try to determine whether
    the model appears to bias in either direction (positively or negatively) with
    regard to those groups. We measure this disparity using a metric referred to as
    the **disparate impact ratio** (**DIR)**, which is defined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>D</mi><mi>I</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>F</mi><mi>U</mi><mi>P</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/12.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>U</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/13.png)
    represents the probability of a favorable outcome for the unprivileged group,
    and P(FP) represents the probability of a favorable outcome for the privileged
    group.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DIR value can be interpreted in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: When DIR is equal to 1, it indicates perfect fairness, where both groups receive
    favorable outcomes at the same rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When DIR is greater than 1, it indicates that the unprivileged group is more
    likely to receive favorable outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When DIR is less than 1, it indicates that the unprivileged group is less likely
    to receive favorable outcomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A commonly accepted threshold is a DIR value of between 0.8 and 1.25; values
    outside this range often indicate potential **disparate** **impact** (**DI**).
  prefs: []
  type: TYPE_NORMAL
- en: Note that DIA can be performed directly on a dataset by using the target feature,
    or it can use a combination of the input features and a model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into the concept of explainability in ML in
    more detail, and explore how we can use explainability frameworks to detect and
    address bias. We will also expand on the concepts covered in this section, and
    discuss how they relate to explainability and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Using explainability to understand ML models and reduce bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the concept of explainability at a high level in the previous
    section. This section dives further into this topic, introducing tools that can
    be used to gain insights into how ML models are working at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability techniques, methods, and tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin by exploring some popular techniques, methods, and tools that we
    can use for implementing explainability in ML, which we describe in the following
    subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Performing data exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now, it should hopefully be clear that understanding the data used to train
    our models is one of the first steps in explaining how the model makes decisions,
    and it is also one of the first lines of defense to identify and combat potential
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: In the practical activities associated with this chapter, we explore the “Adult
    Census Income” dataset ([https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult)),
    which is known to contain imbalances with regard to race and gender. The dataset
    comprises information pertaining to people, such as their race, gender, education
    they’ve received, and their current annual income, expressed as either “<=50K”
    (less than or equal to $50,000 per year) or “>50K” (more than $50,000 per year),
    which creates a binary classification use case when the income is used as the
    target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'When exploring this data, we can ask questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Are feature values, such as race and gender, represented somewhat evenly or
    unevenly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any correlations between some feature values and the income earned
    by that person?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can easily see imbalances in the dataset by using data visualization techniques.
    Let’s take a look at some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Income distribution by gender
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following code will show us the distributions of each gender in the dataset,
    with regard to the two different income categories (that is, people who earn less
    than or equal to $50,000 per year, or people who earn more than $50,000 per year).
    You can open the Jupyter notebook that accompanies this chapter if you’d like
    to follow along with the code examples that we will review. We can again use the
    same Vertex AI Workbench-Notebook Instance that we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-12` directory
    and open the `bias-explainability.ipynb` notebook. You can choose **Python (Local)**
    as the kernel. Again, you can run each cell in the notebook by selecting the cell
    and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant code,
    the notebook contains markdown text that describes what the code is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display a graph like the one shown in *Figure 12**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1: Income distribution by gender](img/B18143_12_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Income distribution by gender'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 12**.1*, we can see that, overall, more people in the dataset earn
    more than $50,000 per year than those who do not. We can also see that the number
    of males in each group far exceeds the number of females in each group. This also
    tells us the entire dataset consists of more data points related to men than women.
  prefs: []
  type: TYPE_NORMAL
- en: Income distribution by race
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following code will show us the distributions of each race in the dataset,
    with regard to income:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display a graph like the one shown in *Figure 12**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2: Income distribution by race](img/B18143_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Income distribution by race'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 12**.2*, for both the “<=50K” category and the “>50K” category, we
    see that the data contains many more data points for White people than any other
    race. This can be seen as a type of bias in the dataset. As we introduced earlier
    in this chapter, this bias may exist for multiple potential reasons, such as bias
    in the collection of the data, or it may happen due to other factors such as geographic
    location. This particular dataset represents the population of a specific area,
    which may somewhat explain its apparent bias toward a particular race. For example,
    if the data were collected in Asia, then it would contain many more data points
    for Asian people than any other race, or if it were collected in central Africa,
    then it would contain many more data points for Black people than any other race.
    It’s important to note any imbalances in the data and determine how they may affect
    the training of an ML model and who that ML model is intended to serve. Generally,
    if features in the dataset have much higher numbers of instances of a specific
    value, then an ML model’s predictions will likely reflect that in some way.
  prefs: []
  type: TYPE_NORMAL
- en: In the Jupyter notebook that accompanies this chapter, we also assess other
    types of distributions in the data, such as occupational distribution by gender
    and educational distribution by race. I encourage you to use the Jupyter notebook
    to explore the data in more detail. For now, let’s move on and look at implementing
    DIA in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DIA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is the code we use to implement DIA in our Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code first creates a pivot table of the `adult_data` DataFrame, grouped
    by gender and income, with the count of people in each group as the value. Then,
    it adds a new column to the pivot table called `rate`, which is the proportion
    of people in each group who earn more than $50,000\. Finally, it calculates the
    DI by dividing the rate for females by the rate for males.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very simple example of implementing DIA on a dataset that is known
    to contain gender imbalances. DIA can be much more complex and may require some
    domain expertise to implement effectively, depending on the contents of the dataset
    and the intended function of an ML model trained on that data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss the topic of feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature importance evaluates the impact that each feature has on the predictions
    made by a model. To explore this concept, let’s imagine we have a dataset that
    contains information about people, and the features in the dataset include height,
    age, eye color, and whether or not they like coffee. We want to use this data
    to train a model to predict the likelihood of each person being a successful basketball
    player. Do you think any of the input features in our dataset may be more important
    than any others in terms of influencing the outcome? Is it likely that height
    would be more important or less important than eye color in determining whether
    a person is likely to be a successful basketball player? Is age an important factor?
    In this case, we’re describing the concept of feature importance in a simple example.
    In reality, we may be dealing with datasets that contain thousands of features,
    and those features may not always represent easily interpretable concepts such
    as height, age, and eye color. For this reason, we can use tooling to help us
    to get these kinds of insights. The following subsections describe the tools we
    can use for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance tools built into popular ML libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 12.3: Feature importance](img/B18143_12_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Feature importance'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 12**.3*, we can see that features such as `age`, `hours-per-week`,
    and `capital-gain` seem to be pretty important features with regard to predicting
    income. Does the influence of those features seem intuitive to you?
  prefs: []
  type: TYPE_NORMAL
- en: Note that feature importance does not imply causality. Just because a feature
    is deemed important doesn’t mean it causes the target variable to change; only
    that there’s an association.
  prefs: []
  type: TYPE_NORMAL
- en: While we’re specifically using the `feature_importances_` attribute in scikit-learn
    in our Jupyter notebook, other popular ML libraries also provide similar mechanisms.
    For example, TensorFlow’s boosted trees (`tf.estimator.BoostedTreesClassifier`
    or `tf.estimator.BoostedTreesRegressor`) also provide `feature_importances_` as
    a property to get the importance of each feature. Similarly, LightGBM and CatBoost
    provide `feature_importances_` and `get_feature_importance()`, respectively, and
    XGBoost provides the `plot_importance()` function for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: While `feature_importances_`, and similar mechanisms from other ML libraries,
    can be very useful, there are yet more advanced tools that we can use in order
    to assess feature importance, which I will describe next.
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence plots
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Partial dependence plots** (**PDPs**) are graphical visualizations used to
    understand the relationship between an input feature (or a set of input features)
    and a model’s predicted outcome. With PDPs, we change the value of just one feature,
    while keeping the values of all other features constant, in order to determine
    how the different values of that particular feature impact the prediction. PDPs
    can also be used with more than one input feature at a time, which can reveal
    interactions among multiple features. There’s also an extended form of PDPs called
    **individual conditional expectation (ICE) plots**. While PDPs show the average
    effect of a feature on predictions, ICE plots display the effect of a feature
    on predictions for individual instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With PDPs, for every unique value of the feature of interest, the following
    high-level steps are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the feature of interest to that value for every instance in the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions using the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the averaged predictions against the unique values of the feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code uses the `PartialDependenceDisplay` attribute from scikit-learn
    to create and display a PDP graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will produce a graph similar to the one shown in *Figure 12**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4: PDP](img/B18143_12_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: PDP'
  prefs: []
  type: TYPE_NORMAL
- en: In the PDP shown in *Figure 12**.4*, the model appears to predict that people
    tend to start increasingly earning more money from the age of about 20 onward
    until they reach the age of around 60, after which their income begins to decrease.
    This is somewhat intuitive, considering that 20 years of age is considered early
    adulthood, and people often tend to retire in their 60s. Similarly, the model
    predicts that working more hours per week may result in higher income.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we move on to discuss more advanced feature importance and explanation
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Local Interpretable Model-agnostic Explanations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As I mentioned earlier in this chapter, some models are naturally more easily
    interpretable and understandable (and therefore explainable) than others. Examples
    provided were linear regression models and decision trees, versus large NNs with
    thousands, millions, or even billions of parameters (perhaps soon to be trillions!).
    **local interpretable model-agnostic explanations** (**LIME**) takes advantage
    of this fact, by training a simpler, **surrogate** model that can be more easily
    explained than the target model. The idea is that even if the overall model is
    complex and non-linear, it can be approximated well by a simpler, interpretable
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'LIME’s inner workings are quite complex, and I would recommend reading the
    original paper (*arXiv:1602.04938*) if you want to delve into that level of detail.
    Such algorithmic details are generally not required for the activities of a solutions
    architect, and you would mainly need to understand what LIME is used for without
    needing to dive into the academic details of its inner workings. Moving on, then,
    the following code provides an example of how to use LIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will produce visualizations similar to those shown in *Figure 12**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5: LIME outputs](img/B18143_12_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: LIME outputs'
  prefs: []
  type: TYPE_NORMAL
- en: At the top of *Figure 12**.5*, we see the model’s prediction for the instance
    that was used as input. In this case, since it’s a binary classification problem,
    it shows the class that the model predicted, along with the probability score
    associated with that prediction. We also see a horizontal bar chart representing
    the influence of various features on the prediction. Each bar represents a feature
    and its impact. The length of the bar indicates the significance of the influence,
    and its direction (left or right) indicates the direction of the influence (for
    example, toward the “<=50K” or “>50K” class). Each bar is labeled with the feature
    name and a small descriptor, which indicates how that feature was quantified for
    the specific instance being interpreted. This provides a clear indication of how
    the feature value for that specific instance influenced the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to highlight that LIME’s explanations are local. They are specifically
    tailored to the instance in question and show how the model made its prediction
    for that one instance, not a general rule for all data. Next, we explore a mechanism
    that can help with both local and global model interpretations and is perhaps
    one of the most popular feature importance and explanation mechanisms in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: SHapley Additive exPlanations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Shapley Additive exPlanations** (**SHAP**) helps to explain ML model predictions
    using a concept called **Shapley values**. These values come from a field of mathematics
    referred to as **game theory** or, more specifically, **cooperative game theory**,
    and they were originally introduced in the 1950s by Lloyd Shapley. The study of
    game theory looks at competitive situations (called “games”) where the results
    of one player’s choices depend on what other players do. Cooperative game theory
    is a sub-branch that looks at games where players can make alliances or **coalitions**
    with other players and work together as a team to benefit the coalition as a whole,
    rather than just looking out for their own interests.'
  prefs: []
  type: TYPE_NORMAL
- en: Diving in further, let’s imagine a game in which the winner will receive a payout
    of $100\. People can either play the game individually, in which case an individual
    would simply receive $100 if they win, or they can form a team and work together
    to try to win the $100 payout. If three people form a team and they win the payout,
    in reality, it’s likely they would just split the payout into three equal parts.
    However, what if the payout should be **fairly** divided based on the contribution
    of each person? This is where Shapley values come into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shapley values represent the **average marginal contribution** of each player
    over all possible coalitions they could enter, as well as all possible orders
    in which they could enter each coalition. Again, striking a balance of how much
    detail to cover on this subject, I will not include the complex mathematical details
    here but would recommend reading the original paper if you would like to dive
    into those details, the formal reference for which is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Shapley, L.S. (1953). A Value for n-Person Games. In “Contributions to the
    Theory of Games volume II”, H.W. Kuhn and A.W. Tucker (eds.), Princeton* *University
    Press.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will focus on how Shapley values are used in the context of ML model
    explainability. We begin with the concept of an **average prediction**, which
    represents the average of all predictions our model makes over our entire dataset.
    For regression models, this is simply the mean of all predicted outputs. For classification
    models, it is the average predicted probability for a given class over the entire
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When computing Shapley values for a particular instance (that is, a row of input
    data), the contributions of each feature are measured with respect to how they
    move the prediction for that instance away from the average prediction. The Shapley
    value for a feature then captures its average marginal contribution over all possible
    combinations of features. For example, consider a binary classification model
    that predicts whether a bank loan will be defaulted on. If, on average, the model
    predicts a 5% probability of default over the entire dataset, this 5% would be
    the “average prediction.” Now, for a particular loan application, if the model
    predicts a 20% probability of default, Shapley values help attribute this 15%
    deviation from the average to each feature in the input (for example, the applicant’s
    income, employment status, credit score, and so on). Each feature’s Shapley value
    will indicate how much, on average, that feature contributes to the deviation
    of the prediction from the average prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code provides an example of how to get feature importance insights
    using the SHAP library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code, a `TreeExplainer` object from the SHAP library is being created.
    This specific explainer is optimized for tree-based models, such as decision trees,
    random forests, and gradient boosted trees. The `clf` instance that we pass to
    it is a tree-based model that we’ve trained. Once the explainer is created, we
    use the `.shap_values()` method to compute the SHAP values for each sample in
    our `X_test` dataset. We then visualize the average impact of each feature on
    the model’s predictions by using a bar plot (the longer the bar, the greater the
    feature’s importance). The code will produce a graph similar to the one shown
    in *Figure 12**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6: SHAP outputs](img/B18143_12_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: SHAP outputs'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 12**.6*, perhaps surprisingly, a person’s marital status
    appears to have the most impact on the model’s output. Considering that the SHAP
    value for a feature is the average contribution of that feature value to every
    possible prediction (averaged over all instances), it takes into account intricate
    interactions with other features, as well as the impact of the feature itself.
  prefs: []
  type: TYPE_NORMAL
- en: While we can import tools such as SHAP into our notebooks, we can also use Vertex
    AI APIs to get explanations directly from models hosted in Vertex AI. The next
    section describes how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting explanations from a deployed model in Vertex AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Conveniently, Vertex AI provides APIs and an SDK that we can use to get explanations
    from our models. In the Jupyter notebook that accompanies this chapter, we use
    the `projects.locations.endpoints.explain` API to get explanations from the model
    that we deployed in our MLOps pipeline in the previous chapter. The following
    is a snippet of the code we use to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will produce an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The fields in the response can be interpreted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`baseline_output_value`: This is the model’s output value for the baseline
    instance. A baseline is a reference point (such as an average or neutral instance)
    against which the prediction for our instance of interest is compared. The difference
    in the model’s output between the instance of interest and the baseline helps
    us understand the contributions of each feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance_output_value`: This is the model’s output value for the instance
    we passed in for explanation. In the context of a binary classifier, this can
    be interpreted as the probability of the instance belonging to the positive class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_attributions_dict`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''dense_input''`: This is the name of the input tensor to the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The list of numbers represents the importance or attribution of each corresponding
    feature in the input for the given prediction. The length of this list matches
    the number of features in our model’s input:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each number represents the marginal contribution of that feature toward the
    model’s prediction for the specific instance we’re explaining, relative to the
    baseline. In other words, how much did this feature move the prediction from the
    average/baseline prediction?
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Positive values indicate that the feature pushed the model’s output in the positive
    class’s direction. For binary classification, this usually means it made the model
    more confident in classifying the instance as the positive class.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative values indicate that the feature pushed the model’s output in the negative
    class’s direction.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero or close to zero suggests that the feature didn’t have a significant impact
    on the prediction for this particular instance.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`approximation_error`: This is the error in the approximation used to compute
    attribution values. Explanation methods often use approximations to compute attributions.
    The approximation error gives an idea of the confidence we can have in the attribution
    values (a smaller error typically indicates more reliable attributions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_name`: This is the name of the model’s output tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Congratulations! You have successfully retrieved an explanation for an input
    sent to a model hosted in Vertex AI. To dive into more detail on Vertex Explainable
    AI, you can reference its documentation at the following link: [https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview).'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve talked about getting and using explanations to understand our
    models. What if we found that some of our model’s predictions were unfair? What
    kinds of actions could we take to counteract that? One type of explanation mechanism
    we can use for this purpose is **counterfactual explanations**, which we explore
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Counterfactual explanations revolve around the question, “What would need to
    change in my input data to alter the decision of a predictive model?” They describe
    a hypothetical alternative to an observed outcome that would have occurred if
    certain conditions were met. This could be a minimal change in input features
    that would change the prediction to a specified output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our loan approval as an example, suppose an applicant, John,
    is denied a loan based on features such as his income, credit score, and employment
    history. A counterfactual explanation might tell John: “If your income was $10,000
    higher, you would have been approved for the loan.”'
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual explanations are important for many reasons. They help individuals
    affected by a model’s prediction to understand why a decision was made. They help
    data scientists understand how to augment their models based on various criteria,
    and they are also important for regulatory compliance reasons.
  prefs: []
  type: TYPE_NORMAL
- en: To find a counterfactual, we need to define a distance measure in the feature
    space. The goal is often to find the counterfactual instance that’s closest to
    the original instance but results in a different prediction. This is often framed
    as an optimization problem, where the objective is to minimize the distance between
    the original instance and its counterfactual, while still resulting in a different
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that for some models, especially **deep NNs** (**DNNs**), finding
    counterfactuals can be computationally challenging. It’s also important to note
    that counterfactuals might suggest changes that are impossible or very hard to
    achieve in real life, so their real-world feasibility needs to be evaluated. In
    the Jupyter notebook that accompanies this chapter, we perform some simple counterfactual
    processing. Bear in mind that counterfactuals are a highly complex topic in a
    quickly evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at some additional mechanisms for reducing bias.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing bias and enhancing fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss proactive steps we can use to reduce bias and enhance
    fairness in our datasets and ML models in each phase of the model development
    life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with anything in the field of data science, the data is usually a good place
    to start. If available, we should gather more data and ensure that the data is
    as balanced as possible. During data preprocessing, we can also mitigate bias
    by using resampling techniques to adjust the representation of underrepresented
    groups in the training data, either by oversampling minority groups or undersampling
    majority groups. During feature engineering, we can create or modify features
    to reduce their potential discriminatory effect.
  prefs: []
  type: TYPE_NORMAL
- en: During model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During the model training process, we could introduce **fairness constraints**
    to ensure equal opportunity by making sure both protected and non-protected groups
    have equal positive and negative rates, for example. There are numerous fairness
    metrics and constraints for this purpose, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Demographic parity** or **statistical parity**, which requires that the probability
    of a positive outcome should be the same across the different groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equal opportunity**, which mandates equality in true positive rates across
    different groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equalized odds**, which extends equal opportunity by requiring both true
    positive rates and false positive rates to be equal across groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Treatment equality**, which requires the ratio of false negatives to false
    positives to be equal across groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness constraints can be implemented as a type of regularization during model
    training. There are even some fairness-aware algorithms we could use that are
    explicitly designed to handle fairness concerns, such as **Fair k-Means**.
  prefs: []
  type: TYPE_NORMAL
- en: Postprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are also some steps we can take during postprocessing, such as adjusting
    decision thresholds for different groups to ensure fairness metrics such as equal
    opportunity or demographic parity, and we could also adjust model predictions
    to ensure they are fair across groups.
  prefs: []
  type: TYPE_NORMAL
- en: It is, of course, also important that we continuously monitor for fairness concerns
    in real-world predictions and retrain models as necessary. In critical decision-making
    scenarios, we could consider having a human in the loop to audit or override model
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Note that fairness enhancement methods might lead to a trade-off with model
    accuracy. The challenge is often to find a balance between fairness and accuracy
    that’s acceptable for the given application. It’s also crucial to understand which
    fairness metric is most relevant to your specific problem, as different fairness
    metrics can sometimes be in conflict with each other.
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up this section on explainability and fairness, let’s take a brief look
    at some other libraries we can use for assessing and implementing these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Additional libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, there is an ever-growing list of libraries being developed for
    the purpose of assessing and promoting explainability and fairness. This section
    describes examples of such libraries.
  prefs: []
  type: TYPE_NORMAL
- en: What-if Tool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **What-if Tool** (**WIT**) was originally developed by Google and is an
    early explainability tool with a visual interface that allows the inspection of
    a model’s predictions, comparison across different models, and examination of
    potential bias. It is relatively easy to use and does not require much coding,
    and it includes support for many of the concepts we discussed in this chapter,
    such as counterfactuals and PDPs.
  prefs: []
  type: TYPE_NORMAL
- en: AI Fairness 360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IBM’s **AI Fairness 360** (**AIF360**) is an open source library that includes
    a set of fairness metrics for datasets and models, and algorithms for mitigating
    bias. It can be used to provide detailed explanations to understand fairness metrics
    and their implications in a given context, and it enables users to visually explore
    bias in their datasets and models. It can also help in identifying if a trained
    model is producing biased outcomes and provides some tools to help mitigate bias
    in each phase of the model development life cycle, such as preprocessing, training,
    and postprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: EthicalML/XAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an open source Python library designed to support interpretable ML and
    responsible AI. It includes tools for preprocessing, model interpretability, bias
    detection, and visualization, and it supports concepts we discussed in this chapter,
    such as feature importance, Shapley values, LIME, and DIA.
  prefs: []
  type: TYPE_NORMAL
- en: Fairlearn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fairlearn is another open source Python-based project that aims to help data
    scientists improve the fairness of AI systems. It includes algorithms for mitigating
    unfairness in classification and regression models, and fairness metrics for comparison.
    Its primary goal is to help ML practitioners reduce unfair disparities in predictions
    by understanding metrics and algorithms, and it provides an interactive UI experience
    for model assessment and comparison, which includes fairness metrics and an assessment
    dashboard. It again supports mitigation techniques in various phases of the ML
    model development life cycle, such as preprocessing, training, and postprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many more explainability and fairness libraries in addition to the
    ones mentioned here, but these ones are particularly popular libraries that are
    currently available in the industry. Google Cloud has recently launched more specific
    model evaluation mechanisms and metrics for fairness. At the time of writing this
    in October 2023, these mechanisms are still in preview mode and are not yet generally
    available. You can find out more about these features in the documentation at
    the following link: [https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness](https://cloud.google.com/vertex-ai/docs/evaluation/intro-evaluation-fairness)'
  prefs: []
  type: TYPE_NORMAL
- en: A note on generative AI
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapters 14* through *17* in this book are dedicated to **generative AI**
    (**GenAI**), which is a relatively new subset of AI/ML. In those chapters, we
    will explore the concepts of **large language models** (**LLMs**) and how they
    differ from other types of ML models that we’ve already covered in this book.
    LLMs are typically trained on extremely large datasets and therefore acquire vast
    amounts of knowledge that can be applied to many different kinds of use cases.
    In those later chapters, we will learn how LLMs can be used as auto-raters to
    open up new kinds of evaluation techniques for ML models, including specific evaluations
    focusing on bias, explainability, and fairness.'
  prefs: []
  type: TYPE_NORMAL
- en: A final topic for us to explore in this chapter is the concept of lineage tracking.
    In the next section, we delve into this topic in detail and assess its importance
    in the context of explainability and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of lineage tracking in ML model development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve touched on the concept of lineage tracking in earlier chapters, and now
    we will explore it in more detail. When we talk about lineage tracking, we’re
    referring to tracking all of the steps and artifacts that were used to create
    a given ML model. This includes items such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The source datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All transformations that were performed on those datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All intermediate datasets that were created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which algorithm was used to train a model on the resulting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which hyperparameters and values were used during model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which platform and tools were used in the training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a hyperparameter tuning job was used, details of that job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details of any evaluation steps performed on the resulting model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is being served for online inference, details of the endpoint at
    which the model is hosted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list is not exhaustive. We generally want to track every step
    that was used to create a model and all of the inputs and outputs in each step.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need to do this? Lineage tracking is important for many reasons. It
    complements the concept of explainability. While lineage tracking by itself will
    not necessarily explain why a model behaves in a particular manner, it is certainly
    important for researchers to understand how the model was created. It’s also important
    for reproducibility and collaboration. We’ve talked about the complexity some
    companies encounter when they need to manage thousands of ML models created by
    many different teams. If a model is behaving problematically, understanding its
    lineage will help in troubleshooting. If one team wants to build on the work another
    team has already performed, such as a model they have trained or datasets they
    have created, understanding the lineage of those artifacts will help the consuming
    team to be more productive in those endeavors. Also, in order to continually enhance
    a model’s performance, we need to know how that model was created. Lineage is
    also important, and sometimes required, for governance and compliance reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Google Cloud provides tools that help us to track lineage. For
    example, Dataplex can be used to track data lineage, and the Vertex ML Metadata
    service can help us track all steps and artifacts in our ML model development
    life cycle. Next, we will take a look at Vertex ML Metadata in more detail; let’s
    first start with some terminology used by the Vertex ML Metadata service.
  prefs: []
  type: TYPE_NORMAL
- en: ML metadata service terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Executions** represent the steps or operations in an ML workflow, such as
    data preprocessing, model training, or evaluation. **Artifacts** represent the
    inputs and outputs of each step, such as datasets, models, or evaluation metrics.
    **Events** represent the relationships between executions and artifacts, such
    as “Artifact X was produced by Execution Y” or “Artifact X was used as an input
    by Execution Y.” Events help us to establish lineage data by associating artifacts
    and executions with each other. **Contexts** represent logical groupings that
    bundle related artifacts and executions together. An example of a context would
    be a specific pipeline run or a model version.'
  prefs: []
  type: TYPE_NORMAL
- en: Collectively, all of the aforementioned resources are referred to as **metadata
    resources**, and they are described by a **MetadataSchema**, which describes the
    schema for particular types of metadata resources. In addition to the predefined
    metadata resources, we can also store custom metadata in the Vertex ML Metadata
    service. All tracked metadata is stored in a **MetadataStore**, and all of this
    information can be used to create a **Lineage Graph**, which is a visual representation
    that connects artifacts, executions, and contexts, and shows the relationships
    and flow between them.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, as with most Google Cloud resources, access to the metadata resources
    can be controlled using Google Cloud **Identity and Access Management** (**IAM**),
    which is important for security and compliance reasons.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the main terminology and concepts, let’s start reviewing
    some metadata in Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Lineage tracking in Vertex AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To explore the lineage tracking features in Vertex AI, we will use the MLOps
    pipeline we built in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288) as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Google Cloud console, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to **Vertex AI** > **Pipelines**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the name of the pipeline run that was created in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288)
    (it should be the most recent pipeline run unless you have run other pipelines
    in this Google Cloud project in the meantime).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll see the execution graph for the pipeline run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the top of the screen, click the toggle button to the left of the words
    **Expand Artifacts** (see *Figure 12**.7* for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.7: Expand Artifacts](img/B18143_12_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Expand Artifacts'
  prefs: []
  type: TYPE_NORMAL
- en: We can now start exploring the metadata related to each of the steps and artifacts
    in our pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You’ll also notice that the **Pipeline run analysis** section on the right side
    of the screen contains lots of information about this pipeline run. The **SUMMARY**
    tab provides information about the pipeline run itself, including the parameters
    that were used as inputs. Those are the parameters we defined in our pipeline
    definition in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can click on elements in the pipeline execution graph in order to see metadata
    related to that specific element. Let’s start right at the beginning. We want
    to know which dataset was used as an initial input to our pipeline. Considering
    that the first step in our pipeline is the data preprocessing step, and that step
    fetches the dataset, click on the preprocessing step, and its metadata will be
    shown on the right side of the screen, as depicted in *Figure 12**.8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.8: Preprocessing step details](img/B18143_12_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Preprocessing step details'
  prefs: []
  type: TYPE_NORMAL
- en: The green arrow in *Figure 12**.8* is pointing to the `source_dataset` input
    parameter, which provides the path to the source dataset (the actual details are
    redacted in the image in order to obscure my bucket name).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also see the `preprocessed_data_path` parameter value, which provides
    the path to the folder in which the preprocessing script will store the resulting
    processed data. If you scroll down (not shown in the screenshot), you will also
    see the `main_python_file_uri` parameter value, which provides the path to the
    PySpark script that we used in the preprocessing step in our pipeline. In fact,
    if we click on the **VIEW JOB** button, we can view the details of the actual
    Serverless Spark job that was used to execute our script in Google Cloud Dataproc,
    including its execution logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we’ve successfully tracked our source dataset, the script and job that performed
    transformations on that dataset, and the resulting processed dataset that was
    used to train our model, let’s move to the next step in our pipeline, which is
    the model training step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the `custom-training-job` step in our pipeline execution graph. In
    the information panel on the right, perhaps the most important parameter is the
    `worker_pool_specs` parameter. As depicted in *Figure 12**.9*, this parameter
    provides a lot of information about how our model is trained, such as the dataset
    that was used for training (which is the output of the previous, preprocessing
    step), the location at which the trained model artifacts are saved, the container
    image that was used to run our custom training code, the hyperparameter values
    used during training, as well as the machine type and number of machines that
    were used by the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.9: worker_pool_specs](img/B18143_12_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: worker_pool_specs'
  prefs: []
  type: TYPE_NORMAL
- en: Again, if we click on the **VIEW JOB** button at the top of the screen, we can
    see the actual job that ran on the Vertex AI training service to train our model,
    as well as the execution logs for that job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because we used a custom script to train our model and simply saved the artifacts
    in Google Cloud Storage, at this point in the pipeline, our model is referred
    to as an `importer` job to import our model artifact for that purpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `model-upload` step in the pipeline is what registers our model in the Vertex
    AI Model Registry. If you click on that step in the execution graph and review
    its metadata, in the **Output Parameters** section, you will see the URI for the
    resulting resource in the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The remaining steps, `endpoint-create` and `model-deploy`, have similar formats.
    As their names suggest, the `endpoint-create` step creates an endpoint in the
    Vertex AI prediction service, and the `model-deploy` step deploys our model to
    that endpoint. Their output parameters will show the URIs for the resources created
    by those steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I want to draw your attention to the `endpoint` and `model` artifacts in the
    pipeline. If you click on those, and click the **View Lineage** button that appears
    in the information panel on the right-hand side of the screen, it will take you
    directly to the console for the Vertex AI Metadata service and will show you another
    view of how the steps and artifacts relate to each other, as depicted in *Figure
    12**.10*. Again, clicking on each element in the graph will display metadata for
    that element:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.10: Lineage graph in Vertex AI Metadata service console](img/B18143_12_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Lineage graph in Vertex AI Metadata service console'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to getting metadata insights via the Google Cloud console, we can
    also use the Vertex AI SDK and API directly to query and manage metadata programmatically.
    For example, the following piece of code will list all artifacts in our Google
    Cloud project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the following lines will list all executions and contexts in our
    Google Cloud project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have now successfully tracked every step and artifact that was used to create
    our model. Next, let’s explore the **Experiments** feature in Vertex AI that is
    closely related to lineage tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex ML Experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we created our pipeline definition in [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288),
    we specified an experiment name with which to associate our pipeline runs. This
    provides another view of how to group the steps and artifacts related to our pipeline
    runs and the model versions they create. This feature can be useful for sharing
    and collaboration, as well as for comparing different model versions against each
    other. To view the experiment associated with our pipeline runs, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to **Vertex AI** > **Experiments**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the name of the experiment that we specified in our MLOps chapter (`aiml-sa-mlops-experiment`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the name of the most recent run, and explore the **ARTIFACTS** tab,
    as shown in *Figure 12**.11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.11: Vertex AI Experiments – Artifacts view](img/B18143_12_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Vertex AI Experiments – Artifacts view'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the links that are shown under each artifact ID to see those artifacts,
    as well as their metadata in the Vertex ML Metadata service (it will bring you
    back to the screens we already explored in the previous section; this is just
    another way to access the same metadata).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ve covered a lot of information in this chapter – let’s summarize what we’ve
    discussed before we move on to the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the concepts of bias, explainability, fairness,
    and lineage. We started off by examining some of the common types of bias that
    can occur at various steps in the ML model development life cycle. This included
    sources of bias such as pre-existing bias, algorithmic bias, and collection or
    measurement bias, which further included sub-categories such as sampling bias,
    response bias, and observer bias. We talked about how to inspect for bias, using
    techniques such as data exploration and DIA.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we dived into the use of explainability techniques to understand how our
    models make their decisions at inference time and to assess their fairness, particularly
    with regard to understanding how the input features in our dataset could influence
    our models’ predictions. We used tools such as PDPs and SHAP for these purposes.
    We then looked at how to use Vertex AI to get explanations from models that were
    hosted on Vertex AI endpoints. Going beyond simply getting explanations, we then
    discussed how to proactively counteract bias by using counterfactual analysis
    to pose questions such as “What would need to change in my input data to alter
    the decision of a predictive model?”
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered the topic of lineage tracking and its importance in terms
    of explainability, as well as other factors such as collaboration, troubleshooting,
    and compliance. We walked through an ML pipeline that we had created in a previous
    chapter and looked at the metadata associated with each component of the pipeline,
    including all of the steps and artifacts that were used to create a specific model.
  prefs: []
  type: TYPE_NORMAL
- en: While the chapters before this one focused on everything that’s required to
    build and run an ML model, this chapter focused on more advanced topics such as
    ensuring that our models are explainable and fair. The next chapter in this book
    continues in a similar vein. We are no longer just looking at the mechanics of
    how to build and deploy ML models but are now incorporating broader ethical and
    architectural considerations. In the next chapter, we dive further into the topics
    of governance, compliance, and architectural best practices in the model development
    life cycle.
  prefs: []
  type: TYPE_NORMAL
