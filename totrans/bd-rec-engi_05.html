<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Building Collaborative Filtering Recommendation Engines" id="aid-1394Q1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Building Collaborative Filtering Recommendation Engines</h1></div></div></div><p>In this chapter, we learn how to implement collaborative filtering recommendation systems using popular data analysis programming languages, R and Python. We will learn how to implement user-based collaborative filtering and item-based collaborative filtering in R and Python programming languages.</p><p>In this chapter, we learn about:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The Jester5k dataset we will be using for this chapter</li><li class="listitem">Exploring the dataset and understanding the data</li><li class="listitem">Recommendation engine packages/libraries available in R and Python</li><li class="listitem">Building user-based collaborative filtering in R</li><li class="listitem">Building item-based collaborative filtering in R</li><li class="listitem">Building user-based collaborative filtering in Python</li><li class="listitem">Item-based collaborative filtering in Python</li><li class="listitem">Evaluating the model</li></ul></div><p>The <span class="strong"><strong>recommenderlab</strong></span>, R package is a framework for developing and testing recommendation algorithms including user-based collaborative filtering, item-based collaborative filtering, SVD, and association rule-based algorithms, which are used to build recommendation engines. This package also provides basic infrastructure or mechanisms to develop our own recommendation engine methodology.</p><div class="section" title="Installing the recommenderlab package in RStudio"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Installing the recommenderlab package in RStudio</h1></div></div></div><p>The following code snippet, will install the <code class="literal">recommenderlab</code> package into RStudio, if it is not available:</p><pre class="programlisting">if(!"recommenderlab" %in% rownames(installed.packages())){ &#13;
install.packages("recommenderlab")} &#13;
</pre><p>First the r-environment checks if there are any previous installations of the recommender lab package, if none are found then it installs as shown below:</p><pre class="programlisting">Loading required package: recommenderlab &#13;
Error in .requirePackage(package) :  &#13;
  unable to find required package 'recommenderlab' &#13;
In addition: Warning message: &#13;
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  : &#13;
  there is no package called 'recommenderlab' &#13;
Loading required package: recommenderlab &#13;
install.packages("recommenderlab") &#13;
Installing package into 'path to installation folder/R/win-library/3.2' &#13;
(as 'lib' is unspecified) &#13;
trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.2/recommenderlab_0.2-0.zip' &#13;
Content type 'application/zip' length 1405353 bytes (1.3 MB) &#13;
downloaded 1.3 MB &#13;
package 'recommenderlab' successfully unpacked and MD5 sums checked &#13;
</pre><p>The following code snippet using <code class="literal">library()</code> loads the <code class="literal">recommenderlab</code> package into the r-environment:</p><pre class="programlisting">library(recommenderlab) &#13;
 &#13;
Loading required package: Matrix &#13;
Loading required package: arules &#13;
 &#13;
Attaching package: 'arules' &#13;
 &#13;
The following objects are masked from 'package:base': &#13;
 &#13;
    abbreviate, write &#13;
 &#13;
Loading required package: proxy &#13;
 &#13;
Attaching package: 'proxy' &#13;
 &#13;
The following object is masked from 'package:Matrix': &#13;
 &#13;
    as.matrix &#13;
 &#13;
The following objects are masked from 'package:stats': &#13;
 &#13;
    as.dist, dist &#13;
 &#13;
The following object is masked from 'package:base': &#13;
 &#13;
    as.matrix &#13;
 &#13;
Loading required package: registry &#13;
</pre><p>To get help on the <code class="literal">recommenderlab</code> package using the help function, run the following command in Rstudio:</p><pre class="programlisting">help(package = "recommenderlab") &#13;
</pre><p>Check the help page for details on the usage of the package by clicking on the links provided:</p><div class="mediaobject"><img src="../Images/image00320.jpeg" alt="Installing the recommenderlab package in RStudio"/></div><p style="clear:both; height: 1em;"> </p></div></div>
<div class="section" title="Datasets available in the recommenderlab package" id="aid-147LC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Datasets available in the recommenderlab package</h1></div></div></div><p>Like any other package available in R, <code class="literal">recommenderlab</code> also comes with default datasets. Run the following command to show the available packages:</p><pre class="programlisting">data_package &lt;- data(package = "recommenderlab") &#13;
data_package$results[,c("Item","Title")] &#13;
</pre><div class="mediaobject"><img src="../Images/image00321.jpeg" alt="Datasets available in the recommenderlab package"/></div><p style="clear:both; height: 1em;"> </p><p>Out of all the available datasets, we have chosen to use the <code class="literal">Jester5k</code> dataset for implementing user-based collaborative filtering and item-based collaborative filtering recommendation engines using R.</p><div class="section" title="Exploring the Jester5K dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec51"/>Exploring the Jester5K dataset</h2></div></div></div><p>In this section, we shall explore the <code class="literal">Jester5K</code> dataset as follows:</p><div class="section" title="Description"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec9"/>Description</h3></div></div></div><p>The dataset contains a sample of 5000 users from the Jester Online Joke Recommender System anonymous ratings data, collected between April 1999 and May 2003.</p></div><div class="section" title="Usage"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec10"/>Usage</h3></div></div></div><pre class="programlisting">data(Jester5k) &#13;
</pre></div><div class="section" title="Format"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec11"/>Format</h3></div></div></div><p>The format of <code class="literal">Jester5k</code> is: <code class="literal">Formal class 'realRatingMatrix' [package "recommenderlab"]</code>.</p><p>The format of <code class="literal">JesterJokes</code> is a vector of character strings.</p></div><div class="section" title="Details"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec12"/>Details</h3></div></div></div><p><code class="literal">Jester5k</code> contains a <span class="emphasis"><em>5000 x 100</em></span> rating matrix (5000 users and 100 jokes) with ratings between -10.00 and +10.00. All selected users have rated 36 or more jokes.</p><p>The data also contains the actual jokes in <code class="literal">JesterJokes</code>.</p><p>The number of ratings present in the real-rating matrix is represented as follows:</p><pre class="programlisting">nratings(Jester5k) &#13;
 &#13;
[1] 362106 &#13;
 &#13;
Jester5k &#13;
5000 x 100 rating matrix of class 'realRatingMatrix' with 362106 ratings. &#13;
</pre><p>You can display the class of the rating matrices by running the following command:</p><pre class="programlisting">class(Jester5k) &#13;
[1] "realRatingMatrix" &#13;
attr(,"package") &#13;
[1] "recommenderlab" &#13;
</pre><p>The <code class="literal">recommenderlab</code> package efficiently stores the rating information in a compact way. Usually, rating matrices are sparse matrices. For this reason, the <code class="literal">realRatingMatrix</code> class supports a compact storage of sparse matrices.</p><p>Let's compare the size of <code class="literal">Jester5k</code> with the corresponding R matrix to understand the advantage of the real rating matrix, as follows:</p><pre class="programlisting">object.size(Jester5k) &#13;
4633560 bytes &#13;
#convert the real-rating matrix into R matrix &#13;
object.size(as(Jester5k,"matrix")) &#13;
4286048 bytes &#13;
object.size(as(Jester5k, "matrix"))/object.size(Jester5k) &#13;
0.925001079083901 bytes &#13;
</pre><p>We observe that the real-rating matrix stores <code class="literal">0.92</code> times less space than the R matrix. For collaborative filtering methods, which are memory-based models, where all the data is loaded into the memory while generating recommendations, storing data efficiently is very important. The <code class="literal">recommenderlab</code> package does this job efficiently.</p><p><code class="literal">The recommenderlab</code> package exposes many functions which can be operated on using the rating matrix object. Run the following command to see the available methods:</p><pre class="programlisting">methods(class = class(Jester5k)) &#13;
</pre><div class="mediaobject"><img src="../Images/image00322.jpeg" alt="Details"/></div><p style="clear:both; height: 1em;"> </p><p>Run the following commands to see the available recommendation algorithms in the <code class="literal">recommenderlab</code> package:</p><pre class="programlisting">names(recommender_models) &#13;
</pre><div class="mediaobject"><img src="../Images/image00323.jpeg" alt="Details"/></div><p style="clear:both; height: 1em;"> </p><p>The following code snippet displays the same result as in the previous image, <code class="literal">lapply()</code> function applies the function to all the elements of the list, in our case, for each of the items in the <code class="literal">recommender_models</code> object, <code class="literal">lapply</code> will extract the description and display the results as follows:</p><pre class="programlisting">lapply(recommender_models, "[[", "description") &#13;
$IBCF_realRatingMatrix &#13;
[1] "Recommender based on item-based collaborative filtering (real data)." &#13;
 &#13;
$POPULAR_realRatingMatrix &#13;
[1] "Recommender based on item popularity (real data)." &#13;
 &#13;
$RANDOM_realRatingMatrix &#13;
[1] "Produce random recommendations (real ratings)." &#13;
 &#13;
$RERECOMMEND_realRatingMatrix &#13;
[1] "Re-recommends highly rated items (real ratings)." &#13;
 &#13;
$SVD_realRatingMatrix &#13;
[1] "Recommender based on SVD approximation with column-mean imputation (real data)." &#13;
 &#13;
$SVDF_realRatingMatrix &#13;
[1] "Recommender based on Funk SVD with gradient descend (real data)." &#13;
 &#13;
$UBCF_realRatingMatrix &#13;
[1] "Recommender based on user-based collaborative filtering (real data)." &#13;
</pre></div></div></div>
<div class="section" title="Exploring the dataset" id="aid-1565U1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Exploring the dataset</h1></div></div></div><p>In this section let's explore the data in more detail. To find the dimensions of the data and the type of data, run the following commands:</p><p>There are <code class="literal">5000</code> users and <code class="literal">100</code> items:</p><pre class="programlisting">dim(Jester5k) 
 
[1] 5000  100 
 
</pre><p>The data is of R Matrix:</p><pre class="programlisting">class(Jester5k@data) 
 
[1] "dgCMatrix" 
attr(,"package") 
[1] "Matrix" 
</pre><div class="section" title="Exploring the rating values"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec52"/>Exploring the rating values</h2></div></div></div><p>The following code snippet will help us understand the rating values distribution:</p><p>Rating distribution is given as:</p><pre class="programlisting">hist(getRatings(Jester5k), main="Distribution of ratings") 
</pre><div class="mediaobject"><img src="../Images/image00324.jpeg" alt="Exploring the rating values"/></div><p style="clear:both; height: 1em;"> </p><p>The preceding image shows the frequency of the ratings available from the <code class="literal">Jester5K</code> dataset. We can observe that the negative ratings are more or less of uniform distribution or the same frequency, and the positive ratings are of a higher frequency and are declining towards the right of the plot. This may attribute to the bias induced by the ratings given by the users.</p></div></div>
<div class="section" title="Building user-based collaborative filtering with recommenderlab"><div class="titlepage" id="aid-164MG2"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Building user-based collaborative filtering with recommenderlab</h1></div></div></div><p>Run the following code in order to load the <code class="literal">recommenderlab</code> library and data into the R environment:</p><pre class="programlisting">library(recommenderlab) &#13;
data("Jester5k") &#13;
</pre><p>Let's look at the sample rating data of the first six users on the first 10 jokes. Run the following command:</p><pre class="programlisting">head(as(Jester5k,"matrix")[,1:10]) &#13;
</pre><div class="mediaobject"><img src="../Images/image00325.jpeg" alt="Building user-based collaborative filtering with recommenderlab"/></div><p style="clear:both; height: 1em;"> </p><p>We have looked at exploring the data in the previous section so we will get directly to building a user-based collaborative recommender system.</p><p>This section is divided as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Building a base recommender model for benchmarking by splitting the data into 80% training data and 20% test data.</li><li class="listitem">Evaluating the recommender model using a k-fold cross-validation approach model</li><li class="listitem">Parameter tuning for the recommender model</li></ul></div><div class="section" title="Preparing training and test data"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec53"/>Preparing training and test data</h2></div></div></div><p>For building and evaluating a recommender model, we need training data and test data. Run the following command to create it:</p><p>We use the seed function for generating reproducible results:</p><pre class="programlisting">set.seed(1) &#13;
which_train &lt;- sample(x = c(TRUE, FALSE), size = nrow(Jester5k),replace = TRUE, prob = c(0.8, 0.2)) &#13;
head(which_train) &#13;
[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE &#13;
</pre><p>The previous code creates a logical object with an equal length to the number of users. True indexes will be part of the train set and false indexes will be part of the test set.</p><pre class="programlisting">rec_data_train &lt;- Jester5k[which_train, ] &#13;
rec_data_test &lt;- Jester5k[!which_train, ] &#13;
 &#13;
dim(rec_data_train) &#13;
[1] 4004  100 &#13;
 &#13;
dim(rec_data_test) &#13;
[1] 996  100 &#13;
</pre></div><div class="section" title="Creating a user-based collaborative model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/>Creating a user-based collaborative model</h2></div></div></div><p>Now, let's create a recommendation model on the whole data of <code class="literal">Jester5k</code>. Before that, let's explore the recommender models available and their parameters in the <code class="literal">recommenderlab</code> package as follows:</p><pre class="programlisting">recommender_models &lt;- recommenderRegistry$get_entries(dataType = "realRatingMatrix") &#13;
 &#13;
recommender_models &#13;
</pre><div class="mediaobject"><img src="../Images/image00326.jpeg" alt="Creating a user-based collaborative model"/></div><p style="clear:both; height: 1em;"> </p><p>Image we just saw displays the 6 different recommender models available and its parameters.</p><p>Run the following code to build a user-based collaborative filtering model:</p><pre class="programlisting">recc_model &lt;- Recommender(data = rec_data_train, method = "UBCF") &#13;
recc_model &#13;
 &#13;
Recommender of type 'UBCF' for 'realRatingMatrix'  &#13;
learned using 4004 users. &#13;
recc_model@model$data &#13;
 &#13;
4004 x 100 rating matrix of class 'realRatingMatrix' with 289640 ratings. &#13;
Normalized using center on rows. &#13;
</pre><p>The <code class="literal">recc_model@model$data</code> object contains the rating matrix. The reason for this is that UBCF is a lazy-learning technique, which means that it needs to access all the data to perform a prediction.</p></div><div class="section" title="Predictions on the test set"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Predictions on the test set</h2></div></div></div><p>Now that we have built the model, let's predict the recommendations on the test set. For this we will use the <code class="literal">predict()</code> function available in the library. We generate 10 recommendations per user. See the following code for the predictions:</p><pre class="programlisting">n_recommended &lt;- 10 &#13;
recc_predicted &lt;- predict(object = recc_model,newdata = rec_data_test, n = n_recommended) &#13;
recc_predicted &#13;
Recommendations as 'topNList' with n = 10 for 996 users.  &#13;
 &#13;
#Let's define list of predicted recommendations: &#13;
rec_list &lt;- sapply(recc_predicted@items, function(x){ &#13;
  colnames(Jester5k)[x] &#13;
}) &#13;
</pre><p>The resultant object is a list type given by the following code:</p><pre class="programlisting">class(rec_list) &#13;
[1] "list" &#13;
</pre><p>The first two recommendations are given as follows:</p><pre class="programlisting">rec_list [1:2] &#13;
$u21505 &#13;
 [1] "j81"  "j73"  "j83"  "j75"  "j100" "j80"  "j72"  "j95"  "j87"  "j96"  &#13;
 &#13;
$u5809 &#13;
 [1] "j97" "j93" "j76" "j78" "j77" "j85" "j89" "j98" "j91" "j80" &#13;
</pre><p>We can observe that for user <code class="literal">u21505</code>, the top 10 recommendations are given as <code class="literal">j81, j73, j83, ... j96</code>.</p><p>The following image shows the recommendations for four users:</p><div class="mediaobject"><img src="../Images/image00327.jpeg" alt="Predictions on the test set"/></div><p style="clear:both; height: 1em;"> </p><p>Let's see how many recommendations are generated for all the test users by running the following code:</p><pre class="programlisting">number_of_items = sort(unlist(lapply(rec_list, length)),decreasing = TRUE) &#13;
table(number_of_items) &#13;
 &#13;
0   1   2   3   4   5   6   7   8   9  10  &#13;
286   3   2   3   3   1   1   1   2   3 691  &#13;
</pre><p>From the above results, we see that for <code class="literal">286</code> users, zero recommendations were generated. The reason is that they have rated all the movies in the original dataset. For 691 users, 10 ratings for each of them has been generated, the reason is that in the original dataset, they have not rated for any movies. Other users who have received 2, 3, 4, and so on recommendations means that they have recommended very few movies.</p></div><div class="section" title="Analyzing the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Analyzing the dataset</h2></div></div></div><p>Before we evaluate the model, let's take one step back and analyze the data. By analyzing the number of ratings given by all the users for the jokes, we can observe that there are 1422 people who have rated all 100 jokes, which seems to be unusual as there are very few people who have rated 80 to 99 jokes. Further analyzing the jokes we find that, there are 221, 364, 312, and 131 users who have rated 71, 72, 73, and 74 jokes respectively which seems to be unusual compared to other joke ratings.</p><p>Run the following code to extract the number of ratings given to each joke:</p><pre class="programlisting">table(rowCounts(Jester5k)) &#13;
</pre><div class="mediaobject"><img src="../Images/image00328.jpeg" alt="Analyzing the dataset"/></div><p style="clear:both; height: 1em;"> </p><p>For the next step, let's remove the records of users who have rated 80 or more jokes as follows:</p><pre class="programlisting">model_data = Jester5k[rowCounts(Jester5k) &lt; 80] &#13;
dim(model_data) &#13;
[1] 3261  100 &#13;
</pre><p>The dimension has been reduced from <code class="literal">5000</code> to <code class="literal">3261</code> records.</p><p>Now let's analyze the average ratings given by each user. A boxplot shows us the average distribution of the joke ratings.</p><pre class="programlisting">boxplot(model_data) &#13;
</pre><div class="mediaobject"><img src="../Images/image00329.jpeg" alt="Analyzing the dataset"/></div><p style="clear:both; height: 1em;"> </p><p>The preceding image shows us that there are very few ratings that deviate from normal behavior. From the preceding image we see that the average ratings that are above 7 (approximately) and below -5 (approximately) are kind of outliers and are less in number. Let's see the count by running the following code:</p><pre class="programlisting">boxplot(rowMeans(model_data [rowMeans(model_data)&gt;=-5 &amp; rowMeans(model_data)&lt;= 7])) &#13;
</pre><div class="mediaobject"><img src="../Images/image00330.jpeg" alt="Analyzing the dataset"/></div><p style="clear:both; height: 1em;"> </p><p>Dropping users who have given very low average ratings and very high average ratings.</p><pre class="programlisting">model_data = model_data [rowMeans(model_data)&gt;=-5 &amp; rowMeans(model_data)&lt;= 7] &#13;
dim(model_data) &#13;
[1] 3163  100 &#13;
</pre><p>Let's examine the rating distribution of the first 100 users in the data as follows:</p><pre class="programlisting">image(model_data, main = "Rating distribution of 100 users") &#13;
</pre><div class="mediaobject"><img src="../Images/image00331.jpeg" alt="Analyzing the dataset"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Evaluating the recommendation model using the k-cross validation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/>Evaluating the recommendation model using the k-cross validation</h2></div></div></div><p>The <code class="literal">recommenderlab</code> package provides an infrastructure for evaluating the models using the <code class="literal">evaluationScheme()</code> function. By definition, from the Cran website, evaluationScheme <span class="emphasis"><em>creates an evaluationScheme object from a data set. The scheme can be a simple split into training and test data, k-fold cross-evaluation or using k independent bootstrap samples</em></span>.</p><p>The following are the arguments for the <code class="literal">evaluationScheme()</code> function:</p><div class="mediaobject"><img src="../Images/image00332.jpeg" alt="Evaluating the recommendation model using the k-cross validation"/></div><p style="clear:both; height: 1em;"> </p><p>We use cross-validation method to split the data, for example the 5-fold cross-validation approach divides the training data into five smaller sets where four sets will be used for training the model and the remaining one set is used for evaluating the model. Let's define the parameters into minimum good ratings, number of folds for cross-validation method, and split method as follows:</p><pre class="programlisting">items_to_keep &lt;- 30 &#13;
rating_threshold &lt;- 3 &#13;
n_fold &lt;- 5 # 5-fold  &#13;
eval_sets &lt;- evaluationScheme(data = model_data, method = "cross-validation",train = percentage_training, given = items_to_keep, goodRating = rating_threshold, k = n_fold) &#13;
 &#13;
Evaluation scheme with 30 items given &#13;
Method: 'cross-validation' with 5 run(s). &#13;
Good ratings: &gt;=3.000000 &#13;
Data set: 3163 x 100 rating matrix of class 'realRatingMatrix' with 186086 ratings. &#13;
</pre><p>Let's examine the size of the five sets formed by the cross-validation approach as follows:</p><pre class="programlisting">size_sets &lt;- sapply(eval_sets@runsTrain, length) &#13;
 size_sets &#13;
[1] 2528 2528 2528 2528 2528 &#13;
</pre><p>In order to extract the sets, we need to use <code class="literal">getData()</code>. There are three sets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>train</strong></span>: This is the training set</li><li class="listitem"><span class="strong"><strong>known</strong></span>: This is the test set, with the item used to build the recommendations</li><li class="listitem"><span class="strong"><strong>unknown</strong></span>: This is the test set, with the item used to test the recommendations</li></ul></div><p>Let's take a look at the training set in the following code:</p><pre class="programlisting">getData(eval_sets, "train") &#13;
2528 x 100 rating matrix of class 'realRatingMatrix' with 149308 ratings. &#13;
</pre></div><div class="section" title="Evaluating user-based collaborative filtering"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Evaluating user-based collaborative filtering</h2></div></div></div><p>Now let's evaluate the models, let's set the parameters <code class="literal">model_to_evaluate</code> with user-based collaborative filtering and <code class="literal">model_parameters</code> with <code class="literal">NULL</code> for using default settings as follows:</p><pre class="programlisting">model_to_evaluate &lt;- "UBCF" &#13;
model_parameters &lt;- NULL &#13;
</pre><p>The next step is to build the recommender model using the recommender() function as follows:</p><pre class="programlisting">eval_recommender &lt;- Recommender(data = getData(eval_sets, "train"),method = model_to_evaluate, parameter = model_parameters) &#13;
 &#13;
Recommender of type 'UBCF' for 'realRatingMatrix'  &#13;
learned using 2528 users &#13;
</pre><p>We have seen that the user-based recommender model has been learned with a training data of <code class="literal">2528 users</code>. Now we can predict the known ratings in <code class="literal">eval_sets</code> and evaluate the results with unknown sets as described earlier.</p><p>Before making the predictions for the known ratings, we have to set the number of items to be recommended. Next, we have to provide the test set to the <code class="literal">predict()</code> function for prediction. The prediction of ratings is done by running the following command:</p><pre class="programlisting">items_to_recommend &lt;- 10 &#13;
eval_prediction &lt;- predict(object = eval_recommender, newdata =getData(eval_sets, "known"), n = items_to_recommend, type = "ratings") &#13;
 &#13;
eval_prediction &#13;
635 x 100 rating matrix of class 'realRatingMatrix' with 44450 ratings &#13;
</pre><p>Executing the <code class="literal">predict()</code> function will take time because the user-based collaborative filtering approach is memory-based and a lazy learning technique implemented at run time, to show that the whole dataset is loaded during the prediction.</p><p>Now we shall evaluate the predictions with the unknown sets and estimate the model accuracy with metrics such as precision, recall, and F1 measure. Run the following code to calculate the model accuracy metrics by calling the <code class="literal">calcPredicitonAccuracy()</code> method:</p><pre class="programlisting">eval_accuracy &lt;- calcPredictionAccuracy(  x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = TRUE) &#13;
head(eval_accuracy) &#13;
           RMSE       MSE      MAE &#13;
u17322 4.536747 20.582076 3.700842 &#13;
u13610 4.609735 21.249655 4.117302 &#13;
u5462  4.581905 20.993858 3.714604 &#13;
u1143  2.178512  4.745912 1.850230 &#13;
u5021  2.664819  7.101260 1.988018 &#13;
u21146 2.858657  8.171922 2.194978 &#13;
</pre><p>By setting <code class="literal">byUser = TRUE</code>, we are calculating the model accuracy for each user. Taking the average will give us the overall accuracy as follows:</p><pre class="programlisting">apply(eval_accuracy,2,mean) &#13;
     RMSE       MSE       MAE  &#13;
 4.098122 18.779567  3.377653  &#13;
</pre><p>By setting <code class="literal">byUser=FALSE</code>, in the previous <code class="literal">calcPredictionAccuracy()</code> we can calculate the overall model accuracy given by the following:</p><pre class="programlisting">eval_accuracy &lt;- calcPredictionAccuracy(  x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = &#13;
    FALSE) &#13;
 &#13;
eval_accuracy &#13;
    RMSE       MSE       MAE  &#13;
 4.372435 19.118191  3.431580  &#13;
</pre><p>In the previous approach, we evaluated the model accuracy using the <span class="strong"><strong>root mean squared error</strong></span> (<span class="strong"><strong>RMSE</strong></span>), and <span class="strong"><strong>mean absolute error</strong></span> (<span class="strong"><strong>MAE</strong></span>), but we can also evaluate the model accuracy using precision/recall For this, we use the <code class="literal">evaluate()</code> function and then the result of the <code class="literal">evaluate()</code> method is used to create a confusion matrix containing precision/recall/f1 measures as follows:</p><pre class="programlisting">results &lt;- evaluate(x = eval_sets, method = model_to_evaluate, n = seq(10, 100, 10)) &#13;
</pre><div class="mediaobject"><img src="../Images/image00333.jpeg" alt="Evaluating user-based collaborative filtering"/></div><p style="clear:both; height: 1em;"> </p><pre class="programlisting">head(getConfusionMatrix(results)[[1]]) &#13;
 &#13;
         TP        FP        FN        TN precision    recall       TPR        FPR &#13;
10  6.63622  3.363780 10.714961 49.285039 0.6636220 0.4490838 0.4490838 0.05848556 &#13;
20 10.03150  9.968504  7.319685 42.680315 0.5015748 0.6142384 0.6142384 0.17854766 &#13;
30 11.20787 18.792126  6.143307 33.856693 0.3735958 0.6714050 0.6714050 0.34877101 &#13;
40 11.91181 28.088189  5.439370 24.560630 0.2977953 0.7106378 0.7106378 0.53041204 &#13;
50 12.96850 37.031496  4.382677 15.617323 0.2593701 0.7679658 0.7679658 0.70444585 &#13;
60 14.82362 45.176378  2.527559  7.472441 0.2470604 0.8567522 0.8567522 0.85919995 &#13;
</pre><p>The first four columns contain the true-false positives/negatives, and they are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>True Positives</strong></span> (<span class="strong"><strong>TP</strong></span>): These are recommended items that have been rated correctly</li><li class="listitem"><span class="strong"><strong>False Positives</strong></span> (<span class="strong"><strong>FP</strong></span>): These are recommended items that haven't been rated</li><li class="listitem"><span class="strong"><strong>False Negatives</strong></span> (<span class="strong"><strong>FN</strong></span>): These are not recommended items that have been rated</li><li class="listitem"><span class="strong"><strong>True Negatives</strong></span> (<span class="strong"><strong>TN</strong></span>): These are not recommended items that haven't been rated</li></ul></div><p>A perfect (or overfitted) model would have only <code class="literal">TP</code> and <code class="literal">TN</code>.</p><p>If we want to take account of all the splits at the same time, we can just sum up the indices as follows:</p><pre class="programlisting">columns_to_sum &lt;- c("TP", "FP", "FN", "TN") &#13;
indices_summed &lt;- Reduce("+", getConfusionMatrix(results))[, columns_to_sum] &#13;
head(indices_summed) &#13;
         TP        FP       FN        TN &#13;
10 32.59528  17.40472 53.22520 246.77480 &#13;
20 49.55276  50.44724 36.26772 213.73228 &#13;
30 55.60787  94.39213 30.21260 169.78740 &#13;
40 59.04724 140.95276 26.77323 123.22677 &#13;
50 64.22205 185.77795 21.59843  78.40157 &#13;
60 73.67717 226.32283 12.14331  37.85669 &#13;
</pre><p>Since summarizing the model is difficult by referring to the above table, we can use a <code class="literal">ROC curve</code> to evaluate the model. Use <code class="literal">plot()</code> to build the ROC plot as follows:</p><pre class="programlisting">plot(results, annotate = TRUE, main = "ROC curve") &#13;
</pre><div class="mediaobject"><img src="../Images/image00334.jpeg" alt="Evaluating user-based collaborative filtering"/></div><p style="clear:both; height: 1em;"> </p><p>The preceding plot shows the relation between <span class="strong"><strong>True Positive Rate</strong></span> (<span class="strong"><strong>TPR</strong></span>) and <span class="strong"><strong>False Positive Rate</strong></span> (<span class="strong"><strong>FPR</strong></span>), but we have to choose the values in such a way that we give a trade-off between TPR and FPR. In our case, we observe that <span class="emphasis"><em>nn=30</em></span> is a very good trade-off since when considering neighbors of 30 we have TPR closer to <span class="emphasis"><em>0.7</em></span>, FPR is <span class="emphasis"><em>0.4</em></span> and when moving to <span class="emphasis"><em>nn=40</em></span> the TPR is still close to <span class="emphasis"><em>0.7</em></span> but the FPR has been changed to <span class="emphasis"><em>0.4.</em></span> This means that the False Positive Rate has been increased.</p></div></div>
<div class="section" title="Building an item-based recommender model"><div class="titlepage" id="aid-173722"><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Building an item-based recommender model</h1></div></div></div><p>As with UBCF, we use the same <code class="literal">Jester5k</code> dataset for the item-based recommender system. In this section, we do not explore the data as we have already done so in the previous section. We first remove the user data of those who have rated all the items and also those records who have rated more than <code class="literal">80</code> as follows:</p><pre class="programlisting">library(recommenderlab) &#13;
data("Jester5k") &#13;
model_data = Jester5k[rowCounts(Jester5k) &lt; 80] &#13;
model_data &#13;
[1] 3261  100 &#13;
</pre><p>Now let's see how the average ratings are distributed for each user:</p><pre class="programlisting">boxplot(rowMeans(model_data)) &#13;
</pre><div class="mediaobject"><img src="../Images/image00335.jpeg" alt="Building an item-based recommender model"/></div><p style="clear:both; height: 1em;"> </p><p>The following code snippet calculates the average ratings given by each user and identifies users who have given extreme ratings - either very high ratings or very low ratings:</p><p>From the below results, we observe that there are 19 records with very high average ratings and 79 records with very low ratings, compared with the majority of users:</p><pre class="programlisting">dim(model_data[rowMeans(model_data) &lt; -5]) &#13;
[1]  79 100 &#13;
dim(model_data[rowMeans(model_data) &gt; 7]) &#13;
[1]  19 100 &#13;
</pre><p>Of the total <code class="literal">3261</code> records, only <code class="literal">98</code> records had much less than the average and much more than the average ratings, so we removed these from our dataset as follows:</p><pre class="programlisting">model_data = model_data [rowMeans(model_data)&gt;=-5 &amp; rowMeans(model_data)&lt;= 7] &#13;
model_data &#13;
[1] 3163  100 &#13;
</pre><p>From here, we divide the sections as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Building the IBCF recommender model using the training and test data.</li><li class="listitem">Evaluating the model</li><li class="listitem">Parameter tuning</li></ul></div><div class="section" title="Building an IBCF recommender model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec59"/>Building an IBCF recommender model</h2></div></div></div><p>The first step in building any recommender model is to prepare the training data. Previously, we have prepared the data required for building the model by removing outlying data. Now run the following code to divide the available data into two sets: 80% training set and 20% test set. We build the recommender model using the training data and generate the recommendations on the test set.</p><p>The following code first creates a logical object of the same length as the original dataset containing 80% elements as TRUE and 20% as a test:</p><pre class="programlisting">which_train &lt;- sample(x = c(TRUE, FALSE), size = nrow(model_data), &#13;
 replace = TRUE, prob = c(0.8, 0.2)) &#13;
class(which_train) &#13;
[1] "logical" &#13;
head(which_train) &#13;
[1] TRUE TRUE TRUE TRUE TRUE TRUE &#13;
</pre><p>Then we use the logical object in the <code class="literal">model_data</code> to generate the training set as follows:</p><pre class="programlisting"> model_data_train &lt;- model_data[which_train, ] &#13;
dim(model_data_train) &#13;
[1] 2506  100 &#13;
</pre><p>Then we use the logical object in the <code class="literal">model_data</code> to generate the test set as follows:</p><pre class="programlisting"> model_data_test &lt;- model_data[!which_train, ] &#13;
 dim(model_data_test) &#13;
[1] 657 100 &#13;
</pre><p>Now that we have prepared the training set and the test set, let's train the model and generate the top recommendations on the test set.</p><p>For model building, as mentioned in the UBCF section, we use the same <code class="literal">recommender()</code> function available in the <code class="literal">recommenderlab</code> package. Run the following code to train the model with training data.</p><p>Set the parameters for the <code class="literal">recommender()</code> function. We set the model to evaluate as <code class="literal">"IBCF"</code> and <code class="literal">k=30</code>. <code class="literal">k</code> is the number of neighbors to be considered while calculating the similarity values as follows:</p><pre class="programlisting">model_to_evaluate &lt;- "IBCF" &#13;
 &#13;
model_parameters &lt;- list(k = 30) &#13;
</pre><p>The following code snippet shows building the recommendation engine model using the <code class="literal">recommender()</code> function and its input parameters such as input data, model to evaluate the parameters, and the k parameter:</p><pre class="programlisting">model_recommender &lt;- Recommender(data = model_data_train,method = model_to_evaluate, parameter = model_parameters) &#13;
</pre><p>The IBCF model object is created as a <code class="literal">model_recommender</code>. This model is trained and learned using the <code class="literal">2506</code> training set we created earlier as follows:</p><pre class="programlisting">model_recommender &#13;
Recommender of type 'IBCF' for 'realRatingMatrix'  learned using 2506 users. &#13;
</pre><p>Now that we have created the model, let's explore the model bit. We use <code class="literal">getModel()</code> available in the <code class="literal">recommenderlab</code> to extract the model details as follows:</p><div class="mediaobject"><img src="../Images/image00336.jpeg" alt="Building an IBCF recommender model"/></div><p style="clear:both; height: 1em;"> </p><p>From the above results, the important parameters to note are <code class="literal">k</code> value, the default similarity value, and method, <code class="literal">cosine similarity</code>.</p><p>The final step is to generate the recommendations on the test set. Run the following code on the test set and generate the recommendations.</p><p><code class="literal">items_to_recommend</code> is the parameter to set the number of recommendations to be generated for each user:</p><pre class="programlisting">items_to_recommend &lt;- 10 &#13;
</pre><p>Call the <code class="literal">predict()</code> method available in the reocommenderlab package to predict the unknowns in the test set:</p><pre class="programlisting">model_prediction &lt;- predict(object = model_recommender, newdata = model_data_test, n = items_to_recommend) &#13;
 &#13;
model_prediction &#13;
Recommendations as 'topNList' with n = 10 for 657 users.  &#13;
 &#13;
print(class(model_prediction)) &#13;
[1] "topNList" &#13;
attr(,"package") &#13;
[1] "recommenderlab" &#13;
 &#13;
</pre><p>We can get the slot details of the prediction object using the <code class="literal">slotNames()</code> method:</p><pre class="programlisting">slotNames(model_prediction) &#13;
[1] "items"      "itemLabels" "n"          &#13;
 &#13;
</pre><p>Let's have a look of the predictions generated for the first user in the test set:</p><pre class="programlisting"> model_prediction@items[[1]] &#13;
 [1]  89  76  72  87  93 100  97  80  94  86 &#13;
 &#13;
</pre><p>Let's add the item labels to each of the predictions:</p><pre class="programlisting"> recc_user_1  = model_prediction@items[[1]] &#13;
 &#13;
 jokes_user_1 &lt;- model_prediction@itemLabels[recc_user_1] &#13;
 &#13;
 jokes_user_1 &#13;
 [1] "j89"  "j76"  "j72"  "j87"  "j93"  "j100" "j97"  "j80"  "j94"  "j86"  &#13;
</pre></div><div class="section" title="Model evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec60"/>Model evaluation</h2></div></div></div><p>Let's take one step back to evaluate the recommender model before we generate the predictions. As we saw in UBCF, we can use the available <code class="literal">evaluationScheme()</code> method. We use the cross-validation setting to generate the training and test sets. Then we make predictions on each test set and evaluate the model accuracy.</p><p>Run the following code to generate the training and test sets.</p><p><code class="literal"> n_fold</code> defines the 4-fold cross-validation, that divides the data into 4 sets; 3 training sets and 1 test set:</p><pre class="programlisting">n_fold &lt;- 4 &#13;
 &#13;
</pre><p><code class="literal">items_to_keep</code> defines the minimum number of items to use to generate recommendations:</p><pre class="programlisting">items_to_keep &lt;- 15 &#13;
 &#13;
</pre><p><code class="literal">rating_threshold</code> defines the minimum rating which is considered as a good rating:</p><pre class="programlisting">rating_threshold &lt;- 3 &#13;
 &#13;
</pre><p><code class="literal">evaluationScheme</code> method creates the test sets:</p><pre class="programlisting">eval_sets &lt;- evaluationScheme(data = model_data, method = "cross-validation",k = n_fold, given = items_to_keep, goodRating =rating_threshold) &#13;
size_sets &lt;- sapply(eval_sets@runsTrain, length) &#13;
size_sets &#13;
[1] 2370 2370 2370 2370 &#13;
</pre><p>Set the <code class="literal">model_to_evaluate</code> to set the recommender method to be used. <code class="literal">model_parameters</code> defines the model parameters such as the number of neighbors to be considered while computing the similarity using cosine. For now we will set it as <code class="literal">NULL</code> in order to make the model choose the default values, as follows:</p><pre class="programlisting">model_to_evaluate &lt;- "IBCF" &#13;
model_parameters &lt;- NULL &#13;
</pre><p>Use the <code class="literal">recommender()</code> method to generate the model. Let's understand each parameter of the <code class="literal">recommender()</code> method:</p><p><code class="literal">getData</code> extracts the training data from <code class="literal">eval_sets</code> and passes it on to the <code class="literal">recommender()</code> method as follows:</p><pre class="programlisting">getData(eval_sets,"train") &#13;
2370 x 100 rating matrix of class 'realRatingMatrix' with 139148 ratings &#13;
</pre><p>Since we are using 4-folds cross-validation, the <code class="literal">recommender()</code> method uses the three sets from <code class="literal">eval_sets</code> for training and the remaining one set for testing/ evaluating the model as follows:</p><pre class="programlisting">eval_recommender &lt;- Recommender(data = getData(eval_sets, "train"),method = model_to_evaluate, parameter = model_parameters) &#13;
#setting the number of items to be set for recommendations &#13;
items_to_recommend &lt;- 10 &#13;
</pre><p>Now we use the built model to make predictions on a <code class="literal">"known"</code> dataset from <code class="literal">eval_sets</code>. As seen before, we use the <code class="literal">predict()</code> method to generate the predictions as follows:</p><pre class="programlisting">eval_prediction &lt;- predict(object = eval_recommender, newdata = getData(eval_sets, "known"), n = items_to_recommend, type = "ratings") &#13;
 &#13;
class(eval_prediction) &#13;
[1] "realRatingMatrix" &#13;
attr(,"package") &#13;
[1] "recommenderlab" &#13;
</pre></div><div class="section" title="Model accuracy using metrics"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"/>Model accuracy using metrics</h2></div></div></div><p>Until now, the procedure has been the same as for making the initial predictions, now we will see how to evaluate the model accuracy for the predictions made on the "known" set of test data from <code class="literal">eval_sets</code>. As we saw in the UBCF section, we use the <code class="literal">calcPredictionAccuracy()</code> method to calculate the prediction accuracy.</p><p>We use the <code class="literal">calcPredictionAccuracy()</code> method and pass the <code class="literal">"unknown"</code> dataset available in the <code class="literal">eval_sets</code> as follows:</p><pre class="programlisting">eval_accuracy &lt;- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = TRUE) &#13;
 &#13;
head(eval_accuracy)&#13;
           RMSE      MSE      MAE &#13;
u238   4.625542 21.39564  4.257505 &#13;
u17322  4.953789  24.54003  3.893797 &#13;
u5462  4.685714   21.95591  4.093891 &#13;
u13120   4.977421  24.77472  4.261627 &#13;
u12519   3.875182  15.01703  2.750987 &#13;
u17883 7.660785 58.68762 6.595489 &#13;
</pre><div class="note" title="Note"><h3 class="title"><a id="note11"/>Note</h3><p>Using <code class="literal">byUser = TRUE</code> in the previous method calculates the accuracy for each user. In the table above we can see that for user - <code class="literal">u238</code> the <code class="literal">RMSE</code> is <code class="literal">4.62</code> and <code class="literal">MAE</code> is <code class="literal">4.25</code></p></div><p>If we want to see the accuracy of the whole model, just calculate the mean of each column, that is to say the average for all the users as follows:</p><pre class="programlisting">apply(eval_accuracy,2,mean) &#13;
  RMSE      MSE      MAE  &#13;
 4.45511 21.94246  3.56437  &#13;
</pre><p>By setting <code class="literal">byUser=FALSE</code> we can calculate the model accuracy for the whole model:</p><pre class="programlisting">eval_accuracy &lt;- calcPredictionAccuracy(x = eval_prediction, data = getData(eval_sets, "unknown"), byUser = FALSE)&#13;
 &#13;
eval_accuracy &#13;
     RMSE       MSE       MAE  &#13;
 4.672386 21.831190  3.555721  &#13;
</pre></div><div class="section" title="Model accuracy using plots"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec62"/>Model accuracy using plots</h2></div></div></div><p>Now we can see the model accuracy using Precision-Recall, ROC curves, and precision/recall curves. These curves help us to decide the trade-off between Precision-Recall while choosing the parameters we use for the recommender models, IBCF in our case.</p><p>We use the <code class="literal">evaluate()</code> method and then set the n value which defines the number of nearest neighbors while calculating the similarities between items as follows:</p><p>Running the following evaluate method makes the model run four times for each dataset:</p><pre class="programlisting">results &lt;- evaluate(x = eval_sets, method = model_to_evaluate, n = seq(10,100,10)) &#13;
IBCF run fold/sample [model time/prediction time] &#13;
 1  [0.145sec/0.327sec]  &#13;
 2  [0.139sec/0.32sec]  &#13;
 3  [0.139sec/0.32sec]  &#13;
 4  [0.137sec/0.322sec]  &#13;
</pre><p>Let's see the model accuracy at each fold:</p><pre class="programlisting">results@results[1]</pre><div class="mediaobject"><img src="../Images/image00337.jpeg" alt="Model accuracy using plots"/></div><p style="clear:both; height: 1em;"> </p><p>Let's sum up all the 4-fold results using the following code:</p><pre class="programlisting">columns_to_sum &lt;- c("TP", "FP", "FN", "TN","precision","recall") &#13;
indices_summed &lt;- Reduce("+", getConfusionMatrix(results))[, columns_to_sum] &#13;
</pre><div class="mediaobject"><img src="../Images/image00338.jpeg" alt="Model accuracy using plots"/></div><p style="clear:both; height: 1em;"> </p><p>From the previous table, we can observe that the model accuracy, Precision-Recall values are good for n values of 30 and 40. The same results can be visually inferred using ROC curves and Precision-Recall plots as follows:</p><pre class="programlisting">plot(results, annotate = TRUE, main = "ROC curve") &#13;
</pre><div class="mediaobject"><img src="../Images/image00339.jpeg" alt="Model accuracy using plots"/></div><p style="clear:both; height: 1em;"> </p><pre class="programlisting">plot(results, "prec/rec", annotate = TRUE, main = "Precision-recall") &#13;
</pre><div class="mediaobject"><img src="../Images/image00340.jpeg" alt="Model accuracy using plots"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Parameter tuning for IBCF"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec63"/>Parameter tuning for IBCF</h2></div></div></div><p>While building the IBCF model there are a few places where we can choose the optimal values before we generate recommendations for building a final model:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">We have to choose, optimal number of neighbors for calculating the similarities between items</li><li class="listitem">Similarity method to be used, whether it is the cosine or Pearson method</li></ul></div><p>See the following steps:</p><p>First set the different k-values:</p><pre class="programlisting">vector_k &lt;- c(5, 10, 20, 30, 40)</pre><p>Use <code class="literal">lapply</code> to generate different models using the cosine method and different values of k:</p><pre class="programlisting"> model1 &lt;- lapply(vector_k, function(k,l){ list(name = "IBCF", param = list(method = "cosine", k = k)) })&#13;
names(model1) &lt;- paste0("IBCF_cos_k_", vector_k)&#13;
names(model1) [1] "IBCF_cos_k_5" "IBCF_cos_k_10" "IBCF_cos_k_20" "IBCF_cos_k_30" [5] "IBCF_cos_k_40" #use Pearson method for similarities model2 &lt;- lapply(vector_k, function(k,l){ list(name = "IBCF", param = list(method = "pearson", k = k)) })&#13;
names(model2) &lt;- paste0("IBCF_pea_k_", vector_k)&#13;
names(model2) [1] "IBCF_pea_k_5" "IBCF_pea_k_10" "IBCF_pea_k_20" "IBCF_pea_k_30" [5] "IBCF_pea_k_40" &#13;
#now let's combine all the methods:&#13;
models = append(model1,model2)</pre><div class="mediaobject"><img src="../Images/image00341.jpeg" alt="Parameter tuning for IBCF"/></div><p style="clear:both; height: 1em;"> </p><p>Set the total number of recommendations to be generated:</p><pre class="programlisting">n_recommendations &lt;- c(1, 5, seq(10, 100, 10))</pre><p>Call the evaluate method to the build 4-fold methods:</p><pre class="programlisting"> list_results &lt;- evaluate(x = eval_sets, method = models, n= n_recommendations)&#13;
IBCF run fold/sample [model time/prediction time] 1 [0.139sec/0.311sec] 2 [0.143sec/0.309sec] 3 [0.141sec/0.306sec] 4 [0.153sec/0.312sec]&#13;
IBCF run fold/sample [model time/prediction time] 1 [0.141sec/0.326sec] 2 [0.145sec/0.445sec] 3 [0.147sec/0.387sec] 4 [0.133sec/0.439sec]&#13;
IBCF run fold/sample [model time/prediction time] 1 [0.14sec/0.332sec] 2 [0.16sec/0.327sec] 3 [0.139sec/0.331sec] 4 [0.138sec/0.339sec] IBCF run fold/sample [model time/prediction time] 1 [0.139sec/0.341sec] 2 [0.157sec/0.324sec] 3 [0.144sec/0.327sec] 4 [0.133sec/0.326sec]</pre><p>Now that we have got the results, let's plot and choose the optimal parameters as follows:</p><pre class="programlisting">plot(list_results, annotate = c(1,2), legend = "topleft")  &#13;
title("ROC curve") &#13;
</pre><div class="mediaobject"><img src="../Images/image00342.jpeg" alt="Parameter tuning for IBCF"/></div><p style="clear:both; height: 1em;"> </p><p>From the preceding plot, the best methods are IBCF with cosine similarity with <span class="emphasis"><em>n = 30</em></span>, and the next best is the Pearson method with <span class="emphasis"><em>n = 40</em></span>.</p><p>Let's confirm this with the <code class="literal">Precision-Recall</code> curve as follows:</p><pre class="programlisting">plot(list_results, "prec/rec", annotate = 1, legend = "bottomright") &#13;
title("Precision-recall") &#13;
</pre><div class="mediaobject"><img src="../Images/image00343.jpeg" alt="Parameter tuning for IBCF"/></div><p style="clear:both; height: 1em;"> </p><p>From the above plot we see that the best Precision-Recall ratio is achieved when the number of recommendations = 30 with cosine similarity and <span class="emphasis"><em>n=40</em></span>. Another good model is achieved with the Pearson similarity method and <span class="emphasis"><em>n=10</em></span>.</p></div></div>
<div class="section" title="Collaborative filtering using Python" id="aid-181NK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Collaborative filtering using Python</h1></div></div></div><p>In the previous section we saw implementations of user-based recommender systems and item-based recommender systems using the R package, <code class="literal">recommenderlab</code>. In this section, we see UBCF and IBCF implementation using the Python programming language.</p><p>For this section, we use the MovieLens 100k dataset, which contains 943 user ratings on 1682 movies. Unlike in R, in Python we do not have a proper Python package dedicated to building recommender engines, at least the neighborhood-based recommenders such as user-based/item-based recommenders.</p><p>We have the Crab Python package available but it is not actively supported. So I thought of building a recommender engine using scientific packages in Python such as NumPy, sklearn, and Pandas.</p><div class="section" title="Installing the required packages"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec64"/>Installing the required packages</h2></div></div></div><p>For this section, please make sure you have the following system requirements:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Python 3.5</li><li class="listitem">Pandas 1.9.2 - Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures, and data analysis tools for the Python programming language.</li><li class="listitem">NumPy 1.9.2 - NumPy is the fundamental package for scientific computing with Python.</li><li class="listitem">sklearn 0.16.1</li></ul></div><div class="note" title="Note"><h3 class="title"><a id="tip12"/>Tip</h3><p>The best way to install the preceding packages is to install, Anaconda distribution which will install all the required packages such as Python, Pandas, and Numpy. Anaconda can be found at:<a class="ulink" href="https://www.continuum.io/downloads">https://www.continuum.io/downloads</a></p></div></div><div class="section" title="Data source"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec65"/>Data source</h2></div></div></div><p>The MovieLens 100k data can be downloaded from the following link:</p><p><a class="ulink" href="http://files.grouplens.org/datasets/movielens/ml-100k.zip">http://files.grouplens.org/datasets/movielens/ml-100k.zip</a></p><p>Let's get started with implementing user-based collaborative filtering. Assuming we have downloaded the data into our local system, let's load the data into a Python environment.</p><p>We load the data using the Pandas package and the <code class="literal">read_csv()</code> method by passing two parameters, path and separator as follows:</p><pre class="programlisting">path = "~/udata.csv"&#13;
df = pd.read_csv(path, sep='\t')</pre><p>The data will be loaded as a DataFrame, a table-like data structure that can easily be used for data handling and manipulation tasks.</p><pre class="programlisting">type(df) &#13;
&lt;class 'pandas.core.frame.DataFrame'&gt; &#13;
</pre><p>Let's see the first six results of the data frame to have a look at how data seems to be using the <code class="literal">head()</code> method available in the Pandas DataFrame object:</p><pre class="programlisting">df.head() &#13;
 UserID  ItemId   Rating  Timestamp &#13;
0     196      242       3  881250949 &#13;
1     186      302       3  891717742 &#13;
2      22      377       1  878887116 &#13;
3     244       51       2  880606923 &#13;
4     166      346       1  886397596 &#13;
</pre><p>Let's see the column names of the data frame, <code class="literal">df</code> using the columns attributes. The result of the following code snippet shows that there are four columns: <code class="literal">UserID</code>, <code class="literal">ItemId</code>, <code class="literal">Rating</code>, <code class="literal">Timestamp</code>, and that it is of the object datatype:</p><pre class="programlisting">df.columns &#13;
Index([u'UserID', u'ItemId ', u'Rating', u'Timestamp'], dtype='object') &#13;
</pre><p>Let's see the size of the data frame by calling the shape attribute; we observe that we have 100k records with 4 columns:</p><pre class="programlisting">df.shape &#13;
(100000, 4) &#13;
</pre></div></div>
<div class="section" title="Data exploration"><div class="titlepage" id="aid-190862"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Data exploration</h1></div></div></div><p>In this section, we will explore the MovieLens dataset and also prepare the data required for building collaborative filtering recommendation engines using python.</p><p>Let's see the distribution of ratings using the following code snippet:</p><pre class="programlisting">import matplotlib.pyplot as plt &#13;
plt.hist(df['Rating']) &#13;
</pre><p>From the following image we see that we have more movies with 4 star ratings:</p><div class="mediaobject"><img src="../Images/image00344.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>Using the following code snippet, we shall see the counts of ratings by applying the <code class="literal">groupby()</code> function and the <code class="literal">count()</code> function on DataFrame:</p><div class="mediaobject"><img src="../Images/image00345.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>The following code snippet shows the distribution of movie views. In the following code we apply the <code class="literal">count()</code> function on DataFrame:</p><pre class="programlisting">plt.hist(df.groupby(['ItemId'])['ItemId'].count()) &#13;
</pre><div class="mediaobject"><img src="../Images/image00346.jpeg" alt="Data exploration"/></div><p style="clear:both; height: 1em;"> </p><p>From the previous image, we can observe that the starting ItemId has more ratings than later movies.</p><div class="section" title="Rating matrix representation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec66"/>Rating matrix representation</h2></div></div></div><p>Now that we have explored the data, let's represent the data in a rating matrix form so that we can get started with our original task of building a recommender engine.</p><p>For creating a rating matrix, we make use of NumPy package capabilities such as arrays and row iterations in a matrix. Run the following code to represent the data frame in a rating matrix:</p><div class="note" title="Note"><h3 class="title"><a id="note13"/>Note</h3><p>In the following code, first we extract all the unique user IDs and then we check the length using the shape parameter.</p></div><p>Create a variable of <code class="literal">n_users</code> to find the total number of unique users in the data:</p><pre class="programlisting">n_users = df.UserID.unique().shape[0] &#13;
</pre><p>Create a variable <code class="literal">n_items</code> to find the total number of unique movies in the data:</p><pre class="programlisting">n_items = df['ItemId '].unique().shape[0] &#13;
</pre><p>Print the counts of unique users and movies:</p><pre class="programlisting">print(str(n_users) + ' users') &#13;
943 users &#13;
 &#13;
print(str(n_items) + ' movies') &#13;
1682 movies &#13;
</pre><p>Create a zero value matrix of size (<span class="emphasis"><em>n_users x n_items</em></span>) to store the ratings in the cell of the matrix ratings:</p><pre class="programlisting">ratings = np.zeros((n_users, n_items)) &#13;
</pre><p>For each tuple in the DataFrame, <code class="literal">df</code> extracts the information from each column of the row and stores it in the rating matrix cell value as follows:</p><pre class="programlisting">for  row in df.itertuples(): &#13;
ratings[row[1]-1, row[2]-1] = row[3] &#13;
</pre><p>Run the loop and the whole DataFrame movie ratings information will be stored in the matrix ratings of the <code class="literal">numpy.ndarray</code> type as follows:</p><pre class="programlisting">type(ratings) &#13;
&lt;type 'numpy.ndarray'&gt; &#13;
</pre><p>Now let's see the dimensions of the multidimensional array 'ratings' using the shape attribute as follows:</p><pre class="programlisting">ratings.shape &#13;
(943, 1682) &#13;
</pre><p>Let's see the sample data for how a ratings multidimensional array looks by running the following code:</p><pre class="programlisting">ratings &#13;
array([[ 5.,  3.,  4., ...,  0.,  0.,  0.], &#13;
       [ 4.,  0.,  0., ...,  0.,  0.,  0.], &#13;
       [ 0.,  0.,  0., ...,  0.,  0.,  0.], &#13;
       ...,  &#13;
       [ 5.,  0.,  0., ...,  0.,  0.,  0.], &#13;
       [ 0.,  0.,  0., ...,  0.,  0.,  0.], &#13;
       [ 0.,  5.,  0., ...,  0.,  0.,  0.]]) &#13;
</pre><p>We observe that the rating matrix is sparse as we see a lot of zeros in the data. Let's determine the <code class="literal">sparsity</code> in the data, by running the following code:</p><pre class="programlisting">sparsity = float(len(ratings.nonzero()[0])) &#13;
sparsity /= (ratings.shape[0] * ratings.shape[1]) &#13;
sparsity *= 100 &#13;
print('Sparsity: {:4.2f}%'.format(sparsity)) &#13;
Sparsity: 6.30% &#13;
</pre><p>We observe that the sparsity is <code class="literal">6.3%</code> that is to say that we only have rating information for <code class="literal">6.3%</code> of the data and for the others it is just zeros. Also please note that, the <code class="literal">0</code> value we see in the rating matrix doesn't represent the rating given by the user, it just means that they are empty.</p></div><div class="section" title="Creating training and test sets"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec67"/>Creating training and test sets</h2></div></div></div><p>Now that we have a ratings matrix, let's create a training set and test set to build the recommender model using a training set and evaluate the model using a test set.</p><p>To divide the data into training and test sets, we use the <code class="literal">sklearn</code> package's capabilities. Run the following code to create training and test sets:</p><p>Load the <code class="literal">train_test_split</code> module into the python environment using the following import functionality:</p><pre class="programlisting">from sklearn.cross_validation import train_test_split</pre><p>Call the <code class="literal">train_test_split()</code> method with a test size of <code class="literal">0.33</code> and random seed of <code class="literal">42</code>:</p><pre class="programlisting"> ratings_train, ratings_test = train_test_split(ratings,test_size=0.33, random_state=42)</pre><p>Let's see the dimensions of the train set:</p><pre class="programlisting"> ratings_train.shape (631, 1682) &#13;
#let's see the dimensions of the test set &#13;
ratings_test.shape (312, 1682)</pre><p>For user-based collaborative filtering, we predict that a user's rating for an item is given by the weighted sum of all other users' ratings for that item, where the weighting is the cosine similarity between each user and the input user.</p></div><div class="section" title="The steps for building a UBCF"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec68"/>The steps for building a UBCF</h2></div></div></div><p>The steps for building a UBCF are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Creating a similarity matrix between the users</li><li class="listitem">Predicting the unknown rating value of item <span class="emphasis"><em>i</em></span> for an active user <span class="emphasis"><em>u</em></span> by calculating the weighted sum of all the users' ratings for the item.<div class="note" title="Note"><h3 class="title"><a id="tip14"/>Tip</h3><p>Here the weighting is the cosine similarity calculated in the previous step between the user and neighboring users.</p></div></li><li class="listitem">Recommending the new items to the users.</li></ul></div></div><div class="section" title="User-based similarity calculation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec69"/>User-based similarity calculation</h2></div></div></div><p>The next step is to create pairwise similarity calculations for each user in the rating matrix, that is to say we have to calculate the similarity of each user with all the other users in the matrix. The similarity calculation we choose here is cosine similarity. For this, we make use of pairwise distance capabilities to calculate the cosine similarity available in the <code class="literal">sklearn</code> package as follows:</p><div class="mediaobject"><img src="../Images/image00347.jpeg" alt="User-based similarity calculation"/></div><p style="clear:both; height: 1em;"> </p><p>Let's see a sample dataset of the distance matrix:</p><pre class="programlisting">dist_out &#13;
</pre><div class="mediaobject"><img src="../Images/image00348.jpeg" alt="User-based similarity calculation"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Predicting the unknown ratings for an active user"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec70"/>Predicting the unknown ratings for an active user</h2></div></div></div><p>As previously mentioned, the unknown values can be calculated for all the users by taking the dot product between the distance matrix and the rating matrix and then normalizing the data with the number of ratings as follows:</p><pre class="programlisting">user_pred = dist_out.dot(ratings_train) / np.array([np.abs(dist_out).sum(axis=1)]).T &#13;
</pre><p>Now that we have predicted the unknown ratings for use in the training set, let's define a function to check the error or performance of the model. The following code defines a function for calculating the root mean square error (RMSE) by taking the predicted values and original values. We use <code class="literal">sklearn</code> capabilities for calculating RMSE as follows:</p><pre class="programlisting">from sklearn.metrics import mean_squared_error &#13;
def get_mse(pred, actual): &#13;
    #Ignore nonzero terms. &#13;
    pred = pred[actual.nonzero()].flatten() &#13;
    actual = actual[actual.nonzero()].flatten() &#13;
    return mean_squared_error(pred, actual) &#13;
</pre><p>We call the <code class="literal">get_mse()</code> method to check the model prediction error rate as follows:</p><pre class="programlisting">get_mse(user_pred, ratings_train) &#13;
7.8821939915510031 &#13;
</pre><p>We see that the model accuracy or RMSE is <code class="literal">7.8</code>. Now let's run the same <code class="literal">get_mse()</code> method on the test data and check the accuracy as follows:</p><pre class="programlisting">get_mse(user_pred, ratings_test) &#13;
8.9224954316965484 &#13;
</pre></div></div>
<div class="section" title="User-based collaborative filtering with the k-nearest neighbors" id="aid-19UOO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>User-based collaborative filtering with the k-nearest neighbors</h1></div></div></div><p>If we observe the RMSE values in the above model, we can see that the error is a bit higher. The reason may be that we have chosen all the users' rating information while making the predictions. Instead of considering all the users, let's consider only the top-N similar users' ratings information and then make the predictions. This may result in improving the model accuracy by eliminating some biases in the data.</p><p>To explain in a more elaborate way; in the previous code we predicted the ratings of the users by taking the weighted sum of the ratings of all users, instead we first chose the top-N similar users for each user and then the ratings were calculated by considering the weighted sum of the ratings of these top-N users.</p><div class="section" title="Finding the top-N nearest neighbors"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec71"/>Finding the top-N nearest neighbors</h2></div></div></div><p>Firstly, for computational easiness, we shall choose the top five similar users by setting a variable, <span class="emphasis"><em>k</em></span>.</p><p><span class="emphasis"><em>k=5</em></span></p><p>We use the k-nearest neighbors method to choose the top five nearest neighbors for an active user. We will see this in action shortly. We choose sklearn.knn capabilities for this task as follows:</p><pre class="programlisting">from sklearn.neighbors import NearestNeighbors &#13;
 &#13;
</pre><p>Define the <code class="literal">NearestNeighbors</code> object by passing k and the similarity method as parameters:</p><pre class="programlisting">neigh = NearestNeighbors(k,'cosine') &#13;
 &#13;
</pre><p>Fit the training data to the <code class="literal">nearestNeighbor</code> object:</p><pre class="programlisting">neigh.fit(ratings_train) &#13;
 &#13;
</pre><p>Calculate the top five similar users for each user and their similarity values, that is the distance values between each pair of users:</p><pre class="programlisting">top_k_distances,top_k_users = neigh.kneighbors(ratings_train, return_distance=True) &#13;
 &#13;
</pre><p>We can observe below that the resultant <code class="literal">top_k_distances</code> ndarray contains similarity values and top five similar users for each users in the training set:</p><pre class="programlisting">top_k_distances.shape &#13;
(631, 5) &#13;
top_k_users.shape &#13;
(631, 5) &#13;
 &#13;
</pre><p>Let's see the top five users that are similar to user 1 in the training set:</p><pre class="programlisting">top_k_users[0] &#13;
array([  0,  82, 511, 184, 207], dtype=int64) &#13;
</pre><p>The next step would be to choose only the top five users for each user and use their rating information while predicting the ratings using the weighted sum of all of the ratings of these top five similar users.</p><p>Run the following code to predict the unknown ratings in the training data:</p><pre class="programlisting">user_pred_k = np.zeros(ratings_train.shape) &#13;
for i in range(ratings_train.shape[0]): &#13;
    user_pred_k[i,:] =   top_k_distances[i].T.dot(ratings_train[top_k_users][i]) &#13;
/np.array([np.abs(top_k_distances[i].T).sum(axis=0)]).T &#13;
</pre><p>Let's see the data predicted by the model as follows:</p><pre class="programlisting">user_pred_k.shape &#13;
(631, 1682) &#13;
 &#13;
user_pred_k &#13;
</pre><p>The following image displays the results for <code class="literal">user_pred_k</code>:</p><div class="mediaobject"><img src="../Images/image00349.jpeg" alt="Finding the top-N nearest neighbors"/></div><p style="clear:both; height: 1em;"> </p><p>Now let's see if the model has improved or not. Run the get_mse() method defined earlier as follows:</p><pre class="programlisting">get_mse(user_pred_k, ratings_train) &#13;
8.9698490022546036 &#13;
get_mse(user_pred_k, ratings_test) &#13;
11.528758029255446 &#13;
</pre></div></div>
<div class="section" title="Item-based recommendations" id="aid-1AT9A1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Item-based recommendations</h1></div></div></div><p>IBCF is very similar to UBCF but with very minor changes in how we use the rating matrix.</p><p>The first step is to calculate the similarities between movies, as follows:</p><p>Since we have to calculate the similarity between movies, we use movie count as <code class="literal">k</code> instead of user count:</p><pre class="programlisting">k = ratings_train.shape[1] 
neigh = NearestNeighbors(k,'cosine') 
</pre><p>We fit the transpose of the rating matrix to the <code class="literal">NearestNeighbors</code> object:</p><pre class="programlisting">neigh.fit(ratings_train.T) 
</pre><p>Calculate the cosine similarity distance between each movie pair:</p><pre class="programlisting">top_k_distances,top_k_users = neigh.kneighbors(ratings_train.T, return_distance=True) 
top_k_distances.shape 
(1682, 1682) 
</pre><p>The next step is to predict the movie ratings using the following code:</p><pre class="programlisting">item__pred = ratings_train.dot(top_k_distances) / np.array([np.abs(top_k_distances).sum(axis=1)]) 
item__pred.shape 
(631, 1682) 
item__pred 
</pre><p>The following image shows the result for <code class="literal">item_pred</code>:</p><div class="mediaobject"><img src="../Images/image00350.jpeg" alt="Item-based recommendations"/></div><p style="clear:both; height: 1em;"> </p><div class="section" title="Evaluating the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec72"/>Evaluating the model</h2></div></div></div><p>Now let's evaluate the model using the <code class="literal">get_mse()</code> method we have defined by passing the prediction ratings and the training and test set as follows:</p><pre class="programlisting">get_mse(item_pred, ratings_train) 
11.130000188318895 
get_mse(item_pred,ratings_test) 
12.128683035513326 
</pre></div><div class="section" title="The training model for k-nearest neighbors"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec73"/>The training model for k-nearest neighbors</h2></div></div></div><p>Run the following code to calculate the distance matrix for the top 40 nearest neighbors and then calculate the weighted sum of ratings by the top 40 users for all the movies. If we closely observe the code, it is very similar to what we have done for UBCF. Instead of passing <code class="literal">ratings_train</code> as is, we transpose the data matrix and pass to the previous code as follows:</p><pre class="programlisting">k = 40 
neigh2 = NearestNeighbors(k,'cosine') 
neigh2.fit(ratings_train.T) 
top_k_distances,top_k_movies = neigh2.kneighbors(ratings_train.T, return_distance=True) 
 
#rating prediction - top k user based  
pred = np.zeros(ratings_train.T.shape) 
for i in range(ratings_train.T.shape[0]): 
    pred[i,:] = top_k_distances[i].dot(ratings_train.T[top_k_users][i])/np.array([np.abs(top_k_distances[i]).sum(axis=0)]).T 
</pre></div><div class="section" title="Evaluating the model"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec74"/>Evaluating the model</h2></div></div></div><p>The follow code snippet calculates the mean squared error for the training and test set. We can observe that the training error is 11.12 whereas the test error is 12.12.</p><pre class="programlisting">get_mse(item_pred_k, ratings_train) 
11.130000188318895 
get_mse(item_pred_k,ratings_test) 
12.128683035513326 
</pre></div></div>
<div class="section" title="Summary" id="aid-1BRPS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Summary</h1></div></div></div><p>In this chapter, we have explored building collaborative filtering approaches such as user-based and item-based approaches in R and Python, the popular data mining programming languages. The recommendation engines are built on MovieLens, and Jester5K datasets available online.</p><p>We have learnt about how to build the model, choose data, explore the data, create training and test sets, and evaluate the models using metrics such as RMSE, Precision-Recall, and ROC curves. Also, we have seen how to tune parameters for model improvements.</p><p>In the next chapter, we will be covering personalized recommendation engines such as content-based recommendation engines and context-aware recommendation engines using R and Python.</p></div></body></html>