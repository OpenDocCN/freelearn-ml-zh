["```py\nimport nltk\nnltk.download('punkt')\n```", "```py\ntext = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\" \n```", "```py\n# Sentence tokenization \nfrom nltk.tokenize import sent_tokenize \n```", "```py\nsent_tokenize_list = sent_tokenize(text) \n```", "```py\nprint(\"Sentence tokenizer:\")\nprint(sent_tokenize_list) \n```", "```py\n# Create a new word tokenizer \nfrom nltk.tokenize import word_tokenize \n\nprint(\"Word tokenizer:\")\nprint(word_tokenize(text)) \n```", "```py\n# Create a new WordPunct tokenizer \nfrom nltk.tokenize import WordPunctTokenizer \n\nword_punct_tokenizer = WordPunctTokenizer() \nprint(\"Word punct tokenizer:\")\nprint(word_punct_tokenizer.tokenize(text)) \n```", "```py\nSentence tokenizer:\n['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n\nWord tokenizer:\n['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n\nWord punct tokenizer:\n['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n```", "```py\nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.stem.lancaster import LancasterStemmer \nfrom nltk.stem.snowball import SnowballStemmer \n```", "```py\nwords = ['table', 'probably', 'wolves', 'playing', 'is',  \n        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision'] \n```", "```py\n# Compare different stemmers stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL'] \n```", "```py\nstemmer_porter = PorterStemmer() stemmer_lancaster = LancasterStemmer() stemmer_snowball = SnowballStemmer('english') \n```", "```py\nformatted_row = '{:>16}' * (len(stemmers) + 1) \nprint('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n```", "```py\nfor word in words: stemmed_words = [stemmer_porter.stem(word), stemmer_lancaster.stem(word), stemmer_snowball.stem(word)] print(formatted_row.format(word, *stemmed_words)) \n```", "```py\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer \n```", "```py\nwords = ['table', 'probably', 'wolves', 'playing', 'is', 'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision'] \n```", "```py\n# Compare different lemmatizers \nlemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER'] \n```", "```py\nlemmatizer_wordnet = WordNetLemmatizer() \n```", "```py\nformatted_row = '{:>24}' * (len(lemmatizers) + 1) \nprint('\\n', formatted_row.format('WORD', *lemmatizers), '\\n') \n```", "```py\nfor word in words: \n    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'), \n           lemmatizer_wordnet.lemmatize(word, pos='v')] \n    print(formatted_row.format(word, *lemmatized_words)) \n```", "```py\nimport numpy as np \nnltk.download('brown')\nfrom nltk.corpus import brown \n```", "```py\n# Split a text into chunks  \ndef splitter(data, num_words): \n    words = data.split(' ') \n    output = [] \n```", "```py\n    cur_count = 0 \n    cur_words = [] \n```", "```py\n    for word in words: \n        cur_words.append(word) \n        cur_count += 1 \n```", "```py\n        if cur_count == num_words: \n            output.append(' '.join(cur_words)) \n            cur_words = [] \n            cur_count = 0 \n```", "```py\n    output.append(' '.join(cur_words) ) \n\n    return output \n```", "```py\nif __name__=='__main__': \n    # Read the data from the Brown corpus \n    data = ' '.join(brown.words()[:10000]) \n```", "```py\n    # Number of words in each chunk  \n    num_words = 1700 \n```", "```py\n    chunks = [] \n    counter = 0 \n```", "```py\n    text_chunks = splitter(data, num_words) \n\n    print(\"Number of text chunks =\", len(text_chunks)) \n```", "```py\nNumber of text chunks = 6\n```", "```py\nimport numpy as np \nfrom nltk.corpus import brown \nfrom chunking import splitter \n```", "```py\nif __name__=='__main__': \n    # Read the data from the Brown corpus \n    data = ' '.join(brown.words()[:10000]) \n```", "```py\n    # Number of words in each chunk  \n    num_words = 2000 \n\n    chunks = [] \n    counter = 0 \n\n    text_chunks = splitter(data, num_words) \n```", "```py\n    for text in text_chunks: \n        chunk = {'index': counter, 'text': text} \n        chunks.append(chunk) \n        counter += 1 \n```", "```py\n    # Extract document term matrix \n    from sklearn.feature_extraction.text import CountVectorizer \n```", "```py\n    vectorizer = CountVectorizer(min_df=5, max_df=.95) \n    doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks]) \n```", "```py\n    vocab = np.array(vectorizer.get_feature_names()) \n    print(\"Vocabulary:\")\n    print(vocab)\n```", "```py\n    print(\"Document term matrix:\") \n    chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4'] \n```", "```py\n    formatted_row = '{:>12}' * (len(chunk_names) + 1) \n    print('\\n', formatted_row.format('Word', *chunk_names), '\\n') \n```", "```py\n    for word, item in zip(vocab, doc_term_matrix.T): \n        # 'item' is a 'csr_matrix' data structure \n        output = [str(x) for x in item.data] \n        print(formatted_row.format(word, *output)) \n```", "```py\nVocabulary:\n['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups \n```", "```py\ncategory_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', \n 'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', \n 'sci.space': 'Space'}\n```", "```py\ntraining_data = fetch_20newsgroups(subset='train',  \n        categories=category_map.keys(), shuffle=True, random_state=7) \n```", "```py\n# Feature extraction \nfrom sklearn.feature_extraction.text import CountVectorizer \n```", "```py\nvectorizer = CountVectorizer() X_train_termcounts = vectorizer.fit_transform(training_data.data) print(\"Dimensions of training data:\", X_train_termcounts.shape)\n```", "```py\n# Training a classifier \nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.feature_extraction.text import TfidfTransformer \n```", "```py\ninput_data = [ \"The curveballs of right handed pitchers tend to curve to the left\", \"Caesar cipher is an ancient form of encryption\", \"This two-wheeler is really good on slippery roads\" \n] \n```", "```py\n# tf-idf transformer \ntfidf_transformer = TfidfTransformer() \nX_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts) \n```", "```py\n# Multinomial Naive Bayes classifier \nclassifier = MultinomialNB().fit(X_train_tfidf, training_data.target) \n```", "```py\nX_input_termcounts = vectorizer.transform(input_data) \n```", "```py\nX_input_tfidf = tfidf_transformer.transform(X_input_termcounts) \n```", "```py\n# Predict the output categories \npredicted_categories = classifier.predict(X_input_tfidf) \n```", "```py\n# Print the outputs \nfor sentence, category in zip(input_data, predicted_categories): \n    print('\\nInput:', sentence, '\\nPredicted category:', \\\n            category_map[training_data.target_names[category]])\n```", "```py\nDimensions of training data: (2968, 40605)\n\nInput: The curveballs of right handed pitchers tend to curve to the left \nPredicted category: Baseball\n\nInput: Caesar cipher is an ancient form of encryption \nPredicted category: Cryptography\n\nInput: This two-wheeler is really good on slippery roads \nPredicted category: Motorcycles\n```", "```py\nimport nltk\nnltk.download('names')\n\nimport random from nltk.corpus import names from nltk import NaiveBayesClassifier from nltk.classify import accuracy as nltk_accuracy \n```", "```py\n# Extract features from the input word def gender_features(word, num_letters=2): return {'feature': word[-num_letters:].lower()} \n```", "```py\nif __name__=='__main__': \n    # Extract labeled names \n    labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n            [(name, 'female') for name in names.words('female.txt')]) \n```", "```py\n    random.seed(7) \n    random.shuffle(labeled_names) \n```", "```py\n    input_names = ['Leonardo', 'Amy', 'Sam'] \n```", "```py\n    # Sweeping the parameter space \n    for i in range(1, 5): \n        print('\\nNumber of letters:', i) \n        featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names] \n```", "```py\n        train_set, test_set = featuresets[500:], featuresets[:500] \n```", "```py\n        classifier = NaiveBayesClassifier.train(train_set) \n```", "```py\n        # Print classifier accuracy \n        print('Accuracy ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%')) \n\n# Predict outputs for new inputs \n        for name in input_names: \n            print(name, '==>', classifier.classify(gender_features(name, i))) \n```", "```py\nNumber of letters: 1\nAccuracy ==> 76.2%\nLeonardo ==> male\nAmy ==> female\nSam ==> male\n\nNumber of letters: 2\nAccuracy ==> 78.6%\nLeonardo ==> male\nAmy ==> female\nSam ==> male\n\nNumber of letters: 3\nAccuracy ==> 76.6%\nLeonardo ==> male\nAmy ==> female\nSam ==> female\n\nNumber of letters: 4\nAccuracy ==> 70.8%\nLeonardo ==> male\nAmy ==> female\nSam ==> female\n```", "```py\nimport nltk.classify.util \nfrom nltk.classify import NaiveBayesClassifier \nfrom nltk.corpus import movie_reviews \n```", "```py\ndef extract_features(word_list): \n    return dict([(word, True) for word in word_list]) \n```", "```py\nif __name__=='__main__': \n    # Load positive and negative reviews   \n    positive_fileids = movie_reviews.fileids('pos') \n    negative_fileids = movie_reviews.fileids('neg') \n```", "```py\n    features_positive = [(extract_features(movie_reviews.words(fileids=[f])),  \n            'Positive') for f in positive_fileids] \n    features_negative = [(extract_features(movie_reviews.words(fileids=[f])),  \n            'Negative') for f in negative_fileids] \n```", "```py\n    # Split the data into train and test (80/20) \n    threshold_factor = 0.8 \n    threshold_positive = int(threshold_factor * len(features_positive)) \n    threshold_negative = int(threshold_factor * len(features_negative)) \n```", "```py\n    features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative] \n    features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]   \n    print(\"Number of training datapoints:\", len(features_train))\n    print(\"Number of test datapoints:\", len(features_test))\n```", "```py\n    # Train a Naive Bayes classifier \n    classifier = NaiveBayesClassifier.train(features_train) \n    print(\"Accuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n```", "```py\n    print(\"Top 10 most informative words:\")\n    for item in classifier.most_informative_features()[:10]:\n        print(item[0])\n```", "```py\n    # Sample input reviews \n    input_reviews = [ \n        \"It is an amazing movie\",  \n        \"This is a dull movie. I would never recommend it to anyone.\", \n        \"The cinematography is pretty great in this movie\",  \n        \"The direction was terrible and the story was all over the place\"  \n    ] \n```", "```py\n    print(\"Predictions:\") \n    for review in input_reviews: \n        print(\"Review:\", review) \n        probdist = classifier.prob_classify(extract_features(review.split())) \n        pred_sentiment = probdist.max() \n```", "```py\n        print(\"Predicted sentiment:\", pred_sentiment) \n        print(\"Probability:\", round(probdist.prob(pred_sentiment), 2))\n```", "```py\nNumber of training datapoints: 1600\nNumber of test datapoints: 400\nAccuracy of the classifier: 0.735\n```", "```py\nTop 10 most informative words:\n\noutstanding\ninsulting\nvulnerable\nludicrous\nuninvolving\nastounding\navoids\nfascination\nseagal\nanna\n```", "```py\nPredictions:\n\nReview: It is an amazing movie\nPredicted sentiment: Positive\nProbability: 0.61\n\nReview: This is a dull movie. I would never recommend it to anyone.\nPredicted sentiment: Negative\nProbability: 0.77\n\nReview: The cinematography is pretty great in this movie\nPredicted sentiment: Positive\nProbability: 0.67\n\nReview: The direction was terrible and the story was all over the place\nPredicted sentiment: Negative\nProbability: 0.63\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer   \nfrom nltk.stem.snowball import SnowballStemmer \nfrom gensim import models, corpora \nfrom nltk.corpus import stopwords \n```", "```py\n# Load input data \ndef load_data(input_file): \n    data = [] \n    with open(input_file, 'r') as f: \n        for line in f.readlines(): \n            data.append(line[:-1]) \n\n    return data \n```", "```py\n# Class to preprocess text \nclass Preprocessor(object): \n    # Initialize various operators \n    def __init__(self): \n        # Create a regular expression tokenizer \n        self.tokenizer = RegexpTokenizer(r'\\w+') \n```", "```py\n        # get the list of stop words  \n        self.stop_words_english = stopwords.words('english') \n```", "```py\n        # Create a Snowball stemmer  \n        self.stemmer = SnowballStemmer('english') \n```", "```py\n    # Tokenizing, stop word removal, and stemming \n    def process(self, input_text): \n        # Tokenize the string \n        tokens = self.tokenizer.tokenize(input_text.lower()) \n```", "```py\n        # Remove the stop words  \n        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english] \n```", "```py\n        # Perform stemming on the tokens  \n        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords] \n```", "```py\n        return tokens_stemmed \n```", "```py\nif __name__=='__main__': \n    # File containing linewise input data  \n    input_file = 'data_topic_modeling.txt' \n\n    # Load data \n    data = load_data(input_file) \n```", "```py\n    # Create a preprocessor object \n    preprocessor = Preprocessor() \n```", "```py\n    # Create a list for processed documents \n    processed_tokens = [preprocessor.process(x) for x in data] \n```", "```py\n    # Create a dictionary based on the tokenized documents \n    dict_tokens = corpora.Dictionary(processed_tokens) \n```", "```py\n    # Create a document term matrix \n    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens] \n```", "```py\n    # Generate the LDA model based on the corpus we just created \n    num_topics = 2 \n    num_words = 4 \n\n    ldamodel = models.ldamodel.LdaModel(corpus,  \n            num_topics=num_topics, id2word=dict_tokens, passes=25) \n```", "```py\n    print(\"Most contributing words to the topics:\")\n    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n        print (\"Topic\", item[0], \"==>\", item[1])\n```", "```py\nMost contributing words to the topics:\nTopic 0 ==> 0.057*\"need\" + 0.034*\"order\" + 0.034*\"work\" + 0.034*\"modern\"\nTopic 1 ==> 0.057*\"need\" + 0.034*\"train\" + 0.034*\"club\" + 0.034*\"develop\"\n```", "```py\nimport spacy\n```", "```py\nnlp = spacy.load('en_core_web_sm')\n```", "```py\n Text = nlp(u'We catched fish, and talked, and we took a swim now and then to keep off sleepiness')\n```", "```py\nfor token in Text:\n    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n          token.shape_, token.is_alpha, token.is_stop) \n```", "```py\nWe -PRON- PRON PRP nsubj Xx True False\ncatched catch VERB VBD ROOT xxxx True False\nfish fish NOUN NN dobj xxxx True False\n, , PUNCT , punct , False False\nand and CCONJ CC cc xxx True True\ntalked talk VERB VBD conj xxxx True False\n, , PUNCT , punct , False False\nand and CCONJ CC cc xxx True True\nwe -PRON- PRON PRP nsubj xx True True\ntook take VERB VBD conj xxxx True False\na a DET DT det x True True\nswim swim NOUN NN dobj xxxx True False\nnow now ADV RB advmod xxx True True\nand and CCONJ CC cc xxx True True\nthen then ADV RB advmod xxxx True True\nto to PART TO aux xx True True\nkeep keep VERB VB conj xxxx True True\noff off PART RP prt xxx True True\nsleepiness sleepiness NOUN NN dobj xxxx True False\n```", "```py\n$ python -m spacy download en\n```", "```py\nimport gensim\nfrom nltk.corpus import abc\n```", "```py\nmodel= gensim.models.Word2Vec(abc.sents())\n```", "```py\n X= list(model.wv.vocab)\n```", "```py\ndata=model.wv.most_similar('science')\n```", "```py\nprint(data)\n```", "```py\n[('law', 0.938495397567749), ('general', 0.9232532382011414), ('policy', 0.9198083877563477), ('agriculture', 0.918685793876648), ('media', 0.9151924252510071), ('discussion', 0.9143469929695129), ('practice', 0.9138249754905701), ('reservoir', 0.9102856516838074), ('board', 0.9069126844406128), ('tight', 0.9067160487174988)]\n```", "```py\nimport nltk\nnltk.download('abc')\n```", "```py\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n```", "```py\ndf = pd.read_csv('spam.csv', sep=',',header=None, encoding='latin-1')\n```", "```py\nX_train_raw, X_test_raw, y_train, y_test = train_test_split(df[1],df[0])\n```", "```py\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train_raw)\n```", "```py\nclassifier = LogisticRegression(solver='lbfgs', multi_class='multinomial')\nclassifier.fit(X_train, y_train)\n```", "```py\nX_test = vectorizer.transform( ['Customer Loyalty Offer:The NEW Nokia6650 Mobile from ONLY å£10 at TXTAUCTION!', 'Hi Dear how long have we not heard.'] )\n```", "```py\npredictions = classifier.predict(X_test)\nprint(predictions)\n```", "```py\n['spam' 'ham']\n```"]