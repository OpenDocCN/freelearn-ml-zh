<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Building Neural Networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Building Neural Networks</h1></div></div></div><p>In this chapter, we will introduce <span class="strong"><strong>Artificial Neural Networks</strong></span> (<span class="strong"><strong>ANNs</strong></span>). <a id="id297" class="indexterm"/>We will study the basic representation of ANNs and then discuss several ANN models that can be used in both supervised and unsupervised machine learning problems. We also introduce the <span class="strong"><strong>Enclog </strong></span>Clojure library<a id="id298" class="indexterm"/> to build ANNs.</p><p>Neural networks are well suited for finding patterns in some given data and have several practical applications, such as handwriting recognition and machine vision, in computing. ANNs are often combined or interconnected to model a given problem. Interestingly, they can be applied to several machine learning problems, such as regression and classification. ANNs have applications in several areas in computing and are not restricted to the scope of machine learning.</p><p>
<span class="strong"><strong>Unsupervised learning</strong></span><a id="id299" class="indexterm"/> is a form of machine learning in which the given training data doesn't contain any information about which class a given sample of input belongs to. As the training data is <span class="emphasis"><em>unlabeled</em></span>, an unsupervised learning algorithm must determine the various categories in the given data completely on its own. Generally, this is done by seeking out similarities between different pieces of data and then grouping the data into several categories. This technique is called <span class="strong"><strong>cluster analysis,</strong></span><a id="id300" class="indexterm"/> and we shall study more about this methodology in the following chapters. ANNs are used in unsupervised machine learning techniques mostly due to their ability to quickly recognize patterns in some unlabeled data. This specialized form of unsupervised learning<a id="id301" class="indexterm"/> exhibited by ANNs is termed as<a id="id302" class="indexterm"/> <span class="strong"><strong>competitive learning</strong></span>.</p><p>An interesting fact about ANNs is that they are modeled from the structure and behavior of the central nervous system of higher-order animals that demonstrate learning capabilities.</p><div class="section" title="Understanding nonlinear regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Understanding nonlinear regression</h1></div></div></div><p>By this time, the reader must be aware of the fact that the gradient descent algorithm can be used to estimate both linear and logistic regression models for regression and classification problems. An obvious question would be: what is the need of neural networks when we can use <a id="id303" class="indexterm"/>gradient descent to estimate linear regression and logistic regression models from the training data? To understand the necessity of ANNs, we must first understand <span class="emphasis"><em>nonlinear regression</em></span>.</p><p>Let's assume that we have a single feature variable <span class="emphasis"><em>X</em></span> and a dependent variable <span class="emphasis"><em>Y</em></span> that varies with <span class="emphasis"><em>X</em></span>, as shown in the following plot:</p><div class="mediaobject"><img src="graphics/4351OS_04_01.jpg" alt="Understanding nonlinear regression"/></div><p>As illustrated in the preceding plot, it's hard, if not impossible, to model the dependent variable <span class="emphasis"><em>Y</em></span> as a linear equation of the independent variable <span class="emphasis"><em>X</em></span>. We could model the dependent variable <span class="emphasis"><em>Y</em></span> to be a high-order polynomial equation of the dependent variable <span class="emphasis"><em>X</em></span>, thus converting the problem into the standard form of linear regressions. Hence, the dependent variable <span class="emphasis"><em>Y</em></span> is said to vary nonlinearly with the independent variable <span class="emphasis"><em>X</em></span>. Of course, there is also a good chance that data cannot be modeled using a polynomial function either.</p><p>It can also be shown that calculating the weights or coefficients of all the terms in a polynomial function using gradient descent has a time complexity of <span class="inlinemediaobject"><img src="graphics/4351OS_04_02.jpg" alt="Understanding nonlinear regression"/></span>, where <span class="emphasis"><em>n</em></span> is the number of features in the training data. Similarly, the algorithmic complexity of calculating the coefficients of all the terms in a third-order polynomial equation is <span class="inlinemediaobject"><img src="graphics/4351OS_04_03.jpg" alt="Understanding nonlinear regression"/></span>. It's apparent that the time complexity of gradient descent <a id="id304" class="indexterm"/>increases geometrically with the number of features of the model. Thus, gradient descent on its own is not efficient enough to model nonlinear regression models with a large number of features.</p><p>ANNs, on the other hand, are very efficient at modeling nonlinear regression models of data with a high number of features. We will now study the foundational ideas of ANNs and several ANN models that can be used in supervised and unsupervised learning problems.</p></div></div>
<div class="section" title="Representing neural networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Representing neural networks</h1></div></div></div><p>ANNs are modeled from the behavior <a id="id305" class="indexterm"/>of the central nervous system of organisms, such as mammals and reptiles, that are capable of learning. The central nervous system of these organisms comprises the organism's brain, spinal cord, and a network of supporting neural tissues. The brain processes information and generates electric signals that are transported through the network of neural fibers to the various organs of the organism. Although the organism's brain performs a lot of complex processing and control, it is actually a collection of neurons. The actual processing of sensory signals, however, is performed by several complex combinations of these neurons. Of course, each neuron is capable of processing an extremely small portion of the information processed by the brain. The brain actually functions by routing electrical signals from the various sensory organs of the body to its motor organs through this complex network of neurons. An individual neuron has a cellular structure as illustrated in the following diagram:</p><div class="mediaobject"><img src="graphics/4351OS_04_04.jpg" alt="Representing neural networks"/></div><p>A neuron has several dendrites close to the nucleus of the cell and a single <span class="emphasis"><em>axon</em></span> that transports signals from the nucleus of the cell. The dendrites are used to receive signals from other neurons and can be thought of as the input to the neuron. Similarly, the axon of the neuron is analogous to the output of the neuron. The neuron can thus be mathematically represented as a <a id="id306" class="indexterm"/>function that processes several inputs and produces a single output.</p><p>Several of these neurons are interconnected, and this network is termed as a <span class="strong"><strong>neural network</strong></span>. A neuron essentially performs its function by relaying weak electrical signals from and to other neurons. The interconnecting space between two neurons is called<a id="id307" class="indexterm"/> a <span class="strong"><strong>synapse</strong></span>.</p><p>An ANN comprises several interconnected neurons. Each neuron can be represented by a mathematical function that consumes several input values and produces an output value, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/4351OS_04_05.jpg" alt="Representing neural networks"/></div><p>A single neuron can be illustrated by the preceding diagram. In mathematical terms, it's simply a function <span class="inlinemediaobject"><img src="graphics/4351OS_04_06.jpg" alt="Representing neural networks"/></span> that maps a set of input values <span class="inlinemediaobject"><img src="graphics/4351OS_04_07.jpg" alt="Representing neural networks"/></span> to an output value <span class="inlinemediaobject"><img src="graphics/4351OS_04_08.jpg" alt="Representing neural networks"/></span>. The function <span class="inlinemediaobject"><img src="graphics/4351OS_04_06.jpg" alt="Representing neural networks"/></span> is called the <span class="strong"><strong>activation function</strong></span><a id="id308" class="indexterm"/> of the neuron, and its output value <span class="inlinemediaobject"><img src="graphics/4351OS_04_08.jpg" alt="Representing neural networks"/></span> is called the <span class="strong"><strong>activation of the neuron</strong></span>. <a id="id309" class="indexterm"/>This representation of a neuron is termed as a <span class="strong"><strong>perceptron</strong></span>. Perceptron can be used on its own <a id="id310" class="indexterm"/>and is effective enough to estimate supervised machine learning models, such as linear regression and logistic regression. However, complex nonlinear data can be better modeled with several interconnected perceptrons.</p><p>Generally, a bias input is added to the set of input values supplied to a perceptron. For the input values <span class="inlinemediaobject"><img src="graphics/4351OS_04_07.jpg" alt="Representing neural networks"/></span>, we add the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_09.jpg" alt="Representing neural networks"/></span> as a bias input such that <span class="inlinemediaobject"><img src="graphics/4351OS_04_10.jpg" alt="Representing neural networks"/></span>. A neuron with this added bias value can be illustrated by the following diagram:</p><div class="mediaobject"><img src="graphics/4351OS_04_11.jpg" alt="Representing neural networks"/></div><p>Each input value <span class="inlinemediaobject"><img src="graphics/4351OS_04_12.jpg" alt="Representing neural networks"/></span> supplied to the perceptron has an associated weight <span class="inlinemediaobject"><img src="graphics/4351OS_04_13.jpg" alt="Representing neural networks"/></span>. This weight is analogous to the coefficients of the features of a<a id="id311" class="indexterm"/> linear regression model. The activation function is applied to these weights and their corresponding input values. We can formally define the estimated output value <span class="inlinemediaobject"><img src="graphics/4351OS_04_06.jpg" alt="Representing neural networks"/></span> of the perceptron in terms of the input values, their weights, and the perceptron's activation function as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_14.jpg" alt="Representing neural networks"/></div><p>The activation function to be used by the nodes of an ANN depends greatly on the sample data that has to be modeled. Generally, the <span class="strong"><strong>sigmoid</strong></span> or <span class="strong"><strong>hyperbolic tangent</strong></span> functions are used as the activation function for classification problems (for more information, refer to <span class="emphasis"><em>Wavelet Neural Network (WNN) approach for calibration model building based on gasoline near infrared (NIR) spectr</em></span>). The sigmoid function is said to be <span class="emphasis"><em>activated</em></span> for a given threshold input. </p><p>We can plot the variance of the sigmoid function to depict this behavior, as shown in the following plot:</p><div class="mediaobject"><img src="graphics/4351OS_04_15.jpg" alt="Representing neural networks"/></div><p>ANNs can be broadly classified into <span class="emphasis"><em>feed-forward neural networks</em></span> and <span class="emphasis"><em>recurrent neural networks</em></span> (for more information, refer to<span class="emphasis"><em> Bidirectional recurrent neural networks</em></span>). The difference between these two types of ANNs is that in feed-forward neural networks, the connections between the nodes of the ANN do not form a directed cycle as opposed to recurrent neural networks <a id="id312" class="indexterm"/>where the node interconnections do form a directed cycle. Thus, in feed-forward neural networks, each node in a given layer of the ANN receives input only from the nodes in the immediate previous layer in the ANN.</p><p>There are several ANN models that have practical applications, and we will explore a few of them in this chapter.</p></div>
<div class="section" title="Understanding multilayer perceptron ANNs"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Understanding multilayer perceptron ANNs</h1></div></div></div><p>We now introduce a simple model of feed-forward neural networks—the <span class="strong"><strong>Multilayer Perceptron</strong></span> model. This model <a id="id313" class="indexterm"/>represents a basic feed-forward neural network and is versatile enough to model regression and classification problems in the domain of supervised learning. All the input flows through a feed-forward neural network in a single direction. This is a direct consequence of the fact that there is no <span class="emphasis"><em>feedback</em></span> from or to any layer in a feed-forward neural network. </p><p>By feedback, we mean that the output of a given layer is fed back as input to the perceptrons in a previous layer in the ANN. Also, using a single layer of perceptrons would mean using only a single activation function, which is equivalent to using <span class="emphasis"><em>logistic regression</em></span> to model the given training data. This would mean that the model cannot be used to fit nonlinear data, which is the primary motivation of ANNs. We must note that we discussed logistic regression in <a class="link" href="ch03.html" title="Chapter 3. Categorizing Data">Chapter 3</a>, <span class="emphasis"><em>Categorizing Data</em></span>.</p><p>A multilayer perceptron ANN can be illustrated<a id="id314" class="indexterm"/> by the following diagram:</p><div class="mediaobject"><img src="graphics/4351OS_04_16.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>A multilayer perceptron ANN comprises several layers of perceptron nodes<a id="id315" class="indexterm"/>. It exhibits a single input layer, a single output layer, and several hidden layers of perceptrons. The input layer simply relays the input values to the first hidden layer of the ANN. These values are then propagated to the output layer through the other hidden layers, where they are weighted and summed using the activation function, to finally produce the output values.</p><p>Each sample in the training data is represented by the <span class="inlinemediaobject"><img src="graphics/4351OS_04_17.jpg" alt="Understanding multilayer perceptron ANNs"/></span> tuple, where <span class="inlinemediaobject"><img src="graphics/4351OS_04_18.jpg" alt="Understanding multilayer perceptron ANNs"/></span> is the expected output and <span class="inlinemediaobject"><img src="graphics/4351OS_04_19.jpg" alt="Understanding multilayer perceptron ANNs"/></span> is the input value of the <span class="inlinemediaobject"><img src="graphics/4351OS_04_20.jpg" alt="Understanding multilayer perceptron ANNs"/></span> training sample. The <span class="inlinemediaobject"><img src="graphics/4351OS_04_19.jpg" alt="Understanding multilayer perceptron ANNs"/></span> input vector comprises a number of values equal to the number of features in the training data.</p><p>The output of each node is termed as the <span class="strong"><strong>activation</strong></span> of the node and is represented by the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_21.jpg" alt="Understanding multilayer perceptron ANNs"/></span> for the <span class="inlinemediaobject"><img src="graphics/4351OS_04_20.jpg" alt="Understanding multilayer perceptron ANNs"/></span> node in the layer <span class="inlinemediaobject"><img src="graphics/4351OS_04_22.jpg" alt="Understanding multilayer perceptron ANNs"/></span>. As we mentioned earlier, the activation function used to produce this value is the sigmoid function or the hyperbolic tangent function. Of course, any other<a id="id316" class="indexterm"/> mathematical function could be used to fit the sample data. The input layer of a multilayer perceptron network simply adds a bias input to the input values, and the set of inputs supplied to the ANN are relayed to the next layer. We can formally represent this equality as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_23.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>The synapses between every pair of layers in the ANN have an associated weight matrix. The number of rows in these matrices is equal to the number of input values, that is, the number of nodes in the layer closer to the input layer of the ANN and the number of columns equal to the number of nodes in the layer of the synapse that is closer to the output layer of the ANN. For a layer <span class="emphasis"><em>l</em></span>, the weight matrix is represented by the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_24.jpg" alt="Understanding multilayer perceptron ANNs"/></span>.</p><p>The activation values of a layer <span class="emphasis"><em>l</em></span> can be determined using the activation function of the ANN. The activation function is applied on the products of the weight matrix and the activation values produced by the previous layer in the ANN. Generally, the activation function used for a multilayer perceptron is a sigmoid function. This equality can be formally represented as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_25.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>Generally, the activation function used for a multilayer perceptron is a sigmoid function. Note that we do not add a bias value in the output layer of an ANN. Also, the output layer can produce any number of output values. To model a <span class="emphasis"><em>k-class</em></span> classification problem, we would require an ANN producing <span class="emphasis"><em>k</em></span> output values.</p><p>To perform binary classification, we can only model a maximum of two classes of input data. The output value generated by an ANN used for binary classification is always 0 or 1. Thus, for <span class="inlinemediaobject"><img src="graphics/4351OS_04_26.jpg" alt="Understanding multilayer perceptron ANNs"/></span> classes, <span class="inlinemediaobject"><img src="graphics/4351OS_04_27.jpg" alt="Understanding multilayer perceptron ANNs"/></span>.</p><p>We can also model a multiclass classification using the <span class="emphasis"><em>k</em></span> binary output values, and thus, the output of the ANN is a <span class="inlinemediaobject"><img src="graphics/4351OS_04_28.jpg" alt="Understanding multilayer perceptron ANNs"/></span> matrix. This can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_29.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>Hence, we can use a multilayer perceptron ANN to perform binary and multiclass classifications. A multilayer perceptron ANN can be trained using the <span class="strong"><strong>backpropagation</strong></span> <span class="strong"><strong>algorithm</strong></span>, <a id="id317" class="indexterm"/>which we will study and implement later in this chapter.</p><p>Let's assume that we want to model the behavior of a logical XOR gate. An XOR gate can be thought of a binary classifier that requires two inputs and generates a single output. An ANN that models the XOR gate would have a structure as shown in the following diagram. Interestingly, linear regression can be used to model both AND and OR logic gates but cannot be used to model an XOR gate. This is due to the nonlinear nature of the output of an XOR gate, and thus, ANNs are used to overcome this limitation.</p><div class="mediaobject"><img src="graphics/4351OS_04_30.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>The multilayer perceptron illustrated<a id="id318" class="indexterm"/> in the preceding diagram has three nodes in the input layer, four nodes in the hidden layer, and one node in the output layer. Observe that every layer other than the output layer adds a bias input to the set of input values for the nodes in the next layer. There are two synapses in the ANN, shown in the preceding diagram, and they are associated with the weight matrices <span class="inlinemediaobject"><img src="graphics/4351OS_04_31.jpg" alt="Understanding multilayer perceptron ANNs"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_04_32.jpg" alt="Understanding multilayer perceptron ANNs"/></span>. Note that the first synapse is between the input layer and hidden layer, and the second synapse is between the hidden layer and the output layer. The weight matrix <span class="inlinemediaobject"><img src="graphics/4351OS_04_31.jpg" alt="Understanding multilayer perceptron ANNs"/></span> has a size of <span class="inlinemediaobject"><img src="graphics/4351OS_04_33.jpg" alt="Understanding multilayer perceptron ANNs"/></span>, and the weight matrix <span class="inlinemediaobject"><img src="graphics/4351OS_04_32.jpg" alt="Understanding multilayer perceptron ANNs"/></span> has a size of <span class="inlinemediaobject"><img src="graphics/4351OS_04_34.jpg" alt="Understanding multilayer perceptron ANNs"/></span>. Also, the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_35.jpg" alt="Understanding multilayer perceptron ANNs"/></span> is used to represent all the weight matrices in the ANN.</p><p>As the activation function of each node in a multilayer perceptron ANN is a sigmoid function, we can define the cost function of the weights of the nodes of the ANN similar to the cost function of a logistic regression model. The cost function of an ANN can be defined in terms of the weight matrices as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_36.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>The preceding cost function is essentially the average of the cost functions of each node in the output layer of an ANN (for more information, refer to <span class="emphasis"><em>Neural Networks in Materials Science</em></span>). For a multilayer perceptron ANN with <span class="emphasis"><em>K</em></span> output values, we perform the average over the <span class="emphasis"><em>K</em></span> terms. Note that <span class="inlinemediaobject"><img src="graphics/4351OS_04_37.jpg" alt="Understanding multilayer perceptron ANNs"/></span> represents the <span class="inlinemediaobject"><img src="graphics/4351OS_04_38.jpg" alt="Understanding multilayer perceptron ANNs"/></span> output value of the ANN, <span class="inlinemediaobject"><img src="graphics/4351OS_04_39.jpg" alt="Understanding multilayer perceptron ANNs"/></span> represents the input variables of the ANN, and <span class="emphasis"><em>N</em></span> is the number of sample values in the training data. The cost function is essentially that of logistic regression but is applied here for the <span class="emphasis"><em>K</em></span> output values. We can add a regularization parameter to the preceding cost function and express the regularized cost function using the following equation:</p><div class="mediaobject"><img src="graphics/4351OS_04_40.jpg" alt="Understanding multilayer perceptron ANNs"/></div><p>The cost function<a id="id319" class="indexterm"/> defined in the preceding equation adds a regularization term similar to that of logistic regression. The regularization term is essentially the sum of the squares of all weights of all input values of the several layers of the ANN, excluding the weights for the added bias input. Also, the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_41.jpg" alt="Understanding multilayer perceptron ANNs"/></span> refers to the number of nodes in layer <span class="emphasis"><em>l</em></span> of the ANN. An interesting point to note is that in the preceding regularized cost function, only the regularization term depends on the number of layers in the ANN. Hence, the <span class="emphasis"><em>generalization</em></span> of the estimated model is based on the number of layers in the ANN.</p></div>
<div class="section" title="Understanding the backpropagation algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Understanding the backpropagation algorithm</h1></div></div></div><p>The <span class="strong"><strong>backpropagation learning</strong></span> algorithm<a id="id320" class="indexterm"/> is used to train a multilayer perceptron ANN from a given set of sample values. In brief, this algorithm first calculates the output value for a set of given<a id="id321" class="indexterm"/> input values and also calculates the amount of error in the output of the ANN. The amount of error in the ANN is determined by comparing the predicted output value of the ANN to the expected output value for the given input values from the training data provided to the ANN. The calculated error is then used to modify the weights of the ANN. Thus, after training the ANN with a reasonable number of samples, the ANN will be able to predict the output value for a set of input values. The algorithm comprises of three distinct phases. They are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A forward propagation phase</li><li class="listitem" style="list-style-type: disc">A backpropagation phase</li><li class="listitem" style="list-style-type: disc">A weight update phase</li></ul></div><p>The weights of the synapses in the ANN are first initialized to random values within the ranges <span class="inlinemediaobject"><img src="graphics/4351OS_04_42.jpg" alt="Understanding the backpropagation algorithm"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_04_43.jpg" alt="Understanding the backpropagation algorithm"/></span>. We initialize the weights to values within this range to avoid a symmetry in the weight matrices. This avoidance of symmetry is called <span class="strong"><strong>symmetry breaking,</strong></span><a id="id322" class="indexterm"/> and it is performed so that each iteration of the backpropagation algorithm produces a noticeable change in the weights of the synapses in the ANN. This is desirable in an ANN as each of its node should learn independently of other nodes in the ANN. If all the nodes were to have identical weights, the estimated<a id="id323" class="indexterm"/> learning model will be either overfit or underfit.</p><p>Also, the backpropagation learning<a id="id324" class="indexterm"/> algorithm requires two additional parameters, which are the learning rate <span class="inlinemediaobject"><img src="graphics/4351OS_04_44.jpg" alt="Understanding the backpropagation algorithm"/></span> and the learning momentum <span class="inlinemediaobject"><img src="graphics/4351OS_04_45.jpg" alt="Understanding the backpropagation algorithm"/></span>. We will see the effects of these parameters in the example later in this section.</p><p>The forward propagation phase of the algorithm simply calculates the activation values of all nodes in the various layers of the ANN. As we mentioned earlier, the activation values of the nodes in the input layer are the input values and the bias input of the ANN. This can be formally defined by using the following equation:</p><div class="mediaobject"><img src="graphics/4351OS_04_23.jpg" alt="Understanding the backpropagation algorithm"/></div><p>Using these activation values from the input layer of the ANN, the activation of the nodes in the other layers of the ANN is determined. This is done by applying the activation function to the products of the weight matrix of a given layer and the activation values from the previous layer in the ANN. This can be formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_46.jpg" alt="Understanding the backpropagation algorithm"/></div><p>The preceding equation explains that the activation value of a layer <span class="emphasis"><em>l</em></span> is equal to the activation function applied to the output (or activation) values of the previous layer and the given layer's weight matrix. Next, the activation values of the output layer are <span class="emphasis"><em>backpropagated</em></span>. By this, we mean that that the activation values are traversed from the output layer through the hidden layers to the input layer of the ANN. During this phase, we determine the amount of error or delta in each node in the ANN. The delta values of the output layer are determined by <a id="id325" class="indexterm"/>calculating the difference between the expected output values, <span class="inlinemediaobject"><img src="graphics/4351OS_04_18.jpg" alt="Understanding the backpropagation algorithm"/></span>, and the activation values of the output layer, <span class="inlinemediaobject"><img src="graphics/4351OS_04_47.jpg" alt="Understanding the backpropagation algorithm"/></span>. This difference calculation can be summarized by the following equation:</p><div class="mediaobject"><img src="graphics/4351OS_04_48.jpg" alt="Understanding the backpropagation algorithm"/></div><p>The term <span class="inlinemediaobject"><img src="graphics/4351OS_04_49.jpg" alt="Understanding the backpropagation algorithm"/></span> of a layer <span class="emphasis"><em>l</em></span> is a matrix of size <span class="inlinemediaobject"><img src="graphics/4351OS_04_50.jpg" alt="Understanding the backpropagation algorithm"/></span> where <span class="emphasis"><em>j</em></span> is the number of nodes in layer <span class="emphasis"><em>l</em></span>. This term can be formally defined as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_51.jpg" alt="Understanding the backpropagation algorithm"/></div><p>The delta terms of the layers<a id="id326" class="indexterm"/> other than the output layer of the ANN are determined by the following equality:</p><div class="mediaobject"><img src="graphics/4351OS_04_52.jpg" alt="Understanding the backpropagation algorithm"/></div><p>In the preceding equation, the binary operation <span class="inlinemediaobject"><img src="graphics/4351OS_04_53.jpg" alt="Understanding the backpropagation algorithm"/></span> is used to represent an element-wise multiplication of two matrices of equal size. Note that this operation is different from matrix multiplication, and an element-wise multiplication will return a matrix composed of the products of the elements with the same position in two matrices of equal size. The term <span class="inlinemediaobject"><img src="graphics/4351OS_04_54.jpg" alt="Understanding the backpropagation algorithm"/></span> represents the derivative of the activation function used in the ANN. As we are using the sigmoid function as our activation function, the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_55.jpg" alt="Understanding the backpropagation algorithm"/></span> has the value <span class="inlinemediaobject"><img src="graphics/4351OS_04_56.jpg" alt="Understanding the backpropagation algorithm"/></span>.</p><p>Thus, we can calculate the delta values of all nodes in the ANN. We can use these delta values to determine the gradients of the synapses of the ANN. We now move on to the final weight update phase of the backpropagation algorithm.</p><p>The gradients of the various synapses are first initialized to matrices with all the elements as 0. The size of a gradient matrix of a given synapse is the same size as the weight matrix of the synapse. The gradient term <span class="inlinemediaobject"><img src="graphics/4351OS_04_57.jpg" alt="Understanding the backpropagation algorithm"/></span> represents the gradients of the synapse layer that is present immediately after layer <span class="emphasis"><em>l</em></span> in the ANN. The initialization of the gradients of the synapses in the ANN is formally expressed as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_58.jpg" alt="Understanding the backpropagation algorithm"/></div><p>For each sample value in<a id="id327" class="indexterm"/> the training data, we calculate the deltas and activation values of all nodes in the ANN. These values are added to the gradients of the synapses using the following equation:</p><div class="mediaobject"><img src="graphics/4351OS_04_59.jpg" alt="Understanding the backpropagation algorithm"/></div><p>We then calculate the average<a id="id328" class="indexterm"/> of the gradients for all the sample values and use the delta and gradient values of a given layer to update the weight matrix as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_60.jpg" alt="Understanding the backpropagation algorithm"/></div><p>Thus, the learning rate and learning momentum parameters of the algorithm come into play only in the weight update phase. The preceding three equations represent a single iteration of the backpropagation algorithm. A large number of iterations must be performed until the overall error in the ANN converges to a small value. We can now summarize the backpropagation learning algorithm<a id="id329" class="indexterm"/> using the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Initialize the weights of the synapses of the ANN to random values.</li><li class="listitem">Select a sample value and forward propagate the sample values through several layers of the ANN to generate the activations of every node in the ANN.</li><li class="listitem">Backpropagate the activations generated by the last layer of the ANN through the hidden layers and to the input layer of the ANN. Through this step, we calculate the error or delta of every node in the ANN.</li><li class="listitem">Calculate the product of the errors generated from step 3 with the synapse weights or input activations for all the nodes in the ANN. This step produces the gradient of weight for each node in the network. Each gradient is represented by a ratio or percentage.</li><li class="listitem">Calculate the changes in the weights of the synapse layers in the ANN using the gradients and deltas of a given layer in the ANN. These changes are then subtracted from the weights of the synapses in the ANN. This is essentially the weight update step of the backpropagation algorithm.</li><li class="listitem">Repeat steps 2 to 5 for<a id="id330" class="indexterm"/> the rest of the samples in the training data.</li></ol></div><p>There are several distinct <a id="id331" class="indexterm"/>parts in the backpropagation learning algorithm, and we will now implement each part and combine it into a complete implementation. As the deltas and weights of the synapses and <a id="id332" class="indexterm"/>activations in an ANN can be represented by matrices, we can write a vectorized implementation of this algorithm.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>Note that for the following example, we require functions from the <code class="literal">incanter.core</code> namespace from the Incanter library. The functions in this namespace actually use the Clatrix library for the representation of a matrix and its manipulation.</p></div></div><p>Let's assume that we need to implement an ANN to model a logical XOR gate. The sample data is simply the truth table of the XOR gate and can be represented as a vector, shown as follows:</p><div class="informalexample"><pre class="programlisting">;; truth table for XOR logic gate
(def sample-data [[[0 0] [0]]
                  [[0 1] [1]]
                  [[1 0] [1]]
                  [[1 1] [0]]])</pre></div><p>Each element defined in the preceding vector <code class="literal">sample-data</code> is itself a vector comprising other vectors for the input and output values of an XOR gate. We will use this vector as our training data for building an ANN. This is essentially a classification problem, and we will use ANNs to model it. In abstract terms, an ANN should be capable of performing both binary and multiclass classifications. We can define the protocol of an ANN as follows:</p><div class="informalexample"><pre class="programlisting">(defprotocol NeuralNetwork
  (run        [network inputs])
  (run-binary [network inputs])
  (train-ann  [network samples]))</pre></div><p>The <code class="literal">NeuralNetwork</code> protocol defined in the preceding code has three functions. The <code class="literal">train-ann</code> function<a id="id333" class="indexterm"/> can be used to train the ANN and requires some sample data. The <code class="literal">run</code> and <code class="literal">run-binary</code> functions can be used on this ANN to perform multiclass and binary classifications, respectively. Both the <code class="literal">run</code> and <code class="literal">run-binary</code> functions require a set of input values.</p><p>The first step of the backpropagation algorithm <a id="id334" class="indexterm"/>is the initialization of the weights of the synapses of the ANN. We can use the <code class="literal">rand</code> and <code class="literal">matrix</code> functions to generate these weights as a matrix, shown as follows:</p><div class="informalexample"><pre class="programlisting">(defn rand-list
  "Create a list of random doubles between 
  -epsilon and +epsilon."
  [len epsilon]
  (map (fn [x] (- (rand (* 2 epsilon)) epsilon))
         (range 0 len)))

(defn random-initial-weights
  "Generate random initial weight matrices for given layers.
  layers must be a vector of the sizes of the layers."
  [layers epsilon]
  (for [i (range 0 (dec (length layers)))]
    (let [cols (inc (get layers i))
          rows (get layers (inc i))]
      (matrix (rand-list (* rows cols) epsilon) cols))))</pre></div><p>The <code class="literal">rand-list</code> function<a id="id335" class="indexterm"/> shown in<a id="id336" class="indexterm"/> the preceding code creates a list of random elements in the positive and negative range of <code class="literal">epsilon</code>. As we described earlier, we choose this range to break the symmetry of the weight matrix.</p><p>The <code class="literal">random-initial-weights</code> function<a id="id337" class="indexterm"/> generates several weight matrices for different layers of the ANN. As defined in the preceding code, the <code class="literal">layers</code> argument must be a vector of the sizes of the layers of the ANN. For an ANN with two nodes in the input layer, three nodes in the hidden layer, and one node in the output layer, we pass <code class="literal">layers</code> as <code class="literal">[2 3 1]</code> to the <code class="literal">random-initial-weights</code> function. Each weight matrix has a number of columns equal to the number of inputs and number of rows equal to the number of nodes in the next layer of the ANN. We set the number of columns in a weight matrix of a given layer to the number of inputs, plus an extra input for the bias of the neural layer. Note that we use a slightly different form of the <code class="literal">matrix</code> function. This form takes a single vector and partitions this vector into a matrix that has a number of columns as specified by second argument to this function. Thus, the vector passed to this form of the <code class="literal">matrix</code> function must have <code class="literal">(* rows cols)</code> elements, where <code class="literal">rows</code> and <code class="literal">cols</code> are the number of rows and columns, respectively, in the weight matrix.</p><p>As we will need to apply the<a id="id338" class="indexterm"/> sigmoid function to all the activations of a layer in the ANN, we must define a function that applies the sigmoid function on all the elements in a given matrix. We can use the <code class="literal">div</code>, <code class="literal">plus</code>, <code class="literal">exp</code>, and <code class="literal">minus</code> functions from the <code class="literal">incanter.core</code> namespace to implement such a function, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn sigmoid
  "Apply the sigmoid function 1/(1+exp(-z)) to all 
  elements in the matrix z."
  [z]
  (div 1 (plus 1 (exp (minus z)))))</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>Note that all of the previously defined functions apply the corresponding arithmetic operation on all the elements in a given matrix and returns a new matrix.</p></div></div><p>We will also need to<a id="id339" class="indexterm"/> implicitly add a bias node to each layer in an ANN. This can be done by wrapping around the <code class="literal">bind-rows</code> function<a id="id340" class="indexterm"/>, which adds a row of elements to a matrix, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn bind-bias
  "Add the bias input to a vector of inputs."
  [v]
  (bind-rows [1] v))</pre></div><p>Since the bias value is always 1, we specify the row of elements as <code class="literal">[1]</code> to the <code class="literal">bind-rows</code> function.</p><p>Using the functions defined earlier, we can implement forward propagation. We essentially have to multiply the weights of a given synapse between two layers in an ANN and then apply the sigmoid function on each of the generated activation values, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn matrix-mult
  "Multiply two matrices and ensure the result is also a matrix."
  [a b]
  (let [result (mmult a b)]
    (if (matrix? result)
      result
      (matrix [result]))))

(defn forward-propagate-layer
  "Calculate activations for layer l+1 given weight matrix 
  of the synapse between layer l and l+1 and layer l activations."
  [weights activations]
  (sigmoid (matrix-mult weights activations)))

(defn forward-propagate
  "Propagate activation values through a network's
  weight matrix and return output layer activation values."
  [weights input-activations]
  (reduce #(forward-propagate-layer %2 (bind-bias %1))
          input-activations weights))</pre></div><p>In the preceding code, we first define a <code class="literal">matrix-mult</code> function<a id="id341" class="indexterm"/>, which performs matrix multiplication and ensures that the result is a matrix. Note that to define <code class="literal">matrix-mult</code>, we use the <code class="literal">mmult</code> function<a id="id342" class="indexterm"/> instead of the <code class="literal">mult</code> function that multiplies the corresponding elements in two matrices of the same size.</p><p>Using the <code class="literal">matrix-mult</code> and <code class="literal">sigmoid</code> functions, we can implement the forward propagation step between two layers in the ANN. This is done in the <code class="literal">forward-propagate-layer</code> function, which<a id="id343" class="indexterm"/> simply multiplies the matrices representing the weights of the synapse between two layers in the ANN and the input activation values while ensuring that the returned <a id="id344" class="indexterm"/>value is always a matrix. To propagate a given set of values through all the layers of an ANN, we must add a bias input and apply the <code class="literal">forward-propagate-layer</code> function for each layer. This can be done concisely using the <code class="literal">reduce</code> function over a closure of the <code class="literal">forward-propagate-layer</code> function as shown in the <code class="literal">forward-propagate</code> function defined in the preceding code.</p><p>Although the <code class="literal">forward-propagate</code> function can determine the output activations of the ANN, we actually require the activations of all the nodes in the ANN to use backpropagation. We can do this by translating the <code class="literal">reduce</code> function to a recursive function and introducing an accumulator variable to store the activations of every layer in the ANN. The <code class="literal">forward-propagate-all-activations</code> function<a id="id345" class="indexterm"/>, which is defined in the following code, implements this idea and uses the <code class="literal">loop</code> form to recursively apply the <code class="literal">forward-propagate-layer</code> function:</p><div class="informalexample"><pre class="programlisting">(defn forward-propagate-all-activations
  "Propagate activation values through the network 
  and return all activation values for all nodes."
  [weights input-activations]
  (loop [all-weights     weights
         activations     (bind-bias input-activations)
         all-activations [activations]]
    (let [[weights
           &amp; all-weights']  all-weights
           last-iter?       (empty? all-weights')
           out-activations  (forward-propagate-layer
                             weights activations)
           activations'     (if last-iter? out-activations
                                (bind-bias out-activations))
           all-activations' (conj all-activations activations')]
      (if last-iter? all-activations'
          (recur all-weights' activations' all-activations')))))</pre></div><p>The <code class="literal">forward-propagate-all-activations</code> function defined in the preceding code requires all the weights of the nodes in the ANN and the input values to pass through the ANN as activation values. We first<a id="id346" class="indexterm"/> use the <code class="literal">bind-bias</code> function to add the bias input to the input activations of the ANN. We then store this value in an accumulator, that is, the variable <code class="literal">all-activations</code>, as a <a id="id347" class="indexterm"/>vector of all the activations in the ANN. The <code class="literal">forward-propagate-layer</code> function<a id="id348" class="indexterm"/> is then applied over the weight matrices of the various layers of the ANN, and each iteration adds a bias input to the input activations of the corresponding layer in the ANN.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>Note that we do not add the bias input in the last iteration as it computes the output layer of the ANN. Thus, the <code class="literal">forward-propagate-all-activations</code> function applies forward propagation of input values through an ANN and returns the activations of every node in the ANN. Note that the activation values in this vector are in the order of the layers of the ANN.</p></div></div><p>We will now implement the backpropagation phase of the backpropagation learning algorithm. First, we would have to implement a function to calculate the error term <span class="inlinemediaobject"><img src="graphics/4351OS_04_49.jpg" alt="Understanding the backpropagation algorithm"/></span> from the equation <span class="inlinemediaobject"><img src="graphics/4351OS_04_61.jpg" alt="Understanding the backpropagation algorithm"/></span>. We will do this with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn back-propagate-layer
  "Back propagate deltas (from layer l+1) and 
  return layer l deltas."
  [deltas weights layer-activations]
  (mult (matrix-mult (trans weights) deltas)
        (mult layer-activations (minus 1 layer-activations))))</pre></div><p>The <code class="literal">back-propagate-layer</code> function<a id="id349" class="indexterm"/> defined in the preceding code calculates the errors, or deltas, of a synapse layer <span class="emphasis"><em>l</em></span> in the ANN from the weights of the layer and the deltas of the next layer in the ANN.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>Note that we only use matrix multiplication to calculate the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_62.jpg" alt="Understanding the backpropagation algorithm"/></span> via the <code class="literal">matrix-mult</code> function. All other multiplication operations are element-wise multiplication of matrices, which is done using the <code class="literal">mult</code> function.</p></div></div><p>Essentially, we have to apply this function from the output layer to the input layer through the various hidden layers of an ANN to produce the delta values of every node in the ANN. These delta values can then be added to the activations of the nodes, thus producing the gradient values by which we must adjust the weights of the nodes in the ANN. We can do this in a manner<a id="id350" class="indexterm"/> similar to the <code class="literal">forward-propagate-all-activations</code> function, that is, by recursively applying the <code class="literal">back-propagate-layer</code> function over the various layers of the ANN. Of course, we have to traverse the layers of the ANN in the reverse order, that is, <a id="id351" class="indexterm"/>starting from the output layer, through the hidden layers, to the input layer. We will do this with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn calc-deltas
  "Calculate hidden deltas for back propagation.
  Returns all deltas including output-deltas."
  [weights activations output-deltas]
  (let [hidden-weights     (reverse (rest weights))
        hidden-activations (rest (reverse (rest activations)))]
    (loop [deltas          output-deltas
           all-weights     hidden-weights
           all-activations hidden-activations
           all-deltas      (list output-deltas)]
      (if (empty? all-weights) all-deltas
        (let [[weights
               &amp; all-weights']      all-weights
               [activations
                &amp; all-activations'] all-activations
              deltas'        (back-propagate-layer
                               deltas weights activations)
              all-deltas'    (cons (rest deltas') 
                                    all-deltas)]
          (recur deltas' all-weights' 
                 all-activations' all-deltas'))))))</pre></div><p>The <code class="literal">calc-deltas</code> function determines the delta values of all the perceptron nodes in the ANN. For this calculation, the input and output activations are not needed. Only the hidden activations, bound to the <code class="literal">hidden-activations</code> variable<a id="id352" class="indexterm"/>, are needed to calculate the delta values. Also, the weights of the input layer are skipped as they are bound to the <code class="literal">hidden-weights</code> variable<a id="id353" class="indexterm"/>. The <code class="literal">calc-deltas</code> function<a id="id354" class="indexterm"/> then applies the <code class="literal">back-propagate-layer</code> function<a id="id355" class="indexterm"/> to all the weight matrices of each synapse layer in the ANN, thus determining the deltas of all the nodes in the matrix. Note that we don't add the delta of the bias nodes to a computed set of deltas. This is done using the <code class="literal">rest</code> function<a id="id356" class="indexterm"/>, <code class="literal">(rest deltas')</code>, on the calculated deltas of a given synapse layer, as the first delta is that of a bias input in a given layer.</p><p>By definition, the gradient vector terms for a given synapse layer <span class="inlinemediaobject"><img src="graphics/4351OS_04_57.jpg" alt="Understanding the backpropagation algorithm"/></span> are determined<a id="id357" class="indexterm"/> by multiplying the matrices <span class="inlinemediaobject"><img src="graphics/4351OS_04_63.jpg" alt="Understanding the backpropagation algorithm"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_04_64.jpg" alt="Understanding the backpropagation algorithm"/></span>, which represent the deltas of the next layer and activations of the given layer respectively. We will do this with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn calc-gradients
  "Calculate gradients from deltas and activations."
  [deltas activations]
  (map #(mmult %1 (trans %2)) deltas activations))</pre></div><p>The <code class="literal">calc-gradients</code> function<a id="id358" class="indexterm"/> shown in the preceding code is a concise implementation of the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_65.jpg" alt="Understanding the backpropagation algorithm"/></span>. As we will be dealing with a sequence of delta and activation<a id="id359" class="indexterm"/> terms, we use the <code class="literal">map</code> function to apply the preceding equality to the corresponding deltas and activations in the ANN. Using the <code class="literal">calc-deltas</code> and <code class="literal">calc-gradient</code> functions, we can determine the total error in the weights of all nodes in the ANN for a given training sample. We will do this with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn calc-error
  "Calculate deltas and squared error for given weights."
  [weights [input expected-output]]
  (let [activations    (forward-propagate-all-activations 
                        weights (matrix input))
        output         (last activations)
        output-deltas  (minus output expected-output)
        all-deltas     (calc-deltas 
                        weights activations output-deltas)
        gradients      (calc-gradients all-deltas activations)]
    (list gradients
          (sum (pow output-deltas 2)))))</pre></div><p>The <code class="literal">calc-error</code> function defined in the preceding code requires two parameters—the weight matrices of the synapse layers in the ANN and a sample training value, which is shown as <code class="literal">[input expected-output]</code>. The activations of all the nodes in the ANN are first calculated using the <code class="literal">forward-propagate-all-activations</code> function<a id="id360" class="indexterm"/>, and the delta value of the last layer is calculated as the difference of the expected output value and the actual output value produced by the ANN. The output value calculated by the ANN is simply the last activation value produced by the ANN, shown as <code class="literal">(last activations)</code> in the preceding code. Using the calculated activations, the deltas of all the perceptron nodes are determined via the <code class="literal">calc-deltas</code> function<a id="id361" class="indexterm"/>. These delta values are in turn used to determine the gradients of weights in the various layers of the ANN using the <code class="literal">calc-gradients</code> function. The <span class="strong"><strong>Mean Square Error</strong></span> (<span class="strong"><strong>MSE</strong></span>) <a id="id362" class="indexterm"/>of the ANN for the given <a id="id363" class="indexterm"/>sample value is also calculated by adding the squares of the delta values of the output layer of the ANN.</p><p>For a given weight matrix of a layer in the ANN, we must initialize the gradients for the layer as a matrix with the same dimensions as the weight matrix, and all the elements in the gradient matrix must be set to <code class="literal">0</code>. This can be implemented using a composition of the <code class="literal">dim</code> function, which returns the size of a matrix as a vector, and a variant form of the <code class="literal">matrix</code> function, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn new-gradient-matrix
  "Create accumulator matrix of gradients with the
  same structure as the given weight matrix
  with all elements set to 0."
  [weight-matrix]
  (let [[rows cols] (dim weight-matrix)]
    (matrix 0 rows cols)))</pre></div><p>In the <code class="literal">new-gradient-matrix</code> function<a id="id364" class="indexterm"/> defined in the preceding code, the <code class="literal">matrix</code> function expects a value, the<a id="id365" class="indexterm"/> number of rows and the number of columns to initialize a matrix. This function produces an initialized gradient matrix with the same structure as the supplied weight matrix.</p><p>We now implement the <code class="literal">calc-gradients-and-error</code> function <a id="id366" class="indexterm"/>to apply the <code class="literal">calc-error</code> function on a set of weight matrices and sample values. We must basically apply the <code class="literal">calc-error</code> function to each sample and accumulate the sum of the gradient and the MSE values. We then calculate the average of these accumulated values to return the gradient matrices and total MSE for the given sample values and weight matrices. We will do this with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn calc-gradients-and-error' [weights samples]
  (loop [gradients   (map new-gradient-matrix weights)
         total-error 1
         samples     samples]
    (let [[sample
           &amp; samples']     samples
           [new-gradients
            squared-error] (calc-error weights sample)
            gradients'     (map plus new-gradients gradients)
            total-error'   (+ total-error squared-error)]
      (if (empty? samples')
        (list gradients' total-error')
        (recur gradients' total-error' samples')))))

(defn calc-gradients-and-error
  "Calculate gradients and MSE for sample
  set and weight matrix."
  [weights samples]
  (let [num-samples   (length samples)
        [gradients
         total-error] (calc-gradients-and-error'
                       weights samples)]
    (list
      (map #(div % num-samples) gradients)    ; gradients
      (/ total-error num-samples))))          ; MSE</pre></div><p>The <code class="literal">calc-gradients-and-error</code> function<a id="id367" class="indexterm"/> defined in the preceding code relies on the <code class="literal">calc-gradients-and-error'</code> helper function. <code class="literal">The calc-gradients-and-error'</code> function initializes the gradient matrices, performs the application<a id="id368" class="indexterm"/> of the <code class="literal">calc-error</code> function, and accumulates the calculated gradient values and MSE. The <code class="literal">calc-gradients-and-error</code> function<a id="id369" class="indexterm"/> simply calculates the average of the accumulated gradient matrices and MSE returned from the <code class="literal">calc-gradients-and-error'</code> function.</p><p>Now, the only missing piece<a id="id370" class="indexterm"/> in our implementation is modifying the weights of the nodes in the ANN using calculated gradients. In brief, we must repeatedly update the weights until a convergence in the MSE is observed. This is actually a form of gradient descent applied to the nodes of an ANN. We will now implement this variant of gradient descent in order to train the ANN by repeatedly modifying the weights of the nodes in the ANN, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn gradient-descent-complete?
  "Returns true if gradient descent is complete."
  [network iter mse]
  (let [options (:options network)]
    (or (&gt;= iter (:max-iters options))
        (&lt; mse (:desired-error options)))))</pre></div><p>The <code class="literal">gradient-descent-complete?</code> function<a id="id371" class="indexterm"/> defined in the preceding code simply checks for the termination condition of gradient descent. This function assumes that the ANN, represented as a network, is a map or record that contains the <code class="literal">:options</code> keyword. The value of this key is in turn another map that contains the various configuration options of the ANN. The <code class="literal">gradient-descent-complete?</code> function checks whether the total MSE of the ANN is less than the desired MSE, which is specified by the <code class="literal">:desired-error</code> option. Also, we add another condition to check if the number of iterations performed exceeds the maximum number of iterations specified by the <code class="literal">:max-iters</code> option.</p><p>Now, we will implement a <code class="literal">gradient-descent</code> function for multilayer perceptron ANNs. In this implementation, <a id="id372" class="indexterm"/>the changes in weights are calculated by the <code class="literal">step</code> function provided by the gradient descent algorithm. These calculated changes are then simply added to the existing<a id="id373" class="indexterm"/> weights of the synapse layers of the ANN. We will implement the <code class="literal">gradient-descent</code> function for multilayer perceptron ANNs with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn apply-weight-changes
  "Applies changes to corresponding weights."
  [weights changes]
  (map plus weights changes))

(defn gradient-descent
  "Perform gradient descent to adjust network weights."
  [step-fn init-state network samples]
  (loop [network network
         state init-state
         iter 0]
    (let [iter     (inc iter)
          weights  (:weights network)
          [gradients
           mse]    (calc-gradients-and-error weights samples)]
      (if (gradient-descent-complete? network iter mse)
        network
        (let [[changes state] (step-fn network gradients state)
              new-weights     (apply-weight-changes 
                               weights changes)
              network         (assoc network 
                              :weights new-weights)]
          (recur network state iter))))))</pre></div><p>The <code class="literal">apply-weight-changes</code> function<a id="id374" class="indexterm"/> defined in the preceding code simply adds the weights and the calculated changes in the weights of the ANN. The <code class="literal">gradient-descent</code> function<a id="id375" class="indexterm"/> requires a <code class="literal">step</code> function (specified as <code class="literal">step-fn</code>), the initial state of the ANN, the ANN itself, and the sample data to train the ANN. This function must calculate the weight changes from the ANN, the initial gradient matrices, and the initial state of the ANN. The <code class="literal">step-fn</code> function also returns the changed state of the ANN. The weights of the ANN are then updated using the <code class="literal">apply-weight-changes</code> function, and this iteration is repeatedly performed until the <code class="literal">gradient-descent-complete?</code> function returns as <code class="literal">true</code>. The weights of the ANN are specified by the <code class="literal">:weights</code> keyword in the <code class="literal">network</code> map. These weights are then updated by simply overwriting the value on the <code class="literal">network</code> specified by the <code class="literal">:weights</code> keyword.</p><p>In the context of the<a id="id376" class="indexterm"/> backpropagation algorithm, we need to specify the learning rate and learning momentum by which the ANN must be trained. These parameters are needed to determine the changes in the weights of the nodes in the ANN. A function implementing this calculation must then be specified as the <code class="literal">step-fn</code> parameter to the <code class="literal">gradient-descent</code> function, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">(defn calc-weight-changes
  "Calculate weight changes:
  changes = learning rate * gradients + 
            learning momentum * deltas."
  [gradients deltas learning-rate learning-momentum]
  (map #(plus (mult learning-rate %1)
              (mult learning-momentum %2))
       gradients deltas))

(defn bprop-step-fn [network gradients deltas]
  (let [options             (:options network)
        learning-rate       (:learning-rate options)
        learning-momentum   (:learning-momentum options)
        changes             (calc-weight-changes
                             gradients deltas
                             learning-rate learning-momentum)]
    [(map minus changes) changes]))

(defn gradient-descent-bprop [network samples]
  (let [gradients (map new-gradient-matrix (:weights network))]
    (gradient-descent bprop-step-fn gradients
                      network samples)))</pre></div><p>The <code class="literal">calc-weight-changes</code> function defined in the preceding code calculates the change of weights, termed as <span class="inlinemediaobject"><img src="graphics/4351OS_04_66.jpg" alt="Understanding the backpropagation algorithm"/></span>, from the gradient values and deltas of a given layer in the ANN. The <code class="literal">bprop-step-fn</code> function extracts the learning rate <a id="id377" class="indexterm"/>and learning momentum parameters from the ANN that is represented by <code class="literal">network</code> and uses the <code class="literal">calc-weight-changes</code> function. As the weights will be added with the changes by the <code class="literal">gradient-descent</code> function, we return the changes in weights as negative values using the <code class="literal">minus</code> function.</p><p>The <code class="literal">gradient-descent-bprop</code> function<a id="id378" class="indexterm"/> simply initializes the gradient matrices for the given weights of the ANN and calls the <code class="literal">gradient-descent</code> function by<a id="id379" class="indexterm"/> specifying <code class="literal">bprop-step-fn</code> as the <code class="literal">step</code> function to be used. Using the <code class="literal">gradient-descent-bprop</code> function, we can implement the abstract <code class="literal">NeuralNetwork</code> protocol we had defined earlier, as follows:</p><div class="informalexample"><pre class="programlisting">(defn round-output
  "Round outputs to nearest integer."
  [output]
  (mapv #(Math/round ^Double %) output))

(defrecord MultiLayerPerceptron [options]
  NeuralNetwork

  ;; Calculates the output values for the given inputs.
  (run [network inputs]
    (let [weights (:weights network)
          input-activations (matrix inputs)]
      (forward-propagate weights input-activations)))

  ;; Rounds the output values to binary values for
  ;; the given inputs.
  (run-binary [network inputs]
    (round-output (run network inputs)))

  ;; Trains a multilayer perceptron ANN from sample data.
  (train-ann [network samples]
    (let [options         (:options network)
          hidden-neurons  (:hidden-neurons options)
          epsilon         (:weight-epsilon options)
          [first-in
           first-out]     (first samples)
          num-inputs      (length first-in)
          num-outputs     (length first-out)
          sample-matrix   (map #(list (matrix (first %)) 
                                      (matrix (second %)))
                               samples)
          layer-sizes     (conj (vec (cons num-inputs 
                                           hidden-neurons))
                                num-outputs)
          new-weights     (random-initial-weights 
                           layer-sizes epsilon)
          network         (assoc network :weights new-weights)]
      (gradient-descent-bprop network sample-matrix))))</pre></div><p>The <code class="literal">MultiLayerPerceptron</code> record defined<a id="id380" class="indexterm"/> in the preceding code trains a multilayer perceptron ANN using the <code class="literal">gradient-descent-bprop</code> function. The <code class="literal">train-ann</code> function first extracts the values for the number of hidden neurons and the constant <span class="inlinemediaobject"><img src="graphics/4351OS_04_43.jpg" alt="Understanding the backpropagation algorithm"/></span> from the options map specified to the ANN. The sizes of the various synapse layers in the<a id="id381" class="indexterm"/> ANN are first determined from the sample data and bound to the <code class="literal">layer-sizes</code> variable. The weights of the ANN are then initialized using the <code class="literal">random-initial-weights</code> function<a id="id382" class="indexterm"/> and updated in the record <code class="literal">network</code> using the <code class="literal">assoc</code> function. Finally, the <code class="literal">gradient-descent-bprop</code> function<a id="id383" class="indexterm"/> is called to train the ANN using the backpropagation learning algorithm.</p><p>The ANN defined by the <code class="literal">MultiLayerPerceptron</code> record also implements two other functions, <code class="literal">run</code> and <code class="literal">run-binary</code>, from the <code class="literal">NeuralNetwork</code> protocol. The <code class="literal">run</code> function uses the <code class="literal">forward-propagate</code> function<a id="id384" class="indexterm"/> to determine the output values of a trained <code class="literal">MultiLayerPerceptron</code> ANN. The <a id="id385" class="indexterm"/>
<code class="literal">run-binary</code> function simply rounds the value of the output returned by the <code class="literal">run</code> function for the given set of input values.</p><p>An ANN created using the <code class="literal">MultiLayerPerceptron</code> record requires a single <code class="literal">options</code> parameter containing the various options we can specify for the ANN. We can define the default options for such an ANN as follows:</p><div class="informalexample"><pre class="programlisting">(def default-options
  {:max-iters 100
   :desired-error 0.20
   :hidden-neurons [3]
   :learning-rate 0.3
   :learning-momentum 0.01
   :weight-epsilon 50})

(defn train [samples]
  (let [network (MultiLayerPerceptron. default-options)]
    (train-ann network samples)))</pre></div><p>The map defined by the <code class="literal">default-options</code> variable<a id="id386" class="indexterm"/> contains the following keys that specify the options for the <code class="literal">MultiLayerPerceptron</code> ANN:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">:max-iter</code>: This key specifies the maximum number of iterations to run the <code class="literal">gradient-descent</code> function.</li><li class="listitem" style="list-style-type: disc"><code class="literal">:desired-error</code>: This variable specifies the expected or acceptable MSE in the ANN.</li><li class="listitem" style="list-style-type: disc"><code class="literal">:hidden-neurons</code>: This variable specifies the number of hidden neural nodes in the network. The value <code class="literal">[3]</code> represents a single hidden layer with three neurons.</li><li class="listitem" style="list-style-type: disc"><code class="literal">:learning-rate </code>and <code class="literal">:learning-momentum</code>: These keys specify the learning rate and learning momentum for the weight update phase of the backpropagation learning algorithm.</li><li class="listitem" style="list-style-type: disc"><code class="literal">:epsilon</code>: This variable specifies the constant used by the <code class="literal">random-initial-weights</code> function<a id="id387" class="indexterm"/> to initialize the weights of the ANN.</li></ul></div><p>We also define a simple<a id="id388" class="indexterm"/> helper function <code class="literal">train</code> to create an ANN of the <code class="literal">MultiLayerPerceptron</code> type and train the ANN using the <code class="literal">train-ann</code> function<a id="id389" class="indexterm"/> and the sample data specified by the <code class="literal">samples</code> parameter. We can now create a trained ANN from the training data specified by the <code class="literal">sample-data</code> variable as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (def MLP (train sample-data))
#'user/MLP</pre></div><p>We can then use the trained<a id="id390" class="indexterm"/> ANN to predict the output of some input values. The output generated by the ANN defined by <code class="literal">MLP</code> closely matches the output of an XOR gate as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (run-binary MLP  [0 1])
[1]
user&gt; (run-binary MLP  [1 0])
[1]</pre></div><p>However, the trained ANN produces incorrect outputs for some set of inputs as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (run-binary MLP  [0 0])
[0]
user&gt; (run-binary MLP  [1 1]) ;; incorrect output generated
[1]</pre></div><p>There are several measures we can implement in order to improve the accuracy of the trained ANN. First, we can regularize the calculated gradients using the weights matrices of the ANN. This modification will produce a noticeable improvement in the preceding implementation. We can also increase the maximum number of iterations to be performed. We can also tune the algorithm to perform better by tweaking the learning rate, the learning momentum, <a id="id391" class="indexterm"/>and the number of hidden nodes in the ANN. These modifications are skipped as they have to be done by the reader.</p><p>The <span class="strong"><strong>Enclog</strong></span> library<a id="id392" class="indexterm"/> (<a class="ulink" href="http://github.com/jimpil/enclog">http://github.com/jimpil/enclog</a>) is a Clojure wrapper library for the <span class="strong"><strong>Encog</strong></span> library for machine learning algorithms and ANNs. The Encog library (<a class="ulink" href="http://github.com/encog">http://github.com/encog</a>) has two primary implementations: one in Java and one in .NET. We can use the Enclog library to easily generate customized ANNs to model both supervised and unsupervised machine learning problems.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/>Note</h3><p>The Enclog library can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div class="informalexample"><pre class="programlisting">[org.encog/encog-core "3.1.0"]
[enclog "0.6.3"]</pre></div><p>Note that the Enclog library requires the Encog Java library as a dependency.</p><p>For the example that will follow, the namespace declaration should look similar to the following declaration:</p><div class="informalexample"><pre class="programlisting">(ns my-namespace
  (:use [enclog nnets training]))</pre></div></div></div><p>We can create an ANN from the Enclog library using the <code class="literal">neural-pattern</code> and <code class="literal">network</code> functions from the <code class="literal">enclog.nnets</code> namespace. The <code class="literal">neural-pattern</code> function<a id="id393" class="indexterm"/> is used to specify a neural network model for the ANN. The <code class="literal">network</code> function accepts a neural network model returned from the <code class="literal">neural-pattern</code> function and creates a new ANN. We can provide several <a id="id394" class="indexterm"/>options to the <code class="literal">network</code> function depending on the specified neural network model. A feed-forward multilayer perceptron network is defined as follows:</p><div class="informalexample"><pre class="programlisting">(def mlp (network (neural-pattern :feed-forward)
                  :activation :sigmoid
                  :input      2
                  :output     1
                  :hidden     [3]))</pre></div><p>For a feed-forward neural network, we can specify the activation function with the <code class="literal">:activation</code> key to the <code class="literal">network</code> function. For our example, we used the sigmoid function, which is specified as <code class="literal">:sigmoid</code>, as the activation function for the ANNs nodes. We also specified the number of nodes in the input, output, and hidden layers of the ANN using the <code class="literal">:input</code>, <code class="literal">:output</code>, and <code class="literal">:hidden</code> keys.</p><p>To train an ANN created by the <code class="literal">network</code> function with some sample data, we use the <code class="literal">trainer</code> and <code class="literal">train</code> functions from the <code class="literal">enclog.training</code> namespace. The learning algorithm to be used to train the ANN must be specified as the first parameter to the <code class="literal">trainer</code> function. For the backpropagation algorithm, this parameter is the <code class="literal">:back-prop</code> keyword. The value returned <a id="id395" class="indexterm"/>by the trainer function represents an ANN as well as the learning algorithm to be used to train the ANN. The <code class="literal">train</code> function is then used to actually run the specified training algorithm on the ANN. We will do this with the help of the following code:</p><div class="informalexample"><pre class="programlisting">(defn train-network [network data trainer-algo]
  (let [trainer (trainer trainer-algo
                         :network network
                         :training-set data)]
    (train trainer 0.01 1000 []))) ;; 0.01 is the expected error</pre></div><p>The <code class="literal">train-network</code> function defined in the preceding code takes three parameters. The first parameter is an ANN created by the network function, the second parameter is the training data to be used to train the ANN, and the third parameter specifies the learning algorithm by which the ANN must be trained. As shown in the preceding code, we can specify the ANN and the training data to the <code class="literal">trainer</code> function<a id="id396" class="indexterm"/> using the key parameters, <code class="literal">:network</code> and <code class="literal">:training-set</code>. The <code class="literal">train</code> function is then used to run the training algorithm on the ANN using the sample data. We can specify the expected error in the ANN and the maximum number of iterations to run the training algorithm as the first and second parameters to the <code class="literal">train</code> function<a id="id397" class="indexterm"/>. In the preceding example, the desired error is <code class="literal">0.01</code>, and the maximum number of iterations is 1000. The last parameter passed to the <code class="literal">train</code> function is a vector <a id="id398" class="indexterm"/>specifying the behaviors of the ANN, and we ignore it by passing it as an empty vector.</p><p>The training data to be used to run the training algorithm on the ANN can be created using Enclog's <code class="literal">data</code> function. For example, we can create a training data for a logical XOR gate using the <code class="literal">data</code> function as follows:</p><div class="informalexample"><pre class="programlisting">(def dataset
  (let [xor-input [[0.0 0.0] [1.0 0.0] [0.0 1.0] [1.0 1.0]]
        xor-ideal [[0.0]     [1.0]     [1.0]     [0.0]]]
        (data :basic-dataset xor-input xor-ideal)))</pre></div><p>The <code class="literal">data</code> function requires the type of data as the first parameter of the function, followed by the input and output values of the training data as vectors. For our example, we will use the <code class="literal">:basic-dataset</code> and <code class="literal">:basic </code>parameters. The <code class="literal">:basic-dataset</code> keyword can be used to create training<a id="id399" class="indexterm"/> data, and the <code class="literal">:basic </code>keyword can be used to specify a set of input values.</p><p>Using the data defined by the <code class="literal">dataset</code> variable and the <code class="literal">train-network</code> function<a id="id400" class="indexterm"/>, we can train the ANN's <code class="literal">MLP</code> to model the output of an XOR gate as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (def MLP (train-network mlp dataset :back-prop))
Iteration # 1 Error: 26.461526% Target-Error: 1.000000%
Iteration # 2 Error: 25.198031% Target-Error: 1.000000%
Iteration # 3 Error: 25.122343% Target-Error: 1.000000%
Iteration # 4 Error: 25.179218% Target-Error: 1.000000%
...
...
Iteration # 999 Error: 3.182540% Target-Error: 1.000000%
Iteration # 1,000 Error: 3.166906% Target-Error: 1.000000%
#'user/MLP</pre></div><p>As shown by the preceding output, the trained ANN has an error of about 3.16 percent. We can now use the trained ANN to predict the output of a set of input values. To do this, we use the Java <code class="literal">compute</code> and <code class="literal">getData</code> methods, which are specified by <code class="literal">.compute</code> and <code class="literal">.getData</code> respectively. We can define a simple helper function to call the <code class="literal">.compute</code> method for a vector of input values and round the output to a binary value as follows:</p><div class="informalexample"><pre class="programlisting">(defn run-network [network input]
  (let [input-data (data :basic input)
        output     (.compute network input-data)
        output-vec (.getData output)]
    (round-output output-vec)))</pre></div><p>We can now use the <code class="literal">run-network</code> function<a id="id401" class="indexterm"/> to test the trained ANN using a vector of input values, as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (run-network MLP [1 1])
[0]
user&gt; (run-network MLP [1 0])
[1]
user&gt; (run-network MLP [0 1])
[1]
user&gt; (run-network MLP [0 0])
[0]</pre></div><p>As shown in the preceding<a id="id402" class="indexterm"/> code, the trained ANN represented by <code class="literal">MLP</code> completely matches the behavior of an XOR gate.</p><p>In conclusion, the Enclog library gives us a small set of powerful functions that can be used to build ANNs. In the preceding example, we explored a feed-forward multilayer perceptron model. The library provides several other ANN models, such as <span class="strong"><strong>Adaptive Resonance Theory</strong></span> (<span class="strong"><strong>ART</strong></span>)<a id="id403" class="indexterm"/>, <span class="strong"><strong>Self-Organizing Maps</strong></span> (<span class="strong"><strong>SOM</strong></span>), and Elman networks. The Enclog library also allows us to customize the activation function of the nodes in a particular neural network model. For the feed-forward network in our example, we've used the sigmoid function. <a id="id404" class="indexterm"/>Several mathematical functions, such as sine, hyperbolic tan, logarithmic, and linear functions, are also supported by the library. There are also several machine learning algorithms supported by the Enclog library that can be used to train an ANN.</p></div>
<div class="section" title="Understanding recurrent neural networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Understanding recurrent neural networks</h1></div></div></div><p>We will now switch our focus to recurrent neural networks<a id="id405" class="indexterm"/> and study a simple recurrent neural network model. An<a id="id406" class="indexterm"/> <span class="strong"><strong>Elman neural network</strong></span><a id="id407" class="indexterm"/> is a simple recurrent ANN with a single input, output, and hidden layer. There is also an extra <span class="emphasis"><em>context layer</em></span> of neural nodes. Elman neural networks are used to simulate short-term memory in supervised and unsupervised machine learning problems. Enclog does include support for Elman neural networks, and we will demonstrate how we can build an Elman neural network using the Enclog library.</p><p>The context layer<a id="id408" class="indexterm"/> of an Elman neural network receives unweighted inputs from the hidden layer of the ANN. In this way, the ANN can remember the previous values that we generated using the hidden layer and use these values to affect the predicted value. Thus, the context layer serves as a type of short-term memory for the ANN. An Elman neural network can be illustrated by the following diagram:</p><div class="mediaobject"><img src="graphics/4351OS_04_67.jpg" alt="Understanding recurrent neural networks"/></div><p>The structure<a id="id409" class="indexterm"/> of an Elman network, as depicted by the preceding diagram, resembles that of a feed-forward multilayer perceptron ANN. An Elman network adds an extra context layer of neural nodes to the ANN. The Elman network illustrated in the preceding diagram takes two inputs and produces two outputs. The input and hidden layers of the Elman network add an extra bias input, similar to a multilayer perceptron. The activations of the hidden layers' neurons are fed directly to the two context nodes <span class="inlinemediaobject"><img src="graphics/4351OS_04_68.jpg" alt="Understanding recurrent neural networks"/></span> and <span class="inlinemediaobject"><img src="graphics/4351OS_04_69.jpg" alt="Understanding recurrent neural networks"/></span>. The values stored in these context nodes are then used later by the nodes in the hidden layer of the ANN to recollect the previous activations to determine the new activation values.</p><p>We can create an Elman network<a id="id410" class="indexterm"/> that specifies the <code class="literal">:elman</code> keyword to the <code class="literal">neural-pattern</code> function from the Enclog library as follows:</p><div class="informalexample"><pre class="programlisting">(def elman-network (network (neural-pattern :elman)
                             :activation :sigmoid
                             :input      2
                             :output     1
                             :hidden     [3]))</pre></div><p>To train the<a id="id411" class="indexterm"/> Elman network, we can use the resilient propagation algorithm (for more information, refer to <span class="emphasis"><em>Empirical Evaluation of the Improved Rprop Learning Algorithm</em></span>). This algorithm can also be used to train other recurrent networks supported by Enclog. Interestingly, the resilient propagation algorithm can be used to train feed-forward networks as well. This algorithm also performs significantly better than the backpropagation learning algorithm. Although a complete description of this algorithm is beyond the scope of this book, the reader is encouraged to learn more about this learning algorithm. The resilient propagation algorithm is specified as the <code class="literal">:resilient-prop</code> keyword to the <code class="literal">train-network</code> function, which we had defined earlier. We can train the Elman neural network using the <code class="literal">train-network</code> function<a id="id412" class="indexterm"/> and the <code class="literal">dataset</code> variable as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (def EN (train-network elman-network dataset 
                             :resilient-prop))
Iteration # 1 Error: 26.461526% Target-Error: 1.000000%
Iteration # 2 Error: 25.198031% Target-Error: 1.000000%
Iteration # 3 Error: 25.122343% Target-Error: 1.000000%
Iteration # 4 Error: 25.179218% Target-Error: 1.000000%
...
...
Iteration # 99 Error: 0.979165% Target-Error: 1.000000%
#'user/EN</pre></div><p>As shown in the preceding code, the resilient propagation algorithm requires a relatively smaller number of iterations in comparison to the backpropagation algorithm. We can now use this trained ANN to simulate an XOR gate just like we did in the previous example.</p><p>In summary, <a id="id413" class="indexterm"/>recurrent neural network models and training algorithms are the other useful models that can be used to model classification or regression problems using ANNs.</p></div>
<div class="section" title="Building SOMs"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Building SOMs</h1></div></div></div><p>SOM (pronounced as <span class="strong"><strong>ess-o-em</strong></span>) is another interesting ANN model that is useful for unsupervised learning. SOMs are used <a id="id414" class="indexterm"/>in several practical applications such as handwriting and image recognition. We will also revisit SOMs when we discuss clustering in <a class="link" href="ch07.html" title="Chapter 7. Clustering Data">Chapter 7</a>, <span class="emphasis"><em>Clustering Data</em></span>.</p><p>In unsupervised learning, the sample data contains no expected output values, and the ANN must recognize and match patterns from the input data entirely on its own. SOMs are used for <span class="emphasis"><em>competitive learning</em></span>, which is a special class of unsupervised learning in which the neurons in the output layer of the ANN compete among themselves for activation. The activated neuron determines the final output value of the ANN, and hence, the activated neuron is also termed as a <a id="id415" class="indexterm"/>
<span class="strong"><strong>winning neuron</strong></span>.</p><p>Neurobiological studies have shown that different sensory inputs sent to the brain are mapped to the corresponding areas of the brain's <span class="emphasis"><em>cerebral cortex</em></span> in an orderly pattern. Thus, neurons that deal with closely related operations are kept close together. This is known as the <span class="strong"><strong>principle of topographic formation</strong></span><a id="id416" class="indexterm"/>, and SOMs are, in fact, modeled on this behavior.</p><p>An SOM essentially transforms input data with a large number of dimensions to a low-dimensional discrete map. The SOM is trained by placing the neurons at the nodes of this map. This internal map of the SOM usually has one or two dimensions. The neurons in the SOM become <span class="emphasis"><em>selectively tuned</em></span> to the patterns in the input values. When a particular neuron in the SOM is activated for a particular input pattern, its neighboring neurons tend to get more excited and more tuned to the pattern in the input values. This behavior is termed as <span class="strong"><strong>lateral interaction</strong></span><a id="id417" class="indexterm"/> of a set of neurons. An SOM, thus, finds patterns in the input data. When a similar pattern is found in a set of inputs, the SOM recognizes this pattern. The layers of neural nodes in an SOM can be illustrated as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_70.jpg" alt="Building SOMs"/></div><p>An SOM has an input layer and a computational layer, as depicted by the preceding diagram. The computational layer is also termed as the <span class="strong"><strong>feature map</strong></span><a id="id418" class="indexterm"/> of the SOM. The input nodes map the input values to several neurons in the computational layer. Each node in the computational<a id="id419" class="indexterm"/> layer has its output connected to its neighboring node, and each of these connections has a weight associated with it. These weights are termed as the <a id="id420" class="indexterm"/>
<span class="strong"><strong>connection weights</strong></span><a id="id421" class="indexterm"/> of the feature map. The SOM remembers patterns in the input values by adjusting the connection weights of the nodes in its computational layer.</p><p>The self-organizing process<a id="id422" class="indexterm"/> of an SOM can be described as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The connection weights are first initialized to random values.</li><li class="listitem">For each input pattern, the neural nodes in the computational layer compute a value using a discriminant function. These values are then used to decide the winning neuron.</li><li class="listitem">The neuron with the least value for the discriminant function is selected, and the connection weights to its surrounding neurons are modified to be activated for similar patterns in the input data.</li></ol></div><p>The weights must be modified such that the value produced by the discriminant function for the neighboring nodes is reduced for the given pattern in the input. Thus, the winning node and its surrounding nodes produce higher output or activation values for similar patterns in the input data. The amount of change by which the weights are adjusted depends on the learning rate specified to the training algorithm.</p><p>For a given number<a id="id423" class="indexterm"/> of dimensions <span class="emphasis"><em>D</em></span> in the input data, the discriminant function can be formally defined as follows:</p><div class="mediaobject"><img src="graphics/4351OS_04_71.jpg" alt="Building SOMs"/></div><p>In the preceding equation, the term <span class="inlinemediaobject"><img src="graphics/4351OS_04_72.jpg" alt="Building SOMs"/></span> is the weight vector of the <span class="inlinemediaobject"><img src="graphics/4351OS_04_73.jpg" alt="Building SOMs"/></span> neuron in the SOM. The length of the vector <span class="inlinemediaobject"><img src="graphics/4351OS_04_72.jpg" alt="Building SOMs"/></span> is equal to the number of neurons connected to the <span class="inlinemediaobject"><img src="graphics/4351OS_04_73.jpg" alt="Building SOMs"/></span> neuron.</p><p>Once we have selected the winning neuron in an SOM, we must select the neighboring neurons of the winning neuron. We must adjust the weights of these neighboring neurons along with the weight of the winning neuron. A variety of schemes can be used for the selection of the winning neuron's neighboring nodes. In the simplest case, we can select a single neighboring neuron. </p><p>We can alternatively use the <code class="literal">bubble</code> function or the <code class="literal">radial bias</code> function<a id="id424" class="indexterm"/> to select a group of neighboring neurons surrounding the winning neuron (for more information, refer to <span class="emphasis"><em>Multivariable functional interpolation and adaptive networks</em></span>).</p><p>To train<a id="id425" class="indexterm"/> an SOM, we must perform the following steps as part of the training algorithm:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Set the weights of the nodes in the computational layer to random values.</li><li class="listitem">Select a sample input pattern from the training data.</li><li class="listitem">Find the winning neuron for the selected set of input patterns.</li><li class="listitem">Update the weights of the winning neuron and its surrounding nodes.</li><li class="listitem">Repeat steps 2 to 4 for all the samples in the training data.</li></ol></div><p>The Enclog library does support the SOM neural network model and training algorithm. We can create and train an SOM from the Enclog library as follows:</p><div class="informalexample"><pre class="programlisting">(def som (network (neural-pattern :som) :input 4 :output 2))

(defn train-som [data]
  (let [trainer (trainer :basic-som :network som
                         :training-set data
                         :learning-rate 0.7
                         :neighborhood-fn 
          (neighborhood-F :single))]
    (train trainer Double/NEGATIVE_INFINITY 10 [])))</pre></div><p>The <code class="literal">som</code> variable<a id="id426" class="indexterm"/> appearing in the preceding code represents an SOM. The <code class="literal">train-som</code> function can be used to train the SOM. The SOM training algorithm is specified as <code class="literal">:basic-som</code>. Note that we specify the learning rate as <code class="literal">0.7</code> using the <code class="literal">:learning-rate</code> key. </p><p>The <code class="literal">:neighborhood-fn</code> key passed to the <code class="literal">trainer</code> function<a id="id427" class="indexterm"/> in the preceding code specifies how we select the neighbors of the winning node in the SOM for a given set of input values. We specify<a id="id428" class="indexterm"/> that a single neighboring node of the winning node must be selected with the help of <code class="literal">(neighborhood-F :single)</code>. We can also specify different neighborhood functions. For example, we can specify the <code class="literal">bubble</code> function as <code class="literal">:bubble</code> or the <code class="literal">radial basis</code> function as <code class="literal">:rbf</code>.</p><p>We can use the <code class="literal">train-som</code> function<a id="id429" class="indexterm"/> to train the SOM with some input patterns. Note that the training data to be used to train the SOM will not have any output values. The SOM must recognize patterns in the input data on its own. Once the SOM is trained, we can use the Java <code class="literal">classify</code> method<a id="id430" class="indexterm"/> to detect patterns in the input. For the following example, we provide only two input patterns to train the SOM:</p><div class="informalexample"><pre class="programlisting">(defn train-and-run-som []
  (let [input [[-1.0, -1.0, 1.0, 1.0 ]
               [1.0, 1.0, -1.0, -1.0]]
        input-data (data :basic-dataset input nil) ;no ideal data
        SOM        (train-som input-data)
        d1         (data :basic (first input))
        d2         (data :basic (second input))]
    (println "Pattern 1 class:" (.classify SOM d1))
    (println "Pattern 2 class:" (.classify SOM d2))
    SOM))</pre></div><p>We can run the <code class="literal">train-and-run-som</code> function<a id="id431" class="indexterm"/> defined in the preceding code and observe that the SOM recognizes the two input patterns in the training data as two distinct classes as follows:</p><div class="informalexample"><pre class="programlisting">user&gt; (train-and-run-som)
Iteration # 1 Error: 2.137686% Target-Error: NaN
Iteration # 2 Error: 0.641306% Target-Error: NaN
Iteration # 3 Error: 0.192392% Target-Error: NaN
...
...
Iteration # 9 Error: 0.000140% Target-Error: NaN
Iteration # 10 Error: 0.000042% Target-Error: NaN
Pattern 1 class: 1
Pattern 2 class: 0
#&lt;SOM org.encog.neural.som.SOM@19a0818&gt;</pre></div><p>In conclusion, SOMs are a<a id="id432" class="indexterm"/> great model for dealing with unsupervised learning problems. Also, we can easily build SOMs to model such problems using the Enclog library.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Summary</h1></div></div></div><p>We have explored a few interesting ANN models in this chapter. These models can be applied to solve both supervised and unsupervised machine learning problems. The following are some of the other points that we covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have explored the necessity of ANNs and their broad types, that is, feed-forward and recurrent ANNs.</li><li class="listitem" style="list-style-type: disc">We have studied the multilayer perceptron ANN and the backpropagation algorithm used to train this ANN. We've also provided a simple implementation of the backpropagation algorithm in Clojure using matrices and matrix operations. </li><li class="listitem" style="list-style-type: disc">We have introduced the Enclog library that can be used to build ANNs. This library can be used to model both supervised and unsupervised machine learning problems.</li><li class="listitem" style="list-style-type: disc">We have explored recurrent Elman neural networks, which can be used to produce ANNs with a small error in a relatively less number of iterations. We've also described how we can create and train such an ANN using the Enclog library.</li><li class="listitem" style="list-style-type: disc">We introduced SOMs, which are neural networks that can be applied in the domain of unsupervised learning. We've also described how we can create and train an SOM using the Enclog library.</li></ul></div></div></body></html>