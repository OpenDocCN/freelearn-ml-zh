- en: The Realm of Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习领域
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Array creation in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中创建数组
- en: Data preprocessing using mean removal
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均值移除进行数据预处理
- en: Data scaling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据缩放
- en: Normalization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化
- en: Binarization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二值化
- en: One-hot encoding
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: One-hot 编码
- en: Label encoding
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签编码
- en: Building a linear regressor
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建线性回归器
- en: Computing regression accuracy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算回归精度
- en: Achieving model persistence
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现模型持久化
- en: Building a ridge regressor
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建岭回归器
- en: Building a polynomial regressor
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建多项式回归器
- en: Estimating housing prices
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计房价
- en: Computing the relative importance of features
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算特征的相对重要性
- en: Estimating bicycle demand distribution
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计自行车需求分布
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will use various Python packages, such as NumPy, SciPy, scikit-learn, and
    Matplotlib, during the course of this book to build various things. If you use
    Windows, it is recommended that you use a SciPy-stack-compatible version of Python.
    You can check the list of compatible versions at [http://www.scipy.org/install.html](http://www.scipy.org/install.html)[.](http://www.scipy.org/install.html) These
    distributions come with all the necessary packages already installed. If you use
    MacOS X or Ubuntu, installing these packages is fairly straightforward. Here are
    some useful links for installation and documentation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，我们将使用各种Python包，如NumPy、SciPy、scikit-learn和Matplotlib来构建各种东西。如果您使用Windows，建议您使用与SciPy兼容的Python版本。您可以在[http://www.scipy.org/install.html](http://www.scipy.org/install.html)查看兼容版本列表。这些发行版已经预装了所有必要的包。如果您使用MacOS
    X或Ubuntu，安装这些包相对简单。以下是安装和文档的一些有用链接：
- en: NumPy: [https://www.numpy.org/devdocs/user/install.html](https://www.numpy.org/devdocs/user/install.html).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy: [https://www.numpy.org/devdocs/user/install.html](https://www.numpy.org/devdocs/user/install.html).
- en: SciPy: [http://www.scipy.org/install.html.](http://www.scipy.org/install.html)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy: [http://www.scipy.org/install.html.](http://www.scipy.org/install.html)
- en: Scikit-learn: [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn: [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html).
- en: Matplotlib: [https://matplotlib.org/users/installing.html](https://matplotlib.org/users/installing.html).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib: [https://matplotlib.org/users/installing.html](https://matplotlib.org/users/installing.html).
- en: Make sure that you have these packages installed on your machine before you
    proceed. In each recipe, we will give a detailed explanation of the functions
    that we will use in order to make it simple and fast.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您的机器上已安装了这些包。在每个食谱中，我们将详细解释我们将使用的函数，以便使其简单快捷。
- en: Introduction
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Machine learning is a multidisciplinary field created at the intersection of,
    and with synergy between, computer science, statistics, neurobiology, and control
    theory. It has played a key role in various fields and has radically changed the
    vision of programming software. For humans, and more generally, for every living
    being, learning is a form of adaptation of a system to its environment through
    experience. This adaptation process must lead to improvement without human intervention.
    To achieve this goal, the system must be able to learn, which means that it must
    be able to extract useful information on a given problem by examining a series
    of examples associated with it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个跨学科领域，它诞生于计算机科学、统计学、神经生物学和控制理论交叉的地方，并且在这些领域之间产生了协同效应。它在各个领域都发挥了关键作用，并且彻底改变了编程软件的视野。对于人类，以及更广泛地，对于每一个生物体，学习是通过经验使系统适应其环境的一种适应形式。这种适应过程必须在没有人类干预的情况下导致改进。为了实现这一目标，系统必须能够学习，这意味着它必须能够通过检查与它相关的一系列示例来提取有关给定问题的有用信息。
- en: If you are familiar with the basics of machine learning, you will certainly
    know what supervised learning is all about. To give you a quick refresher, **supervised
    learning** refers to building a machine learning model that is based on labeled
    samples. The algorithm generates a function which connects input values to a desired
    output via of a set of labeled examples, where each data input has its relative
    output data. This is used to construct predictive models. For example, if we build
    a system to estimate the price of a house based on various parameters, such as
    size, locality, and so on, we first need to create a database and label it. We
    need to tell our algorithm what parameters correspond to what prices. Based on
    this data, our algorithm will learn how to calculate the price of a house using
    the input parameters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉机器学习的基础知识，您当然会知道什么是监督学习。为了快速回顾，**监督学习**指的是基于标记样本构建机器学习模型。算法生成一个函数，通过一组标记示例将输入值连接到期望的输出，其中每个数据输入都有其相对的输出数据。这用于构建预测模型。例如，如果我们构建一个基于各种参数（如大小、地区等）来估算房价的系统，我们首先需要创建一个数据库并对其进行标记。我们需要告诉我们的算法哪些参数对应于哪些价格。基于这些数据，我们的算法将学习如何使用输入参数来计算房价。
- en: '**Unsupervised learning** is in stark contrast to what we just discussed. There
    is no labeled data available here. The algorithm tries to acquire knowledge from
    general input without the help of a set of pre-classified examples that are used
    to build descriptive models. Let''s assume that we have a bunch of data points,
    and we just want to separate them into multiple groups. We don''t exactly know
    what the criteria of separation would be. So, an unsupervised learning algorithm
    will try to separate the given dataset into a fixed number of groups in the best
    possible way. We will discuss unsupervised learning in the upcoming chapters.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**与我们刚才讨论的内容截然不同。这里没有可用的标记数据。算法试图在没有一组用于构建描述性模型的预分类示例的帮助下，从一般输入中获取知识。让我们假设我们有一组数据点，我们只想将它们分成多个组。我们并不确切知道分离的标准会是什么。因此，无监督学习算法将尝试以最佳方式将给定的数据集分成固定数量的组。我们将在接下来的章节中讨论无监督学习。'
- en: In the following recipes, we will look at various data preprocessing techniques.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下菜谱中，我们将探讨各种数据预处理技术。
- en: Array creation in Python
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的数组创建
- en: Arrays are the essential elements of many programming languages. Arrays are
    sequential objects that behave very similarly to lists, except that the types
    of elements contained in them are constrained. The type is specified when the
    object is created using a single character called **type code**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数组是许多编程语言的基本元素。数组是顺序对象，其行为与列表非常相似，只是它们包含的元素类型受到限制。类型是在使用单个字符**类型码**创建对象时指定的。
- en: Getting ready
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will cover an array creation procedure. We will first create
    an array using the NumPy library, and then display its structure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将介绍数组创建过程。我们首先将使用NumPy库创建一个数组，然后显示其结构。
- en: How to do it…
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to create an array in Python:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中创建一个数组：
- en: 'To start off, import the NumPy library as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始，按照以下方式导入NumPy库：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We just imported a necessary package, `numpy`. This is the fundamental package
    for scientific computing with Python. It contains, among other things, the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚导入了一个必要的包，`numpy`。这是Python进行科学计算的基本包。它包含了许多其他内容，例如以下内容：
- en: A powerful N-dimensional array object
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强大的N维数组对象
- en: Sophisticated broadcasting functions
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的广播功能
- en: Tools for integrating C, C++, and FORTRAN code
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于集成C、C++和FORTRAN代码的工具
- en: Useful linear algebra, Fourier transform, and random number capabilities
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有用的线性代数、傅里叶变换和随机数功能
- en: Besides its obvious uses, NumPy is also used as an efficient multidimensional
    container of generic data. Arbitrary data types can be found. This enables NumPy
    to integrate with different types of databases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其明显的用途外，NumPy还用作高效的多维通用数据容器。可以找到任意数据类型。这使得NumPy能够与不同类型的数据库集成。
- en: Remember, to import a library that is not present in the initial distribution
    of Python, you must use the `pip install` command followed by the name of the
    library. This command should be used only once and not every time you run the
    code.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，要导入Python初始发行版中不存在的库，您必须使用`pip install`命令，后跟库的名称。此命令应仅使用一次，而不是每次运行代码时都使用。
- en: 'Let''s create some sample data. Add the following line to the Python Terminal:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一些示例数据。在Python终端中添加以下行：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `np.array` function creates a NumPy array. A NumPy array is a grid of values,
    all of the same type, indexed by a tuple of non-negative integers. `rank` and `shape`
    are essential features of a NumPy array. The `rank` variable is the number of
    dimensions of the array. The `shape` variable is a tuple of integers that returns
    the size of the array along each dimension.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.array`函数创建一个NumPy数组。NumPy数组是一个值网格，所有值类型相同，通过一个非负整数元组进行索引。`rank`和`shape`是NumPy数组的基本特性。`rank`变量是数组的维度数。`shape`变量是一个整数元组，返回数组在每个维度上的大小。'
- en: 'We display the newly created array with this snippet:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用以下代码片段显示新创建的数组：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following result is returned:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We are now ready to operate on this data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始对这个数据进行操作。
- en: How it works…
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: NumPy is an extension package in the Python environment that is fundamental
    for scientific calculation. This is because it adds to the tools that are already
    available, the typical features of N-dimensional arrays, element-by-element operations,
    a massive number of mathematical operations in linear algebra, and the ability
    to integrate and recall source code written in C, C++, and FORTRAN. In this recipe,
    we learned how to create an array using the NumPy library.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy是Python环境中用于科学计算的扩展包，它是基础性的。这是因为它增加了现有工具的典型特性，包括N维数组的典型特征、逐元素操作、线性代数中的大量数学运算，以及集成和调用用C、C++和FORTRAN编写的源代码的能力。在这个菜谱中，我们学习了如何使用NumPy库创建数组。
- en: There's more...
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'NumPy provides us with various tools for creating an array. For example, to
    create a one-dimensional array of equidistant values with numbers from 0 to 10,
    we would use the `arange()` function, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy为我们提供了创建数组的各种工具。例如，要创建一个从0到10的等距值的单维数组，我们将使用`arange()`函数，如下所示：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following result is returned:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To create a numeric array from 0 to 50, with a step of 5 (using a predetermined
    step between successive values), we will write the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个从0到50的数值数组，步长为5（使用预定的步长），我们将编写以下代码：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following array is printed:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的数组被打印出来：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Also, to create a one-dimensional array of 50 numbers between two limit values
    and that are equidistant in this range, we will use the `linspace()` function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要创建一个在两个极限值之间且在此范围内等距的50个数字的单维数组，我们将使用`linspace()`函数：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following result is returned:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These are just some simple samples of NumPy. In the following sections, we will
    delve deeper into the topic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是NumPy的一些简单示例。在接下来的章节中，我们将更深入地探讨这个主题。
- en: See also
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: NumPy developer guide ([https://docs.scipy.org/doc/numpy/dev/](https://docs.scipy.org/doc/numpy/dev/)).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy开发者指南([https://docs.scipy.org/doc/numpy/dev/](https://docs.scipy.org/doc/numpy/dev/)).
- en: NumPy tutorial ([https://docs.scipy.org/doc/numpy/user/quickstart.html](https://docs.scipy.org/doc/numpy/user/quickstart.html)).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy教程([https://docs.scipy.org/doc/numpy/user/quickstart.html](https://docs.scipy.org/doc/numpy/user/quickstart.html)).
- en: NumPy reference ([https://devdocs.io/numpy~1.12/](https://devdocs.io/numpy~1.12/)).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy参考([https://devdocs.io/numpy~1.12/](https://devdocs.io/numpy~1.12/)).
- en: Data preprocessing using mean removal
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用均值移除进行数据预处理
- en: In the real world, we usually have to deal with a lot of raw data. This raw
    data is not readily ingestible by machine learning algorithms. To prepare data
    for machine learning, we have to preprocess it before we feed it into various
    algorithms. This is an intensive process that takes plenty of time, almost 80
    percent of the entire data analysis process, in some scenarios. However, it is
    vital for the rest of the data analysis workflow, so it is necessary to learn
    the best practices of these techniques. Before sending our data to any machine
    learning algorithm, we need to cross check the quality and accuracy of the data.
    If we are unable to reach the data stored in Python correctly, or if we can't
    switch from raw data to something that can be analyzed, we cannot go ahead. Data
    can be preprocessed in many ways—standardization, scaling, normalization, binarization,
    and one-hot encoding are some examples of preprocessing techniques. We will address
    them through simple examples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们通常必须处理大量的原始数据。这些原始数据不能直接被机器学习算法摄取。为了准备机器学习数据，我们必须在将其输入各种算法之前对其进行预处理。这是一个耗时的工作，在某些情况下几乎占整个数据分析过程的80%，然而，对于数据分析工作流程的其余部分来说至关重要，因此有必要学习这些技术的最佳实践。在我们将数据发送到任何机器学习算法之前，我们需要交叉检查数据的质和准确度。如果我们无法正确访问存储在Python中的数据，或者如果我们不能从原始数据切换到可以分析的数据，我们就无法继续。数据可以通过多种方式进行预处理——标准化、缩放、归一化、二值化和独热编码是一些预处理技术的例子。我们将通过简单的示例来介绍它们。
- en: Getting ready
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: '**Standardization** or **mean removal** is a technique that simply centers
    data by removing the average value of each characteristic, and then scales it
    by dividing non-constant characteristics by their standard deviation. It''s usually
    beneficial to remove the mean from each feature so that it''s centered on zero.
    This helps us remove bias from features. The formula used to achieve this is the
    following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化**或**均值移除**是一种通过移除每个特征的均值来简单地将数据中心化的技术，然后通过将非常数特征除以其标准差来缩放它。通常从每个特征中移除均值以便它以零为中心是有益的。这有助于我们从特征中去除偏差。用于实现此目的的公式如下：'
- en: '![](img/8282b2fb-9150-4786-8efd-2ec5f41423be.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8282b2fb-9150-4786-8efd-2ec5f41423be.png)'
- en: 'Standardization results in the rescaling of features, which in turn represents
    the properties of a standard normal distribution:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化导致特征重新缩放，这反过来又代表了标准正态分布的特性：
- en: '**mean **= 0'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值** = 0'
- en: '**sd **= 1'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准差** = 1'
- en: In this formula, **mean** is the mean and **sd** is the standard deviation from
    the mean.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在此公式中，**均值**是均值，**sd**是从均值的标准差。
- en: How to do it...
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s see how to preprocess data in Python:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中预处理数据：
- en: 'Let''s start by importing the library:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入库：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `sklearn` library is a free software machine learning library for the Python
    programming language. It features various classification, regression, and clustering
    algorithms, including **support vector machines** (**SVMs**), random forests,
    gradient boosting, k-means, and DBSCAN, and is designed to interoperate with the
    Python numerical and scientific libraries, NumPy and SciPy.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn` 库是Python编程语言的免费软件机器学习库。它具有各种分类、回归和聚类算法，包括**支持向量机**（**SVMs**）、随机森林、梯度提升、k-means和DBSCAN，并且旨在与Python数值和科学库NumPy和SciPy互操作。'
- en: 'To understand the outcome of mean removal on our data, we first visualize the
    mean and standard deviation of the vector we have just created:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了理解均值移除对我们数据的影响，我们首先可视化我们刚刚创建的向量的均值和标准差：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `mean()` function returns the sample arithmetic mean of data, which can
    be a sequence or an iterator. The `std()` function returns the standard deviation,
    a measure of the distribution of the array elements. The `axis` parameter specifies
    the axis along which these functions are computed (`0` for columns, and `1` for
    rows).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean()` 函数返回数据的样本算术平均值，它可以是序列或迭代器。`std()` 函数返回标准差，它是数组元素分布的度量。`axis` 参数指定这些函数计算的轴（`0`
    为列，`1` 为行）。'
- en: 'The following results are returned:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can proceed with standardization:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以进行标准化：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `preprocessing.scale()` function standardizes a dataset along any axis.
    This method centers the data on the mean and resizes the components in order to
    have a unit variance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocessing.scale()` 函数沿任何轴标准化数据集。此方法将数据中心在均值上，并调整组件的大小以使其具有单位方差。'
- en: 'Now we recalculate the mean and standard deviation on the standardized data:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们重新计算标准化数据上的均值和标准差：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following results are returned:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can see that the mean is almost 0 and the standard deviation is 1.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到平均值几乎为0，标准差为1。
- en: How it works...
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `sklearn.preprocessing` package provides several common utility functions
    and transformer classes to modify the features available in a representation that
    best suits our needs. In this recipe, the `scale()` function has been used (z-score
    standardization).  In summary, the z-score (also called the standard score) represents
    the number of standard deviations by which the value of an observation point or
    data is greater than the mean value of what is observed or measured. Values more
    than the mean have positive z-scores, while values less than the mean have negative
    z-scores. The z-score is a quantity without dimensions that is obtained by subtracting
    the population's mean from a single rough score and then dividing the difference
    by the standard deviation of the population.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.preprocessing`包提供了几个常见的实用函数和转换类，以修改适合我们需求的表示中的特征。在这个配方中，使用了`scale()`函数（z分数标准化）。总结来说，z分数（也称为标准分数）表示观察点或数据的值相对于观察或测量的平均值超出多少个标准差。超过平均值的值有正的z分数，而低于平均值的值有负的z分数。z分数是一个没有维度的量，通过从单个粗略分数中减去总体平均值，然后除以总体的标准差来获得。'
- en: There's more...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Standardization is particularly useful when we do not know the minimum and maximum
    for data distribution. In this case, it is not possible to use other forms of
    data transformation. As a result of the transformation, the normalized values
    do not have a minimum and a fixed maximum. Moreover, this technique is not influenced
    by the presence of outliers, or at least not the same as other methods.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化在我们不知道数据分布的最小值和最大值时特别有用。在这种情况下，无法使用其他形式的数据转换。作为转换的结果，归一化值没有最小值和固定的最大值。此外，这种技术不受异常值存在的影响，或者至少与其他方法不同。
- en: See also
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Scikit-learn''s official documentation of the `sklearn.preprocessing.scale()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的`sklearn.preprocessing.scale()`函数官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html)。
- en: Data scaling
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据缩放
- en: The values of each feature in a dataset can vary between random values. So,
    sometimes it is important to scale them so that this becomes a level playing field.
    Through this statistical procedure, it's possible to compare identical variables
    belonging to different distributions and different variables.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中每个特征值可能在随机值之间变化。因此，有时对它们进行缩放以使这成为一个公平的竞争环境是很重要的。通过这个统计过程，可以比较属于不同分布和不同变量的相同变量。
- en: Remember, it is good practice to rescale data before training a machine learning
    algorithm. With rescaling, data units are eliminated, allowing you to easily compare
    data from different locations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在训练机器学习算法之前重新缩放数据是一种良好的实践。通过缩放，数据单位被消除，这使得你可以轻松地比较来自不同位置的数据。
- en: Getting ready
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We''ll use the **min-max** method (usually called **feature scaling**) to get
    all of the scaled data in the range [0, 1]. The formula used to achieve this is
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**最小-最大**方法（通常称为**特征缩放**）来获取所有缩放数据在范围[0, 1]内。实现这一目标的公式如下：
- en: '![](img/8da27764-09bf-477e-8b25-1e8a0e956188.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8da27764-09bf-477e-8b25-1e8a0e956188.png)'
- en: To scale features between a given minimum and maximum value—in our case, between
    0 and 1—so that the maximum absolute value of each feature is scaled to unit size,
    the `preprocessing.MinMaxScaler()` function can be used.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将特征缩放到给定的最小值和最大值之间——在我们的例子中，在0和1之间——使得每个特征的绝对最大值缩放到单位大小，可以使用`preprocessing.MinMaxScaler()`函数。
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s see how to scale data in Python:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中缩放数据：
- en: 'Let''s start by defining the `data_scaler` variable:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先定义`data_scaler`变量：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we will use the `fit_transform()` method, which fits the data and then
    transforms it (we will use the same data as in the previous recipe):'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`fit_transform()`方法，该方法先拟合数据然后转换它（我们将使用与上一个配方相同的数据）：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A NumPy array of a specific shape is returned. To understand how this function
    has transformed data, we display the minimum and maximum of each column in the
    array.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个特定形状的NumPy数组。为了理解这个函数如何转换数据，我们显示数组中每列的最小值和最大值。
- en: 'First, for the starting data and then for the processed data:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，对于原始数据，然后对于处理后的数据：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following results are returned:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s do the same for the scaled data using the following code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码对缩放数据进行相同的操作：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following results are returned:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果被返回：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: After scaling, all the feature values range between the specified values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放后，所有特征值都在指定的值之间。
- en: 'To display the scaled array, we will use the following code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要显示缩放数组，我们将使用以下代码：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will be displayed as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, all the data is included in the same interval.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有数据都包含在同一个区间内。
- en: How it works...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: When data has different ranges, the impact on response variables might be higher
    than the one with a lesser numeric range, which can affect the prediction accuracy.
    Our goal is to improve predictive accuracy and ensure this doesn't happen. Hence,
    we may need to scale values under different features so that they fall within
    a similar range. Through this statistical procedure, it's possible to compare
    identical variables belonging to different distributions and different variables
    or variables expressed in different units.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据有不同的范围时，对响应变量的影响可能比数值范围较小的数据更高，这可能会影响预测精度。我们的目标是提高预测精度并确保这种情况不会发生。因此，我们可能需要在不同特征下缩放值，使它们落在相似的范围之内。通过这个统计过程，可以比较属于不同分布的相同变量或不同变量，或者以不同单位表示的变量。
- en: There's more...
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Feature scaling consists of limiting the excursion of a set of values within
    a certain predefined interval. It guarantees that all functionalities have the
    exact same scale, but does not handle anomalous values well. This is because extreme
    values become the extremes of the new range of variation. In this way, the actual
    values are compressed by keeping the distance to the anomalous values.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放 包括限制一组值在一定预定义区间内的波动。这保证了所有 功能都有相同的尺度，但处理异常值并不好。这是因为极端值成为新变化范围的极端。通过这种方式，实际值通过保持与异常值的距离而被压缩。
- en: See also
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'Scikit-learn''s official documentation of the `sklearn.preprocessing.MinMaxScaler()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的`sklearn.preprocessing.MinMaxScaler()`函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).
- en: Normalization
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: Data normalization is used when you want to adjust the values in the feature
    vector so that they can be measured on a common scale. One of the most common
    forms of normalization that is used in machine learning adjusts the values of
    a feature vector so that they sum up to 1.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化用于当你想要调整特征向量中的值，以便它们可以在一个共同的尺度上进行测量时。在机器学习中使用的最常见的归一化形式之一是将特征向量的值调整为总和为1。
- en: Getting ready
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To normalize data, the `preprocessing.normalize()` function can be used.  This
    function scales input vectors individually to a unit norm (vector length). Three
    types of norms are provided, **l[1]**, **l[2]**, or **max**, and they are explained
    next. If *x* is the vector of covariates of length *n*, the normalized vector
    is *y=x/z*, where *z* is defined as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要归一化数据，可以使用`preprocessing.normalize()`函数。此函数将输入向量单独缩放到单位范数（向量长度）。提供了三种类型的范数，**l[1]**，**l[2]**或**max**，它们将在下面解释。如果*x*是长度为*n*的协变量向量，则归一化向量为*y=x/z*，其中*z*定义如下：
- en: '![](img/de658bc4-42c5-4a71-b740-b2b037556c0b.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de658bc4-42c5-4a71-b740-b2b037556c0b.png)'
- en: The **norm** is a function that assigns a positive length to each vector belonging
    to a vector space, except 0.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**范数**是一个函数，它将正长度分配给属于向量空间中的每个向量，除了0。'
- en: How to do it...
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s see how to normalize data in Python:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中归一化数据：
- en: 'As we said, to normalize data, the `preprocessing.normalize()` function can
    be used as follows (we will use the same data as in the previous recipe):'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们所说，要归一化数据，可以使用`preprocessing.normalize()`函数如下（我们将使用与上一个配方相同的数据）：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To display the normalized array, we will use the following code:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要显示归一化数组，我们将使用以下代码：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following output is returned:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出被返回：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is used a lot to make sure that datasets don't get boosted artificially
    due to the fundamental nature of their features.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于确保数据集不会因为其特征的基本性质而人为地被提升。
- en: 'As already mentioned, the normalized array along the columns (features) must
    return a sum equal to 1\. Let''s check this for each column:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，沿列（特征）归一化的数组必须返回一个等于1的总和。让我们检查每一列：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the first line of code, we used the `np.abs()` function to evaluate the
    absolute value of each element in the array. In the second row of code, we used
    the `sum()` function to calculate the sum of each column (`axis=0`). The following
    results are returned:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第一行，我们使用了`np.abs()`函数来计算数组中每个元素的绝对值。在代码的第二行，我们使用了`sum()`函数来计算每列的总和（`axis=0`）。以下结果被返回：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Therefore, the sum of the absolute value of the elements of each column is equal
    to 1, so the data is normalized.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每列元素绝对值的总和等于1，所以数据被归一化。
- en: How it works...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we normalized the data at our disposal to the unitary norm.
    Each sample with at least one non-zero component was rescaled independently of
    other samples so that its norm was equal to one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将可用的数据归一化到单位范数。每个至少有一个非零成分的样本都会独立于其他样本进行缩放，使其范数等于1。
- en: There's more...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Scaling inputs to a unit norm is a very common task in text classification and
    clustering problems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分类和聚类问题中，将输入缩放到单位范数是一个非常常见的任务。
- en: See also
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'Scikit-learn''s official documentation of the `sklearn.preprocessing.normalize()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的`sklearn.preprocessing.normalize()`函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html)。
- en: Binarization
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二值化
- en: Binarization is used when you want to convert a numerical feature vector into
    a Boolean vector. In the field of digital image processing, image binarization
    is the process by which a color or grayscale image is transformed into a binary
    image, that is, an image with only two colors (typically, black and white).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 二值化用于将数值特征向量转换为布尔向量。在数字图像处理领域，图像二值化是将彩色或灰度图像转换为二值图像的过程，即只包含两种颜色（通常是黑白）的图像。
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This technique is used for the recognition of objects, shapes, and, specifically,
    characters. Through binarization, it is possible to distinguish the object of
    interest from the background on which it is found. Skeletonization is instead
    an essential and schematic representation of the object, which generally preludes
    the subsequent real recognition.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术用于对象的识别、形状的识别，特别是字符的识别。通过二值化，可以区分感兴趣的对象及其背景。相反，骨架化是对象的一种基本和简化的表示，通常作为后续真实识别的前奏。
- en: How to do it...
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s see how to binarize data in Python:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中二值化数据：
- en: 'To binarize data, we will use the  `preprocessing.Binarizer()` function as
    follows (we will use the same data as in the previous recipe):'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要进行二值化，我们将使用`preprocessing.Binarizer()`函数如下（我们将使用与上一个食谱相同的数据）：
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `preprocessing.Binarizer()` function binarizes data according to an imposed
    `threshold`. Values greater than the `threshold` map to 1, while values less than
    or equal to the `threshold` map to 0\. With the default `threshold` of 0, only
    positive values map to 1\. In our case, the `threshold` imposed is `1.4`, so values
    greater than `1.4` are mapped to 1, while values less than `1.4` are mapped to
    0.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocessing.Binarizer()`函数根据设定的`threshold`值对数据进行二值化。大于`threshold`的值映射为1，而小于或等于`threshold`的值映射为0。默认的`threshold`值为0，只有正数映射为1。在我们的例子中，设定的`threshold`值为`1.4`，所以大于`1.4`的值映射为1，而小于`1.4`的值映射为0。'
- en: 'To display the binarized array, we will use the following code:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要显示二值化数组，我们将使用以下代码：
- en: '[PRE30]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following output is returned:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下输出：
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This is a very useful technique that's usually used when we have some prior
    knowledge of the data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常有用的技术，通常在我们对数据有一些先验知识时使用。
- en: How it works...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we binarized the data. The fundamental idea of ​​this technique
    is to draw a fixed demarcation line. It is therefore a matter of finding an appropriate
    threshold and affirming that all the points of the image whose light intensity
    is below a certain value belong to the object (background), and all the points
    with greater intensity belong to the background (object).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们对数据进行二值化。这种技术的核心思想是画一条固定的分界线。因此，这是一个找到适当的阈值，并断言所有光强度低于一定值的图像点属于对象（背景），而所有光强度更高的点属于背景（对象）。
- en: There's more...
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Binarization is a widespread operation on count data, in which the analyst can
    decide to consider only the presence or absence of a characteristic rather than
    a quantified number of occurrences. Otherwise, it can be used as a preprocessing
    step for estimators that consider random Boolean variables.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 二值化是计数数据上的一种常见操作，分析师可以选择只考虑特征的呈现或缺失，而不是发生次数的数量。否则，它可以作为考虑随机布尔变量的估计器的预处理步骤。
- en: See also
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'Scikit-learn''s official documentation of the `sklearn.preprocessing.Binarizer()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的`sklearn.preprocessing.Binarizer()`函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html)。
- en: One-hot encoding
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: One-hot编码
- en: We often deal with numerical values that are sparse and scattered all over the
    place. We don't really need to store these values. This is where one-hot encoding
    comes into the picture. We can think of one-hot encoding as a tool that tightens
    feature vectors. It looks at each feature and identifies the total number of distinct
    values. It uses a *one-of-k* scheme to encode values. Each feature in the feature
    vector is encoded based on this scheme. This helps us to be more efficient in
    terms of space.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常处理稀疏且散布各处的数值。我们实际上不需要存储这些值。这就是one-hot编码发挥作用的地方。我们可以将one-hot编码视为一个工具，它可以紧缩特征向量。它查看每个特征并确定不同值的总数。它使用一个*one-of-k*方案来编码值。特征向量中的每个特征都是基于这个方案进行编码的。这有助于我们在空间方面更加高效。
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let's say we are dealing with four-dimensional feature vectors. To encode the
    *nth* feature in a feature vector, the encoder will go through the *nth* feature
    in each feature vector and count the number of distinct values. If the number
    of distinct values is *k*, it will transform the feature into a *k*-dimensional
    vector where only one value is 1 and all other values are 0. Let's take a simple
    example to understand how this works.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在处理四维特征向量。为了编码特征向量中的*n*特征，编码器将遍历每个特征向量中的*n*特征，并计算不同值的数量。如果不同值的数量是*k*，它将特征转换为一个*k*维向量，其中只有一个值为1，其余值都为0。让我们用一个简单的例子来理解它是如何工作的。
- en: How to do it...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let''s see how to encode data in Python:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中编码数据：
- en: 'Let''s take an array with four rows (vectors) and three columns (features):'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以一个有四行（向量）和三列（特征）的数组为例：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following result is printed:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出以下结果：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s analyze the values present in each column (feature):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析每个列（特征）中存在的值：
- en: 'The first feature has two possible values: 0, 1'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个特征有两个可能的值：0，1
- en: 'The second feature has three possible values: 0, 1, 2'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个特征有三个可能的值：0，1，2
- en: 'The third feature has four possible values: 0, 1, 2, 3'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个特征有四个可能的值：0，1，2，3
- en: 'So, overall, the sum of the possible values present in each feature is given
    by 2 + 3 + 4 = 9\. This means that 9 entries are required to uniquely represent
    any vector. The three features will be represented as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总的来说，每个特征中可能存在的值的总和为2 + 3 + 4 = 9。这意味着需要9个条目来唯一表示任何向量。这三个特征将如下表示：
- en: Feature 1 starts at index `0`
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征1从索引`0`开始
- en: Feature 2 starts at index `2`
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征2从索引`2`开始
- en: Feature 3 starts at index `5`
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征3从索引`5`开始
- en: 'To encode categorical integer features as a one-hot numeric array, the `preprocessing.OneHotEncoder()`
    function can be used as follows:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将分类整数特征编码为one-hot数值数组，可以使用`preprocessing.OneHotEncoder()`函数如下：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The first row of code sets the encoder, then the `fit()` function fits the `OneHotEncoder` object
    to a data array.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行代码设置了编码器，然后`fit()`函数将`OneHotEncoder`对象拟合到数据数组。
- en: 'Now we can transform the data array using one-hot encoding. To do this, the `transform()`
    function will be used as follows:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用单热编码转换数据数组。为此，将使用 `transform()` 函数，如下所示：
- en: '[PRE35]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If you were to print `encoded_vector`, the expected output would be:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打印 `encoded_vector`，预期的输出将是：
- en: '[PRE36]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The result is clear: the first feature (1) has an index of 1, the second feature
    (3) has an index of 4, and the third feature (3) has an index of 8\. As we can
    verify, only these positions are occupied by a 1; all the other positions have
    a 0\. Remember that Python indexes the positions starting from 0, so the 9 entries
    will have indexes from 0 to 8.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很清晰：第一个特征（1）的索引为1，第二个特征（3）的索引为4，第三个特征（3）的索引为8。正如我们可以验证的那样，只有这些位置被1占据；所有其他位置都是0。记住，Python从0开始索引位置，所以9个条目将有从0到8的索引。
- en: How it works...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `preprocessing.OneHotEncoder()` function encodes categorical integer features
    as a one-hot numeric array. Starting from an array of integers or strings that
    denotes the values assumed by categorical characteristics (discrete), this function
    encodes the characteristics using a one-hot coding scheme, returning dummy variables.
    This creates a binary column for each category and returns a sparse array or a
    dense array.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocessing.OneHotEncoder()` 函数将分类整数特征编码为单热数值数组。从表示分类特征（离散）所取值的整数或字符串数组开始，此函数使用单热编码方案对特征进行编码，返回虚拟变量。这为每个类别创建一个二进制列，并返回一个稀疏数组或密集数组。'
- en: There's more...
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: It often happens that you have to convert categorical data. This is due to the
    fact that many machine learning algorithms can't work directly with categorical
    data. To use these methods, it is necessary to first transform categorical data
    into numerical data. This is required for both input and output variables.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 经常发生的情况是你需要转换分类数据。这是因为许多机器学习算法不能直接处理分类数据。为了使用这些方法，首先需要将分类数据转换为数值数据。这既适用于输入变量也适用于输出变量。
- en: See also
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'Scikit-learn''s official documentation of the `sklearn.preprocessing.OneHotEncoder()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn的 `sklearn.preprocessing.OneHotEncoder()` 函数的官方文档：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)。
- en: Label encoding
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签编码
- en: In supervised learning, we usually deal with a variety of labels. These can
    be either numbers or words. If they are numbers, then the algorithm can use them
    directly. However, labels often need to be in a human-readable form. So, people
    usually label the training data with words.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们通常处理各种标签。这些可以是数字或单词。如果是数字，则算法可以直接使用它们。然而，标签通常需要以人类可读的形式存在。因此，人们通常用单词标记训练数据。
- en: Getting ready
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Label encoding refers to transforming word labels into a numerical form so that
    algorithms can understand how to operate on them. Let's take a look at how to
    do this.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码是指将单词标签转换为数值形式，以便算法可以理解如何操作它们。让我们看看如何做到这一点。
- en: How to do it…
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s see how to carry out label encoding in Python:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Python中执行标签编码：
- en: 'Create a new Python file and import the `preprocessing()` package:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Python文件并导入 `preprocessing()` 包：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This package contains various functions that are needed for data preprocessing.
    To encode labels with a value between 0 and `n_classes`-1, the `preprocessing.LabelEncoder()`
    function can be used. Let''s define the label encoder, as follows:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此软件包包含用于数据预处理的所需的各种函数。要使用介于0和 `n_classes`-1之间的值对标签进行编码，可以使用 `preprocessing.LabelEncoder()`
    函数。让我们定义标签编码器，如下所示：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `label_encoder` object knows how to understand word labels. Let''s create
    some labels:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`label_encoder` 对象知道如何理解单词标签。让我们创建一些标签：'
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We are now ready to encode these labels—first, the `fit()` function is used
    to fit the label encoder, and then the class mapping encoders are printed:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好对这些标签进行编码——首先，使用 `fit()` 函数来拟合标签编码器，然后打印类映射编码器：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Run the code, and you will see the following output on your Terminal:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码，你将在终端上看到以下输出：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As shown in the preceding output, the words have been transformed into zero-indexed
    numbers. Now, when you encounter a set of labels, you can simply transform them,
    as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前一个输出所示，单词已经被转换为零索引的数字。现在，当你遇到一组标签时，你可以简单地转换它们，如下所示：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here is the output that you''ll see on your Terminal:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你在终端上看到的输出：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This is way easier than manually maintaining mapping between words and numbers.
    You can check the correctness by transforming numbers back into word labels:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To transform labels back to their original encoding, the `inverse_transform()`
    function has been applied.  Here is the output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: As you can see, the mapping is preserved perfectly.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the `preprocessing.LabelEncoder()` function to transform
    word labels into numerical form. To do this, we first set up a series of labels
    to as many car brands. We then turned these labels into numerical values. Finally,
    to verify the operation of the procedure, we printed the values corresponding
    to each class labeled.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last two recipes, *Label encoding* and *One-hot encoding*, we have seen
    how to transform data. Both methods are suitable for dealing with categorical
    data. But what are the pros and cons of the two methodologies? Let''s take a look:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Label encoding can transform categorical data into numeric data, but the imposed
    ordinality creates problems if the obtained values are submitted to mathematical
    operations.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding has the advantage that the result is binary rather than ordinal,
    and that everything is in an orthogonal vector space. The disadvantage is that
    for high cardinality, the feature space can explode.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-learn's official documentation on the `sklearn.preprocessing.LabelEncoder()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a linear regressor
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression refers to finding the underlying function with the help of
    linear combination of input variables. The previous example had an input variable
    and an output variable. A simple linear regression is easy to understand, but
    represents the basis of regression techniques. Once these concepts are understood,
    it will be easier for us to address the other types of regression.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following diagram:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4706fcd1-824e-488d-9851-6b16480b4150.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: The linear regression method consists of precisely identifying a line that is
    capable of representing point distribution in a two-dimensional plane, that is,
    if the points corresponding to the observations are near the line, then the chosen
    model will be able to describe the link between the variables effectively.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, there are an infinite number of lines that may approximate the observations,
    while in practice, there is only one mathematical model that optimizes the representation
    of the data. In the case of a linear mathematical relationship, the observations
    of the *y* variable can be obtained by a linear function of the observations of
    the *x* variable. For each observation, we will use the following formula:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b7ea69c-14c8-4875-bd76-5d3b11f596ff.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *x* is the explanatory variable and *y* is the response
    variable. The α and β parameters, which represent the slope of the line and the
    intercept with the y-axis respectively, must be estimated based on the observations
    collected for the two variables included in the model.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*x*是解释变量，*y*是响应变量。α和β参数，分别代表线的斜率和与y轴的截距，必须根据收集到的模型中包含的两个变量的观察值来估计。
- en: The slope, α, is of particular interest, that is, the variation of the mean
    response for every single increment of the explanatory variable. What about a
    change in this coefficient? If the slope is positive, the regression line increases
    from left to right, and if the slope is negative, the line decreases from left
    to right. When the slope is zero, the explanatory variable has no effect on the
    value of the response. But it is not just the sign of α that establishes the weight
    of the relationship between the variables. More generally, its value is also important.
    In the case of a positive slope, the mean response is higher when the explanatory
    variable is higher, while in the case of a negative slope, the mean response is
    lower when the explanatory variable is higher.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率，α，特别有趣，即解释变量每增加一个增量时平均响应的变化。这个系数的变化又如何呢？如果斜率是正的，回归线从左到右上升，如果斜率是负的，线从左到右下降。当斜率为零时，解释变量对响应值没有影响。但不仅仅是α的符号建立了变量之间关系的权重。更普遍地说，它的值也很重要。在正斜率的情况下，当解释变量较高时，平均响应也较高，而在负斜率的情况下，当解释变量较高时，平均响应较低。
- en: 'The main aim of linear regression is to get the underlying linear model that
    connects the input variable to the output variable. This in turn reduces the sum
    of squares of differences between the actual output and the predicted output using
    a linear function. This method is called **ordinary least squares**. In this method,
    the coefficients are estimated by determining numerical values that minimize the
    sum of the squared deviations between the observed responses and the fitted responses,
    according to the following equation:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的主要目的是获取连接输入变量和输出变量的潜在线性模型。这反过来又通过线性函数减少了实际输出和预测输出之间的差异的平方和。这种方法被称为**普通最小二乘法**。在这个方法中，系数是通过确定数值来估计的，这些数值最小化了观察到的响应和拟合响应之间的平方偏差之和，根据以下方程：
- en: '![](img/777ee9f3-007c-4b3d-b8f0-676da7e107ae.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/777ee9f3-007c-4b3d-b8f0-676da7e107ae.png)'
- en: This quantity represents the sum of the squares of the distances to each experimental
    datum (x[i], y[i]) from the corresponding point on the straight line.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量表示每个实验数据点（x[i]，y[i]）到对应直线上的点的距离的平方和。
- en: You might say that there might be a curvy line out there that fits these points
    better, but linear regression doesn't allow this. The main advantage of linear
    regression is that it's not complex. If you go into non-linear regression, you
    may get more accurate models, but they will be slower. As shown in the preceding
    diagram, the model tries to approximate the input data points using a straight
    line. Let's see how to build a linear regression model in Python.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会说，可能存在一条曲线更适合这些点，但线性回归不允许这样做。线性回归的主要优势是它不复杂。如果你进行非线性回归，你可能会得到更精确的模型，但它们会慢一些。如图所示，模型试图使用直线来近似输入数据点。让我们看看如何在Python中构建线性回归模型。
- en: Getting ready
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Regression is used to find out the relationship between input data and the
    continuously-valued output data. This is generally represented as real numbers,
    and our aim is to estimate the core function that calculates the mapping from
    the input to the output. Let''s start with a very simple example. Consider the
    following mapping between input and output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 回归用于找出输入数据与连续值输出数据之间的关系。这通常表示为实数，我们的目标是估计计算输入到输出映射的核心函数。让我们从一个非常简单的例子开始。考虑以下输入和输出之间的映射：
- en: '[PRE46]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If I ask you to estimate the relationship between the inputs and the outputs,
    you can easily do this by analyzing the pattern. We can see that the output is
    twice the input value in each case, so the transformation would be as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我让你估计输入和输出之间的关系，你可以通过分析模式轻松做到这一点。我们可以看到，在每种情况下，输出都是输入值的两倍，因此转换如下：
- en: '![](img/2c8db7e1-fd67-41f7-85cc-68043b1121bf.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2c8db7e1-fd67-41f7-85cc-68043b1121bf.png)'
- en: This is a simple function, relating the input values with the output values.
    However, in the real world, this is usually not the case. Functions in the real
    world are not so straightforward!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: You have been provided with a data file called `VehiclesItaly.txt`. This contains
    comma-separated lines, where the first element is the input value and the second
    element is the output value that corresponds to this input value. Our goal is
    to find the linear regression relation between the vehicle registrations in a
    state and the population of a state. You should use this as the input argument.
    As anticipated, the `Registrations` variable contains the number of vehicles registered
    in Italy and the `Population` variable contains the population of the different
    regions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a linear regressor in Python:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file called `regressor.py` and add the following lines:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We just loaded the input data into `X` and `y`, where `X` refers to the independent
    variable (explanatory variables) and `y` refers to the dependent variable (response
    variable). Inside the loop in the preceding code, we parse each line and split
    it based on the comma operator. We then convert them into floating point values
    and save them in `X` and `y`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'When we build a machine learning model, we need a way to validate our model
    and check whether it is performing at a satisfactory level. To do this, we need
    to separate our data into two groups—a training dataset and a testing dataset.
    The training dataset will be used to build the model, and the testing dataset
    will be used to see how this trained model performs on unknown data. So, let''s
    go ahead and split this data into training and testing datasets:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: First, we have put aside 80% of the data for the training dataset and the remaining
    20% is for the testing dataset. Then, we have built four arrays: `X_train`, `X_test`,`y_train`,
    and `y_test`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to train the model. Let''s create a `regressor` object, as
    follows:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: First, we have imported `linear_model` methods from the `sklearn` library, which
    are methods used for regression, wherein the target value is expected to be a
    linear combination of the input variables. Then, we have used the `LinearRegression()`
    function, which performs ordinary least squares linear regression. Finally, the `fit()`
    function is used to fit the linear model. Two parameters are passed—training data
    (`X_train`), and target values (`y_train`).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'We just trained the linear regressor, based on our training data. The `fit()`
    method takes the input data and trains the model. To see how it all fits, we have
    to predict the training data with the model fitted:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To plot the outputs, we will use the `matplotlib` library as follows:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'When you run this in the Terminal, the following diagram is shown:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae90be23-b653-4c31-b7cb-25464dd69853.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, we used the trained model to predict the output for our
    training data. This wouldn't tell us how the model performs on unknown data, because
    we are running it on the training data. This just gives us an idea of how the
    model fits on training data. Looks like it's doing okay, as you can see in the
    preceding diagram!
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s predict the test dataset output based on this model and plot it, as
    follows:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'When you run this in the Terminal, the following output is returned:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50b4e413-27d5-47e8-a76b-7a8ccdf158d5.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: As you might expect, there's a positive association between a state's population
    and the number of vehicle registrations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we looked for the linear regression relation between the vehicle
    registrations in a state and the population of a state. To do this we used the `LinearRegression()`
    function of the `linear_model` method of the `sklearn` library. After constructing
    the model, we first used the data involved in training the model to visually verify
    how well the model fits the data. Then, we used the test data to verify the results.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to appreciate the results of a simulation is to display those using special
    charts. In fact, we have already used this technique in this section. I am referring
    to the chart in which we drew the scatter plot of the distribution with the regression
    line. In Chapter 5, *Visualizing Data*, we will see other plots that will allow
    us to check the model's hypotheses.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn''s official documentation of the `sklearn.linear_model.LinearRegression()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression Analysis with R*, Giuseppe Ciaburro, Packt Publishing.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linear regression*: [https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introduction to Linear Regression*: [http://onlinestatbook.com/2/regression/intro.html](http://onlinestatbook.com/2/regression/intro.html).'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing regression accuracy
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to build a regressor, it's important to understand how
    to evaluate the quality of a regressor as well. In this context, an error is defined
    as the difference between the actual value and the value that is predicted by
    the regressor.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's quickly take a look at the metrics that can be used to measure the quality
    of a regressor. A regressor can be evaluated using many different metrics. There
    is a module in the `scikit-learn` library that provides functionalities to compute
    all the following metrics. This is the `sklearn.metrics` module, which includes
    score functions, performance metrics, pairwise metrics, and distance computations.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to compute regression accuracy in Python:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will use the functions available to evaluate the performance of the
    linear regression model we developed in the previous recipe:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following results are returned:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: An R2 score near 1 means that the model is able to predict the data very well. Keeping
    track of every single metric can get tedious, so we pick one or two metrics to
    evaluate our model. A good practice is to make sure that the mean squared error
    is low and the explained variance score is high.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A regressor can be evaluated using many different metrics, such as the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean absolute error**: This is the average of absolute errors of all the
    data points in the given dataset.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean squared error**: This is the average of the squares of the errors of
    all the data points in the given dataset. It is one of the most popular metrics
    out there!'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Median absolute error**: This is the median of all the errors in the given
    dataset. The main advantage of this metric is that it''s robust to outliers. A
    single bad point in the test dataset wouldn''t skew the entire error metric, as
    opposed to a mean error metric.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explained variance score**: This score measures how well our model can account
    for the variation in our dataset. A score of 1.0 indicates that our model is perfect.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R2 score**: This is pronounced as R-squared, and this score refers to the
    coefficient of determination. This tells us how well the unknown samples will
    be predicted by our model. The best possible score is 1.0, but the score can be
    negative as well.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sklearn.metrics` module contains a series of simple functions that measure
    prediction error:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Functions ending with  `_score` return a value to maximize; the higher the better
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions ending with `_error` or `_loss` return a value to minimize; the lower
    the better
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn''s official documentation of the `sklearn.metrics` module: [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression Analysis with R*, Giuseppe Ciaburro, Packt Publishing.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving model persistence
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we train a model, it would be nice if we could save it as a file so that
    it can be used later by simply loading it again.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how to achieve model persistence programmatically. To do this, the
    `pickle` module can be used. The `pickle` module is used to store Python objects.
    This module is a part of the standard library with your installation of Python.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to achieve model persistence in Python:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following lines to the `regressor.py` file:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The regressor object will be saved in the `saved_model.pkl` file. Let''s look
    at how to load it and use it, as follows:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following result is returned:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here, we just loaded the regressor from the file into the `model_linregr` variable.
    You can compare the preceding result with the earlier result to confirm that it's
    the same.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `pickle` module transforms an arbitrary Python object into a series of bytes.
    This process is also called the serialization of the object. The byte stream representing
    the object can be transmitted or stored, and subsequently rebuilt to create a
    new object with the same characteristics. The inverse operation is called **unpickling**.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python, there is also another way to perform serialization, by using the
    `marshal` module. In general, the `pickle` module is recommended for serializing
    Python objects. The `marshal` module can be used to support Python `.pyc` files.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python''s official documentation of the `pickle` module: [https://docs.python.org/3/library/pickle.html](https://docs.python.org/3/library/pickle.html)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a ridge regressor
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main problems of linear regression is that it's sensitive to outliers.
    During data collection in the real world, it's quite common to wrongly measure
    output. Linear regression uses ordinary least squares, which tries to minimize
    the squares of errors. The outliers tend to cause problems because they contribute
    a lot to the overall error. This tends to disrupt the entire model.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to deepen our understanding of the concept of outliers: outliers
    are values that, compared to others, are particularly extreme (values that are
    clearly distant from the other observations). Outliers are an issue because they
    might distort data analysis results; more specifically, descriptive statistics
    and correlations. We need to find these in the data cleaning phase, however, we
    can also get started on them in the next stage of data analysis. Outliers can
    be univariate when they have an extreme value for a single variable, or multivariate
    when they have a unique combination of values for a number of variables. Let''s
    consider the following diagram:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8aca32b8-8e7e-4825-a2ff-6de47b89e32c.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
- en: 'The two points on the bottom right are clearly outliers, but this model is
    trying to fit all the points. Hence, the overall model tends to be inaccurate. Outliers
    are the extreme values of a distribution that are characterized by being extremely
    high or extremely low compared to the rest of the distribution, and thus representing
    isolated cases with respect to the rest of the distribution. By visual inspection,
    we can see that the following output is a better model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2f0a320-f45e-407c-9d06-249bb6f1cb72.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
- en: Ordinary least squares considers every single data point when it's building
    the model. Hence, the actual model ends up looking like the dotted line shown
    in the preceding graph. We can clearly see that this model is suboptimal.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: The `regularization` method involves modifying the performance function, normally
    selected as the sum of the squares of regression errors on the training set. When
    a large number of variables are available, the least square estimates of a linear
    model often have a low bias but a high variance with respect to models with fewer
    variables. Under these conditions, there is an overfitting problem. To improve
    precision prediction by allowing greater bias but a small variance, we can use
    variable selection methods and dimensionality reduction, but these methods may
    be unattractive for computational burdens in the first case or provide a difficult
    interpretation in the other case.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to address the problem of overfitting is to modify the estimation
    method by neglecting the requirement of an unbiased parameter estimator and instead
    considering the possibility of using a biased estimator, which may have smaller
    variance. There are several biased estimators, most of which are based on regularization:
    `Ridge`, `Lasso`, and `ElasticNet` are the most popular methods.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ridge regression** is a regularization method where a penalty is imposed
    on the size of the coefficients. As we said in the *Building a linear regressor*
    section, in the ordinary least squares method, the coefficients are estimated
    by determining numerical values that minimize the sum of the squared deviations
    between the observed responses and the fitted responses, according to the following
    equation:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a08263f-c6fb-480e-8c55-43a5438c9acb.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: 'Ridge regression, in order to estimate the β coefficients, starts from the
    basic formula of the **residual sum of squares** (**RSS**) and adds the penalty
    term. λ (≥ 0) is defined as the tuning parameter, which is multiplied by the sum
    of the β coefficients squared (excluding the intercept) to define the penalty
    period, as shown in the following equation:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/111a48a2-b8e3-4a99-abd0-64e7c095d4f7.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: It is evident that having λ = 0 means not having a penalty in the model, that
    is, we would produce the same estimates as the least squares. On the other hand,
    having a λ tending toward infinity means having a high penalty effect, which will
    bring many coefficients close to zero, but will not imply their exclusion from
    the model. Let's see how to build a ridge regressor in Python.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a ridge regressor in Python:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: You can use the data already used in the previous example: *Building a linear
    regressor* (`VehiclesItaly.txt`). This file contains two values in each line.
    The first value is the explanatory variable, and the second is the response variable.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following lines to `regressor.py`. Let''s initialize a ridge regressor
    with some parameters:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `alpha` parameter controls the complexity. As `alpha` gets closer to 0,
    the ridge regressor tends to become more like a linear regressor with ordinary
    least squares. So, if you want to make it robust against outliers, you need to
    assign a higher value to `alpha`. We considered a value of `0.01`, which is moderate.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s train this regressor, as follows:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Run this code to view the error metrics. You can build a linear regressor to
    compare and contrast the results on the same data to see the effect of introducing
    regularization into the model.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ridge regression** is a regularization method where a penalty is imposed
    on the size of the coefficients. Ridge regression is identical to least squares,
    barring the fact that ridge coefficients are computed by decreasing a quantity
    that is somewhat different. In ridge regression, a scale transformation has a
    substantial effect. Therefore, to avoid obtaining different results depending
    on the predicted scale of measurement, it is advisable to standardize all predictors
    before estimating the model. To standardize the variables, we must subtract their
    means and divide by their standard deviations.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn''s official documentation of the `linear_model.Ridge` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ridge Regression*, Columbia University: [https://www.mailman.columbia.edu/research/population-health-methods/ridge-regression](https://www.mailman.columbia.edu/research/population-health-methods/ridge-regression)'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multicollinearity and Other Regression Pitfalls*, The Pennsylvania State University:
    [https://newonlinecourses.science.psu.edu/stat501/node/343/](https://newonlinecourses.science.psu.edu/stat501/node/343/)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a polynomial regressor
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main constraints of a linear regression model is the fact that it
    tries to fit a linear function to the input data. The polynomial regression model
    overcomes this issue by allowing the function to be a polynomial, thereby increasing
    the accuracy of the model.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Polynomial models should be applied where the relationship between response
    and explanatory variables is curvilinear. Sometimes, polynomial models can also
    be used to model a non-linear relationship in a small range of explanatory variable.
    A polynomial quadratic (squared) or cubic (cubed) term converts a linear regression
    model into a polynomial curve. However, since it is the explanatory variable that
    is squared or cubed and not the beta coefficient, it is still considered as a
    linear model. This makes it a simple and easy way to model curves, without needing
    to create big non-linear models. Let''s consider the following diagram:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fedb0c26-3b38-490a-a9cb-7122ebb501e8.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: 'We can see that there is a natural curve to the pattern of data points. This
    linear model is unable to capture this. Let''s see what a polynomial model would
    look like:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cb0ae6a-1e8a-486f-ab37-3ec49ae2560e.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
- en: 'The dotted line represents the linear regression model, and the solid line
    represents the polynomial regression model. The curviness of this model is controlled
    by the degree of the polynomial. As the curviness of the model increases, it gets
    more accurate. However, curviness adds complexity to the model as well, making
    it slower. This is a trade-off: you have to decide how accurate you want your
    model to be given the computational constraints.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a polynomial regressor in Python:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will only deal with second-degree parabolic regression. Now,
    we''ll show how to model data with a polynomial. We measured the temperature for
    a few hours of the day. We want to know the temperature trend even at times of
    the day when we did not measure it. Those times are, however, between the initial
    time and the final time at which our measurements took place:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, we will show the temperature at a few points during the day:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The following graph is produced:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25d15503-2c7c-465a-8f43-6993180067b9.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: 'If we analyze the graph, it is possible to note a curvilinear pattern of the
    data that can be modeled through a second-degree polynomial such as the following
    equation:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0086aae-d6df-4f4d-8bd0-7e4818aab3dd.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
- en: The unknown coefficients, β[0], β[1], and β[2], are estimated by decreasing
    the value of the sum of the squares. This is obtained by minimizing the deviations
    of the data from the model to its lowest value (least squares fit).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the polynomial coefficients:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The `numpy.polyfit()` function returns the coefficients for a polynomial of
    degree *n* (given by us) that is the best fit for the data. The coefficients returned
    by the function are in descending powers (highest power first), and their length
    is *n+1* if *n* is the degree of the polynomial.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the model, let''s verify that it actually fits our data. To
    do this, use the model to evaluate the polynomial at uniformly spaced times. To
    evaluate the model at the specified points, we can use the `poly1d()` function.
    This function returns the value of a polynomial of degree *n* evaluated at the
    points provided by us. The input argument is a vector of length *n+1* whose elements
    are the coefficients in descending powers of the polynomial to be evaluated:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As you can see in the upcoming graph, this is close to the output value. If
    we want it to get closer, we need to increase the degree of the polynomial.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can plot the original data and the model on the same plot:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The following graph is printed:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b9f0864-acfc-4e76-a48d-d213ea82599e.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: If we analyze the graph, we can see that the curve fits our data sufficiently.
    This model fits the data to a greater extent than a simple linear regression model. In
    regression analysis, it's important to keep the order of the model as low as possible.
    In the first analysis, we keep the model as a first order polynomial. If this
    is not satisfactory, then a second-order polynomial is tried. The use of higher-order polynomials
    can lead to incorrect evaluations.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Polynomial regression should be used when linear regression is not good enough. With
    polynomial regression, we approached a model in which some predictors appear in
    degrees equal to or greater than two to fit the data with a curved line. Polynomial
    regression is usually used when the relationship between variables looks curved.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At what degree of the polynomial must we stop? It depends on the degree of precision
    we are looking for. The higher the degree of the polynomial, the greater the precision
    of the model, but the more difficult it is to calculate. In addition, it is necessary
    to verify the significance of the coefficients that are found, but let's get to
    it right away.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-410
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python's official documentation of the `numpy.polyfit()` function ([https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html))
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python's official documentation of the `numpy.poly1d()` function ([https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.poly1d.html](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.poly1d.html))
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating housing prices
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to apply our knowledge to a real-world problem. Let's apply all these
    principles to estimate house prices. This is one of the most popular examples
    that is used to understand regression, and it serves as a good entry point. This
    is intuitive and relatable, hence making it easier to understand the concepts
    before we perform more complex things in machine learning. We will use a decision
    tree regressor with AdaBoost to solve this problem.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a tree where each node makes a simple decision that contributes
    to the final output. The leaf nodes represent the output values, and the branches
    represent the intermediate decisions that were made, based on input features.
    **AdaBoost** stands for **adaptive boosting**, and this is a technique that is
    used to boost the accuracy of the results from another system. This combines the
    outputs from different versions of the algorithms, called weak learners, using
    a weighted summation to get the final output. The information that's collected
    at each stage of the AdaBoost algorithm is fed back into the system so that the
    learners at the latter stages focus on training samples that are difficult to
    classify. In this way, it increases the accuracy of the system.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Using AdaBoost, we fit a regressor on the dataset. We compute the error and
    then fit the regressor on the same dataset again, based on this error estimate.
    We can think of this as fine-tuning of the regressor until the desired accuracy
    is achieved. You are given a dataset that contains various parameters that affect
    the price of a house. Our goal is to estimate the relationship between these parameters
    and the house price so that we can use this to estimate the price given unknown
    input parameters.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to estimate housing prices in Python:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called `housing.py` and add the following lines:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: There is a standard housing dataset that people tend to use to get started with
    machine learning. You can download it at [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).
    We will be using a slightly modified version of the dataset, which has been provided
    along with the code files.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The good thing is that `scikit-learn` provides a function to directly load
    this dataset:'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Each data point has 12 input parameters that affect the price of a house. You
    can access the input data using `housing_data.data` and the corresponding price
    using `housing_data.target`. The following attributes are available:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '`crim`: Per capita crime rate by town'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zn`: Proportion of residential land zoned for lots that are over 25,000 square
    feet'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indus`: Proportion of non-retail business acres per town'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nox`: Nitric oxides concentration (parts per ten million)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rm`: Average number of rooms per dwelling'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: Proportion of owner-occupied units built prior to 1940'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dis`: Weighted distances to the five Boston employment centers'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rad`: Index of accessibility to radial highways'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tax`: Full-value property-tax rate per $10,000'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ptratio`: Pupil-teacher ratio by town'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstat`: Percent of the lower status of the population'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target`: Median value of owner-occupied homes in $1000'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these, `target` is the response variable, while the other 12 variables are
    possible predictors. The goal of this analysis is to fit a regression model that
    best explains the variation in `target`.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s separate this into input and output. To make this independent of the
    ordering of the data, let''s shuffle it as well:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `sklearn.utils.shuffle()` function shuffles arrays or sparse matrices in
    a consistent way to do random permutations of collections. Shuffling data reduces
    variance and makes sure that the patterns remain general and less overfitted. The `random_state` parameter
    controls how we shuffle data so that we can have reproducible results.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s divide the data into training and testing. We''ll allocate 80% for training
    and 20% for testing:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Remember, machine learning algorithms, train models by using a finite set of
    training data. In the training phase, the model is evaluated based on its predictions
    of the training set. But the goal of the algorithm is to produce a model that
    predicts previously unseen observations, in other words, one that is able to generalize
    the problem by starting from known data and unknown data. For this reason, the
    data is divided into two datasets: training and test. The training set is used
    to train the model, while the test set is used to verify the ability of the system
    to generalize.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to fit a decision tree regression model.  Let''s pick a tree
    with a maximum depth of 4, which means that we are not letting the tree become
    arbitrarily deep:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The `DecisionTreeRegressor` function has been used to build a decision tree
    regressor.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also fit the decision tree regression model with AdaBoost:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The `AdaBoostRegressor` function has been used to compare the results and see
    how AdaBoost really boosts the performance of a decision tree regressor.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s evaluate the performance of the decision tree regressor:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: First, we used the `predict()` function to predict the response variable based
    on the test data. Next, we calculated mean squared error and explained variance.
    `Mean squared error` is the average of the squared difference between actual and
    predicted values across all data points in the input. The `explained variance`
    is an indicator that, in the form of proportion, indicates how much variability
    of our data is explained by the model in question.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s evaluate the performance of AdaBoost:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Here is the output on the Terminal:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The error is lower and the variance score is closer to 1 when we use AdaBoost,
    as shown in the preceding output.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DecisionTreeRegressor` builds a decision tree regressor.  Decision trees are
    used to predict a response or class *y*, from several input variables; *x1*, *x2*,…,*xn*.
    If y is a continuous response, it''s called a regression tree, if *y* is categorical,
    it''s called a classification tree. The algorithm is based on the following procedure:
    We see the value of the input *x[i]* at each node of the tree, and based on the
    answer, we continue to the left or to the right branch. When we reach a leaf,
    we will find the prediction. In regression trees, we try to divide the data space
    into tiny parts, where we can equip a simple different model on each of them.
    The non-leaf part of the tree is just the way to find out which model we will
    use for predicting it.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: A regression tree is formed by a series of nodes that split the root branch
    into two child branches. Such subdivision continues to cascade. Each new branch,
    then, can go in another node, or remain a leaf with the predicted value.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An AdaBoost regressor is a meta-estimator that starts by equipping a regressor
    on the actual dataset and adding additional copies of the regressor on the same
    dataset, but where the weights of instances are adjusted according to the error
    of the current prediction. As such, consecutive regressors look at difficult cases. This
    will help us compare the results and see how AdaBoost really boosts the performance
    of a decision tree regressor.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-465
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-learn's official documentation of the `DecisionTreeRegressor` function ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html))
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn's official documentation of the `AdaBoostRegressor` function ([https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html))
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the relative importance of features
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are all features equally important? In this case, we used 13 input features,
    and they all contributed to the model. However, an important question here is,
    *How do we know which features are more important?* Obviously, not all features
    contribute equally to the output. In case we want to discard some of them later,
    we need to know which features are less important. We have this functionality
    available in scikit-learn.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's calculate the relative importance of the features. Feature importance
    provides a measure that indicates the value of each feature in the construction
    of the model. The more an attribute is used to build the model, the greater its
    relative importance. This importance is explicitly calculated for each attribute
    in the dataset, allowing you to classify and compare attributes to each other.
     Feature importance is an attribute contained in the model (`feature_importances_`).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to compute the relative importance of features:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to extract this. Add the following lines to `housing.py`:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The regressor object has a callable `feature_importances_` method that gives
    us the relative importance of each feature. To compare the results, the importance
    values have been normalized. Then, we ordered the index values and turned them
    upside down so that they are arranged in descending order of importance. Finally,
    for display purposes, the location of the labels on the *x*-axis has been centered.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the results, we will plot the bar graph:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We just take the values from the `feature_importances_` method and scale them
    so that they range between 0 and 100\. Let''s see what we will get for a decision
    tree-based regressor in the following output:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c0bbaef3-09d7-4963-9707-0836cc795d1e.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
- en: So, the decision tree regressor says that the most important feature is RM.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we carry out a similar procedure for the AdaBoost model:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'To visualize the results, we will plot the bar graph:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s take a look at what AdaBoost has to say in the following output:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e104325-5dd2-46a6-a499-aedd8dd7b96e.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
- en: According to AdaBoost, the most important feature is LSTAT. In reality, if you
    build various regressors on this data, you will see that the most important feature
    is in fact LSTAT. This shows the advantage of using AdaBoost with a decision tree-based
    regressor.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-489
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature importance provides a measure that indicates the value of each feature
    in the construction of a model. The more an attribute is used to build a model,
    the greater its relative importance. In this recipe, the `feature_importances_` attribute
    was used to extract the relative importance of the features from the model.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Relative importance returns the utility of each characteristic in the construction
    of decision trees. The more an attribute is used to make predictions with decision
    trees, the greater its relative importance. This importance is explicitly calculated
    for each attribute in the dataset, allowing you to classify and compare attributes
    to each other.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn''s official documentation of the `DecisionTreeRegressor` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating bicycle demand distribution
  id: totrans-495
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's use a different regression method to solve the bicycle demand distribution
    problem. We will use the random forest regressor to estimate the output values.
    A random forest is a collection of decision trees. This basically uses a set of
    decision trees that are built using various subsets of the dataset, and then it
    uses averaging to improve the overall performance.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the `bike_day.csv` file that is provided to you. This is also available
    at [https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).
    There are 16 columns in this dataset. The first two columns correspond to the
    serial number and the actual date, so we won't use them for our analysis. The
    last three columns correspond to different types of outputs. The last column is
    just the sum of the values in the fourteenth and fifteenth columns, so we can
    leave those two out when we build our model. Let's go ahead and see how to do
    this in Python. We will analyze the code line by line to understand each step.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-499
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to estimate bicycle demand distribution:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to import a couple of new packages, as follows:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-502
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We are processing a CSV file, so the CSV package for useful in handling these
    files. Let''s import the data into the Python environment:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'This piece of code just read all the data from the CSV file. The `csv.reader()`
    function returns a `reader` object, which will iterate over lines in the given CSV
    file. Each row read from the CSV file is returned as a list of strings. So, two
    lists are returned: `X` and `y`. We have separated the data from the output values
    and returned them. Now we will extract feature names:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The feature names are useful when we display them on a graph. So, we have to
    remove the first row from X and y because they are feature names:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-508
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: We have also converted the two lists into two arrays.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s shuffle these two arrays to make them independent of the order in which
    the data is arranged in the file:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'As we did earlier, we need to separate the data into training and testing data.
    This time, let''s use 90% of the data for training and the remaining 10% for testing:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Let''s go ahead and train the regressor:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The `RandomForestRegressor()` function builds a random forest regressor. Here,
    `n_estimators` refers to the number of estimators, which is the number of decision
    trees that we want to use in our random forest. The `max_depth` parameter refers
    to the maximum depth of each tree, and the `min_samples_split` parameter refers
    to the number of data samples that are needed to split a node in the tree.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s evaluate the performance of the random forest regressor:'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The following results are returned:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Let''s extract the relative importance of the features:'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'To visualize the results, we will plot a bar graph:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The following output is plotted:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39f8941f-eaaa-4222-bbbf-ec944eb99161.png)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
- en: Looks like the temperature is the most important factor controlling bicycle
    rentals.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-528
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A random forest is a special regressor formed of a set of simple regressors
    (decision trees), represented as independent and identically distributed random
    vectors, where each one chooses the mean prediction of the individual trees. This
    type of structure has made significant improvements in regression accuracy and
    falls within the sphere of ensemble learning. Each tree within a random forest
    is constructed and trained from a random subset of the data in the training set.
    The trees therefore do not use the complete set, and the best attribute is no
    longer selected for each node, but the best attribute is selected from a set of
    randomly selected attributes.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: Randomness is a factor that then becomes part of the construction of regressors
    and aims to increase their diversity and thus reduce correlation. The final result
    returned by the random forest is nothing but the average of the numerical result
    returned by the different trees in the case of a regression, or the class returned
    by the largest number of trees if the random forest algorithm was used to perform
    classification.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  id: totrans-531
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see what happens when you include the fourteenth and fifteenth columns
    in the dataset. In the feature importance graph, every feature other than these
    two has to go to zero. The reason is that the output can be obtained by simply
    summing up the fourteenth and fifteenth columns, so the algorithm doesn''t need
    any other features to compute the output. Make the following change inside the
    `for` loop (the rest of the code remains unchanged):'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'If you plot the feature importance graph now, you will see the following:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51725b53-5281-49a8-af7f-9f9621ba88d9.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
- en: 'As expected, it says that only these two features are important. This makes
    sense intuitively because the final output is a simple summation of these two
    features. So, there is a direct relationship between these two variables and the
    output value. Hence, the regressor says that it doesn''t need any other variable
    to predict the output. This is an extremely useful tool to eliminate redundant
    variables in your dataset. But this is not the only difference from the previous
    model. If we analyze the model''s performance, we can see a substantial improvement:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'We therefore have 99% of the variance explained: a very good result.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another file, called `bike_hour.csv`, that contains data about how
    the bicycles are shared hourly. We need to consider columns 3 to 14, so let''s
    make this change in the code (the rest of the code remains unchanged):'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'If you run the new code, you will see the performance of the regressor displayed,
    as follows:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The feature importance graph will look like the following:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe9a90c7-f085-47e7-9706-9b0e92b13a00.png)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
- en: This shows that the hour of the day is the most important feature, which makes
    sense intuitively if you think about it! The next important feature is temperature,
    which is consistent with our earlier analysis.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-546
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn''s official documentation of the `RandomForestRegressor` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
