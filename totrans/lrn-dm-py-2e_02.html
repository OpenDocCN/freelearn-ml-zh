<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Classifying with scikit-learn Estimators</h1>
            </header>

            <article>
                
<p>The scikit-learn library is a collection of data mining algorithms, written in Python and using a. This library allows users to easily try different algorithms as well as utilize standard tools for doing effective testing and parameter searching. There are many algorithms and utilities in scikit-learn, including many of the commonly used algorithms in modern machine learning.</p>
<p>In this chapter, we focus on setting up a good framework for running data mining procedures. We will use this framework in later chapters, which focus on applications and techniques to use in those situations.</p>
<p>The key concepts introduced in this chapter are as follows:</p>
<ul>
<li><strong>Estimators:</strong> This is to perform classification, clustering, and regression</li>
<li><strong>Transformers</strong>: This is to perform pre-processing and data alterations</li>
<li><strong>Pipelines</strong>: This is to put together your workflow into a replicable format</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">scikit-learn estimators</h1>
            </header>

            <article>
                
<p><strong>Estimators</strong> that allows for the standardized implementation and testing of algorithms a common, lightweight interface for classifiers to follow. By using this interface, we can apply these tools to arbitrary classifiers, without needing to worry about how the algorithms work.</p>
<p>Estimators must have the following two important functions:</p>
<ul>
<li><kbd>fit()</kbd>: This function performs the training of the algorithm - setting the values of internal parameters. The <kbd>fit()</kbd> takes two inputs, the training sample dataset and the corresponding classes for those samples.</li>
<li><kbd>predict()</kbd>: This the class of the testing samples that we provide as the only input. This function returns a <kbd>NumPy</kbd> array with the predictions of each input testing sample.</li>
</ul>
<p>Most scikit-learn estimators use <kbd>NumPy</kbd> arrays or a related format for input and output. However this is by convention and not required to use the interface.</p>
<p>There are many estimators implemented in scikit-learn and more in other open source projects that use the same interface. These (SVM), random forests. We will use many<br/>
of these algorithms in later chapters. In this chapter, we will use the nearest neighbor<br/>
algorithm.</p>
<div class="packt_infobox">For this chapter, you will need to install a new library called <kbd>matplotlib</kbd>. The easiest way to install it is to use <kbd>pip3</kbd>, as you did in <a href="3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Getting Started with Data Mining</em>, to install scikit-learn:<br/>
<kbd><strong>$pip3 install matplotlib</strong></kbd><br/>
If you have <kbd>matplotlib</kbd>, seek the official installation instructions at:<br/>
<a href="http://matplotlib.org/users/installing.html%22http://matplotlib.org/users/installing.html">http://matplotlib.org/users/installing.html</a></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Nearest neighbors</h1>
            </header>

            <article>
                
<p>The <strong>Nearest neighbors</strong> algorithm is our new sample. We take the most similar samples<br/>
and predict the same class that most of these nearby samples have. This vote is often simply a simple count,although more complicated methods do exist such as weighted voting.</p>
<p>As an example in the below diagram, we wish to predict the class of the triangle, based on which class it is more like (represented here by having similar objects closer together). We seek the three nearest neighbors, which are the two diamonds and one square within the drawn circle. There are more diamonds than circles, and the predicted class for the triangle is, therefore, a diamond:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="187" src="assets/image_02_001-1.jpg" width="205"/></div>
<p>Nearest neighbors are used for nearly any dataset - however, it can be computationally expensive to compute the distance between all pairs of samples. For example, if there are ten samples in the dataset, there are 45 unique distances to compute. However, if there are 1000 samples, there are nearly 500,000! Various methods exist for improving this speed, such as the use of tree structures for distance computation. Some of these algorithms can be quite complex, but thankfully a version is implemented in scikit-learn already, enabling us to classify on larger datasets. As these tree structures are the default in scikit-learn, we do not need to configure anything to use it. </p>
<p>Nearest neighbors can do poorly in <strong>categorical-based datasets</strong>, with categorical features, and another algorithm should be used for these instead. Nearest Neighbor's issue is due to the difficulty in comparing differences in categorical values, something better left to an algorithm that gives weight to each feature's importance. Comparing categorical features can be done with some distance metrics or pre-processing steps such as one hot encoding that we use in later chapters. Choosing the correct algorithm for the task is one of the difficult issues in data mining, often it can be easiest to test a set of algorithms and see which performs best on your task.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Distance metrics</h1>
            </header>

            <article>
                
<p>A key underlying concept in data mining is that of <strong>distance</strong>. If we have two samples, we need to answer questions such as <em>are these two samples more similar than the other two? </em>Answering questions like these is important to the outcome of the data mining exercise.</p>
<p>The most common use is <strong>Euclidean</strong> distance, which is the <em>real-world</em> distance between two objects. If you were to plot the points on a graph and measure the distance with a ruler, the result would be the Euclidean distance. </p>
<div class="packt_infobox">A little more formally, the Euclidean distances between points a and b is the square root of the sum of the squared distances for each feature.</div>
<p>Euclidean distance is intuitive but provides poor accuracy if some features have larger values than a value of 0, known as a sparse matrix.</p>
<p>There are other distance metrics in use; two commonly employed ones are the Manhattan and Cosine distance.</p>
<div class="packt_infobox">The <strong>Manhattan</strong> distance is the sum of the absolute differences in each feature (with no use of square distances).</div>
<p>Intuitively we can imagine Manhattan distance of as the number of moves a Rook piece<br/>
(also called a Castle) in if it were limited to moving one square at a time. While the Manhattan distance does suffer if some features have larger values than others, the effect is not as dramatic as in the case of Euclidean points if it were limited to moving one square at a time. While the Manhattan distance does suffer if some features have larger values than others, the effect is not as dramatic as in the case of Euclidean.</p>
<div class="packt_infobox">The <strong>Cosine</strong> distance is better suited to cases where some features are larger than others and when there are lots of zeros in the dataset.</div>
<p>Intuitively, we draw a line from the origin to each of the samples and measure the angle between those lines. We can observe the differences between the algorithms in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="173" src="assets/image_02_002-1.jpg" width="454"/></div>
<p>In this example, each of the gray circles is exactly the same distance from the white circle. In (a), the distances are Euclidean, and therefore, similar distances fit around a circle. This distance can be measured using a ruler. In (b), the distances are Manhattan, also called City Block. We compute the distance by moving across rows and columns, like how a Rook (Castle) in Chess moves. Finally, in (c), we have the Cosine distance that is measured by computing the angle between the lines drawn from the sample to the vector and ignore the actual length of the line.</p>
<div class="packt_infobox">The distance metric chosen can have a large impact on the final performance.</div>
<p>For example, if you have many features, the Euclidean distance between random samples converges (due to the famous <em>curse of dimensionality</em>). Euclidean distances in high dimension have a hard time comparing samples, as the distances are always nearly the same!</p>
<p>Manhattan distance can be more stable in these circumstances, but if some features have very large values, this can <em>overrule</em> lots similarity in other features. For example, if feature A has values between 1 and 2, and another feature B has values between 1000 and 2000, in such a case feature A is unlikely to have any impact on the result. This problem can be addressed through normalization, which makes Manhattan (and Euclidean) distance more reliable with different features, which we will see later in this chapter.</p>
<p>Finally, Cosine distance is a good metric for comparing items with many features, but it discards some information about the length of the vector, which is useful in some applications. We would often use Cosine distance in text mining due to the large number of features inherent in text mining (see <span class="ChapterrefPACKT"><a href="ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml">Chapter 6</a>,</span> <em>Social Media Insight Using Naive Bayes</em>).</p>
<div class="packt_infobox">Ultimately, either a theoretical approach is needed to determine which distance method is needed, or an empirical evaluation is needed to see which performed more effectively. I prefer the empirical approach, but either approach can yield good results.</div>
<p>For this chapter, we will stay with Euclidean distance, using other metrics in later chapters. If you'd like to experiment, then try setting the metric to Manhattan and see how that affects the results.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading the dataset</h1>
            </header>

            <article>
                
<p>The dataset <kbd>Ionosphere</kbd>, which high-frequency antennas. The aim of the antennas is to determine whether there is a structure in the ionosphere and a region in the upper atmosphere. We consider readings with a structure to be good, while those that do not have structure are deemed bad. The aim of this application is to build a data mining classifier that can determine whether an image is good or bad.</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="222" src="assets/image_02_003.png" width="407"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">(Image Credit: https://www.flickr.com/photos/geckzilla/16149273389/)</div>
<p>You can download this dataset  for different data mining applications. Go to <a href="http://archive.ics.uci.edu/ml/datasets/Ionosphere">http://archive.ics.uci.edu/ml/datasets/Ionosphere</a>  and click on <span class="packt_screen">Data Folder</span>. Download the <kbd>ionosphere.data</kbd> and <kbd>ionosphere.names</kbd> files to a folder on your computer. For this example, I'll assume that you have put the dataset in a directory called <kbd>Data</kbd> in your <kbd>home</kbd> folder. You can place the data in another folder, just be sure to update your data folder (here, and in all other chapters).</p>
<div class="packt_infobox">The location of your home folder depends on your operating system. For Windows, it is usually at <kbd>C:Documents</kbd> and <kbd>Settingsusername</kbd>. For Mac or Linux machines, it is usually at <kbd>/home/username</kbd>. You can get your home folder by running this python code inside a Jupyter Notebook:</div>
<pre>
import os print(os.path.expanduser("~"))
</pre>
<p>For each row in the dataset, there are 35 values. The first 34 are measurements taken from the 17 antennas (two values for each antenna). The last is either 'g' or 'b'; that stands for good and bad, respectively.</p>
<p>Start the Jupyter Notebook server and create a new notebook called <span class="packt_screen">Ionosphere Nearest Neighbors.</span> To start with, we load up the <kbd>NumPy</kbd> and <kbd>csv</kbd> libraries that we will need for our code, and set the data's filename that we will need for our code.</p>
<pre>
import numpy as np <br/>import csv <br/>data_filename = "data/ionosphere.data"
</pre>
<p>We then create the <kbd>X</kbd> and <kbd>y</kbd> <kbd>NumPy</kbd> arrays to store the dataset in. The sizes of these arrays are known from the dataset. Don't worry if you don't know the size of future datasets - we will use other methods to load the dataset in future chapters and you won't need to know this size beforehand:</p>
<pre>
X = np.zeros((351, 34), dtype='float') <br/>y = np.zeros((351,), dtype='bool')
</pre>
<p>The dataset is in a <strong>Comma-Separated Values</strong> (<strong>CSV</strong>) format, which is a commonly used format for datasets. We are going to use the <kbd>csv</kbd> module to load this file. Import it and set up a <kbd>csv</kbd> reader object, then loop through the file, setting the appropriate row in <kbd>X</kbd> and class value in <kbd>y</kbd> for every line in our dataset:</p>
<pre>
with open(data_filename, 'r') as input_file: <br/>    reader = csv.reader(input_file) <br/>    for i, row in enumerate(reader): <br/>        # Get the data, converting each item to a float <br/>        data = [float(datum) for datum in row[:-1]] <br/>        # Set the appropriate row in our dataset <br/>        X[i] = data <br/>        # 1 if the class is 'g', 0 otherwise <br/>        y[i] = row[-1] == 'g'
</pre>
<p>We now have a dataset of samples and features in <kbd>X</kbd> as well as the corresponding classes in <kbd>y</kbd>, as we did in the classification example in <a href="3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml" target="_blank">Chapter 1</a>, Getting Started with Data Mining.</p>
<p>To begin with, try applying the OneR algorithm from <a href="3dc86298-cd8c-4d02-a373-6cd303d5c558.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Getting Started with Data Mining</em> to this dataset. It won't work very well, as the information in this dataset is spread out within the correlations of certain features. OneR is only interested in the values of a single feature and cannot pick up information in more complex datasets very well. Other algorithms, including Nearest Neighbor, merge information from multiple features, making them applicable in more scenarios. The downside is that they are often more computationally expensive to compute.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Moving towards a standard workflow</h1>
            </header>

            <article>
                
<p>Estimators scikit-learn have two and <kbd>predict()</kbd>. We train the algorithm using the<br/>
<kbd>predict()</kbd> method on our testing set. We evaluate it using the <kbd>predict()</kbd> method on our testing set.</p>
<ol>
<li>First, we need to create these training and testing sets. As before, import and run the <kbd>train_test_split</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">
from sklearn.cross_validation import train_test_split <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14)
</pre>
<ol start="2">
<li>
<p>Then, we import the <kbd>nearest neighbor</kbd> class and create an instance for it. We leave the parameters as defaults for now and will test other values later in this chapter. By default, the algorithm will choose the five nearest neighbors to predict the class of a testing sample:</p>
</li>
</ol>
<pre style="padding-left: 60px">
from sklearn.neighbors import KNeighborsClassifier estimator = KNeighborsClassifier()
</pre>
<ol start="3">
<li>After creating our <kbd>estimator</kbd>, we must then fit it on our training dataset. For the <kbd>nearest neighbor</kbd> class, this training step simply records our dataset, allowing us to find the nearest neighbor for a new data point, by comparing that point to the training dataset:</li>
</ol>
<pre style="padding-left: 60px">
estimator.fit(X_train, y_train)
</pre>
<ol start="4">
<li>We then train the algorithm with our test set and evaluate with our testing set:</li>
</ol>
<pre style="padding-left: 60px">
y_predicted = estimator.predict(X_test) <br/>accuracy = np.mean(y_test == y_predicted) * 100     <br/>print("The accuracy is {0:.1f}%".format(accuracy))
</pre>
<p>This model scores 86.4 percent accuracy, which is impressive for a default algorithm and just a few lines of code! Most scikit-learn default parameters are chosen deliberately to work well with a range of datasets. However, you should always aim to choose parameters based on knowledge of the application experiment. We will use strategies for doing this <strong>parameter search</strong> in later chapters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Running the algorithm</h1>
            </header>

            <article>
                
<p>The previous results are quite good, based on our testing set of data, based on the testing set. However, what happens if we get lucky and choose an easy testing set? Alternatively, what if it was particularly troublesome? We can discard a good model due to poor results resulting from such an <em>unlucky</em> split of our data.</p>
<p>The <strong>cross-fold validation</strong> framework is a way to address the problem of choosing a single testing set and is a standard <em>best-practice </em>methodology in data mining. The process works by doing many experiments with different training and testing splits, but using each sample in a testing set only once. The procedure is as follows:</p>
<ol>
<li>Split the entire dataset into several sections called folds.</li>
<li>For each fold in the data, execute the following steps:
<ol>
<li>Set that fold aside as the current testing set</li>
<li>Train the algorithm on the remaining folds</li>
<li>Evaluate on the current testing set</li>
</ol>
</li>
</ol>
<ol start="3">
<li>Report on all the evaluation scores, including the average score.</li>
</ol>
<div class="packt_infobox">In this process, each sample is used in the testing set only once, reducing (but not eliminating) the likelihood of choosing lucky testing sets.</div>
<div class="packt_infobox">Throughout this book, the code examples build upon each other within a chapter. Each chapter's code should be entered into the same Jupyter Notebook unless otherwise specified in-text.</div>
<p>The scikit-learn library contains a few cross-fold validation methods. A <kbd>helper</kbd> function is given that performs the preceding procedure. We can import it now in our Jupyter Notebook:</p>
<pre>
from sklearn.cross_validation import cross_val_score
</pre>
<div class="packt_infobox">By <kbd>cross_val_score</kbd> uses a specific methodology called <strong>Stratified K-Fold</strong> to create folds that have approximately the same proportion of classes in each fold, again reducing the likelihood of choosing poor folds. Stratified K-Fold is a great default -we won't mess with it right now.</div>
<p>Next, we use this new function to evaluate our model using cross-fold validation:</p>
<pre>
scores = cross_val_score(estimator, X, y, scoring='accuracy') <br/>average_accuracy = np.mean(scores) * 100 <br/>print("The average accuracy is {0:.1f}%".format(average_accuracy))
</pre>
<p>Our new code returns a slightly more modest result of 82.3 percent, but it is still quite good considering we have not yet tried setting better parameters. In the next section, we will see how we would go about changing the parameters to achieve a better outcome.</p>
<p>It is quite natural for variation in results when performing data mining, and attempting to repeat experiments. This is due to variations in how the folds are created and randomness inherent in some classification algorithms. We can deliberately choose to replicate an experiment exactly by setting the random state (which we will do in later chapters). In practice, it's a good idea to rerun experiments multiple times to get a sense of the average result and the spread of the results (the mean and standard deviation) across all experiments.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting parameters</h1>
            </header>

            <article>
                
<p>Almost all parameters that the user can set, letting algorithms focus more on the specific dataset, rather than only being applicable across a small and specific range of problems. Setting these parameters can be quite difficult, as choosing good parameter values is often highly reliant on features of the dataset.</p>
<p>The nearest neighbor algorithm has several parameters, but the most important one is that of the number of nearest neighbors to use when predicting the class of an unseen attribution. In <kbd>-learn</kbd>, this parameter is called <kbd>n_neighbors</kbd>. In the following figure, we show that when this number is too low, a randomly labeled sample can cause an error. In contrast, when it is too high, the actual nearest neighbors have a lower effect on the result:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="146" src="assets/image_02_004-1.jpg" width="332"/></div>
<p>In figure (a), on the left-hand side, we would usually expect to classify the test sample (the triangle) as a circle. However, if <kbd>n_neighbors</kbd> is 1, the single red diamond in this area (likely a noisy sample) causes the sample to be predicted as a diamond. In figure (b), on the right-hand side, we would usually expect to classify the test sample as a diamond. However, if <kbd>n_neighbors</kbd> is 7, the three nearest neighbors (which are all diamonds) are overridden by a large number of circle samples. Nearest neighbors a difficult problem to solve, as the parameter can make a huge difference. Luckily, most of the time the specific parameter value does not greatly affect the end result, and the standard values (usually 5 or 10) are often <em>near enough</em>.</p>
<p>With that in mind, we can test out a range of values, and investigate the impact that this parameter has on performance. If we want to test a number of values for the <kbd>n_neighbors</kbd> parameter, for example, each of the values from 1 to 20, we can rerun the experiment many times by setting <kbd>n_neighbors</kbd> and observing the result. The code below does this, storing the values in the <kbd>avg_scores</kbd> and <kbd>all_scores</kbd> variables.</p>
<pre>
avg_scores = [] <br/>all_scores = [] <br/>parameter_values = list(range(1, 21))  # Include 20 <br/>for n_neighbors in parameter_values: <br/>    estimator = KNeighborsClassifier(n_neighbors=n_neighbors) <br/>    scores = cross_val_score(estimator, X, y, scoring='accuracy')     avg_scores.append(np.mean(scores))     <br/>all_scores.append(scores)
</pre>
<p>We can then plot the relationship between the value of <kbd>n_neighbors</kbd> and the accuracy. First, we tell the Jupyter Notebook that we want to show plots <kbd>inline</kbd> in the notebook itself:</p>
<pre>
%matplotlib inline
</pre>
<p>We then import <kbd>pyplot</kbd> from the <kbd>matplotlib</kbd> library and plot the parameter values alongside average scores:</p>
<pre>
from matplotlib import pyplot as plt plt.plot(parameter_values,  avg_scores, '-o')
</pre>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="282" src="assets/image_02_005.png" width="450"/></div>
<p>While there is a lot of variance, the plot shows a decreasing trend as the number of neighbors increases. With regard to the variance, you can expect large amounts of variance whenever you do evaluations of this nature. To compensate, update the code to run 100 tests, per value of <kbd>n_neighbors</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Preprocessing</h1>
            </header>

            <article>
                
<p>When taking measurements of real-world objects, we can often get features in different ranges. For instance, if we measure the qualities of an animal, we might have several features, as follows:</p>
<ul>
<li><strong>Number of legs</strong>: This is between the range of 0-8 for most animals, while some have more! more! more!</li>
<li><strong>Weight</strong>: This is between the ranges of only a few micrograms, all the way to a blue whale with a weight of 190,000 kilograms!</li>
<li><strong>Number of hearts</strong>: This can be between zero to five, in the case of the earthworm.</li>
</ul>
<p>For a mathematical-based algorithm to compare each of these features, the differences in the scale, range, and units can be difficult to interpret. If we used the above features in many algorithms, the weight would probably be the most influential feature due to only the larger numbers and not anything to do with the actual effectiveness of the feature.</p>
<p>One of the possible strategies <em>normalizes</em> the features so that they all have the same range, or the values are turned into categories like <em>small</em>, <em>medium</em> and <em>large</em>. Suddenly, the large differences in the types of features have less of an impact on the algorithm and can lead to large increases in the accuracy.</p>
<p>Pre-processing can also be used to choose only the more effective features, create new features, and so on. Pre-processing in scikit-learn is done through <kbd>Transformer</kbd> objects, which take a dataset in one form and return an altered dataset after some transformation of the data. These don't have to be numerical, as Transformers are also used to extract features-however, in this section, we will stick with pre-processing.</p>
<p>We can show an example of the problem by <em>breaking</em> the <kbd>Ionosphere</kbd> dataset. While this is only an example, many real-world datasets have problems of this form.</p>
<ol>
<li>First, we create a copy of the array so that we do not alter the original dataset:</li>
</ol>
<pre style="padding-left: 60px">
X_broken = np.array(X)
</pre>
<ol start="2">
<li>Next, we <em>break</em> the dataset by dividing every second feature by <kbd>10</kbd>:</li>
</ol>
<pre style="padding-left: 60px">
X_broken[:,::2] /= 10
</pre>
<p>In theory, this should not have a great effect on the result. After all, the values of these features are still relatively the same. The major issue is that the scale has changed and the odd features are now <em>larger</em> than the even features. We can see the effect of this by computing the accuracy:</p>
<pre>
estimator = KNeighborsClassifier() <br/>original_scores = cross_val_score(estimator, X, y,scoring='accuracy') <br/>print("The original average accuracy for is {0:.1f}%".format(np.mean(original_scores) * 100)) <br/>broken_scores = cross_val_score(estimator, X_broken, y,   scoring='accuracy') <br/>print("The 'broken' average accuracy for is   {0:.1f}%".format(np.mean(broken_scores) * 100))
</pre>
<p>This testing methodology gives a score of 82.3 percent for the original dataset, which drops down to 71.5 percent on the broken dataset. We can fix this by scaling all the features to the range <kbd>0</kbd> to <kbd>1</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Standard pre-processing</h1>
            </header>

            <article>
                
<p>The pre-processing we will perform for this experiment is called feature-based normalization, which we perform using scikit-learn's <kbd>MinMaxScaler</kbd> class. Continuing with the Jupyter Notebook from the rest of this chapter, first, we import this class:</p>
<pre>
fromsklearn.preprocessing import MinMaxScaler
</pre>
<p>This class takes each feature and scales it to the range <kbd>0</kbd> to <kbd>1</kbd>. This pre-processor replaces the minimum value with <kbd>0</kbd>, the maximum with <kbd>1</kbd>, and the other values somewhere in between based on a linear mapping.</p>
<p>To apply our pre-processor, we run the <kbd>transform</kbd> function on it. Transformers often need to be trained first, in the same way that the classifiers do. We can combine these steps by running the <kbd>fit_transform</kbd> function instead:</p>
<pre>
X_transformed = MinMaxScaler().fit_transform(X)
</pre>
<p>Here, <kbd>X_transformed</kbd> will have the same shape as <kbd><em>X</em></kbd>. However, each column will have a maximum of <kbd>1</kbd> and a minimum of <kbd>0</kbd>.</p>
<p>There are various other forms of normalizing in this way, which is effective for other applications and feature types:</p>
<ul>
<li>Ensure the sum of the values for each sample equals to 1, using <kbd>sklearn.preprocessing.Normalizer</kbd></li>
<li>Force each feature to have a zero mean and a variance of 1, using <kbd>sklearn.preprocessing.StandardScaler</kbd>, which is a commonly used starting point for normalization</li>
<li>Turn numerical features into binary features, where any value above a threshold is 1 and any below is 0, using <kbd>sklearn.preprocessing.Binarizer</kbd></li>
</ul>
<p>We will use combinations of these pre-processors in later chapters, along with other types of <kbd>Transformers</kbd> object.</p>
<div class="packt_infobox">Pre-processing is a critical step in the data mining pipeline and one that can mean the difference between a bad and great result.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Putting it all together</h1>
            </header>

            <article>
                
<p>We can now create a workflow by combining the code from the previous sections, using the broken dataset previously calculated:</p>
<pre>
X_transformed = MinMaxScaler().fit_transform(X_broken) <br/>estimator = KNeighborsClassifier() <br/>transformed_scores = cross_val_score(estimator, X_transformed, y,    scoring='accuracy') <br/>print("The average accuracy for is {0:.1f}%".format(np.mean(transformed_scores) * 100))
</pre>
<p>We now recover our original score of 82.3 percent accuracy. The <kbd>MinMaxScaler</kbd> resulted in features of the same scale, meaning that no features overpowered others by simply being bigger values. While the Nearest Neighbor algorithm can be confused with larger features, some algorithms handle scale differences better. In contrast, some are much worse!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Pipelines</h1>
            </header>

            <article>
                
<p>As experiments grow, so does the complexity of the operations. We may split up our dataset, binarize features, perform feature-based scaling, perform sample-based scaling, and many more operations.</p>
<p>Keeping track of these operations can get quite confusing and can result in being unable to replicate the result. Problems include forgetting a step, incorrectly applying a transformation, or adding a transformation that wasn't needed.</p>
<p>Another issue is the order of the code. In the previous section, we created our <kbd>X_transformed</kbd> dataset and then created a new estimator for the cross validation.If we had multiple steps, we would need to track these changes to the dataset in code.</p>
<p>Pipelines are a construct that addresses these problems (and others, which we will see in the next chapter). Pipelines store the steps in your data mining workflow. They can take your raw data in, perform all the necessary transformations, and then create a prediction. This allows us to use pipelines in functions such as <kbd>cross_val_score</kbd>, where they expect an estimator. First, import the <kbd>Pipeline</kbd> object:</p>
<pre>
fromsklearn.pipeline import Pipeline
</pre>
<p>Pipelines take a list of steps as input, representing the chain of the data mining application. The last step needs to be an Estimator, while all previous steps are Transformers. The input dataset is altered by each Transformer, with the output of one step being the input of the next step. Finally, we classify the samples by the last step's estimator. In our pipeline, we have two steps:</p>
<ol>
<li>Use <kbd>MinMaxScaler</kbd> to scale the feature values from 0 to 1</li>
<li>Use <kbd>KNeighborsClassifier</kbd> as the classification algorithms</li>
</ol>
<p>We then represent each step using a tuple <kbd>('name',</kbd> <kbd>step)</kbd>. We can then create our pipeline:</p>
<pre>
scaling_pipeline = Pipeline([('scale', MinMaxScaler()), <br/>                             ('predict', KNeighborsClassifier())])
</pre>
<p>The key here is the list of tuples. The first tuple is our scaling step and the second tuple is the predicting step. We give each step a name: the first we call <kbd>scale</kbd> and the second we call <kbd>predict</kbd>, but you can choose your own names. The second part of the tuple is the actual <kbd>Transformer</kbd> or <kbd>estimator</kbd> object.</p>
<p>Running this pipeline is now very easy, using the cross-validation code from before:</p>
<pre>
scores = cross_val_score(scaling_pipeline, X_broken, y, scoring='accuracy') <br/>print("The pipeline scored an average accuracy for is {0:.1f}%".format(np.mean(transformed_scores) * 100))
</pre>
<p>This gives us the same score as before (82.3 percent), which is expected, as we are running exactly the same steps, just with an improved interface.</p>
<p>In later chapters, we will use more advanced testing methods and setting up pipelines is a great way to ensure that the code complexity does not grow unmanageably.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we used several of scikit-learn's methods for building a standard workflow to run and evaluate data mining models. We introduced the Nearest Neighbors algorithm, which is implemented in scikit-learn as an estimator. Using this class is quite easy; first, we call the <kbd>fit</kbd> function on our training data, and second, we use the <kbd>predict</kbd> function to predict the class of testing samples.</p>
<p>We then looked at pre-processing by fixing poor feature scaling. This was done using a <kbd>Transformer</kbd> object and the <kbd>MinMaxScaler</kbd> class. These functions also have a <kbd>fit</kbd> method and then a transform, which takes data of one form as an input and returns a transformed dataset as an output.</p>
<p>To investigate these transformations further, try swapping out the <kbd>MinMaxScaler</kbd> with some of the other mentioned transformers. Which is the most effective and why would this be the case?</p>
<p>Other transformers also exist in scikit-learn, which we will use later in this book, such as PCA. Try some of these out as well, referencing scikit-learn's excellent documentation at <a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank">https://<span class="URLPACKT">scikit-learn.org/stable/modules/preprocessing.html</span></a></p>
<p>In the next chapter, we will use these concepts in a larger example, predicting the outcome of sports matches using real-world data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>