["```py\nusing namespace Dlib;\nusing NetworkType = loss_mean_squared<fc<1, input<matrix<double>>>>;\nusing SampleType = matrix<double, 1, 1>;\nusing KernelType = linear_kernel<SampleType>;\n```", "```py\nsize_t n = 1000;\nstd::vector<matrix<double>> x(n);\nstd::vector<float> y(n);\nstd::random_device rd;\nstd::mt19937 re(rd());\nstd::uniform_real_distribution<float> dist(-1.5, 1.5);\n// generate data\nfor (size_t i = 0; i < n; ++i) {\n  x[i](0, 0) = i;\n  y[i] = func(i) + dist(re);\n}\n```", "```py\ndouble func(double x) {\n  return 4\\. + 0.3 * x;\n}\n```", "```py\nvector_normalizer<matrix<double>> normalizer_x;\nnormalizer_x.train(x);\nfor (size_t i = 0; i < x.size(); ++i) {\n  x[i] = normalizer_x(x[i]);\n}\n```", "```py\nvoid TrainAndSaveKRR(const std::vector<matrix<double>>& x,\n                     const std::vector<float>& y) {\n  krr_trainer<KernelType> trainer;\n  trainer.set_kernel(KernelType());\n  decision_function<KernelType> df = trainer.train(x, y);\n  serialize(\"Dlib-krr.dat\") << df;\n}\n```", "```py\nserialize(\"Dlib-krr.dat\") << df;\n```", "```py\nvoid TrainAndSaveNetwork(\n    const std::vector<matrix<double>>& x,\n    const std::vector<float>& y) {\n  NetworkType network;\n  sgd solver;\n  dnn_trainer<NetworkType> trainer(network, solver);\n  trainer.set_learning_rate(0.0001);\n  trainer.set_mini_batch_size(50);\n  trainer.set_max_num_epochs(300);\n  trainer.be_verbose();\n  trainer.train(x, y);\n  network.clean();\n  serialize(\"Dlib-net.dat\") << network;\n  net_to_xml(network, \"net.xml\");\n}\n```", "```py\nstd::cout << \"Target values \\n\";\nstd::vector<matrix<double>> new_x(5);\nfor (size_t i = 0; i < 5; ++i) {\n  new_x[i].set_size(1, 1);\n  new_x[i](0, 0) = i;\n  new_x[i] = normalizer_x(new_x[i]);\n  std::cout << func(i) << std::endl;\n}\n```", "```py\nvoid LoadAndPredictKRR(\n    const std::vector<matrix<double>>& x) {\n  decision_function<KernelType> df;\n  deserialize(\"Dlib-krr.dat\") >> df;\n  // Predict\n  std::cout << \"KRR predictions \\n\";\n  for (auto& v : x) {\n    auto p = df(v);\n    std::cout << static_cast<double>(p) << std::endl;\n  }\n}\n```", "```py\nvoid LoadAndPredictNetwork(\n    const std::vector<matrix<double>>& x) {\n  NetworkType network;\n  deserialize(\"Dlib-net.dat\") >> network;\n  // Predict\n  auto predictions = network(x);\n  std::cout << \"Net predictions \\n\";\n  for (auto p : predictions) {\n    std::cout << static_cast<double>(p) << std::endl;\n  }\n}\n```", "```py\nint64_t n = 10000;\nauto x = fl::randn({n});\nauto y = x * 0.3f + 0.4f;\n// Define dataset\nstd::vector<fl::Tensor> fields{x, y};\nauto dataset = std::make_shared<fl::TensorDataset>(fields);\nfl::BatchDataset batch_dataset(dataset, /*batch_size=*/64);\n```", "```py\nfl::Sequential model;\nmodel.add(fl::View({1, 1, 1, -1}));\nmodel.add(fl::Linear(1, 8));\nmodel.add(fl::ReLU());\nmodel.add(fl::Linear(8, 16));\nmodel.add(fl::ReLU());\nmodel.add(fl::Linear(16, 32));\nmodel.add(fl::ReLU());\nmodel.add(fl::Linear(32, 1));\n```", "```py\nauto loss = fl::MeanSquaredError();\nfloat learning_rate = 0.01;\nfloat momentum = 0.5;\nauto sgd = fl::SGDOptimizer(model.params(), \n                            learning_rate,\n                            momentum);\nconst int epochs = 5;\nfor (int epoch_i = 0; epoch_i < epochs; ++epoch_i) {\n  for (auto& batch : batch_dataset) {\n    sgd.zeroGrad();\n    auto predicted = model(fl::input(batch[0]));\n    auto local_batch_size = batch[0].shape().dim(0);\n    auto target =\n        fl::reshape(batch[1], {1, 1, 1, local_batch_size});\n    auto loss_value = loss(predicted, fl::noGrad(target));\n    loss_value.backward();\n    sgd.step();\n  }\n}\n```", "```py\nfl::save(\"model.dat\", model);\n```", "```py\nfl::Sequential model_loaded;\nfl::load(\"model.dat\", model_loaded);\n```", "```py\nauto predicted = model_loaded(fl::noGrad(new_x));\n```", "```py\nfl::save(\"model_params.dat\", model.params());\n```", "```py\nstd::vector<fl::Variable> params;\nfl::load(\"model_params.dat\", params);\nfor (int i = 0; i < static_cast<int>(params.size()); ++i) {\n  model.setParams(params[i], i);\n}\n```", "```py\nusing ModelType = FFN<MeanSquaredError, ConstInitialization>;\nModelType make_model() {\n  MeanSquaredError loss;\n  ConstInitialization init(0.);\n  ModelType model(loss, init);\n  model.Add<Linear>(8);\n  model.Add<ReLU>();\n  model.Add<Linear>(16);\n  model.Add<ReLU>();\n  model.Add<Linear>(32);\n  model.Add<ReLU>();\n  model.Add<Linear>(1);\n  return model;\n}\n```", "```py\nsize_t n = 10000;\narma::mat x = arma::randn(n).t();\narma::mat y = x * 0.3f + 0.4f;\n```", "```py\nens::Adam optimizer;\nauto model = make_model();\nmodel.Train(x, y, optimizer);\n```", "```py\ndata::Save(\"model.bin\", model.Parameters(), true);\n```", "```py\nauto new_model = make_model();\ndata::Load(\"model.bin\", new_model.Parameters());\n```", "```py\narma::mat predictions;\nnew_model.Predict(new_x, predictions);\n```", "```py\ntorch::DeviceType device = torch::cuda::is_available()\n? torch::DeviceType::CUDA\n: torch::DeviceType::CPU;\n```", "```py\nstd::random_device rd;\nstd::mt19937 re(rd());\nstd::uniform_real_distribution<float> dist(-0.1f, 0.1f);\n```", "```py\nsize_t n = 1000;\ntorch::Tensor x;\ntorch::Tensor y;\n{\n  std::vector<float> values(n);\n  std::iota(values.begin(), values.end(), 0);\n  std::shuffle(values.begin(), values.end(), re);\n  std::vector<torch::Tensor> x_vec(n);\n  std::vector<torch::Tensor> y_vec(n);\n  for (size_t i = 0; i < n; ++i) {\n    x_vec[i] = torch::tensor(\n        values[i],\n        torch::dtype(torch::kFloat).device(\n                                    device).requires_grad(false));\n    y_vec[i] = torch::tensor(\n        (func(values[i]) + dist(re)),\n        torch::dtype(torch::kFloat).device(\n                                    device).requires_grad(false));\n  }\n  x = torch::stack(x_vec);\n  y = torch::stack(y_vec);\n}\n```", "```py\nfloat func(float x) {\n  return 4.f + 0.3f * x;\n}\n```", "```py\nauto x_mean = torch::mean(x, /*dim*/ 0);\nauto x_std = torch::std(x, /*dim*/ 0);\nx = (x - x_mean) / x_std;\n```", "```py\nclass NetImpl : public torch::nn::Module {\n public:\n  NetImpl() {\n    l1_ = torch::nn::Linear(torch::nn::LinearOptions(\n                                1, 8).with_bias(true));\n    register_module(\"l1\", l1_);\n    l2_ = torch::nn::Linear(torch::nn::LinearOptions(\n                                8, 4).with_bias(true));\n    register_module(\"l2\", l2_);\n    l3_ = torch::nn::Linear(torch::nn::LinearOptions(\n                                4, 1).with_bias(true));\n    register_module(\"l3\", l3_);\n    // initialize weights\n    for (auto m : modules(false)) {\n      if (m->name().find(\"Linear\") != std::string::npos) {\n        for (auto& p : m->named_parameters()) {\n          if (p.key().find(\"weight\") != std::string::npos) {\n            torch::nn::init::normal_(p.value(), 0, 0.01);\n                    }\n          if (p.key().find(\"bias\") != std::string::npos) {\n            torch::nn::init::zeros_(p.value());\n          }\n        }\n      }\n    }\n  }\ntorch::Tensor forward(torch::Tensor x) {\n  auto y = l1_(x);\n  y = l2_(y);\n  y = l3_(y);\n  return y;\n}\nprivate:\n  torch::nn::Linear l1_{nullptr};\n  torch::nn::Linear l2_{nullptr};\n  torch::nn::Linear l3_{nullptr};\n}\nTORCH_MODULE(Net);\n```", "```py\nNet model;\nmodel->to(device);\n// initialize optimizer -----------------------------------\ndouble learning_rate = 0.01;\ntorch::optim::Adam optimizer(model->parameters(),\ntorch::optim::AdamOptions(learning_rate).weight_decay(0.00001));\n// training\nint64_t batch_size = 10;\nint64_t batches_num = static_cast<int64_t>(n) / batch_size;\nint epochs = 10;\nfor (int epoch = 0; epoch < epochs; ++epoch) {\n  // train the model\n  // -----------------------------------------------\n  model->train();  // switch to the training mode\n  // Iterate the data\n  double epoch_loss = 0;\n  for (int64_t batch_index = 0; batch_index < batches_num;\n       ++batch_index) {\n    auto batch_x =\n        x.narrow(0, batch_index * batch_size, batch_size)\n            .unsqueeze(1);\n    auto batch_y =\n        y.narrow(0, batch_index * batch_size, batch_size)\n            .unsqueeze(1);\n    // Clear gradients\n    optimizer.zero_grad();\n    // Execute the model on the input data\n    torch::Tensor prediction = model->forward(batch_x);\n    torch::Tensor loss =\n        torch::mse_loss(prediction, batch_y);\n    // Compute gradients of the loss and parameters of\n    // our model\n    loss.backward();\n    // Update the parameters based on the calculated\n    // gradients.\n    optimizer.step();\n  }\n}\n```", "```py\ntorch::save(model, \"pytorch_net.pt\");\n```", "```py\nNet model_loaded;\ntorch::load(model_loaded, \"pytorch_net.pt\");\n```", "```py\nvoid NetImpl::SaveWeights(const std::string& file_name) {\n  torch::serialize::OutputArchive archive;\n  auto parameters = named_parameters(true /*recurse*/);\n  auto buffers = named_buffers(true /*recurse*/);\n  for (const auto& param : parameters) {\n    if (param.value().defined()) {\n      archive.write(param.key(), param.value());\n    }\n  }\n  for (const auto& buffer : buffers) {\n    if (buffer.value().defined()) {\n      archive.write(buffer.key(), buffer.value(),\n                    /*is_buffer*/ true);\n    }\n  }\n  archive.save_to(file_name);\n}\n```", "```py\nvoid NetImpl::LoadWeights(const std::string& file_name) {\n  torch::serialize::InputArchive archive;\n  archive.load_from(file_name);\n  torch::NoGradGuard no_grad;\n  auto parameters = named_parameters(true /*recurse*/);\n  auto buffers = named_buffers(true /*recurse*/);\n  for (auto& param : parameters) {\n      archive.read(param.key(), param.value());\n  }\n  for (auto& buffer : buffers) {\n      archive.read(buffer.key(), buffer.value(),\n          /*is_buffer*/ true);\n  }\n}\n```", "```py\nmodel_loaded->to(device);\nmodel_loaded->eval();\nstd::cout << \"Test:\\n\";\nfor (int i = 0; i < 5; ++i) {\n  auto x_val = static_cast<float>(i) + 0.1f;\n  auto tx = torch::tensor(\n      x_val, torch::dtype(torch::kFloat).device(device));\n  tx = (tx - x_mean) / x_std;\n  auto ty = torch::tensor(\n      func(x_val),\n      torch::dtype(torch::kFloat).device(device));\n  torch::Tensor prediction = model_loaded->forward(tx);\n  std::cout << \"Target:\" << ty << std::endl;\n  std::cout << \"Prediction:\" << prediction << std::endl;\n}\n```", "```py\nir_version: 3\ngraph {\n  node {\n  input: \"data\"\n  input: \"resnetv24_batchnorm0_gamma\"\n  input: \"resnetv24_batchnorm0_beta\"\n  input: \"resnetv24_batchnorm0_running_mean\"\n  input: \"resnetv24_batchnorm0_running_var\"\n  output: \"resnetv24_batchnorm0_fwd\"\n  name: \"resnetv24_batchnorm0_fwd\"\n  op_type: \"BatchNormalization\"\n  attribute {\n      name: \"epsilon\"\n      f: 1e-05\n      type: FLOAT\n  }\n  attribute {\n      name: \"momentum\"\n      f: 0.9\n      type: FLOAT\n  }\n  attribute {\n      name: \"spatial\"\n      i: 1\n      type: INT\n  }\n}\nnode {\n  input: \"resnetv24_batchnorm0_fwd\"\n  input: \"resnetv24_conv0_weight\"\n  output: \"resnetv24_conv0_fwd\"\n  name: \"resnetv24_conv0_fwd\"\n  op_type: \"Conv\"\n  attribute {\n      name: \"dilations\"\n      ints: 1\n      ints: 1\n      type: INTS\n  }\n  attribute {\n      name: \"group\"\n      i: 1\n      type: INT\n  }\n  attribute {\n      name: \"kernel_shape\"\n      ints: 7\n      ints: 7\n      type: INTS\n  }\n  attribute {\n      name: \"pads\"\n      ints: 3\n      ints: 3\n      ints: 3\n      ints: 3\n      type: INTS\n  }\n  attribute {\n      name: \"strides\"\n      ints: 2\n      ints: 2\n      type: INTS\n  }\n}\n...\n}\n```", "```py\n#include <onnxruntime_cxx_api.h>\n```", "```py\nOrt::Env env;\nOrt::Session session(env,\n                     \"resnet50-v1-7.onnx\",\n                     Ort::SessionOptions{nullptr});\n```", "```py\nvoid show_model_info(const Ort::Session& session) {\n  Ort::AllocatorWithDefaultOptions allocator;\n```", "```py\n  auto num_inputs = session.GetInputCount();\n  for (size_t i = 0; i < num_inputs; ++i) {\n    auto input_name = session.GetInputNameAllocated(i,\n                                           allocator);\n    std::cout << \"Input name \" << i << \" : \" << input_name\n              << std::endl;\n    Ort::TypeInfo type_info = session.GetInputTypeInfo(i);\n    auto tensor_info =\n        type_info.GetTensorTypeAndShapeInfo();\n    auto tensor_shape = tensor_info.GetShape();\n    std::cout << \"Input shape \" << i << \" : \";\n    for (size_t j = 0; j < tensor_shape.size(); ++j)\n      std::cout << tensor_shape[j] << \" \";\n    std::cout << std::endl;\n  }\n```", "```py\n auto num_outputs = session.GetOutputCount();\n  for (size_t i = 0; i < num_outputs; ++i) {\n    auto output_name = session.GetOutputNameAllocated(i,\n                                             allocator);\n  std::cout << \"Output name \" << i << \" : \" <<\n                       output_name << std::endl;\n  Ort::TypeInfo type_info = session.GetOutputTypeInfo(i);\n  auto tensor_info = type_info.GetTensorTypeAndShapeInfo();\n  auto tensor_shape = tensor_info.GetShape();\n  std::cout << \"Output shape \" << i << \" : \";\n  for (size_t j = 0; j < tensor_shape.size(); ++j)\n    std::cout << tensor_shape[j] << \" \";\n  std::cout << std::endl;\n  }\n}\n```", "```py\nconstexpr const int width = 224;\nconstexpr const int height = 224;\nstd::array<int64_t, 4> input_shape{1, 3, width, height};\nstd::vector<float> input_image(3 * width * height);\nread_image(argv[3], width, height, input_image);\nauto memory_info = Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator,\n                                              OrtMemTypeCPU);\nOrt::Value input_tensor =\n    Ort::Value::CreateTensor<float>(memory_ info,\n                                    input_image.data(),\n                                    input_image.size(),\n                                    input_shape.data(),\n                                    input_shape.size());\n```", "```py\nstd::array<int64_t, 2> output_shape{1, 1000};\nstd::vector<float> result(1000);\nOrt::Value output_tensor =\n    Ort::Value::CreateTensor<float>(memory_ info,\n                                    result.data(),\n                                    result.size(),\n                                    output_shape.data(),\n                                    output_shape.size());\n```", "```py\nOrt::Value output_tensor{nullptr};\n```", "```py\nconst char* input_names[] = {\"data\"};\nconst char* output_names[] = {\"resnetv17_dense0_fwd\"};\nOrt::RunOptions run_options;\nsession.Run(run_options,\n            input_names,\n            &input_tensor,\n            1,\n            output_names,\n            &output_tensor,\n            1);\n\n```", "```py\nstd::map<size_t, std::string> classes = read_classes(\"synset.txt\");\nstd::vector<std::pair<float, size_t>> pairs;\nfor (size_t i = 0; i < result.size(); i++) {\n  if (result[i] > 0.01f) {  // threshold check\n    pairs.push_back(std::make_pair(\n        output[i], i + 1));  // 0 –//background\n  }\n}\nstd::sort(pairs.begin(), pairs.end());\nstd::reverse(pairs.begin(), pairs.end());\npairs.resize(std::min(5UL, pairs.size()));\nfor (auto& p : pairs) {\n  std::cout << \"Class \" << p.second << \" Label \"\n            << classes.at(p.second) << « Prob « << p.first\n            << std::endl;\n}\n```", "```py\nvoid read_image(const std::string& file_name,\n                       int width,\n                       int height,\n                       std::vector<float>& image_data)\n...\n}\n```", "```py\n// load image\nauto image = cv::imread(file_name, cv::IMREAD_COLOR);\nif (!image.cols || !image.rows) {\n  return {};\n}\nif (image.cols != width || image.rows != height) {\n  // scale image to fit\n  cv::Size scaled(\n      std::max(height * image.cols / image.rows, width),\n      std::max(height, width * image.rows / image.cols));\n  cv::resize(image, image, scaled);\n  // crop image to fit\n  cv::Rect crop((image.cols - width) / 2,\n                (image.rows - height) / 2, width, height);\n  image = image(crop);\n}\n```", "```py\nimage.convertTo(image, CV_32FC3);\ncv::cvtColor(image, image, cv::COLOR_BGR2RGB);\n```", "```py\nstd::vector<cv::Mat> channels(3);\ncv::split(image, channels);\nstd::vector<double> mean = {0.485, 0.456, 0.406};\nstd::vector<double> stddev = {0.229, 0.224, 0.225};\nsize_t i = 0;\nfor (auto& c : channels) {\n  c = ((c / 255) - mean[i]) / stddev[i];\n  ++i;\n}\n```", "```py\ncv::vconcat(channels[0], channels[1], image);\ncv::vconcat(image, channels[2], image);\nassert(image.isContinuous());\n```", "```py\nstd::vector<int64_t> dims = {1, 3, height, width};\nstd::copy_n(reinterpret_cast<float*>(image.data),\nimage.size().area(),\nimage_data.begin());\n```", "```py\nusing Classes = std::map<size_t, std::string>;\nClasses read_classes(const std::string& file_name) {\n  Classes classes;\n  std::ifstream file(file_name);\n  if (file) {\n    std::string line;\n    std::string id;\n    std::string label;\n    std::string token;\n    size_t idx = 1;\n   while (std::getline(file, line)) {\n      std::stringstream line_stream(line);\n      size_t i = 0;\n      while (std::getline(line_stream, token, ' ')) {\n        switch (i) {\n          case 0:\n            id = token;\n            break;\n          case 1:\n            label = token;\n            break;\n        }\n        token.clear();\n        ++i;\n      }\n      classes.insert({idx, label});\n      ++idx;\n    }\n  }\n  return classes;\n```"]