- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll go through a number of dimension-reduction tasks. We’ll
    look at the conditions in which dimension-reduction is required and learn how
    to use dimension-reduction algorithms efficiently in C++ with various libraries.
    Dimensionality reduction involves transforming high-dimensional data into a new
    representation with fewer dimensions while preserving the most crucial information
    from the original data. Such a transformation can help us visualize multidimensional
    space, which can be useful in the data exploration stage or when identifying the
    most relevant features in dataset samples. Some **machine learning** (**ML**)
    techniques can perform better or faster if our data has a smaller number of features
    since it can consume fewer computational resources. The main purpose of this kind
    of transformation is to save the essential features—those features that hold the
    most critical information present in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of dimension-reduction methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring linear methods for dimension-reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring non-linear methods for dimension-reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding dimension-reduction algorithms with various С++ libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technologies you’ll need for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The Tapkee library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Dlib` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `plotcpp` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CMake build system, version >= 3.24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found at the following GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06).'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of dimension-reduction methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal of dimension-reduction methods is to make the dimension of the
    transformed representation correspond with the internal dimension of the data.
    In other words, it should be similar to the minimum number of variables necessary
    to express all the possible properties of the data. Reducing the dimension helps
    mitigate the impact of the curse of dimensionality and other undesirable properties
    that occur in high-dimensional spaces. As a result, reducing dimensionality can
    effectively solve problems regarding classification, visualization, and compressing
    high-dimensional data. It makes sense to apply dimensionality reduction only when
    particular data is redundant; otherwise, we can lose important information. In
    other words, if we are able to solve the problem using data of smaller dimensions
    with the same level of efficiency and accuracy, then some of our data is redundant.
    Dimensionality reduction allows us to reduce the time and computational costs
    of solving a problem. It also makes data and the results of data analysis easier
    to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to reduce the number of features when the information that can
    be used to solve the problem at hand qualitatively is contained in a specific
    subset of features. Non-informative features are a source of additional noise
    and affect the accuracy of the model parameter’s estimation. In addition, datasets
    with a large number of features can contain groups of correlated variables. The
    presence of such feature groups leads to the duplication of information, which
    may distort the model’s results and affect how well it estimates the values of
    its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The methods surrounding dimensionality reduction are mainly unsupervised because
    we don’t know which features or variables can be excluded from the original dataset
    without losing the most crucial information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some real-life examples of dimensionality reduction include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In recommender systems, dimensionality reduction can be used to represent users
    and items as vectors in a lower-dimensional space, making it easier to find similar
    users or items
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In image recognition, dimensionality reduction techniques, such as **principal
    component analysis** (**PCA**), can be applied to reduce the size of images while
    preserving their important features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In text analysis, dimensionality reduction can be employed to transform large
    collections of documents into lower-dimensional representations that capture the
    main topics discussed in the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimensionality reduction methods can be classified into two groups: feature
    selection and the creation of new low-dimensional features. These methods can
    then be subdivided into **linear** and **non-linear** approaches, depending on
    the nature of the data and the mathematical apparatus being used.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature selection methods don’t change the initial values of the variables
    or features; instead, they remove the irrelevant features from the source dataset.
    Some of the feature selection methods we can use are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing value ratio**: This method is based on the idea that a feature that
    misses many values should be eliminated from a dataset because it doesn’t contain
    valuable information and can distort the model’s performance results. So, if we
    have some criteria for identifying missing values, we can calculate their ratio
    to typical values and set a threshold that we can use to eliminate features with
    a high missing value ratio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low variance filter**: This method is used to remove features with low variance
    because such features don’t contain enough information to improve model performance.
    To apply this method, we need to calculate the variance for each feature, sort
    them in ascending order by this value, and leave only those with the highest variance
    values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High correlation filter**: This method is based on the idea that if two features
    have a high correlation, then they carry similar information. Also, highly correlated
    features can significantly reduce the performance of some ML models, such as linear
    and logistic regression. Therefore, the primary goal of this method is to leave
    only the features that have a high correlation with target values and don’t have
    much correlation with each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest**: This method can be used for feature selection effectively
    (although it wasn’t initially designed for this kind of task). After we’ve built
    the forest, we can estimate what features are most important by estimating the
    impurity factor in the tree’s nodes. This factor shows the measure of split distinctness
    in the tree’s nodes, and it demonstrates how well the current feature (a random
    tree only uses one feature in a node to split input data) splits data into two
    distinct buckets. Then, this estimation can be averaged across all the trees in
    the forest. Features that split data better than others can be selected as the
    most important ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward feature elimination and forward feature selection**: These are iterative
    methods that are used for feature selection. In backward feature elimination,
    after we’ve trained the model with a full feature set and estimated its performance,
    we remove its features one by one and train the model with a reduced feature set.
    Then, we compare the model’s performances and decide how much performance is improved
    by removing feature changes—in other words, we’re deciding how important each
    feature is. In forward feature selection, the training process goes in the opposite
    direction. We start with one feature and then add more of them. These methods
    are very computationally expensive and can only be used on small datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dimensionality reduction methods transform an original feature set into a new
    feature set that usually contains new features that weren’t present in the initial
    dataset. These methods can also be divided into two subclasses—linear and non-linear.
    The non-linear methods are usually more computationally expensive, so if we have
    a prior assumption about our feature’s data linearity, we can choose the more
    suitable class of methods at the initial stage.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will describe the various linear and non-linear methods
    we can use for dimension-reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring linear methods for dimension-reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will describe the most popular linear methods that are
    used for dimension-reduction, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular value** **decomposition** (**SVD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent component** **analysis** (**ICA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear discriminant** **analysis** (**LDA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multidimensional** **scaling** (**MDS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**PCA** is one of the most intuitively simple and frequently used methods for
    applying dimension-reduction to data and projecting it onto an orthogonal subspace
    of features. In a very general form, it can be represented as the assumption that
    all our observations look like some ellipsoid in the subspace of our original
    space. Our new basis in this space coincides with the axes of this ellipsoid.
    This assumption allows us to get rid of strongly correlated features simultaneously
    since the basis vectors of the space we project them onto are orthogonal.'
  prefs: []
  type: TYPE_NORMAL
- en: The dimension of this ellipsoid is equal to the dimension of the original space,
    but our assumption that the data lies in a subspace of a smaller dimension allows
    us to discard the other subspaces in the new projection; namely, the subspace
    with the least extension of the ellipsoid. We can do this greedily, choosing a
    new element one by one on the basis of our new subspace, and then taking the axis
    of the ellipsoid with maximum dispersion successively from the remaining dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce the dimension of our data from ![](img/B19849_Formula_001.png) to
    ![](img/B19849_Formula_002.png), we need to choose the top ![](img/B19849_Formula_003.png)
    axes of such an ellipsoid, sorted in descending order by dispersion along the
    axes. To begin with, we calculate the variances and covariances of the original
    features. This is done by using a **covariance matrix**. By the definition of
    covariance, for two signs, ![](img/B19849_Formula_004.png) and ![](img/B19849_Formula_005.png),
    their covariance should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_007.png) is the mean of the ![](img/B19849_Formula_008.png)
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we note that the covariance is symmetric and that the covariance
    of the vector itself is equal to its dispersion. Thus, the covariance matrix is
    a symmetric matrix where the dispersions of the corresponding features lie on
    the diagonal and the covariances of the corresponding pairs of features lie outside
    the diagonal. In the matrix view, where ![](img/B19849_Formula_009.png) is the
    observation matrix, our covariance matrix looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The covariance matrix is a generalization of variance in the case of multidimensional
    random variables—it also describes the shape (spread) of a random variable, as
    does the variance. Matrices, such as linear operators, have eigenvalues and eigenvectors.
    They are interesting because when we act on the corresponding linear space or
    transform it with our matrix, the eigenvectors remain in place, and they are only
    multiplied by the corresponding eigenvalues. This means they define a subspace
    that remains in place or *goes into itself* when we apply a linear operator matrix
    to it. Formally, an eigenvector, ![](img/B19849_Formula_011.png), with an eigenvalue
    for a matrix is defined simply as ![](img/B19849_Formula_012.png).
  prefs: []
  type: TYPE_NORMAL
- en: The covariance matrix for our sample, ![](img/B19849_Formula_013.png), can be
    represented as a product, ![](img/B19849_Formula_014.png). From the Rayleigh relation,
    it follows that the maximum variation of our dataset can be achieved along the
    eigenvector of this matrix, which corresponds to the maximum eigenvalue. This
    is also true for projections on a higher number of dimensions—the variance (covariance
    matrix) of the projection onto the *m*-dimensional space is maximum in the direction
    of ![](img/B19849_Formula_015.png) eigenvectors with maximum eigenvalues. Thus,
    the principal components that we would like to project our data for are simply
    the eigenvectors of the corresponding top *k* pieces of the eigenvalues of this
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The largest vector has a direction similar to the regression line, and by projecting
    our sample onto it, we lose information, similar to the sum of the residual members
    of the regression. It is necessary to make the operation, ![](img/B19849_Formula_016.png)
    (the vector length (magnitude) should be equal to one), perform the projection.
    If we don’t have a single vector and have a hyperplane instead, then instead of
    the vector, ![](img/B19849_Formula_017.png), we take the matrix of basis vectors,
    ![](img/B19849_Formula_018.png). The resulting vector (or matrix) is an array
    of projections of our observations; that is, we need to multiply our data matrix
    on the basis vectors matrix, and we get the projection of our data orthogonally.
    Now, if we multiply the transpose of our data matrix and the matrix of the principal
    component vectors, we restore the original sample in the space where we projected
    it onto the basis of the principal components. If the number of components is
    less than the dimension of the original space, we lose some information.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVD is an important method that’s used to analyze data. The resulting matrix
    decomposition has a meaningful interpretation from an ML point of view. It can
    also be used to calculate PCA. SVD is rather slow. Therefore, when the matrices
    are too large, randomized algorithms are used. However, the SVD calculation is
    computationally more efficient than the calculation for the covariance matrix
    and its eigenvalues in the original PCA approach. Therefore, PCA is often implemented
    in terms of SVD. Let’s take a look.
  prefs: []
  type: TYPE_NORMAL
- en: 'The essence of SVD is straightforward—any matrix (real or complex) is represented
    as a product of three matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B19849_Formula_020.png) is a unitary matrix of order ![](img/B19849_Formula_0211.png)
    and ![](img/B19849_Formula_0221.png) is a matrix of size ![](img/B19849_Formula_023.png)
    on the main diagonal, which is where there are non-negative numbers called singular
    values (elements outside the main diagonal are zero—such matrices are sometimes
    called rectangular diagonal matrices). ![](img/B19849_Formula_024.png) is a Hermitian-conjugate
    ![](img/B19849_Formula_025.png) matrix of order ![](img/B19849_Formula_026.png).
    The ![](img/B19849_Formula_027.png) columns of the matrices ![](img/B19849_Formula_028.png)
    and ![](img/B19849_Formula_029.png) columns of the matrix ![](img/B19849_Formula_030.png)
    are called the left and right singular vectors of matrix ![](img/B19849_Formula_0311.png),
    respectively. To reduce the number of dimensions, matrix ![](img/B19849_Formula_0221.png)
    is important, the elements of which, when raised to the second power, can be interpreted
    as a variance that each component puts into a joint distribution, and they are
    in descending order: ![](img/B19849_Formula_033.png). Therefore, when we choose
    the number of components in SVD (as in PCA), we should take the sum of their variances
    into account.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The relation between SVD and PCA can be described in the following way: ![](img/B19849_Formula_034.png)
    is the covariance matrix given by ![](img/B19849_Formula_035.png). It is a symmetric
    matrix, so it can be diagonalized as ![](img/B19849_Formula_036.png), where ![](img/B19849_Formula_025.png)
    is a matrix of eigenvectors (each column is an eigenvector) and ![](img/B19849_Formula_038.png)
    is a diagonal matrix of eigenvalues, ![](img/B19849_Formula_039.png), in decreasing
    order on the diagonal. The eigenvectors are called principal axes or principal
    directions of the data. Projections of the data on the principal axes are called
    **principal components**, also known as **principal component scores**. They are
    newly transformed variables. The ![](img/B19849_Formula_040.png) principal component
    is given by the ![](img/B19849_Formula_0411.png) column of ![](img/B19849_Formula_042.png).
    The coordinates of the ![](img/B19849_Formula_043.png) data point in the new principal
    component’s space are given by the ![](img/B19849_Formula_044.png) row of ![](img/B19849_Formula_045.png).'
  prefs: []
  type: TYPE_NORMAL
- en: By performing SVD on ![](img/B19849_Formula_046.png), we get ![](img/B19849_Formula_047.png),
    where ![](img/B19849_Formula_048.png) is a unitary matrix and ![](img/B19849_Formula_049.png)
    is the diagonal matrix of singular values, ![](img/B19849_Formula_050.png). We
    can observe that ![](img/B19849_Formula_0511.png), which means that the right
    singular vectors, ![](img/B19849_Formula_052.png), are principal directions and
    that singular values are related to the eigenvalues of the covariance matrix via
    ![](img/B19849_Formula_053.png). Principal components are given by ![](img/B19849_Formula_054.png).
  prefs: []
  type: TYPE_NORMAL
- en: Independent component analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **ICA** method was proposed as a way to solve the problem of **blind signal
    separation** (**BSS**); that is, selecting independent signals from mixed data.
    Let’s look at an example of the task of BSS. Suppose we have two people in the
    same room who are talking and generating acoustic waves. We have two microphones
    in different parts of the room, recording sound. The analysis system receives
    two signals from the two microphones, each of which is a digitized mixture of
    two acoustic waves—one from people speaking and one from some other noise (for
    example, playing music). Our goal is to select our initial signals from the incoming
    mixtures. Mathematically, the problem can be described as follows. We represent
    the incoming mixture in the form of a linear combination, where ![](img/B19849_Formula_055.png)
    represents the displacement coefficients and ![](img/B19849_Formula_056.png) represents
    the values of the vector of independent components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_057.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In matrix form, this can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_058.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have to find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_059.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, ![](img/B19849_Formula_060.png) is a matrix of input signal
    values, ![](img/B19849_Formula_0611.png) is a matrix of displacement coefficients
    or mixing matrix, and ![](img/B19849_Formula_062.png) is a matrix of independent
    components. Thus, the problem is divided into two. The first part is to get an
    estimate, ![](img/B19849_Formula_063.png), of the variables,![](img/B19849_Formula_064.png),
    of the original independent components. The second part is to find the matrix,
    ![](img/B19849_Formula_065.png). How this method works is based on two principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Independent components must be statistically independent (![](img/B19849_Formula_066.png)
    matrix values). Roughly speaking, the values of one vector of an independent component
    do not affect the values of another component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent components must have a non-Gaussian distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theoretical basis of ICA is the central limit theorem, which states that
    the distribution of the sum (average or linear combination) of ![](img/B19849_Formula_067.png)
    independent random variables approaches Gaussian for ![](img/B19849_Formula_068.png).
    In particular, if ![](img/B19849_Formula_069.png) are random variables independent
    of each other, taken from an arbitrary distribution with an average, ![](img/B19849_Formula_070.png),
    and a variance of ![](img/B19849_Formula_0711.png), then if we denote the mean
    of these variables as ![](img/B19849_Formula_072.png), we can say that ![](img/B19849_Formula_073.png)
    approaches the Gaussian with a mean of `0` and a variance of `1`. To solve the
    BSS problem, we need to find the matrix, ![](img/B19849_Formula_074.png), so that
    ![](img/B19849_Formula_075.png). Here, the ![](img/B19849_Formula_076.png) should
    be as close as possible to the original independent sources. We can consider this
    approach as the inverse process of the central limit theorem. All ICA methods
    are based on the same fundamental approach —finding a matrix, *W*, that maximizes
    non-Gaussianity, thereby minimizing the independence of ![](img/B19849_Formula_076.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fast ICA algorithm aims to maximize the function, ![](img/B19849_Formula_078.png),
    where ![](img/B19849_Formula_079.png) are components of ![](img/B19849_Formula_080.png).
    Therefore, we can rewrite the function’s equation in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_0811.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the ![](img/B19849_Formula_0821.png) vector is the *i*th row of the matrix,
    *W*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ICA algorithm performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It chooses the initial value of *w*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It calculates ![](img/B19849_Formula_083.png), where ![](img/B19849_Formula_084.png)
    is the derivative of the function, *G(z)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It normalizes ![](img/B19849_Formula_085.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It repeats the previous two steps until *w* stops changing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To measure non-Gaussianity, Fast ICA relies on a nonquadratic non-linear function,
    *G (z)*, that can take the following forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_086.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear discriminant analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LDA is a type of multivariate analysis that allows us to estimate differences
    between two or more groups of objects at the same time. The basis of discriminant
    analysis is the assumption that the descriptions of the objects of each *k*th
    class are instances of a multidimensional random variable that’s distributed according
    to the normal (Gaussian) law, ![](img/B19849_Formula_087.png), with an average,
    ![](img/B19849_Formula_088.png), and the following covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_089.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The index, ![](img/B19849_Formula_015.png), indicates the dimension of the
    feature space. Consider a simplified geometric interpretation of the LDA algorithm
    for the case of two classes. Let the discriminant variables, ![](img/B19849_Formula_0911.png),
    be the axes of the ![](img/B19849_Formula_092.png)-dimensional Euclidean space.
    Each object (sample) is a point of this space with coordinates representing the
    fixed values of each variable. If both classes differ from each other in observable
    variables (features), they can be represented as clusters of points in different
    regions of the considered space that may partially overlap. To determine the position
    of each class, we can calculate its **centroid**, which is an imaginary point
    whose coordinates are the average values of the variables (features) in the class.
    The task of discriminant analysis is to create an additional ![](img/B19849_Formula_093.png)
    axis that passes through a cloud of points in such a way that the projections
    on it provide the best separability into two classes (in other words, it maximizes
    the distance between classes). Its position is given by a **linear discriminant**
    (**LD**) function with weights, ![](img/B19849_Formula_094.png), that determine
    the contribution of each initial variable, ![](img/B19849_Formula_095.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_096.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we assume that the covariance matrices of the objects of classes 1 and 2
    are equal, that is, ![](img/B19849_Formula_097.png), then the vector of coefficients,
    ![](img/B19849_Formula_098.png), of the LD, ![](img/B19849_Formula_099.png), can
    be calculated using the formula ![](img/B19849_Formula_100.png), where ![](img/B19849_Formula_1011.png)
    is the inverse of the covariance matrix and ![](img/B19849_Formula_088.png) is
    the mean of the ![](img/B19849_Formula_103.png) class. The resulting axis coincides
    with the equation of a line passing through the centroids of two groups of class
    objects. The generalized Mahalanobis distance, which is equal to the distance
    between them in the multidimensional feature space, is estimated as ![](img/B19849_Formula_104.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in addition to the assumption regarding the normal (Gaussian) distribution
    of class data, which in practice occurs quite rarely, the LDA has a stronger assumption
    about the statistical equality of intragroup dispersions and correlation matrices.
    If there are no significant differences between them, they are combined into a
    calculated covariance matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_105.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This principle can be generalized to a larger number of classes. The final
    algorithm may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_106.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The interclass scattering matrix is calculated like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_107.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_108.png) is the mean of all objects (samples),
    ![](img/B19849_Formula_029.png) is the number of classes, ![](img/B19849_Formula_110.png)
    is the number of objects in the *i*th class, ![](img/B19849_Formula_1111.png)
    is the intraclass’ mean, ![](img/B19849_Formula_1121.png) is the scattering matrix
    for the *i*th class, and ![](img/B19849_Formula_1131.png) is a centering matrix,
    where ![](img/B19849_Formula_114.png) is the *n* x *n* matrix of all 1s.
  prefs: []
  type: TYPE_NORMAL
- en: Based on these matrices, the ![](img/B19849_Formula_115.png) matrix is calculated,
    for which the eigenvalues and the corresponding eigenvectors are determined. In
    the diagonal elements of the matrix, we must select the *s* of the largest eigenvalues
    and transform the matrix, leaving only the corresponding *s* rows in it. The resulting
    matrix can be used to convert all objects into lower-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: This method requires labeled data, meaning it is a supervised method.
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Factor analysis** is used to reduce the number of variables that are used
    to describe data and determine the relationships between them. During the analysis,
    variables that correlate with each other are combined into one factor. As a result,
    the dispersion between components is redistributed, and the structure of factors
    becomes more understandable. After combining the variables, the correlation of
    components within each factor becomes higher than their correlation with components
    from other factors. It is assumed that known variables depend on a smaller number
    of unknown variables and that we have a random error that can be expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_116.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_117.png) is the load and ![](img/B19849_Formula_118.png)
    is the factor.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of **factor load** is essential. It is used to describe the role
    of the factor (variable) when we wish to form a specific vector from a new basis.
    The essence of factor analysis is the procedure of rotating factors, that is,
    redistributing the dispersion according to a specific method. The purpose of rotations
    is to define a simple structure of factor loadings. Rotation can be orthogonal
    and oblique. In the first form of rotation, each successive factor is determined
    to maximize the variability that remains from the previous factors. Therefore,
    the factors are independent and uncorrelated with each other. The second type
    is a transformation in which factors correlate with each other. There are about
    13 methods of rotation that are used in both forms. The factors that have a similar
    effect on the elements of the new basis are combined into one group. Then, from
    each group, it is recommended to leave one representative. Some algorithms, instead
    of choosing a representative, calculate a new factor with some heuristics that
    become central to the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction occurs while transitioning to a system of factors
    that are representatives of groups, and the other factors are discarded. There
    are several commonly used criteria for determining the number of factors. Some
    of these criteria can be used together to complement each other. An example of
    a criterion that’s used to determine the number of factors is the Kaiser criterion
    or the eigenvalue criterion: only factors with eigenvalues equal to or greater
    than *one* are selected. This means that if a factor does not select a variance
    equivalent to at least one variance of one variable, then it is omitted. The general
    factor analysis algorithm follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It calculates the correlation matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It selects the number of factors for inclusion, for example, with the Kaiser
    criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It extracts the initial set of factors. There are several different extraction
    methods, including maximum likelihood, PCA, and principal axis extraction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It rotates the factors to a final solution that is equal to the one that was
    obtained in the initial extraction but that has the most straightforward interpretation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multidimensional scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MDS can be considered as an alternative to factor analysis when, in addition
    to the correlation matrices, an arbitrary type of object similarity matrix can
    be used as input data. MDS is not so much a formal mathematical procedure but
    rather a method of efficiently placing objects, thus keeping an appropriate distance
    between them in a new feature space. The dimension of the new space in MDS is
    always substantially less than the original space. The data that’s used for analysis
    by MDS is often obtained from the matrix of pairwise comparisons of objects.
  prefs: []
  type: TYPE_NORMAL
- en: The main MDS algorithm’s goal is to restore the unknown dimension, ![](img/B19849_Formula_119.png),
    of the analyzed feature space and assign coordinates to each object in such a
    way that the calculated pairwise Euclidean distances between the objects coincide
    as much as possible with the specified pairwise comparison matrix. We are talking
    about restoring the coordinates of the new reduced feature space with the accuracy
    of orthogonal transformation, ensuring the pairwise distances between the objects
    do not change.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the aim of MDS methods can also be formulated in order to display the
    configuration information of the original multidimensional data that’s given by
    the pairwise comparison matrix. This is provided as a configuration of points
    in the corresponding space of lower dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classical MDS assumes that the unknown coordinate matrix, ![](img/B19849_Formula_009.png),
    can be expressed by eigenvalue decomposition, ![](img/B19849_Formula_1211.png).
    ![](img/B19849_Formula_1221.png) can be computed from the proximity matrix ![](img/B19849_Formula_1231.png)
    (a matrix with distances between samples) by using double centering. The general
    MDS algorithm follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: It computes the squared proximity matrix, ![](img/B19849_Formula_1241.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It applies double centering, ![](img/B19849_Formula_125.png), using the centering
    matrix, ![](img/B19849_Formula_126.png), where ![](img/B19849_Formula_026.png)
    is the number of objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It determines the ![](img/B19849_Formula_128.png) largest eigenvalues, ![](img/B19849_Formula_129.png),
    and the corresponding eigenvectors, ![](img/B19849_Formula_130.png), of ![](img/B19849_Formula_1311.png)
    (where ![](img/B19849_Formula_1321.png) is the number of dimensions desired for
    the output).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It computes ![](img/B19849_Formula_133.png), where ![](img/B19849_Formula_134.png)
    is the matrix of ![](img/B19849_Formula_135.png) eigenvectors and ![](img/B19849_Formula_136.png)
    is the diagonal matrix of ![](img/B19849_Formula_137.png) eigenvalues of ![](img/B19849_Formula_138.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The disadvantage of the MDS method is that it does not take into account the
    distribution of nearby points since it uses Euclidean distances in calculations.
    If you ever find multidimensional data lying on a curved manifold, the distance
    between data points can be much more than Euclidean.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed the linear methods we can use for dimension-reduction,
    let’s look at what non-linear methods exist.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring non-linear methods for dimension-reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss the widespread non-linear methods and algorithms
    that are used for dimension-reduction, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sammon mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed **stochastic neighbor** **embedding** (**SNE**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classic PCA is a linear projection method that works well if the data is linearly
    separable. However, in the case of linearly non-separable data, a non-linear approach
    is required. The basic idea of working with linearly inseparable data is to project
    it into a space with a larger number of dimensions, where it becomes linearly
    separable. We can choose a non-linear mapping function, ![](img/B19849_Formula_139.png),
    so that the sample mapping, *x*, can be written as ![](img/B19849_Formula_140.png).
    This is called the **kernel function**. The term *kernel* describes a function
    that calculates the scalar product of mapping (in a higher-order space) samples
    *x* with ![](img/B19849_Formula_1411.png). This scalar product can be interpreted
    as the distance measured in the new space. In other words, the ![](img/B19849_Formula_1421.png)
    function maps the original *d*-dimensional elements into the *k*-dimensional feature
    space of a higher dimension by creating non-linear combinations of the original
    objects. For example, a function that displays 2D samples, ![](img/B19849_Formula_1431.png),
    in 3D space can look like ![](img/B19849_Formula_144.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a linear PCA approach, we are interested in the principal components that
    maximize the variance in the dataset. We can maximize variance by calculating
    the eigenvectors (principal components) that correspond to the largest eigenvalues
    based on the covariance matrix of our data and project our data onto these eigenvectors.
    This approach can be generalized to data that is mapped into a higher dimension
    space using the kernel function. However, in practice, the covariance matrix in
    a multidimensional space is not explicitly calculated since we can use a method
    called the **kernel trick**. The kernel trick allows us to project data onto the
    principal components without explicitly calculating the projections, which is
    much more efficient. The general approach is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the **kernel matrix** equal to ![](img/B19849_Formula_145.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make it so that it has a zero mean value, ![](img/B19849_Formula_146.png), where
    ![](img/B19849_Formula_147.png) is a matrix of *N x N* size with *1/N* elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvalues and eigenvectors of ![](img/B19849_Formula_148.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvectors in descending order, according to their eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take ![](img/B19849_Formula_149.png) eigenvectors that correspond to the largest
    eigenvalues, where ![](img/B19849_Formula_150.png) is the number of dimensions
    of a new feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These eigenvectors are projections of our data onto the corresponding main components.
    The main difficulty of this process is selecting the correct kernel and configuring
    its hyperparameters. Two frequently used kernels are the polynomial kernel ![](img/B19849_Formula_1511.png)and
    the Gaussian (Radial Basis Function (RBF)) ![](img/B19849_Formula_1521.png) ones.
  prefs: []
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Isomap** algorithm is based on the manifold projection technique. In mathematics,
    the **manifold** is a topological space (which is, in general, a set of points
    with their neighbors) that locally resembles the Euclidian space near each point.
    For example, one-dimensional manifolds include lines and circles but not figures
    with self-intersections. 2D manifolds are called **surfaces**; for example, they
    can be a sphere, a plane, or a torus, but these surfaces can’t have self-intersection.
    For example, a circle is a one-dimensional manifold embedded into a 2D space.
    Here, each arc of the circle locally resembles a straight-line segment. A 3D curve
    can also be a manifold if it can be divided into straight-line segments that can
    be embedded in 3D space without self-intersections. A 3D shape can be a manifold
    if its surface can be divided into flat plane patches without self-intersections.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of applying manifold projection techniques are to search for a manifold
    that is close to the data, project the data onto the manifold, and then unfold
    it. The most popular technique that’s used to find the manifold is to build a
    graph based on information about data points. Usually, these data points are placed
    into the graph nodes, and the edges simulate the relationships between the data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Isomap algorithm depends on two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of neighbors, ![](img/B19849_Formula_153.png), used to search for
    geodetic distances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimension of the final space, ![](img/B19849_Formula_1321.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In brief, the Isomap algorithm follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it constructs a graph representing geodesic distances. For each point,
    we search the ![](img/B19849_Formula_155.png) nearest neighbors and construct
    a weighted, undirected graph from the distances to these nearest neighbors. The
    edge weight is the Euclidean distance to the neighbor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using an algorithm to find the shortest distance in the graph, for example,
    Dijkstra’s algorithm, we need to find the shortest distance between each pair
    of vertices. We can consider this distance as a geodesic distance on a manifold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the matrix of pairwise geodesic distances we obtained in the previous
    step, train the MDS algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The MDS algorithm associates a set of points in the ![](img/B19849_Formula_027.png)-dimensional
    space with the initial set of distances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sammon mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sammon mapping** is one of the first non-linear dimensionality reduction
    algorithms. In contrast to traditional dimensionality reduction methods, such
    as PCA, Sammon mapping does not define a data conversion function directly. On
    the contrary, it only determines the measure of how well the conversion results
    (a specific dataset of a smaller dimension) correspond to the structure of the
    original dataset. In other words, it does not try to find the optimal transformation
    of the original data; instead, it searches for another dataset of lower dimensions
    with a structure that’s as close to the original one as possible. The algorithm
    can be described as follows. Let’s say we have ![](img/B19849_Formula_157.png)-dimensional
    vectors, ![](img/B19849_Formula_158.png). Here, ![](img/B19849_Formula_026.png)
    vectors are defined in the ![](img/B19849_Formula_160.png)-dimensional space,
    ![](img/B19849_Formula_1611.png), which is denoted by ![](img/B19849_Formula_1621.png).
    The distances between the vectors in the ![](img/B19849_Formula_163.png)-dimensional
    space will be denoted by ![](img/B19849_Formula_164.png) and in the ![](img/B19849_Formula_160.png)-dimensional
    space, ![](img/B19849_Formula_166.png). To determine the distance between the
    vectors, we can use any metric; in particular, the Euclidean distance. The goal
    of non-linear Sammon mapping is to search a selection of vectors, ![](img/B19849_Formula_19.png),
    in order to minimize the error function, ![](img/B19849_Formula_168.png), which
    is defined by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_169.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To minimize the error function, ![](img/B19849_Formula_168.png), Sammon used
    Newton’s minimization method, which can be simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_171.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *η* is the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed stochastic neighbor embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SNE problem is formulated as follows: we have a dataset with points described
    by a multidimensional variable with a dimension of space substantially higher
    than three. It is necessary to obtain a new variable that exists in a 2D or 3D
    space that would maximally preserve the structure and patterns in the original
    data. The difference between t-SNE and the classic SNE lies in the modifications
    that simplify the process of finding the global minima. The main modification
    is replacing the normal distribution with the Student’s t-distribution for low-dimensional
    data. SNE begins by converting the multidimensional Euclidean distance between
    points into conditional probabilities that reflect the similarity of points. Mathematically,
    it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_172.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This formula shows how close the point ![](img/B19849_Formula_173.png) lies
    to the point ![](img/B19849_Formula_312.png) with a Gaussian distribution around
    ![](img/B19849_Formula_312.png), with a given deviation of ![](img/B19849_Formula_176.png).
    ![](img/B19849_Formula_176.png) is different for each point. It is chosen so that
    the points in areas with higher density have less variance than others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s denote 2D or 3D mappings of the (![](img/B19849_Formula_312.png), ![](img/B19849_Formula_173.png))
    pair as the (![](img/B19849_Formula_111.png), ![](img/B19849_Formula_1811.png))
    pair. It is necessary to estimate the conditional probability using the same formula.
    The standard deviation is ![](img/B19849_Formula_1821.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_183.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If the mapping points, ![](img/B19849_Formula_111.png) and ![](img/B19849_Formula_1811.png),
    correctly simulate the similarity between the original points of the higher dimension,
    ![](img/B19849_Formula_312.png) and ![](img/B19849_Formula_187.png), then the
    corresponding conditional probabilities, ![](img/B19849_Formula_188.png) and ![](img/B19849_Formula_189.png),
    will be equivalent. As an obvious assessment of the quality of how ![](img/B19849_Formula_189.png)
    reflects ![](img/B19849_Formula_188.png), divergence, or the Kullback-Leibler
    distance is used. SNE minimizes the sum of such distances for all mapping points
    using gradient descent. The following formula determines the loss function for
    this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_192.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It has the following gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_193.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The authors of this problem proposed the following physical analogy for the
    optimization process. Let’s imagine that springs connect all the mapping points.
    The stiffness of the spring connecting points ![](img/B19849_Formula_194.png)
    and ![](img/B19849_Formula_195.png) depends on the difference between the similarity
    of two points in a multidimensional space and two points in a mapping space. In
    this analogy, the gradient is the resultant force that acts on a point in the
    mapping space. If we let the system go, after some time, it results in balance,
    and this is the desired distribution. Algorithmically, it searches for balance
    while taking the following moments into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_196.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B19849_Formula_197.png) is the learning rate and ![](img/B19849_Formula_198.png)
    is the coefficient of inertia. Classic SNE also allows us to get good results
    but can be associated with difficulties when optimizing the loss function and
    the crowding problem. t-SNE doesn’t solve these problems in general, but it makes
    them much more manageable.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function in t-SNE has two principal differences from the loss function
    of classic SNE. The first one is that it has a symmetric form of similarity in
    a multidimensional space and a simpler gradient version. Secondly, instead of
    using a Gaussian distribution for points from the mapping space, the t-distribution
    (Student) is used.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Autoencoders** represent a particular class of neural networks that are configured
    so that the output of the autoencoder is as close as possible to the input signal.
    In its most straightforward representation, the autoencoder can be modeled as
    a multilayer perceptron in which the number of neurons in the output layer is
    equal to the number of inputs. The following diagram shows that by choosing an
    intermediate hidden layer of a smaller dimension, we compress the source data
    into the lower dimension. Usually, values from this intermediate layer are a result
    of an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Autoencoder architecture](img/B19849_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Autoencoder architecture
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the linear and non-linear methods that can be
    used for dimension-reduction and explored the components of each of the methods
    in detail, we can enhance our implementation of dimension-reduction with the help
    of some practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dimension-reduction algorithms with various С++ libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s look at how to use dimensionality reduction algorithms in practice. All
    of these examples use the same dataset, which contains four normally distributed
    2D point sets that have been transformed with Swiss roll mapping, ![](img/B19849_Formula_199.png),
    into a 3D space. You can find the dataset and related details in the book''s GitHub
    repository here: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition).
    The following graph shows the result of this mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Swiss roll dataset](img/B19849_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Swiss roll dataset
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is labeled. Each of the normally distributed parts has its own
    labels, and we can see these labels as a certain color on the result. We use these
    colors to show transformation results for each of the algorithms we’ll be using
    in the following samples. This gives us an idea of how the algorithm works. The
    following sections provide concrete examples of how to use the `Dlib`, `Tapkee`,
    and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Dlib library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are three dimensionality reduction methods in the `Dlib` library—two linear
    ones, known as PCA and LDA, and one non-linear one, known as Sammon mapping.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PCA is one of the most popular dimensionality reduction algorithms and it has
    a couple of implementations in the `Dlib` library. There is the `Dlib::vector_normalizer_pca`
    type, for which objects can be used to perform PCA on user data. This implementation
    also normalizes the data. In some cases, this automatic normalization is useful
    because we always have to perform PCA on normalized data. An object of this type
    should be parameterized with the input data sample type. After we’ve instantiated
    an object of this type, we use the `train()` method to fit the model to our data.
    The `train()` method takes `std::vector` as samples and the `eps` value as parameters.
    The `eps` value controls how many dimensions should be preserved after the PCA
    has been transformed. This can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After the algorithm has been trained, we use the object to transform individual
    samples. Take a look at the first loop in the code and notice how the `pca([data[i]])`
    call performs this transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the result of the PCA transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Dlib PCA transformation visualization](img/B19849_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Dlib PCA transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: Data compression with PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use dimensionality reduction algorithms for a slightly different task—data
    compression with information loss. This can be easily demonstrated when applying
    the PCA algorithm to images. Let’s implement PCA from scratch with the `Dlib`
    library using SVD decomposition. We can’t use an existing implementation because
    it performs normalization in a way we can’t fully control.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to load an image and transform it into matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After we’ve loaded the RGB image, we convert it into grayscale and transform
    its values into floating points. The next step is to transform the image matrix
    into samples that we can use for PCA training. This can be done by splitting the
    image into rectangular patches that are 8 x 8 in size with the `Dlib::subm()`
    function and then flattening them with the `Dlib::reshape_to_column_vector()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When we have our samples, we can normalize them by subtracting the mean and
    dividing them by their standard deviation. We can make these operations vectorized
    by converting our vector of samples into the matrix type. We do this with the
    `Dlib::mat()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After we’ve prepared the data samples, we calculate the covariance matrix with
    the `Dlib::` **covariance()** function and perform SVD with the `Dlib::svd()`
    function. The SVD results are the eigenvalues matrix and the eigenvectors matrix.
    We sorted the eigenvectors according to the eigenvalues and left only a small
    number (in our case, 10 of them) of eigenvectors corresponding to the biggest
    eigenvalues. The number of eigenvectors we left is the number of dimensions in
    the new feature space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Our PCA transformation matrix is called `pca`. We used it to reduce the dimensions
    of each of our samples with simple matrix multiplication. Look at the following
    cycle and notice the `pca *` `data[i]` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data has been compressed and we can see its new size in the console output.
    Now, we can restore the original dimension of the data to be able to see the image.
    To do this, we need to use the transposed PCA matrix to multiply the reduced samples.
    Also, we need to denormalize the restored sample to get actual pixel values. This
    can be done by multiplying the standard deviation and adding the mean we got from
    the previous steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After we’ve restored the pixel values, we reshape them and place them in their
    original location in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the result of compressing a standard test image that is widely
    used in image processing. The following is the Lena 512 x 512 px image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Original image before compression](img/B19849_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Original image before compression
  prefs: []
  type: TYPE_NORMAL
- en: 'Its original grayscale size is 262,144 bytes. After we perform PCA compression
    with only 10 principal components, its size becomes 45,760 bytes. We can see the
    result in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Image after compression](img/B19849_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Image after compression
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that most of the essential visual information was preserved,
    despite the high compression rate.
  prefs: []
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Dlib` library also has an implementation of the LDA algorithm, which can
    be used for dimensionality reduction. It’s a supervised algorithm, so it needs
    labeled data. This algorithm is implemented with the `Dlib::compute_lda_transform()`
    function, which takes four parameters. The first one is the input/output parameter—as
    input, it is used to pass input training data (in matrix form) and as output,
    it receives the LDA transformation matrix. The second parameter is the output
    for the mean values. The third parameter is the labels for the input data, while
    the fourth one is the desired number of target dimensions. The following code
    shows an example of how to use LDA for dimension-reduction with the `Dlib` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform an actual LDA transform after the algorithm has been trained, we
    multiply our samples with the LDA matrix. In our case, we also transposed them.
    The following code shows the essential part of this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the result of using LDA reduction on two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The Dlib LDA transformation visualization](img/B19849_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – The Dlib LDA transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the following block, we will see how to use Sammon mapping dimensionality
    reduction algorithm implementation from the `Dlib` library.
  prefs: []
  type: TYPE_NORMAL
- en: Sammon mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the `Dlib` library, Sammon mapping is implemented with the `Dlib::sammon_projection`
    type. We need to create an instance of this type and then use it as a functional
    object. Functional object call arguments are the data that we need to transform
    and the number of dimensions of the new feature space. The input data should be
    in the form of the `std::vector` of the single samples of the `Dlib::matrix` type.
    All samples should have the same number of dimensions. The result of using this
    functional object is a new vector of samples with a reduced number of dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the result of using this dimensionality reduction
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The Dlib Sammon mapping transformation visualization](img/B19849_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – The Dlib Sammon mapping transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to use the Tapkee library for solving
    dimensionality reduction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Tapkee library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Tapkee library contains numerous dimensionality reduction algorithms, both
    linear and non-linear ones. This is the headers-only C++ template library so it
    doesn’t require any compilation, and can be easily integrated into your application.
    It has a couple of dependencies: the `fmt` library for the formatted output and
    the Eigen3 as the math backend.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are no special classes for algorithms in this library because it provides
    a uniform API based on parameters to build a dimensionality reduction object.
    So, using this approach, we can define a single function that will take a set
    of parameters and high-dimensional input data and perform dimensionality reduction.
    The result will be a 2D plot. The following code sample shows its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is the single function that we will use to see several algorithms from
    the Tapkee library. The main parameter it takes is `tapkee::ParametersSet`; an
    object of this type can be initialized with the dimensionality reduction method
    type, the number of target dimensions, and some special parameters that configure
    the selected method. The `with_kernel` parameter specifies if this function will
    attach the kernel transform to the algorithm pipeline or not. The library API
    allows you to attach the kernel transform for any algorithm but it will be used
    only if the algorithm implementation uses it. In our case, we will use it only
    for the `KernelPCA` method. The last parameters for the Reduction function are
    the input data, labels for plotting, and the output file name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look into the implementation. At first, we define the callback functors
    for a method pipeline. There are the Gaussian kernel callback, the linear distance
    callback, and the feature callback. Notice that all of these callback objects
    were initialized with the input data. It was done to reduce the data coping in
    the pipeline. The library API requires callback objects to be able to produce
    some result for two data indices. These callback objects should implement functionality
    to get access to the original data. So, all our callbacks were constructed from
    library-defined classes and store references to the original data containers.
    `tapkee::DenseMatrix` is just a typedef for the Eigen3 dense matrix. Another important
    thing is the index map usage for the data access, for example, the `indices` variable;
    it allows you to use your data more flexibly. The following snippet shows the
    dimensionality reduction object creation and application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the API is uniform and the builder functions that perform configurations
    start with the word `with`. At first, we passed the parameters to the build, then
    three callback functors, and finally called the `embedRange` method that does
    the actual dimensionality reduction. `eigen_features_callback` is needed to access
    particular data values with indices, and `eigen_distance_callback` is used in
    some algorithms to measure the distance between item vectors; in our case, it’s
    just the Euclidean distance, but you can define any you need.
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of the function, the `Clusters` object is populated with new
    2D coordinates and labels. Then, this object is used to plot the dimensionality
    reduction result. The plotting approach from the previous chapters was applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following subsections, we will learn how to use different dimensionality
    reduction methods with our general function. These methods will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-distributed SNEs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having the general function for the dimensionality reduction, we can use it
    for applying different methods. The following code shows how to create a parameter
    set to configure the PCA method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is the initialization of the `tapkee::ParametersSet` type
    object. We used the `tapkee::PCA` enumeration value and specified the number of
    target dimensions. Also, we didn’t use a kernel. `input_data` and `labels_data`
    are input data loaded from the file, and these variables have `tapkee::DenseMatrix`
    type, which actually is the Eigen3 dense matrix. The last parameter is the name
    of the output file for the plot image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the result of applying the Tapkee PCA implementation
    to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – The Tapkee PCA transformation visualization](img/B19849_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – The Tapkee PCA transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: You can see that this method was not able to spatially separate our 3D data
    in the 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The non-linear version of PCA is also implemented in the Tapkee library. To
    use this method, we define the kernel method and pass it as a callback to the
    `withKernel` builder method of library API. We already did it in our general function,
    so the only thing we have to do is pass the `with_kernel` parameter as `true`.
    We used the Gaussian kernel, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_200.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *ᵧ* is the sale coefficient that can be estimated as a median of the
    item difference, or just configured manually. The kernel callback function is
    defined in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Tapkee` requires that you define the method named `kernel` and the function
    operator. Our implementation is very simple; the main feature here is that the
    reference to the particular data is stored as a member. It’s done in a such way
    because the library will use only indices to call the kernel functor. The actual
    call to our general function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the result of applying the Tapkee kernel PCA implementation
    to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – The Tapkee kernel PCA transformation visualization](img/B19849_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – The Tapkee kernel PCA transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: We can see that this type of kernel makes some parts of the data separated,
    but that other ones were reduced too much.
  prefs: []
  type: TYPE_NORMAL
- en: MDS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To use the MDS algorithm, we should just pass the method name to our general
    dimensionality reduction function. There are no other configurable parameters,
    especially for this algorithm. The following example shows how to use this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the result of applying the Tapkee MDS algorithm to
    our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The Tapkee MDS transformation visualization](img/B19849_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – The Tapkee MDS transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the result is very similar to the PCA algorithm, and there
    is the same problem that the method was not able to spatially separate our data
    in the 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: Isomap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Isomap method can be applied to our data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the method name and target dimensions, the `num_neighbors` parameter
    was passed. This is the number of nearest neighbor values that will be used by
    the `Isomap` algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the result of applying the Tapkee Isomap implementation
    to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – The Tapkee Isomap transformation visualization](img/B19849_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – The Tapkee Isomap transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: You can see that this method can spatially separate our data in the 2D space,
    but the clusters are too close to each other. Also, you can play with the number
    of neighbor parameters to get another separation.
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The factor analysis method can be applied to our data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the method name and target dimensions, the `fa_epsilon` and `max_iteration`
    parameters can be passed. `fa_epsilon` is used to check the algorithm’s convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the result of applying the Tapkee factor analysis
    implementation to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – The Tapkee factor analysis transformation visualization](img/B19849_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – The Tapkee factor analysis transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: This method also fails to clearly separate our data in the 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The t-SNE method can be applied to our data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the method name and target dimensions, the `sne_perplexity` parameter
    was specified. This parameter regulates the algorithm convergence. Also, you can
    change the `sne_theta` value, which is the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph shows the result of applying the Tapkee t-SNE implementation
    to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – The Tapkee t-SNE transformation visualization](img/B19849_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – The Tapkee t-SNE transformation visualization
  prefs: []
  type: TYPE_NORMAL
- en: You can see that this method gave the most reasonable separation of our data
    in the 2D space; there are distinct bounds between all clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned that dimensionality reduction is the process of
    transferring data that has a higher dimension into a new representation of data
    with a lower dimension. It is used to reduce the number of correlated features
    in a dataset and extract the most informative features. Such a transformation
    can help increase the performance of other algorithms, reduce computational complexity,
    and make human-readable visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that there are two different approaches to solving this task. One
    is feature selection, which doesn’t create new features, while the second one
    is dimensionality reduction algorithms, which make new feature sets. We also learned
    that dimensionality reduction algorithms are linear and non-linear and that we
    should select either type, depending on our data. We saw that there are a lot
    of different algorithms with different properties and computational complexity
    and that it makes sense to try different ones to see which are the best solutions
    for particular tasks. Note that different libraries have different implementations
    for identical algorithms, so their results can differ, even for the same data.
  prefs: []
  type: TYPE_NORMAL
- en: The area of dimensionality reduction algorithms is a field that’s in continual
    development. There is, for example, a new algorithm called **Uniform Manifold
    Approximation and Projection** (**UMAP**) that’s based on Riemannian geometry
    and algebraic topology. It competes with the t-SNE algorithm in terms of visualization
    quality but also preserves more of the original data’s global structure after
    the transformation is complete. It is also much more computationally effective,
    which makes it suitable for large-scale datasets. However, at the moment, there
    is no C++ implementation of it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss classification tasks and how to solve them.
    Usually, when we have to solve a classification task, we have to divide a group
    of objects into several subgroups. Objects in such subgroups share some common
    properties that are distinct from the properties in other subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A survey of dimensionality reduction techniques: [https://arxiv.org/pdf/1403.2877.pdf](https://arxiv.org/pdf/1403.2877.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A short tutorial for dimensionality reduction: [https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf](https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guide to 12 dimensionality reduction techniques (with Python code): [https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A geometric and intuitive explanation of the covariance matrix and its relationship
    with linear transformation, an essential building block for understanding and
    using PCA and SVD: [https://datascienceplus.com/understanding-the-covariance-matrix](https://datascienceplus.com/understanding-the-covariance-matrix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The kernel trick: [https://dscm.quora.com/The-Kernel-Trick](https://dscm.quora.com/The-Kernel-Trick)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
