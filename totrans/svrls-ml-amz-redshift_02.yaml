- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Loading and Analytics on Redshift Serverless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced you to **Amazon Redshift Serverless**
    and demonstrated how to create a serverless endpoint from the Amazon Redshift
    console. We also explained how to connect and query your data warehouse using
    **Amazon Redshift query editor v** In this chapter, we will dive deeper into the
    different ways you can load data into your Amazon Redshift Serverless data warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover three main topics in this chapter to help you load your data efficiently
    into Redshift Serverless. First, we will demonstrate how to load data using Amazon
    Redshift query editor v where you will learn how to load data from your Amazon
    S3 bucket and local data file onto your computer using the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore the `COPY` command in detail, and you will learn how to
    load a file by writing a `COPY` command to load the data. We will cover everything
    you need to know to use this command effectively and load your data smoothly into
    Redshift Serverless.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will cover the built-in native API interface to access and load
    data into your Redshift Serverless endpoint using Jupyter Notebook. We will guide
    you through the process of setting up and using the **Redshift** **Data API**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data loading using Amazon Redshift query editor v
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data loading from Amazon S3 using the COPY command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data loading using the Redshift Data API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of this chapter is to equip you with the knowledge and skills to load
    data into Amazon Redshift Serverless using different mechanisms. By the end of
    this chapter, you will be able to load data quickly and efficiently into Redshift
    Serverless using the methods covered in this chapter, which will enable you to
    perform analytics on your data and extract valuable insights.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a web browser and access to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift Query Editor v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker for Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code snippets in this chapter are available in this book’s GitHub repository
    at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2).
  prefs: []
  type: TYPE_NORMAL
- en: 'The data files used in this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).'
  prefs: []
  type: TYPE_NORMAL
- en: Data loading using Amazon Redshift Query Editor v2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Query Editor v2 supports different database actions, including **data definition
    language** (**DDL**), to create schema and tables and load data from data files
    with just a click of a button. Let’s take a look at how you can carry out these
    tasks to enable easy analytics on your data warehouse. Log in to your AWS console,
    navigate to your Amazon Redshift Serverless endpoint, and select **Query data**.
    This will open **Redshift query editor v2** in a new tab. Using the steps we followed
    in [*Chapter 1*](B19071_01.xhtml#_idTextAnchor015), log in to your database and
    perform the tasks outlined in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Creating tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Query editor v2 provides a wizard to execute the DDL commands shown in *Figure
    2**.1*. Let’s create a new schema named `chapter2` first:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Create** and select **Schema**, as shown here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The creation wizard](img/B19071_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – The creation wizard
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that your `chapter2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click on **Create schema**, as shown in *Figure 2**.2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Create schema](img/B19071_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Create schema
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your schema created, navigate to the `chapter2` in the `customer`.
    With Query Editor v2, you can either enter the column names and their data type
    manually, or you can use the data file to automatically infer the column names
    and their data type.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a table with a data file. We will use `customer.csv`, which is
    available in this book’s GitHub repository at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).
    You can download this file locally to create the table using the wizard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file contains a subset of the data from the `TPC-H` dataset, available
    in this book''s GitHub repository: [https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Create table** wizard, click on **Load from CSV** under the **Columns**
    tab, and provide a path to the CSV file. Once the file is selected, the schema
    will be inferred and automatically populated from the file, as shown in *Figure
    2**.3*. Optionally, you can modify the schema in the **Column name**, **Data type**,
    and **Encoding** fields, and under **Column options**, you can select different
    options such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a default value for the column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, you can turn on **Automatically increment** if you want the column
    values to increment. If you enable this option, only then can you specify a value
    for **Auto increment seed** and **Auto** **increment step**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enter a size value for the column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You also have the option to define constraints such as **Not NULL**, **Primary
    key**, and **Unique key**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Create table](img/B19071_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Create table
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, as shown in *Figure 2**.4*, under the **Table details** tab, you
    can optionally set the table properties, such as **Distribution key**, **Distribution
    style**, **Sort key**, and **Sort type**. When these options are not set, Redshift
    will pick default settings for you, which are **Auto Distribution Key** and **Auto**
    **Sort Key**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Table details](img/B19071_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Table details
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift distributes data in a table according to the table’s distribution
    style (`DISTSTYLE`). The data rows are distributed within each compute node according
    to the number of slices. When you run a query against the table, all the slices
    of the compute node process the rows that are assigned in parallel. As a best
    practice ([https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html)),
    select a table’s `DISTSTYLE` parameter to ensure even distribution of the data
    or use automatic distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift orders data within each slice using the table’s sort key. Amazon
    Redshift also enables you to define a table with compound sort keys, interleaved
    sort keys, or no sort keys. As a best practice ([https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html)),
    define the sort keys and style according to your data access pattern. Having a
    proper sort key defined on a table can hugely improve your query performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, under **Other options** you can select the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to include your table in automated and manual snapshots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether to create a session-based temporary table instead of a permanent database
    table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have entered all the details, you can view the DDL of your table by
    clicking **Open query in editor**. You can use this later or even share it with
    other users.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create our table by clicking on the **Create table** button (*Figure
    2**.4*).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it is easy for any data scientist, analyst, or user to use this
    wizard to create database objects (such as tables) without having to write DDL
    and enter each column's data type and its length.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now work on loading data in the customer table. Query Editor v2 enables
    you to load data from Amazon S3 or the local file on your computer. Please note
    that, at the time of writing, the option to load a local file currently supports
    only CSV files with a maximum size of 5 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from Amazon S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query editor v2 enables you to load data from Amazon S3 buckets into an existing
    Redshift table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `COPY` command, which really makes it easier for a data analyst or data
    scientist, as they don’t have to remember the intricacies of the `COPY` command.
    You can load data from various file formats supported by the `COPY` command, such
    as CSV, JSON, Parquet, Avro, and Orc. Refer to this link for all the supported
    data formats: [https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at loading data using the `customer.csv`), which is stored in the
    following Amazon S3 location: s3://packt-serverless-ml-redshift/chapter02/customer.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you want to use your own Amazon S3 bucket to load the data, then
    download the data file from the GitHub location mentioned in the *Technical* *requirements*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download a data file from GitHub, navigate to your repository, select the
    file, right-click the **View raw** button at the top of the file, select **Save
    Link As…** (as shown in the following screenshot), choose the location on your
    computer where you want to save the file, and select **Save**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Saving the data file](img/B19071_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Saving the data file
  prefs: []
  type: TYPE_NORMAL
- en: On Query Editor v2, click on **Load data**, which opens up the data load wizard.
  prefs: []
  type: TYPE_NORMAL
- en: Under **Data source**, select the **Load from S3** radio button. You can browse
    the S3 bucket in your account to select the data file or a folder that you want
    to load, or you can select a manifest file. For this exercise, paste the aforementioned
    S3 file location.
  prefs: []
  type: TYPE_NORMAL
- en: If the data file is in a different region than your Amazon Redshift Serverless,
    you can select the source region from the S3 file location dropdown. The wizard
    provides different options if you want to load a Parquet file. Then, select an
    option from **File format**, or under **File** options, you can select **Delimiter**
    if your data is delimited by a different character. If your file is compressed,
    then you can select the appropriate compression from the dropdown, such as **gzip**,
    **lzop**, **zstd**, or **bzip2**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Advanced settings**, note that there are two options, **Data conversion
    parameters** and **Load operations**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the `TIMEFORMAT`) as `‘MM.DD.YYYY HH:MI:SS''`. Refer to this documentation
    link for a full list of parameters: [https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Under `COMPROWS`) as `1,000,000`. Refer to this documentation for a full list
    of options: [https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As our file contains the header row, please ensure that under **Advanced settings**
    | **Data conversion parameters** | **Frequently used parameters**, the **Ignore
    header rows (as 1)** option is checked.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 2**.6*, select the **Target table** parameters and **IAM
    role** to load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Load data](img/B19071_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Load data
  prefs: []
  type: TYPE_NORMAL
- en: Once you click on `COPY` command in the editor and start loading by running
    the `COPY` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have loaded our data, let’s quickly verify the load and check the
    data by querying the table, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Querying the data](img/B19071_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Querying the data
  prefs: []
  type: TYPE_NORMAL
- en: 'Query Editor v2 enables you to save your queries in the editor for later use.
    You can do so by clicking on the `COPY` command) in the future and, let’s say,
    the target table is the same but the data location on Amazon S3 is different,
    then you can easily modify this query and load the data quickly. Alternatively,
    you can even parameterize the query to pass, for example, an S3 location as `${s3_location}`,
    as shown in *Figure 2**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Saving the query](img/B19071_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – Saving the query
  prefs: []
  type: TYPE_NORMAL
- en: Sharing queries
  prefs: []
  type: TYPE_NORMAL
- en: 'With Query Editor v2, you can share your saved queries with your team. This
    way, many users can collaborate and share the same query. Internally, Query Editor
    manages the query versions, so you can track the changes as well. To learn more
    about this, refer to this AWS documentation: [https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered how Query Editor v2 enables users to easily create
    database objects and load data using the UI interface with a click of a few buttons,
    let us dive into Amazon Redshift’s `COPY` command to load the data into your data
    warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from a local drive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Query Editor v2 enables users to load data from a local file on their computer
    and perform analysis on it quickly. Often, database users such as data analysts
    or data scientists have data files on their local computer that they want to load
    quickly into a Redshift table, without moving the file into a remote location
    such as Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to load the data from a local file, Query Editor v2 requires a staging
    Amazon S3 bucket in your account. If it is not configured, then you will see an
    error similar to the one seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – An error message](img/B19071_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – An error message
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid the preceding error, users must do the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The account users must be configured with the proper permissions, as follows.
    Attach the following policy to your Redshift Serverless IAM role. Replace the
    resource names as highlighted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your administrator must configure the common Amazon S3 bucket in the **Account
    settings** window, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the settings icon (![](img/B19071_02_icon_1.png)) and select **Account
    settings**, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Account settings](img/B19071_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Account settings
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Account settings** window, under **General settings** | **S3 bucket**
    | **S3 URI**, enter the URI of the S3 bucket that will be used for staging during
    the local file load, and then click on **Save**. Ensure that your IAM role has
    permission to read and write on the S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Enter the URI of the S3 bucket under General settings](img/B19071_02_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Enter the URI of the S3 bucket under General settings
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to this documentation for complete information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a table and loading data from a local CSV file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s create a new table. Navigate to Query Editor v2 and create a supplier
    table using the following DDL command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the data into our supplier table from our data file (`supplier.csv`),
    which is stored in the following GitHub location: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv).'
  prefs: []
  type: TYPE_NORMAL
- en: To download the file on your local computer, right-click on **Raw** and click
    on **Save** **Link as**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to load data into the supplier table from Query Editor v2, click on
    `supplier.csv` file from your local drive. Under the **Target table** options,
    set **Schema** as **chapter2** and **Table** as **supplier**. Click on **Load
    data** to start the load:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – The Load data wizard](img/B19071_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – The Load data wizard
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the data is loaded successfully, you would see a message like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – The message after successfully loading the data](img/B19071_02_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – The message after successfully loading the data
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the data load by running the following SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to see 100 rows loaded from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Data load verification](img/B19071_02_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Data load verification
  prefs: []
  type: TYPE_NORMAL
- en: We have now successfully loaded our data from the Query Editor v2 `COPY` command
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data loading from Amazon S3 using the COPY command
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data warehouses are typically designed to ingest and store huge volumes of data,
    and one of the key aspects of any analytical process is to ingest such huge volumes
    in the most efficient way. Loading such huge data can take a long time as well
    as consume a lot of compute resources. As pointed out earlier, there are several
    ways to load data in your Redshift Serverless data warehouse, and one of the fastest
    and most scalable methods is the `COPY` command.
  prefs: []
  type: TYPE_NORMAL
- en: The `COPY` command loads your data in parallel from files, taking advantage
    of Redshift’s **massively parallel processing** (**MPP**) architecture. It can
    load data from Amazon S3, Amazon EMR, Amazon DynamoDB, or text files on remote
    hosts (SSH). It is the most efficient way to load a table in your Redshift data
    warehouse. With proper IAM policies, you can securely control who can access and
    load data in your database.
  prefs: []
  type: TYPE_NORMAL
- en: In the earlier section, we saw how Query Editor v2 generates the `COPY` command
    to load data from the wizard. In this section, we will dive deep and talk about
    how you can write the `COPY` command and load data from Amazon S3, and what some
    of the best practices are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the `COPY` command to load data into your Redshift data
    warehouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `COPY` command requires three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`table-name`: The target table name existing in the database (persistent or
    temporary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_source`: The data source location (such as the S3 bucket)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`authorization`: The authentication method (for example, the IAM role)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the `COPY` command source data format is expected to be in character-delimited
    UTF-8 text files, with a pipe character (`|`) as the default delimiter. If your
    source data is in another format, you can pass it as a parameter to specify the
    data format. Amazon Redshift supports different data formats, such as fixed-width
    text files, character-delimited files, CSV, JSON, Parquet, and Avro.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the `COPY` command provides optional parameters to handle data
    conversion such as the data format, `null`, and encoding. To get the latest details,
    refer to this AWS documentation: [https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax).'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from a Parquet file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the earlier section, we worked on loading a CSV file into the customer table
    in our database. For this exercise, let’s try to load a columnar data format file
    such as Parquet. We will be using a subset of `TPC-H` data, which may be found
    here: [https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The TPC is an organization focused on developing data benchmark standards.
    You may read more about TPC here: [https://www.tpc.org/default5.asp](https://www.tpc.org/default5.asp).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified data (`lineitem.parquet`) is available on GitHub: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data needed for the `COPY` command is available here: `s3://packt-serverless-ml-redshift/chapter02/lineitem.parquet`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This file contains approximately 6 million rows and is around 200 MB in size:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first start by creating a table named `lineitem` in the `chapter2` schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s load the data using the `COPY` command from the `lineitem.parquet`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have loaded our data, let’s quickly verify the load and check the
    data by querying the table, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15 – The query table](img/B19071_02_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – The query table
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how the `COPY` command helps load your data in
    different formats, such as CSV, Parquet, and JSON, from Amazon S3 buckets. Let’s
    see how you can automate the `COPY` command to load the data as soon as it is
    available in an Amazon S3 bucket. The next section on automating a `COPY` job
    is currently in public preview at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Automating file ingestion with a COPY job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your data warehouse, data is continuously ingested from Amazon S3\. Previously,
    you wrote custom code externally or locally to achieve this continuous ingestion
    of data with scheduling tools. With Amazon Redshift’s auto-copy feature, users
    can easily automate data ingestion from Amazon S3 to Amazon Redshift. To achieve
    this, you will write a simple SQL command to create a `COPY` job ([https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html)),
    and the `COPY` command will trigger automatically as and when it detects new files
    in the source Amazon S3 path. This will ensure that users have the latest data
    for processing available shortly after it lands in the S3 path, without having
    to build an external custom framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you can set up a `COPY` job, as shown here, or modify the existing
    `COPY` command by adding the `JOB` `CREATE` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: '`job-name` is the name of the job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO ON | OFF` indicates whether the data from Amazon S3 has loaded automatically
    into an Amazon Redshift table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the `COPY` job is an extension of the `COPY` command, and auto-ingestion
    of `COPY` jobs is enabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to run a `COPY` job, you can do so by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'For the latest details, refer to this AWS documentation: [https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html](https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for the COPY command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following best practices will help you get the most out of the `COPY` command:'
  prefs: []
  type: TYPE_NORMAL
- en: Make the most of parallel processing by splitting data into multiple compressed
    files or by defining distribution keys on your target tables, as we did in our
    example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a single `COPY` command to load data from multiple files. If you use multiple
    concurrent `COPY` commands to load the same target table from multiple files,
    then the load is done serially, which is much slower than a single `COPY` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your data file contains an uneven or mismatched number of fields, then provide
    the list of columns as comma-separated values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When you want to load a single target table from multiple data files and your
    data files have a similar structure but different naming conventions, or are in
    different folders in an Amazon S3 bucket, then use a manifest file. You can supply
    the full path of the files to be loaded in a JSON-formatted text file. The following
    is the syntax to use a manifest file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For a `COPY` job, use unique filenames for each file that you want to load.
    If a file is already processed and any changes are done after that, then the `COPY`
    job will not process the file, so remember to rename the updated file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have seen two approaches to data loading in your Amazon Redshift
    data warehouse – using the Query Editor v2 wizard and writing an individual `COPY`
    command to trigger ad hoc data loading. Let us now look into how you can use an
    AWS SDK to load data using the Redshift Data API.
  prefs: []
  type: TYPE_NORMAL
- en: Data loading using the Redshift Data API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Amazon Redshift Data API is a built-in native API interface to access your
    Amazon Redshift database without configuring any **Java Database Connectivity**
    (**JDBC**) or **Open Database Connectivity** (**ODBC**) drivers. You can ingest
    or query data with a simple API endpoint without managing a persistent connection.
    The Data API provides a secure way to access your database by using either IAM
    temporary credentials or AWS Secrets Manager. It provides a secure HTTP endpoint
    to run SQL statements asynchronously, meaning you can retrieve your results later.
    By default, your query results are stored for 24 hours. The Redshift Data API
    integrates seamlessly with different AWS SDKs, such as Python, Go, Java, Node.js,
    PHP, Ruby, and C++. You can also integrate the API with AWS Glue for an ETL data
    pipeline or use it with AWS Lambda to invoke different SQL statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many use cases where you can utilize the Redshift Data API, such
    as ETL orchestration with AWS Step Functions, web service-based applications,
    event-driven applications, and accessing your Amazon Redshift database using Jupyter
    notebooks. If you want to just run an individual SQL statement, then you can use
    the **AWS Command-Line Interface** (**AWS CLI**) or any programming language.
    The following is an example of executing a single SQL statement in Amazon Redshift
    Serverless from the AWS CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, for Redshift Serverless, you only need to provide the workgroup
    name and database name. Temporary user credentials are pulled from IAM authorization.
    For Redshift Serverless, add the following permission in the IAM policy attached
    to your cluster IAM role to access the Redshift Data API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: In order to showcase how you can ingest data using the Redshift Data API, we
    will carry out the following steps using Jupyter Notebook. Let’s create a notebook
    instance in our AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: On the console home page, search for `Amazon SageMaker`. Click on the hamburger
    icon (![](img/B19071_02_icon_2.png)) in the top-left corner, then **Notebook**,
    and then **Notebook instances**. Click on **Create notebook instance** and provide
    the necessary input. Once the notebook instance is in service, click on **Open
    Jupyter**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a created notebook instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Creating a notebook instance](img/B19071_02_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Creating a notebook instance
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jupyter notebook for this exercise is available at this GitHub location:
    [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb).
    Download this notebook to your local machine and save it in a folder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data (`orders.parquet`) for this exercise is available on GitHub at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2),
    as well as this Amazon S3 location: `s3://packt-serverless-ml-redshift/chapter2/orders.parquet`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a subset of the `orders` data, which is referenced from the `TPC-H`
    dataset available here: [https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first open the downloaded notebook (`Chapter2.ipynb`) by following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the Jupyter Notebook landing page, click on **Upload** and open the previously
    downloaded notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the kernel (`conda_python3`) once the notebook is uploaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Redshift Serverless requires your `boto3` version to be greater than version
    1.24.32.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check our `boto3` library version, as shown in *Figure 2**.17*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Checking the boto3 version](img/B19071_02_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Checking the boto3 version
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to install a specific version greater than 1.24.32, then check
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Creating table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you can see in the `chapter2.ipynb` notebook, we have provided step-by-step
    instructions to connect to your Redshift Serverless endpoint and perform the necessary
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by setting up the parameters and importing the necessary libraries
    for this exercise. We will set the following two parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`REDSHIFT_WORKGROUP`: The name of the Redshift Serverless workgroup'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`S3_DATA_FILE`: The source data file for the load:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember to set the parameters as per your settings in the Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create the table, let’s first prepare our DDL and assign it to
    a `table_ddl` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the `boto3` library, we will connect to the Redshift Serverless workgroup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are different methods that are available to execute different operations
    on your Redshift Serverless endpoint. Check out the entire list in this documentation:
    [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `execute_statement` method to run an SQL statement, which can
    be in the `BatchExecuteStatement`. To get a complete list of different methods
    and how to use them, please refer to this AWS documentation: [https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html](https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see from the preceding code block, we will first set the client as
    `redshift-data` and then call `execute_statement` to connect the Serverless endpoint,
    using the `Database` name and `WorkgroupName`. The method uses temporary credentials
    to connect to your Serverless workgroup.
  prefs: []
  type: TYPE_NORMAL
- en: We will also pass `table_ddl` as a parameter to create the table. We will create
    the `Orders` table in our `chapter2` schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Redshift Data API sends back a response element once the action is successful,
    in a JSON format as a dictionary object. One of the response elements is a SQL
    statement identifier. This value is universally unique and generated by the Amazon
    Redshift Data API. As you can see in the following code, we have captured the
    response element, `Id`, from the output object, `res`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In order to make sure that your query is completed, you can use the `describe_statement`
    method and pass your `id` statement as a parameter. This method sends out the
    response, which contains information that includes when the query started, when
    it finished, the query status, the number of rows returned, and the SQL statement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Checking the query status](img/B19071_02_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Checking the query status
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 2**.18*, we have captured the status of the statement
    that we ran, and it sends out the status as `FINISHED`. This means that we have
    created our table in the database, and you can verify this by writing a simple
    `SELECT` statement against the table.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data using the Redshift Data API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s move forward to load data into this newly created table. You can
    use the S3 location for the source data, as mentioned previously. If you use a
    different S3 location, then remember to replace the path in the parameter (`S3_DATA_FILE`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write a `COPY` command, as shown in the following code block. We will
    create the `COPY` command in the `load_data` variable, using the S3 path as a
    parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will use the `execute_statement` method to run this `COPY` command
    and capture the `id` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Be sure to check whether the status of the query is `FINISHED`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the statement status is defined as `FINISHED`, we will verify our data
    load by running a count query, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now print the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Count query results](img/B19071_02_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Count query results
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 2**.19*, we have successfully loaded 1.5 million rows.
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook, we have provided a combined code block to show how you can
    convert all these steps into a function, calling it as and when you require it
    to load data into a new table.
  prefs: []
  type: TYPE_NORMAL
- en: We also have a GitHub repository ([https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/](https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/)),
    which showcases how to get started with the Amazon Redshift Data API in different
    languages, such as Go, Java, JavaScript, Python, and TypeScript. You can go through
    the step-by-step process explained in the repository to build your custom application
    in all these languages, using the Redshift Data API.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showcased how you can load data into your Amazon Redshift
    Serverless database using three different tools and methods, by using the query
    editor v GUI interface, the Redshift `COPY` command to load the data, and the
    Redshift Data API using Python in a Jupyter notebook. All three methods are efficient
    and easy to use for your different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We also talked about some of the best practices for the `COPY` command to make
    efficient use of it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start with our first topic concerning Amazon Redshift
    machine learning, and you will see how you can leverage it in your Amazon Redshift
    Serverless data warehouse.
  prefs: []
  type: TYPE_NORMAL
