<html><head></head><body>
		<h1 id="_idParaDest-51" class="chapter-number"><a id="_idTextAnchor051"/>4</h1>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Key Machine Learning Algorithms and Concepts</h1>
			<p>In the previous chapters, we explored the different concepts of MSA and the role it plays when creating <span class="No-Break">enterprise systems.</span></p>
			<p>In the coming chapters, we will begin to shift our focus from learning about MSA concepts to learning about key machine learning concepts. We will also learn about the different libraries and packages being used in machine learning models <span class="No-Break">using Python.</span></p>
			<p>We will cover the following areas in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>The differences between <strong class="bold">artificial intelligence</strong>, <strong class="bold">machine learning</strong>, and <span class="No-Break"><strong class="bold">deep learning</strong></span></li>
				<li>Common deep learning packages and libraries used <span class="No-Break">in Python</span></li>
				<li>Building <span class="No-Break"><strong class="bold">regression</strong></span><span class="No-Break"> models</span></li>
				<li>Building <span class="No-Break">multiclass </span><span class="No-Break"><strong class="bold">classification</strong></span></li>
				<li>Text sentiment analysis and <span class="No-Break">topic modeling</span></li>
				<li>Pattern analysis and forecasting using <span class="No-Break">machine learning</span></li>
				<li>Building enhanced models using <span class="No-Break">deep learning</span></li>
			</ul>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>The differences between artificial intelligence, machine learning, and deep learning</h1>
			<p>Despite the recent<a id="_idIndexMarker153"/> rise in popularity<a id="_idIndexMarker154"/> of artificial<a id="_idIndexMarker155"/> intelligence and<a id="_idIndexMarker156"/> machine learning, the field<a id="_idIndexMarker157"/> of artificial intelligence<a id="_idIndexMarker158"/> has been around since the 1960s. With different sub-fields emerging, it is important to be able to differentiate between them and understand them and what <span class="No-Break">they entail.</span></p>
			<p>To start, artificial intelligence is the overarching field that encompasses all the sub-fields we see today, such as machine learning, deep learning, and more. Any system that perceives or receives information from its environment and carries out an action to maximize the reward or achieve its goal is considered to be an artificially <span class="No-Break">intelligent machine.</span></p>
			<p>This is commonly used today when it comes to robotics. Most of our machines are designed so that they can capture data using their sensors, such as cameras, sonars, or gyroscopes, and use the data captured to respond to a particular task most efficiently. This concept is very similar to how humans function. We use our senses to “capture” information from our environment and based on the information we receive, we carry out <span class="No-Break">certain actions.</span></p>
			<p>Artificial intelligence is an expansive field, but it can be broken into different sub-fields, one we commonly know today as machine learning. What makes machine learning unique is that this field works on creating systems or machines that can continually learn and improve their model without explicitly <span class="No-Break">being programmed.</span></p>
			<p>Machine learning does this by<a id="_idIndexMarker159"/> collecting data, also known as training data, and trying to find patterns in the data to make accurate predictions without being programmed to do so. There are many different methods used in machine learning to learn the data and the methods are tailored to the different problems <span class="No-Break">we encounter.</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18934_04_1.jpg" alt="Figure 4.1: Different fields in artificial intelligence"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1: Different fields in artificial intelligence</p>
			<p>Machine learning problems can be broken down into three different tasks: <strong class="bold">supervised learning</strong>, <strong class="bold">unsupervised learning</strong>, and <strong class="bold">reinforcement learning</strong>. For now, we will focus on supervised<a id="_idIndexMarker160"/> and unsupervised learning. This distinction<a id="_idIndexMarker161"/> is based on the training data that we have. Supervised learning<a id="_idIndexMarker162"/> is when we have<a id="_idIndexMarker163"/> the input data<a id="_idIndexMarker164"/> and the expected output for the particular<a id="_idIndexMarker165"/> set of data, which<a id="_idIndexMarker166"/> is also called the label. Unsupervised learning, on the other hand, only consists of the input without an <span class="No-Break">expected output.</span></p>
			<p>Supervised learning works by understanding the relationship between the input and output data. One common example of supervised learning is predicting the price of a home in a certain city. We can collect data on existing homes by capturing their specifications and their current prices and then learn the pattern between the characteristics of these homes and their prices. We can then take a home, not in our training set, and test our model by inputting the features of the house into our program and have the model predict the price of <span class="No-Break">the home.</span></p>
			<p>Unsupervised learning works by learning about the structure of the data either using grouping or clustering methods. This method is commonly used for marketing purposes. For example, a store wants to cluster its customers into different groups so that it can efficiently tailor its products to different demographics. It can capture the purchase history of its customers, use that data to learn about purchasing patterns, and suggest certain items or goods that would interest them, thus maximizing <span class="No-Break">its revenue.</span></p>
			<p>Before we<a id="_idIndexMarker167"/> can understand<a id="_idIndexMarker168"/> deep learning, which is a sub-field<a id="_idIndexMarker169"/> of machine learning, we must first understand what <strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>) are. Taking inspiration from neurons in a brain, ANNs are models that comprise a network<a id="_idIndexMarker170"/> of fully connected nodes, also known as artificial neurons. They contain<a id="_idIndexMarker171"/> a set of inputs, hidden layers<a id="_idIndexMarker172"/> connecting the neurons, and also<a id="_idIndexMarker173"/> an output node. Each neuron has an input<a id="_idIndexMarker174"/> and output, which can be propagated throughout the network. In order to calculate the output of a neuron, we take the weighted sum of all the inputs, multiply it by the weight of the neuron, and then usually add a <span class="No-Break">bias term.</span></p>
			<p>We continue to perform these actions until we reach the last layer, which is the output neuron. We perform a nonlinear activation function, such as a sigmoid function, to give us the final<a id="_idIndexMarker175"/> prediction. We then take the predicted output value and input it in a <strong class="bold">cost function</strong>. This function tells us how well our network is learning. We take this value and backpropagate through our layers back to the first layer, adjusting the weights of the neurons depending on how our network is performing. With this, we can create strong models that can perform tasks such as handwriting recognition, game-playing AI, and <span class="No-Break">much more.</span></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">A program is considered to be a machine learning model if it can take input data and learn the patterns to make predictions without being explicitly <span class="No-Break">programmed to.</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B18934_04_2.jpg" alt="Figure 4.2: An ANN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2: An ANN</p>
			<p>While ANNs<a id="_idIndexMarker176"/> are capable<a id="_idIndexMarker177"/> of performing<a id="_idIndexMarker178"/> many tasks, there<a id="_idIndexMarker179"/> are significant<a id="_idIndexMarker180"/> downsides<a id="_idIndexMarker181"/> that limit<a id="_idIndexMarker182"/> their use<a id="_idIndexMarker183"/> in <span class="No-Break">today’s market:</span></p>
			<ul>
				<li>It can be difficult to understand how the model performs. As you add more hidden layers to the network, it becomes complicated to try and debug <span class="No-Break">the network.</span></li>
				<li>Training the model takes a long time, especially with copious amounts of training data, and can drain hardware resources, as it is difficult to perform all these mathematical operations on <span class="No-Break">a CPU.</span></li>
				<li>The biggest issue with ANNs is overfitting. As we add more hidden layers, there is a point at which the weights assigned to the neurons will be heavily tailored to our training data. This makes our network perform very poorly when we try to test it with data it has not <span class="No-Break">seen before.</span></li>
			</ul>
			<p>This is where deep learning comes into play. Deep learning<a id="_idIndexMarker184"/> can be categorized by these <span class="No-Break">key features:</span></p>
			<ul>
				<li><strong class="bold">The hierarchical composition of layers</strong>: Rather than having only fully connected layers in a network, we can create and combine multiple different layers, consisting of non-linear and linear transformations. These different layers play a role in extracting key features in the data that would be otherwise difficult to find in <span class="No-Break">an ANN.</span></li>
				<li><strong class="bold">End-to-end learning</strong>: The network starts with a method called feature extraction. It looks at the data<a id="_idIndexMarker185"/> and finds a way to group redundant information and identify the important features of the data. The network then uses these features to train and predict or classify using fully <span class="No-Break">connected layers.</span></li>
				<li><strong class="bold">A distributed representation of neurons</strong>: With feature extraction, the network can group neurons to encode a bigger feature of the data. Unlike in an ANN, no single neuron encodes everything. This allows the model to reduce the number of parameters it has to learn while still retaining the key elements in <span class="No-Break">the data.</span></li>
			</ul>
			<p>Deep learning is prevalent<a id="_idIndexMarker186"/> in computer vision. Due<a id="_idIndexMarker187"/> to the advances in the technology of capturing<a id="_idIndexMarker188"/> photos and videos, it has become very difficult<a id="_idIndexMarker189"/> for ANNs to learn<a id="_idIndexMarker190"/> and perform well when it comes<a id="_idIndexMarker191"/> to image detection. For starters, when we use an image to train our model, we have to look at every pixel in an image as an input to the model. So, for an image of resolution 256x256, we would be looking at over 65,000 input parameters. Depending on the number of neurons in your fully connected layer, you could be looking at millions of parameters. With the sheer number of parameters, this will be bound to cause overfitting and could take days <span class="No-Break">of training.</span></p>
			<p>With deep learning, we can create a group of layers called <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>). These layers are responsible for reducing<a id="_idIndexMarker192"/> the number of <a id="_idIndexMarker193"/>parameters that we have to learn in our model while still retaining the key features<a id="_idIndexMarker194"/> in our data. With these additions, we can learn<a id="_idIndexMarker195"/> how to extract certain features and use<a id="_idIndexMarker196"/> those to train<a id="_idIndexMarker197"/> our model to predict<a id="_idIndexMarker198"/> with efficiency<a id="_idIndexMarker199"/> <span class="No-Break">and accuracy.</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B18934_04_3.jpg" alt="Figure 4.3: A CNN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3: A CNN</p>
			<p>In the next section, we will be looking at the different Python libraries used for machine learning and deep learning and their different <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Common deep learning and machine learning libraries used in Python</h1>
			<p>Now that we have gone<a id="_idIndexMarker200"/> over the concepts<a id="_idIndexMarker201"/> of artificial intelligence<a id="_idIndexMarker202"/> and machine<a id="_idIndexMarker203"/> learning, we can start looking at the programming aspect of implementing these concepts. Many programming languages are used today when it comes to creating machine learning models. Commonly used are MATLAB, R, and Python. Among them, Python has grown to be the most popular programming language in machine learning due to its versatility as a programming language and the extensive number of libraries, which makes creating machine learning models easier. In this section, we will be going over the most commonly used <span class="No-Break">libraries today.</span></p>
			<h3>NumPy</h3>
			<p>NumPy is an essential package<a id="_idIndexMarker204"/> when it comes to building machine learning models in Python. You will be mostly working with large, multi-dimensional matrices when building your models. Most of the effort is spent on transforming, splicing, and performing advanced mathematical operations on matrices, and NumPy provides the tools need to perform these actions while retaining speed <span class="No-Break">and efficiency.</span></p>
			<p>For more information on the different APIs<a id="_idIndexMarker205"/> that NumPy offers, you can visit the documentation on its <span class="No-Break">website: </span><span class="No-Break">https://numpy.org/doc/stable/reference/index.html</span><span class="No-Break">.</span></p>
			<p>Here, we will look at the example<a id="_idIndexMarker206"/> code. This section shows us how we can initialize a NumPy array. In this example, we will create a 3x3 matrix with initialized values of 1 <span class="No-Break">through 9:</span></p>
			<pre class="console">import numpy as np
# creates a 3x3 numpy array
arr = np.array([[1,2,3],[4,5,6],[7, 8, 9]])</pre>
			<p>Here, we will print out <span class="No-Break">the results:</span></p>
			<pre class="console">print(arr)
[[1 2 3]
 [4 5 6]
 [7 8 9]]</pre>
			<p>Now, we can show how we can splice and extract certain elements from <span class="No-Break">our array.</span></p>
			<p>This line of code allows us to pull all the values that are in the second column of our array. Keep in mind that in NumPy our arrays and lists are zero-indexed, meaning that the zero index refers to the first element in the array <span class="No-Break">or list:</span></p>
			<pre class="console">print(arr[:,1]) # print the second column of the array
[2 5 8]</pre>
			<p>In this example, we extract all of the values in the row of <strong class="source-inline">2</strong> in <span class="No-Break">our array:</span></p>
			<pre class="console">print(arr[2,:]) # print the last row of the array
[7 8 9]</pre>
			<p>Another useful aspect of NumPy arrays is that we can apply mathematical functions to our matrices without having to implement code to perform basic functions. Not only is this much easier but it also is much faster and <span class="No-Break">more efficient.</span></p>
			<p>In this example, we simply perform<a id="_idIndexMarker207"/> a multiplication between our matrix and a scalar value <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">-1</strong></span><span class="No-Break">:</span></p>
			<pre class="console">print(np.multiply(arr, -1)) # multiplies every element in the array by -1
[[-1 -2 -3]
 [-4 -5 -6]
 [-7 -8 -9]]</pre>
			<h3>Matplotlib</h3>
			<p>In order to see how your model<a id="_idIndexMarker208"/> is learning and performing, it is important to be able to visualize your results and your data. Matplotlib offers a simple way to graph your data, from something as simple as a line plot to more advanced plots, such as contour plots and 3D plots. What makes this library so popular is its seamlessness when working <span class="No-Break">with NumPy.</span></p>
			<p>For more information on their different<a id="_idIndexMarker209"/> functions, you can visit their <span class="No-Break">website: </span><a href="https://matplotlib.org/stable/index.html"><span class="No-Break">https://matplotlib.org/stable/index.html</span></a><span class="No-Break">.</span></p>
			<p>In this example, we will create a simple line graph. We first initialize two arrays, <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong>, and both arrays will contain values from 0 to 9. Then, using Matplotlib’s APIs, we can plot and show our <span class="No-Break">simple graph:</span></p>
			<pre class="console">import matplotlib.pyplot as plt
import numpy as np
x = np.arange(10) # creates an array from 0-9
y = np.arange(10)
plt.plot(x,y)
plt.show()</pre>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B18934_04_4.jpg" alt="Figure 4.4: A simple line graph using Matplotlib"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4: A simple line graph using Matplotlib</p>
			<h3>Pandas</h3>
			<p>With the recent trend of storing data<a id="_idIndexMarker210"/> in CSV files, Pandas has become a staple in the Python community due to its ease and versatility. Pandas is commonly used for data analysis. It stores the data in a tabular format, and it provides users with simple functions to pre-process and manipulate the data to fit their needs. It has also become useful when dealing with time-series data, which is helpful when building <span class="No-Break">forecasting models.</span></p>
			<p>For more information on the different<a id="_idIndexMarker211"/> functions, you can view the documentation on its <span class="No-Break">website: </span><a href="https://pandas.pydata.org/docs/"><span class="No-Break">https://pandas.pydata.org/docs/</span></a><span class="No-Break">.</span></p>
			<p>In this example, first, we will simply initialize a DataFrame. This is the data structure used to store our data in a two-dimensional tabular format in Pandas. Usually, we store the data from the files we read from, but it is also possible to create a DataFrame with your <span class="No-Break">own data:</span></p>
			<pre class="console">import pandas as pd
data = {
    "Number of Bedrooms": [5, 4, 2, 3],
    "Year Build": [2019, 2017, 2010, 2015],
    "Size(Sq ft.)": [14560, 12487, 9882, 10110],
    "Has Garage": ["Yes", "Yes", "No", "Yes"],
    "Price": [305000, 275600, 175000, 235000],
}
df = pd.DataFrame(data)
print(df)</pre>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B18934_04_5.jpg" alt="Figure 4.5: Output of our DataFrame"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5: Output of our DataFrame</p>
			<p>As with NumPy, we can extract<a id="_idIndexMarker212"/> certain columns and rows of our DataFrame. In this code, we can view the first row of <span class="No-Break">our DataFrame:</span></p>
			<pre class="console">print(df.iloc[0]) # view the first entry in the table</pre>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B18934_04_6.jpg" alt="Figure 4.6: Output of the first row of our DataFrame"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6: Output of the first row of our DataFrame</p>
			<p>With Pandas, we can also extract certain columns from our DataFrame by using the name of the column rather than <span class="No-Break">the index:</span></p>
			<pre class="console">print(df["Price"]) # print all the values in the Prices column</pre>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18934_04_7.jpg" alt="Figure 4.7: Output of all the values in the Price column"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7: Output of all the values in the Price column</p>
			<h3>TensorFlow and Keras</h3>
			<p>TensorFlow and Keras are the foundation<a id="_idIndexMarker213"/> when it comes<a id="_idIndexMarker214"/> to building deep learning models. While both can be used individually, Keras is used as an interface for the TensorFlow framework, allowing users to easily create powerful deep <span class="No-Break">learning models.</span></p>
			<p>TensorFlow, created by Google, functions as the backend when creating machine learning models. It works by creating static data flow graphs that specify how the data moves through the deep learning pipeline. The graph contains nodes and edges, where the nodes represent mathematical operations. It passes<a id="_idIndexMarker215"/> this data using multidimensional arrays known <span class="No-Break">as Tensors.</span></p>
			<p>Keras, later to be integrated with TensorFlow, can be viewed as the frontend for designing deep learning models. It was implemented to be user-friendly by allowing users to focus on designing their neural network models without having to deal with a complicated backend. It is similar to object-oriented programming, as it replicates the style of creating objects. Users can freely add different types of layers, activation functions, and more. They can even use prebuilt neural networks for easy training <span class="No-Break">and testing.</span></p>
			<p>In the following example code, we can see how we can create a simple, hidden two-layer neural network. This block of code allows us to initialize a <strong class="source-inline">Sequential</strong> model, which consists of a simple stack <span class="No-Break">of layers:</span></p>
			<pre class="console">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
model = Sequential()
model.add(Flatten(input_shape=[256,256]))</pre>
			<p>Depending on our application, we can add multiple layers with different configurations, such as the number of nodes, the activation functions, and the <span class="No-Break">kernel regularizer:</span></p>
			<pre class="console"> #Adding First Hidden Layer
model.add(tf.keras.layers.Dense(units=6,kernel_regularizer='l2',activation="leaky_relu"))
 #Adding Second Hidden Layer
model.add(tf.keras.layers.Dense(units=1,kernel_regularizer='l2',activation="leaky_relu"))
#Adding Output Layer
model.add(tf.keras.layers.Dense(units=1,kernel_regularizer='l2',activation="sigmoid"))</pre>
			<p>Finally, we can compile our model, which essentially gathers all the different layers and combines<a id="_idIndexMarker216"/> them into one simple <span class="No-Break">neural</span><span class="No-Break"><a id="_idIndexMarker217"/></span><span class="No-Break"> network:</span></p>
			<pre class="console">#Compiling ANN
model.compile(optimizer='sgd',loss="binary_crossentropy",metrics=['accuracy'])</pre>
			<h3>PyTorch</h3>
			<p>PyTorch is another machine learning framework<a id="_idIndexMarker218"/> created by Meta, formally known as Facebook. Much like Keras/TensorFlow, it allows the users to create machine learning<a id="_idIndexMarker219"/> models. The framework is well suited to <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) and computer vision problems but can be tailored to most applications. What makes PyTorch unique is its dynamic computational graph. It has a module called Autograd, which allows you to perform<a id="_idIndexMarker220"/> automatic differentiation dynamically, compared to TensorFlow, in which it is static. Also, PyTorch is more in line with the Python language, which makes it easier to understand and takes advantage of useful features of Python such as parallel programming. For more<a id="_idIndexMarker221"/> information, visit the documentation on their <span class="No-Break">website: </span><a href="https://pytorch.org/docs/stable/index.html"><span class="No-Break">https://pytorch.org/docs/stable/index.html</span></a><span class="No-Break">.</span></p>
			<p>In this section of code, we can create a simple single-layer neural network. Similar to Keras, we can initialize a <strong class="source-inline">Sequential</strong> model and add layers<a id="_idIndexMarker222"/> depending on <span class="No-Break">our needs:</span></p>
			<pre class="console">import torch
model = torch.nn.Sequential( # create a single layer Neural Network
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)
loss = torch.nn.MSELoss(reduction='sum')</pre>
			<h3>SciPy</h3>
			<p>This library is designed for scientific<a id="_idIndexMarker223"/> computing. There are many built-in functions and methods used for linear algebra, optimization, and integration, which are commonly used in machine learning. This library is useful when trying to compute certain statistics and transformations as you build your machine learning model. For more information on the different<a id="_idIndexMarker224"/> functions it provides, view the documentation on its <span class="No-Break">website: </span><a href="https://docs.scipy.org/doc/scipy/"><span class="No-Break">https://docs.scipy.org/doc/scipy/</span></a><span class="No-Break">.</span></p>
			<p>In this example code, we can create a 3x3 array using NumPy and then we can use SciPy to calculate <span class="No-Break">the determinate:</span></p>
			<pre class="console">import numpy as np
from scipy import linalg
a = np.array([[1,4,2], [3,9,7], [8,5,6]])
print(linalg.det(a)) # calculate the matrix determinate
57.0</pre>
			<h3>scikit-learn</h3>
			<p>scikit-learn is a machine learning library<a id="_idIndexMarker225"/> that is an extension of SciPy and is built using NumPy and Matplotlib. It contains many prebuilt machine learning models, such as random forests, K-means, and support vector machines. For more information on the different<a id="_idIndexMarker226"/> APIs it provides, view the documentation by visiting its <span class="No-Break">website: </span><a href="https://scikit-learn.org/stable/user_guide.html"><span class="No-Break">https://scikit-learn.org/stable/user_guide.html</span></a><span class="No-Break">.</span></p>
			<p>In the following example, we will use an example dataset provided by scikit-learn and build a simple logistic regression model. First, we import all the required libraries and then load the Iris dataset provided by scikit-learn. We can use a handy API from scikit-learn to split our data into training and <span class="No-Break">test datasets:</span></p>
			<pre class="console">from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
# Load the iris dataset
X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1) Create linear regression object</pre>
			<p>We can then initialize our logistic regression<a id="_idIndexMarker227"/> model and simply run the <strong class="source-inline">fit</strong> function with our training data to train the model. Once we train our model, we can use it to make predictions and then measure <span class="No-Break">its accuracy:</span></p>
			<pre class="console"># Create Logistic Regression model
model = LogisticRegression()
# Train the model using the training sets
model.fit(X_train, y_train)
# Make predictions using the testing set
y_pred = model.predict(X_test)
print(accuracy_score(y_test, y_pred))</pre>
			<p>In the next few sections, we will start looking at the different models we can build using these libraries. We will understand what makes these models unique, how they are structured, and for what purposes and applications they can best serve <span class="No-Break">our needs.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Building regression models</h1>
			<p>First, we will look at regression<a id="_idIndexMarker228"/> models. Regression models or regression analysis are modeling techniques used to find the relationship between independent and dependent variables. The output of a regression model is typically a continuous value, also known as<a id="_idIndexMarker229"/> a quantitative variable. Some common examples are predicting the price of a home based on its features or predicting the sales of a certain product in a new store based on previous <span class="No-Break">sales information.</span></p>
			<p>Before building a regression model, we must first understand the data and how it is structured. The majority of regression models involve supervised learning. This consists of features and an output<a id="_idIndexMarker230"/> variable, known as a label. This will help the model by adjusting the weights to better fit the data we have observed so far. We usually denote our features as X and our labels as Y to help us understand the mathematical models used to solve <span class="No-Break">regression models:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18934_04_8.jpg" alt="Figure 4.8: Example of a supervised learning data structure"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8: Example of a supervised learning data structure</p>
			<p>Typically, our data is split into two subsets, <strong class="bold">training</strong> and <strong class="bold">testing</strong> sets. The training dataset usually consists of between 70-80% of the original data and the testing dataset contains the rest. This is to allow the model to learn on the training dataset and validate its result on the testing dataset to show its performance. From the results, we can infer how our model is performing on <span class="No-Break">the dataset.</span></p>
			<p>For a linear regression model<a id="_idIndexMarker231"/> to perform effectively, our data must be structured linearly. The model uses this formula to train on and learn about <span class="No-Break">the data:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/Formula_04_001.jpg" alt=""/>
				</div>
			</div>
			<p>In this equation, <img src="image/Formula_04_002.png" alt=""/> represents the output of the model, or what we usually call the prediction. The prediction is calculated by taking the intercept <img src="image/Formula_04_003.png" alt=""/> and the slope <img src="image/Formula_04_004.png" alt=""/>. The slope, which is also referred to as the weight, is applied to all the features in the data, which represents <img src="image/Formula_04_005.png" alt=""/>. When working with the data, we usually represent it as a matrix, which makes it easy to understand and easy to work with when <span class="No-Break">using Python:</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/Formula_04_006.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/Formula_04_007.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/Formula_04_008.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/Formula_04_009.jpg" alt=""/>
				</div>
			</div>
			<p>The number of features<a id="_idIndexMarker232"/> describes what type of problem you are solving. If your data only has one feature, it is considered a simple linear regression model. While it can solve straightforward problems, for more advanced data and problems, it can be difficult to map relationships. Therefore, you can create a multiple linear regression model by adding more features (<img src="image/Formula_04_010.png" alt=""/>). This allows the model to be more robust and find <span class="No-Break">deeper relationships.</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B18934_04_9.jpg" alt="Figure 4.9: A simple linear regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9: A simple linear regression model</p>
			<p>Once we train our model, we need to learn<a id="_idIndexMarker233"/> how to evaluate our model and understand how it performs against the test data. When it comes to linear<a id="_idIndexMarker234"/> regression, the two common metrics<a id="_idIndexMarker235"/> we use to assess our model are the <strong class="bold">Root Mean Square Error</strong> (<strong class="bold">RMSE</strong>) and the <span class="No-Break"><strong class="bold">R</strong></span><span class="No-Break"><span class="superscript">2</span></span><span class="No-Break"> metrics.</span></p>
			<p>The RMSE is the standard deviation of the <strong class="bold">residual</strong> errors across the predictions. The residual is the measure<a id="_idIndexMarker236"/> of the distance from the actual data points to the regression line. The further the average distance of all the points is from the line, the higher the error is. This indicates a weak model, as it’s unable to find the correlation between the data points. This metric can be calculated by using this formula where <img src="image/Formula_04_011.png" alt=""/> is the actual value, <img src="image/Formula_04_012.png" alt=""/> is the predicted value, and <img src="image/Formula_04_013.png" alt=""/> is the number of <span class="No-Break">data points:</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/Formula_04_014.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B18934_04_10.jpg" alt="Figure 4.10: Calculating the residual of a linear regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10: Calculating the residual of a linear regression model</p>
			<p>R<span class="superscript">2</span>, also known as the coefficient<a id="_idIndexMarker237"/> of determination, measures<a id="_idIndexMarker238"/> the proportion of variance in the dependent variables (Y) that can be explained by the independent variables (X). It essentially tells us how well the data fits the model. Unlike the RMSE, which can be an arbitrary number, R<span class="superscript">2</span> is given as a percentage, which can be easier to understand. The higher the percentage, the better the correlation of data. Although useful, a higher percentage is not always indicative of a strong model. What determines a good R<span class="superscript">2</span> value depends on the application and how the user understands the data. R<span class="superscript">2 </span>can be calculated by using <span class="No-Break">this formula:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/Formula_04_015.jpg" alt=""/>
				</div>
			</div>
			<p>Many more metrics can evaluate the effectiveness of your regression model, but these two are more than enough to get an understanding of how your model is performing. When building and evaluating your model, it is important to plot and visualize your data and model, as this can identify key points. The plots<a id="_idIndexMarker239"/> can help you determine<a id="_idIndexMarker240"/> whether your model is <strong class="bold">overfitting</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="bold">underfitting</strong></span><span class="No-Break">.</span></p>
			<p>Overfitting occurs when your model<a id="_idIndexMarker241"/> is too suited to your training data. Your RMSE will be really low, and you will have a training accuracy of almost 100%. While this seems tempting, it is an indication of a poor model. This can be caused by one of two things: not enough data or too many parameters. As a result, when you test your model on new data it has not seen before, it will perform very poorly due to it not being able to generalize <span class="No-Break">the data.</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B18934_04_11.jpg" alt="Figure 4.11: An overfitted linear regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11: An overfitted linear regression model</p>
			<p>To address overfitting, you can try to increase the amount of training data, or make the model less complex. It also helps to randomly<a id="_idIndexMarker242"/> shuffle your data before you split it into the training<a id="_idIndexMarker243"/> and testing set. Another important technique is called <strong class="bold">regularization</strong>. While there are many different regularization techniques (L1 or L2 regularization) depending on the model, they<a id="_idIndexMarker244"/> all work similarly in that they add <strong class="bold">bias</strong> or noise into the model to prevent overfitting. In the regression equation we previously saw, we can add another term, <img src="image/Formula_04_016.png" alt=""/>, to show that regularization is being applied to <span class="No-Break">our model:</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Formula_04_017.jpg" alt=""/>
				</div>
			</div>
			<p>On the other end, underfitting occurs when your model is unable to find any meaningful correlation within the data. This is not as common as overfitting since it is easy to find patterns in most data. If this occurs, either your data has too much noise and is severely uncorrelated, your model is too simple and doesn’t have enough parameters, or the model is not effective for the application at hand. It is also useful to debug your code and make sure there are no bugs when it comes to preprocessing your data or setting up <span class="No-Break">your model:</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B18934_04_12.jpg" alt="Figure 4.12: An under-fitted linear regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12: An under-fitted linear regression model</p>
			<p>Therefore, the goal<a id="_idIndexMarker245"/> is to find the best-fitting model, between an overfit and an underfit. It takes time and experimentation to find a model that works for your needs, but using the key indicators and metrics discussed here can help guide you in the <span class="No-Break">right direction:</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B18934_04_13.jpg" alt="Figure 4.13: A best-fitting linear regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13: A best-fitting linear regression model</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Feature engineering<a id="_idIndexMarker246"/> is a critical part of building a comprehensive model. Understanding your data can help determine which features or parameters to include in your model so that you can capture the relationship between the independent and dependent variables without <span class="No-Break">causing overfitting.</span></p>
			<p>There are some key notes to keep in mind when collecting and working with the data for <span class="No-Break">your model:</span></p>
			<ul>
				<li><strong class="bold">Normalize your data</strong>: It is possible to have features with very high<a id="_idIndexMarker247"/> or low numbers, so to prevent them from overwhelming the model and creating biases, it is imperative to normalize all your data to make it uniform across <span class="No-Break">the features.</span></li>
				<li><strong class="bold">Clean your data</strong>: In the real world, the data we collect isn’t always perfect and can contain<a id="_idIndexMarker248"/> missing or egregious data. It is important to deal with these issues because they can cause outliers and impact the <span class="No-Break">model negatively.</span></li>
				<li><strong class="bold">Understand the data</strong>: It is a common practice to perform<a id="_idIndexMarker249"/> statistical analysis, also known as <strong class="bold">Exploratory Data Analysis</strong> (<strong class="bold">EDA</strong>), on your data to get a better understanding<a id="_idIndexMarker250"/> of how the data can impact your model. This can include plotting graphs, running statical methods, and even using machine learning techniques to reduce the dimensionality of the data, which will be discussed later in <span class="No-Break">the chapter.</span></li>
			</ul>
			<p>In the next section, we will discuss <span class="No-Break">classification models.</span></p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Building multiclass classification</h1>
			<p>Unlike regression models<a id="_idIndexMarker251"/> that produce a continuous output, models are considered classification models when they produce a finite output. Some examples include email spam detection, image classification, and <span class="No-Break">speech recognition.</span></p>
			<p>Classification models are considered versatile since they can apply to both supervised and unsupervised learning while regression models are mostly used for supervised learning. There are some regression models (such as logistic regression and support vector machine) that are also considered classification models since they use a threshold to split the output of continuous values into <span class="No-Break">different categories.</span></p>
			<p>Unsupervised learning is a common application used in today’s market. Although supervised learning usually performs better and provides meaningful results since we know the expected output, the majority of the data we collect is unlabeled. It costs companies time and money for human experts to sift through the data and label it. Unsupervised learning helps reduce the cost and time by getting the model to try and determine the labels for the data and extract meaningful information. They can even perform better than <span class="No-Break">humans sometimes.</span></p>
			<p>The number of categories in the output of a classification model determines what type of model it is. For models with only two outputs (i.e., spam and not spam), this is called a binary classifier, while models<a id="_idIndexMarker252"/> with more<a id="_idIndexMarker253"/> than two outputs are called <span class="No-Break">multiclass classifiers:</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B18934_04_14.jpg" alt="Figure 4.14: Binary and multiclass classifiers"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14: Binary and multiclass classifiers</p>
			<p>From those<a id="_idIndexMarker254"/> classifiers, there<a id="_idIndexMarker255"/> are two types of learners: <strong class="bold">lazy learners</strong> and <span class="No-Break"><strong class="bold">eager learners</strong></span><span class="No-Break">.</span></p>
			<p>Lazy learners essentially store<a id="_idIndexMarker256"/> the training data and wait until they receive new test data. Once they get the test data, the model classifies the new data based on the already existing data. These types of learners take less time when training since you can continuously add new data without having to retrain the entire model, but take longer when performing classification since they have to go through<a id="_idIndexMarker257"/> all the data points. One common type of lazy learner is the <strong class="bold">K-Nearest Neighbors</strong> (<span class="No-Break"><strong class="bold">KNN</strong></span><span class="No-Break">) algorithm.</span></p>
			<p>On the other hand, eager learners work in the opposite way. Whenever new data is added to the model, they have to retrain the model again. Although this takes more time compared to lazy learners, querying the model is much faster since they don’t have to go through all the data points. Some examples of eager learners are decision trees, naïve Bayes, <span class="No-Break">and ANNs.</span></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Supervised learning will generally perform better than unsupervised learning since we know what the expected output should be during training, but it is costly to have to collect and label the data, so unsupervised learning<a id="_idIndexMarker258"/> excels in this area of training on <span class="No-Break">unlabeled data.</span></p>
			<p>In the next few sections, we will be looking at a few niche models that can be used for unique problems that most basic classification or regression models <span class="No-Break">can’t solve.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Text sentiment analysis and topic modeling</h1>
			<p>A popular field in the machine learning<a id="_idIndexMarker259"/> field is topic modeling<a id="_idIndexMarker260"/> and text analysis. With a plethora of text on the internet, being able to understand that data and create complex models such as chatbots and translation services<a id="_idIndexMarker261"/> has become a hot topic. Interacting with human language using software is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">NLP</strong></span><span class="No-Break">.</span></p>
			<p>Despite the amount of data we can use to train our models, it is a difficult task to create meaningful models. Language itself is complex and contains many grammar rules, especially when trying to translate between languages. Certain powerful techniques can help us when creating NLP <span class="No-Break">models though.</span></p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Before implementing any NLP models, it is imperative to preprocess the data in some way. Documents and text tend to contain extraneous data, such as stopping words (the/a/and) or random characters, which can affect the model and produce <span class="No-Break">flawed results.</span></p>
			<p>The first idea we will discuss is <strong class="bold">topic modeling</strong>. This is the process of grouping text or words from documents into different topics or fields. This is useful when you have a document or text and want to classify and group it into a certain genre without having to go through the tedious process of reading documents one by one. There are many different models used for <span class="No-Break">topic modeling:</span></p>
			<ul>
				<li><strong class="bold">Latent Semantic </strong><span class="No-Break"><strong class="bold">Analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LSA</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Probabilistic Latent Semantic </strong><span class="No-Break"><strong class="bold">Analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PLSA</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Latent Dirichlet </strong><span class="No-Break"><strong class="bold">Allocation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LDA</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>We will focus on LDA. LDA<a id="_idIndexMarker262"/> uses statistics<a id="_idIndexMarker263"/> to find patterns and repeated occurrences<a id="_idIndexMarker264"/> of words or phrases and groups them into their topics. It assumes that each document contains a mixture of topics and that each topic contains a mixture of words. LDA first starts the process of going through the documents and keeping a word matrix, where it contains the count of each word in <span class="No-Break">each document:</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18934_04_15.jpg" alt="Figure 4.15: A word matrix"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15: A word matrix</p>
			<p>After creating a word matrix, we determine the number of topics, <img src="image/Formula_04_018.png" alt=""/>, that we want to split up the words into and use statistics to find the probability of the words belonging to a certain topic. Using Bayesian statistics, we can then calculate the probability and use that to cluster the words into <span class="No-Break">different topics:</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B18934_04_16.jpg" alt="Figure 4.16: An LDA model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.16: An LDA model</p>
			<p>Another rising application in NLP is <strong class="bold">sentiment analysis</strong>. This involves the process of taking words<a id="_idIndexMarker265"/> or text and understanding the user’s intent or emotion. This is common today when dealing with online reviews or social media posts. It determines whether a piece of text contains positive, neutral, or <span class="No-Break">negative emotions.</span></p>
			<p>Many different methods and models can solve this problem. The simplest approach is through statistics by using Bayes’ theorem. This formula is used for predictive analysis, as it uses previous words in a text to update the model. The probability can be calculated using <span class="No-Break">this formula:</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/Formula_04_019.jpg" alt=""/>
				</div>
			</div>
			<p>Deep learning has become a powerful tool<a id="_idIndexMarker266"/> for NLP and can be useful for sentiment analysis. CNNs and <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>) are two types of deep learning models that can drastically improve<a id="_idIndexMarker267"/> models for NLP, especially for sentiment<a id="_idIndexMarker268"/> analysis. We will discuss these neural networks and how they perform more later in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor058"/>Pattern analysis and forecasting in machine learning</h1>
			<p>With the uncertainty<a id="_idIndexMarker269"/> of time, being<a id="_idIndexMarker270"/> able to predict certain trends and patterns has become a hot topic in today’s industry. Most regression models, while powerful, are not able to make confident time predictions. As a result, some researchers have devised models that take time into consideration when making certain predictions, such as gas prices, stock market, and sales forecasting. Before we go into the different models, we must first understand the different concepts in <span class="No-Break">time-series analysis.</span></p>
			<p>The first step when dealing with time-series problems is familiarizing yourself with the data. The data usually contains one of four <span class="No-Break">data components:</span></p>
			<ul>
				<li><strong class="bold">Trend</strong> – The data follows an increasing<a id="_idIndexMarker271"/> or decreasing continuous timeline and there are no <span class="No-Break">periodic changes</span></li>
				<li><strong class="bold">Seasonality</strong> – The data changes in a set <span class="No-Break">periodic</span><span class="No-Break"><a id="_idIndexMarker272"/></span><span class="No-Break"> timeline</span></li>
				<li><strong class="bold">Cyclical</strong> – The data changes<a id="_idIndexMarker273"/> but there is no set <span class="No-Break">periodic timeline</span></li>
				<li><strong class="bold">Irregular</strong> – The data changes randomly<a id="_idIndexMarker274"/> with <span class="No-Break">no pattern</span></li>
			</ul>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18934_04_17.jpg" alt="Figure 4.17: Different components of time-series data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.17: Different components of time-series data</p>
			<p>These different trends <a id="_idIndexMarker275"/>can be split into two<a id="_idIndexMarker276"/> different <span class="No-Break">data types:</span></p>
			<ul>
				<li><strong class="bold">Stationary</strong> – Certain attributes of the data, such<a id="_idIndexMarker277"/> as mean, variance, and covariance, do not change <span class="No-Break">over time</span></li>
				<li><strong class="bold">Non-stationary</strong> – Attributes of the data change<a id="_idIndexMarker278"/> <span class="No-Break">over time</span></li>
			</ul>
			<p>Often, you will work with non-stationary data, and creating machine learning models using this type of data will generate unreliable results. To resolve this issue, we use certain techniques to change our data into stationary data. They include the <span class="No-Break">following methods:</span></p>
			<ul>
				<li><strong class="bold">Differencing</strong> – A mathematical method<a id="_idIndexMarker279"/> used to normalize the mean and remove the variance. It can be calculated by using <span class="No-Break">this formula:</span><div id="_idContainer086" class="IMG---Figure"><img src="image/Formula_04_020.jpg" alt=""/></div></li>
			</ul>
			<ul>
				<li><strong class="bold">Transformation</strong> – Mathematical methods are used to remove<a id="_idIndexMarker280"/> the change in variance. Among<a id="_idIndexMarker281"/> the transformations, these are three<a id="_idIndexMarker282"/> <span class="No-Break">commonly used:</span><ul><li><span class="No-Break">Log transform</span></li>
<li><span class="No-Break">Square root</span></li>
<li><span class="No-Break">Power transform</span></li>
</ul></li>
			</ul>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18934_04_18.jpg" alt="Figure 4.18: Differencing non-stationary data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.18: Differencing non-stationary data</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Time is already uncertain, and this makes it almost impossible to create a model that can confidently predict future trends. The more we can remove uncertainty in our data, the better our model can find the relationships in <span class="No-Break">our data.</span></p>
			<p>Once we can transform our data, we can start looking at models to help us with forecasting. Of the different<a id="_idIndexMarker283"/> models, the most popular model for time-series analysis is an <strong class="bold">Auto-Regressive Integrated Moving Average</strong> (<strong class="bold">ARIMA</strong>) model. This linear regression model consists of <span class="No-Break">three subcomponents:</span></p>
			<ul>
				<li><strong class="bold">Auto-Regression</strong> (<strong class="bold">AR</strong>) – A regression model that uses the dependencies<a id="_idIndexMarker284"/> of the current time and previous time to <span class="No-Break">make predictions</span></li>
				<li><strong class="bold">Integrated</strong> (<strong class="bold">I</strong>) – The process of differencing<a id="_idIndexMarker285"/> in order to make the <span class="No-Break">data stationary</span></li>
				<li><strong class="bold">Moving Average</strong> (<strong class="bold">MA</strong>) – Models between the expected data<a id="_idIndexMarker286"/> and the residual error by calculating the MA of the lagged <span class="No-Break">observed data</span></li>
			</ul>
			<p>Along with ARIMA, other machine learning<a id="_idIndexMarker287"/> models can be used for time-series<a id="_idIndexMarker288"/> problems. Another well-known model is the RNN model. This is a type of deep<a id="_idIndexMarker289"/> learning model used for data that has some sort of sequence. We will be going into more detail on how they work in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>Enhancing models using deep learning</h1>
			<p>Earlier in the chapter, we briefly discussed<a id="_idIndexMarker290"/> deep learning<a id="_idIndexMarker291"/> and the advantages it brings when enhancing simple machine learning models. In this section, we will go into more information on the different deep <span class="No-Break">learning models.</span></p>
			<p>Before we can build our model, we will briefly go over the structure of the deep learning models. A simple ANN model usually contains about two to three fully connected layers and is usually strong enough to model most complex linear functions, but as we add more layers, the improvement to the model significantly diminishes, and it is unable to perform more complex applications due <span class="No-Break">to overfitting.</span></p>
			<p>Deep learning allows us to add multiple hidden layers to our ANN while reducing our time to train the model and increasing the performance. We can do this by adding one or both types of hidden layers common in deep learning – a CNN <span class="No-Break">or RNN.</span></p>
			<p>A CNN is mostly applied in the image detection and video recognition field due to how the neural network is structured. The CNN architecture comprises the following <span class="No-Break">key features:</span></p>
			<ul>
				<li><span class="No-Break">Convolutional layers</span></li>
				<li><span class="No-Break">Activation layers</span></li>
				<li><span class="No-Break">Pooling layers</span></li>
			</ul>
			<p>The convolutional layer is the core<a id="_idIndexMarker292"/> element in the CNN model. Its primary task is to convolve or group sections of the data using a <strong class="bold">kernel</strong> and produces an output called a <strong class="bold">feature map</strong>. This map contains<a id="_idIndexMarker293"/> all the key features extracted<a id="_idIndexMarker294"/> from the data, which can then be used for training<a id="_idIndexMarker295"/> in the fully connected layer. Each element in the feature map indicates a receptive field, which is used to denote which part of the input is used to map to the output. As you add more convolutional layers, you can extract more features, and this allows your model to adapt to more <span class="No-Break">complex models:</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18934_04_19.jpg" alt="Figure 4.19: Convolutional layer output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.19: Convolutional layer output</p>
			<p>In <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.19</em>, we can see how the feature map is created in the convolutional layer. The layer slides the kernel across the input data, performs the dot operation, and produces an output matrix, which is the result of the convolution function. The size of the kernel and the step size can dictate the output size of the feature map. In this example, we use a kernel size of 2x2 and a stride or step size of 2, which gives us a feature map of size 2x2 based on our input size. The output of the feature map can then be used for more future convolutional layers depending on the requirements of <span class="No-Break">the user.</span></p>
			<p>Before we use our newly created feature map<a id="_idIndexMarker296"/> as an input for another convolutional<a id="_idIndexMarker297"/> or fully connected layer, we pass the feature map through an activation layer. It is important to pass your data through some type of nonlinear function during the training process, as this allows your model to map to more complex functions. Many different types of activation functions can be used throughout your model and have their benefits depending on the type of model you are planning to build. Among the many activation functions, these are the most <span class="No-Break">commonly used:</span></p>
			<ul>
				<li><strong class="bold">Rectified Linear </strong><span class="No-Break"><strong class="bold">Activation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ReLU</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break"><strong class="bold">Logistic (sigmoid)</strong></span></li>
				<li>The <strong class="bold">Hyperbolic </strong><span class="No-Break"><strong class="bold">Tangent</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">Tanh</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>The ReLU function<a id="_idIndexMarker298"/> is the most popular activation function used today. It is very simple and helps the model learn and converge more quickly than most other activation functions. It is calculated using <span class="No-Break">this function:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/Formula_04_021.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B18934_04_20.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.20: ReLU function</p>
			<p>The logistic function<a id="_idIndexMarker299"/> is another<a id="_idIndexMarker300"/> commonly<a id="_idIndexMarker301"/> used activation function. This is the same function used in logistical regression models. This function helps bound the output of the feature map between 0 and 1. While useful, this function is computationally heavy and may slow down the training process. It is calculated using <span class="No-Break">this function:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/Formula_04_022.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B18934_04_21.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.21: Sigmoid function</p>
			<p>The Tanh function<a id="_idIndexMarker302"/> is similar<a id="_idIndexMarker303"/> to the sigmoid function in that it bounds<a id="_idIndexMarker304"/> the values from the feature map. Rather than bounding it from 0 to 1, it bounds the values from -1 to 1, and it usually performs better than a sigmoid function. The function is calculated using <span class="No-Break">this formula:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/Formula_04_023.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B18934_04_22.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.22: Tanh function</p>
			<p>Each activation has its uses<a id="_idIndexMarker305"/> and benefits depending on the task or model at hand. The ReLU function is commonly used in CNN models while sigmoid and Tanh are mostly found in RNN models, but they can be used interchangeably and bring <span class="No-Break">different results.</span></p>
			<p>After we run our feature map<a id="_idIndexMarker306"/> through an activation layer, we come to the final piece – the pooling<a id="_idIndexMarker307"/> layer. As mentioned before, a key element in deep learning is the reduction of parameters. This allows the model to train on fewer parameters while still retaining the important features extracted from our convolutional layers. The pooling layer is responsible for this step of downsizing our parameter size. There are many common pooling functions but the most commonly used is max pooling. This is similar to the convolutional layer, where we use a kernel or filter to slide through our input data and only take the maximum value from <span class="No-Break">each window:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B18934_04_23.jpg" alt="Figure 4.23: Max pooling layer output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.23: Max pooling layer output</p>
			<p>In <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.23</em>, we are using a kernel size of 2x2 with a stride size of 2. Here, we can see our output where only the maximum value from each window <span class="No-Break">is selected.</span></p>
			<p>Other layers and functions<a id="_idIndexMarker308"/> can be added to the model to help address<a id="_idIndexMarker309"/> certain issues and applications, such as a batch normalization layer, but with these three foundational layers, we can add multiple layers of different sizes and still build a <span class="No-Break">powerful model.</span></p>
			<p>In the final layer, we feed our output into the fully connected layer. By that time, we are able to extract the important features of the data and still learn more complex models with less time to train as compared to a simple <span class="No-Break">ANN model.</span></p>
			<p>Next, we will go over the RNN architecture. Due to the nature of how the RNN model is structured, it is designed for tasks that need to take into consideration a set of sequence data, in which the data later in the sequence is dependent on earlier data. This model is commonly used for certain fields such as NLP and <span class="No-Break">signal processing.</span></p>
			<p>The basics of an RNN are built by having a hidden layer in which the output of the layer is fed back into the same hidden layer. This way, the model is able to learn based on previous data and adjust the <span class="No-Break">weights accordingly.</span></p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B18934_04_24.jpg" alt="Figure 4.24: RNN architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.24: RNN architecture</p>
			<p>To better understand how the model<a id="_idIndexMarker310"/> works, you can envision a single layer<a id="_idIndexMarker311"/> for each data point. Each layer takes in the data point as an input <img src="image/Formula_04_024.png" alt=""/> and produces an output <img src="image/Formula_04_025.png" alt=""/>. We then transfer the weights between the layers and then take the total average of all the cost functions from all the layers in <span class="No-Break">the model.</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B18934_04_25.jpg" alt="Figure 4.25: An unraveled RNN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.25: An unraveled RNN</p>
			<p>This model works<a id="_idIndexMarker312"/> for most simple problems. As the sequence<a id="_idIndexMarker313"/> increases, it encounters a <span class="No-Break">few issues:</span></p>
			<ul>
				<li><strong class="bold">Vanishing gradient</strong>: This occurs during the training<a id="_idIndexMarker314"/> process when the gradient approaches zero. The weights aren’t updated properly as a result and the model <span class="No-Break">performs poorly.</span></li>
				<li><strong class="bold">Lack of context</strong>: The model is unidirectional and cannot look further or previously into the data. Therefore, the model is only able to predict based on data around the current sequence point and is more likely to make a poor prediction based on <span class="No-Break">incorrect context.</span></li>
			</ul>
			<p>There are different variations of RNNs created to address some of the issues mentioned here. Among them, the most common one used today is the <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>) model. The LSTM model comprise<a id="_idIndexMarker315"/> <span class="No-Break">three components:</span></p>
			<ul>
				<li>An <span class="No-Break">input gate</span></li>
				<li>An <span class="No-Break">output gate</span></li>
				<li>A <span class="No-Break">forget gate</span></li>
			</ul>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B18934_04_26.jpg" alt="Figure 4.26: An LSTM neural network"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.26: An LSTM neural network</p>
			<p>These gates work by regulating which data points are needed to contextualize the sequence. That way, the model can predict more accurately without being <span class="No-Break">easily manipulated.</span></p>
			<p>The forget gate is specifically<a id="_idIndexMarker316"/> responsible for removing<a id="_idIndexMarker317"/> previous data or context that is no longer needed. This gate uses the sigmoid function to determine whether it uses or “forgets” <span class="No-Break">the data.</span></p>
			<p>The input gate is used to determine whether the new data is relevant to the current sequence or not. This is so that only important data is being used to train the model and not redundant or <span class="No-Break">irrelevant information.</span></p>
			<p>Lastly, the output gate’s primary function is to filter the current state’s information and only send relevant information to the next state. As with the other gates, it uses the context from previous states to apply a filter, which helps the model properly contextualize <span class="No-Break">the data.</span></p>
			<p>CNN and RNN models are mostly designed for supervised learning problems. When it comes to unsupervised learning, different models<a id="_idIndexMarker318"/> are needed to solve certain problems. Let’s <span class="No-Break">discuss </span><span class="No-Break"><strong class="bold">autoencoders</strong></span><span class="No-Break">.</span></p>
			<p>Autoencoders work by taking<a id="_idIndexMarker319"/> the input data, compressing<a id="_idIndexMarker320"/> it, and then reconstructing it by decompressing it. While straightforward, it can be used for some advanced applications, such as generating audio or images, or it can be used as an <span class="No-Break">anomaly detector.</span></p>
			<p>The autoencoder comprises <span class="No-Break">two parts:</span></p>
			<ul>
				<li><span class="No-Break">An encoder</span></li>
				<li><span class="No-Break">A decoder</span></li>
			</ul>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B18934_04_27.jpg" alt="Figure 4.27: The components of an autoencoder"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.27: The components of an autoencoder</p>
			<p>The encoder and decoder are usually built with a one-layer ANN. The encoder is responsible for taking the data and compressing or flattening the data. Then, the decoder works on taking the flattened data and trying to reconstruct the <span class="No-Break">input data.</span></p>
			<p>The hidden layer in the middle of the encode and decoder is usually referred to as the bottleneck. The number<a id="_idIndexMarker321"/> of nodes in the hidden layer must be less than those in the encoder and decoder. This forces the model to try and find the pattern or representation in the input data so that it can reconstruct the data with little information. Thus, the cost function is there to calculate and minimize the difference between the input and <span class="No-Break">output data.</span></p>
			<p>One aspect of autoencoders<a id="_idIndexMarker322"/> that is an integral part of deep learning is <strong class="bold">dimensionality reduction</strong>. This is the process of reducing the number of parameters or features used when training your model. As mentioned earlier in this chapter, to build a complex model that can build a deeper representation of the data, it is important to include more features. However, adding too many features can lead to overfitting, so how do we find the best number of features to use in <span class="No-Break">our model?</span></p>
			<p>There are many models and techniques, such as autoencoders, that can perform dimensionality reduction to help us find<a id="_idIndexMarker323"/> the best features to use in our model. Among the different techniques, <strong class="bold">Principle Component Analysis</strong> (<strong class="bold">PCA</strong>) is the most popular. This technique can take an N-dimensional dataset and reduce the number of dimensions in the data using linear algebra. It is a common practice to use a dimensionality reduction<a id="_idIndexMarker324"/> technique before using your data to train<a id="_idIndexMarker325"/> your model, as this can help to remove noise in the data and <span class="No-Break">avoid overfitting.</span></p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor060"/>Summary</h1>
			<p>In this chapter, we discussed what is considered artificial intelligence and the different sub-fields that <span class="No-Break">it contains.</span></p>
			<p>We discussed the different characteristics of regression and classification models. From there, we also went over the structure of our data and how the model performs when training over our data. We then discussed the different ways of analyzing our model’s performance and how to address the different issues that we can come across when training <span class="No-Break">our model.</span></p>
			<p>We briefly viewed the different packages and libraries that are used today in machine learning models and their different <span class="No-Break">use cases.</span></p>
			<p>We also analyzed different topics such as topic modeling and time-series analysis and what they entail. With that, we were able to look at the different methods and techniques used to solve those types <span class="No-Break">of problems.</span></p>
			<p>Lastly, we went into deep learning and the different ways it improves on machine learning. We went over the two different types of neural networks – CNNs and RNNs – how they are structured, and their benefits and <span class="No-Break">use cases.</span></p>
			<p>In the next chapter, we will take what we have learned and start looking into how we can design and build an end-to-end machine learning system and the different components that <span class="No-Break">it contains.</span></p>
</body></html>