<html><head></head><body>
		<div id="_idContainer140">
			<h1 id="_idParaDest-163"><a id="_idTextAnchor189"/>Chapter 10: Essentials of Production Release</h1>
			<p><a id="_idTextAnchor190"/>In this chapter, you will learn about the <strong class="bold">continuous integration </strong>and<strong class="bold"> continuous delivery</strong> (<strong class="bold">CI/CD</strong>) pipeline, the essentials of a production environment, and how to set up a production environment to serve your previously tested and approved <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models to end users. We will set up the required infrastructure for the CI/CD pipeline's production environment, configure processes for production deployments, configure pipeline execution triggers for complete automation, and learn how to manage production releases. This chapter will cover the essential fundamentals of the CI/CD pipeline and production environment since <em class="italic">the pipeline is the product, not the model</em>. </p>
			<p>By learning about the fundamentals of CI/CD pipelines, you will be able to develop, test, and configure automated CI/CD pipelines for your use cases or business. We will cover an array of topics around production deployments and then delve into a primer on monitoring ML models in production. </p>
			<p>We are going to cover the following topics in this chapter:<a id="_idTextAnchor191"/></p>
			<ul>
				<li>Setting up the production infrastructure </li>
				<li>Setting up our production environment in the CI/CD pipeline </li>
				<li>Testing our production-ready pipeline</li>
				<li>Configuring pipeline triggers for automation </li>
				<li>Pipeline release management </li>
				<li>Toward continuous monitoring the service </li>
			</ul>
			<p>Let's begin by setting up the infrastructure that's required to build the CI/CD pipeline.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor192"/>Setting up the production infrastructure</h1>
			<p>In this section, we will set up the <a id="_idIndexMarker687"/>required infrastructure to serve our business use case (to predict weather conditions – raining or not raining at the port of Turku to plan and optimize resources at the port). We will set up an autoscaling Kubernetes cluster to deploy our ML model in the form of a web service. Kubernetes is an open source container orchestration system for automating software application deployment, scaling, and management. Many cloud service providers offer a <a id="_idIndexMarker688"/>Kubernetes-based infrastructure as a service. Similarly, Microsoft Azure provides a Kubernetes-based infrastructure <a id="_idIndexMarker689"/>as a service called <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>). We will use AKS to orchestrate our infrastructure.</p>
			<p>There are multiple ways to provision an autoscaling Kubernetes cluster on Azure. We will explore the following two ways to learn about the different perspectives of infrastructure provisioning:</p>
			<ul>
				<li>Azure Machine Learning workspace portal </li>
				<li>Azure SDK</li>
			</ul>
			<p>Let's look into the easiest way first; that is, using the Azure Machine Learning workspace to provision an Azure Kubernetes cluster for production. </p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor193"/>Azure Machine Learning workspace</h2>
			<p>In this section, we <a id="_idIndexMarker690"/>will provision an Azure Kubernetes cluster using the Azure Machine Learning workspace. Perform the following steps:</p>
			<ol>
				<li>Go to the Azure Machine Learning workspace and then go to the <strong class="bold">Compute</strong> section, which presents options for creating different types of computes. Select <strong class="bold">Inference clusters</strong> and click <strong class="bold">Create</strong>, as shown in the following screenshot:<div id="_idContainer123" class="IMG---Figure"><img src="image/B16572_10_01.jpg" alt="Figure 10.1 – Provisioning inference clusters &#13;&#10;"/></div><p class="figure-caption">Figure 10.1 – Provisioning inference clusters </p></li>
				<li>Clicking the <strong class="bold">Create</strong> button will <a id="_idIndexMarker691"/>present various compute options you can use to create a Kubernetes service. You will be prompted to select a <strong class="bold">Region</strong>, which is where your compute will be provisioned, and some configuration so that you can provision in terms of cores, RAM, and storage. Select a suitable option (it is recommended that you select <strong class="bold">Standard_D2_v4</strong> as a cost-optimal choice for this experiment), as shown in the following screenshot:<div id="_idContainer124" class="IMG---Figure"><img src="image/B16572_10_02.jpg" alt="Figure 10.2 – Selecting a suitable compute option&#13;&#10;"/></div><p class="figure-caption">Figure 10.2 – Selecting a suitable compute option</p></li>
				<li>After selecting a <a id="_idIndexMarker692"/>suitable compute option, you will be prompted to <strong class="bold">Configure Settings</strong> for the cluster, as shown in the following screenshot. Name our <strong class="bold">Compute</strong> (for example, <strong class="source-inline">'prod-aks'</strong> - meaning <strong class="bold">production Azure Kubernetes Service</strong>), set <strong class="bold">Cluster purpose</strong> to <strong class="bold">Production</strong> (as we are setting up for production), choose <strong class="bold">Number of nodes</strong> for the cluster, and select the <strong class="bold">Basic</strong> option for <strong class="bold">Network configuration</strong>. Omit <strong class="bold">Enable SSL configuration</strong> for simplicity. However, it is recommended to enable SSL connections for more security in production, as per your needs:<div id="_idContainer125" class="IMG---Figure"><img src="image/B16572_10_03.jpg" alt="Figure 10.3 - Configure Settings&#13;&#10;"/></div><p class="figure-caption">Figure 10.3 - Configure Settings</p></li>
				<li>Click the <strong class="bold">Create</strong> button to <a id="_idIndexMarker693"/>provision the Kubernetes cluster for production. It will take around 15 minutes to create and provision the compute for production use:<div id="_idContainer126" class="IMG---Figure"><img src="image/B16572_10_04.jpg" alt="Figure 10.4 – Provisioned Kubernetes cluster &#13;&#10;"/></div><p class="figure-caption">Figure 10.4 – Provisioned Kubernetes cluster </p></li>
				<li>Once your AKS cluster has been provisioned, you will see a running Kubernetes cluster with the <a id="_idIndexMarker694"/>name you provided for the compute (for example, <strong class="source-inline">prod-aks</strong>), as shown in the preceding screenshot.  </li>
			</ol>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor194"/>Azure Machine Learning SDK</h2>
			<p>An alternative <a id="_idIndexMarker695"/>way of creating and provisioning a Kubernetes cluster on Azure is by using the Azure Machine Learning SDK. You can use a premade script named <strong class="source-inline">create_aks_cluster.py</strong>, which can be found in the <strong class="source-inline">10_Production_Release</strong> folder. The prerequisite to running the <strong class="source-inline">create_aks_cluster.py</strong> script is the <strong class="source-inline">config.json</strong> (it can be downloaded from the Azure Machine Learning workspace) file for your Azure Machine Learning workspace, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B16572_10_05.jpg" alt="Figure 10.5 – Fetching the config file from your Azure Machine Learning workspace &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Fetching the config file from your Azure Machine Learning workspace </p>
			<p>Go to your Azure Machine Learning workspace and click on <strong class="bold">Download config.json</strong> to download your config file. After downloading it, copy or move the <strong class="source-inline">config.json</strong> file into the same directory (<strong class="source-inline">10_Production_Release</strong>) that the <strong class="source-inline">create_aks_cluster.py</strong> file is in, as shown here:</p>
			<p class="source-code">├──10_Production_Release</p>
			<p class="source-code">├── create_aks_cluster.py</p>
			<p class="source-code">├── config.json</p>
			<p>With this, you are now set to run the script (<strong class="source-inline">create_aks_cluster.py</strong>) to create AKS compute for production deployments. Let's look at the <strong class="source-inline">create_aks_cluster.py</strong> script: </p>
			<ol>
				<li value="1">Import the necessary functions from the <strong class="source-inline">azureml.core</strong> SDK or library. Functions such as <strong class="source-inline">Workspace</strong>, <strong class="source-inline">Model</strong>, <strong class="source-inline">ComputeTarget</strong>, <strong class="source-inline">AksCompute</strong>, and so on will be used to provision your AKS cluster:<p class="source-code">from azureml.core import Workspace</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">from azureml.core.compute import ComputeTarget</p><p class="source-code">from azureml.core.compute_target import ComputeTargetException</p><p class="source-code">from azureml.core.compute import AksCompute, ComputeTarget</p></li>
				<li>By importing the <a id="_idIndexMarker696"/>necessary functions, you can start using them by connecting to your Azure Machine Learning workspace and creating the <strong class="source-inline">ws</strong> object. Do this by using the <strong class="source-inline">Workspace</strong> function and pointing it to your <strong class="source-inline">config.json</strong> file, like so:<p class="source-code">ws = Workspace.from_config()</p><p class="source-code">print(ws.name, ws.resource_group, ws.location, sep = '\n')</p></li>
				<li>By default, the <strong class="source-inline">from_config()</strong> function looks for the <strong class="source-inline">config.json</strong> file in same directory where you are executing the <strong class="source-inline">create_aks.py</strong> file. If your <strong class="source-inline">config.json</strong> file is in some other location, then point to the location of the file in the <strong class="source-inline">from_config()</strong> function. After successfully executing the <strong class="source-inline">workspace.from_config()</strong> function, you will see the workspace's name, resource group, and location printed out.</li>
				<li>Next, we will create an AKS Kubernetes cluster for production deployments. Start by choose a name for your AKS cluster (reference it to the <strong class="source-inline">aks_name</strong> variable), such as <strong class="source-inline">prod-aks</strong>. The script will check if a cluster with the chosen name already exists. We can use the <strong class="source-inline">try</strong> statement to check whether the AKS target with the chosen name exists by using the <strong class="source-inline">ComputeTarget()</strong> function. It takes the <strong class="source-inline">workspace</strong> object and <strong class="source-inline">aks_name</strong> as parameters. If a cluster is found with the chosen name, it will print the cluster that was found and stop execution. Otherwise, a <a id="_idIndexMarker697"/>new cluster will be created using the <strong class="source-inline">ComputeTarget.create()</strong> function, which takes the <strong class="source-inline">provisioning</strong> config with the default configuration:<p class="source-code"># Choose a name for your AKS cluster</p><p class="source-code">aks_name = 'prod-aks' </p><p class="source-code"># Verify that cluster does not exist already</p><p class="source-code">try:</p><p class="source-code">    aks_target = ComputeTarget(workspace=ws, name=aks_name)</p><p class="source-code">    print('Found existing cluster, use it.')</p><p class="source-code">except ComputeTargetException:</p><p class="source-code">    # Use the default configuration (can also provide parameters to customize)</p><p class="source-code">    prov_config = AksCompute.provisioning_configuration()</p><p class="source-code">    # Create the cluster</p><p class="source-code">    aks_target = ComputeTarget.create(workspace = ws, </p><p class="source-code">                                    name = aks_name, </p><p class="source-code">provisioning_configuration = prov_config)</p><p class="source-code">if aks_target.get_status() != "Succeeded":</p><p class="source-code">    aks_target.wait_for_completion(show_output=True)</p></li>
			</ol>
			<p>After successfully executing the preceding code, a new cluster with the chosen name (that is, <strong class="source-inline">prod-aks</strong>) will be created. Usually, creating a new cluster takes around 15 minutes. Once the cluster has <a id="_idIndexMarker698"/>been created, it can be spotted in the Azure Machine Learning workspace, as we saw in <em class="italic">Figure 10.4</em>. Now that we have set up the prerequisites for enhancing the CI/CD pipeline for our production environment, let's start setting it up!</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor195"/>Setting up our production environment in the CI/CD pipeline</h1>
			<p>Perform the <a id="_idIndexMarker699"/>following steps to set up a <a id="_idIndexMarker700"/>production environment in the CI/CD pipeline:</p>
			<ol>
				<li value="1">Go to the Azure DevOps project you worked on previously and revisit the <strong class="bold">Pipelines</strong> | <strong class="bold">Releases</strong> section to view your <strong class="bold">Port Weather ML Pipeline</strong>. We will enhance this pipeline by creating a production stage. </li>
				<li>Click on the <strong class="bold">Edit</strong> button to get started and click on <strong class="bold">Add</strong> under the <strong class="bold">DEV TEST</strong> stage, as shown in the following screenshot: <div id="_idContainer128" class="IMG---Figure"><img src="image/B16572_10_06.jpg" alt="Figure 10.6 – Adding a new stage&#13;&#10;"/></div><p class="figure-caption">Figure 10.6 – Adding a new stage</p></li>
				<li>Clicking the <strong class="bold">Add</strong> button <a id="_idIndexMarker701"/>under the <strong class="bold">DEV TEST</strong> stage will prompt you <a id="_idIndexMarker702"/>to select a template to create a new stage. Select <strong class="bold">EMPTY JOB</strong> option (under Select a template text) and name the stage <strong class="source-inline">production</strong> or <strong class="source-inline">PROD</strong> and save it, as shown in the following screenshot: <div id="_idContainer129" class="IMG---Figure"><img src="image/B16572_10_07.jpg" alt="Figure 10.7 – Adding and saving the production stage (PROD)&#13;&#10;"/></div><p class="figure-caption">Figure 10.7 – Adding and saving the production stage (PROD)</p></li>
				<li>A new production stage named <strong class="bold">PROD</strong> will be created. Now, you can configure jobs and processes at the production stage. To configure jobs for <strong class="bold">PROD</strong>, click on the 1 job, 0 task link (as shown in the preceding screenshot, in the <strong class="bold">PROD</strong> stage) in the <strong class="bold">PROD</strong> stage. You will be directed to the <strong class="bold">Tasks</strong> section, which is where you can add jobs to the <strong class="bold">PROD</strong> stage. In this stage, we will deploy models from our Azure Machine <a id="_idIndexMarker703"/>Learning workspace, so <a id="_idIndexMarker704"/>we will connect to it using the AzureML Model Deploy template we used previously in <a href="B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Building Robust CI and CD Pipelines</em>. Click on the <strong class="bold">+</strong> sign on the right-hand side of the <strong class="bold">Agent job</strong> section to add a task, as shown here:<div id="_idContainer130" class="IMG---Figure"><img src="image/B16572_10_08.jpg" alt="Figure 10.8 – Adding an AzureML Model Deploy task &#13;&#10;"/></div><p class="figure-caption">Figure 10.8 – Adding an AzureML Model Deploy task </p></li>
				<li>Search for the <strong class="bold">AzureML Model Deploy</strong> template or task and add it. After adding the task, you will be prompted to configure it by connecting to your Azure Machine Learning workspace. Select your Azure Machine Learning workspace (for example, <strong class="source-inline">mlops_ws</strong>) and point to <strong class="bold">Model Artifact</strong> as your model source. We are doing this because we will be using the Model Artifacts we trained in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>. </li>
				<li>Next, point to your inference configuration file from the Azure DevOps repository, as shown in the following screenshot. The inference configuration file represents the configuration settings for a custom environment that's used for deployment. We will use the same inference <strong class="source-inline">Config.yml</strong> file that we used for the <strong class="bold">DEV TEST</strong> environment as the environment for deployment should be the same:<div id="_idContainer131" class="IMG---Figure"><img src="image/B16572_10_09.jpg" alt="Figure 10.9 – Selecting your inference configuration &#13;&#10;"/></div><p class="figure-caption">Figure 10.9 – Selecting your inference configuration </p><p>Basic <a id="_idIndexMarker705"/>configuration is done by <a id="_idIndexMarker706"/>selecting the <strong class="source-inline">inferenceConfig.yml</strong> file. </p></li>
				<li>Next, we will configure <strong class="bold">Deployment Information</strong> by specifying the deployment target type as <strong class="bold">Azure Kubernetes Service</strong>. We're doing this because this is the production environment we will use to autoscale our Kubernetes cluster for production deployments. After selecting <strong class="bold">Azure Kubernetes service</strong> as your deployment target, select the AKS cluster you created previously (for example, <strong class="source-inline">prod-aks</strong>), name your deployment or web service (for example, <strong class="source-inline">prod-webservice</strong>), and select the <strong class="bold">Deployment Configuration</strong> file from the Azure DevOps repository that's connected to the pipeline, as shown here:<div id="_idContainer132" class="IMG---Figure"><img src="image/B16572_10_10.jpg" alt="Figure 10.10 – Configuring your deployment information &#13;&#10;"/></div><p class="figure-caption">Figure 10.10 – Configuring your deployment information </p><p>The deployment <a id="_idIndexMarker707"/>configuration file we will use for the <a id="_idIndexMarker708"/>production environment is called <strong class="source-inline">AksDeploymentConfig.yml</strong>. It contains the configuration details for our deployment, including enabling autoscaling with a min and max number of replicas, authentication configuration, monitoring configuration and container resource requirements with CPU (for other situations where inference needs to be very fast or larger in-memory processing is needed, you may want to consider using a GPU resource), and memory requirements, as shown here:</p><p class="source-code">AksDeploymentConfig.yml</p><p class="source-code">computeType: AKS</p><p class="source-code">autoScaler:</p><p class="source-code">    autoscaleEnabled: True</p><p class="source-code">    minReplicas: 1</p><p class="source-code">    maxReplicas: 3</p><p class="source-code">    refreshPeriodInSeconds: 10</p><p class="source-code">    targetUtilization: 70</p><p class="source-code">authEnabled: True</p><p class="source-code">containerResourceRequirements:</p><p class="source-code">    cpu: 1</p><p class="source-code">    memoryInGB: 1</p><p class="source-code">appInsightsEnabled: False</p><p class="source-code">scoringTimeoutMs: 1000</p><p class="source-code">maxConcurrentRequestsPerContainer: 2</p><p class="source-code">maxQueueWaitMs: 1000</p><p class="source-code">sslEnabled: False</p></li>
				<li>Select <a id="_idIndexMarker709"/>the <strong class="source-inline">AksDeploymentConfig.yml</strong> file as <a id="_idIndexMarker710"/>our <strong class="bold">Deployment Configuration</strong> file. Now, hit the <strong class="bold">Save</strong> button to set up the <strong class="bold">PROD</strong> environment.</li>
			</ol>
			<p>With that, you have successfully set up the production environment and integrated it with your CI/CD pipeline for automation. Now, let's test the pipeline by executing it. </p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor196"/>Testing our production-ready pipeline</h1>
			<p>Congr<a id="_idTextAnchor197"/>atulations on <a id="_idIndexMarker711"/>setting up the production pipeline! Next, we will test its robustness. One great way to do this is to create a new release and observe and study whether the production pipeline successfully deploys the model to production (in the production Kubernetes cluster setup containing the pipeline). Follow these steps to test the pipeline:</p>
			<ol>
				<li value="1">First, create a new release, go to the <strong class="bold">Pipelines</strong> | <strong class="bold">Releases</strong> section, select your previously created pipeline (for example, <strong class="bold">Port Weather ML Pipeline</strong>), and click on the <strong class="bold">Create Release</strong> button at the top right-hand side of the screen to initiate a new release, as shown here:<div id="_idContainer133" class="IMG---Figure"><img src="image/B16572_10_11.jpg" alt="Figure 10.11 – Create a new release&#13;&#10;"/></div><p class="figure-caption">Figure 10.11 – Create a new release</p></li>
				<li>Select the artifacts you would like to deploy in the pipeline (for example, <strong class="source-inline">Learn_MLOps repo</strong>, <strong class="source-inline">_scaler</strong>, and <strong class="source-inline">support-vector-classifier</strong> model and select their versions. Version 1 is recommended for testing PROD deployments for the first time), and click on the <strong class="bold">Create</strong> button at the top right-hand side of the screen, as shown in the preceding screenshot. Once you've done this, a new release is initiated, as shown in the following screenshot:<div id="_idContainer134" class="IMG---Figure"><img src="image/B16572_10_12.jpg" alt="Figure 10.12 – New release’s execution &#13;&#10;"/></div><p class="figure-caption">Figure 10.12 – New release's execution </p></li>
				<li>After executing the pipeline, both the <strong class="bold">DEV TEST</strong> and <strong class="bold">PROD</strong> stages will be deployed (for example, <strong class="bold">Release-5</strong>, as shown in the preceding screenshot). You can check each step in each stage by <a id="_idIndexMarker712"/>monitoring the logs of each step of any stage (DEV TEST or PROD) while the pipeline release is in progress, until the pipeline is deployed successfully. You can also check the logs of previous releases. </li>
				<li>Upon successfully working on a release, both the <strong class="bold">DEV TEST</strong> and <strong class="bold">PROD</strong> stages will be deployed using CI and CD. You must ensure that the pipeline is robust. Next, we can customize the pipeline further by adding custom triggers that will automate the pipeline without any human supervision. Automating CI/CD pipelines without any human supervision can be risky but may have advantages, such as real-time continuous learning (monitoring and retraining models) and faster deployments. It is good to know how to automate the CI/CD pipeline without any human supervision in the loop. Note that it is not recommended in many cases as there is a lot of room for error. In some cases, it may be useful – it really depends on your use case and ML system goals. Now, let's look at triggers for full automation. </li>
			</ol>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor198"/>Configuring pipeline triggers for automation</h1>
			<p>In this section, we will <a id="_idIndexMarker713"/>configure three triggers based on artifacts that we have already connected to the pipeline. The triggers we will set up are as follows:</p>
			<ul>
				<li><strong class="bold">Git trigger</strong>: For making code changes to the master branch.</li>
				<li><strong class="bold">Artifactory trigger</strong>: For when a new model or artifact is created or trained. </li>
				<li><strong class="bold">Schedule trigger</strong>: A weekly periodic trigger. </li>
			</ul>
			<p>Let's look at each of these pipeline triggers in detail.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor199"/>Setting up a Git trigger </h2>
			<p>In teams, it is common <a id="_idIndexMarker714"/>to set a trigger for deployment when code changes are made to a certain branch in the repository. For example, when code changes are made to the <strong class="bold">master</strong> branch or the <strong class="bold">develop</strong> branch, CI/CD pipelines are triggered to deploy the application to the PROD or DEV TEST environments, respectively. When a pull request is made to merge code in the <strong class="bold">master</strong> or <strong class="bold">develop</strong> branch, the QA expert or product manager accepts the pull request in order to merge with the respective branch. Upon making code changes to the master or develop branch, a trigger is generated to create a new release in the pipeline. Follow these steps to create a trigger for the master branch for the experiment, as shown in the following screenshot: </p>
			<ol>
				<li value="1">Go to the <strong class="bold">Pipelines</strong> | <strong class="bold">Releases</strong> section and select your pipeline (for example, <strong class="bold">Port Weather ML pipeline</strong>). Then, click <strong class="bold">Edit</strong>:<div id="_idContainer135" class="IMG---Figure"><img src="image/B16572_10_13.jpg" alt="Figure 10.13 – Setting up continuous deployment triggers (git triggers)&#13;&#10;"/></div><p class="figure-caption">Figure 10.13 – Setting up continuous deployment triggers (git triggers)</p><p>You will be directed to a portal <a id="_idIndexMarker715"/>where you can edit your pipeline (for example, <strong class="bold">Port Weather ML pipeline</strong>) so that you can configure continuous deployment triggers for your artifacts. </p></li>
				<li>To set up a Git trigger for the master branch (when changes are made to the master branch, a new release is triggered), click on the <strong class="bold">Trigger</strong> icon (thunder icon) and move the on/off switch button from disabled to enabled. This will enable the continuous deployment trigger. </li>
				<li>Lastly, add a branch filter and point to the branch that you would like to set up a trigger for – in this case, the master branch – as shown in the preceding screenshot. Save your changes to set up the Git trigger. </li>
			</ol>
			<p>By implementing these steps, you have set up a continuous deployment trigger to initiate a new release when changes are made to the master branch.  </p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor200"/>Setting up an Artifactory trigger</h2>
			<p>For ML applications, Artifactory triggers are quite useful. When new models or artifacts (files) have been <a id="_idIndexMarker716"/>trained by the Data Scientists in the team, it is useful to deploy those models to a test environment, and <a id="_idIndexMarker717"/>then eventually to production if they are promising or better than the previous models or trigger. Follow these steps to set up a continuous deployment trigger that will create a new release for the pipeline when a new model is trained, as shown in the following screenshot:</p>
			<ol>
				<li value="1">Go to the <strong class="bold">Pipelines</strong> | <strong class="bold">Releases</strong> section and select your pipeline (for example, <strong class="bold">Port Weather ML pipeline</strong>). Then, click <strong class="bold">Edit</strong>:<div id="_idContainer136" class="IMG---Figure"><img src="image/B16572_10_14.jpg" alt="Figure 10.14 – Setting up CD for Artifact triggers (SVC model)&#13;&#10;"/></div><p class="figure-caption">Figure 10.14 – Setting up CD for Artifact triggers (SVC model)</p><p>Upon clicking the <strong class="bold">Edit</strong> button, you will be directed to a portal where you can edit your pipeline, as shown in the preceding screenshot.</p></li>
				<li>To set up an Artifact trigger for your model, click on <a id="_idIndexMarker718"/>your choice of model, such as <strong class="bold">Support Vector Classifier</strong> (<strong class="bold">SVC</strong>), and enable <strong class="bold">Continuous deployment trigger</strong>. In the preceding screenshot , a trigger has been enabled for a model (SVC). Whenever a new SVC model is trained and registered to the model registry that's connected to your Azure Machine Learning workspace, a new release will be triggered to deploy the new model via the pipeline. </li>
				<li>Lastly, save your changes to set up an Artifact trigger for the SVC model. You have a continuous <a id="_idIndexMarker719"/>deployment trigger <a id="_idIndexMarker720"/>set up to initiate a new release when a new SVC model is trained and registered on your Azure Machine Learning workspace. The pipeline will fetch the new model and deploy it to the DEV TEST and PROD environments.  </li>
			</ol>
			<p>By implementing these steps, you have a continuous deployment trigger set up to initiate a new pipeline release when a new artifact is created or registered in your Azure Machine Learning workspace. </p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor201"/>Setting up a Schedule trigger</h2>
			<p>Now, we will <a id="_idIndexMarker721"/>set up a time-specific Schedule trigger for the pipeline. This kind of trigger is useful for keeping the system <a id="_idIndexMarker722"/>healthy and updated via periodic new releases. Schedule triggers create new releases at set time intervals. We will set up a Schedule trigger for every week on Monday at 11:00 A.M. At this time, a new release is triggered to deploy the recent version of the SVC model to both the DEV TEST and PROD environments. Follow these steps to set up a Schedule trigger: </p>
			<ol>
				<li value="1">Go to the <strong class="bold">Pipelines</strong> | <strong class="bold">Releases</strong> section and select your pipeline (for example, <strong class="bold">Port Weather ML pipeline</strong>). Then, click <strong class="bold">Edit</strong>, as shown in the following screenshot:<div id="_idContainer137" class="IMG---Figure"><img src="image/B16572_10_15.jpg" alt="Figure 10.15 – Setting up a schedule trigger&#13;&#10;"/></div><p class="figure-caption">Figure 10.15 – Setting up a schedule trigger</p><p>Upon clicking the <strong class="bold">Edit</strong> button, you will be directed to a portal where you can edit your pipeline. </p></li>
				<li>To set up a scheduled trigger for the pipeline, click on <strong class="bold">Schedule Set</strong> and enable <strong class="bold">Scheduled release trigger</strong>. Then, select times when you want to trigger <a id="_idIndexMarker723"/>a release. For <a id="_idIndexMarker724"/>example, in the preceding screenshot, a trigger has been enabled for every week on Monday at 11:00 A.M. </li>
				<li>Lastly, save your changes to set up a set up Schedule trigger trigger for the pipeline.</li>
			</ol>
			<p>By implementing these steps, you have a continuous deployment trigger set up to initiate a new pipeline release at a set time interval. </p>
			<p>Congratulations on setting up Git, Artifact, and Schedule triggers. These triggers enable full automation for the pipeline. The pipeline has been set up and can now successfully test and deploy models. You also have the option to semi-automate the pipeline by adding <a id="_idIndexMarker725"/>a human or <strong class="bold">Quality Assurance</strong> (<strong class="bold">QA</strong>) expert to approve each stage in the pipeline. For example, after the test stage, an approval can be made by the QA expert so that you can start production deployment if everything was successful in the test stage. As a QA expert, it is <a id="_idIndexMarker726"/>vital to monitor your CI/CD pipeline. In <a id="_idIndexMarker727"/>the next section, we'll look at some best practices when it comes to managing pipeline releases. </p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor202"/>Pipeline release management</h1>
			<p>Releases in the CI/CD pipelines allow your team to automate fully and continuously deliver software to your <a id="_idIndexMarker728"/>customers faster and with lower risk. Releases allow you to test and deliver your software in multiple stages of production or set up semi-automated processes with approvals and on-demand deployments. It is vital to monitor and manage these releases. We can manage releases by accessing the pipeline from <strong class="bold">Pipelines</strong> | <strong class="bold">Releases</strong> and selecting our CI/CD pipeline (for example, <strong class="bold">Port Weather ML Pipeline</strong>), as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B16572_10_16.jpg" alt="Figure 10.16 – Pipeline Release Management &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.16 – Pipeline Release Management </p>
			<p>Here, you can keep track of all the releases and their history and perform operations for each release, such as redeploying, abandoning, checking logs, and so on. You can see the releases shown in the following screenshot. By clicking on individual releases (for example, <strong class="bold">Release 4</strong>), we can check which model and artifacts were deployed in the release and how the release was triggered (manual or using automatic triggers). It provides end-to-end traceability of the pipeline. This information is crucial for the governance and compliance of the ML system:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B16572_10_17.jpg" alt="Figure 10.17 – Inspecting a release  &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – Inspecting a release  </p>
			<p>Prevention is better <a id="_idIndexMarker729"/>than finding a cure. Just as we conduct incident reviews after a failure, it helps to prevent possible failures by conducting post-release reviews after deploying a new service or model. A thorough analysis of the release after deployment can enable us to understand answers to critical questions, such as the following:</p>
			<ul>
				<li>What works and what doesn't during a release?</li>
				<li>Were there any roadblocks with the release? </li>
				<li>Are there any unclear processes that you could solve and make more explainable for the next release?</li>
			</ul>
			<p>Thoroughly understanding these questions post-release can help you improve and iterate on your strategy and develop better release management practices.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor203"/>Toward continuous monitoring</h1>
			<p>With that, we have <a id="_idIndexMarker730"/>set up a fully automated and robust pipeline. So far, we have successfully implemented the deployment part or module in the MLOps workflow (as we discussed in <a href="B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Fundamentals of MLOps Workflow</em>). It is vital to monitor the deployed ML model and service in real time to understand the system's performance, as this helps maximize its business impact. One of the reasons ML projects are failing to bring value to businesses is because of the lack of trust and transparency in their decision making. Building trust into AI systems is vital these days, especially if we wish to adapt to the changing environment, regulatory frameworks, and dynamic customer needs. Continuous monitoring will enable us to monitor the ML system's performance and build trust into AIs to maximize our business value. In the next chapter, we will learn about the monitoring module in the MLOps workflow and how it facilitates continuous monitoring.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor204"/>Summary</h1>
			<p>In this chapter, we covered the essential fundamentals of the CI/CD pipeline and production environment. We did some hands-on implementation to set up the production infrastructure and then set up processes in the production environment of the pipeline for production deployments. We tested the production-ready pipeline to test its robustness. To take things to the next level, we fully automated the CI/CD pipeline using various triggers. Lastly, we looked at release management practices and capabilities and discussed the need to continuous monitor the ML system. A key takeaway is that <em class="italic">the pipeline is the product, not the model</em>. It is better to focus on building a robust and efficient pipeline more than building the best model. </p>
			<p>In the next chapter, we will explore the MLOps workflow monitoring module and learn more about the game-changing explainable monitoring framework. </p>
		</div>
	</body></html>