- en: Chapter 4. Detecting and Tracking Different Body Parts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how to detect and track different body
    parts in a live video stream. We will start by discussing the face detection pipeline
    and how it's built from the ground up. We will learn how to use this framework
    to detect and track other body parts, such as eyes, ears, mouth, and nose.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use Haar cascades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are integral images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is adaptive boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect and track faces in a live video stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect and track eyes in a live video stream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to automatically overlay sunglasses on top of a person's face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect ears, nose, and mouth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect pupils using shape analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Haar cascades to detect things
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we say Haar cascades, we are actually talking about cascade classifiers
    based on Haar features. To understand what this means, we need to take a step
    back and understand why we need this in the first place. Back in 2001, Paul Viola
    and Michael Jones came up with a very effective object detection method in their
    seminal paper. It has become one of the major landmarks in the field of machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: In their paper, they have described a machine learning technique where a boosted
    cascade of simple classifiers is used to get an overall classifier that performs
    really well. This way, we can circumvent the process of building a single complex
    classifier that performs with high accuracy. The reason this is so amazing is
    because building a robust single-step classifier is a computationally intensive
    process. Besides, we need a lot of training data to build such a classifier. The
    model ends up becoming complex and the performance might not be up to the mark.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we want to detect an object like, say, a pineapple. To solve this,
    we need to build a machine learning system that will learn what a pineapple looks
    like. It should be able to tell us if an unknown image contains a pineapple or
    not. To achieve something like this, we need to train our system. In the realm
    of machine learning, we have a lot of methods available to train a system. It's
    a lot like training a dog, except that it won't fetch the ball for you! To train
    our system, we take a lot of pineapple and non-pineapple images, and then feed
    them into the system. Here, pineapple images are called positive images and the
    non-pineapple images are called negative images.
  prefs: []
  type: TYPE_NORMAL
- en: As far as the training is concerned, there are a lot of routes available. But
    all the traditional techniques are computationally intensive and result in complex
    models. We cannot use these models to build a real time system. Hence, we need
    to keep the classifier simple. But if we keep the classifier simple, it will not
    be accurate. The trade off between speed and accuracy is common in machine learning.
    We overcome this problem by building a set of simple classifiers and then cascading
    them together to form a unified classifier that's robust. To make sure that the
    overall classifier works well, we need to get creative in the cascading step.
    This is one of the main reasons why the **Viola-Jones** method is so effective.
  prefs: []
  type: TYPE_NORMAL
- en: Coming to the topic of face detection, let's see how to train a system to detect
    faces. If we want to build a machine learning system, we first need to extract
    features from all the images. In our case, the machine learning algorithms will
    use these features to learn what a face looks like. We use Haar features to build
    our feature vectors. Haar features are simple summations and differences of patches
    across the image. We do this at multiple image sizes to make sure our system is
    scale invariant.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are curious, you can learn more about the formulation at [http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf](http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Once we extract these features, we pass it through a cascade of classifiers.
    We just check all the different rectangular sub-regions and keep discarding the
    ones that don't have faces in them. This way, we arrive at the final answer quickly
    to see if a given rectangle contains a face or not.
  prefs: []
  type: TYPE_NORMAL
- en: What are integral images?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to compute Haar features, we will have to compute the summations
    of many different rectangular regions within the image. If we want to effectively
    build the feature set, we need to compute these summations at multiple scales.
    This is a very expensive process! If we want to build a real time system, we cannot
    spend so many cycles in computing these sums. So we use something called integral
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '![What are integral images?](img/B04554_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To compute the sum of any rectangle in the image, we don''t need to go through
    all the elements in that rectangular area. Let''s say AP indicates the sum of
    all the elements in the rectangle formed by the top left point and the point P
    in the image as the two diagonally opposite corners. So now, if we want to compute
    the area of the rectangle ABCD, we can use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Area of the rectangle ABCD = AC – (AB + AD - AA)*'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we care about this particular formula? As we discussed earlier, extracting
    Haar features includes computing the areas of a large number of rectangles in
    the image at multiple scales. A lot of those computations are repetitive and the
    overall process is very slow. In fact, it is so slow that we cannot afford to
    run anything in real time. That's the reason we use this formulation! The good
    thing about this approach is that we don't have to recalculate anything. All the
    values for the areas on the right hand side of this equation are already available.
    So we just use them to compute the area of any given rectangle and extract the
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and tracking faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV provides a nice face detection framework. We just need to load the cascade
    file and use it to detect the faces in an image. Let''s see how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the above code, it will look something like the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting and tracking faces](img/B04554_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding it better
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a classifier model that can be used to detect the faces in an image.
    OpenCV provides an xml file that can be used for this purpose. We use the function
    `CascadeClassifier` to load the xml file. Once we start capturing the input frames
    from the webcam, we convert it to grayscale and use the function `detectMultiScale`
    to get the bounding boxes for all the faces in the current image. The second argument
    in this function specifies the jump in the scaling factor. As in, if we don't
    find an image in the current scale, the next size to check will be, in our case,
    1.3 times bigger than the current size. The last parameter is a threshold that
    specifies the number of adjacent rectangles needed to keep the current rectangle.
    It can be used to increase the robustness of the face detector.
  prefs: []
  type: TYPE_NORMAL
- en: Fun with faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to detect and track faces, let''s have some fun with it.
    When we capture a video stream from the webcam, we can overlay funny masks on
    top of our faces. It will look something like this next image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fun with faces](img/B04554_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you are a fan of Hannibal, you can try this next one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fun with faces](img/B04554_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the code to see how to overlay the skull mask on top of the
    face in the input video stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like before, we first load the face cascade classifier xml file. The face
    detection steps work as usual. We start the infinite loop and keep detecting the
    face in every frame. Once we know where the face is, we need to modify the coordinates
    a bit to make sure the mask fits properly. This manipulation process is subjective
    and depends on the mask in question. Different masks require different levels
    of adjustments to make it look more natural. We extract the region-of-interest
    from the input frame in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the required region-of-interest, we need to overlay the mask
    on top of this. So we resize the input mask to make sure it fits in this region-of-interest.
    The input mask has a white background. So if we just overlay this on top of the
    region-of-interest, it will look unnatural because of the white background. We
    need to overlay only the skull-mask pixels and the remaining area should be transparent.
  prefs: []
  type: TYPE_NORMAL
- en: So in the next step, we create a mask by thresholding the skull image. Since
    the background is white, we threshold the image so that any pixel with an intensity
    value greater than 180 becomes 0, and everything else becomes 255\. As far as
    the frame region-of-interest is concerned, we need to black out everything in
    this mask region. We can do that by simply using the inverse of the mask we just
    created. Once we have the masked versions of the skull image and the input region-of-interest,
    we just add them up to get the final image.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting eyes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand how to detect faces, we can generalize the concept to
    detect other body parts too. It's important to understand that Viola-Jones framework
    can be applied to any object. The accuracy and robustness will depend on the uniqueness
    of the object. For example, a human face has very unique characteristics, so it's
    easy to train our system to be robust. On the other hand, an object like towel
    is too generic, and there are no distinguishing characteristics as such; so it's
    more difficult to build a robust towel detector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to build an eye detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this program, the output will look something like the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting eyes](img/B04554_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Afterthought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you notice, the program looks very similar to the face detection program.
    Along with loading the face detection cascade classifier, we load the eye detection
    cascade classifier as well. Technically, we don't need to use the face detector.
    But we know that eyes are always on somebody's face. We use this information and
    search for eyes only in the relevant region of interest, that is the face. We
    first detect the face, and then run the eye detector on this sub-image. This way,
    it's faster and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Fun with eyes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know how to detect eyes in an image, let''s see if we can do something
    fun with it. We can do something like what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fun with eyes](img/B04554_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the code to see how to do something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Positioning the sunglasses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like we did earlier, we load the image and detect the eyes. Once we detect
    the eyes, we resize the sunglasses image to fit the current region of interest.
    To create the region of interest, we consider the distance between the eyes. We
    resize the image accordingly and then go ahead to create a mask. This is similar
    to what we did with the skull mask earlier. The positioning of the sunglasses
    on the face is subjective. So you will have to tinker with the weights if you
    want to use a different pair of sunglasses.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting ears
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we know how the pipeline works, let''s just jump into the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the above code on an image, you should see something like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting ears](img/B04554_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Detecting a mouth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting a mouth](img/B04554_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It's time for a moustache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s overlay a moustache on top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![It''s time for a moustache](img/B04554_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Detecting a nose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following program shows how you detect a nose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks something like the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting a nose](img/B04554_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Detecting pupils
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to take a different approach here. Pupils are too generic to take
    the Haar cascade approach. We will also get a sense of how to detect things based
    on their shape. Following is what the output will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting pupils](img/B04554_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see how to build the pupil detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you run this program, you will see the output as shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Deconstructing the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed earlier, we are not going to use Haar cascade to detect pupils.
    If we can''t use a pre-trained classifier, then how are we going to detect the
    pupils? Well, we can use shape analysis to detect the pupils. We know that pupils
    are circular, so we can use this information to detect them in the image. We invert
    the input image and then convert it into grayscale image as shown in the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see here, we can invert an image using the tilde operator. Inverting
    the image is helpful in our case because the pupil is black in color, and black
    corresponds to a low pixel value. We then threshold the image to make sure that
    there are only black and white pixels. Now, we have to find out the boundaries
    of all the shapes. OpenCV provides a nice function to achieve this, that is `findContours`.
    We will discuss more about this in the upcoming chapters. But for now, all we
    need to know is that this function returns the set of boundaries of all the shapes
    that are found in the image.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to identify the shape of the pupil and discard the rest. We
    will use certain properties of the circle to zero-in on this shape. Let's consider
    the ratio of width to height of the bounding rectangle. If the shape is a circle,
    this ratio will be 1\. We can use the function `boundingRect` to obtain the coordinates
    of the bounding rectangle. Let's consider the area of this shape. If we roughly
    compute the radius of this shape and use the formula for the area of the circle,
    then it should be close to the area of this contour. We can use the function `contourArea`
    to compute the area of any contour in the image. So we can use these conditions
    and filter out the shapes. After we do that, we are left with two pupils in the
    image. We can refine it further by limiting the search region to the face or the
    eyes. Since you know how to detect faces and eyes, you can give it a try and see
    if you can get it working for a live video stream.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Haar cascades and integral images. We understood
    how the face detection pipeline is built. We learnt how to detect and track faces
    in a live video stream. We discussed how to use the face detection pipeline to
    detect various body parts like eyes, ears, nose, and mouth. We learnt how to overlay
    masks on top on the input image using the results of body parts detection. We
    used the principles of shape analysis to detect the pupils.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss feature detection and how it can
    be used to understand the image content.
  prefs: []
  type: TYPE_NORMAL
