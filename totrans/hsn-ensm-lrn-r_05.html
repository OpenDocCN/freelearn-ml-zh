<html><head></head><body>
<div class="book" title="Chapter&#xA0;5.&#xA0;The Bare Bones Boosting Algorithms" id="181NK1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. The Bare Bones Boosting Algorithms</h1></div></div></div><p class="calibre7">What do we mean by bare bones boosting algorithms? The boosting algorithm (and its variants) is arguably one of the most important algorithms in the machine learning toolbox. Any data analyst needs to know this algorithm, and eventually the push for higher accuracy invariably drives towards the need for the boosting technique. It has been reported on the <a class="calibre1" href="http://www.kaggle.org">www.kaggle.org</a> forums that boosting algorithms for complex and voluminous data run for several weeks and that most award-winning solutions are based on this. Furthermore, the algorithms run on modern graphical device machines.</p><p class="calibre7">Taking its importance into account, we will study the boosting algorithm in detail here. <span class="strong"><em class="calibre9">Bare bones</em></span> is certainly not a variant of the boosting algorithm. Since the boosting algorithm is one of the very important and vital algorithms, we will first state the algorithm and implement it in a rudimentary fashion, which will show each step of the algorithm in action.</p><p class="calibre7">We will begin with the adaptive boosting algorithm—popularly known and abbreviated as the <span class="strong"><strong class="calibre8">AdaBoost</strong></span> algorithm<a id="id202" class="calibre1"/>—and using very simple and raw code, we will illustrate it for a classification problem. The illustration is carried over a <code class="literal">toy</code> dataset so that the steps can be clearly followed by the reader.</p><p class="calibre7">In the next section, we will extend the classification boosting algorithm to the regression problem. For this problem, the boosting <a id="id203" class="calibre1"/>variant is the famous <span class="strong"><strong class="calibre8">gradient boosting algorithm</strong></span>. An interesting nonlinear regression problem will be improvised through a<a id="id204" class="calibre1"/> series of basic decision trees with a single split, also known as <span class="strong"><strong class="calibre8">stumps</strong></span>. The gradient boosting algorithm will be illustrated for the choice of the squared-error loss function. Variable importance computation will be clarified for the boosting method. The details of the <code class="literal">gbm</code> package will be discussed in the penultimate section of the chapter. The concluding section will carry out a comparison of the bagging, random forests, and boosting methods for a spam dataset. This chapter consists of the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The general boosting algorithm</li><li class="listitem">Adaptive boosting</li><li class="listitem">Gradient boosting<div class="book"><ul class="itemizedlist1"><li class="listitem">Gradient boosting based on stumps</li><li class="listitem">Gradient boosting with squared-error loss</li></ul></div></li><li class="listitem">Variable importance in the boosting technique</li><li class="listitem">Using the gbm package</li><li class="listitem">Comparison of bagging, random forests, and boosting algorithms</li></ul></div></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;The Bare Bones Boosting Algorithms" id="181NK1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec40" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will be using the following libraries in the chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">rpart</code></li><li class="listitem"><code class="literal">gbm</code></li></ul></div></div></div>

<div class="book" title="The general boosting algorithm"><div class="book" id="190862-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec41" class="calibre1"/>The general boosting algorithm</h1></div></div></div><p class="calibre7">The tree-based <a id="id205" class="calibre1"/>ensembles in the previous chapters, <span class="strong"><em class="calibre9">Bagging</em></span> and <span class="strong"><em class="calibre9">Random Forests</em></span>, cover an important extension of the decision trees. However, while bagging provides greater stability by averaging multiple decision trees, the bias persists. This limitation motivated Breiman to sample the covariates at each split point to generate an ensemble of "independent" trees and lay the foundation for random forests. The trees in the random forests can be developed in parallel, as is the case with bagging. The idea of averaging over multiple trees is to ensure the balance between the bias and variance trade-off. Boosting is the third most important extension of the decision trees, and probably the most effective one. It is again based on ensembling homogeneous base learners (in this case, trees), as are the bagging and random forests. The design of the boosting algorithm is completely different though. It is a <span class="strong"><em class="calibre9">sequential</em></span> ensemble method in that the residual/misclassified point of the previous learner is improvised in the next run of the algorithm.</p><p class="calibre7">The general boosting technique consists of a family of algorithms that convert weak learners to provide a final strong learner. In the context of classification problems, a weak learner is a technique that is better than a random guess. That the method converts a weak learning algorithm into a better one is the reason it gets the name <span class="strong"><em class="calibre9">boosting</em></span>. The boosting technique is designed to deliver a strong learner that is closer to the perfect classifier.</p><div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Sub-space <span><img src="../images/00199.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Sub-space <span><img src="../images/00200.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Sub-space <span><img src="../images/00201.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Sub-space <span><img src="../images/00202.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Accuracy</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<span><img src="../images/00203.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">0.75</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<span><img src="../images/00204.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">0.75</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<span><img src="../images/00205.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">0.75</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<span><img src="../images/00206.jpeg" alt="The general boosting algorithm" class="calibre23"/></span>
</td><td class="calibre25">
<p class="calibre22">Q</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">R</p>
</td><td class="calibre25">
<p class="calibre22">0.75</p>
</td></tr></tbody></table></div><p class="calibre7">Table 1 A simple classifier scenario</p><p class="calibre7">A broader<a id="id206" class="calibre1"/> motivation for boosting can be understood through a simple example. Suppose that the random sample of size <span class="strong"><em class="calibre9">n</em></span> is drawn as IID from the sample space <span class="strong"><img src="../images/00207.jpeg" alt="The general boosting algorithm" class="calibre15"/></span>. The distribution of the random sample is assumed to be D. Suppose we have <span class="strong"><em class="calibre9">T=4</em></span> classifiers in <span class="strong"><img src="../images/00208.jpeg" alt="The general boosting algorithm" class="calibre15"/></span>, with the classifiers being used for the truth function <span class="strong"><em class="calibre9">f</em></span>.</p><p class="calibre7">Consider a hypothetical scenario where the sample space <span class="strong"><img src="../images/00209.jpeg" alt="The general boosting algorithm" class="calibre15"/></span> consists of four parts in <span class="strong"><img src="../images/00210.jpeg" alt="The general boosting algorithm" class="calibre15"/></span>, and the four classifiers perform as indicated in previous table. The idea behind the development of the boosting method is to improvise the classifier in a sequential manner. That is, combining the classifier is approached one after another and not all at the same time. Now, the errors of <span class="strong"><img src="../images/00211.jpeg" alt="The general boosting algorithm" class="calibre15"/></span> will be corrected to a new distribution D' with the errors of the classifier being given more weight for the region <span class="strong"><img src="../images/00212.jpeg" alt="The general boosting algorithm" class="calibre15"/></span>. The classifier <span class="strong"><img src="../images/00213.jpeg" alt="The general boosting algorithm" class="calibre15"/></span> will use the distribution D' and its error zone instances in the region <span class="strong"><img src="../images/00214.jpeg" alt="The general boosting algorithm" class="calibre15"/></span> will be given more weight leading to a distribution D''. The boosting method will continue the process for the remaining classifiers and give an overall combiner/ensemble. A pseudo-boosting algorithm (see <a class="calibre1" title="Chapter 2. Bootstrapping" href="part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee">Chapter 2</a> of Zhou (2012)) is summarized in the following:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Step 0: The initial sample distribution is D, and set D1 = D</li><li class="listitem" value="2">
Steps <span class="strong"><img src="../images/00215.jpeg" alt="The general boosting algorithm" class="calibre15"/></span>:
<div class="book"><ul class="itemizedlist1"><li class="listitem"><span class="strong"><img src="../images/00216.jpeg" alt="The general boosting algorithm" class="calibre15"/></span> Dt
</li><li class="listitem"><span class="strong"><img src="../images/00217.jpeg" alt="The general boosting algorithm" class="calibre15"/></span></li><li class="listitem"><span class="strong"><em class="calibre9">Dt+1 = Improve Distribution (Dt,</em></span><span class="strong"><img src="../images/00218.jpeg" alt="The general boosting algorithm" class="calibre15"/></span><span class="strong"><em class="calibre9">)</em></span></li></ul></div></li><li class="listitem" value="3">
Final step: <span class="strong"><img src="../images/00219.jpeg" alt="The general boosting algorithm" class="calibre15"/></span></li></ol><div class="calibre13"/></div><p class="calibre7">The two steps of the algorithm in <span class="strong"><em class="calibre9">Improve distribution</em></span> and <span class="strong"><em class="calibre9">Combine outputs</em></span> clearly need implementable actions. In the next section, we will develop the adaptive boosting method with a clear numerical illustration.</p></div>

<div class="book" title="The general boosting algorithm">
<div class="book" title="Adaptive boosting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl1sec42" class="calibre1"/>Adaptive boosting</h2></div></div></div><p class="calibre7">Schapire and Freund invented<a id="id207" class="calibre1"/> the adaptive boosting method. <span class="strong"><strong class="calibre8">Adaboost</strong></span> is a popular abbreviation of this technique.</p><p class="calibre7">The generic adaptive boosting algorithm is as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Initialize the observation weights uniformly:<div class="mediaobject"><img src="../images/00220.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre26"> </p></li><li class="listitem">For <span class="strong"><em class="calibre9">m</em></span>, classifier <span class="strong"><em class="calibre9">hm</em></span>, from <span class="strong"><em class="calibre9">1</em></span> to <span class="strong"><em class="calibre9">m</em></span> number of passes over with the data, perform the following tasks:<div class="book"><ul class="itemizedlist1"><li class="listitem">
Fit a classifier <span class="strong"><em class="calibre9">hm</em></span> to the training<a id="id208" class="calibre1"/> data using the weights <span class="strong"><img src="../images/00221.jpeg" alt="Adaptive boosting" class="calibre15"/></span></li><li class="listitem">Compute the error for each classifier as follows:<div class="mediaobject"><img src="../images/00222.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre11"> </p></li><li class="listitem">Compute the <span class="strong"><em class="calibre9">voting power</em></span> of the classifier <span class="strong"><em class="calibre9">hm</em></span>:<div class="mediaobject"><img src="../images/00223.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre11"> </p></li><li class="listitem">
Set <span class="strong"><img src="../images/00224.jpeg" alt="Adaptive boosting" class="calibre15"/></span><div class="mediaobject"><img src="../images/00225.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre11"> </p></li></ul></div></li><li class="listitem">Output:<div class="mediaobject"><img src="../images/00226.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre26"> </p></li></ul></div><p class="calibre7">Simply put, the algorithm unfolds as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">
Initially, we start with uniform weights <span class="strong"><img src="../images/00227.jpeg" alt="Adaptive boosting" class="calibre15"/></span> for all observations.
</li><li class="listitem" value="2">
In the next step, we calculate<a id="id209" class="calibre1"/> the weighted error <span class="strong"><img src="../images/00228.jpeg" alt="Adaptive boosting" class="calibre15"/></span> for each of the classifiers under consideration.
</li><li class="listitem" value="3">A classifier (usually stumps, or decision trees with a single split) needs to be selected and the practice is to select the classifier with the maximum accuracy.</li><li class="listitem" value="4">In <span class="strong"><em class="calibre9">Improve distribution and Combine outputs</em></span> case of ties, any accuracy tied classifier is selected.</li><li class="listitem" value="5">Next, the misclassified observations are given more weights and the values that are correctly classified are down-weighted. An important point needs to be recorded here:</li></ol><div class="calibre13"/></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note03" class="calibre1"/>Note</h3><p class="calibre7">In the weights update step, the sum of weights correctly classified as observations will equal the sum of weights of the misclassified observations.</p></div><p class="calibre7">The steps from computing the error of the classifier to the weight updating step are repeated M number of times, and the voting power of each classifier is obtained. For any given observation, we then make the prediction by using the predictions across the M classifiers weighted by their respective voting power and using the sign function as specified in the algorithm.</p><p class="calibre7">As simplified as the algorithm may be, it is a useful exercise to undertake the working of the adaptive boosting <a id="id210" class="calibre1"/>method through a toy dataset. The data and computational approach is taken from the video of Jessica Noss, available at <a class="calibre1" href="https://www.youtube.com/watch?v=gmok1h8wG-Q">https://www.youtube.com/watch?v=gmok1h8wG-Q</a>. The illustration of the adaptive boosting algorithm begins now.</p><p class="calibre7">Consider a toy data set with five triplet points: two explanatory variables and one binary output value. The variables and data can be summarized with <span class="strong"><img src="../images/00229.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, and here we have the data points as <span class="strong"><img src="../images/00230.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, <span class="strong"><img src="../images/00231.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, <span class="strong"><img src="../images/00232.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, <span class="strong"><img src="../images/00233.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, and <span class="strong"><img src="../images/00234.jpeg" alt="Adaptive boosting" class="calibre15"/></span>. The data will be first entered in R and then visualized as a preliminary step:</p><div class="informalexample"><pre class="programlisting">&gt; # ADAPTIVE BOOSTING with a Toy Dataset
&gt; # https://www.youtube.com/watch?v=gmok1h8wG-Q 
&gt; # Jessica Noss
&gt; # The Toy Data
&gt; x1 &lt;- c(1,5,3,1,5)
&gt; x2 &lt;- c(5,5,3,1,1)
&gt; y &lt;- c(1,1,-1,1,1)
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="The TOY Data Depiction")
&gt; text(x1,x2,labels=names(y),pos=1)</pre></div><div class="mediaobject"><img src="../images/00235.jpeg" alt="Adaptive boosting" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: A simple depiction of the toy dataset</p></div></div><p class="calibre11"> </p><p class="calibre7">Stumps are a particular case of a decision tree that has been mentioned in the discussion. Here, we will use the stumps as the base learners. A simple look at the preceding diagram helps us to easily find stumps that have an accuracy higher than a random guess.</p><p class="calibre7">For example, we <a id="id211" class="calibre1"/>can put a stump at <span class="strong"><img src="../images/00236.jpeg" alt="Adaptive boosting" class="calibre15"/></span> and mark all the observations on the left side as positives and those on the right as negatives. In the following program, the points in the green shaded region are positives as predicted by the stumps, and those in the red shaded region are negatives. Similarly, we can use additional stumps at <span class="strong"><img src="../images/00237.jpeg" alt="Adaptive boosting" class="calibre15"/></span> and <span class="strong"><img src="../images/00238.jpeg" alt="Adaptive boosting" class="calibre15"/></span>. The predictions can be swapped for the same stumps too, thanks to <code class="literal">symmetry()</code>. Thus, earlier we put the green shaded region to the left of <span class="strong"><img src="../images/00239.jpeg" alt="Adaptive boosting" class="calibre15"/></span> and predicted the values as positives, and by reversing the order the area on the right side of the stump <span class="strong"><img src="../images/00240.jpeg" alt="Adaptive boosting" class="calibre15"/></span> will be marked as positives. A similar classification is made for the negatives. The task is repeated at stumps <span class="strong"><img src="../images/00241.jpeg" alt="Adaptive boosting" class="calibre15"/></span> and <span class="strong"><img src="../images/00242.jpeg" alt="Adaptive boosting" class="calibre15"/></span>. Using the <code class="literal">par</code>, <code class="literal">plot</code>, <code class="literal">text</code>, and <code class="literal">rect</code> graphical functions, we present visual depictions of these base learners in the following:</p><div class="informalexample"><pre class="programlisting">&gt; # Visualizing the stump models
&gt; windows(height=200,width=300)
&gt; par(mfrow=c(2,3))
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&lt;2")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; plim &lt;- par("usr")
&gt; rect(xleft=2,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 2,ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&lt;4")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=4,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 4,ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&lt;6")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=6,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 6,ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&gt;2")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=2,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 2,ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&gt;4")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=4,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 4,ytop = plim[4],
+      border = "red",col="red",density=20 )
&gt; plot(x1,x2,pch=c("+","+","-","+","+"),cex=2,
+      xlim=c(0,6),ylim=c(0,6),
+      xlab=expression(x[1]),ylab=expression(x[2]),
+      main="Classification with Stump X1&gt;6")
&gt; text(x1,x2,labels=names(y),pos=1)
&gt; rect(xleft=6,ybottom = plim[3],xright = plim[2],ytop = plim[4],
+      border = "green",col="green",density=20 )
&gt; rect(xleft=plim[1],ybottom = plim[3],xright = 6,ytop = plim[4],
+      border = "red",col="red",density=20 )</pre></div><p class="calibre7">The result of the preceding R<a id="id212" class="calibre1"/> program is shown in the following diagram:</p><div class="mediaobject"><img src="../images/00243.jpeg" alt="Adaptive boosting" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Stump classifiers based on X1</p></div></div><p class="calibre11"> </p><p class="calibre7">Note that a similar <a id="id213" class="calibre1"/>classification can be obtained for the variable <span class="strong"><img src="../images/00244.jpeg" alt="Adaptive boosting" class="calibre15"/></span> at the points 2, 4, and 6. Though there is no need to give the complete R program for stumps based on <span class="strong"><img src="../images/00245.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, we simply produce the output in the following diagram. The program is available in the code bundle. The stumps based on <span class="strong"><img src="../images/00246.jpeg" alt="Adaptive boosting" class="calibre15"/></span> will be ignored in the rest of the discussion:</p><div class="mediaobject"><img src="../images/00247.jpeg" alt="Adaptive boosting" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Stump classifiers based on X2</p></div></div><p class="calibre11"> </p><p class="calibre7">The choice<a id="id214" class="calibre1"/> of the stump based on <span class="strong"><img src="../images/00248.jpeg" alt="Adaptive boosting" class="calibre15"/></span> leads to a few misclassifications, and we can see that the observations P1, P4, and P3 are correctly classified while P2 and P5 are misclassified. The predictions based on this stump can then be put as (1,-1,-1,1,-1). The stump based on <span class="strong"><img src="../images/00249.jpeg" alt="Adaptive boosting" class="calibre15"/></span> classifies points P1 and P4 correctly, while P2, P3, and P5 are misclassified, and the prediction in vector form here is (1,-1,1,1,-1). The six models considered here will be denoted in the R program by M1, M2, …, M6, and in terms of the algorithm specified earlier, we have <span class="strong"><img src="../images/00250.jpeg" alt="Adaptive boosting" class="calibre15"/></span>. Similarly, we have predictions for the other four stumps and we enter them in R, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # The Simple Stump Models
&gt; M1 &lt;- c(1,-1,-1,1,-1)   # M1 = X1&lt;2 predicts 1, else -1
&gt; M2 &lt;- c(1,-1,1,1,-1)    # M2 = X1&lt;4 predicts 1, else -1
&gt; M3 &lt;- c(1,1,1,1,1)      # M3 = X1&lt;6 predicts 1, else -1
&gt; M4 &lt;- c(-1,1,1,-1,1)    # M4 = X1&gt;2 predicts 1, else -1;M4=-1*M1
&gt; M5 &lt;- c(-1,1,-1,-1,1)   # M5 = X1&gt;4 predicts 1, else -1;M5=-1*M2
&gt; M6 &lt;- c(-1,-1,-1,-1,-1) # M6 = X1&gt;6 predicts 1, else -1;M6=-1*M3</pre></div><p class="calibre7">With the predictions given by the six models <code class="literal">M1-M6</code>, we can compare them with the true labels in <code class="literal">y</code> and see which<a id="id215" class="calibre1"/> observations are misclassified in each of these models:</p><div class="informalexample"><pre class="programlisting">&gt; # Stem Model Errors
&gt; Err_M1 &lt;- M1!=y
&gt; Err_M2 &lt;- M2!=y
&gt; Err_M3 &lt;- M3!=y
&gt; Err_M4 &lt;- M4!=y
&gt; Err_M5 &lt;- M5!=y
&gt; Err_M6 &lt;- M6!=y
&gt; # Their Misclassifications
&gt; rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)
          P1    P2    P3    P4    P5
Err_M1 FALSE  TRUE FALSE FALSE  TRUE
Err_M2 FALSE  TRUE  TRUE FALSE  TRUE
Err_M3 FALSE FALSE  TRUE FALSE FALSE
Err_M4  TRUE FALSE  TRUE  TRUE FALSE
Err_M5  TRUE FALSE FALSE  TRUE FALSE
Err_M6  TRUE  TRUE FALSE  TRUE  TRUE</pre></div><p class="calibre7">Thus, the values of <code class="literal">TRUE</code> mean that the column named points is misclassified in the row named model. The weights <span class="strong"><img src="../images/00251.jpeg" alt="Adaptive boosting" class="calibre15"/></span> are initialized and the weighted errors <span class="strong"><img src="../images/00252.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, are computed for each of the models in the following R block:</p><div class="informalexample"><pre class="programlisting">&gt; # ROUND 1
&gt; # Weighted Error Computation
&gt; weights_R1 &lt;- rep(1/length(y),length(y)) #Initializaing the weights
&gt; Err_R1 &lt;- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%
+   weights_R1
&gt; Err_R1 # Error rate
       [,1]
Err_M1  0.4
Err_M2  0.6
Err_M3  0.2/
Err_M4  0.6
Err_M5  0.4
Err_M6  0.8</pre></div><p class="calibre7">Since the error corresponding to Model 3, or <span class="strong"><img src="../images/00253.jpeg" alt="Adaptive boosting" class="calibre15"/></span>, is the minimum, we select it first and calculate the voting power <span class="strong"><img src="../images/00254.jpeg" alt="Adaptive boosting" class="calibre15"/></span> assignable to it as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # The best classifier error rate
&gt; err_rate_r1 &lt;- min(Err_R1)
&gt; alpha_3 &lt;- 0.5*log((1-err_rate_r1)/err_rate_r1)
&gt; alpha_3
[1] 0.6931472</pre></div><p class="calibre7">Consequently, the boosting algorithm steps state that <span class="strong"><img src="../images/00255.jpeg" alt="Adaptive boosting" class="calibre15"/></span> gives us the required predictions:</p><div class="informalexample"><pre class="programlisting">&gt; alpha_3*M3
[1] 0.6931472 0.6931472 0.6931472 0.6931472 0.6931472
&gt; sign(alpha_3*M3)
[1] 1 1 1 1 1</pre></div><p class="calibre7">The central <a id="id216" class="calibre1"/>observation, <code class="literal">P3</code>, remains misclassified and so we proceed to the next step.</p><p class="calibre7">Now we need to update the weights <span class="strong"><img src="../images/00256.jpeg" alt="Adaptive boosting" class="calibre15"/></span> and for the classification problem the rule in a simplified form is given using:</p><div class="mediaobject"><img src="../images/00257.jpeg" alt="Adaptive boosting" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Consequently, we need a function that will take the weight of a previous run, the error rate, and the misclassifications by the model as inputs and then return them as the updated weights that incorporated the preceding formula. We define such a function as follows:</p><div class="informalexample"><pre class="programlisting"> 
&gt; # Weights Update Formula and Function
&gt; Weights_update &lt;- function(weights,error,error_rate){
+   weights_new &lt;- NULL
+   for(i in 1:length(weights)){
+     if(error[i]==FALSE) weights_new[i] &lt;- 0.5*weights[i]/(1-error_rate)
+     if(error[i]==TRUE) weights_new[i] &lt;- 0.5*weights[i]/error_rate
+   }
+   return(weights_new)
+ }</pre></div><p class="calibre7">Now, we will update the weights and calculate the error for each of the six models:</p><div class="informalexample"><pre class="programlisting">&gt; # ROUND 2
&gt; # Update the weights and redo the analyses
&gt; weights_R2 &lt;- Weights_update(weights=weights_R1,error=Err_M3,
+                              error_rate=err_rate_r1)
&gt; Err_R2 &lt;- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%
+   weights_R2
&gt; Err_R2 # Error rates
       [,1]
Err_M1 0.25
Err_M2 0.75
Err_M3 0.50
Err_M4 0.75
Err_M5 0.25
Err_M6 0.50</pre></div><p class="calibre7">Here, models <code class="literal">M1</code> and <code class="literal">M5</code> have equal error rates<a id="id217" class="calibre1"/> with the new weights, and we simply choose Model 1, calculate its voting power, and predict based on the updated model:</p><div class="informalexample"><pre class="programlisting">&gt; err_rate_r2 &lt;- min(Err_R2)
&gt; alpha_1 &lt;- 0.5*log((1-err_rate_r2)/err_rate_r2)
&gt; alpha_1
[1] 0.5493061
&gt; alpha_3*M3+alpha_1*M1
[1] 1.242453 0.143841 0.143841 1.242453 0.143841
&gt; sign(alpha_3*M3+alpha_1*M1)
[1] 1 1 1 1 1</pre></div><p class="calibre7">Since the point <code class="literal">P3</code> is still misclassified, we proceed with the iterations and apply the cycle once more:</p><div class="informalexample"><pre class="programlisting">&gt; # ROUND 3
&gt; # Update the weights and redo the analyses
&gt; weights_R3 &lt;- Weights_update(weights=weights_R2,error=Err_M1,
+                              error_rate=err_rate_r2)
&gt; Err_R3 &lt;- rbind(Err_M1,Err_M2,Err_M3,Err_M4,Err_M5,Err_M6)%*%
+   weights_R3
&gt; Err_R3 # Error rates
            [,1]
Err_M1 0.5000000
Err_M2 0.8333333
Err_M3 0.3333333
Err_M4 0.5000000
Err_M5 0.1666667
Err_M6 0.6666667
&gt; err_rate_r3 &lt;- min(Err_R3)
&gt; alpha_5 &lt;- 0.5*log((1-err_rate_r3)/err_rate_r3)
&gt; alpha_5
[1] 0.804719
&gt; alpha_3*M3+alpha_1*M1+alpha_5*M5
[1]  0.4377344  0.9485600 -0.6608779  0.4377344  0.9485600
&gt; sign(alpha_3*M3+alpha_1*M1+alpha_5*M5)
[1]  1  1 -1  1  1</pre></div><p class="calibre7">Now the classification is perfect and after three iterations, we don't have any misclassifications or errors. The purpose of the programming in this section was to demonstrate the steps in the adaptive boosting algorithm in an elementary way. In the next section, we will look at<a id="id218" class="calibre1"/> the <span class="strong"><em class="calibre9">gradient boosting</em></span> technique.</p></div></div>

<div class="book" title="The general boosting algorithm">
<div class="book" title="Gradient boosting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl1sec43" class="calibre1"/>Gradient boosting</h2></div></div></div><p class="calibre7">The adaptive boosting method<a id="id219" class="calibre1"/> can't be applied to the regression problem since it is constructed to address the classification problem. The gradient boosting method can be used for both the classification and regression problems with suitable loss functions. In fact, the use of gradient boosting methods goes beyond these two standard problems. The technique originated from some of Breiman's observations and developed into regression problems by Freidman (2000). We will take the rudimentary code explanation in the next section without even laying out the algorithm. After the setup is clear, we will formally state the boosting algorithm for the squared-error loss function in the following subsection and create a new function implementing the algorithm.</p><p class="calibre7">The following diagram is a depiction of the standard sine wave function. It is clearly a nonlinear relationship. Without explicitly using sine transformations, we will see the use of the boosting algorithm to learn this function. Of course, we need simple regression stumps and we begin with a simple function, <code class="literal">getNode</code>, that will give us the desired split:</p><div class="mediaobject"><img src="../images/00258.jpeg" alt="Gradient boosting" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Can boosting work for nonlinear sine data?</p></div></div><p class="calibre11"> </p><div class="book" title="Building it from scratch"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch05lvl2sec29" class="calibre1"/>Building it from scratch</h3></div></div></div><p class="calibre7">In the previous section, we used<a id="id220" class="calibre1"/> simple classification stumps. In that example, a simple visual inspection sufficed to identify the stumps, and we quickly obtained 12 classification stumps. For the regression problem, we will first define a <code class="literal">getNode</code> function, which is a slight modification of the function defined in Chapter 9 of Tattar (2017). The required notation is first set up.</p><p class="calibre7">Suppose we have n pairs of data points <span class="strong"><img src="../images/00259.jpeg" alt="Building it from scratch" class="calibre15"/></span> and we are trying to learn the relationship <span class="strong"><img src="../images/00260.jpeg" alt="Building it from scratch" class="calibre15"/></span>, where the form of <span class="strong"><em class="calibre9">f </em></span>is completely unknown to us.</p><p class="calibre7">For the regression tree, the split criteria are rather straightforward. For data split by an x-value, we calculate the sum of mean difference squares of <span class="strong"><em class="calibre9">ys</em></span> in each of the partitioned part and then add them up. The split criteria are chosen as that x-value. This maximizes the sum of the mean difference squares in the variable of interest. The R function, <code class="literal">getNode</code>, implements this thinking:</p><div class="informalexample"><pre class="programlisting">&gt; getNode &lt;- function(x,y)	{
+   xu &lt;- sort(unique(x),decreasing=TRUE)
+   ss &lt;- numeric(length(xu)-1)
+   for(i in 1:length(ss))	{
+     partR &lt;- y[x&gt;xu[i]]
+     partL &lt;- y[x&lt;=xu[i]]
+     partRSS &lt;- sum((partR-mean(partR))^2)
+     partLSS &lt;- sum((partL-mean(partL))^2)
+     ss[i] &lt;- partRSS + partLSS
+   }
+   xnode &lt;- xu[which.min(ss)]
+   minss &lt;- min(ss)
+   pR &lt;- mean(y[x&gt;xnode])
+   pL &lt;- mean(y[x&lt;=xnode])
+   return(list(xnode=xnode,yR=pR,yL=pL))
+ }</pre></div><p class="calibre7">The first step in the <code class="literal">getNode</code> function is finding the unique values of <code class="literal">x</code> and then sorting them in decrease(ing) order. For the unique values, we calculate the sum of squares through a for loop. The first step in the loop is to partition the data in right and left parts.</p><p class="calibre7">The sum of mean difference <a id="id221" class="calibre1"/>squares is calculated in each of the partitions for a specific unique value, and then summed up to get the overall residual sum of squares.</p><p class="calibre7">We then obtain the value of <code class="literal">x</code>, which leads to the least residual sum of squares. The prediction in the partitioned regions is the mean of the y-values in those regions.</p><p class="calibre7">The <code class="literal">getNode</code> function closes by returning the split value of <code class="literal">x</code>, and the predictions for the right and left partitions. We are now ready to create regression stumps.</p><p class="calibre7">The sine wave data is first easily created and we allow the x-values to range in the interval <span class="strong"><img src="../images/00261.jpeg" alt="Building it from scratch" class="calibre15"/></span>. The y-value is simply the sin function applied on the x vector:</p><div class="informalexample"><pre class="programlisting">&gt; # Can Boosting Learn the Sine Wave!
&gt; x &lt;- seq(0,2*pi,pi/20)
&gt; y &lt;- sin(x)
&gt; windows(height=300,width=100)
&gt; par(mfrow=c(3,1))
&gt; plot(x,y,"l",col="red",main="Oh My Waves!")</pre></div><p class="calibre7">The result of the preceding display will be <span class="strong"><em class="calibre9">Figure 1</em></span>. We proceed to obtain the first split of data and then display the mean of the right and left partitions on the graph. The residuals will be from the sine wave and they are also put on the same display, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; first_split &lt;- getNode(x,y)
&gt; first_split
$xnode
[1] 3.141593
$yR
[1] -0.6353102
$yL
[1] 0.6050574</pre></div><p class="calibre7">Now, our first split point occurs at <code class="literal">x</code> value of, <span class="strong"><img src="../images/00262.jpeg" alt="Building it from scratch" class="calibre15"/></span> here, <code class="literal">3.141593</code>. The prediction for the right side of the split point is <code class="literal">-0.6353102</code> and for the left side it is <code class="literal">0.6050574</code>. The predictions are plotted on the same display using the segments function:</p><div class="informalexample"><pre class="programlisting">&gt; segments(x0=min(x),y0=first_split$yL,
+          x1=first_split$xnode,y1=first_split$yL)
&gt; segments(x0=first_split$xnode,y0=first_split$yR,
+          x1=max(x),y1=first_split$yR)</pre></div><p class="calibre7">Now, the predictions <a id="id222" class="calibre1"/>are easy to obtain here and a simple <code class="literal">ifelse</code> function helps in calculating them. The deviation from the sine wave is the residuals, and we calculate that the first set of residuals and <code class="literal">summary</code> function give the brief of the residual values:</p><div class="informalexample"><pre class="programlisting">&gt; yfit1 &lt;- ifelse(x&lt;first_split$xnode,first_split$yL,first_split$yR)
&gt; GBFit &lt;- yfit1
&gt; segments(x0=x,x1=x,y0=y,y1=yfit1)
&gt; first_residuals &lt;- y-yfit1
&gt; summary(first_residuals)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.60506 -0.25570  0.04752  0.03025  0.32629  0.63531 </pre></div><p class="calibre7">The first step in the prediction is saved in the <code class="literal">GBFit</code> object and the difference between the fit and predictions is found in the <code class="literal">first_residuals</code> vector. This completes the first iteration of the gradient boosting algorithm. The residuals of the first iteration will become the regressand/output variable for the second iteration. Using the <code class="literal">getNode</code> function, we carry out the second iteration, which mimics the earlier set of code: </p><div class="informalexample"><pre class="programlisting">&gt; second_split &lt;- getNode(x,first_residuals)
&gt; plot(x,first_residuals,"l",col="red",main="The Second Wave!")
&gt; segments(x0=min(x),y0=second_split$yL,
+          x1=second_split$xnode,y1=second_split$yL)
&gt; segments(x0=second_split$xnode,y0=second_split$yR,
+          x1=max(x),y1=second_split$yR)
&gt; yfit2 &lt;- ifelse(x&lt;second_split$xnode,second_split$yL,second_split$yR)
&gt; GBFit &lt;- GBFit+yfit2
&gt; segments(x0=x,x1=x,y0=first_residuals,y1=yfit2)
&gt; second_residuals &lt;- first_residuals-yfit2
&gt; summary(second_residuals)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.51678 -0.24187 -0.02064 -0.01264  0.25813  0.56715 </pre></div><p class="calibre7">An important difference<a id="id223" class="calibre1"/> here is that we update the prediction not by averaging but by adding. Note that we are modeling the residual of the first step and hence the remaining part of the residual explained by the next fitting needs to be added and not averaged. What is the range of residuals? The reader is advised to compare the residual values with earlier iterations. A similar extension is carried out for the third iteration: </p><div class="informalexample"><pre class="programlisting">&gt; third_split &lt;- getNode(x,second_residuals)
&gt; plot(x,second_residuals,"l",col="red",main="The Third Wave!")
&gt; segments(x0=min(x),y0=third_split$yL,
+          x1=third_split$xnode,y1=third_split$yL)
&gt; segments(x0=third_split$xnode,y0=third_split$yR,
+          x1=max(x),y1=third_split$yR)
&gt; yfit3 &lt;- ifelse(x&lt;third_split$xnode,third_split$yL,third_split$yR)
&gt; GBFit &lt;- GBFit+yfit3
&gt; segments(x0=x,x1=x,y0=second_residuals,y1=yfit3)
&gt; third_residuals &lt;- second_residuals-yfit3
&gt; summary(third_residuals)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.47062 -0.27770 -0.03927 -0.01117  0.18196  0.61331 </pre></div><p class="calibre7">All of the visual display is shown in the following diagram:</p><div class="mediaobject"><img src="../images/00263.jpeg" alt="Building it from scratch" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Three iterations of the gradient boosting algorithm</p></div></div><p class="calibre11"> </p><p class="calibre7">Obviously, we can't keep on carrying<a id="id224" class="calibre1"/> out the iterations in a detailed execution every time and looping is important. The code is kept in a block and 22 more iterations are performed. The output at the end of each iteration is depicted in the diagram and we put them all in an external file, <code class="literal">Sine_Wave_25_Iterations.pdf</code>:</p><div class="informalexample"><pre class="programlisting">&gt; pdf("Sine_Wave_25_Iterations.pdf")
&gt; curr_residuals &lt;- third_residuals
&gt; for(j in 4:25){
+   jth_split &lt;- getNode(x,curr_residuals)
+   plot(x,curr_residuals,"l",col="red",main=paste0(c("The ", j, "th Wave!")))
+   segments(x0=min(x),y0=jth_split$yL,
+            x1=jth_split$xnode,y1=jth_split$yL)
+   segments(x0=jth_split$xnode,y0=jth_split$yR,
+            x1=max(x),y1=jth_split$yR)
+   yfit_next &lt;- ifelse(x&lt;jth_split$xnode,jth_split$yL,jth_split$yR)
+   GBFit &lt;- GBFit+yfit_next
+   segments(x0=x,x1=x,y0=curr_residuals,y1=yfit_next)
+   curr_residuals &lt;- curr_residuals-yfit_next
+ }
&gt; dev.off()
&gt; summary(curr_residuals)
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-0.733811 -0.093432  0.008481 -0.001632  0.085192  0.350122 </pre></div><p class="calibre7">Following the 25 iterations, we have <a id="id225" class="calibre1"/>an overall fit in <code class="literal">GBFit</code> and we can plot this against the actual y values to see how well the gradient boosting algorithm has performed:</p><div class="informalexample"><pre class="programlisting">&gt; plot(y,GBFit,xlab="True Y",ylab="Gradient Boosting Fit")</pre></div><div class="mediaobject"><img src="../images/00264.jpeg" alt="Building it from scratch" class="calibre10"/><div class="caption"><p class="calibre14">Figure 6: Gradient fit versus actual sine data</p></div></div><p class="calibre11"> </p><p class="calibre7">The fit is reasonably <a id="id226" class="calibre1"/>good for a nonlinear model. The approach was to get a clear understanding of the gradient boosting algorithm. A more general form of the boosting algorithm is discussed and developed in the next subsection.</p></div><div class="book" title="Squared-error loss function"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch05lvl2sec30" class="calibre1"/>Squared-error loss function</h3></div></div></div><p class="calibre7">Denote the data by <span class="strong"><img src="../images/00265.jpeg" alt="Squared-error loss function" class="calibre15"/></span>, and fix the number of iterations/trees as a number <span class="strong"><em class="calibre9">B</em></span>. Choose a shrinkage factor <span class="strong"><img src="../images/00266.jpeg" alt="Squared-error loss function" class="calibre15"/></span> and tree <a id="id227" class="calibre1"/>depth <span class="strong"><em class="calibre9">d</em></span>. The gradient boosting algorithm based on the squared-error loss function is stated here briefly. See Algorithm 17.2 of Efron and Hastie (2016), as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">
Initialize residuals <span class="strong"><img src="../images/00267.jpeg" alt="Squared-error loss function" class="calibre15"/></span> and the gradient boosting prediction as <span class="strong"><img src="../images/00268.jpeg" alt="Squared-error loss function" class="calibre15"/></span></li><li class="listitem">
For <span class="strong"><img src="../images/00269.jpeg" alt="Squared-error loss function" class="calibre15"/></span>:
<div class="book"><ul class="itemizedlist1"><li class="listitem">
Fit a regression tree of depth <span class="strong"><em class="calibre9">d</em></span> for the data <span class="strong"><img src="../images/00270.jpeg" alt="Squared-error loss function" class="calibre15"/></span></li><li class="listitem">
Obtain the predicted values as <span class="strong"><img src="../images/00271.jpeg" alt="Squared-error loss function" class="calibre15"/></span></li><li class="listitem">
Update the boosting prediction by <span class="strong"><img src="../images/00272.jpeg" alt="Squared-error loss function" class="calibre15"/></span></li><li class="listitem">
Update the residuals <span class="strong"><img src="../images/00273.jpeg" alt="Squared-error loss function" class="calibre15"/></span></li></ul></div></li><li class="listitem">
Return the sequence of functions <span class="strong"><img src="../images/00274.jpeg" alt="Squared-error loss function" class="calibre15"/></span></li></ul></div><p class="calibre7">Now, we will define a function <code class="literal">GB_SqEL</code>, which will implement the gradient boosting algorithm driven by the squared-error loss function. The function must be provided with five arguments: <code class="literal">y</code> and <code class="literal">x </code>will <a id="id228" class="calibre1"/>constitute the data, <code class="literal">depth</code> will specify the depth of the trees (that is, the number of splits in the regression tree), <code class="literal">iter</code> for the number of iterations, and <code class="literal">shrinkage</code> is the <span class="strong"><img src="../images/00275.jpeg" alt="Squared-error loss function" class="calibre15"/></span> factor. The <code class="literal">GB_SqEL</code> function is set up as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Gradiend Boosting Using the Squared-error Loss Function
&gt; GB_SqEL &lt;- function(y,X,depth,iter,shrinkage){
+   curr_res &lt;- y
+   GB_Hat &lt;- data.frame(matrix(0,nrow=length(y),ncol=iter))
+   fit &lt;- y*0
+   for(i in 1:iter){
+     tdf &lt;- cbind(curr_res,X)
+     tpart &lt;- rpart(curr_res~.,data=tdf,maxdepth=depth)
+     gb_tilda &lt;- predict(tpart)
+     gb_hat &lt;- shrinkage*gb_tilda
+     fit &lt;- fit+gb_hat
+     curr_res &lt;- curr_res-gb_hat
+     GB_Hat[,i] &lt;- fit
+   }
+   return(list(GB_Hat = GB_Hat))
+ }</pre></div><p class="calibre7">The initialization takes place in specifications, and the line <code class="literal">fit &lt;- y*0</code>. The depth argument of the algorithm is taken in the line <code class="literal">maxdepth=depth</code>, and using the <code class="literal">rpart</code> function, we create a tree of the necessary depth. The <code class="literal">predict</code> function gives the values of <span class="strong"><img src="../images/00276.jpeg" alt="Squared-error loss function" class="calibre15"/></span> as required at each iteration, while <code class="literal">fit+gb_hat</code> does the necessary update. Note that <code class="literal">GB_Hat[,i]</code> consists of the predicted values at the end of each iteration.</p><p class="calibre7">We will illustrate the algorithm with the<a id="id229" class="calibre1"/> example of Efron and Hastie (2016). The data considered is related with Lu Gerig's disease, or <span class="strong"><strong class="calibre8">amyotrophic lateral sclerosis</strong></span> (<span class="strong"><strong class="calibre8">ALS</strong></span>). The dataset <a id="id230" class="calibre1"/>has information on 1,822 individuals with the ALS disease. The goal is to predict the rate of progression <code class="literal">dFRS</code> of a functional rating score. The study has information on 369 predictors/covariates. Here, we will use the <code class="literal">GB_SqEL</code> function to fit the gradient boosting technique and analyze the mean square error as the number of iterations increases. The details and the data can be obtained from the source at <a class="calibre1" href="https://web.stanford.edu/~hastie/CASI/data.html">https://web.stanford.edu/~hastie/CASI/data.html</a>. We will now put the squared-error loss function-driven boosting method into action:</p><div class="informalexample"><pre class="programlisting">&gt; als &lt;- read.table("../Data/ALS.txt",header=TRUE)
&gt; alst &lt;- als[als$testset==FALSE,-1]
&gt; temp &lt;- GB_SqEL(y=alst$dFRS,X=alst[,-1],depth=4,
+                 iter=500,shrinkage = 0.02)
&gt; MSE_Train &lt;- 0
&gt; for(i in 1:500){
+   MSE_Train[i] &lt;- mean(temp$GB_Hat[,i]-alst$dFRS)^2
+ }
&gt; windows(height=100,width=100)
&gt; plot.ts(MSE_Train)</pre></div><p class="calibre7">Using the <code class="literal">read.table</code> function, we import the data from the code bundle into the <code class="literal">als</code> object. The data is available<a id="id231" class="calibre1"/> from the source in the <code class="literal">.txt</code> format. The column <code class="literal">testset</code> indicates whether the observations were marked for training purposes or for tests. We select the training observations and also drop the first variable <code class="literal">testset</code> and store it in the object <code class="literal">alst</code>. The <code class="literal">GB_SqEL</code> function is applied on the <code class="literal">alst</code> object with appropriate specifications.</p><p class="calibre7">Following each iteration, we compute the mean-squared error and store it in <code class="literal">GB_Hat</code>, as explained earlier. We can see from the following diagram that as the iterations increase, the mean squared error decreases. Here, the algorithm stabilizes after nearly 200 iterations:</p><div class="mediaobject"><img src="../images/00277.jpeg" alt="Squared-error loss function" class="calibre10"/><div class="caption"><p class="calibre14">Figure 7: Gradient boosting and the MSE by iterations</p></div></div><p class="calibre11"> </p><p class="calibre7">In the next section, we will see the use <a id="id232" class="calibre1"/>of two powerful R packages.</p></div></div></div>

<div class="book" title="The general boosting algorithm">
<div class="book" title="Using the adabag and gbm packages"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl1sec44" class="calibre1"/>Using the adabag and gbm packages</h2></div></div></div><p class="calibre7">Using the boosting method as an ensemble<a id="id233" class="calibre1"/> technique is indeed very effective. The algorithm was illustrated for classification and regression problems from scratch. Once the<a id="id234" class="calibre1"/> understand the algorithm clear and transparent, we can then use R packages to deliver results going forward. A host of packages are available for implementing the boosting technique. However, we will use the two most popular packages <code class="literal">adabag</code> and <code class="literal">gbm</code> in this section. First, a look at the options of the two functions is in order. The names are obvious and <code class="literal">adabag</code> implements the adaptive boosting methods while <code class="literal">gbm</code> deals with gradient boosting methods. First, we look at the options available in these two functions in the following code:</p><div class="mediaobject"><img src="../images/00278.jpeg" alt="Using the adabag and gbm packages" class="calibre10"/><div class="caption"><p class="calibre14">The boosting and gbm functions</p></div></div><p class="calibre11"> </p><p class="calibre7">The formula is the usual argument. The argument <code class="literal">mfinal</code> in <code class="literal">adabag</code> and <code class="literal">n.trees</code> in <code class="literal">gbm</code> allows the specification of the number of trees or iterations. The boosting function gives the option of <code class="literal">boos</code>, which is the bootstrap sample of the training set drawn using the weights for each observation on that iteration. Gradient boosting is a more generic algorithm that is capable of<a id="id235" class="calibre1"/> handling more than the regression structure. It can be used for classification <a id="id236" class="calibre1"/>problems as well. The option of <code class="literal">distribution</code> in the <code class="literal">gbm</code> function gives those options. Similarly, one can see here that the <code class="literal">gbm</code> function offers a host of other options. We will neither undertake the daunting task of explaining them all nor apply them to complex datasets. The two datasets that were used to explain and elaborate adaptive and gradient boosting algorithms will be continued with the <code class="literal">boosting</code> and <code class="literal">gbm</code> functions.</p><p class="calibre7">The toy datasets need to be changed and we will replicate them multiple times over so that we have enough observations for running the <code class="literal">boosting</code> and <code class="literal">gbm</code> functions: </p><div class="informalexample"><pre class="programlisting">&gt; # The adabag and gbm Packages
&gt; x1 &lt;- c(1,5,3,1,5)
&gt; x1 &lt;- rep(x1,times=10)
&gt; x2 &lt;- c(5,5,3,1,1)
&gt; x2 &lt;- rep(x2,times=10)
&gt; y &lt;- c(1,1,0,1,1)
&gt; y &lt;- rep(y,times=10)
&gt; toy &lt;- data.frame(x1=x1,x2=x2,y=y)
&gt; toy$y &lt;- as.factor(toy$y)
&gt; AB1 &lt;- boosting(y~.,data=toy,boos=TRUE,mfinal = 10,
+                 maxdepth=1,minsplit=1,minbucket=1)
&gt; predict.boosting(AB1,newdata=toy[,1:2])$class
 [1] "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0"
[19] "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1" "1"
[37] "1" "0" "1" "1" "1" "1" "0" "1" "1" "1" "1" "0" "1" "1"</pre></div><p class="calibre7">The <code class="literal">maxdepth=1</code> function ensures that we are using only stumps as the base classifiers. It is easily seen that the boosting function<a id="id237" class="calibre1"/> works perfectly, as all the observations are correctly classified.</p><p class="calibre7">As with the <code class="literal">boosting</code> function, we <a id="id238" class="calibre1"/>need to have more data points. We increase this with the <code class="literal">seq</code> function and using the <code class="literal">distribution="gaussian"</code> option, we ask the <code class="literal">gbm</code> function to fit the regression boosting technique:</p><div class="informalexample"><pre class="programlisting">&gt; x &lt;- seq(0,2*pi,pi/200)
&gt; y &lt;- sin(x)
&gt; sindata &lt;- data.frame(cbind(x,y))
&gt; sin_gbm &lt;- gbm(y~x,distribution="gaussian",data=sindata,
+                n.trees=250,bag.fraction = 0.8,shrinkage = 0.1)
&gt; par(mfrow=c(1,2))
&gt; plot.ts(sin_gbm$fit, main="The gbm Sine Predictions")
&gt; plot(y,sin_gbm$fit,main="Actual vs gbm Predict")</pre></div><p class="calibre7">Using the plot functions, we make a comparison of the fit with the gradient boosting method. The following diagram suggests that the fit has been appropriate. However, the plots also show that something is not quite correct with the story either. The function approximation at <span class="strong"><img src="../images/00279.jpeg" alt="Using the adabag and gbm packages" class="calibre15"/></span> and <span class="strong"><img src="../images/00280.jpeg" alt="Using the adabag and gbm packages" class="calibre15"/></span> by the boosting method leaves a lot to be desired, and the actual versus predicted plot suggests discontinuity/poor performance at 0. However, we will not delve too far into these issues:</p><div class="mediaobject"><img src="../images/00281.jpeg" alt="Using the adabag and gbm packages" class="calibre10"/><div class="caption"><p class="calibre14">Figure 8: Sine wave approximation by using the gbm function</p></div></div><p class="calibre11"> </p><p class="calibre7">Next, we will discuss the concept of variable importance.</p></div></div>

<div class="book" title="The general boosting algorithm">
<div class="book" title="Variable importance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl1sec45" class="calibre1"/>Variable importance</h2></div></div></div><p class="calibre7">Boosting methods essentially use trees <a id="id239" class="calibre1"/>as base learners, and hence the idea of variable importance gets carried over here the same as with trees, bagging, and random forests. We simply add the importance of the variables across the trees as we do with bagging or random forests.</p><p class="calibre7">For a boosting fitted object from the <code class="literal">adabag</code> package, the variable importance is extracted as follows:</p><div class="informalexample"><pre class="programlisting">&gt; AB1$importance
 x1  x2 
100   0 </pre></div><p class="calibre7">This means that the boosting method has not used the <code class="literal">x2</code> variable at all. For the gradient boosting objects, the importance is given by the <code class="literal">summary</code> function: </p><div class="informalexample"><pre class="programlisting">&gt; summary(sin_gbm)
  var rel.inf
x   x     100</pre></div><p class="calibre7">It is now apparent that we only have one variable and so it is important to explain the regressand and we certainly did not require some software to tell us. Of course, it is useful in complex cases. Comparisons are for different ensembling methods based on trees. Let us move on to the next section.</p></div></div>

<div class="book" title="The general boosting algorithm">
<div class="book" title="Comparing bagging, random forests, and boosting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch05lvl1sec46" class="calibre1"/>Comparing bagging, random forests, and boosting</h2></div></div></div><p class="calibre7">We carried out comparisons between the<a id="id240" class="calibre1"/> bagging and random forest <a id="id241" class="calibre1"/>methods in the previous chapter. Using the <code class="literal">gbm</code> function, we now add boosting accuracy to the earlier analyses:</p><div class="informalexample"><pre class="programlisting">&gt; data("spam")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(spam),replace = TRUE,
+ prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; spam_Train &lt;- spam[Train_Test=="Train",]
&gt; spam_TestX &lt;- within(spam[Train_Test=="Test",],
+                      rm(type))
&gt; spam_TestY &lt;- spam[Train_Test=="Test","type"]
&gt; spam_Formula &lt;- as.formula("type~.")
&gt; spam_rf &lt;- randomForest(spam_Formula,data=spam_Train,coob=TRUE,
+                         ntree=500,keepX=TRUE,mtry=5)
&gt; spam_rf_predict &lt;- predict(spam_rf,newdata=spam_TestX,type="class")
&gt; rf_accuracy &lt;- sum(spam_rf_predict==spam_TestY)/nrow(spam_TestX)
&gt; rf_accuracy
[1] 0.9436117
&gt; spam_bag &lt;- randomForest(spam_Formula,data=spam_Train,coob=TRUE,
+                          ntree=500,keepX=TRUE,mtry=ncol(spam_TestX))
&gt; spam_bag_predict &lt;- predict(spam_bag,newdata=spam_TestX,type="class")
&gt; bag_accuracy &lt;- sum(spam_bag_predict==spam_TestY)/nrow(spam_TestX)
&gt; bag_accuracy
[1] 0.9350464
&gt; spam_Train2 &lt;- spam_Train
&gt; spam_Train2$type &lt;- ifelse(spam_Train2$type=="spam",1,0)
&gt; spam_gbm &lt;- gbm(spam_Formula,distribution="bernoulli",data=spam_Train2,
+                 n.trees=500,bag.fraction = 0.8,shrinkage = 0.1)
&gt; spam_gbm_predict &lt;- predict(spam_gbm,newdata=spam_TestX,
+                             n.trees=500,type="response")
&gt; spam_gbm_predict_class &lt;- ifelse(spam_gbm_predict&gt;0.5,"spam","nonspam")
&gt; gbm_accuracy &lt;- sum(spam_gbm_predict_class==spam_TestY)/nrow(spam_TestX)
&gt; gbm_accuracy
[1] 0.945753
&gt; summary(spam_gbm)
                                var      rel.inf
charExclamation     charExclamation 21.985502703
charDollar               charDollar 18.665385239
remove                       remove 11.990552362
free                           free  8.191491706
hp                               hp  7.304531600


num415                       num415  0.000000000
direct                       direct  0.000000000
cs                               cs  0.000000000
original                   original  0.000000000
table                         table  0.000000000
charHash                   charHash  0.000000000</pre></div><p class="calibre7">The boosting <a id="id242" class="calibre1"/>accuracy of <code class="literal">0.9457</code> is higher than the random forest <a id="id243" class="calibre1"/>accuracy of <code class="literal">0.9436</code>. Further fine tuning, to be explored in the next chapter, will help in improving the accuracy. The variable importance is also<a id="id244" class="calibre1"/> easily obtained <a id="id245" class="calibre1"/>using the <code class="literal">summary</code> function.</p></div></div>
<div class="book" title="Summary" id="19UOO1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec47" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">Boosting is yet another ramification of decision trees. It is a sequential iteration technique where the error from a previous iteration is targeted with more impunity. We began with the important adaptive boosting algorithm and used very simple toy data to illustrate the underpinnings. The approach was then extended to the regression problem and we illustrated the gradient boosting method with two different approaches. The two packages <code class="literal">adabag</code> and <code class="literal">gbm</code> were briefly elaborated on and the concept of variable importance was emphasized yet again. For the spam dataset, we got more accuracy with boosting and hence the deliberations of the boosting algorithm are especially more useful.</p><p class="calibre7">The chapter considered different variants of the boosting algorithm. However, we did not discuss why it works at all. In the next chapter, these aspects will be covered in more detail.</p></div></body></html>