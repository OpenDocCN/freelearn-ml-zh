<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer143">
<h1 class="chapter-number" id="_idParaDest-116"><a id="_idTextAnchor172"/>9</h1>
<h1 id="_idParaDest-117"><a id="_idTextAnchor173"/>Exploring Transformers</h1>
<p><strong class="bold">Transformers</strong> are a<a id="_idIndexMarker700"/> new type of machine learning model that has completely revolutionized the way human language is processed and understood. These models can analyze huge amounts of data, find and understand complex patterns with hitherto unmatched accuracy, and produce insights that would otherwise be impossible for humans to obtain on tasks such as translation, text summarization, and <span class="No-Break">text generation.</span></p>
<p>Transformers are powerful because they can handle large amounts of data, and learn from previous examples to make better predictions. They have totally “transformed” (pun intended) <strong class="bold">NLP</strong> and <a id="_idIndexMarker701"/>have outperformed traditional methods in many NLP tasks, quickly becoming state of <span class="No-Break">the art.</span></p>
<p>In this chapter, we will introduce transformers, discuss how they work, and look at some of their key components. We will then present Hugging Face and see how it helps in our task before introducing some useful existing transformer models. We’ll also show how we can use Hugging Face to implement two models <span class="No-Break">using transformers.</span></p>
<p>This chapter will demonstrate how to build transformer models while taking you through important steps such as linking Google Colab to Google Drive so that files can be persisted, preparing the data, using auto classes, and ultimately building models that can be used <span class="No-Break">for classification.</span></p>
<p>We will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Introduction <span class="No-Break">to transformers</span></li>
<li>How data flows through <span class="No-Break">the transformer</span></li>
<li><span class="No-Break">Hugging Face</span></li>
<li><span class="No-Break">Existing models</span></li>
<li>Transformers <span class="No-Break">for classification</span></li>
<li><span class="No-Break">Implementing transformers</span></li>
</ul>
<p>Let’s begin by having a closer look at transformers, who invented them, and how <span class="No-Break">they work.</span></p>
<h1 id="_idParaDest-118"><a id="_idTextAnchor174"/>Introduction to transformers</h1>
<p>In this chapter, we <a id="_idIndexMarker702"/>will briefly introduce transformers. In language and NLP tasks, context plays a crucial role – that is, to know what a word means, knowledge about the situation (that is, the context) must also be taken into account. Before transformers came along, sequence-to-sequence models were used for many NLP tasks. These are models that generate an output sequence by predicting a single word at a time and encode the source text to gain knowledge about the context. However, the problem with languages is that they are complex, fluid, and difficult to turn into a rigid rule-based structure. The context itself is also hard to track as it is often found far away (that is, many words, sentences, or even paragraphs) from where it is required. To address this problem, sequence-to-sequence models work by using neural networks, which have some limited form <span class="No-Break">of memory:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 9.1 – Sequence-to-sequence model versus transformer" height="282" src="image/B18714_09_01.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Sequence-to-sequence model versus transformer</p>
<p>The <a id="_idIndexMarker703"/>ubiquitous paper on transformers, <em class="italic">Attention Is All You Need</em>, was published by Vaswani et al., in 2017. They presented a<a id="_idIndexMarker704"/> new type of neural network architecture, known as <strong class="bold">transformers</strong>, which could be used for NLP tasks. Transformers have several components, as can be seen in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, with an “encoder” (on the left), a “decoder” (on the right), and a block of attention and feed-forward components <a id="_idIndexMarker705"/>that repeat <span class="No-Break"><em class="italic">N</em></span><span class="No-Break"> times:</span><a id="_idTextAnchor175"/></p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 9.2 – From the &quot;Attention Is All You Need&quot; paper by Vaswani et al." height="2098" src="image/B18714_09_02.jpg" width="1280"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – From the "Attention Is All You Need" paper by Vaswani et al.</p>
<p>The transformer consists of encoder and decoder layers; each usually has more than one identical instance (for example, six as in the original research paper), and each has its own set of weights. On the left-hand side, the encoder’s job is to convert a sequence of inputs into a set of continuous representations. On the right-hand side, the decoder uses the output from the encoder as well as the output from the previous time step to produce a sequence of outputs. The first encoder and decoder in each stack of the architecture has an embedding layer and positional encoding as inputs. Each encoder contains a self-attention layer that calculates the relationships between different words and a feed-forward layer. Each decoder also contains a feed-forward layer, but it has two self-attention layers. The output from the last encoder is used as the input to the first decoder. These components combine to make the transformer architecture faster and more efficient <a id="_idIndexMarker706"/>and allow it to handle much longer sequences, making the separation between words irrelevant. Consequently, the architecture can outperform other, more <span class="No-Break">traditional, methods.</span></p>
<p>The transformer architecture <a id="_idIndexMarker707"/>described by Vaswani et al. was created with the goal of translation. The encoder is fed inputs (that is, sentences) in one language (for example, English) during training, while the decoder is fed the same inputs (that is, sentences) in the intended target language (for example, French). The attention layers in the encoder make use of each word in an input sentence, but the encoder operates sequentially and can only focus on the words in the translated text (that is, only the words before the word that is currently being generated). For example, if the first <em class="italic">N</em> words of the translated target have been predicted, these are input to the decoder, which uses all the inputs of the encoder to predict the word <span class="No-Break">at </span><span class="No-Break"><em class="italic">N+1</em></span><span class="No-Break">.</span></p>
<p>The decoder is supplied with the entire target sentence but is constrained from using forthcoming words. Consequently, when predicting a word, the decoder cannot refer to any words beyond it in the target sentence. For instance, while predicting the <em class="italic">Nth</em> word, only the words in positions <em class="italic">1</em> to <em class="italic">N-1</em> can be considered by the attention layer. This constraint is crucial to guarantee that the task remains suitably demanding for the model to acquire <span class="No-Break">knowledge competently.</span></p>
<h1 id="_idParaDest-119"><a id="_idTextAnchor176"/>How data flows through the transformer model</h1>
<p>In this section, we <a id="_idIndexMarker708"/>will take a closer look at how data flows through the transformer model. Understanding how data flows through a transformer, and the steps that transform raw input into meaningful output, is crucial to understanding its power and potential. The transformer enables efficient and effective modeling of long-range dependencies in data, making it highly capable of capturing context and semantics. By exploring these inner mechanisms of data flow within the transformer, we will gain a deeper understanding of its ability to process and understand language. We will look at input <span class="No-Break">embeddings first.</span></p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor177"/>Input embeddings</h2>
<p>Starting on the<a id="_idIndexMarker709"/> left-hand side, the inputs into the encoder are word tokens from the source text. This textual data has to be converted into a numeric representation (of size 512 according to the authors) using methods such as GloVe or Word2Vec, <span class="No-Break">among others.</span></p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor178"/>Positional encoding</h2>
<p>A <a id="_idIndexMarker710"/>positional element is then added to these embeddings. This is important as it allows the transformer to discover information about the distances between words and the order of the words. This information is then passed to the self-attention layer of the first <span class="No-Break">encoder block.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Positional encodings do not alter the <span class="No-Break">vector dimensions.</span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor179"/>Encoders</h2>
<p>Each encoder <a id="_idIndexMarker711"/>has several sub-layers <span class="No-Break">within it:</span></p>
<ul>
<li><strong class="bold">Multi-headed attention</strong>: This <a id="_idIndexMarker712"/>allows the transformer to attend to different parts of the input sequence at the same time, thus improving its input processing abilities, and allowing it to obtain more context and make much better-informed decisions. It is the most important part of the architecture and is also the most computationally expensive. When working on words in the input, self-attention relates every word in the input to every other word. It is interesting to consider how the transformer decides which set of weights will yield the best results. The aim is that the attention value should be large for words that are somehow related in a sentence, and vice versa. For example, let’s look at the sentence <em class="italic">The weather was </em><span class="No-Break"><em class="italic">very sunny.</em></span></li>
</ul>
<p>The words <em class="italic">weather</em> and <em class="italic">sunny</em> are related and hence should generate a high attention value; conversely, the attention value for <em class="italic">The</em> and <em class="italic">was</em> should be small. As described earlier, transformers are trained on embeddings. Consequently, the transformer will learn from them and be able to produce the vectors required so that words produce attention values that correspond to the relatedness of the words. Furthermore, instead of just considering individual meanings, the self-attention mechanism weighs the input words differently according to their importance and their relationships with other words. This allows it to be able to handle the aforementioned long-distance context problems and hence achieve much<a id="_idIndexMarker713"/> better performance on NLP tasks. Briefly, the attention value is computed using three matrices, with each row in each matrix representing an input word. It is important to note that the values in these rows are learned by the model so<a id="_idIndexMarker714"/> that the desired outputs are generated.  Let’s take a look at each of these important matrices <span class="No-Break">in turn:</span></p>
<ul>
<li><strong class="bold">Query</strong>: Each row corresponds to the embeddings of an input word. In other words, the query word is the specific word for which an attention value is <span class="No-Break">being calculated.</span></li>
<li><strong class="bold">Key</strong>: Each word in the input text that the model is comparing the query to – that is, the word that is being paid attention to – to calculate its importance to the <span class="No-Break">query word.</span></li>
<li><strong class="bold">Value</strong>: The information that the model is trying to generate based on the comparison between the query and key matrices. The key and value matrices can be <span class="No-Break">the same.</span></li>
</ul>
<p>Given these matrices, the attention value is obtained by calculating the dot product of the query and key matrices. These are then used to weigh the value matrix, hence effectively allowing the model to “learn” which words in the input text should be <span class="No-Break">focused upon.</span></p>
<ul>
<li><strong class="bold">Add and norm</strong>: These<a id="_idIndexMarker715"/> layers comprise a residual connection layer followed by a normalization layer. For our purposes, it is enough to know that they help address the vanishing gradient problem and improve the <span class="No-Break">model’s performance.</span></li>
<li><strong class="bold">Feed-forward neural network</strong>: This<a id="_idIndexMarker716"/> is a neural network that processes the attention vector inputs and transforms them into a form, of the same dimensions as the input, that can be input into the next layer. These attention vectors are independent of each other; consequently, parallelization can be used in this stage, rather than processing<a id="_idIndexMarker717"/> them sequentially as in the <span class="No-Break">sequence-to-sequence architecture.</span></li>
</ul>
<p>Let’s now move on <span class="No-Break">to decoders.</span></p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor180"/>Decoders</h2>
<p>The<a id="_idIndexMarker718"/> output from the encoder is used as the input to the second layer of each decoder in the <span class="No-Break">decoder stack.</span></p>
<p>Similar to the encoder, masked multi-headed attention takes an output embedding and a positional embedding. The target for the transformer is to learn how to generate the output, given both the input and the <span class="No-Break">required output.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">During training, the required output (for example, a translation) is provided to <span class="No-Break">the decoder.</span></p>
<p>Some of the words are masked so that the model can learn how to predict them. These are changed during each iteration. The decoders process this along with the encoded representation from the encoders to produce a <span class="No-Break">target sequence.</span></p>
<p>However, during prediction, an empty sequence (with a special <strong class="bold">start of sentence</strong> (<strong class="bold">&lt;SOS&gt;</strong>) token) is used. This is converted into an embedding; positional encoding is added and is used as input to the decoder. The decoder and other layers work as before but the last word from the output sequence is used to fill in the first blank of the input sequence, hence the input is now the &lt;SOS&gt; and the first predicted word. This is, again, fed into the decoder and the process is repeated until the end of <span class="No-Break">the sentence.</span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor181"/>Linear layer</h2>
<p>The output <a id="_idIndexMarker719"/>from the decoder is used as input to this linear layer, a simple fully connected neural network that generates the vectors for the <span class="No-Break">next layer.</span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor182"/>Softmax layer</h2>
<p>The <a id="_idIndexMarker720"/>softmax layer transforms the input into a probability distribution – that is, it takes a set of numbers and turns them into positive numbers that sum up to 1, applying higher importance to higher values and less importance to <span class="No-Break">smaller values.</span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor183"/>Output probabilities</h2>
<p>Finally, the<a id="_idIndexMarker721"/> output probabilities are the word tokens for the target. The transformer compares this output with the target sequence that came from the training data and uses it to improve the results via back-propagation. </p>
<p>Now that we’ve learned how transformers work, in the next section, we will have a very brief look at how one organization has made the process of implementing and experimenting with transformer-based models easy, making it accessible <span class="No-Break">to all.</span></p>
<h1 id="_idParaDest-127"><a id="_idTextAnchor184"/>Hugging Face</h1>
<p>Transformers need a <em class="italic">lot</em> of data to be effective and produce good results. Furthermore, a huge amount of computing power and time is also needed; the best models are usually trained using multiple GPUs and can take days (or even longer) to complete training. Consequently, not everyone can afford to train such models and usually, this is done by the big players such as Google, Facebook, and OpenAI. Luckily, there are pretrained <span class="No-Break">models available.</span></p>
<p>The name <a id="_idIndexMarker722"/>Hugging Face (named after the emoji with a smiling face and open hands) is synonymous with transformers, models, and NLP. Hugging Face (<a href="https://huggingface.co">https://huggingface.co</a>) provides a <a id="_idIndexMarker723"/>repository for pretrained transformer (and other) models to be published. These can then be easily downloaded, free of charge, and used for a wide range of NLP tasks. Furthermore, if the task involves a domain that has unique nomenclature, terminology, and domain-specific language, then models can be “fine-tuned” to improve the model’s performance. Fine-tuning is a process that uses the weights from a pretrained model as a starting point and uses new domain-specific data to update them so that the model becomes better at the domain-specific task. As well as publishing models, Hugging Face also provides services that allow models to be fine-tuned, trained, and <span class="No-Break">much more.</span></p>
<p>Hugging Face also provides a Python library that provides a high-level interface for NLP tasks. The library <a id="_idIndexMarker724"/>offers a wide range of state-of-the-art pretrained models, including BERT, GPT, RoBERTa, T5, and many others (see the next section). It can be installed using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
pip install transformers</pre> <p>Apart from downloading pretrained models, the library can also be used to download tokenizers. Both of these can then be used with your datasets to fine-tune tasks such as classification to create state-of-the-art <span class="No-Break">NLP systems.</span></p>
<p>In summary, the Hugging Face <strong class="source-inline">transformers</strong> library is a powerful tool for working with NLP models, making it easy to work with transformer models. It has an intuitive design and extensive model selection and is well worth <span class="No-Break">a look.</span></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor185"/>Existing models</h1>
<p>Driven by boosts in computing, storage, and data capacity, transformers have taken the world by storm. Some<a id="_idIndexMarker725"/> of the more famous pretrained models include <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>): Created by <a id="_idIndexMarker726"/>the Google AI team, and trained on a huge corpus of text data, BERT takes the context from both the left and right sides of each word <span class="No-Break">into account.</span></li>
<li><strong class="bold">Efficiently Learning an Encoder that Classifies Token Accurately</strong> (<strong class="bold">ELECTRA</strong>): ELECTRA <a id="_idIndexMarker727"/>uses a generator-discriminator <a id="_idIndexMarker728"/>model to distinguish between generated and real text. The generator is trained to generate text that is similar to real text, while the discriminator is trained to distinguish between real and <span class="No-Break">generated text.</span></li>
<li><strong class="bold">Generative Pre-trained Transformer 3</strong> (<strong class="bold">GPT-3</strong>): Developed by OpenAI and pretrained <a id="_idIndexMarker729"/>on a huge range of internet text, GPT-3 <a id="_idIndexMarker730"/>has 175 billion parameters and is one of the largest models available <span class="No-Break">to date.</span></li>
<li><strong class="bold">Megatron (a large transformer model trained by NVIDIA)</strong>: Developed by NVIDIA, Megatron<a id="_idIndexMarker731"/> is <a id="_idIndexMarker732"/>scalable and can be trained on hundreds of GPUs, so it can use much <span class="No-Break">larger models.</span></li>
<li><strong class="bold">Robustly Optimized BERT</strong> (<strong class="bold">RoBERTa</strong>): Based on BERT, RoBERTa is designed to <a id="_idIndexMarker733"/>improve upon BERT by using<a id="_idIndexMarker734"/> a larger training corpus and more training steps to learn more robust representations <span class="No-Break">of text.</span></li>
<li><strong class="bold">Text-to-Text Transfer Transformer</strong> (<strong class="bold">T5</strong>): Developed by Google, T5 treats NLP <a id="_idIndexMarker735"/>problems<a id="_idIndexMarker736"/> as “text-to-text” problems. It is trained on unlabeled and labeled data and then fine-tunes it individually for a variety <span class="No-Break">of tasks.</span></li>
<li><strong class="bold">Transformer with extra-long context</strong> (<strong class="bold">Transformer-XL</strong>): This model introduces <a id="_idIndexMarker737"/>a memory module that <a id="_idIndexMarker738"/>allows the model to handle and understand long-term dependencies <span class="No-Break">much better.</span></li>
<li><strong class="bold">XLNet (generalized autoregressive pretraining)</strong>: Developed by Google, XLNet <a id="_idIndexMarker739"/>takes <a id="_idIndexMarker740"/>the <a id="_idIndexMarker741"/>best bits from Transformer-XL and BERT and models dependencies between all <span class="No-Break">input words.</span></li>
</ul>
<p>In the next section, we look will look closer at how transformers are trained for the task we are interested in: classification. Inspiration for this section came from the Hugging <span class="No-Break">Face pages.</span></p>
<h1 id="_idParaDest-129"><a id="_idTextAnchor186"/>Transformers for classification</h1>
<p>Transformer <a id="_idIndexMarker742"/>models are trained as language models. These are a type of algorithm that has been trained by analyzing patterns of human language to understand and produce <span class="No-Break">human language.</span></p>
<p>They have knowledge of grammar, syntax, and semantics, and can discern patterns and connections among words and phrases. Moreover, they can detect named entities, such as individuals, locations, and establishments, and interpret the context in which they are referenced. Essentially, a transformer model is a computer program that uses statistical models to analyze and <span class="No-Break">generate language.</span></p>
<p>Language models <a id="_idIndexMarker743"/>are trained in a self-supervised manner on<a id="_idIndexMarker744"/> large amounts of text data, such as books, articles, and online content, to learn patterns and relationships between words and phrases. Some of the popular<a id="_idIndexMarker745"/> datasets used for pretraining transformers include<a id="_idIndexMarker746"/> Common Crawl, Wikipedia, and <a id="_idIndexMarker747"/>BooksCorpus. For example, BERT was trained using around 3.5 billion words in total with around 2.5 billion from Wikipedia and around 1 billion from BooksCorpus. This allows the model to predict the likelihood of a certain word or phrase occurring after a given sequence of words. The outputs of a pretrained large language model typically involve predictions based on the input text. The model may output probabilities of certain words or phrases being used next in a sentence, make predictions of the most likely word to follow a given input word, or generate an entire sentence or paragraph based on the input text. The output can be used for various purposes, such as text generation, translation, sentiment analysis, <span class="No-Break">and more.</span></p>
<p>Self-supervised learning is <a id="_idIndexMarker748"/>a type of machine learning where the model learns to extract useful information from unlabeled data without requiring any explicit labels or supervision. Instead, the model is trained on a task such as predicting the missing part of an image or reconstructing a corrupted sentence. Consequently, this type of model builds an understanding of the language it has been trained on – but only from a statistical point of view. However, this approach lacks practicality for everyday tasks, and therefore, a <a id="_idIndexMarker749"/>generalized <strong class="bold">pretrained</strong> model must be customized through supervised <strong class="bold">fine-tuning</strong> using human-annotated labels specific to the task <span class="No-Break">at hand.</span></p>
<p>Pretraining (that is, training a model from scratch) requires huge amounts of data, and hence the process can take weeks or months. Fine-tuning is then performed <em class="italic">on</em> the pretrained model, so a pretrained language model is required to do the fine-tuning. Essentially, fine-tuning is a further training step with a dataset that suits <span class="No-Break">the task.</span></p>
<p>Fine-tuning<a id="_idIndexMarker750"/> a model typically adjusts the weights of the model’s pretrained layers to better fit a new dataset or task. The process of fine-tuning involves initializing the weights of the pretrained layers, and then training the entire model on the new dataset or task. During training, the weights of the pretrained layers are updated along with the weights of the newly added layers, allowing the model to learn more nuanced features from the new dataset while preserving the knowledge learned from the pretrained model. The degree to which the weights of the pretrained layers are updated during fine-tuning depends on the specifics of the new task <a id="_idIndexMarker751"/>and the amount of available data. In some cases, only the weights of the newly added layers are updated, while in others, the weights of the pretrained layers may be updated significantly. Another option is to keep all layers fixed apart from the final layer, whose weights are then modified during training. Consequently, fixing the layers with these techniques, and using a smaller learning rate, during the fine-tuning process, often yields a performance improvement. Sometimes, this goes hand in hand with adding new layers on top of the old architecture, thus persisting the old fixed weights and only allowing the weights of new layers to <span class="No-Break">be changed.</span></p>
<p>But what exactly <a id="_idIndexMarker752"/>happens when fine-tuning? There are various techniques, but the general thought is that early layers learn generic patterns that are irrelevant to the actual task (for example, classification), while later layers learn the patterns that are relevant to the task. This intuition has been verified by various <span class="No-Break">research teams.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">During the gradient descent calculation, the size of the step that’s taken in each iteration is determined by the learning rate, with the overall goal being to locate the minimum of a <span class="No-Break">loss function.</span></p>
<p>In practical terms, for<a id="_idIndexMarker753"/> classification, we would download a model such as <strong class="bold">BertForSequenceClassification</strong> – a BERT model with a linear layer for sentence classification. So, the final layer produces a probability vector that indicates the probability of each potential class label for the <span class="No-Break">input sequence.</span></p>
<p>In short, fine-tuning a model enables it to adapt features it has learned to a new task or dataset, which can result in <span class="No-Break">improved performance.</span></p>
<p>We looked at the individual bits of the model earlier in this chapter and saw how the model has encoder and decoder blocks. Depending on the task, each of these parts can be utilized separately. For the classification task, an encoder-only model is recommended. For more details, there are some great Packt books available, such as <em class="italic">Transformers for Natural Language Processing</em>, by <span class="No-Break">Denis Rothman.</span></p>
<p>In the next section, we will fine-tune a model using a training dataset, make some predictions on a test dataset, and evaluate <span class="No-Break">the results.</span></p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor187"/>Implementing transformers</h1>
<p>In this section, we <a id="_idIndexMarker754"/>will work through the code for implementing transformers for both single-emotion and multi-emotion datasets. We will be using <strong class="bold">Google Colaboratory</strong> (<strong class="bold">Colab</strong>) as <a id="_idIndexMarker755"/>it simplifies the implementation of transformers by providing a powerful cloud-based environment with pre-installed libraries and resources. So, let’s begin by looking <span class="No-Break">at that.</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor188"/>Google Colab</h2>
<p>Google Colab is a free<a id="_idIndexMarker756"/> notebook environment service that runs in the cloud (<a href="https://colab.research.google.com">https://colab.research.google.com</a>). There <a id="_idIndexMarker757"/>are many benefits of using Colab; for example, it allows developers to start programming rapidly without having to worry about setup, it allows code to <a id="_idIndexMarker758"/>be shared with people who do not have the correct software installed locally, and it also integrates well <a id="_idIndexMarker759"/>with GitHub. However, one of the biggest advantages is that Google provides free access to GPUs. Machine learning, at its core, involves lots and lots of mathematical operations – something that GPUs are good at. In practical terms, even for simple models with small training datasets, the time-saving between GPU and non-GPU-powered systems can be many hours (for example, 10 minutes compared to <span class="No-Break">10 hours).</span></p>
<p>A few words of warning, though. Colab is ephemeral – in other words, files (for example, data files) uploaded to a session or generated by a session (for example, results) will eventually disappear. The workaround for this is to upload files to Google Drive and give permission for Colab to <span class="No-Break">access them.</span></p>
<p>Debugging on Colab is also a little more cumbersome than, say, via VS Code. It involves installing and importing the <strong class="source-inline">ipdb</strong> (IPython-enabled Python <span class="No-Break">Debugger) package:</span></p>
<pre class="source-code">
!pip install -Uqq ipdbimport ipdb</pre>
<p>Breakpoints are <a id="_idIndexMarker760"/>useful for developers and these can be set via code to cause the debugger <span class="No-Break">to stop:</span></p>
<pre class="source-code">
ipdb.set_trace()</pre> <p>We can <a id="_idIndexMarker761"/>use command-line arguments to control the debugger, such as <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="source-inline">c</strong>: <span class="No-Break">Continue execution</span></li>
<li><strong class="source-inline">n</strong>: Move to the <span class="No-Break">next line</span></li>
<li><strong class="source-inline">r</strong>: Continue execution until the current <span class="No-Break">function returns</span></li>
</ul>
<p>Debugging can be globally turned off using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
%pdb off</pre> <p>Debugging can also be globally turned on using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
%pdb on</pre> <p>Now that we know what a transformer is, how it works, and how to implement it, let’s implement a transformer in Python using Colab to classify the datasets introduced in the <span class="No-Break">previous chapters.</span></p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor189"/>Single-emotion datasets</h2>
<p>We will <a id="_idIndexMarker762"/>implement two transformers to cater to the two different types of datasets. Let’s start with the single-emotion task. Broadly, we will be following <span class="No-Break">these steps:</span></p>
<ol>
<li>Install the <span class="No-Break">necessary libraries.</span></li>
<li>Import the <span class="No-Break">necessary libraries.</span></li>
<li>Provide access to <span class="No-Break">Google Drive.</span></li>
<li>Create dataset and <span class="No-Break">model variables.</span></li>
<li>Load and prepare <span class="No-Break">the datasets.</span></li>
<li>Tokenize <span class="No-Break">the datasets.</span></li>
<li>Load a model <span class="No-Break">for classification.</span></li>
<li>Set up <span class="No-Break">trainer arguments.</span></li>
<li>Train <span class="No-Break">the model.</span></li>
<li>Use the trained model <span class="No-Break">to predict.</span></li>
<li><span class="No-Break">Evaluate.</span></li>
</ol>
<p>Classifying the single-emotion tweets is a somewhat easier task, so let’s being <span class="No-Break">with this.</span></p>
<p>Let’s begin by installing <span class="No-Break">some libraries:</span></p>
<pre class="source-code">
!pip install datasets!pip install evaluate
!pip install transformers</pre>
<p>These libraries are used to easily access datasets, evaluate the results of a model, and access the pretrained models available from Hugging Face, respectively. We can now import these into <span class="No-Break">our code:</span></p>
<pre class="source-code">
import datasetsfrom datasets import load_dataset
from enum import Enum
import evaluate
from evaluate import evaluator
import numpy as np
from sklearn.metrics import jaccard_score
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Pipeline,
    Trainer,
    TrainingArguments
)
import pandas as pd
from pathlib import Path
from google.colab import drive</pre>
<p>As mentioned <a id="_idIndexMarker763"/>previously, we <a id="_idIndexMarker764"/>want to upload our training and test files once and access them on-demand whenever we need them, so let’s give Colab access to our Google Drive, which is where the files are uploaded. In reality, some of these files are already available via the <strong class="source-inline">datasets</strong> library, but for now, let’s assume we want to access them from <span class="No-Break">our repository:</span></p>
<pre class="source-code">
drive.mount("/content/gdrive/", force_remount=True)BASE_PATH = "/content/gdrive/MyDrive/PacktBook/Data/C9"</pre>
<p class="callout-heading">Note</p>
<p class="callout">You should replace <strong class="source-inline">BASE_PATH</strong> with your <span class="No-Break">own path.</span></p>
<p>We should now set up a few things to make our task easier for us. Different datasets need different parameters, so <strong class="source-inline">enum</strong> can be used to control the execution flow of the code. We must name our files so that the names contain the language code of the tweets within the file (that is, <strong class="source-inline">AR</strong>, <strong class="source-inline">ES</strong>, and <strong class="source-inline">EN</strong>), and then use <strong class="source-inline">enum</strong> and the filename to set variables that are useful in <span class="No-Break">the code:</span></p>
<pre class="source-code">
class Dataset(Enum):  SEM4_EN=1
  WASSA_EN=2
  CARER_EN=3
  SEM4_AR=4
  SEM4_ES=5
  IMDB_EN=6</pre>
<p>For now, we<a id="_idIndexMarker765"/> will also set a variable such as <strong class="source-inline">NUM_LABELS</strong> to tell the model how many labels there are. Later, we will see <a id="_idIndexMarker766"/>that we don’t need to <span class="No-Break">do this:</span></p>
<pre class="source-code">
# set the required dataset hereds = Dataset.SEM4_EN
NUM_LABELS = 4
COLS = 'ID', 'tweet', 'label'</pre>
<p>Now, we can use <strong class="source-inline">enum</strong> to set some dataset-specific variables. This way, when we want to try other datasets, we only need to modify the <span class="No-Break"><strong class="source-inline">ds</strong></span><span class="No-Break"> variable:</span></p>
<pre class="source-code">
if (ds == Dataset.SEM4_EN):  training_file = "SEM4_EN_train.csv"
  test_file = "SEM4_EN_dev.csv"
elif (ds == Dataset.WASSA_EN):
  training_file = "WASSA_train.csv"
  test_file = "WASSA_dev.csv"
elif(ds == Dataset.CARER_EN):
  training_file = "CARER_EN_train.csv"
  test_file = "CARER_EN_dev.csv"
  NUM_LABELS = 6
elif(ds == Dataset.SEM4_ES):
  training_file = "SEM4_ES_train.csv"
  test_file = "SEM4_ES_dev.csv"
  NUM_LABELS = 5
elif(ds == Dataset.SEM4_AR):
  training_file = "SEM4_AR_train.csv"
  test_file = "SEM4_AR_dev.csv"
elif(ds == Dataset.IMDB_EN):
  NUM_LABELS = 2
  training_file = "IMDB_EN_train.csv"
  test_file = "IMDB_EN_dev.csv"</pre>
<p>We<a id="_idIndexMarker767"/> must also set <strong class="source-inline">model_name</strong> to<a id="_idIndexMarker768"/> tell the program which language-specific model <span class="No-Break">to use:</span></p>
<pre class="source-code">
# select a modelif "_AR_" in training_file:
  model_name = "asafaya/bert-base-arabic"
elif "_EN_" in training_file:
  model_name = "bert-base-cased"
elif "_ES_" in training_file:
  model_name = "dccuchile/bert-base-spanish-wwm-cased"</pre>
<p>Then, we can set various file <span class="No-Break">path variables:</span></p>
<pre class="source-code">
# add the base pathtraining_file = f"{BASE_PATH}/{training_file}"
test_file = f"{BASE_PATH}/{test_file}"</pre>
<p>Finally, we must set a variable called <strong class="source-inline">stub</strong>, which we will use to save <span class="No-Break">our model:</span></p>
<pre class="source-code">
# get file name for savingstub = (Path(training_file).stem)</pre>
<p>The Hugging Face <strong class="source-inline">transformers</strong> library works well with the <strong class="source-inline">datasets</strong> library. So, next, we will load the data files, remove any unwanted columns, and create a <strong class="source-inline">DatasetDict</strong> object <a id="_idIndexMarker769"/>that will be used <a id="_idIndexMarker770"/>in subsequent parts of <span class="No-Break">the pipeline:</span></p>
<pre class="source-code">
def get_tweets_dataset():  data_files = {"train": training_file, "test": test_file}
  ds = datasets.load_dataset("csv", data_files=data_files,
                             delimiter=",",
                             encoding='utf-8')
  ds_columns = ds['train'].column_names
  drop_columns = [x for x in ds_columns if x not in COLS]
  ds = ds.remove_columns(drop_columns)
  dd = datasets.DatasetDict({"train":ds["train"],
                             "test":ds["test"]})
  return dd
dataset = get_tweets_dataset()</pre>
<p>Next, we must create a function that tokenizes the tweets from the training and test datasets that are held in the <strong class="source-inline">dataset</strong> variable. Simply put, the job of the tokenizer is to prepare the data, making it ready for input to a model. It does this by splitting sentences into words (tokens) and then splitting words into pieces (for example, <em class="italic">flyfishing</em> would be split into <em class="italic">fly</em>, <em class="italic">fish</em>, and <em class="italic">ing</em>). These tokens are then split into IDs (numbers) via a lookup table. Typically, you would use the tokenizer associated with the model that you are using. For example, for <a id="_idIndexMarker771"/>the <strong class="source-inline">bert-base-cased</strong> model, you would use <strong class="source-inline">BertTokenizer</strong>. However, in the following code, we have used something called <strong class="source-inline">AutoTokenizer</strong>. <strong class="source-inline">AutoTokenizer</strong> is a generic tokenizer auto class that automatically fetches the correct tokenizer class from the Hugging Face tokenizers library, as well as the data associated <a id="_idIndexMarker772"/>with the model’s tokenizer. An auto class is a generic class that simplifies the coding process by automatically finding the architecture of a pretrained model based on <a id="_idIndexMarker773"/>its name. All we need to do is choose the appropriate <strong class="source-inline">AutoModel</strong> for our task. Essentially, they are more flexible and make the programming <span class="No-Break">somewhat simpler:</span></p>
<pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained(model_name)def tokenise_function(tweets):
    return tokenizer(tweets["tweet"],
                     padding="max_length",
                     truncation=True,
                     max_length = 512)
tokenised_datasets = dataset.map(tokenise_function, batched=True)</pre>
<p>Now comes the interesting part! We need to load a model for classification. As before, we could have used a specific BERT model that has been trained for sentence classification, such<a id="_idIndexMarker774"/> as <strong class="source-inline">BertForSequenceClassification</strong>. However, we have chosen to use an auto class to obtain the text classification model. In this case, since we are classifying text, we used <strong class="source-inline">AutoModelForSequenceClassification</strong> as <strong class="source-inline">AutoModel</strong>. We just supplied the name of the model and the number of labels that we are dealing with – the library takes care of <span class="No-Break">the rest:</span></p>
<pre class="source-code">
model = AutoModelForSequenceClassification.from_pretrained(    model_name,
    num_labels=NUM_LABELS)
training_args = TrainingArguments(output_dir=f"{stub}")</pre>
<p>We are now ready to train the model, but first, we need to set up some arguments specifying what we want the trainer to do. We can do this by simply creating a <strong class="source-inline">TrainingArguments</strong> instance telling it where to save our model and that we want it to evaluate at the end of each epoch. The arguments are passed to <strong class="source-inline">Trainer</strong>, along with the model and<a id="_idIndexMarker775"/> the training and test datasets. Now, it is a simple matter of invoking the training and waiting for the results. Note how we save the <span class="No-Break">resultant model:</span></p>
<pre class="source-code">
training_args = TrainingArguments(    output_dir=f"{stub}",
    evaluation_strategy="epoch")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenised_datasets["train"],
    eval_dataset=tokenised_datasets["test"],
)
trainer.train()
trainer.save_model(stub)</pre>
<p>If all went <a id="_idIndexMarker776"/>well, you should see something like <span class="No-Break">this (truncated):</span></p>
<pre class="source-code">
The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, tweet. If ID, tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message./usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 3860
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed &amp; accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1449
  Number of trainable parameters = 108313348
 [1449/1449 19:47, Epoch 3/3]
Epoch  Training Loss  Validation Loss
1  No log  0.240059
2  0.555900  0.210987
3  0.208900  0.179072
Training completed. Do not forget to share your model on huggingface.co/models =)
Saving model checkpoint to SEM4_EN_train
Configuration saved in SEM4_EN_train/config.json
Model weights saved in SEM4_EN_train/pytorch_model.bin</pre>
<p class="callout-heading">Note</p>
<p class="callout">The algorithms and programs used in this chapter all randomize aspects of the data, particularly the initial assignments of weights to internal nodes of the network, and hence the results that you obtain by running the same scripts on the same data may vary slightly from the results in <span class="No-Break">the text.</span></p>
<p>Now <a id="_idIndexMarker777"/>that we have a model <a id="_idIndexMarker778"/>fine-tuned on our dataset, we can see whether it does a good job on our <span class="No-Break">test dataset:</span></p>
<pre class="source-code">
predictions = trainer.predict(tokenized_datasets["test"])</pre> <p>Finally, we can set up a dictionary of measures and iterate through them, computing and printing as <span class="No-Break">we go:</span></p>
<pre class="source-code">
model_predictions = np.argmax(predictions.predictions,    axis=1)
model_predictions = model_predictions.tolist()
model_references = tokenised_datasets["test"]["label"]
measures = [
              ["precision" , "macro"],
              ["recall" , "macro"],
              ["f1" , "micro"],
              ["f1" , "macro"],
              ["jaccard" , "macro"],
              ["accuracy" , None],
            ]
for measure in measures:
  measure_name = measure[0]
  average = measure[1]
  if measure_name = = "jaccard":
    results = get_jaccard_score(references = model_references,
    predictions = model_predictions,average = average)
  else:
    metric = evaluate.load(measure_name)
    if measure_name=="accuracy":
      results = metric.compute(references = model_references,
      predictions = model_predictions)
    else:
      results = metric.compute(references = model_references,
        predictions = model_predictions, average = average)
  print(measure_name, average, results[measure_name])</pre>
<p>This<a id="_idIndexMarker779"/> will generate something <span class="No-Break">like </span><span class="No-Break"><a id="_idIndexMarker780"/></span><span class="No-Break">this:</span></p>
<pre class="source-code">
precision macro 0.9577305808563304recall macro 0.9592563645499727
f1 micro 0.9576446280991735
f1 macro 0.9576513771741846
jaccard macro 0.9192365565992706
accuracy None 0.9576446280991735</pre>
<p>The results from the model are summarized in the <span class="No-Break">following table:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-9">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Dataset</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Jaccard</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">SEM4-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.962</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.964</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.962</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.962</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.927</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">WASSA-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.855</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.861</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.855</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.856</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.753</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">CARER-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.881</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.921</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.927</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.896</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.816</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">SEM4-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.817</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.837</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.843</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.825</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.710</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">SEM4-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.791</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.786</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.807</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.787</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.663</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">IMDB-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.905</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.905</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.905</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.905</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.826</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 – Scores for transformer-based models for the single-label datasets</p>
<p>Most <a id="_idIndexMarker781"/>of the scores here are better<a id="_idIndexMarker782"/> than the scores that we obtained using the classifiers earlier in this book, though the best classifier for WASSA-EN and CARER-EN remains the single-class SVM. The scores for SEM4-AR and SEM4-ES are both significantly better than the previous scores, possibly because the pretrained models do a better job of finding roots, and maybe even doing disambiguation, than the simple stemmers we used in the earlier chapters. It is very hard to extract intermediate results from a complex DNN such as a transformer, so it is even more difficult than was the case in previous chapters to analyze why one classifier of this kind does better than another, but it seems likely that this is a key factor in <span class="No-Break">these cases.</span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor190"/>Multi-emotion datasets</h2>
<p>Now, let’s <a id="_idIndexMarker783"/>build a transformer model to <a id="_idIndexMarker784"/>classify multi-label tweets. Much of the code is similar, so we won’t reproduce that, concentrating instead on the interesting bits of the multi-classification problem. We will be following <span class="No-Break">these steps:</span></p>
<ol>
<li>Install the <span class="No-Break">necessary libraries.</span></li>
<li>Import the <span class="No-Break">necessary libraries.</span></li>
<li>Provide access to <span class="No-Break">Google Drive.</span></li>
<li>Create <span class="No-Break">dataset variables.</span></li>
<li>Convert datasets <span class="No-Break">into </span><span class="No-Break"><strong class="source-inline">DatasetDict</strong></span><span class="No-Break">.</span></li>
<li>Load and prepare <span class="No-Break">the datasets.</span></li>
<li>Tokenize <span class="No-Break">the datasets.</span></li>
<li>Load a model <span class="No-Break">for classification.</span></li>
<li>Define <span class="No-Break">metric functions.</span></li>
<li>Set up <span class="No-Break">trainer arguments.</span></li>
<li>Train <span class="No-Break">the model.</span></li>
<li><span class="No-Break">Evaluate.</span></li>
</ol>
<p><span class="No-Break">Let’s begin!</span></p>
<p>We must install and<a id="_idIndexMarker785"/> import the libraries<a id="_idIndexMarker786"/> as before, and also allow access to Google Drive as before. Now, let’s get the data files from the online repository. However, the KWT files are in Google Drive, so we need some code to load and convert these into a <span class="No-Break"><strong class="source-inline">DatasetDict</strong></span><span class="No-Break"> object:</span></p>
<pre class="source-code">
def get_kwt_tweets_dataset(code):  if code == "KWTM":
    training_file = "train-KWT-M.csv"
    test_file = "test-KWT-M.csv"
  else:
    training_file = "train-KWT-U.csv"
    test_file = "test-KWT-U.csv"
  # add the base path
  training_file = f"{BASE_PATH}/{training_file}"
  test_file = f"{BASE_PATH}/{test_file}"
  data_files = {"train": training_file, "validation": test_file}
  ds = datasets.load_dataset("csv", data_files=data_files,
        delimiter=",",encoding='utf-8')
  dd = datasets.DatasetDict(
                            {"train":ds["train"],
                             "validation":ds["validation"]
                            })
  return dd</pre>
<p>Our <strong class="source-inline">Dataset</strong> enum<a id="_idIndexMarker787"/> now also reflects the fact that we are working with different files, so let’s use <strong class="source-inline">enum</strong> to get the <a id="_idIndexMarker788"/>right data files and set <span class="No-Break">the model:</span></p>
<pre class="source-code">
class Dataset(Enum):  SEM11_AR=1
  SEM11_EN=2
  SEM11_ES=3
  KWT_M_AR=4
  KWT_U_AR=5
ds = Dataset.SEM11_EN
if (ds == Dataset.SEM11_AR):
  dataset = load_dataset("sem_eval_2018_task_1",
    "subtask5.arabic")
  model_name = "asafaya/bert-base-arabic"
elif (ds == Dataset.SEM11_EN):
  dataset = load_dataset("sem_eval_2018_task_1",
    "subtask5.english")
  model_name = "bert-base-cased"
elif(ds == Dataset.SEM11_ES):
  dataset = load_dataset("sem_eval_2018_task_1",
    "subtask5.spanish")
  model_name = "dccuchile/bert-base-spanish-wwm-cased"
elif(ds == Dataset.KWT_M_AR):
  dataset = get_tweets_dataset("KWTM")
  model_name = "asafaya/bert-base-arabic"
elif(ds == Dataset.KWT_U_AR):
  dataset = get_tweets_dataset("KWTU")
  model_name = "asafaya/bert-base-arabic"</pre>
<p>There are<a id="_idIndexMarker789"/> three types of datasets: train, test, and validation. We <a id="_idIndexMarker790"/>will use the train and <span class="No-Break">validation datasets:</span></p>
<pre class="source-code">
DatasetDict({    train: Dataset({
        features: ['ID', 'Tweet', 'anger', 'anticipation',
        'disgust', 'fear', 'joy', 'love', 'optimism',
        'pessimism', 'sadness', 'surprise', 'trust'],
        num_rows: 6838
    })
    test: Dataset({
        features: ['ID', 'Tweet', 'anger', 'anticipation',
        'disgust', 'fear', 'joy', 'love', 'optimism',
        'pessimism', 'sadness', 'surprise', 'trust'],
        num_rows: 3259
    })
    validation: Dataset({
        features: ['ID', 'Tweet', 'anger', 'anticipation',
        'disgust', 'fear', 'joy', 'love', 'optimism',
        'pessimism', 'sadness', 'surprise', 'trust'],
        num_rows: 886
    })
})</pre>
<p>Note <a id="_idIndexMarker791"/>how in the first example, we<a id="_idIndexMarker792"/> had to set <strong class="source-inline">NUM_LABELS</strong> and we had no idea of what the actual labels were. Here, we are going to dynamically work out the labels and also create some lookup tables that allow us to easily go from an emotion to a label and <span class="No-Break">vice versa:</span></p>
<pre class="source-code">
Labels = [label for label in dataset[ 'train'].features.keys() if label not in ['ID', 'Tweet']]id2label = {idx:label for idx, label in enumerate(labels)}
label2id = {label:idx for idx, label in enumerate(labels)}</pre>
<p>The following output clarifies what each of these <span class="No-Break">looks like:</span></p>
<pre class="source-code">
['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']{0: 'anger', 1: 'anticipation', 2: 'disgust', 3: 'fear', 4: 'joy', 5: 'love', 6: 'optimism', 7: 'pessimism', 8: 'sadness', 9: 'surprise', 10: 'trust'}
{'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'love': 5, 'optimism': 6, 'pessimism': 7, 'sadness': 8, 'surprise': 9, 'trust': 10}</pre>
<p>Now, we <a id="_idIndexMarker793"/>need to tokenize the datasets, as we did previously. The task is slightly more complicated here as we have multiple labels for each tweet, and the labels are loaded as <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong>, whereas we need <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> for our model. The <strong class="source-inline">tokenize_function</strong> takes 1,000 tweets at a time, tokenizes the<a id="_idIndexMarker794"/> tweet text as before, and converts the labels into an array of 1s <span class="No-Break">and 0s:</span></p>
<pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained(model_name)def tokenise_function(tweets):
  text = tweets["Tweet"]
 encoding = tokenizer(text,
                      padding="max_length",
                      truncation=True,
                      max_length=512)
  labels_batch = {k: tweets[k] for k in tweets.keys() if k in labels}
  labels_matrix = np.zeros((len(text), len(labels)))
  for idx, label in enumerate(labels):
    labels_matrix[:, idx] = labels_batch[label]
  encoding["labels"] = labels_matrix.tolist()
  return encoding
encoded_dataset = dataset.map(tokenise_function,
        batched=True,
        remove_columns = dataset['train'].column_names)</pre>
<p>We <a id="_idIndexMarker795"/>are using <strong class="source-inline">pytorch</strong> in this example, so we need to set the format of the dataset so that <span class="No-Break">it’s compatible:</span></p>
<pre class="source-code">
encoded_dataset.set_format("torch")</pre> <p>Now, we can instantiate an auto class, as we did previously. Note how we have to set <strong class="source-inline">problem_type</strong> and also pass in the <strong class="source-inline">id-label</strong> and <strong class="source-inline">label-id</strong> <span class="No-Break">mapping objects:</span></p>
<pre class="source-code">
model = AutoModelForSequenceClassification.from_pretrained(    model_name,
    problem_type="multi_label_classification",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id
    )</pre>
<p>Next, we<a id="_idIndexMarker796"/> need to define some functions to calculate some metrics for us. Because we have multi-labels, we are dealing with probabilities. Consequently, we need a threshold to distinguish between a 0 and a 1 for the emotion – we have arbitrarily set this to <strong class="source-inline">0.5</strong> for now. In practice, this would need to be carefully determined. These probabilities are turned into 0s and 1s using the threshold and, as before, we piggyback on <strong class="source-inline">scikit-learn</strong> functions to do the heavy lifting <span class="No-Break">for us:</span></p>
<pre class="source-code">
def compute_multi_label_metrics(predictions,        labels, threshold=0.5):
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs &gt;= threshold)] = 1
    y_true = labels
    f1_macro_average = f1_score(y_true=y_true,
                                y_pred=y_pred,
                                average='macro')
    f1_micro_average = f1_score(y_true=y_true,
                                y_pred=y_pred,
                                average='micro')
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred,
        average = 'macro')
    recall = recall_score(y_true, y_pred,
        average = 'macro')
    jaccard = jaccard_score(y_true, y_pred,
        average='macro')
    metrics = {
                'precision': precision,
                'recall': recall,
                'f1_micro_average': f1_micro_average,
                'f1_macro_average': f1_macro_average,
                'jaccard': jaccard,
                'accuracy': accuracy
              }
    return metrics
def compute_metrics(p: EvalPrediction):
    if isinstance(p.predictions, tuple):
      preds = p.predictions[0]
    else:
      preds = p.predictions
    result = compute_multi_label_metrics(predictions=preds,
                                         labels=p.label_ids)
    return result</pre>
<p>We <a id="_idIndexMarker797"/>can now set up some <strong class="source-inline">TrainingArguments</strong> and <a id="_idIndexMarker798"/>train <span class="No-Break">the model:</span></p>
<pre class="source-code">
metric_name = "jaccard"training_args = TrainingArguments(
    model_name,
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    num_train_epochs = 3,
    load_best_model_at_end = True,
    metric_for_best_model = metric_name,
)
trainer = Trainer(
    model,
    training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()</pre>
<p>The final <a id="_idIndexMarker799"/>step is to evaluate the results using our metric functions. Notice how we pass the name of the <strong class="source-inline">compute_metrics</strong> function as a parameter. This function, in turn, calls <strong class="source-inline">compute_multi_label_metrics</strong> to calculate the <span class="No-Break">various metrics:</span></p>
<pre class="source-code">
trainer.evaluate()</pre> <p>The final <a id="_idIndexMarker800"/>results should look something <span class="No-Break">like this:</span></p>
<pre class="source-code">
{'eval_loss': 0.3063639998435974, 'eval_precision': 0.6944130688122799,
 'eval_recall': 0.4961206747689895,
 'eval_f1_micro_average': 0.7107381546134663,
 'eval_f1_macro_average': 0.539464842236441,
 'eval_jaccard': 0.4181996269238169,
 'eval_accuracy': 0.30242437923250563,
 'eval_runtime': 26.6373,
 'eval_samples_per_second': 33.262,
 'eval_steps_per_second': 4.167,
 'epoch': 3.0}</pre>
<p>The results from the model are summarized in the <span class="No-Break">following table:</span></p>
<table class="No-Table-Style" id="table002-6">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Dataset</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Jaccard</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.694</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.496</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.710</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.539</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.418</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.552</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.441</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.658</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.462</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.359</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.132</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.074</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.224</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.092</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.053</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.594</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.399</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.597</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.463</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.340</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.2 – Scores for transformer-based models for the multi-class datasets</p>
<p>Again, the transformer-based models work better for some, but not all,  datasets. It is worth noting that the previous best classifier for SEM11-AR was the stemmed version of the simple lexical model from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Sentiment Lexicons and Vector-Space Models</em>, with a Jaccard score of 0.386. For SEM11-ES, it was the stemmed version of the conditional probability model, also from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Sentiment Lexicons and Vector Space Models</em>, with a Jaccard score of 0.278. As with the single-class datasets, it seems likely that using the pretrained models may have helped us with identifying and disambiguating tokens, but this time, the underlying model is less good at handling <a id="_idIndexMarker801"/>multi-class cases. The score for the KWT.M-AR dataset is particularly poor: using transformers of the kind described here does not seem to be a good way to handle datasets with large numbers of tweets with no emotion ascribed <span class="No-Break">to them.</span></p>
<p>In several cases, using <a id="_idIndexMarker802"/>transformers produced better results than the classifiers from previous chapters. The following table shows the scores for a range of classifiers on our datasets (given the number of classifiers we have looked at now, this table only includes ones that were the best on at least <span class="No-Break">one dataset):</span></p>
<table class="No-Table-Style" id="table003-5">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LEX (unstemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LEX (stemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SVM (single)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SNN (single)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Transformers</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.503</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.497</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.845</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.829</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.927</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.347</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.348</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.224</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.242</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.418</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">WASSA-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.445</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.437</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.770</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.737</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.753</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">CARER-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.350</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.350</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.770</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.820</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.816</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">IMDB-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.722</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.667</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.736</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.793</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.826</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.506</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.509</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.514</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.504</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.710</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.378</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.386</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.216</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.221</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.359</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.687</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.663</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.631</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.028</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.053</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.425</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.420</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.412</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.337</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.663</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.269</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.271</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.226</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.221</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break" lang="en-US" xml:lang="en-US">0.340</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.3 – Best scores to date for the standard datasets – Jaccard scores</p>
<p>The results of <a id="_idIndexMarker803"/>using transformers are better than any of the previous classifiers for 6 of our 10 datasets, though surprisingly, the very simple lexicon-based classifiers from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em> still produce the best results for the multi-class <span class="No-Break">Arabic datasets!</span></p>
<p>It remains <a id="_idIndexMarker804"/>the case that there is a drop-off in performance between the single-emotion datasets and the multi-emotion datasets. As before, this is likely to be due to a combination of factors. The fact that the multi-class datasets have more labels than the others also makes the task harder, simply because there is more scope for mistakes. However, we know that multi-emotion classification is much more difficult than single-emotion classification because it involves working out how many emotions a text expresses, from zero upward, rather than just choosing the one with the highest scores. We will look at ways of dealing with this kind of data in more detail in <a href="B18714_10.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter </em></span><span class="No-Break"><em class="italic">10</em></span></a><span class="No-Break">, </span><span class="No-Break"><em class="italic">Multiclassifiers</em></span><span class="No-Break">.</span></p>
<p>An interesting, natural question here is, why do transformers perform better than other methods? We have already seen how the self-attention mechanism allows transformers to focus on different parts of the input sequence when making predictions, hence allowing them to capture important long-range dependencies and contextual information. This is important for robust classification. Furthermore, we have also seen how transformers use multi-head attention, which allows them to attend to different parts of the input sequence simultaneously, thus making them more effective at capturing the different types of information that may be important for robust classification. Transformers also handle long input sequences without losing important information, and this may also be more useful in classification than other tasks. Finally, as we have seen, transformers are <a id="_idIndexMarker805"/>pretrained on huge datasets. Hence, even before fine-tuning, they already know <a id="_idIndexMarker806"/>general representations of language. These concepts can be combined in a highly effective way to create a mechanism that can generate <span class="No-Break">good results.</span></p>
<p>Now, let’s summarize what we’ve learned in <span class="No-Break">this chapter.</span></p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor191"/>Summary</h1>
<p>Transformers have proved to be very successful in a range of natural language tasks, with numerous recently released chatbots outperforming existing models in their ability to understand and manipulate human language. In this chapter, we looked at how transformers can be used for the task of assigning emotions to informal texts and investigated how well they perform on this task with a range of datasets. We started by taking a brief look at transformers, focusing on the individual components of a transformer, and how data flows through them. Transformers need a lot of data to be effective and produce good results, and a huge amount of computing power and time is also needed. Then, we introduced Hugging Face, discussed why it was useful, and introduced some of the more common pretrained models that are available on the Hugging Face platform, before moving on to discussing how transformers are used for classification. Finally, we showed how to code classifiers using transformers for single-emotion datasets and multi-emotion datasets before rounding off this chapter by discussing the results. In the next chapter, we will look <span class="No-Break">at multiclassifiers.</span></p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor192"/>References</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
<ul>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I., 2017. <em class="italic">Attention Is All You Need</em>. Advances in neural information processing <span class="No-Break">systems, 30.</span></li>
<li>Rothman, D., 2021. <em class="italic">Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more</em>. Packt <span class="No-Break">Publishing Ltd.</span></li>
<li>Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., 2018. <em class="italic">Bert: Pre-training of deep bidirectional transformers for language understanding</em>. arXiv <span class="No-Break">preprint arXiv:1810.04805.</span></li>
<li>Clark, K., Luong, M. T., Le, Q. V. and Manning, C. D., 2020. <em class="italic">Electra: Pre-training text encoders as discriminators rather than generators</em>. arXiv <span class="No-Break">preprint arXiv:2003.10555.</span></li>
<li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., and Agarwal, S., 2020. <em class="italic">Language models are few-shot learners</em>. Advances in neural information processing systems, <span class="No-Break">33, pp.1877-1901.</span></li>
<li>Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B., 2019. <em class="italic">Megatron-lm: Training multi-billion parameter language models using model parallelism</em>. arXiv <span class="No-Break">preprint arXiv:1909.08053.</span></li>
<li>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V., 2019. <em class="italic">RoBERTa: A Robustly Optimized BERT Pretraining Approach</em>. arXiv <span class="No-Break">preprint arXiv:1907.11692.</span></li>
<li>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J., 2020. <em class="italic">Exploring the limits of transfer learning with a unified text-to-text transformer</em>. J. Mach. Learn. Res., <span class="No-Break">21(140), pp.1-67.</span></li>
<li>Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R., 2019. <em class="italic">Transformer-xl: Attentive language models beyond a fixed-length context</em>. arXiv <span class="No-Break">preprint arXiv:1901.02860.</span></li>
<li>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V., 2019. <em class="italic">XLNet: Generalized autoregressive pretraining for language understanding</em>. Advances in neural information processing <span class="No-Break">systems, 32.</span></li>
</ul>
</div>
</div></body></html>