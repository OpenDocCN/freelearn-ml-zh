- en: Chapter 6. Classification (II) – Neural Network and SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data with a support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the cost of a support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing an SVM fit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting labels based on a model trained by a support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning a support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network with neuralnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a neural network trained by neuralnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting labels based on a model trained by neuralnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network with nnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting labels based on a model trained by nnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most research has shown that **support vector machines** (**SVM**) and **neural
    networks** (**NN**) are powerful classification tools, which can be applied to
    several different areas. Unlike tree-based or probabilistic-based methods that
    were mentioned in the previous chapter, the process of how support vector machines
    and neural networks transform from input to output is less clear and can be hard
    to interpret. As a result, both support vector machines and neural networks are
    referred to as black box methods.
  prefs: []
  type: TYPE_NORMAL
- en: The development of a neural network is inspired by human brain activities. As
    such, this type of network is a computational model that mimics the pattern of
    the human mind. In contrast to this, support vector machines first map input data
    into a high dimension feature space defined by the kernel function, and find the
    optimum hyperplane that separates the training data by the maximum margin. In
    short, we can think of support vector machines as a linear algorithm in a high
    dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both these methods have advantages and disadvantages in solving classification
    problems. For example, support vector machine solutions are the global optimum,
    while neural networks may suffer from multiple local optimums. Thus, choosing
    between either depends on the characteristics of the dataset source. In this chapter,
    we will illustrate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to train a support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observing how the choice of cost can affect the SVM classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the SVM fit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the labels of a testing dataset based on the model trained by SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning the SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the neural network section, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: How to train a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to visualize a neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the labels of a testing dataset based on a model trained by `neuralnet`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will show how to train a neural network with `nnet`, and how to
    use it to predict the labels of a testing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying data with a support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two most well known and popular support vector machine tools are `libsvm`
    and `SVMLite`. For R users, you can find the implementation of `libsvm` in the
    `e1071` package and `SVMLite` in the `klaR` package. Therefore, you can use the
    implemented function of these two packages to train support vector machines. In
    this recipe, we will focus on using the `svm` function (the `libsvm` implemented
    version) from the `e1071` package to train a support vector machine based on the
    telecom customer churn data training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will continue to use the telecom churn dataset as the input
    data source to train the support vector machine. For those who have not prepared
    the dataset, please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to train the SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `e1071` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the support vector machine using the `svm` function with `trainset` as
    the input dataset, and use `churn` as the classification category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can obtain overall information about the built model with `summary`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The support vector machine constructs a hyperplane (or set of hyperplanes)
    that maximize the margin width between two classes in a high dimensional space.
    In these, the cases that define the hyperplane are support vectors, as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Support Vector Machine'
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine starts from constructing a hyperplane that maximizes
    the margin width. Then, it extends the definition to a nonlinear separable problem.
    Lastly, it maps the data to a high dimensional space where the data can be more
    easily separated with a linear boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using SVM is that it builds a highly accurate model through
    an engineering problem-oriented kernel. Also, it makes use of the regularization
    term to avoid over-fitting. It also does not suffer from local optimal and multicollinearity.
    The main limitation of SVM is its speed and size in the training and testing time.
    Therefore, it is not suitable or efficient enough to construct classification
    models for data that is large in size. Also, since it is hard to interpret SVM,
    how does the determination of the kernel take place? Regularization is another
    problem that we need tackle.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we continue to use the telecom `churn` dataset as our example
    data source. We begin training a support vector machine using `libsvm` provided
    in the `e1071` package. Within the training function, `svm`, one can specify the
    `kernel` function, cost, and the `gamma` function. For the `kernel` argument,
    the default value is radial, and one can specify the kernel to a linear, polynomial,
    radial basis, and sigmoid. As for the `gamma` argument, the default value is equal
    to (1/data dimension), and it controls the shape of the separating hyperplane.
    Increasing the `gamma` argument usually increases the number of support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: As for the cost, the default value is set to 1, which indicates that the regularization
    term is constant, and the larger the value, the smaller the margin is. We will
    discuss more on how the cost can affect the SVM classifier in the next recipe.
    Once the support vector machine is built, the `summary` function can be used to
    obtain information, such as calls, parameters, number of classes, and the types
    of label.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another popular support vector machine tool is `SVMLight`. Unlike the `e1071`
    package, which provides the full implementation of `libsvm`, the `klaR` package
    simply provides an interface to `SVMLight` only. To use `SVMLight`, one can perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `klaR` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Download the `SVMLight` source code and binary for your platform from [http://svmlight.joachims.org/](http://svmlight.joachims.org/).
    For example, if your guest OS is Windows 64-bit, you should download the file
    from [http://download.joachims.org/svm_light/current/svm_light_windows64.zip](http://download.joachims.org/svm_light/current/svm_light_windows64.zip).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, you should unzip the file and put the workable binary in the working
    directory; you may check your working directory by using the `getwd` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the support vector machine using the `svmlight` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Choosing the cost of a support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The support vector machines create an optimum hyperplane that separates the
    training data by the maximum margin. However, sometimes we would like to allow
    some misclassifications while separating categories. The SVM model has a cost
    function, which controls training errors and margins. For example, a small cost
    creates a large margin (a soft margin) and allows more misclassifications. On
    the other hand, a large cost creates a narrow margin (a hard margin) and permits
    fewer misclassifications. In this recipe, we will illustrate how the large and
    small cost will affect the SVM classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use the `iris` dataset as our example data source.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate two different classification examples
    with different costs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subset the `iris` dataset with columns named as `Sepal.Length`, `Sepal.Width`,
    `Species`, with species in `setosa` and `virginica`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can generate a scatter plot with `Sepal.Length` as the x-axis and
    the `Sepal.Width` as the y-axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00113.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2: Scatter plot of Sepal.Length and Sepal.Width with subset of iris
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you can train SVM based on `iris.subset` with the cost equal to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can circle the support vector with blue circles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00114.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3: Circling support vectors with blue ring'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lastly, we can add a separation line on the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00115.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4: Add separation line to scatter plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In addition to this, we create another SVM classifier where `cost = 10,000`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00116.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: A classification example with large cost'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how different costs can affect the SVM classifier.
    First, we create an iris subset with the columns, `Sepal.Length`, `Sepal.Width`,
    and `Species` containing the species, `setosa` and `virginica`. Then, in order
    to create a soft margin and allow some misclassification, we use an SVM with small
    cost (where `cost = 1`) to train the support of the vector machine. Next, we circle
    the support vectors with blue circles and add the separation line. As per *Figure
    5*, one of the green points (`virginica`) is misclassified (it is classified to
    `setosa`) to the other side of the separation line due to the choice of the small
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, we would like to determine how a large cost can affect
    the SVM classifier. Therefore, we choose a large cost (where `cost = 10,000`).
    From Figure 5, we can see that the margin created is narrow (a hard margin) and
    no misclassification cases are present. As a result, the two examples show that
    the choice of different costs may affect the margin created and also affect the
    possibilities of misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of soft margin, which allows misclassified examples, was suggested
    by Corinna Cortes and Vladimir N. Vapnik in 1995 in the following paper: Cortes,
    C., and Vapnik, V. (1995). *Support-vector networks. Machine learning*, 20(3),
    273-297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing an SVM fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To visualize the built model, one can first use the plot function to generate
    a scatter plot of data input and the SVM fit. In this plot, support vectors and
    classes are highlighted through the color symbol. In addition to this, one can
    draw a contour filled plot of the class regions to easily identify misclassified
    samples from the plot.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will use two datasets: the `iris` dataset and the telecom
    `churn` dataset. For the telecom `churn` dataset, one needs to have completed
    the previous recipe by training a support vector machine with SVM, and to have
    saved the SVM fit model.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to visualize the SVM fit object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use SVM to train the support vector machine based on the iris dataset, and
    use the `plot` function to visualize the fitted model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00117.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6: The SVM classification plot of trained SVM fit based on iris dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the SVM fit object, `model`, using the `plot` function with the dimensions
    of `total_day_minutes` and `total_intl_charge`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00118.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: The SVM classification plot of trained SVM fit based on churn dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use the `plot` function to visualize the
    SVM fit. In the first plot, we train a support vector machine using the `iris`
    dataset. Then, we use the `plot` function to visualize the fitted SVM.
  prefs: []
  type: TYPE_NORMAL
- en: In the argument list, we specify the fitted model in the first argument and
    the dataset (this should be the same data used to build the model) as the second
    parameter. The third parameter indicates the dimension used to generate the classification
    plot. By default, the `plot` function can only generate a scatter plot based on
    two dimensions (for the x-axis and y-axis). Therefore, we select the variables,
    `Petal.Length` and `Petal.Width` as the two dimensions to generate the scatter
    plot.
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 6*, we find `Petal.Length` assigned to the x-axis, `Petal.Width`
    assigned to the y-axis, and data points with `X` and `O` symbols scattered on
    the plot. Within the scatter plot, the `X` symbol shows the support vector and
    the `O` symbol represents the data points. These two symbols can be altered through
    the configuration of the `svSymbol` and `dataSymbol` options. Both the support
    vectors and true classes are highlighted and colored depending on their label
    (green refers to viginica, red refers to versicolor, and black refers to setosa).
    The last argument, `slice`, is set when there are more than two variables. Therefore,
    in this example, we use the additional variables, `Sepal.width` and `Sepal.length`,
    by assigning a constant of `3` and `4`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we take the same approach to draw the SVM fit based on customer churn
    data. In this example, we use `total_day_minutes` and `total_intl_charge` as the
    two dimensions used to plot the scatterplot. As per *Figure 7*, the support vectors
    and data points in red and black are scattered closely together in the central
    region of the plot, and there is no simple way to separate them.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are other parameters, such as `fill`, `grid`, `symbolPalette`, and so
    on, that can be configured to change the layout of the plot. You can use the `help`
    function to view the following document for further information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Predicting labels based on a model trained by a support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we trained an SVM based on the training dataset. The
    training process finds the optimum hyperplane that separates the training data
    by the maximum margin. We can then utilize the SVM fit to predict the label (category)
    of new observations. In this recipe, we will demonstrate how to use the `predict`
    function to predict values based on a model trained by SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by generating a fitted SVM, and
    save the fitted model in model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to predict the labels of the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict the label of the testing dataset based on the fitted SVM and attributes
    of the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can use the `table` function to generate a classification table with
    the prediction result and labels of the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can use `classAgreement` to calculate coefficients compared to the
    classification agreement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can use `confusionMatrix` to measure the prediction performance based
    on the classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we first used the `predict` function to obtain the predicted
    labels of the testing dataset. Next, we used the `table` function to generate
    the classification table based on the predicted labels of the testing dataset.
    So far, the evaluation procedure is very similar to the evaluation process mentioned
    in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We then introduced a new function, `classAgreement`, which computes several
    coefficients of agreement between the columns and rows of a two-way contingency
    table. The coefficients include diag, kappa, rand, and crand. The `diag` coefficient
    represents the percentage of data points in the main diagonal of the classification
    table, `kappa` refers to `diag`, which is corrected for an agreement by a change
    (the probability of random agreements), `rand` represents the Rand index, which
    measures the similarity between two data clusters, and `crand` indicates the Rand
    index, which is adjusted for the chance grouping of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used `confusionMatrix` from the `caret` package to measure the performance
    of the classification model. The accuracy of 0.9185 shows that the trained support
    vector machine can correctly classify most of the observations. However, accuracy
    alone is not a good measurement of a classification model. One should also reference
    sensitivity and specificity.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides using SVM to predict the category of new observations, you can use SVM
    to predict continuous values. In other words, one can use SVM to perform regression
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will show how to perform a simple regression prediction
    based on a fitted SVM with the type specified as `eps-regression`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to train a regression model with SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a support vector machine based on a `Quartet` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `predict` function to obtain prediction results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the predicted points as squares and the training data points as circles
    on the same plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![There''s more...](img/00119.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8: The scatter plot contains predicted data points and training data
    points'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tuning a support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides using different feature sets and the `kernel` function in support vector
    machines, one trick that you can use to tune its performance is to adjust the
    gamma and cost configured in the argument. One possible approach to test the performance
    of different gamma and cost combination values is to write a `for` loop to generate
    all the combinations of gamma and cost as inputs to train different support vector
    machines. Fortunately, SVM provides a tuning function, `tune.svm`, which makes
    the tuning much easier. In this recipe, we will demonstrate how to tune a support
    vector machine through the use of `tune.svm`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by preparing a training dataset,
    `trainset`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to tune the support vector machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, tune the support vector machine using `tune.svm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can use the `summary` function to obtain the tuning result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After retrieving the best performance parameter from tuning the result, you
    can retrain the support vector machine with the best performance parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, you can use the `predict` function to predict labels based on the fitted
    SVM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, generate a classification table based on the predicted and original labels
    of the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, generate a class agreement to measure the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can use a confusion matrix to measure the performance of the retrained
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To tune the support vector machine, you can use a trial and error method to
    find the best gamma and cost parameters. In other words, one has to generate a
    variety of combinations of gamma and cost for the purpose of training different
    support vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we generate different gamma values from *10^-6* to *10^-1*,
    and cost with a value of either 10 or 100\. Therefore, you can use the tuning
    function, `svm.tune`, to generate 12 sets of parameters. The function then makes
    10 cross-validations and outputs the error dispersion of each combination. As
    a result, the combination with the least error dispersion is regarded as the best
    parameter set. From the summary table, we found that gamma with a value of 0.01
    and cost with a value of 100 are the best parameters for the SVM fit.
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining the best parameters, we can then train a new support vector
    machine with gamma equal to 0.01 and cost equal to 100\. Additionally, we can
    obtain a classification table based on the predicted labels and labels of the
    testing dataset. We can also obtain a confusion matrix from the classification
    table. From the output of the confusion matrix, you can determine the accuracy
    of the newly trained model in comparison to the original model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information about how to tune SVM with `svm.tune`, you can use the
    `help` function to access this document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training a neural network with neuralnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The neural network is constructed with an interconnected group of nodes, which
    involves the input, connected weights, processing element, and output. Neural
    networks can be applied to many areas, such as classification, clustering, and
    prediction. To train a neural network in R, you can use neuralnet, which is built
    to train multilayer perceptron in the context of regression analysis, and contains
    many flexible functions to train forward neural networks. In this recipe, we will
    introduce how to use neuralnet to train a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will use an `iris` dataset as our example dataset. We will
    first split the `iris` dataset into a training and testing datasets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to train a neural network with neuralnet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First load the `iris` dataset and split the data into training and testing
    datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, install and load the `neuralnet` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the columns versicolor, setosa, and virginica based on the name matched
    value in the `Species` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, train the neural network with the `neuralnet` function with three hidden
    neurons in each layer. Notice that the results may vary with each training, so
    you might not get the same result. However, you can use set.seed at the beginning,
    so you can get the same result in every training process
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can view the `summary` information by accessing the `result.matrix`
    attribute of the built neural network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can view the generalized weight by accessing it in the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The neural network is a network made up of artificial neurons (or nodes). There
    are three types of neurons within the network: input neurons, hidden neurons,
    and output neurons. In the network, neurons are connected; the connection strength
    between neurons is called weights. If the weight is greater than zero, it is in
    an excitation status. Otherwise, it is in an inhibition status. Input neurons
    receive the input information; the higher the input value, the greater the activation.
    Then, the activation value is passed through the network in regard to weights
    and transfer functions in the graph. The hidden neurons (or output neurons) then
    sum up the activation values and modify the summed values with the transfer function.
    The activation value then flows through hidden neurons and stops when it reaches
    the output nodes. As a result, one can use the output value from the output neurons
    to classify the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Artificial Neural Network'
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of a neural network are: first, it can detect nonlinear relationships
    between the dependent and independent variable. Second, one can efficiently train
    large datasets using the parallel architecture. Third, it is a nonparametric model
    so that one can eliminate errors in the estimation of parameters. The main disadvantages
    of a neural network are that it often converges to the local minimum rather than
    the global minimum. Also, it might over-fit when the training process goes on
    for too long.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to train a neural network. First, we split
    the `iris` dataset into training and testing datasets, and then install the `neuralnet`
    package and load the library into an R session. Next, we add the columns `versicolor`,
    `setosa`, and `virginica` based on the name matched value in the `Species` column,
    respectively. We then use the `neuralnet` function to train the network model.
    Besides specifying the label (the column where the name equals to versicolor,
    virginica, and setosa) and training attributes in the function, we also configure
    the number of hidden neurons (vertices) as three in each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we examine the basic information about the training process and the trained
    network saved in the network. From the output message, it shows the training process
    needed 11,063 steps until all the absolute partial derivatives of the error function
    were lower than 0.01 (specified in the threshold). The error refers to the likelihood
    of calculating **Akaike Information Criterion** (**AIC**). To see detailed information
    on this, you can access the `result.matrix` of the built neural network to see
    the estimated weight. The output reveals that the estimated weight ranges from
    -18 to 24.40; the intercepts of the first hidden layer are 1.69, 1.41 and 24.40,
    and the two weights leading to the first hidden neuron are estimated as 0.95 (`Sepal.Length`),
    -7.22 (`Sepal.Width`), 1.79 (`Petal.Length`), and 9.94 (`Petal.Width`). We can
    lastly determine that the trained neural network information includes generalized
    weights, which express the effect of each covariate. In this recipe, the model
    generates 12 generalized weights, which are the combination of four covariates
    (`Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width`) to three responses
    (`setosa`, `virginica`, `versicolor`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a more detailed introduction on neuralnet, one can refer to the following
    paper: Günther, F., and Fritsch, S. (2010). *neuralnet: Training of neural networks*.
    *The R journal*, 2(1), 30-38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing a neural network trained by neuralnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The package, `neuralnet`, provides the `plot` function to visualize a built
    neural network and the `gwplot` function to visualize generalized weights. In
    following recipe, we will cover how to use these two functions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by training a neural network
    and have all basic information saved in the network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to visualize the neural network and the generalized
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can visualize the trained neural network with the `plot` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00121.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 10: The plot of the trained neural network'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Furthermore, you can use `gwplot` to visualize the generalized weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00122.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 11: The plot of generalized weights'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to visualize the trained neural network and
    the generalized weights of each trained attribute. As per *Figure 10*, the plot
    displays the network topology of the trained neural network. Also, the plot includes
    the estimated weight, intercepts and basic information about the training process.
    At the bottom of the figure, one can find the overall error and number of steps
    required to converge.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11* presents the generalized weight plot in regard to `network$generalized.weights`.
    The four plots in *Figure 11* display the four covariates: `Petal.Width`, `Sepal.Width`,
    `Petal.Length`, and `Petal.Width`, in regard to the versicolor response. If all
    the generalized weights are close to zero on the plot, it means the covariate
    has little effect. However, if the overall variance is greater than one, it means
    the covariate has a nonlinear effect.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information about `gwplot`, one can use the `help` function to access
    the following document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Predicting labels based on a model trained by neuralnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to other classification methods, we can predict the labels of new observations
    based on trained neural networks. Furthermore, we can validate the performance
    of these networks through the use of a confusion matrix. In the following recipe,
    we will introduce how to use the `compute` function in a neural network to obtain
    a probability matrix of the testing dataset labels, and use a table and confusion
    matrix to measure the prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by generating the training dataset,
    `trainset`, and the testing dataset, `testset`. The trained neural network needs
    to be saved in the network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to measure the prediction performance of the trained
    neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, generate a prediction probability matrix based on a trained neural network
    and the testing dataset, `testset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, obtain other possible labels by finding the column with the greatest
    probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a classification table based on the predicted labels and the labels
    of the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, generate `classAgreement` from the classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, use `confusionMatrix` to measure the prediction performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to predict labels based on a model trained
    by neuralnet. Initially, we use the `compute` function to create an output probability
    matrix based on the trained neural network and the testing dataset. Then, to convert
    the probability matrix to class labels, we use the `which.max` function to determine
    the class label by selecting the column with the maximum probability within the
    row. Next, we use a table to generate a classification matrix based on the labels
    of the testing dataset and the predicted labels. As we have created the classification
    table, we can employ a confusion matrix to measure the prediction performance
    of the built neural network.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we use the `net.result` function, which is the overall result
    of the neural network, used to predict the labels of the testing dataset. Apart
    from examining the overall result by accessing `net.result`, the `compute` function
    also generates the output from neurons in each layer. You can examine the output
    of neurons to get a better understanding of how `compute` works:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training a neural network with nnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `nnet` package is another package that can deal with artificial neural networks.
    This package provides the functionality to train feed-forward neural networks
    with traditional back propagation. As you can find most of the neural network
    function implemented in the `neuralnet` package, in this recipe we provide a short
    overview of how to train neural networks with `nnet`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we do not use the `trainset` and `trainset` generated from the
    previous step; please reload the `iris` dataset again.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to train the neural network with `nnet`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install and load the `nnet` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, split the dataset into training and testing datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, train the neural network with `nnet`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `summary` to obtain information about the trained neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate steps to train a neural network model with the
    `nnet` package. We first use `nnet` to train the neural network. With this function,
    we can set the classification formula, source of data, number of hidden units
    in the `size` parameter, initial random weight in the `rang` parameter, parameter
    for weight decay in the `decay` parameter, and the maximum iteration in the `maxit`
    parameter. As we set `maxit` to 200, the training process repeatedly runs till
    the value of the fitting criterion plus the decay term converge. Finally, we use
    the `summary` function to obtain information about the built neural network, which
    reveals that the model is built with 4-2-3 networks with 19 weights. Also, the
    model shows a list of weight transitions from one node to another at the bottom
    of the printed message.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those who are interested in the background theory of `nnet` and how it
    is made, please refer to the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: Ripley, B. D. (1996) *Pattern Recognition and Neural Networks*. Cambridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Venables, W. N., and Ripley, B. D. (2002). *Modern applied statistics with S.
    Fourth edition*. Springer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting labels based on a model trained by nnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have trained a neural network with `nnet` in the previous recipe, we can
    now predict the labels of the testing dataset based on the trained neural network.
    Furthermore, we can assess the model with a confusion matrix adapted from the
    `caret` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to have completed the previous recipe by generating the training dataset,
    `trainset`, and the testing dataset, `testset`, from the `iris` dataset. The trained
    neural network also needs to be saved as `iris.nn`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to predict labels based on the trained neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the predictions of the testing dataset based on the model, `iris.nn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a classification table based on the predicted labels and labels of
    the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, generate a confusion matrix based on the classification table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to other classification methods, one can also predict labels based on
    the neural networks trained by `nnet`. First, we use the `predict` function to
    generate the predicted labels based on a testing dataset, `testset`. Within the
    `predict` function, we specify the `type` argument to the class, so the output
    will be class labels instead of a probability matrix. Next, we use the `table`
    function to generate a classification table based on predicted labels and labels
    written in the testing dataset. Finally, as we have created the classification
    table, we can employ a confusion matrix from the `caret` package to measure the
    prediction performance of the trained neural network.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the `predict` function, if the `type` argument to `class` is not specified,
    by default, it will generate a probability matrix as a prediction result, which
    is very similar to `net.result` generated from the `compute` function within the
    `neuralnet` package:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
