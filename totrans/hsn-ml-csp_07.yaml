- en: Facial and Motion Detection – Imaging Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have seen and heard of it everywhere. Facial recognition, motion detection.
    We have motion sensors in our homes as part of our security systems. Everybody
    is doing facial recognition—there are security cameras on our streets, in our
    airports, and probably even in our homes. And if we think about everything that
    an autonomous vehicle must do, wow! There is a recent link (at the time of writing)
    on how facial recognition technology identified a suspect amongst a crowd of 50,000
    faces! [https://www.digitaltrends.com/cool-tech/facial-recognition-china-50000/](https://www.digitaltrends.com/cool-tech/facial-recognition-china-50000/)
  prefs: []
  type: TYPE_NORMAL
- en: But what exactly does that mean? How does it do what it does? What happens behind
    the scenes? And how can I use it in my applications? In this chapter, we are going
    to show two separate examples, one for facial detection and the other for motion
    detection. We'll show you exactly what goes on and just how fast you can add these
    capabilities into your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motion detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding detection to your application
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start out with facial detection. In our example, I'm going to use my friendly
    little French Bulldog, Frenchie, as our assistant. Tried to get my beautiful wife
    to do the honors but makeup, hair; well, I'm sure you know that story! Frenchie
    the Bulldog, however, had no complaints.
  prefs: []
  type: TYPE_NORMAL
- en: Before I start, please re-read the chapter title. No matter how many times you
    read it, you'll probably miss the key point here, and it's so very important.
    Notice it says **facial detection** and not **facial recognition**. This is so
    very important that I needed to stop and re-stress it. We are not trying to identify
    Joe, Bob, or Sally. We are trying to identify that, out of everything we see via
    our camera, we can detect a face. We are not concerned with whose face it is,
    just the fact that it is a face! It is so important that we understand this before
    moving on! Otherwise, your expectations will be so incorrectly biased (another
    buzzword for your checklist) that you'll make yourself confused and upset, and
    we don't want that!
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection, as I will stress again later, is the first part of facial
    recognition, a much more complicated beast. If you can't identify that there are
    one or more faces out of all the things on the screen, then you'll never be able
    to recognize whose face that is!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start things off by taking a quick look at our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76680c1c-486b-4f1a-b1c6-dff6bb74f804.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have a very simple screen. In our case, the laptop camera
    is our video capture device. Frenchie is kindly posing in front of the camera
    for us, just standing there enjoying life. But, as soon as we enable facial tracking,
    watch what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40008629-0a3a-459a-bd61-60d66bd3a4d9.png)'
  prefs: []
  type: TYPE_IMG
- en: The facial features of Frenchie are now being tracked. What you see surrounding
    Frenchie are the tracking containers (white boxes), which tell us we know that
    there is a face and where it is, and our angle detector (red line), which provides
    some insight into the horizontal aspect of our face .
  prefs: []
  type: TYPE_NORMAL
- en: 'As we move Frenchie around, the tracking container and angle detector will
    track him. That''s all well and good, but what happens if we enable facial tracking
    on a real human face? As you see here, the tracking containers and angles are
    tracking the facial figures of our guest poser, just like they did for Frenchie:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/857c656a-62f6-4f80-a86b-5eead320ed31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As our poser moves his head from side to side, the camera tracks this and you
    can see the angle detectors adjusting to what it recognizes as the horizontal
    angle of the face. In this case, you will notice that the Color space is in black
    and white and not color. This is a histogram back projection and is an option
    that you can change:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88668c56-7e07-4141-b90e-cde7763e75a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even as we move farther away from the camera, where other objects come into
    view, the facial detector can keep track of our face among the noise, as shown
    in the following screenshot. This is exactly how the facial recognition systems
    you see in movies work, albeit more advanced; and, using the code and samples
    we''ll show you shortly, you too can be up and running with your own facial recognition
    application in minutes! We''ll provide the detection; you provide the recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c941ebd9-8f3e-42d6-9a83-bdf0ae6cbd84.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we've seen how our application appears from the outside, let's look
    under the hood at what is going on.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by asking ourselves exactly what the problem is that we are trying
    to solve here. As we mentioned in previous sections, we are trying to detect (notice
    again I did not say recognize) facial images. While this is easy for a human,
    a computer needs very detailed instruction sets to accomplish this feat.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there is a very famous algorithm called the Viola-Jones algorithm that
    will do the heavy lifting for us. Why did we pick this algorithm?
  prefs: []
  type: TYPE_NORMAL
- en: It has very high detection rates and very low false positives.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is very good at real-time processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is very good at detecting faces from non-faces. Detecting faces is the first
    step in facial recognition!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This algorithm requires that the camera has a full frontal, upright view of
    the face. To be detected, the face will need to point straight towards the camera,
    not tilted and not looking up or down. Remember again; for the moment, we are
    just interested in facial detection!
  prefs: []
  type: TYPE_NORMAL
- en: 'To delve into the technical side of things, our algorithm will require four
    stages to accomplish its job. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Haar feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an integral image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaBoost training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cascading classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's think about what facial detection actually accomplished. All faces, be
    it human, animal, or otherwise, share some similar properties. For example, the
    eye is darker than the upper cheeks, the nose bridge is brighter than the eyes,
    and your forehead may be lighter than the rest of your face. Our algorithm matches
    these intuitions up by using what is known as **Haar features**. We can come up
    with matchable facial features by looking at the location and size of the eyes,
    mouth, bridge of the nose, and so forth. However, we do have an obstacle.
  prefs: []
  type: TYPE_NORMAL
- en: In a 24x24 pixel window, there are a total of 162,336 possible features. Obviously,
    to try and evaluate them all would be prohibitively expensive in both time and
    computation, if it works at all. So, we are going to work with a technique known
    as **adaptive boosting**, or more commonly, **AdaBoost**. It's another one for
    your buzzword-compliant list. If you have delved into or researched machine learning,
    I'm sure you've heard about a technique called **boosting.** That's exactly what
    AdaBoost is. Our learning algorithm will use AdaBoost to select the best features
    and train classifiers to use them.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost can be used with many types of learning algorithms and is considered
    the best out-of-the-box algorithm for many tasks where boosting is required. You
    usually won't notice how good and fast it is until you switch to a different algorithm
    and benchmark it. I have done this countless number of times, and I can tell you
    that the difference is very noticeable.
  prefs: []
  type: TYPE_NORMAL
- en: Let's give a little more definition to boosting before we continue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Boosting** takes the output from other **weak-learning** algorithms and combines
    them with a weighted sum that is the final output of the boosted classifier. The
    adaptive part of AdaBoost comes from the fact that subsequent learners are tweaked
    in favor of those instances that have been incorrectly classified by previous
    classifiers. We must be careful with our data preparation though, as AdaBoost
    is sensitive to noisy data and outliers (remember how we stressed those in [Chapter
    1](7a1f2cca-1be5-426a-8e8a-6a4a3828cd76.xhtml), *Machine Learning Basics*). The
    algorithm tends to **overfit** the data more than other algorithms, which is why,
    in our earlier chapters, we stressed on data preparation for missing data and
    outliers. In the end, if weak learning algorithms are better than random guessing,
    AdaBoost can be a valuable addition to our process.'
  prefs: []
  type: TYPE_NORMAL
- en: With that brief description behind us, let's look under the covers at what's
    happening. For this example, we will again use the **Accord framework** and we
    will work with the vision face tracking sample.
  prefs: []
  type: TYPE_NORMAL
- en: We start by creating a `FaceHaarCascade` object. This object holds a collection
    of Haar-like features' weak classification stages. There will be many stages provided,
    each containing a set of classifier trees which will be used in the decision-making
    process. We are now technically working with a decision tree. The beauty of the
    Accord framework is that `FaceHaarCascade` automatically creates all these stages
    and trees for us without exposing us to the details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what a particular stage might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now don't let that scare you off. As you can see, we are building a decision
    tree underneath the hood by providing the nodes for each stage with the numeric
    values for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, we can use our cascade object to create our `HaarObjectDetector`,
    which is what we will use for our detection. It takes:'
  prefs: []
  type: TYPE_NORMAL
- en: Our facial cascade objects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minimum window size to use when searching for objects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our search mode, given that we are searching for only a single object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The re-scaling factor to use when re-scaling our search window during the search
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to tackle the topic of our video collection source. In our
    examples, we will simply use the local camera to capture all images. However,
    the Accord.NET framework makes it easy to use other sources for image capture,
    such as a `.avi` files, animated `.jpg` files, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'We connect to the camera, select the resolution, and are then ready to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With the application now running and our video source selected, our application
    will look like this. Once again, enter Frenchie the Bulldog! Please excuse the
    mess; Frenchie is not the tidiest of assistants and he even left his empty cup
    of coffee on my table!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d88366e3-4ffc-46f6-8270-91654ffd5c7d.png)'
  prefs: []
  type: TYPE_IMG
- en: For this demonstration, you will notice that Frenchie is facing the camera,
    and in the background, we have two 55" monitors as well as many other items my
    wife likes to refer to as junk. I myself prefer to think of it as stochastic noise!
    This is done to show how the face detection algorithm can distinguish Frenchie's
    face amongst everything else. If our detector cannot handle this, it is going
    to get lost in the noise and be of little use to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our video source now coming in, we need to be notified when a new frame
    is received so that we can process it, apply our markers, and so on. We do this
    by attaching to the `NewFrameReceived` event handler of the video source player.
    As a C# developer I am assuming that you are familiar with events such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a video source and video coming in, let's look at what happens
    each time we are notified that a new video frame is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we need to do is `downsample` the image to make it easier
    to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With the image in a more manageable size, we will process the frame. If we
    have not found a facial region, we will stay in tracking mode waiting for a frame
    that has a detectable face. Once we have found a facial region, we will reset
    our tracker, locate the face, reduce its size in order to flush away any background
    noise, initialize the tracker, and apply the marker window to the image. All of
    this is accomplished with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once a face is detected, our image frame looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0b15633-e9ba-4100-8815-3634799816a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If Frenchie tilts his head to the side, our image now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1e9e136-2b6e-4d68-bb99-3722d5328748.png)'
  prefs: []
  type: TYPE_IMG
- en: Motion detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can already see that we are not only doing facial detection, but motion
    detection as well. So let's turn our focus to be a bit wider scale and detect
    any motion, not just faces. Again, we'll use Accord.NET for this and use the motion
    detection sample. As with facial detection, you will see just how simple it is
    to add this capability to your applications and instantly become a hero at work!
  prefs: []
  type: TYPE_NORMAL
- en: 'With motion detection, we will highlight anything that moves on the screen
    in red. The amount of movement is indicated by the thickness of red in any one
    area. So, using the following image, you can see that the fingers are moving but
    that everything else is motionless:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e8ac5b7-d62c-4b78-b197-dc58104d1f08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the movement of the hand increases, you can see increased movement in the
    entire hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa75855d-ffc6-4101-90df-0c00238cdaf5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the entire hand starts to move, you can see not only more red but also
    the total amount of red increasing relative to the movement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b01b4ad-3a56-45f1-a8fa-0787d465a1ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we do not wish to process the entire screen area for motion, we can define
    Motion Regions; motion detection will occur only in those regions. In the following
    image, you can see that I''ve defined a Motion Regions. You will notice in upcoming
    images that this is the only area that motion will be processed from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35721f38-ec53-4718-9b7b-aef3520b9aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we create some motion for the camera (fingers moving), we will see
    that only motion from our defined region is being processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b428b005-8bf3-4f60-94e7-eb929444dd42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also see that, with a Motion Regions defined and Peter the meditating
    Gnome in front of the region, we are still able to detect motion behind him while
    filtering out non-interesting items; but his face is not a part of the recognition.
    You could, of course, combine both processes to have the best of both worlds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce20e943-3941-4e92-9a21-906edc8af75b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another option that we can use is grid motion highlighting. This makes the
    motion detected region highlighted in red squares based on a defined grid. Basically,
    the motion area is now a red box, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a5657c4-837e-423e-ad74-62f4877516fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding detection to your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's a simple example of all you need to do to add video recognition to your
    application. As you can see, it couldn't be any easier!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we receive a new video frame, that''s when all the magic happens. Here''s
    all the code it takes to make processing a new video frame a success:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The key here is detecting the amount of motion that is happening in the frame,
    which is done with the following code. For this example, we are using a motion
    alarm level of `2`, but you can use whatever you like. Once this threshold has
    been passed, you can implement your desired logic, such as sending an alert email,
    a text, and starting video capture, and so forth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about facial and motion detection. We discovered
    various algorithms you can employ in your current environment to easily integrate
    this functionality into your applications. We also showed some easy and simple
    code you can use to quickly add such capabilities. In the next chapter, we are
    going to step into the world of artificial neural networks and tackle some very
    exciting problems!
  prefs: []
  type: TYPE_NORMAL
