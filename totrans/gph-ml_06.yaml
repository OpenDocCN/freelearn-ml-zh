- en: 'Chapter 4: Supervised Graph Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：监督图学习
- en: '**Supervised learning** (**SL**) most probably represents the majority of practical
    **machine learning** (**ML**) tasks. Thanks to more and more active and effective
    data collection activities, it is very common nowadays to deal with labeled datasets.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习（Supervised learning**）（SL）很可能是大多数实际**机器学习（ML**）任务的代表。多亏了越来越活跃和有效的数据收集活动，如今处理带标签的数据集是非常常见的。'
- en: This is also true for graph data, where labels can be assigned to nodes, communities,
    or even to an entire structure. The task, then, is to learn a mapping function
    between the input and the label (also known as a target or an annotation).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于图数据，其中标签可以分配给节点、社区，甚至整个结构。那么，任务就是学习一个从输入到标签的映射函数（也称为目标或注释）。
- en: For example, given a graph representing a social network, we might be asked
    to guess which user (node) will close their account. We can learn this predictive
    function by training graph ML on **retrospective data**, where each user is labeled
    as "faithful" or "quitter" based on whether they closed their account after a
    few months.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一个表示社交网络的图，我们可能会被要求猜测哪个用户（节点）会关闭他们的账户。我们可以通过在**历史数据**上训练图机器学习来学习这个预测函数，其中每个用户根据他们在几个月后是否关闭账户被标记为“忠诚”或“退出”。
- en: 'In this chapter, we will explore the concept of SL and how it can be applied
    on graphs. Therefore, we will also be providing an overview of the main supervised
    graph embedding methods. The following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨监督学习（SL）的概念以及它如何在图上应用。因此，我们还将提供主要监督图嵌入方法的概述。以下主题将涵盖：
- en: The supervised graph embedding roadmap
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督图嵌入路线图
- en: Feature-based methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征的方法
- en: Shallow embedding methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浅层嵌入方法
- en: Graph regularization methods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图正则化方法
- en: Graph **convolutional neural networks** (**CNNs**)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图**卷积神经网络（CNNs**）
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using *Jupyter* Notebooks with *Python* 3.8 for all of our exercises.
    In the following code block, you can see a list of the Python libraries that will
    be installed for this chapter using `pip` (for example, run `pip install networkx==2.5`
    on the command line):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用带有 *Python* 3.8 的 *Jupyter* 笔记本来进行所有练习。在下面的代码块中，你可以看到使用 `pip` 安装本章所需库的列表（例如，在命令行中运行
    `pip install networkx==2.5`）：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the rest of this book, if not clearly stated, we will refer to `nx` as the
    result of the `import networkx as nx` Python command.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，除非明确说明，否则我们将把 `nx` 作为 `import networkx as nx` Python 命令的结果来引用。
- en: All code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关的所有代码文件均可在[https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04)找到。
- en: The supervised graph embedding roadmap
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督图嵌入路线图
- en: In SL, a training set consists of a sequence of ordered pairs (*x*, *y*), where
    *x* is a set of input features (often signals defined on graphs) and *y* is the
    output label assigned to it. The goal of the ML models, then, is to learn the
    function mapping each *x* value to each *y* value. Common supervised tasks include
    predicting user properties in a large social network or predicting molecules'
    attributes, where each molecule is a graph.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习（SL）中，训练集由一系列有序对 (*x*, *y*) 组成，其中 *x* 是一组输入特征（通常是定义在图上的信号），而 *y* 是分配给它的输出标签。因此，机器学习模型的目标是学习将每个
    *x* 值映射到每个 *y* 值的函数。常见的监督任务包括预测大型社交网络中的用户属性或预测分子的属性，其中每个分子都是一个图。
- en: Sometimes, however, not all instances can be provided with a label. In this
    scenario, a typical dataset consists of a small set of labeled instances and a
    larger set of unlabeled instances. For such situations, **semi-SL** (**SSL**)
    is proposed, whereby algorithms aim to exploit label dependency information reflected
    by available label information in order to learn the predicting function for the
    unlabeled samples.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，然而，并非所有实例都能提供标签。在这种情况下，一个典型的数据集由一小部分带标签的实例和一大部分未带标签的实例组成。对于这种情况，提出了**半监督学习（semi-SL**）（也称为**SSL**），其中算法旨在利用可用标签信息反映的标签依赖信息，以便学习对未标记样本的预测函数。
- en: 'With regard to supervised graph ML techniques, many algorithms have been developed.
    However as previously reported by different scientific papers ([https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)),
    they can be grouped into macro-groups such as **feature-based methods**, **shallow
    embedding methods**, **regularization methods**, and **graph neural networks**
    (**GNNs**), as graphically depicted in the following diagram:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于监督图机器学习技术，已经开发了许多算法。然而，正如不同科学论文之前所报告的（[https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)），它们可以被分组为如**基于特征的方法**、**浅层嵌入方法**、**正则化方法**和**图神经网络**（**GNNs**）等宏观组，如下面的图中所示：
- en: '![Figure 4.1 – Hierarchical structure of the different supervised embedding
    algorithms described in this book ](img/B16069_04_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 本书中描述的不同监督嵌入算法的层次结构](img/B16069_04_01.jpg)'
- en: Figure 4.1 – Hierarchical structure of the different supervised embedding algorithms
    described in this book
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 本书中描述的不同监督嵌入算法的层次结构
- en: In the following sections, you will learn the main principles behind each group
    of algorithms. We will try to provide insight into the most well-known algorithms
    in the field as well, as these can be used to solve real-world problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习到每组算法背后的主要原理。我们还将尝试对领域内最著名的算法提供洞察，因为这些算法可以用来解决现实世界的问题。
- en: Feature-based methods
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于特征的方法
- en: One very simple (yet powerful) method for applying ML on graphs is to consider
    the encoding function as a simple embedding lookup. When dealing with supervised
    tasks, one simple way of doing this is to exploit graph properties. In [*Chapter
    1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014), *Getting Started with Graphs*,
    we have learned how graphs (or nodes in a graph) can be described by means of
    structural properties, each "encoding" important information from the graph itself.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在图上应用机器学习的一个非常简单（但强大）的方法是将编码函数视为一个简单的嵌入查找。在处理监督任务时，实现这一点的简单方法之一是利用图属性。在[*第一章*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014)《开始使用图》中，我们学习了如何通过结构属性来描述图（或图中的节点），每个“编码”都从图中本身提取了重要信息。
- en: 'Let''s forget graph ML for a moment: in classical supervised ML, the task is
    to find a function that maps a set of (descriptive) features of an instance to
    a particular output. Such features should be carefully engineered so that they
    are sufficiently representative to learn that concept. Therefore, as the number
    of petals and the sepal length might be good descriptors for a flower, when describing
    a graph we might rely on its average degree, its global efficiency, and its characteristic
    path length.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时忘记图上的机器学习：在经典监督机器学习中，任务是找到一个函数，将实例的（描述性）特征集合映射到特定的输出。这些特征应该精心设计，以便它们足够具有代表性，可以学习该概念。因此，当花瓣数量和萼片长度可能是一个花的良好描述符时，在描述图时，我们可能依赖于其平均度、全局效率和其特征路径长度。
- en: 'This shallow approach acts in two steps, outlined as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种浅层方法分为两个步骤，具体如下：
- en: Select a set of *good* descriptive graph properties.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一组*良好*的描述性图属性。
- en: Use such properties as input for a traditional ML algorithm.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些属性作为传统机器学习算法的输入。
- en: Unfortunately, there is no general definition of *good* descriptive properties,
    and their choice strictly depends on the specific problem to solve. However, you
    can still compute a wide variety of graph properties and then perform *feature
    selection* to select the most informative ones. **Feature selection** is a widely
    studied topic in ML, but providing details about the various methods is outside
    the scope of this book. However, we refer you to the book *Machine Learning Algorithms
    – Second Edition* ([https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999)),
    published by Packt Publishing, for further reading on this subject.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有关于*良好*描述性属性的通用定义，它们的选择严格依赖于要解决的问题的具体性。然而，你仍然可以计算大量的图属性，然后进行*特征选择*来选择最有信息量的那些。**特征选择**是机器学习中的一个广泛研究的话题，但提供关于各种方法的详细信息超出了本书的范围。不过，我们建议你参考Packt
    Publishing出版的《机器学习算法——第二版》（[https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999)），以进一步了解这个主题。
- en: 'Let''s now see a practical example of how such a basic method can be applied.
    We will be performing a supervised graph classification task by using a `PROTEINS`
    dataset. The `PROTEINS` dataset contains several graphs representing protein structures.
    Each graph is labeled, defining whether the protein is an enzyme or not. We will
    follow these next steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将这样一个基本方法应用到实际例子中。我们将通过使用`PROTEINS`数据集来执行一个监督图分类任务。`PROTEINS`数据集包含表示蛋白质结构的几个图。每个图都有标签，定义蛋白质是否是酶。我们将遵循以下步骤：
- en: 'First, let''s load the dataset through the `stellargraph` Python library, as
    follows:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们通过`stellargraph` Python库加载数据集，如下所示：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For computing graph properties, we will be using `networkx`, as described in
    [*Chapter 1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014), *Getting Started
    with Graphs*. To that end, we need to convert graphs from the `stellargraph` format
    to the `networkx` format. This can be done in two steps: first, convert the graphs
    from the `stellargraph` representation to `numpy` adjacency matrices. Then, use
    the adjacency matrices to retrieve the `networkx` representation. In addition,
    we also transform the labels (which are stored as a `pandas` Series) to a `numpy`
    array, which can be better exploited by the evaluation functions, as we will see
    in the next steps. The code is illustrated in the following snippet:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了计算图属性，我们将使用`networkx`，如[*第1章*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014)中所述，*开始使用图*。为此，我们需要将图从`stellargraph`格式转换为`networkx`格式。这可以通过两个步骤完成：首先，将图从`stellargraph`表示转换为`numpy`邻接矩阵。然后，使用邻接矩阵检索`networkx`表示。此外，我们还将标签（存储为`pandas`
    Series）转换为`numpy`数组，这样评价函数可以更好地利用它，正如我们将在下一步中看到的。代码如下所示：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, for each graph, we compute global metrics to describe it. For this example,
    we have chosen the number of edges, the average cluster coefficient, and the global
    efficiency. However, we suggest you compute several other properties you may find
    worth exploring. We can extract the graph metrics using `networkx`, as follows:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对于每个图，我们计算全局指标来描述它。在这个例子中，我们选择了边的数量、平均聚类系数和全局效率。然而，我们建议您计算其他您可能认为值得探索的属性。我们可以使用`networkx`提取图指标，如下所示：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now exploit `scikit-learn` utilities to create train and test sets.
    In our experiments, we will be using 70% of the dataset as the training set and
    the remainder as the test set. We can do that by using the `train_test_split`
    function provided by `scikit-learn`, as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以利用`scikit-learn`工具来创建训练集和测试集。在我们的实验中，我们将使用70%的数据集作为训练集，其余的作为测试集。我们可以通过使用`scikit-learn`提供的`train_test_split`函数来实现这一点，如下所示：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It''s now time for training a proper ML algorithm. We chose a `SVC` module
    of `scikit-learn`, as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候训练一个合适的机器学习算法了。我们选择了`scikit-learn`的`SVC`模块，如下所示：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This should be the output of the previous snippet of code:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该是之前代码片段的输出：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We used `Accuracy`, `Precision`, `Recall`, and `F1-score` to evaluate how well
    the algorithm is performing on the test set. We achieved about 80% for the F1
    score, which is already quite good for such a naïve task.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Accuracy`、`Precision`、`Recall`和`F1-score`来评估算法在测试集上的表现效果。我们达到了大约80%的F1分数，对于这样一个简单任务来说已经相当不错了。
- en: Shallow embedding methods
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浅层嵌入方法
- en: As we already described in [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046),
    *Unsupervised Graph Learning*, shallow embedding methods are a subset of graph
    embedding methods that learn node, edge, or graph representation for only a finite
    set of input data. They cannot be applied to other instances different from the
    ones used to train the model. Before starting our discussion, it is important
    to define how supervised and unsupervised shallow embedding algorithms differ.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第3章*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046)中描述的，*无监督图学习*，浅层嵌入方法是图嵌入方法的一个子集，它只为有限的数据集学习节点、边或图表示。它们不能应用于与训练模型所用的实例不同的其他实例。在我们开始讨论之前，定义监督和无监督浅层嵌入算法之间的区别是很重要的。
- en: The main difference between unsupervised and supervised embedding methods essentially
    lies in the task they attempt to solve. Indeed, if unsupervised shallow embedding
    algorithms try to learn a good graph, node, or edge representation in order to
    build well-defined clusters, the supervised algorithms try to find the best solution
    for a prediction task such as node, label, or graph classification.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督嵌入方法和监督嵌入方法之间的主要区别本质上在于它们试图解决的任务。确实，如果无监督浅层嵌入算法试图学习一个好的图、节点或边表示以构建定义良好的聚类，那么监督算法则试图找到预测任务（如节点、标签或图分类）的最佳解决方案。
- en: In this section, we will explain in detail some of those supervised shallow
    embedding algorithms. Moreover, we will enrich our description by providing several
    examples of how to use those algorithms in Python. For all the algorithms described
    in this section, we will present a custom implementation using the base classes
    available in the `scikit-learn` library.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细解释一些这些监督浅层嵌入算法。此外，我们将通过提供如何在 Python 中使用这些算法的几个示例来丰富我们的描述。在本节中描述的所有算法，我们将使用
    `scikit-learn` 库中可用的基类提供一个自定义实现。
- en: Label propagation algorithm
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签传播算法
- en: The label propagation algorithm is a well-known semi-supervised algorithm widely
    applied in data science and used to solve the node classification task. More precisely,
    the algorithm *propagates* the label of a given node to its neighbors or to nodes
    having a high probability of being reached from that node.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播算法是一种广为人知的半监督算法，在数据科学中得到广泛应用，用于解决节点分类任务。更确切地说，该算法会将给定节点的标签传播到其邻居或具有从该节点到达的高概率的节点。
- en: 'The general idea behind this approach is quite simple: given a graph with a
    set of labeled and unlabeled nodes, the labeled nodes propagate their label to
    the nodes having the highest probability of being reached. In the following diagram,
    we can see an example of a graph having labeled and unlabeled nodes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的基本思想相当简单：给定一个具有一组标记和未标记节点的图，标记节点会将它们的标签传播到具有最高到达概率的节点。在下面的图中，我们可以看到一个具有标记和未标记节点的图的示例：
- en: '![Figure 4.2 – Example of a graph with two labeled nodes (class 0 in red and
    class 1 in green) and six unlabeled nodes](img/B16069_04_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 具有两个标记节点（红色表示类别 0，绿色表示类别 1）和六个未标记节点的图示例](img/B16069_04_02.jpg)'
- en: Figure 4.2 – Example of a graph with two labeled nodes (class 0 in red and class
    1 in green) and six unlabeled nodes
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 具有两个标记节点（红色表示类别 0，绿色表示类别 1）和六个未标记节点的图示例
- en: According to *Figure 4.2*, using the information of the labeled nodes (node
    **0** and **6**), the algorithm will calculate the probability of moving to another
    unlabeled node. The nodes having the highest probability from a labeled node will
    get the label of that node.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 *图 4.2*，使用标记节点的信息（节点 **0** 和 **6**），算法将计算移动到另一个未标记节点的概率。从标记节点具有最高概率的节点将获得该节点的标签。
- en: 'Formally, let ![](img/B16069__04_001.png) be a graph and let ![](img/B16069__04_002.png)
    be a set of labels. Since the algorithm is semi-supervised, just a subset of nodes
    will have an assigned label. Moreover, let ![](img/B16069__04_003.png) be the
    adjacency matrix of the input graph G and ![](img/B16069__04_004.png) be the diagonal
    degree matrix where each element ![](img/B16069__04_005.png) is defined as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，设 ![](img/B16069__04_001.png) 为一个图，设 ![](img/B16069__04_002.png) 为一组标签。由于该算法是半监督的，只有一部分节点会被分配标签。此外，设
    ![](img/B16069__04_003.png) 为输入图 G 的邻接矩阵，设 ![](img/B16069__04_004.png) 为对角度矩阵，其中每个元素
    ![](img/B16069__04_005.png) 定义如下：
- en: '![](img/B16069__04_006.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_006.jpg)'
- en: 'In other words, the only nonzero elements of the degree matrix are the diagonal
    elements whose values are given by the degree of the node represented by the row.
    In the following figure, we can see the diagonal degree matrix of the graph represented
    in *Figure 4.2*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，度矩阵中唯一的非零元素是对角元素，其值由表示该行的节点的度数给出。在下面的图中，我们可以看到 *图 4.2* 中表示的图的对角度矩阵：
- en: '![Figure 4.3 – Diagonal degree matrix for the graph in Figure 4.2'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – 图 4.2 中图的对角度矩阵'
- en: '](img/B16069_04_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_04_03.jpg)'
- en: Figure 4.3 – Diagonal degree matrix for the graph in Figure 4.2
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 图 4.2 中图的对角度矩阵
- en: 'From *Figure 4.3*, it is possible to see how only the diagonal elements of
    the matrix contain nonzero values, and those values represent the degree of the
    specific node. We also need to introduce the transition matrix ![](img/B16069__04_007.png).
    This matrix defines the probability of a node being reached from another node.
    More precisely, ![](img/B16069__04_008.png) is the probability of reaching node
    ![](img/B16069__04_009.png) from node ![](img/B16069__04_010.png). The following
    figure shows the transition matrix ![](img/B16069__04_011.png) for the graph depicted
    in *Figure 4.2*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图4.3*中，我们可以看到矩阵的对角线元素包含非零值，这些值代表特定节点的度数。我们还需要引入转移矩阵![](img/B16069__04_007.png)。这个矩阵定义了从另一个节点到达节点的概率。更确切地说，![](img/B16069__04_008.png)是从节点![](img/B16069__04_010.png)到达节点![](img/B16069__04_009.png)的概率。以下图显示了*图4.2*中描述的图的转移矩阵![](img/B16069__04_011.png)：
- en: '![Figure 4.4 – Transition matrix for the graph in Figure 4.2'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 – 图4.2中图的转移矩阵'
- en: '](img/B16069_04_04.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16069_04_04.jpg)'
- en: Figure 4.4 – Transition matrix for the graph in Figure 4.2
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 图4.2中图的转移矩阵
- en: 'In *Figure 4.4*, the matrix shows the probability of reaching an end node given
    a start node. For instance, from the first row of the matrix, we can see how from
    node 0 it is possible to reach, with equal probability of 0.5, only nodes 1 and
    2\. If we defined with ![](img/B16069__04_012.png) the initial label assignment,
    the probability of label assignment for each node obtained using the ![](img/B16069__04_013.png)
    matrix can be computed as ![](img/B16069__04_014.png). The ![](img/B16069__04_015.png)
    matrix computed for the graph in *Figure 4.2* is shown in the following figure:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.4*中，矩阵显示了给定起始节点达到终止节点的概率。例如，从矩阵的第一行，我们可以看到从节点0出发，以0.5的概率仅能到达节点1和2。如果我们用![](img/B16069__04_012.png)定义初始标签分配，使用![](img/B16069__04_013.png)矩阵获得的每个节点的标签分配概率可以计算为![](img/B16069__04_014.png)。*图4.2*中图的![](img/B16069__04_015.png)矩阵在以下图中显示：
- en: '![Figure 4.5 – Solution obtained using the matrix for the graph in Figure 4.2'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5 – 使用图4.2中图的矩阵得到的解'
- en: '](img/B16069_04_05.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B16069_04_05.jpg)'
- en: Figure 4.5 – Solution obtained using the matrix for the graph in Figure 4.2
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 使用图4.2中图的矩阵得到的解
- en: From *Figure 4.5*, we can see that using the transition matrix, node 1 and node
    2 have a probability of being assigned to the ![](img/B16069__04_016.png) label
    of 0.5 and 0.33 respectively, while node 5 and node 6 have a probability of being
    assigned to the ![](img/B16069__04_017.png) label of 0.33 and 0.5, respectively.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图4.5*中，我们可以看到，使用转移矩阵，节点1和节点2被分配到![](img/B16069__04_016.png)标签的概率分别为0.5和0.33，而节点5和节点6被分配到![](img/B16069__04_017.png)标签的概率分别为0.33和0.5。
- en: 'Moreover, if we better analyze *Figure 4.5*, we can see two main problems,
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们更好地分析*图4.5*，我们可以看到两个主要问题，如下所示：
- en: With this solution, it is possible to assign only to nodes [1 2] and [5 7] a
    probability associated with a label.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个解决方案，可以将概率仅分配给节点[1 2]和[5 7]与一个标签相关联。
- en: The initial labels of nodes 0 and 6 are different from the one defined in ![](img/B16069__04_018.png).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点0和6的初始标签与![](img/B16069__04_018.png)中定义的不同。
- en: 'In order to solve the first point, the algorithm will perform ![](img/B16069__04_019.png)
    different iterations; at each iteration ![](img/B16069__04_020.png), the algorithm
    will compute the solution for that iteration, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个问题，算法将执行![](img/B16069__04_019.png)次不同的迭代；在每次迭代![](img/B16069__04_020.png)中，算法将计算该迭代的解，如下所示：
- en: '![](img/B16069__04_021.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B16069__04_021.jpg)'
- en: The algorithm stops its iteration when a certain condition is met. The second
    problem is solved by the label propagation algorithm by imposing, in the solution
    of a given iteration ![](img/B16069__04_022.png), the labeled nodes to have the
    initial class values. For example, after computing the result visible in *Figure
    4.5*, the algorithm will force the first line of the result matrix to be ![](img/B16069__04_023.png)
    and the seventh line of the matrix to be ![](img/B16069__04_024.png).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在满足一定条件时停止迭代。第二个问题通过在给定迭代的解![](img/B16069__04_022.png)中强制标签节点具有初始类值来解决。例如，在计算*图4.5*中可见的结果后，算法将强制结果矩阵的第一行是![](img/B16069__04_023.png)，矩阵的第七行是![](img/B16069__04_024.png)。
- en: Here, we propose a modified version of the `LabelPropagation` class available
    in the `scikit-learn` library. The main reason behind this choice is given by
    the fact that the `LabelPropagation` class takes as input a matrix representing
    a dataset. Each row of the matrix represents a sample, and each column represents
    a feature.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提出了`scikit-learn`库中`LabelPropagation`类的修改版。选择这个方案的主要原因是`LabelPropagation`类接受一个表示数据集的矩阵作为输入。矩阵的每一行代表一个样本，每一列代表一个特征。
- en: Before performing a `fit` operation, the `LabelPropagation` class internally
    executes the `_build_graph` function. This function will build, using a parametric
    kernel (`_get_kernel` function), a graph describing the input dataset. As a result,
    the original dataset is transformed into a graph (in its adjacency matrix representation)
    where each node is a sample (a row of the input dataset) and each edge is an *interaction*
    between the samples.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行`fit`操作之前，`LabelPropagation`类内部执行`_build_graph`函数。此函数将使用参数化核（`_get_kernel`函数）构建一个描述输入数据集的图。因此，原始数据集被转换为一个图（其邻接矩阵表示），其中每个节点是一个样本（输入数据集的行），每条边是样本之间的*交互*。
- en: 'In our specific case, the input dataset is already a graph, so we need to define
    a new class capable of dealing with a `networkx` graph and performing the computation
    operation on the original graph. The goal is achieved by creating a new class—namely,
    `GraphLabelPropagation—b`y extending the `ClassifierMixin`, `BaseEstimator`, and
    `ABCMeta` base classes. The algorithm proposed here is mainly used in order to
    help you understand the concept behind the algorithm. The whole algorithm is provided
    in the `04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb` notebook
    available in the GitHub repository of this book. In order to describe the algorithm,
    we will use only the `fit(X,y)` function as a reference. The code is illustrated
    in the following snippet:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特定情况下，输入数据集已经是一个图，因此我们需要定义一个新的类，能够处理`networkx`图并在原始图上执行计算操作。通过创建一个新的类——即`GraphLabelPropagation`，通过扩展`ClassifierMixin`、`BaseEstimator`和`ABCMeta`基类来实现这一目标。这里提出的算法主要用于帮助您理解算法背后的概念。整个算法在本书GitHub仓库的`04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb`笔记本中提供。为了描述算法，我们将仅使用`fit(X,y)`函数作为参考。代码如下所示：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `fit(X,y)` function takes as input a `networkx` graph ![](img/B16069__04_025.png)
    and an array ![](img/B16069__04_026.png) representing the labels assigned to each
    node. Nodes without labels should have a representative value of -1\. The `while`
    loop performs the real computation. More precisely, it computes the ![](img/B16069__04_027.png)
    value at each iteration and forces the labeled nodes in the solution to be equal
    to their original input value. The algorithm performs the computation until the
    two stop conditions are satisfied. In this implementation, the following two criteria
    have been used:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit(X,y)`函数接受一个`networkx`图![img/B16069__04_025.png](img/B16069__04_025.png)和一个表示分配给每个节点的标签的数组![img/B16069__04_026.png](img/B16069__04_026.png)。没有标签的节点应有一个代表值-1。`while`循环执行实际计算。更确切地说，它在每次迭代中计算![img/B16069__04_027.png](img/B16069__04_027.png)的值，并强制将标记节点在解中的值等于其原始输入值。算法执行计算直到满足两个停止条件。在此实现中，使用了以下两个标准：'
- en: '**Number of iterations**: The algorithm runs the computation until a given
    number of iterations has been performed.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代次数**：算法运行计算直到完成指定的迭代次数。'
- en: '**Solution tolerance error**: The algorithm runs the computation until the
    absolute difference of the solution obtained in two consecutive iterations, ![](img/B16069__04_028.png)
    and ![](img/B16069__04_029.png), is lower than a given threshold value.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解的容差误差**：算法运行计算直到连续两次迭代获得的解的绝对差值，![img/B16069__04_028.png](img/B16069__04_028.png)和![img/B16069__04_029.png](img/B16069__04_029.png)，低于给定的阈值值。'
- en: 'The algorithm can be applied to the example graph depicted in *Figure 4.2*
    using the following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法可以使用以下代码应用于*图4.2*所示的示例图：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result obtained by the algorithm is shown in the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 算法得到的结果如下所示：
- en: '![Figure 4.6 – Result of the label propagation algorithm on the graph of Figure
    4.2: on the left, the final labeled graph; on the right, the final probability
    assignment matrix](img/B16069_04_06.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 标签传播算法在图4.2上的结果：左侧为最终标记的图；右侧为最终概率分配矩阵](img/B16069_04_06.jpg)'
- en: 'Figure 4.6 – Result of the label propagation algorithm on the graph of Figure
    4.2: on the left, the final labeled graph; on the right, the final probability
    assignment matrix'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 标签传播算法在图 4.2 上的结果：左侧为最终标记图；右侧为最终概率分配矩阵
- en: In *Figure 4.6*, we can see the results of the algorithm applied to the example
    shown in *Figure 4.2*. From the final probability assignment matrix, it is possible
    to see how the probability of the initial labeled nodes is 1 due to the constraints
    of the algorithm and how nodes that are "near" to labeled nodes get their label.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 4.6* 中，我们可以看到算法应用于 *图 4.2* 中示例的结果。从最终的概率分配矩阵中，我们可以看到由于算法的约束，初始标记节点的概率为
    1，以及“靠近”标记节点的节点如何获得它们的标签。
- en: Label spreading algorithm
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签传播算法
- en: 'The label spreading algorithm is another semi-supervised shallow embedding
    algorithm. It was built in order to overcome one big limitation of the label propagation
    method: the **initial labeling**. Indeed, according to the label propagation algorithm,
    the initial labels cannot be modified in the training process and, in each iteration,
    they are forced to be equal to their original value. This constraint could generate
    incorrect results when the initial labeling is affected by errors or noise. As
    a consequence, the error will be propagated in all nodes of the input graph.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播算法是另一种半监督浅嵌入算法。它是为了克服标签传播方法的一个大限制：**初始标记**。确实，根据标签传播算法，初始标签在训练过程中不能修改，并且在每次迭代中，它们被迫等于其原始值。这种约束可能会在初始标记受到错误或噪声影响时产生错误的结果。因此，错误将传播到输入图的所有节点。
- en: In order to solve this limitation, the label spreading algorithm tries to relax
    the constraint of the original labeled data, allowing the labeled input nodes
    to change their label during the training process.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一限制，标签传播算法试图放松原始标记数据的约束，允许标记输入节点在训练过程中改变其标签。
- en: 'Formally, let ![](img/B16069__04_030.png) be a graph and let ![](img/B16069__04_031.png)
    be a set of labels (since the algorithm is semi-supervised, just a subset of nodes
    will have an assigned label), and let ![](img/B16069__04_032.png) and ![](img/B16069__04_033.png)
    be the adjacency matrix diagonal degree matrix of graph G, respectively. Instead
    of computing the probability transition matrix, the label spreading algorithm
    uses the normalized graph **Laplacian matrix**, defined as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，设 ![](img/B16069__04_030.png) 为一个图，设 ![](img/B16069__04_031.png) 为一组标签（由于算法是半监督的，只有一部分节点会被分配标签），设
    ![](img/B16069__04_032.png) 和 ![](img/B16069__04_033.png) 分别为图 G 的邻接矩阵对角度矩阵。标签传播算法不是计算概率转移矩阵，而是使用以下定义的归一化图拉普拉斯矩阵：
- en: '![](img/B16069__04_034.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_034.jpg)'
- en: 'As with label propagation, this matrix can be seen as a sort of compact low-dimensional
    representation of the connections defined in the whole graph. This matrix can
    be easily computed using `networkx` with the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签传播一样，这个矩阵可以看作是整个图中定义的连接的某种紧凑的低维表示。这个矩阵可以使用以下代码通过 `networkx` 容易地计算：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As a result, we get the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 4.7 – The normalized graph Laplacian matrix'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 归一化图拉普拉斯矩阵'
- en: '](img/B16069_04_07.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16069_04_07.jpg)'
- en: Figure 4.7 – The normalized graph Laplacian matrix
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 归一化图拉普拉斯矩阵
- en: 'The most important difference between the label spreading and label propagation
    algorithms is related to the function used to extract the labels. If we defined
    with ![](img/B16069__04_035.png) the initial label assignment, the probability
    of a label assignment for each node obtained using the ![](img/B16069__04_036.png)
    matrix can be computed as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 标签传播算法与标签传播算法之间最重要的区别与用于提取标签的函数有关。如果我们用 ![](img/B16069__04_035.png) 定义初始标签分配，使用
    ![](img/B16069__04_036.png) 矩阵获得的每个节点的标签分配概率可以按以下方式计算：
- en: '![](img/B16069__04_037.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_037.jpg)'
- en: 'As with label propagation, label spreading has an iterative process to compute
    the final solution. The algorithm will perform ![](img/B16069__04_038.png) different
    iterations; in each iteration ![](img/B16069__04_039.png), the algorithm will
    compute the solution for that iteration, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签传播一样，标签传播有一个迭代过程来计算最终解。算法将执行 ![](img/B16069__04_038.png) 次不同的迭代；在每次迭代 ![](img/B16069__04_039.png)
    中，算法将计算该迭代的解，如下所示：
- en: '![](img/B16069__04_040.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_040.jpg)'
- en: The algorithm stops its iteration when a certain condition is met. It is important
    to underline the term ![](img/B16069__04_041.png) of the equation. Indeed, as
    we said, label spreading does not force the labeled element of the solution to
    be equal to its original value. Instead, the algorithm uses a regularization parameter
    ![](img/B16069__04_042.png) to weight the influence of the original solution at
    each iteration. This allows us to explicitly impose the "quality" of the original
    solution and its influence in the final solution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在满足一定条件时停止迭代。重要的是要强调方程中的术语![img/B16069__04_041.png](img/B16069__04_041.png)。实际上，正如我们所说，标签传播并不强制解的标记元素等于其原始值。相反，算法使用正则化参数![img/B16069__04_042.png](img/B16069__04_042.png)在每个迭代中对原始解的影响进行加权。这允许我们明确地强制原始解的“质量”及其在最终解中的影响。
- en: 'As with the label propagation algorithm, in the following code snippet, we
    propose a modified version of the `LabelSpreading` class available in the `scikit-learn`
    library due to the motivations we already mentioned in the previous section. We
    propose the `GraphLabelSpreading` class by extending our `GraphLabelPropagation`
    class, since the only difference will be in the `fit()` method of the class. The
    whole algorithm is provided in the `04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb`
    notebook available in the GitHub repository of this book:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签传播算法一样，在以下代码片段中，我们提出了由于我们在上一节中提到的动机而修改的`LabelSpreading`类，该类可在`scikit-learn`库中找到。我们通过扩展我们的`GraphLabelPropagation`类提出了`GraphLabelSpreading`类，因为唯一的区别将在于类的`fit()`方法。整个算法在本书GitHub仓库中的`04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb`笔记本中提供：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Also in this class, the `fit()` function is the focal point. The function takes
    as input a `networkx` graph ![](img/B16069__04_043.png) and an array ![](img/B16069__04_044.png)
    representing the labels assigned to each node. Nodes without labels should have
    a representative value of -1\. The `while` loop computes the ![](img/B16069__04_045.png)
    value at each iteration, weighting the influence of the initial labeling via the
    parameter ![](img/B16069__04_046.png). Also, for this algorithm, the number of
    iterations and the difference between two consecutive solutions are used as stop
    criteria.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，`fit()`函数是焦点。该函数接受一个`networkx`图![img/B16069__04_043.png](img/B16069__04_043.png)和一个表示每个节点分配的标签的数组![img/B16069__04_044.png](img/B16069__04_044.png)。没有标签的节点应有一个代表值-1。`while`循环在每个迭代中计算![img/B16069__04_045.png](img/B16069__04_045.png)值，通过参数![img/B16069__04_046.png](img/B16069__04_046.png)加权初始标记的影响。此外，对于这个算法，迭代次数和连续两个解之间的差异被用作停止标准。
- en: 'The algorithm can be applied to the example graph depicted in *Figure 4.2*
    using the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法可以使用*图4.2*中描述的示例图，以下代码实现：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following diagram, the result obtained by the algorithm is shown:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，展示了算法得到的结果：
- en: '![Figure 4.8 – Result of the label propagation algorithm on graph in Figure
    4.2: on the left, the final labeled graph; on the right, the final probability
    assignment matrix](img/B16069_04_08.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – 标签传播算法在图4.2中的结果：左侧为最终标记的图；右侧为最终概率分配矩阵](img/B16069_04_08.jpg)'
- en: 'Figure 4.8 – Result of the label propagation algorithm on graph in Figure 4.2:
    on the left, the final labeled graph; on the right, the final probability assignment
    matrix'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 标签传播算法在图4.2中的结果：左侧为最终标记的图；右侧为最终概率分配矩阵
- en: The result visible in the diagram shown in *Figure 4.8* looks similar to the
    one obtained using the label propagation algorithm. The main difference is related
    to the probability of label assignment. Indeed, in this case, it is possible to
    see how nodes 0 and 6 (the ones having an initial labeling) have a probability
    of 0.5, which is significantly lower compared to the probability of 1 obtained
    using the label propagation algorithm. This behavior is expected since the influence
    of the initial label assignment is weighted by the regularization parameter ![](img/B16069__04_047.png).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.8*中显示的结果看起来与使用标签传播算法获得的结果相似。主要区别与标签分配的概率有关。实际上，在这种情况下，我们可以看到节点0和6（具有初始标签的节点）的概率为0.5，这比使用标签传播算法获得的概率1显著低。这种行为是预期的，因为初始标签分配的影响是通过正则化参数![img/B16069__04_047.png](img/B16069__04_047.png)加权的。
- en: In the next section, we will continue our description of supervised graph embedding
    methods. We will describe how network-based information helps regularize the training
    and create more robust models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续描述监督图嵌入方法。我们将描述基于网络的信息如何帮助正则化训练并创建更鲁棒的模型。
- en: Graph regularization methods
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图正则化方法
- en: Shallow embedding methods described in the previous section show how topological
    information and relations between data points can be encoded and leveraged in
    order to build more robust classifiers and address semi-supervised tasks. In general
    terms, network information can be extremely useful in constraining models and
    enforcing the output to be smooth within neighboring nodes. As we have already
    seen in previous sections, this idea can be efficiently used in semi-supervised
    tasks, when propagating the information on neighbor unlabeled nodes.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中描述的浅层嵌入方法展示了如何将拓扑信息和数据点之间的关系编码并利用，以构建更鲁棒的分类器并解决半监督任务。一般来说，网络信息在约束模型和确保输出在相邻节点内平滑时可以非常有用。正如我们在前面的章节中已经看到的，这个想法可以有效地用于半监督任务，在传播邻居无标记节点的信息时。
- en: 'On the other hand, this can also be used to regularize the learning phase in
    order to create more robust models that tend to better generalize to unseen examples.
    Both the label propagation and the label spreading algorithms we have seen previously
    can be implemented as a cost function to be minimized when we add an additional
    regularization term. Generally, in supervised tasks, we can write the cost function
    to be minimized in the following form:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这也可以用来正则化学习阶段，以创建更鲁棒且倾向于更好地泛化到未见示例的模型。我们之前看到的标签传播和标签扩散算法可以作为成本函数实现，当添加一个额外的正则化项时进行最小化。通常，在监督任务中，我们可以将最小化的成本函数写成以下形式：
- en: '![](img/B16069__04_048.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_048.jpg)'
- en: Here, ![](img/B16069__04_049.png) and ![](img/B16069__04_050.png) represent
    the labeled and unlabeled samples, and the second term acts as a regularization
    term that depends on the topological information of the graph ![](img/B16069__04_051.png).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B16069__04_049.png) 和 ![](img/B16069__04_050.png) 分别代表标记和无标记的样本，第二个项作为一个正则化项，它依赖于图
    ![](img/B16069__04_051.png) 的拓扑信息。
- en: In this section, we will further describe such an idea and see how this can
    be very powerful, especially when regularizing the training of neural networks,
    which—as you might know—naturally tend to overfit and/or need large amounts of
    data to be trained efficiently.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步描述这样一个想法，并看看它如何非常强大，尤其是在正则化神经网络训练时，如您所知，神经网络自然倾向于过拟合，并且/或者需要大量的数据才能有效地进行训练。
- en: Manifold regularization and semi-supervised embedding
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形正则化和半监督嵌入
- en: '**Manifold regularization** (Belkin et al., 2006) extends the label propagation
    framework by parametrizing the model function in the **reproducing kernel Hilbert
    space** (**RKHS**) and using as a supervised loss function (first term in the
    previous equation) the **mean square error** (**MSE**) or the hinge loss. In other
    words, when training an SVM or a least squares fit, they apply a graph regularization
    term based on the Laplacian matrix *L*, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**流形正则化**（Belkin 等人，2006年）通过在**再生核希尔伯特空间**（**RKHS**）中对模型函数进行参数化，并使用**均方误差**（**MSE**）或**折损损失**作为监督损失函数（前一个方程中的第一个项），扩展了标签传播框架。换句话说，当训练支持向量机或最小二乘拟合时，它们会基于拉普拉斯矩阵
    *L* 应用图正则化项，如下所示：'
- en: '![](img/B16069__04_052.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_052.jpg)'
- en: For this reason, these methods are generally labeled as **Laplacian regularization**,
    and such a formulation leads to **Laplacian regularized least squares** (**LapRLS**)
    and **LapSVM** classifications. Label propagation and label spreading can be seen
    as a special case of manifold regularization. Besides, these algorithms can also
    be used in the case of no-labeled data (first term in the equation disappearing)
    reducing to **Laplacian eigenmaps**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些方法通常被标记为**拉普拉斯正则化**，这样的公式导致了**拉普拉斯正则化最小二乘法**（**LapRLS**）和**LapSVM**分类。标签传播和标签扩散可以看作是流形正则化的一个特例。此外，这些算法也可以在没有标记数据的情况下使用（方程中的第一个项消失），这会简化为**拉普拉斯特征映射**。
- en: On the other hand, they can also be used in the case of a fully labeled dataset,
    in which case the preceding terms constrain the training phase to regularize the
    training and achieve more robust models. Moreover, being the classifier parametrized
    in the RKHS, the model can be used on unobserved samples and does not require
    test samples to belong to the input graph. In this sense, it is therefore an *inductive*
    model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，它们也可以用于完全标记的数据集的情况，在这种情况下，前面的术语将约束训练阶段以正则化训练并实现更鲁棒的模型。此外，由于模型是在RKHS中参数化的分类器，因此它可以用于未观察到的样本，并且不需要测试样本属于输入图。从这个意义上说，它因此是一个*归纳*模型。
- en: '**Manifold learning** still represents a form of shallow learning, whereby
    the parametrized function does not leverage on any form of intermediate embeddings.
    **Semi-supervised embedding** (Weston et al., 2012) extends the concepts of graph
    regularization to deeper architectures by imposing the constraint and the smoothness
    of the function on intermediate layers of a neural network. Let''s define ![](img/B16069__04_053.png)
    as the intermediate output of the *k*th hidden layer. The regularization term
    proposed in the semi-supervised embedding framework reads as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**流形学习**仍然代表一种浅层学习形式，其中参数化的函数不利用任何形式的中间嵌入。**半监督嵌入**（Weston等人，2012年）通过在神经网络的中间层上施加函数的约束和光滑性，将图正则化的概念扩展到更深的架构。让我们将
    ![](img/B16069__04_053.png) 定义为第 *k* 个隐藏层的中间输出。半监督嵌入框架中提出的正则化项如下所示：'
- en: '![](img/B16069__04_054.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B16069__04_054.jpg](img/B16069__04_054.jpg)'
- en: 'Depending on where the regularization is imposed, three different configurations
    (shown in *Figure 4.9*) can be achieved, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 根据正则化施加的位置，可以实现三种不同的配置（如图 *4.9* 所示），如下所示：
- en: Regularization is applied to the final output of the network. This corresponds
    to a generalization of the manifold learning technique to multilayer neural networks.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化应用于网络的最终输出。这对应于将流形学习技术泛化到多层神经网络。
- en: Regularization is applied to an intermediate layer of the network, thus regularizing
    the embedding representation.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化应用于网络的中间层，从而正则化嵌入表示。
- en: Regularization is applied to an auxiliary network that shares the first k-1
    layers. This basically corresponds to training an unsupervised embedding network
    while simultaneously training a supervised network. This technique basically imposes
    a derived regularization on the first k-1 layers that are constrained by the unsupervised
    network as well and simultaneously promotes an embedding of the network nodes.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化应用于共享前k-1层的辅助网络。这基本上对应于在同时训练监督网络的同时训练无监督嵌入网络。这种技术基本上对受无监督网络约束的前k-1层施加了派生的正则化，并同时促进了网络节点的嵌入。
- en: 'The following diagram shows an illustration of the three different configurations—with
    their similarities and differences—that can be achieved using a semi-supervised
    embedding framework:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了使用半监督嵌入框架可以实现的三个不同配置的示意图——它们的相似之处和不同之处：
- en: '![Figure 4.9 – Semi-supervised embedding regularization configurations: graph
    regularization, indicated by the cross, can be applied to the output (left), to
    an intermediate layer (center), or to an auxiliary network (right)](img/B16069_04_09.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 半监督嵌入正则化配置：用交叉表示的图正则化可以应用于输出（左）、中间层（中）或辅助网络（右）](img/B16069_04_09.jpg)'
- en: 'Figure 4.9 – Semi-supervised embedding regularization configurations: graph
    regularization, indicated by the cross, can be applied to the output (left), to
    an intermediate layer (center), or to an auxiliary network (right)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 半监督嵌入正则化配置：用交叉表示的图正则化可以应用于输出（左）、中间层（中）或辅助网络（右）
- en: 'In its original formulation, the loss function used for the embeddings is the
    one derived from the Siamese network formulation, shown as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在其原始公式中，用于嵌入的损失函数是从Siamese网络公式推导出来的，如下所示：
- en: '![](img/B16069__04_055.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B16069__04_055.jpg](img/B16069__04_055.jpg)'
- en: As can be seen by this equation, the loss function ensures the embeddings of
    neighboring nodes stay close. On the other hand, non-neighbors are instead pulled
    apart to a distance (at least) specified by the threshold ![](img/B16069__04_056.png).
    As compared to the regularization based on the Laplacian ![](img/B16069__04_057.png)
    (although for neighboring points, the penalization factor is effectively recovered),
    the one shown here is generally easier to be optimized by gradient descent.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程可以看出，损失函数确保相邻节点的嵌入保持接近。另一方面，非相邻节点被拉远到由阈值！[](img/B16069__04_056.png)指定的距离（至少）。与基于拉普拉斯算子！[](img/B16069__04_057.png)的正则化（尽管对于相邻点，惩罚因子实际上得到了恢复）相比，这里展示的通常更容易通过梯度下降进行优化。
- en: The best choice among the three configurations presented in *Figure 4.9* is
    largely influenced by the data at your disposal as well as on your specific use
    case—that is, whether you need a regularized model output or to learn a high-level
    data representation. However, you should always keep in mind that when using softmax
    layers (usually done at the output layer), the regularization based on the hinge
    loss may not be very appropriate or suited for log probabilities. In such cases,
    regularized embeddings and relative loss should instead be introduced at intermediate
    layers. However, be aware that embeddings lying in deeper layers are generally
    harder to be trained and require a careful tuning of learning rate and margins
    to be used.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.9*中展示的三种配置中，最佳选择在很大程度上受到可用数据以及特定用例的影响——也就是说，您是否需要一个正则化模型输出或学习高级数据表示。然而，您应该始终记住，当使用softmax层（通常在输出层进行）时，基于hinge损失的正则化可能不太合适或适合对数概率。在这种情况下，应该在中间层引入正则化嵌入和相对损失。然而，请注意，位于深层层的嵌入通常更难训练，需要仔细调整学习率和边界以使用。
- en: Neural Graph Learning
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经图学习
- en: '**Neural graph learning** (**NGL**) basically generalizes the previous formulations
    and, as we will see, makes it possible to seamlessly apply graph regularization
    to any form of a neural network, including CNNs and **recurrent neural networks**
    (**RNNs**). In particular, there exists an extremely powerful framework named
    **Neural Structured Learning** (**NSL**) that allows us to extend in a very few
    lines of code a neural network implemented in TensorFlow with graph regularization.
    The networks can be of any kind: natural or synthetic.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经图学习**（**NGL**）基本上推广了之前的公式，并且正如我们将看到的，使得将图正则化无缝应用于任何形式的神经网络成为可能，包括CNN和**循环神经网络**（**RNN**）。特别是，存在一个名为**神经结构学习**（**NSL**）的极其强大的框架，它允许我们通过非常少的代码行将TensorFlow中实现的神经网络扩展到图正则化。网络可以是任何类型：自然或合成。'
- en: When synthetic, graphs can be generated in different ways, using—for instance—embeddings
    learned in an unsupervised manner and/or by using a similarity/distance metric
    between samples using their features. You can also generate synthetic graphs using
    adversarial examples. Adversarial examples are artificially generated samples
    obtained by perturbing actual (real) examples in such a way that we confound the
    network, trying to force a prediction error. These very carefully designed samples
    (obtained by perturbing a given sample in the gradient-descent direction in order
    to maximize errors) can be connected to their related samples, thus generating
    a graph. These connections can then be used to train a graph-regularized version
    of the network, allowing us to obtain models that are more robust against adversarially
    generated examples.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当是合成的，图可以通过不同的方式生成，例如使用无监督方式学习的嵌入和/或使用样本特征之间的相似性/距离度量。您还可以使用对抗性示例生成合成图。对抗性示例是通过以某种方式扰动实际（真实）示例而人工生成的样本，以混淆网络，试图强制预测错误。这些精心设计的样本（通过在梯度下降方向上扰动给定样本以最大化错误而获得）可以与其相关样本连接，从而生成图。然后可以使用这些连接来训练网络的图正则化版本，使我们能够获得对对抗性生成的示例更具鲁棒性的模型。
- en: 'NGL extends the regularization by augmenting the tuning parameters for graph
    regularization in neural networks, decomposing the contribution of labeled-labeled,
    labeled-unlabeled, and unlabeled-unlabeled relations using three parameters, ![](img/B16069__04_058.png),
    ![](img/B16069__04_059.png), and ![](img/B16069__04_060.png), respectively, as
    follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: NGL通过增强神经网络中图正则化的调整参数来扩展正则化，分别使用三个参数！[](img/B16069__04_058.png)、！[](img/B16069__04_059.png)和！[](img/B16069__04_060.png)分解标签-标签、标签-未标记和未标记-未标记关系的贡献，如下所示：
- en: '![](img/B16069__04_061.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![img/B16069__04_061.png](img/B16069__04_061.png)'
- en: 'The function ![](img/B16069__04_062.png) represents a generic distance between
    two vectors—for instance, the L2 norm ![](img/B16069__04_063.png). By varying
    the coefficients and the definition of ![](img/B16069__04_064.png), we can arrive
    at the different algorithms seen previously as limiting behavior, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 函数![img/B16069__04_062.png](img/B16069__04_062.png)代表两个向量之间的通用距离——例如，L2范数![img/B16069__04_063.png](img/B16069__04_063.png)。通过改变系数和![img/B16069__04_064.png](img/B16069__04_064.png)的定义，我们可以得到之前作为极限行为的不同算法，如下所示：
- en: When ![](img/B16069_04_065a.png) ![](img/B16069_04_065b.png)we retrieve the
    non-regularized version of a neural network.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当![img/B16069_04_065a.png](img/B16069_04_065a.png)![img/B16069_04_065b.png](img/B16069_04_065b.png)时，我们检索到神经网络的非正则化版本。
- en: When only ![](img/B16069__04_066.png), we recover a fully supervised formulation
    where relationships between nodes act to regularize the training.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当只有![img/B16069__04_066.png](img/B16069__04_066.png)时，我们恢复了一个完全监督的公式，其中节点之间的关系起到正则化训练的作用。
- en: When we substitute ![](img/B16069__04_067.png) (which are parametrized by a
    set of alpha coefficients) with a set of values ![](img/B16069__04_068.png) (to
    be learned) that map each sample to its instance class, we recover the label propagation
    formulation.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将由一组α系数参数化的![img/B16069__04_067.png](img/B16069__04_067.png)（要学习的值![img/B16069__04_068.png](img/B16069__04_068.png)映射到每个样本的实例类别时），我们恢复了标签传播公式。
- en: Loosely speaking, the NGL formulations can be seen as a non-linear version of
    the label propagation and label spreading algorithms, or as a form of a graph-regularized
    neural network for which the manifold learning or semi-supervising embedding can
    be obtained.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通俗地说，NGL公式可以看作是标签传播和标签扩散算法的非线性版本，或者是一种图正则化神经网络，其中可以获取流形学习或半监督嵌入。
- en: We will now apply NGL to a practical example, where you will learn how to use
    graph regularization in neural networks. To do so, we will use the NLS framework
    ([https://github.com/tensorflow/neural-structured-learning](https://github.com/tensorflow/neural-structured-learning)),
    which is a library built on top of TensorFlow that makes it possible to implement
    graph regularization with only a few lines of codes on top of standard neural
    networks.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将NGL应用于一个实际示例，你将学习如何在神经网络中应用图正则化。为此，我们将使用NLS框架([https://github.com/tensorflow/neural-structured-learning](https://github.com/tensorflow/neural-structured-learning))，这是一个建立在TensorFlow之上的库，它使得在标准神经网络上仅用几行代码即可实现图正则化。
- en: For our example, we will be using the `Cora` dataset, which is a labeled dataset
    that consists of 2,708 scientific papers in computer science that have been classified
    into seven classes. Each paper represents a node that is connected to other nodes
    based on citations. In total, there are 5,429 links in the network.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将使用`Cora`数据集，这是一个包含2,708篇计算机科学论文的有标签数据集，这些论文被分为七个类别。每篇论文代表一个节点，该节点根据引用与其他节点相连。网络中总共有5,429个链接。
- en: 'Moreover, each node is further described by a 1,433-long vector of binary values
    (0 or 1) that represent a dichotomic `Cora` dataset can be downloaded directly
    from the `stellargraph` library with a few lines of code, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个节点还由一个1,433个二进制值（0或1）的向量进一步描述，这些值代表一个二分`Cora`数据集，可以直接从`stellargraph`库中用几行代码下载，如下所示：
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This returns two outputs, outlined as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回两个输出，如下所述：
- en: '`G`, which is the citation network containing the network nodes, edges, and
    the features describing the BOW representation.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`G`是包含网络节点、边和描述BOW表示的特征的引用网络。'
- en: '`labels`, which is a `pandas` Series that provides the mapping between the
    paper ID and one of the classes, as follows:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`是一个`pandas` Series，它提供了论文ID与一个类别之间的映射，如下所示：'
- en: '[PRE13]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Starting from this information, we create a training set and a validation set.
    In the training samples, we will include information relating to neighbors (which
    may or may not belong to the training set and therefore have a label), and this
    will be used to regularize the training.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些信息开始，我们创建一个训练集和一个验证集。在训练样本中，我们将包括与邻居相关的信息（这些邻居可能属于也可能不属于训练集，因此可能有标签），这将用于正则化训练。
- en: Validation samples, on the other hand, will not have neighbor information and
    the predicted label will only depend on the node features—namely, the BOW representation.
    Therefore, we will leverage both labeled and unlabeled samples (semi-supervised
    task) in order to produce an inductive model that can also be used against unobserved
    samples.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，验证样本将没有邻域信息，预测标签将仅取决于节点特征——即词袋（BOW）表示。因此，我们将利用标记和无标记样本（半监督任务）来生成一个可以用于未观察样本的归纳模型。
- en: 'To start with, we conveniently structure the node features as a DataFrame,
    whereas we store the graph as an adjacency matrix, as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将节点特征方便地结构化为一个 DataFrame，而将图存储为邻接矩阵，如下所示：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Using `adjMatrix`, we implement a helper function that is able to retrieve
    the closest `topn` neighbors of a node, returning the node ID and the edge weight,
    as illustrated in the following code snippet:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `adjMatrix`，我们实现了一个辅助函数，能够检索节点的最接近的 `topn` 邻居，返回节点 ID 和边权重，如下面的代码片段所示：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Using the preceding information together with the helper function, we can merge
    the information into a single DataFrame, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的信息和辅助函数，我们可以将信息合并到一个单独的 DataFrame 中，如下所示：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This DataFrame represents the node-centric feature space. This would suffice
    if we were to use a regular classifier that does not exploit the information of
    the relationships between nodes. However, in order to allow the computation of
    the graph-regularization term, we need to join the preceding DataFrame with information
    relating to the neighborhood of each node. We then define a function able to retrieve
    and join the neighborhood information, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 DataFrame 代表了以节点为中心的特征空间。如果我们使用一个不利用节点间关系信息的常规分类器，这将足够了。然而，为了允许计算图正则化项，我们需要将前面的
    DataFrame 与与每个节点的邻域相关的信息连接起来。然后我们定义一个函数，能够检索并连接邻域信息，如下所示：
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As shown in the preceding code snippet, when the neighbors are less than `topn`,
    we set the weight and the one-hot encoding of the words to `0`. The `GRAPH_PREFIX`
    constant is a prefix that is to be prepended to all features that will later be
    used by the `nsl` library to regularize the training. Although it can be changed,
    in the following code snippet we will keep its value equal to the default value:
    `"NL_nbr"`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，当邻居数量少于 `topn` 时，我们将权重和单词的 one-hot 编码设置为 `0`。`GRAPH_PREFIX` 常量是一个前缀，它将被添加到所有将后来由
    `nsl` 库用于正则化的特征之前。尽管它可以更改，但在下面的代码片段中，我们将保持其默认值：`"NL_nbr"`。
- en: 'This function can be applied to the DataFrame in order to compute the full
    feature space, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以应用于 DataFrame，以计算完整的特征空间，如下所示：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We now have in `allFeatures` all the ingredients we need to implement our graph-regularized
    model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`allFeatures` 中包含了我们实现图正则化模型所需的所有成分。
- en: 'We start by splitting our dataset into a training set and a validation set,
    as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据集分为训练集和验证集，如下所示：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: By changing the ratio, we can change the amount of labeled versus unlabeled
    data points. As the ratio decreases, we expect the performance of standard non-regularized
    classifiers to reduce. However, such a reduction can be compensated by leveraging
    network information provided by unlabeled data. We thus expect graph-regularized
    neural networks to provide better performance thanks to the augmented information
    they leverage. For the following code snippet, we will assume a `ratio` value
    equal to `0.2`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变比率，我们可以改变标记数据点与无标记数据点的数量。随着比率的降低，我们预计标准非正则化分类器的性能会降低。然而，这种降低可以通过利用无标记数据提供的网络信息来补偿。因此，我们预计图正则化神经网络将提供更好的性能，因为它们利用了增强的信息。对于下面的代码片段，我们将假设
    `ratio` 值等于 `0.2`。
- en: Before feeding this data into our neural network, we convert the DataFrame into
    a TensorFlow tensor and dataset, which is a convenient representation that will
    allow the model to refer to feature names in its input layers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在将此数据输入到我们的神经网络之前，我们将 DataFrame 转换为 TensorFlow 张量和数据集，这是一个方便的表示，将允许模型在其输入层中引用特征名称。
- en: 'Since the input features have different data types, it is best to handle the
    dataset creation separately for `weights`, `words`, and `labels` values, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入特征具有不同的数据类型，最好分别处理 `weights`、`words` 和 `labels` 值的数据集创建，如下所示：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now that we have the tensor, we can merge all this information into a TensorFlow
    dataset, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了张量，我们可以将所有这些信息合并到一个 TensorFlow 数据集中，如下所示：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can similarly create a validation set. As mentioned previously, since we
    want to design an inductive algorithm, the validation dataset does not need any
    neighborhood information. The code is illustrated in the following snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地创建一个验证集。如前所述，由于我们想要设计一个归纳算法，验证数据集不需要任何邻域信息。代码如下所示：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before feeding the dataset into the model, we split the features from the labels,
    as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据集输入模型之前，我们需要将特征与标签分开，如下所示：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'That''s it! We have generated the inputs to our model. We could also inspect
    one sample batch of our dataset by printing the values of features and labels,
    as shown in the following code block:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经生成了我们模型的输入。我们还可以通过打印特征和标签的值来检查我们数据集的一个样本批次，如下面的代码块所示：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is now time to create our first model. To do this, we start from a simple
    architecture that takes as input the one-hot representation and has two hidden
    layers, composed of a `Dense` layer plus a `Dropout` layer with 50 units each,
    as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候创建我们的第一个模型了。为此，我们从简单的架构开始，该架构以单热表示作为输入，并有两个隐藏层，每个隐藏层由一个`Dense`层和一个具有50个单位的`Dropout`层组成，如下所示：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Indeed, we could also train this model without graph regularization by simply
    compiling the model to create a computational graph, as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们也可以通过简单地编译模型以创建计算图来训练这个模型而不使用图正则化，如下所示：
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And then, we could run it as usual, also allowing the history file to be written
    to disk in order to be monitored using `TensorBoard`, as illustrated in the following
    code snippet:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像往常一样运行它，同时允许将历史文件写入磁盘，以便使用`TensorBoard`进行监控，如下面的代码片段所示：
- en: '[PRE27]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At the end of the process, we should have something similar to the following
    output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理过程的最后，我们应该得到以下类似的输出：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'With a top performance around 0.6 in accuracy, we now need to create a graph-regularized
    version of the preceding model. First of all, we need to recreate our model from
    scratch. This is important when comparing the results. If we were to use layers
    already initialized and used in the previous model, the layer weights would not
    be random but would be used with the ones already optimized in the preceding run.
    Once a new model has been created, adding a graph regularization technique to
    be used at training time can be done in just a few lines of code, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在准确率大约为0.6的顶级性能下，我们现在需要创建前面模型的图正则化版本。首先，我们需要从头开始重新创建我们的模型。这在比较结果时很重要。如果我们使用之前模型中已经初始化并使用的层，则层权重将不会是随机的，而是会使用之前运行中已经优化的权重。一旦创建了一个新的模型，我们只需几行代码就可以在训练时添加图正则化技术，如下所示：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s analyze the different hyperparameters of the regularization, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析正则化的不同超参数，如下所示：
- en: '`max_neighbors` tunes the number of neighbors that ought to be used for computing
    the regularization loss for each node.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_neighbors` 调整用于计算每个节点正则化损失的邻居数量。'
- en: '`multiplier` corresponds to the coefficients that tune the importance of the
    regularization loss. Since we only consider labeled-labeled and labeled-unlabeled,
    this effectively corresponds to ![](img/B16069__04_069.png) and ![](img/B16069__04_070.png).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`multiplier` 对应于调整正则化损失重要性的系数。由于我们只考虑有标签-有标签和有标签-无标签，这实际上对应于 ![img/B16069__04_069.png](img/B16069__04_069.png)
    和 ![img/B16069__04_070.png](img/B16069__04_070.png)。'
- en: '`distance_type` represents the pairwise distance ![](img/B16069__04_071.png)
    to be used.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distance_type` 表示要使用的成对距离 ![img/B16069__04_071.png](img/B16069__04_071.png)。'
- en: '`sum_over_axis` sets whether the weighted average sum should be calculated
    with respect to features (when set to `None`) or to samples (when set to -1).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sum_over_axis` 设置是否应该根据特征（当设置为`None`时）或样本（当设置为-1时）计算加权平均和。'
- en: 'The graph-regularized model can be compiled and run in the same way as before
    with the following commands:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图正则化模型可以使用以下命令以与之前相同的方式进行编译和运行：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that the loss function now also accounts for the graph-regularization
    term, as defined previously. Therefore, we now also introduce information coming
    from neighboring nodes that regularizes the training of our neural network. The
    preceding code, after about 200 iterations, provides the following output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在的损失函数现在也考虑了之前定义的图正则化项。因此，我们现在还引入了来自相邻节点的信息，以正则化我们神经网络的训练。前面的代码在大约200次迭代后提供了以下输出：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, graph regularization, when compared to the vanilla version,
    has allowed us to boost the performance in terms of accuracy by about 5%. Not
    bad at all!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与原始版本相比，图正则化使我们能够在准确率方面提升大约 5%。这绝对不错！
- en: You can perform several experiments, changing the ratio of labeled/unlabeled
    samples, the number of neighbors to be used, the regularization coefficient, the
    distance, and more. We encourage you to play around with the notebook that is
    provided with this book to explore the effect of different parameters yourself.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进行多个实验，改变标记/未标记样本的比例、要使用的邻居数量、正则化系数、距离等。我们鼓励你使用本书提供的笔记本进行探索，以自己研究不同参数的影响。
- en: In the right panel of the following screenshot, we show the dependence of the
    performance measured by the accuracy as the supervised ratio increases. As expected,
    performance increases as the ratio increases. On the left panel, we show the accuracy
    increments on the validation set for various configuration of neighbors and supervised
    ratio, defined by
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图的右侧面板中，我们展示了随着监督比率的增加，通过准确率测量的性能依赖关系。正如预期的那样，随着比率的增加，性能也会提高。在左侧面板中，我们展示了不同邻居配置和监督比率在验证集上的准确率提升。
- en: '![](img/B16069__04_072.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_072.jpg)'
- en: ':'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ':'
- en: '![Figure 4.10 – (Left) Accuracy on the validation set for the graph-regularized
    neural networks with neighbors = 2 and various supervised ratios; (Right) accuracy
    increments on the validation set for the graph-regularized neural networks compared
    to the vanilla version](img/B16069_04_10(merged).jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – (左) 邻居数为 2 和各种监督比率的图正则化神经网络在验证集上的准确率；(右) 与原始版本相比，图正则化神经网络在验证集上的准确率提升](img/B16069_04_10(merged).jpg)'
- en: Figure 4.10 – (Left) Accuracy on the validation set for the graph-regularized
    neural networks with neighbors = 2 and various supervised ratios; (Right) accuracy
    increments on the validation set for the graph-regularized neural networks compared
    to the vanilla version
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – (左) 邻居数为 2 和各种监督比率的图正则化神经网络在验证集上的准确率；(右) 与原始版本相比，图正则化神经网络在验证集上的准确率提升
- en: As can be seen in *Figure 4.10*, almost all graph-regularized versions outperform
    the vanilla models. The only exceptions are configuration neighbors = 2 and ratio
    = 0.5, for which the two models perform very similarly. However, the curve has
    a clear positive trend and we reasonably expect the graph-regularized version
    to outperform the vanilla model for a larger number of epochs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 4.10* 所示，几乎所有图正则化版本都优于原始模型。唯一的例外是邻居数为 2 和比率为 0.5 的配置，这两个模型的表现非常相似。然而，曲线有明显的上升趋势，我们有理由期待图正则化版本在更多轮次中优于原始模型。
- en: Note that in the notebook, we also use another interesting feature of TensorFlow
    for creating the datasets. Instead of using a `pandas` DataFrame, as we did previously,
    we will create a dataset using the TensorFlow `Example`, `Features`, and `Feature`
    classes, which, besides providing a high-level description of samples, also allow
    us to serialize the input data (using `protobuf`) to make them compatible across
    platforms and programming languages.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在笔记本中，我们还使用了 TensorFlow 的另一个有趣特性来创建数据集。与之前使用 `pandas` DataFrame 不同，我们将使用
    TensorFlow 的 `Example`、`Features` 和 `Feature` 类来创建数据集，这些类不仅提供了样本的高级描述，还允许我们使用
    `protobuf` 序列化输入数据，使其在不同平台和编程语言之间兼容。
- en: If you are interested in further using `TensorFlow` both for prototyping models
    and deploying them into production via data-driven applications (maybe written
    in other languages), we strongly advise you to dig further into these concepts.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对进一步使用 `TensorFlow` 进行模型原型设计和通过数据驱动应用（可能用其他语言编写）将其部署到生产环境中感兴趣，我们强烈建议你深入研究这些概念。
- en: Planetoid
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Planetoid
- en: The methods discussed so far provide graph regularization that is based on the
    Laplacian matrix. As we have seen in previous chapters, enforcing constraints
    based on ![](img/B16069__04_073.png) ensures that first-order proximity is preserved.
    Yang et al. (2016) proposed a method to extend graph regularization in order to
    also account for higher-order proximities. Their approach, which they named **Planetoid**
    (short for **Predicting Labels And Neighbors with Embeddings Transductively Or
    Inductively from Data**), extends skip-gram methods used for computing node embeddings
    to incorporate node-label information.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的方法提供了基于拉普拉斯矩阵的图正则化。如我们在前面的章节中看到的，基于 ![](img/B16069__04_073.png) 的约束确保了第一阶邻近性的保留。Yang
    等人（2016）提出了一种扩展图正则化的方法，以便也考虑高阶邻近性。他们的方法，他们命名为 **Planetoid**（代表 **从数据中通过归纳或演绎预测标签和邻居**），扩展了用于计算节点嵌入的
    skip-gram 方法，以包含节点标签信息。
- en: 'As we have seen in the previous chapter, skip-gram methods are based on generating
    random walks through a graph and then using the generated sequences to learn embeddings
    via a skip-gram model. The following diagram shows how the unsupervised version
    is modified to account for the supervised loss:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章所见，skip-gram 方法基于在图中生成随机游走，然后使用生成的序列通过 skip-gram 模型学习嵌入。以下图表展示了如何修改无监督版本以考虑监督损失：
- en: '![Figure 4.11 – Sketch of the Planetoid architecture: the dashed line represents
    a parametrized function that allows the method to extend from transductive to
    inductive](img/B16069_04_11.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – Planetoid 架构草图：虚线表示一个参数化函数，允许方法从归纳扩展到演绎](img/B16069_04_11.jpg)'
- en: 'Figure 4.11 – Sketch of the Planetoid architecture: the dashed line represents
    a parametrized function that allows the method to extend from transductive to
    inductive'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – Planetoid 架构草图：虚线表示一个参数化函数，允许方法从归纳扩展到演绎
- en: 'As shown in *Figure 4.11*, embeddings are fed to both of the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 4.11* 所示，嵌入被输入到以下两个部分：
- en: A softmax layer to predict the graph context of the sampled random-walk sequences
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 softmax 层来预测采样随机游走序列的图上下文
- en: A set of hidden layers that combine together with the hidden layers derived
    from the node features in order to predict the class labels
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组隐藏层，它们与从节点特征中导出的隐藏层结合在一起，以预测类别标签
- en: 'The cost function to be minimized to train the combined network is composed
    of a supervised and an unsupervised loss—![](img/B16069__04_074.png) and ![](img/B16069__04_075.png),
    respectively. The unsupervised loss is analogous to the one used with skip-gram
    with negative sampling, whereas the supervised loss minimizes the conditional
    probability and can be written as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 训练组合网络的成本函数由监督损失和无监督损失组成——分别表示为 ![](img/B16069__04_074.png) 和 ![](img/B16069__04_075.png)。无监督损失类似于与负采样一起使用的
    skip-gram，而监督损失最小化条件概率，可以表示如下：
- en: '![](img/B16069__04_076.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16069__04_076.jpg)'
- en: The preceding formulation is *transductive* as it requires samples to belong
    to the graph in order to be applied. In a semi-supervised task, this method can
    be efficiently used to predict labels for unlabeled examples. However, it cannot
    be used for unobserved samples. As shown by the dashed line in *Figure 4.11*,
    an inductive version of the Planetoid algorithm can be obtained by parametrizing
    the embeddings as a function of the node features, via dedicated connected layers.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公式是 *归纳* 的，因为它要求样本属于图才能应用。在半监督任务中，这种方法可以有效地用于预测未标记样本的标签。然而，它不能用于未观察到的样本。如图
    4.11 中的虚线所示，通过将嵌入参数化为节点特征的函数，通过专用连接层，可以获得 Planetoid 算法的归纳版本。
- en: Graph CNNs
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图卷积网络（Graph CNNs）
- en: In [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046), *Unsupervised
    Graph Learning*, we have learned the main concepts behind GNNs and **graph convolutional
    networks** (**GCNs**). We have also learned the difference between spectral graph
    convolution and spatial graph convolution. More precisely, we have further seen
    that GCN layers can be used to encode graphs or nodes under unsupervised settings
    by learning how to preserve graph properties such as node similarity.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 3 章*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046) *无监督图学习* 中，我们学习了
    GNN 和 **图卷积网络**（**GCNs**）背后的主要概念。我们还学习了频谱图卷积和空间图卷积之间的区别。更确切地说，我们进一步看到 GCN 层可以在无监督设置下使用，通过学习如何保留图属性（如节点相似性）来编码图或节点。
- en: In this chapter, we will explore such methods under supervised settings. This
    time, our goal is to learn graphs or node representations that can accurately
    *predict node or graph labels*. It is indeed worth noting that the encoding function
    remains the same. What will change is the objective!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在监督设置下的此类方法。这次，我们的目标是学习能够准确预测节点或图标签的图或节点表示。确实值得指出的是，编码函数保持不变。将发生变化的是目标函数！
- en: Graph classification using GCNs
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GCN进行图分类
- en: 'Let''s consider again our `PROTEINS` dataset. Let''s load the dataset as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次考虑我们的`PROTEINS`数据集。让我们按照以下方式加载数据集：
- en: '[PRE32]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the following example, we are going to use (and compare) one of the most
    widely used GCN algorithms for graph classification: *GCN* by Kipf and Welling:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用（并比较）最广泛使用的GCN算法之一进行图分类：Kipf和Welling的*GCN*：
- en: '`stellargraph`, which we are using for building the model, uses `tf.Keras`
    as the backend. According to its specific criteria, we need a data generator to
    feed the model. More precisely, since we are addressing a supervised graph classification
    problem, we can use an instance of the `PaddedGraphGenerator` class of `stellar``graph`,
    which automatically resolves differences in the number of nodes by using padding.
    Here is the code required for this step:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用的`stellargraph`在构建模型时使用`tf.Keras`作为后端。根据其特定标准，我们需要一个数据生成器来为模型提供数据。更确切地说，由于我们正在解决一个监督图分类问题，我们可以使用`stellar``graph`的`PaddedGraphGenerator`类的实例，它通过填充自动解决节点数量差异。以下是这一步骤所需的代码：
- en: '[PRE33]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We are now ready to actually create our first model. We will create and stack
    together four GCN layers through the `utility` function of `stellargraph`, as
    follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好实际创建我们的第一个模型了。我们将通过`stellargraph`的`utility`函数创建并堆叠四个GCN层，如下所示：
- en: '[PRE34]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This *backbone* will be concatenated to `tf.Keras`, as follows:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个*骨干*将被连接到`tf.Keras`，如下所示：
- en: '[PRE35]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s create and compile a model using `tf.Keras` utilities. We will train
    the model with a `binary_crossentropy` loss function (to measure the difference
    between predicted labels and ground truth) with the `Adam` optimizer and a *learning
    rate* of 0.0001\. We will also monitor the accuracy metric while training. The
    code is illustrated in the following snippet:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`tf.Keras`工具创建和编译一个模型。我们将使用`binary_crossentropy`损失函数（用于衡量预测标签和真实标签之间的差异）以及`Adam`优化器和0.0001的*学习率*来训练模型。我们还将监控训练过程中的准确率指标。以下代码片段展示了这一过程：
- en: '[PRE36]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now exploit `scikit-learn` utilities to create train and test sets.
    In our experiments, we will be using 70% of the dataset as a training set and
    the remainder as a test set. In addition, we need to use the `flow` method of
    the generator to supply them to the model. The code to achieve this is shown in
    the following snippet:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以利用`scikit-learn`工具创建训练集和测试集。在我们的实验中，我们将使用数据集的70%作为训练集，其余部分作为测试集。此外，我们需要使用生成器的`flow`方法将它们提供给模型。以下代码片段展示了如何实现这一点：
- en: '[PRE37]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'It''s now time for training. We train the model for 100 epochs. However, feel
    free to play with the hyperparameters to gain better performance. Here is the
    code for this:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是进行训练的时候了。我们将模型训练100个epoch。然而，你可以随意调整超参数以获得更好的性能。以下是相应的代码：
- en: '[PRE38]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'After 100 epochs, this should be the output:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经过100个epoch后，应该得到以下输出：
- en: '[PRE39]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, we are achieving about 76% accuracy on the training set and about 73%
    accuracy on the test set.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在训练集上达到了大约76%的准确率，在测试集上达到了大约73%的准确率。
- en: Node classification using GraphSAGE
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GraphSAGE进行节点分类
- en: In the next example, we will train `GraphSAGE` to classify nodes of the `Cora`
    dataset.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将训练`GraphSAGE`以对`Cora`数据集的节点进行分类。
- en: 'Let''s first load the dataset using `stellargraph` utilities, as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用`stellargraph`工具来加载数据集，如下所示：
- en: '[PRE40]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Follow this list of steps to train `GraphSAGE` to classify nodes of the `Cora`
    dataset:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤来训练`GraphSAGE`以对`Cora`数据集的节点进行分类：
- en: 'As in the previous example, the first step is to split the dataset. We will
    be using 90% of the dataset as a training set and the remainder for testing. Here
    is the code for this step:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前一个示例，第一步是分割数据集。我们将使用数据集的90%作为训练集，其余部分用于测试。以下是这一步骤的代码：
- en: '[PRE41]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This time, we will convert labels using `c` be the number of possible targets
    (seven, in the case of the `Cora` dataset), and each label will be converted in
    a vector of size `c`, where all the elements are `0` except for the one corresponding
    to the target class. The code is illustrated in the following snippet:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次，我们将使用`c`表示可能的靶标数量（在`Cora`数据集中为七个），每个标签将被转换为一个大小为`c`的向量，其中所有元素都是`0`，除了对应于目标类别的那个元素。代码在下面的代码片段中展示：
- en: '[PRE42]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s create a generator to feed the data into the model. We will be using
    an instance of the `GraphSAGENodeGenerator` class of `stellargraph`. We will use
    the `flow` method to feed the model with the train and test sets, as follows:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个生成器来将数据输入到模型中。我们将使用`stellargraph`的`GraphSAGENodeGenerator`类的实例。我们将使用`flow`方法将训练集和测试集输入到模型中，如下所示：
- en: '[PRE43]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, let''s create the model and compile it. For this exercise, we will
    be using a `GraphSAGE` encoder with three layers of 32, 32, and 16 dimensions,
    respectively. The encoder will then be connected to a dense layer with *softmax*
    activation to perform the classification. We will use an `Adam` optimizer with
    a *learning rate* of 0.03 and `categorical_crossentropy` as the loss function.
    The code is illustrated in the following snippet:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们创建模型并编译它。在这个练习中，我们将使用一个具有32、32和16维度的三层`GraphSAGE`编码器。编码器随后将连接到一个具有*softmax*激活函数的密集层以执行分类。我们将使用*学习率*为0.03的`Adam`优化器，并将`categorical_crossentropy`作为损失函数。代码在下面的代码片段中展示：
- en: '[PRE44]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'It''s now time to train the model. We will train the model for 20 epochs, as
    follows:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候训练模型了。我们将训练模型20个周期，如下所示：
- en: '[PRE45]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This should be the output:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这应该是输出：
- en: '[PRE46]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We achieved about 89% accuracy over the training set and about 80% accuracy
    over the test set.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上达到了大约89%的准确率，在测试集上达到了大约80%的准确率。
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned how supervised ML can be effectively applied
    on graphs to solve real problems such as node and graph classification.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何有效地将监督机器学习应用于图来解决节点和图分类等实际问题。
- en: In particular, we first analyzed how graph and node properties can be directly
    used as features to train classic ML algorithms. We have seen shallow methods
    and simple approaches to learning node, edge, or graph representations for only
    a finite set of input data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是首先，我们分析了如何直接使用图和节点属性作为特征来训练经典的机器学习算法。我们已经看到了针对有限输入数据集的节点、边或图表示的浅层方法和简单学习方法。
- en: We have than learned how regularization techniques can be used during the learning
    phase in order to create more robust models that tend to generalize better.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了如何在学习阶段使用正则化技术来创建更鲁棒且泛化能力更强的模型。
- en: Finally, we have seen how GNNs can be applied to solve supervised ML problems
    on graphs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经看到了图神经网络（GNNs）如何应用于解决图上的监督机器学习问题。
- en: But what can those algorithms be useful for? In the next chapter, we will explore
    common problems on graphs that need to be solved through ML techniques.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些算法有什么用途呢？在下一章中，我们将探讨需要通过机器学习技术解决的问题的常见图问题。
