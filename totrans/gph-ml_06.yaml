- en: 'Chapter 4: Supervised Graph Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Supervised learning** (**SL**) most probably represents the majority of practical
    **machine learning** (**ML**) tasks. Thanks to more and more active and effective
    data collection activities, it is very common nowadays to deal with labeled datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: This is also true for graph data, where labels can be assigned to nodes, communities,
    or even to an entire structure. The task, then, is to learn a mapping function
    between the input and the label (also known as a target or an annotation).
  prefs: []
  type: TYPE_NORMAL
- en: For example, given a graph representing a social network, we might be asked
    to guess which user (node) will close their account. We can learn this predictive
    function by training graph ML on **retrospective data**, where each user is labeled
    as "faithful" or "quitter" based on whether they closed their account after a
    few months.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the concept of SL and how it can be applied
    on graphs. Therefore, we will also be providing an overview of the main supervised
    graph embedding methods. The following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: The supervised graph embedding roadmap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-based methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shallow embedding methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph regularization methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph **convolutional neural networks** (**CNNs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using *Jupyter* Notebooks with *Python* 3.8 for all of our exercises.
    In the following code block, you can see a list of the Python libraries that will
    be installed for this chapter using `pip` (for example, run `pip install networkx==2.5`
    on the command line):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the rest of this book, if not clearly stated, we will refer to `nx` as the
    result of the `import networkx as nx` Python command.
  prefs: []
  type: TYPE_NORMAL
- en: All code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: The supervised graph embedding roadmap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In SL, a training set consists of a sequence of ordered pairs (*x*, *y*), where
    *x* is a set of input features (often signals defined on graphs) and *y* is the
    output label assigned to it. The goal of the ML models, then, is to learn the
    function mapping each *x* value to each *y* value. Common supervised tasks include
    predicting user properties in a large social network or predicting molecules'
    attributes, where each molecule is a graph.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, however, not all instances can be provided with a label. In this
    scenario, a typical dataset consists of a small set of labeled instances and a
    larger set of unlabeled instances. For such situations, **semi-SL** (**SSL**)
    is proposed, whereby algorithms aim to exploit label dependency information reflected
    by available label information in order to learn the predicting function for the
    unlabeled samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'With regard to supervised graph ML techniques, many algorithms have been developed.
    However as previously reported by different scientific papers ([https://arxiv.org/abs/2005.03675](https://arxiv.org/abs/2005.03675)),
    they can be grouped into macro-groups such as **feature-based methods**, **shallow
    embedding methods**, **regularization methods**, and **graph neural networks**
    (**GNNs**), as graphically depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Hierarchical structure of the different supervised embedding
    algorithms described in this book ](img/B16069_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Hierarchical structure of the different supervised embedding algorithms
    described in this book
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, you will learn the main principles behind each group
    of algorithms. We will try to provide insight into the most well-known algorithms
    in the field as well, as these can be used to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-based methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very simple (yet powerful) method for applying ML on graphs is to consider
    the encoding function as a simple embedding lookup. When dealing with supervised
    tasks, one simple way of doing this is to exploit graph properties. In [*Chapter
    1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014), *Getting Started with Graphs*,
    we have learned how graphs (or nodes in a graph) can be described by means of
    structural properties, each "encoding" important information from the graph itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s forget graph ML for a moment: in classical supervised ML, the task is
    to find a function that maps a set of (descriptive) features of an instance to
    a particular output. Such features should be carefully engineered so that they
    are sufficiently representative to learn that concept. Therefore, as the number
    of petals and the sepal length might be good descriptors for a flower, when describing
    a graph we might rely on its average degree, its global efficiency, and its characteristic
    path length.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This shallow approach acts in two steps, outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a set of *good* descriptive graph properties.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use such properties as input for a traditional ML algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unfortunately, there is no general definition of *good* descriptive properties,
    and their choice strictly depends on the specific problem to solve. However, you
    can still compute a wide variety of graph properties and then perform *feature
    selection* to select the most informative ones. **Feature selection** is a widely
    studied topic in ML, but providing details about the various methods is outside
    the scope of this book. However, we refer you to the book *Machine Learning Algorithms
    – Second Edition* ([https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789347999)),
    published by Packt Publishing, for further reading on this subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see a practical example of how such a basic method can be applied.
    We will be performing a supervised graph classification task by using a `PROTEINS`
    dataset. The `PROTEINS` dataset contains several graphs representing protein structures.
    Each graph is labeled, defining whether the protein is an enzyme or not. We will
    follow these next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the dataset through the `stellargraph` Python library, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For computing graph properties, we will be using `networkx`, as described in
    [*Chapter 1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014), *Getting Started
    with Graphs*. To that end, we need to convert graphs from the `stellargraph` format
    to the `networkx` format. This can be done in two steps: first, convert the graphs
    from the `stellargraph` representation to `numpy` adjacency matrices. Then, use
    the adjacency matrices to retrieve the `networkx` representation. In addition,
    we also transform the labels (which are stored as a `pandas` Series) to a `numpy`
    array, which can be better exploited by the evaluation functions, as we will see
    in the next steps. The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, for each graph, we compute global metrics to describe it. For this example,
    we have chosen the number of edges, the average cluster coefficient, and the global
    efficiency. However, we suggest you compute several other properties you may find
    worth exploring. We can extract the graph metrics using `networkx`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now exploit `scikit-learn` utilities to create train and test sets.
    In our experiments, we will be using 70% of the dataset as the training set and
    the remainder as the test set. We can do that by using the `train_test_split`
    function provided by `scikit-learn`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time for training a proper ML algorithm. We chose a `SVC` module
    of `scikit-learn`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should be the output of the previous snippet of code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used `Accuracy`, `Precision`, `Recall`, and `F1-score` to evaluate how well
    the algorithm is performing on the test set. We achieved about 80% for the F1
    score, which is already quite good for such a naïve task.
  prefs: []
  type: TYPE_NORMAL
- en: Shallow embedding methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we already described in [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046),
    *Unsupervised Graph Learning*, shallow embedding methods are a subset of graph
    embedding methods that learn node, edge, or graph representation for only a finite
    set of input data. They cannot be applied to other instances different from the
    ones used to train the model. Before starting our discussion, it is important
    to define how supervised and unsupervised shallow embedding algorithms differ.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between unsupervised and supervised embedding methods essentially
    lies in the task they attempt to solve. Indeed, if unsupervised shallow embedding
    algorithms try to learn a good graph, node, or edge representation in order to
    build well-defined clusters, the supervised algorithms try to find the best solution
    for a prediction task such as node, label, or graph classification.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explain in detail some of those supervised shallow
    embedding algorithms. Moreover, we will enrich our description by providing several
    examples of how to use those algorithms in Python. For all the algorithms described
    in this section, we will present a custom implementation using the base classes
    available in the `scikit-learn` library.
  prefs: []
  type: TYPE_NORMAL
- en: Label propagation algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The label propagation algorithm is a well-known semi-supervised algorithm widely
    applied in data science and used to solve the node classification task. More precisely,
    the algorithm *propagates* the label of a given node to its neighbors or to nodes
    having a high probability of being reached from that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general idea behind this approach is quite simple: given a graph with a
    set of labeled and unlabeled nodes, the labeled nodes propagate their label to
    the nodes having the highest probability of being reached. In the following diagram,
    we can see an example of a graph having labeled and unlabeled nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Example of a graph with two labeled nodes (class 0 in red and
    class 1 in green) and six unlabeled nodes](img/B16069_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Example of a graph with two labeled nodes (class 0 in red and class
    1 in green) and six unlabeled nodes
  prefs: []
  type: TYPE_NORMAL
- en: According to *Figure 4.2*, using the information of the labeled nodes (node
    **0** and **6**), the algorithm will calculate the probability of moving to another
    unlabeled node. The nodes having the highest probability from a labeled node will
    get the label of that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let ![](img/B16069__04_001.png) be a graph and let ![](img/B16069__04_002.png)
    be a set of labels. Since the algorithm is semi-supervised, just a subset of nodes
    will have an assigned label. Moreover, let ![](img/B16069__04_003.png) be the
    adjacency matrix of the input graph G and ![](img/B16069__04_004.png) be the diagonal
    degree matrix where each element ![](img/B16069__04_005.png) is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, the only nonzero elements of the degree matrix are the diagonal
    elements whose values are given by the degree of the node represented by the row.
    In the following figure, we can see the diagonal degree matrix of the graph represented
    in *Figure 4.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Diagonal degree matrix for the graph in Figure 4.2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Diagonal degree matrix for the graph in Figure 4.2
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Figure 4.3*, it is possible to see how only the diagonal elements of
    the matrix contain nonzero values, and those values represent the degree of the
    specific node. We also need to introduce the transition matrix ![](img/B16069__04_007.png).
    This matrix defines the probability of a node being reached from another node.
    More precisely, ![](img/B16069__04_008.png) is the probability of reaching node
    ![](img/B16069__04_009.png) from node ![](img/B16069__04_010.png). The following
    figure shows the transition matrix ![](img/B16069__04_011.png) for the graph depicted
    in *Figure 4.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Transition matrix for the graph in Figure 4.2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Transition matrix for the graph in Figure 4.2
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 4.4*, the matrix shows the probability of reaching an end node given
    a start node. For instance, from the first row of the matrix, we can see how from
    node 0 it is possible to reach, with equal probability of 0.5, only nodes 1 and
    2\. If we defined with ![](img/B16069__04_012.png) the initial label assignment,
    the probability of label assignment for each node obtained using the ![](img/B16069__04_013.png)
    matrix can be computed as ![](img/B16069__04_014.png). The ![](img/B16069__04_015.png)
    matrix computed for the graph in *Figure 4.2* is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Solution obtained using the matrix for the graph in Figure 4.2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Solution obtained using the matrix for the graph in Figure 4.2
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 4.5*, we can see that using the transition matrix, node 1 and node
    2 have a probability of being assigned to the ![](img/B16069__04_016.png) label
    of 0.5 and 0.33 respectively, while node 5 and node 6 have a probability of being
    assigned to the ![](img/B16069__04_017.png) label of 0.33 and 0.5, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, if we better analyze *Figure 4.5*, we can see two main problems,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: With this solution, it is possible to assign only to nodes [1 2] and [5 7] a
    probability associated with a label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial labels of nodes 0 and 6 are different from the one defined in ![](img/B16069__04_018.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to solve the first point, the algorithm will perform ![](img/B16069__04_019.png)
    different iterations; at each iteration ![](img/B16069__04_020.png), the algorithm
    will compute the solution for that iteration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm stops its iteration when a certain condition is met. The second
    problem is solved by the label propagation algorithm by imposing, in the solution
    of a given iteration ![](img/B16069__04_022.png), the labeled nodes to have the
    initial class values. For example, after computing the result visible in *Figure
    4.5*, the algorithm will force the first line of the result matrix to be ![](img/B16069__04_023.png)
    and the seventh line of the matrix to be ![](img/B16069__04_024.png).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we propose a modified version of the `LabelPropagation` class available
    in the `scikit-learn` library. The main reason behind this choice is given by
    the fact that the `LabelPropagation` class takes as input a matrix representing
    a dataset. Each row of the matrix represents a sample, and each column represents
    a feature.
  prefs: []
  type: TYPE_NORMAL
- en: Before performing a `fit` operation, the `LabelPropagation` class internally
    executes the `_build_graph` function. This function will build, using a parametric
    kernel (`_get_kernel` function), a graph describing the input dataset. As a result,
    the original dataset is transformed into a graph (in its adjacency matrix representation)
    where each node is a sample (a row of the input dataset) and each edge is an *interaction*
    between the samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our specific case, the input dataset is already a graph, so we need to define
    a new class capable of dealing with a `networkx` graph and performing the computation
    operation on the original graph. The goal is achieved by creating a new class—namely,
    `GraphLabelPropagation—b`y extending the `ClassifierMixin`, `BaseEstimator`, and
    `ABCMeta` base classes. The algorithm proposed here is mainly used in order to
    help you understand the concept behind the algorithm. The whole algorithm is provided
    in the `04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb` notebook
    available in the GitHub repository of this book. In order to describe the algorithm,
    we will use only the `fit(X,y)` function as a reference. The code is illustrated
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fit(X,y)` function takes as input a `networkx` graph ![](img/B16069__04_025.png)
    and an array ![](img/B16069__04_026.png) representing the labels assigned to each
    node. Nodes without labels should have a representative value of -1\. The `while`
    loop performs the real computation. More precisely, it computes the ![](img/B16069__04_027.png)
    value at each iteration and forces the labeled nodes in the solution to be equal
    to their original input value. The algorithm performs the computation until the
    two stop conditions are satisfied. In this implementation, the following two criteria
    have been used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of iterations**: The algorithm runs the computation until a given
    number of iterations has been performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution tolerance error**: The algorithm runs the computation until the
    absolute difference of the solution obtained in two consecutive iterations, ![](img/B16069__04_028.png)
    and ![](img/B16069__04_029.png), is lower than a given threshold value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The algorithm can be applied to the example graph depicted in *Figure 4.2*
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result obtained by the algorithm is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Result of the label propagation algorithm on the graph of Figure
    4.2: on the left, the final labeled graph; on the right, the final probability
    assignment matrix](img/B16069_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6 – Result of the label propagation algorithm on the graph of Figure
    4.2: on the left, the final labeled graph; on the right, the final probability
    assignment matrix'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4.6*, we can see the results of the algorithm applied to the example
    shown in *Figure 4.2*. From the final probability assignment matrix, it is possible
    to see how the probability of the initial labeled nodes is 1 due to the constraints
    of the algorithm and how nodes that are "near" to labeled nodes get their label.
  prefs: []
  type: TYPE_NORMAL
- en: Label spreading algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The label spreading algorithm is another semi-supervised shallow embedding
    algorithm. It was built in order to overcome one big limitation of the label propagation
    method: the **initial labeling**. Indeed, according to the label propagation algorithm,
    the initial labels cannot be modified in the training process and, in each iteration,
    they are forced to be equal to their original value. This constraint could generate
    incorrect results when the initial labeling is affected by errors or noise. As
    a consequence, the error will be propagated in all nodes of the input graph.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this limitation, the label spreading algorithm tries to relax
    the constraint of the original labeled data, allowing the labeled input nodes
    to change their label during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let ![](img/B16069__04_030.png) be a graph and let ![](img/B16069__04_031.png)
    be a set of labels (since the algorithm is semi-supervised, just a subset of nodes
    will have an assigned label), and let ![](img/B16069__04_032.png) and ![](img/B16069__04_033.png)
    be the adjacency matrix diagonal degree matrix of graph G, respectively. Instead
    of computing the probability transition matrix, the label spreading algorithm
    uses the normalized graph **Laplacian matrix**, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As with label propagation, this matrix can be seen as a sort of compact low-dimensional
    representation of the connections defined in the whole graph. This matrix can
    be easily computed using `networkx` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The normalized graph Laplacian matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – The normalized graph Laplacian matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important difference between the label spreading and label propagation
    algorithms is related to the function used to extract the labels. If we defined
    with ![](img/B16069__04_035.png) the initial label assignment, the probability
    of a label assignment for each node obtained using the ![](img/B16069__04_036.png)
    matrix can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As with label propagation, label spreading has an iterative process to compute
    the final solution. The algorithm will perform ![](img/B16069__04_038.png) different
    iterations; in each iteration ![](img/B16069__04_039.png), the algorithm will
    compute the solution for that iteration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm stops its iteration when a certain condition is met. It is important
    to underline the term ![](img/B16069__04_041.png) of the equation. Indeed, as
    we said, label spreading does not force the labeled element of the solution to
    be equal to its original value. Instead, the algorithm uses a regularization parameter
    ![](img/B16069__04_042.png) to weight the influence of the original solution at
    each iteration. This allows us to explicitly impose the "quality" of the original
    solution and its influence in the final solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the label propagation algorithm, in the following code snippet, we
    propose a modified version of the `LabelSpreading` class available in the `scikit-learn`
    library due to the motivations we already mentioned in the previous section. We
    propose the `GraphLabelSpreading` class by extending our `GraphLabelPropagation`
    class, since the only difference will be in the `fit()` method of the class. The
    whole algorithm is provided in the `04_supervised_graph_machine_learning/02_Shallow_embeddings.ipynb`
    notebook available in the GitHub repository of this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Also in this class, the `fit()` function is the focal point. The function takes
    as input a `networkx` graph ![](img/B16069__04_043.png) and an array ![](img/B16069__04_044.png)
    representing the labels assigned to each node. Nodes without labels should have
    a representative value of -1\. The `while` loop computes the ![](img/B16069__04_045.png)
    value at each iteration, weighting the influence of the initial labeling via the
    parameter ![](img/B16069__04_046.png). Also, for this algorithm, the number of
    iterations and the difference between two consecutive solutions are used as stop
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm can be applied to the example graph depicted in *Figure 4.2*
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following diagram, the result obtained by the algorithm is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Result of the label propagation algorithm on graph in Figure
    4.2: on the left, the final labeled graph; on the right, the final probability
    assignment matrix](img/B16069_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8 – Result of the label propagation algorithm on graph in Figure 4.2:
    on the left, the final labeled graph; on the right, the final probability assignment
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: The result visible in the diagram shown in *Figure 4.8* looks similar to the
    one obtained using the label propagation algorithm. The main difference is related
    to the probability of label assignment. Indeed, in this case, it is possible to
    see how nodes 0 and 6 (the ones having an initial labeling) have a probability
    of 0.5, which is significantly lower compared to the probability of 1 obtained
    using the label propagation algorithm. This behavior is expected since the influence
    of the initial label assignment is weighted by the regularization parameter ![](img/B16069__04_047.png).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will continue our description of supervised graph embedding
    methods. We will describe how network-based information helps regularize the training
    and create more robust models.
  prefs: []
  type: TYPE_NORMAL
- en: Graph regularization methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shallow embedding methods described in the previous section show how topological
    information and relations between data points can be encoded and leveraged in
    order to build more robust classifiers and address semi-supervised tasks. In general
    terms, network information can be extremely useful in constraining models and
    enforcing the output to be smooth within neighboring nodes. As we have already
    seen in previous sections, this idea can be efficiently used in semi-supervised
    tasks, when propagating the information on neighbor unlabeled nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, this can also be used to regularize the learning phase in
    order to create more robust models that tend to better generalize to unseen examples.
    Both the label propagation and the label spreading algorithms we have seen previously
    can be implemented as a cost function to be minimized when we add an additional
    regularization term. Generally, in supervised tasks, we can write the cost function
    to be minimized in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B16069__04_049.png) and ![](img/B16069__04_050.png) represent
    the labeled and unlabeled samples, and the second term acts as a regularization
    term that depends on the topological information of the graph ![](img/B16069__04_051.png).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will further describe such an idea and see how this can
    be very powerful, especially when regularizing the training of neural networks,
    which—as you might know—naturally tend to overfit and/or need large amounts of
    data to be trained efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Manifold regularization and semi-supervised embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Manifold regularization** (Belkin et al., 2006) extends the label propagation
    framework by parametrizing the model function in the **reproducing kernel Hilbert
    space** (**RKHS**) and using as a supervised loss function (first term in the
    previous equation) the **mean square error** (**MSE**) or the hinge loss. In other
    words, when training an SVM or a least squares fit, they apply a graph regularization
    term based on the Laplacian matrix *L*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For this reason, these methods are generally labeled as **Laplacian regularization**,
    and such a formulation leads to **Laplacian regularized least squares** (**LapRLS**)
    and **LapSVM** classifications. Label propagation and label spreading can be seen
    as a special case of manifold regularization. Besides, these algorithms can also
    be used in the case of no-labeled data (first term in the equation disappearing)
    reducing to **Laplacian eigenmaps**.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, they can also be used in the case of a fully labeled dataset,
    in which case the preceding terms constrain the training phase to regularize the
    training and achieve more robust models. Moreover, being the classifier parametrized
    in the RKHS, the model can be used on unobserved samples and does not require
    test samples to belong to the input graph. In this sense, it is therefore an *inductive*
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Manifold learning** still represents a form of shallow learning, whereby
    the parametrized function does not leverage on any form of intermediate embeddings.
    **Semi-supervised embedding** (Weston et al., 2012) extends the concepts of graph
    regularization to deeper architectures by imposing the constraint and the smoothness
    of the function on intermediate layers of a neural network. Let''s define ![](img/B16069__04_053.png)
    as the intermediate output of the *k*th hidden layer. The regularization term
    proposed in the semi-supervised embedding framework reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_054.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Depending on where the regularization is imposed, three different configurations
    (shown in *Figure 4.9*) can be achieved, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is applied to the final output of the network. This corresponds
    to a generalization of the manifold learning technique to multilayer neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization is applied to an intermediate layer of the network, thus regularizing
    the embedding representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization is applied to an auxiliary network that shares the first k-1
    layers. This basically corresponds to training an unsupervised embedding network
    while simultaneously training a supervised network. This technique basically imposes
    a derived regularization on the first k-1 layers that are constrained by the unsupervised
    network as well and simultaneously promotes an embedding of the network nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows an illustration of the three different configurations—with
    their similarities and differences—that can be achieved using a semi-supervised
    embedding framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Semi-supervised embedding regularization configurations: graph
    regularization, indicated by the cross, can be applied to the output (left), to
    an intermediate layer (center), or to an auxiliary network (right)](img/B16069_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9 – Semi-supervised embedding regularization configurations: graph
    regularization, indicated by the cross, can be applied to the output (left), to
    an intermediate layer (center), or to an auxiliary network (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In its original formulation, the loss function used for the embeddings is the
    one derived from the Siamese network formulation, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_055.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen by this equation, the loss function ensures the embeddings of
    neighboring nodes stay close. On the other hand, non-neighbors are instead pulled
    apart to a distance (at least) specified by the threshold ![](img/B16069__04_056.png).
    As compared to the regularization based on the Laplacian ![](img/B16069__04_057.png)
    (although for neighboring points, the penalization factor is effectively recovered),
    the one shown here is generally easier to be optimized by gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The best choice among the three configurations presented in *Figure 4.9* is
    largely influenced by the data at your disposal as well as on your specific use
    case—that is, whether you need a regularized model output or to learn a high-level
    data representation. However, you should always keep in mind that when using softmax
    layers (usually done at the output layer), the regularization based on the hinge
    loss may not be very appropriate or suited for log probabilities. In such cases,
    regularized embeddings and relative loss should instead be introduced at intermediate
    layers. However, be aware that embeddings lying in deeper layers are generally
    harder to be trained and require a careful tuning of learning rate and margins
    to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Graph Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Neural graph learning** (**NGL**) basically generalizes the previous formulations
    and, as we will see, makes it possible to seamlessly apply graph regularization
    to any form of a neural network, including CNNs and **recurrent neural networks**
    (**RNNs**). In particular, there exists an extremely powerful framework named
    **Neural Structured Learning** (**NSL**) that allows us to extend in a very few
    lines of code a neural network implemented in TensorFlow with graph regularization.
    The networks can be of any kind: natural or synthetic.'
  prefs: []
  type: TYPE_NORMAL
- en: When synthetic, graphs can be generated in different ways, using—for instance—embeddings
    learned in an unsupervised manner and/or by using a similarity/distance metric
    between samples using their features. You can also generate synthetic graphs using
    adversarial examples. Adversarial examples are artificially generated samples
    obtained by perturbing actual (real) examples in such a way that we confound the
    network, trying to force a prediction error. These very carefully designed samples
    (obtained by perturbing a given sample in the gradient-descent direction in order
    to maximize errors) can be connected to their related samples, thus generating
    a graph. These connections can then be used to train a graph-regularized version
    of the network, allowing us to obtain models that are more robust against adversarially
    generated examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'NGL extends the regularization by augmenting the tuning parameters for graph
    regularization in neural networks, decomposing the contribution of labeled-labeled,
    labeled-unlabeled, and unlabeled-unlabeled relations using three parameters, ![](img/B16069__04_058.png),
    ![](img/B16069__04_059.png), and ![](img/B16069__04_060.png), respectively, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_061.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function ![](img/B16069__04_062.png) represents a generic distance between
    two vectors—for instance, the L2 norm ![](img/B16069__04_063.png). By varying
    the coefficients and the definition of ![](img/B16069__04_064.png), we can arrive
    at the different algorithms seen previously as limiting behavior, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When ![](img/B16069_04_065a.png) ![](img/B16069_04_065b.png)we retrieve the
    non-regularized version of a neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When only ![](img/B16069__04_066.png), we recover a fully supervised formulation
    where relationships between nodes act to regularize the training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we substitute ![](img/B16069__04_067.png) (which are parametrized by a
    set of alpha coefficients) with a set of values ![](img/B16069__04_068.png) (to
    be learned) that map each sample to its instance class, we recover the label propagation
    formulation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loosely speaking, the NGL formulations can be seen as a non-linear version of
    the label propagation and label spreading algorithms, or as a form of a graph-regularized
    neural network for which the manifold learning or semi-supervising embedding can
    be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: We will now apply NGL to a practical example, where you will learn how to use
    graph regularization in neural networks. To do so, we will use the NLS framework
    ([https://github.com/tensorflow/neural-structured-learning](https://github.com/tensorflow/neural-structured-learning)),
    which is a library built on top of TensorFlow that makes it possible to implement
    graph regularization with only a few lines of codes on top of standard neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we will be using the `Cora` dataset, which is a labeled dataset
    that consists of 2,708 scientific papers in computer science that have been classified
    into seven classes. Each paper represents a node that is connected to other nodes
    based on citations. In total, there are 5,429 links in the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, each node is further described by a 1,433-long vector of binary values
    (0 or 1) that represent a dichotomic `Cora` dataset can be downloaded directly
    from the `stellargraph` library with a few lines of code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns two outputs, outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`G`, which is the citation network containing the network nodes, edges, and
    the features describing the BOW representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`, which is a `pandas` Series that provides the mapping between the
    paper ID and one of the classes, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Starting from this information, we create a training set and a validation set.
    In the training samples, we will include information relating to neighbors (which
    may or may not belong to the training set and therefore have a label), and this
    will be used to regularize the training.
  prefs: []
  type: TYPE_NORMAL
- en: Validation samples, on the other hand, will not have neighbor information and
    the predicted label will only depend on the node features—namely, the BOW representation.
    Therefore, we will leverage both labeled and unlabeled samples (semi-supervised
    task) in order to produce an inductive model that can also be used against unobserved
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we conveniently structure the node features as a DataFrame,
    whereas we store the graph as an adjacency matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `adjMatrix`, we implement a helper function that is able to retrieve
    the closest `topn` neighbors of a node, returning the node ID and the edge weight,
    as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the preceding information together with the helper function, we can merge
    the information into a single DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This DataFrame represents the node-centric feature space. This would suffice
    if we were to use a regular classifier that does not exploit the information of
    the relationships between nodes. However, in order to allow the computation of
    the graph-regularization term, we need to join the preceding DataFrame with information
    relating to the neighborhood of each node. We then define a function able to retrieve
    and join the neighborhood information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code snippet, when the neighbors are less than `topn`,
    we set the weight and the one-hot encoding of the words to `0`. The `GRAPH_PREFIX`
    constant is a prefix that is to be prepended to all features that will later be
    used by the `nsl` library to regularize the training. Although it can be changed,
    in the following code snippet we will keep its value equal to the default value:
    `"NL_nbr"`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This function can be applied to the DataFrame in order to compute the full
    feature space, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We now have in `allFeatures` all the ingredients we need to implement our graph-regularized
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by splitting our dataset into a training set and a validation set,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: By changing the ratio, we can change the amount of labeled versus unlabeled
    data points. As the ratio decreases, we expect the performance of standard non-regularized
    classifiers to reduce. However, such a reduction can be compensated by leveraging
    network information provided by unlabeled data. We thus expect graph-regularized
    neural networks to provide better performance thanks to the augmented information
    they leverage. For the following code snippet, we will assume a `ratio` value
    equal to `0.2`.
  prefs: []
  type: TYPE_NORMAL
- en: Before feeding this data into our neural network, we convert the DataFrame into
    a TensorFlow tensor and dataset, which is a convenient representation that will
    allow the model to refer to feature names in its input layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the input features have different data types, it is best to handle the
    dataset creation separately for `weights`, `words`, and `labels` values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the tensor, we can merge all this information into a TensorFlow
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can similarly create a validation set. As mentioned previously, since we
    want to design an inductive algorithm, the validation dataset does not need any
    neighborhood information. The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Before feeding the dataset into the model, we split the features from the labels,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! We have generated the inputs to our model. We could also inspect
    one sample batch of our dataset by printing the values of features and labels,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now time to create our first model. To do this, we start from a simple
    architecture that takes as input the one-hot representation and has two hidden
    layers, composed of a `Dense` layer plus a `Dropout` layer with 50 units each,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, we could also train this model without graph regularization by simply
    compiling the model to create a computational graph, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we could run it as usual, also allowing the history file to be written
    to disk in order to be monitored using `TensorBoard`, as illustrated in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the process, we should have something similar to the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With a top performance around 0.6 in accuracy, we now need to create a graph-regularized
    version of the preceding model. First of all, we need to recreate our model from
    scratch. This is important when comparing the results. If we were to use layers
    already initialized and used in the previous model, the layer weights would not
    be random but would be used with the ones already optimized in the preceding run.
    Once a new model has been created, adding a graph regularization technique to
    be used at training time can be done in just a few lines of code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the different hyperparameters of the regularization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_neighbors` tunes the number of neighbors that ought to be used for computing
    the regularization loss for each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multiplier` corresponds to the coefficients that tune the importance of the
    regularization loss. Since we only consider labeled-labeled and labeled-unlabeled,
    this effectively corresponds to ![](img/B16069__04_069.png) and ![](img/B16069__04_070.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distance_type` represents the pairwise distance ![](img/B16069__04_071.png)
    to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sum_over_axis` sets whether the weighted average sum should be calculated
    with respect to features (when set to `None`) or to samples (when set to -1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The graph-regularized model can be compiled and run in the same way as before
    with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the loss function now also accounts for the graph-regularization
    term, as defined previously. Therefore, we now also introduce information coming
    from neighboring nodes that regularizes the training of our neural network. The
    preceding code, after about 200 iterations, provides the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, graph regularization, when compared to the vanilla version,
    has allowed us to boost the performance in terms of accuracy by about 5%. Not
    bad at all!
  prefs: []
  type: TYPE_NORMAL
- en: You can perform several experiments, changing the ratio of labeled/unlabeled
    samples, the number of neighbors to be used, the regularization coefficient, the
    distance, and more. We encourage you to play around with the notebook that is
    provided with this book to explore the effect of different parameters yourself.
  prefs: []
  type: TYPE_NORMAL
- en: In the right panel of the following screenshot, we show the dependence of the
    performance measured by the accuracy as the supervised ratio increases. As expected,
    performance increases as the ratio increases. On the left panel, we show the accuracy
    increments on the validation set for various configuration of neighbors and supervised
    ratio, defined by
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ':'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – (Left) Accuracy on the validation set for the graph-regularized
    neural networks with neighbors = 2 and various supervised ratios; (Right) accuracy
    increments on the validation set for the graph-regularized neural networks compared
    to the vanilla version](img/B16069_04_10(merged).jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – (Left) Accuracy on the validation set for the graph-regularized
    neural networks with neighbors = 2 and various supervised ratios; (Right) accuracy
    increments on the validation set for the graph-regularized neural networks compared
    to the vanilla version
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 4.10*, almost all graph-regularized versions outperform
    the vanilla models. The only exceptions are configuration neighbors = 2 and ratio
    = 0.5, for which the two models perform very similarly. However, the curve has
    a clear positive trend and we reasonably expect the graph-regularized version
    to outperform the vanilla model for a larger number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the notebook, we also use another interesting feature of TensorFlow
    for creating the datasets. Instead of using a `pandas` DataFrame, as we did previously,
    we will create a dataset using the TensorFlow `Example`, `Features`, and `Feature`
    classes, which, besides providing a high-level description of samples, also allow
    us to serialize the input data (using `protobuf`) to make them compatible across
    platforms and programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in further using `TensorFlow` both for prototyping models
    and deploying them into production via data-driven applications (maybe written
    in other languages), we strongly advise you to dig further into these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Planetoid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The methods discussed so far provide graph regularization that is based on the
    Laplacian matrix. As we have seen in previous chapters, enforcing constraints
    based on ![](img/B16069__04_073.png) ensures that first-order proximity is preserved.
    Yang et al. (2016) proposed a method to extend graph regularization in order to
    also account for higher-order proximities. Their approach, which they named **Planetoid**
    (short for **Predicting Labels And Neighbors with Embeddings Transductively Or
    Inductively from Data**), extends skip-gram methods used for computing node embeddings
    to incorporate node-label information.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in the previous chapter, skip-gram methods are based on generating
    random walks through a graph and then using the generated sequences to learn embeddings
    via a skip-gram model. The following diagram shows how the unsupervised version
    is modified to account for the supervised loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Sketch of the Planetoid architecture: the dashed line represents
    a parametrized function that allows the method to extend from transductive to
    inductive](img/B16069_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11 – Sketch of the Planetoid architecture: the dashed line represents
    a parametrized function that allows the method to extend from transductive to
    inductive'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 4.11*, embeddings are fed to both of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A softmax layer to predict the graph context of the sampled random-walk sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of hidden layers that combine together with the hidden layers derived
    from the node features in order to predict the class labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cost function to be minimized to train the combined network is composed
    of a supervised and an unsupervised loss—![](img/B16069__04_074.png) and ![](img/B16069__04_075.png),
    respectively. The unsupervised loss is analogous to the one used with skip-gram
    with negative sampling, whereas the supervised loss minimizes the conditional
    probability and can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16069__04_076.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding formulation is *transductive* as it requires samples to belong
    to the graph in order to be applied. In a semi-supervised task, this method can
    be efficiently used to predict labels for unlabeled examples. However, it cannot
    be used for unobserved samples. As shown by the dashed line in *Figure 4.11*,
    an inductive version of the Planetoid algorithm can be obtained by parametrizing
    the embeddings as a function of the node features, via dedicated connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: Graph CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046), *Unsupervised
    Graph Learning*, we have learned the main concepts behind GNNs and **graph convolutional
    networks** (**GCNs**). We have also learned the difference between spectral graph
    convolution and spatial graph convolution. More precisely, we have further seen
    that GCN layers can be used to encode graphs or nodes under unsupervised settings
    by learning how to preserve graph properties such as node similarity.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore such methods under supervised settings. This
    time, our goal is to learn graphs or node representations that can accurately
    *predict node or graph labels*. It is indeed worth noting that the encoding function
    remains the same. What will change is the objective!
  prefs: []
  type: TYPE_NORMAL
- en: Graph classification using GCNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider again our `PROTEINS` dataset. Let''s load the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we are going to use (and compare) one of the most
    widely used GCN algorithms for graph classification: *GCN* by Kipf and Welling:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stellargraph`, which we are using for building the model, uses `tf.Keras`
    as the backend. According to its specific criteria, we need a data generator to
    feed the model. More precisely, since we are addressing a supervised graph classification
    problem, we can use an instance of the `PaddedGraphGenerator` class of `stellar``graph`,
    which automatically resolves differences in the number of nodes by using padding.
    Here is the code required for this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to actually create our first model. We will create and stack
    together four GCN layers through the `utility` function of `stellargraph`, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This *backbone* will be concatenated to `tf.Keras`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create and compile a model using `tf.Keras` utilities. We will train
    the model with a `binary_crossentropy` loss function (to measure the difference
    between predicted labels and ground truth) with the `Adam` optimizer and a *learning
    rate* of 0.0001\. We will also monitor the accuracy metric while training. The
    code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now exploit `scikit-learn` utilities to create train and test sets.
    In our experiments, we will be using 70% of the dataset as a training set and
    the remainder as a test set. In addition, we need to use the `flow` method of
    the generator to supply them to the model. The code to achieve this is shown in
    the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time for training. We train the model for 100 epochs. However, feel
    free to play with the hyperparameters to gain better performance. Here is the
    code for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 100 epochs, this should be the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are achieving about 76% accuracy on the training set and about 73%
    accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Node classification using GraphSAGE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next example, we will train `GraphSAGE` to classify nodes of the `Cora`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first load the dataset using `stellargraph` utilities, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow this list of steps to train `GraphSAGE` to classify nodes of the `Cora`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous example, the first step is to split the dataset. We will
    be using 90% of the dataset as a training set and the remainder for testing. Here
    is the code for this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This time, we will convert labels using `c` be the number of possible targets
    (seven, in the case of the `Cora` dataset), and each label will be converted in
    a vector of size `c`, where all the elements are `0` except for the one corresponding
    to the target class. The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a generator to feed the data into the model. We will be using
    an instance of the `GraphSAGENodeGenerator` class of `stellargraph`. We will use
    the `flow` method to feed the model with the train and test sets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s create the model and compile it. For this exercise, we will
    be using a `GraphSAGE` encoder with three layers of 32, 32, and 16 dimensions,
    respectively. The encoder will then be connected to a dense layer with *softmax*
    activation to perform the classification. We will use an `Adam` optimizer with
    a *learning rate* of 0.03 and `categorical_crossentropy` as the loss function.
    The code is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time to train the model. We will train the model for 20 epochs, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should be the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We achieved about 89% accuracy over the training set and about 80% accuracy
    over the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how supervised ML can be effectively applied
    on graphs to solve real problems such as node and graph classification.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we first analyzed how graph and node properties can be directly
    used as features to train classic ML algorithms. We have seen shallow methods
    and simple approaches to learning node, edge, or graph representations for only
    a finite set of input data.
  prefs: []
  type: TYPE_NORMAL
- en: We have than learned how regularization techniques can be used during the learning
    phase in order to create more robust models that tend to generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have seen how GNNs can be applied to solve supervised ML problems
    on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: But what can those algorithms be useful for? In the next chapter, we will explore
    common problems on graphs that need to be solved through ML techniques.
  prefs: []
  type: TYPE_NORMAL
