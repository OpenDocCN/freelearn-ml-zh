<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">An Introduction to Create ML</h1>
                </header>
            
            <article>
                
<p class="mce-root">The intention of this book has been to explore ways to apply machine learning on the iPhone, specifically focusing on computer vision tasks. Even with this narrow focus, we have only scratched the surface of what is currently possible. But, hopefully, we've covered enough to spark your curiosity and provided enough intuition behind the details of machine learning models to help you on your journey to build intelligent apps. </p>
<p class="mce-root">This chapter is intended as a primer into continuing that journey by introducing <strong>Create ML</strong>, a tool released with Core ML 2 that provides an easy way to create some common models using custom data. Even though we only provide a high-level introduction, specifically around computer vision, it still should be enough to help you make use of it in your own applications. </p>
<p class="mce-root">By the end of this chapter, you will have:</p>
<ul>
<li class="mce-root">Revised the machine learning workflow </li>
<li>Appreciated the importance of splitting your data into sets for training and validation</li>
<li>Used Create ML to create a custom image classifier </li>
<li>Seen other tools and frameworks to continue your journey </li>
</ul>
<p>Let's begin by reviewing a typical machine learning workflow. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A typical workflow </h1>
                </header>
            
            <article>
                
<p>As with any project, you enter the process with some understanding of what you are trying to build. The better you understand this (the problem), the better you are able to solve it. </p>
<p>After understanding what it is that you're trying to do, your next question (in the context of building a machine learning model) is <em>what data do I need?</em> This includes an exploration into what data is available and what data you may need to generate yourself. </p>
<p>Once you've understood what you're trying to do and what data you need, your next question/task is to decide on what algorithm (or model) is needed. This is obviously dependent on your task and the data you have; in some instances, you may be required to create your own model, but more often than not, there will be an adequate model available for you to use, or at least an architecture you can use with your own data. The following table shows some typical computer vision tasks and their related machine learning counterparts:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 45.7847%"><strong>Task</strong></td>
<td style="width: 53.4371%"><strong>Machine learning algorithm</strong></td>
</tr>
<tr>
<td style="width: 45.7847%">Label images</td>
<td style="width: 53.4371%">Image classification</td>
</tr>
<tr>
<td style="width: 45.7847%">Recognize multiple objects and their location</td>
<td style="width: 53.4371%">Object detection and semantic segmentation </td>
</tr>
<tr>
<td style="width: 45.7847%">Find similar images </td>
<td style="width: 53.4371%">Image similarity </td>
</tr>
<tr>
<td style="width: 45.7847%">Creating stylized images </td>
<td style="width: 53.4371%">Style transfer</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The next step is to train your model; typically, this is an iterative process with a lot of fine-tuning until you have a model that sufficiently achieves its task on data it hadn't been trained on. </p>
<p>Finally, with a trained model, you can deploy and use your model in your application. This process is summarized in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e953380d-6b7e-4500-b2b6-46326d593c02.png" style="width:31.83em;height:18.67em;"/></div>
<div class="packt_infobox">The previous diagram is an oversimplification of the process; typically the workflow is more cyclic, with multiple iterations between training and selecting and tuning your model. It is also common to run multiple models (and model parameters) concurrently.</div>
<p>To make the concepts of this chapter more concrete, let's work with the hypothetical brief of having to build a fun application to assist toddlers to learn the names of fruits. You and your team have come up with the concept of a game that asks the toddler to find a specific fruit. The toddler earns points when they correctly identify the fruit using the device's camera. With our task now defined, let's discuss what data we need. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>For our task, we require a collection of labeled photos of fruits. As you may recall from <a href="7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Machine Learning</em>, this type of machine learning problem is known as <strong>supervised learning</strong>. We need our model to take in an image and return the label of what it thinks the image is, also known as <strong>multi-class classification</strong>. </p>
<p>Go ahead and collect photos of fruits. Create ML allows for multiple ways of organizing your data, but I find that ad hoc collection is easiest done by organizing it in folders, as shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/6438e297-85ec-4c59-972b-01cbb68c4a05.png" style="width:41.67em;height:21.08em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Source: http://www.image-net.org/ </div>
<p>Here, we have organized our data into folders, where the folder name is used as a label for its contents. An alternative is labeling each image, where each instance of a specific class has a suffix number, for example <kbd>banana.0.jpg</kbd>, <kbd>banana.1.jpg</kbd>, and so on. Or you can simply pass in a dictionary of labels with their associated list of image URLs.</p>
<p>At this stage, you may be wondering how many images you should get. Apple has suggested a minimum of 10 images per class, but<strong> </strong>you typically want to collect as many as possible, to help the model generalize by ensuring that it sees a lot of variations during training. It's also important to, wherever possible, obtain images that are as close as possible to the real data the model will be used on (in the real<em> </em>world). This is because the model is not biased according to what it learns. It just learns what it needs to. That is, if all your apple examples were of red apples with a white background, then it's likely that your model will learn to associate these colors with apples, and any time it sees these colors, it will predict that the image contains an apple.</p>
<p>As mentioned previously, Apple has suggested a minimum of 10 images; this should have somewhat surprised you. Typically, when you talk about training deep neural networks, you expect the dataset to be large, very large. For example, a standard dataset used for training image classifiers is ImageNet. This dataset consists of over 14 million images; and this is part of the secret. As we've discussed throughout this book, layers of a CNN learn how to extract meaningful features from images, which they then use to infer an image's class. A common practice for specialized classifiers, like our fruit classifier, is to borrow these learnings from a model that has trained on millions of images and use the features it extracts to train a classifier on our smaller dataset—a technique known as <strong>transfer learning</strong>.</p>
<p>The following two diagrams provide an illustrative example of this, with the first showing a network that has been trained on a large dataset and the second using what it has learnt to train on a more specialized dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a7895715-bc54-4eb0-9088-3f8d269a4548.png"/></div>
<p>We are interested in the feature vectors that the convolutional layers learn; you can think of this as an encoding of its understanding of the input image. This is what we want to use to train our own classifier, shown in the following diagrams:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9486a355-fcc1-4043-ae53-a26ba3451891.png"/></div>
<p>With this approach, we forgo having to learn how to extract features and are left with just having to train the weights of a fully connected network for classification, taking advantage of the previous network's ability to extract meaningful features. Create ML uses this technique for its image classifiers. Using a pre-trained model that resides on the device and has been trained over 1,000 categories means that we are left just having to train a relatively small network for classification. This is done using the features provided by the pre-trained network. This not only allows us to learn from a smaller dataset but also reduces the amount of time required for training.  </p>
<p>Another feature Create ML offers, and performs on our behalf, to train effectively on small datasets is something called data augmentation.<strong> </strong>Data augmentation is simply a way of increasing the variance of our dataset by applying a number of random transformations to each image before the image is passed into the network during training, for example, horizontally flipping an image. The goal is that at training time, your model will see many variations of an image so as to improve your model's ability to generalize, that is, learn meaningful features that work on data it hasn't seen before. The following figure illustrates some of the transformations typically performed for data augmentation:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/1ea43127-3807-4955-8920-6c37ad861901.png"/></div>
<p>Another convenience offered by Create ML out of the box is that it handles the typical preprocessing tasks required when working with images, such as cropping and resizing. They typically have fixed-size inputs and outputs, requiring you to either explicitly preprocess the images to match the model or use the Vision framework to handle this for you. An extra consequence of Create ML being built on top of Vision is that it handles a lot of the pipeline you <span>would </span>typically need to do manually when training models. </p>
<p>There is just one more important topic I would like to highlight before moving on to creating and training our model; this has to do with balanced datasets, or the effects of imbalanced datasets. Balanced datasets refer to having an equal amount of examples for each class; that is, you avoid having a large variance between the number of examples you have in each of your classes. Why is this important? To answer this, let's remind ourselves of how a model is trained and what it learns. The <span>following </span>figure illustrates the process of training, where training is an iterative process of performing inference (forward pass) for a given input. Then, small adjustments are made to the weights of the model so that they reduce any discrepancies between the prediction and expected value (loss):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/95712ee3-0d50-4c57-88a0-d8298e4b1a79.png"/></div>
<p>Put another way, overexposing a class will dominate this process of adjusting weights such that the weights will better fit their own class over others. This is especially true when training with batches, as the error is typically the average over all samples in the batch. So, if your model can effectively predict the dominant class, it's likely to achieve a reasonable loss and be unable to learn anything useful for the other classes.</p>
<p>At this point, we know what we are trying to achieve, have our balanced training set, and know what machine learning task we need; we are now ready to build and train our model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and training a model</h1>
                </header>
            
            <article>
                
<p>Thanks to the great effort by Apple's engineers, the process of creating common machine learning models is incredibly easy and <span>will</span><span> </span><span>no doubt spark a new wave of intelligent apps over the coming months.</span></p>
<p>In this section, you will see just how easy it is as we walk through creating an image classifier for our application using Create ML.</p>
<p>Create ML is accessible using Xcode Playground, so there is a good place to start. Open up Xcode and create a new Playground, ensuring that you select <span class="packt_screen">macOS</span> as the platform, as shown here: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e0fa97a3-d516-4921-93ad-e7a2ce12dd30.png" style="width:42.83em;height:31.00em;"/></div>
<p>Once in the playground, import <kbd>CreateML</kbd> and <kbd>Foundation</kbd> as follows: </p>
<pre>import CreateML<br/>import Foundation</pre>
<p>Next, create a <kbd>URL</kbd> that points to the directory that contains your training data:</p>
<pre>let trainingDir = URL(fileURLWithPath: "<strong>/&lt;PATH TO DIRECTORY WITH TRAINING DATA&gt;</strong>")</pre>
<p>The only thing left to do is to create an instance of our model, passing in the path to our training data (I did say it was incredibly easy):</p>
<pre>let model = try MLImageClassifier(<br/>    trainingData: .labeledDirectories(at: trainingDir))</pre>
<p>Create ML offers you the flexibility of providing a custom dictionary of labels and their associated files or through the convenience of a <kbd>MLImageClassifier.DataSource</kbd>. This can either be a hierarchical directory structure where classes are organized into their respective folders, <kbd>MLImageClassifier.DataSource.labeledDirectories</kbd> (as we have done in this example), or one where each file has been named with respect to their associated class, <kbd>MLImageClassifier.DataSource.labeledFiles</kbd>. </p>
<p>As soon as the model is instantiated, it will begin training. Once finished, it will output the accuracy achieved on your training set to the console, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/5f7c863b-0f7e-4108-8e93-96b0fb99fce8.png"/></div>
<p>We are almost done; this tells us that our model has fit our training data well, but it doesn't tell us how well it will generalize, that is, how well it will work on images it hasn't seen before. It's possible (and common) for deep neural networks to remember their training data, commonly referred to as overfitting. To avoid overfitting, and therefore make it more likely to produce something usable in the real world, it's a common practice to split your data into three buckets. The first bucket is used to train your model. The second bucket, called validation data, is used during training (typically at the end of each iteration/epoch) to see how well the model is generalizing. It also provides clues as to when the model starts overfitting (when the training accuracy and validation accuracy begin to diverge). The last bucket is only used once you are satisfied with how your model performs on the validation data and is the determinant of how well your model actually works; this bucket is known as the test data. </p>
<div class="packt_infobox">How much data do you reserve for validation and testing? For shallow learners, it was common to have a 70/20/10 <span>(training, validation, and test) </span>split. But deep learning normally implies big datasets, in which case the reserved data for validation and test may be excessive. So the answer really depends on how much data you have and what type of data it is. </div>
<p>Therefore, before deploying our model, we evaluate it on a dataset it hasn't seen during training. Once again, collect an equal amount of data for each of your classes and return here once you've done so.</p>
<p>As we had done before, create a URL that points to the directory that contains your validation data:</p>
<pre>let validationDir = URL(fileURLWithPath: <strong>"/&lt;PATH TO DIRECTORY WITH VALIDATION DATA&gt;"</strong>)</pre>
<p>Now it's simply a matter of calling <kbd>evaluation</kbd> on the model, as shown here:</p>
<pre>model.evaluation(on: .labeledDirectories(at: validationDir))</pre>
<p>This will perform inference on each of our validation samples and report the accuracy, which you can access via quick looks:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/23aa4d83-4680-49a7-9b94-9a7ba54db124.png"/></div>
<p>Satisfied with our validation accuracy, we are now ready to export our model, but just before we do so, let's perform a prediction on an individual image.</p>
<p>You can easily do this by calling the <kbd>prediction</kbd> method of your model instance (or <kbd>predictions</kbd> if you have multiple samples you want to perform inference on), as shown in this snippet:</p>
<pre>let strawberryUrl = URL(<br/>    fileURLWithPath: "/<strong>&lt;PATH TO STRAWBERRY&gt;</strong>")<br/><br/>print(try model.prediction(from: strawberryUrl)) </pre>
<p>If all goes well, then <kbd>Strawberry</kbd> should be output to your console. Now, feeling confident with our model, it's time to export it.</p>
<p>In k<span>eeping </span><span>with the </span>nature of Create ML, exporting is simply a single line of code: </p>
<pre>try model.write(toFile: "&lt;PATH TO FILE&gt;")</pre>
<p>From here, it's just a matter of importing the Core ML model into your project, as we have seen many times throughout this book. </p>
<p>We have almost concluded our brief introduction to Create ML; but before we move on, I want to quickly highlight a few things, starting with model parameters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model parameters</h1>
                </header>
            
            <article>
                
<p>In the previous section, I mentioned the usefulness of data augmentation for small datasets. So, how do you use this during your training? The options are exposed to you using the  <kbd>MLImageClassifier.ModelParameters</kbd> structure, which you can pass an instance of when instantiating the classifier. One of the parameters is the <kbd>OptionSet</kbd>  <kbd>CreateML.MLImageClassifier.ImageAugmentationOptions</kbd>, which allows you to toggle various augmentation techniques on and off.</p>
<p><kbd>MLImageClassifier.ModelParameters</kbd> also allows you to specify the maximum number of iterations, version of the feature extraction, and validation data. You can learn more about these on the official web page at <a href="https://developer.apple.com/documentation/create_ml/mlimageclassifier/modelparameters">https://developer.apple.com/documentation/create_ml/mlimageclassifier/modelparameters</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model metadata</h1>
                </header>
            
            <article>
                
<p>When working with the Core ML Tools package in <a href="a89287b3-5c90-4f77-801f-371f7a8f2d36.xhtml" target="_blank">Chapters 5</a>, <em>Emotion Detection with CNNs,</em> and <a href="40971e0d-b260-42e1-a9fb-5c4a56b0ebb2.xhtml" target="_blank">Chapter 6</a>, <em>Creating Art with Style Transfer,</em> to convert a Keras model to Core ML, we saw how we could explicitly set the metadata, which is shown in Xcode. Create ML provides a way of explicitly setting this data by passing in an instance of <kbd>MLModelMetadata</kbd> when exporting the model. It provides you all the metadata we had seen when working with the Core ML Tools package, such as name, description and so on. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alternative workflow (graphical) </h1>
                </header>
            
            <article>
                
<p>The last point before moving on to the next section! In this chapter, we have walked through programmatically creating, training, and validating a model. Create ML offers an alternative, where, instead of using code to build your model, you can use a graphical interface. This is accessible via the <kbd>CreateMLUI</kbd> library, where you simply create an instance of <kbd>MLImageClassifierBuilder</kbd> and call its <kbd>showInLiveView</kbd> method:  </p>
<pre>import CreateMLUI<br/><br/>let builder = MLImageClassifierBuilder()<br/>builder.showInLiveView()</pre>
<p>Once this runs, you will see a widget in the live view, which allows you to train the model simply by dragging and dropping in your training and validation examples. The following figure shows this widget after training and validation, and the panel for entering metadata:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c03a3709-578e-48fc-8b41-cb4f7284b09f.png" style="width:42.25em;height:24.75em;"/></div>
<p>This concludes this section, the chapter, and the book. We will wrap up with some closing thoughts, including a list of some other tools to help you on your journey to creating more intelligent apps. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Closing thoughts</h1>
                </header>
            
            <article>
                
<p>This tool essentially democratizes machine learning by way of allowing anyone (who is able) to create custom models, but there is always a trade-off between simplicity and expressiveness. So, here is a short list of tools you may want to explore:</p>
<ul>
<li><strong>Turi create</strong>: comes from a firm acquired by Apple in 2016; it provides tight integration with Core ML, allowing for easy deployment and custom models. It also provides a more comprehensive suite of machine learning models such as Style Transfer and segmentation. You can learn more about Turi create here: <a href="https://github.com/apple/turicreate">https://github.com/apple/turicreate</a>.</li>
<li><strong>IBM Watson Services for Core ML</strong>: IBM Watson is IBM's AI platform, exposing an array of common machine learning models as a service. They have recently made available some of these services via Core ML models, allowing your application to leverage IBM Watson's services even when offline. </li>
<li><strong>ML Kit</strong>: Google announced an ML Kit in early 2018 as a platform for common machine learning tasks such as image labeling and optical character recognition. The platform also takes care of model distribution, including custom ones.</li>
<li><strong>TensorFlowLite</strong>: A lightweight version of the popular machine learning framework TensorFlow. Like Core ML, it enables on-device inference.</li>
</ul>
<p>These are only a few of the options available to integrate machine learning into your application, and all this is likely to grow significantly over the coming years. But, as we have seen throughout this book, the machine learning algorithm is <span>(literally) </span>only one part of the equation; data is what drives the experience, so I encourage you to seek out and experiment with new datasets to see what unique experiences you can come up with using what you have learnt here. </p>
<p>Machine learning is evolving at an incredible pace. The website Arxiv is a popular repository for researchers to publish their papers; by just monitoring this site for over a week, you will be amazed and excited by the volume of papers being published and the advancements being made. </p>
<p>But, right now, there is a gap between the research community and industry practitioners, which in part motivated me to write this book. I hope that what you have read in the pages of this book has given you enough intuition behind deep neural networks and, more importantly, sparked enough curiosity and excitement for you to continue exploring and experimenting. As I mentioned at the start of this chapter, we have just scratched the surface of what is currently out and possible, never mind what will be around in 12 months.</p>
<p>So, consider this as an invite or challenge to join me in creating the next generation of applications. I look forward to seeing what you create!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we introduced Create ML, a tool that makes it incredibly easy to train and deploy common machine learning models. We saw how easy it is to create an image classifier using a minimal amount of examples and minimal amount of code. We discussed how this was achieved through the use of transfer learning, and then covered some considerations to keep in mind with regard to your training data and the importance of splitting it for validation and testing.</span></p>


            </article>

            
        </section>
    </body></html>