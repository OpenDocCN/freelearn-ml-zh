<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using ONNX with ML.NET</h1>
                </header>
            
            <article>
                
<p>Now that we have completed our deep dive into using TensorFlow with a <strong>Windows Presentation Foundation</strong> (<strong>WPF</strong>) application and ML.NET, it is now time to dive into using <strong>Open Neural Network eXchange</strong><span> (</span><strong>ONNX</strong>) with ML.NET. Specifically, in this final chapter, we will review what ONNX is, in addition to creating a new example application with a pre-trained ONNX model called <strong>YOLO</strong>. This application will build on the previous chapter and show the bounding boxes of the objects that the model detects. In addition, we will close out the chapter with suggestions on improving the example, for it to either become a production-grade application or be integrated into a production application.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Breaking down ONNX and YOLO</li>
<li>Creating the ONNX object detection application</li>
<li>Exploring additional production application enhancements</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking down ONNX and YOLO</h1>
                </header>
            
            <article>
                
<p>As mentioned in <a href="b8d873e1-9234-4f11-ad94-76df5ffbb228.xhtml">Chapter 1</a>, <em>Getting Started with Machine Learning and ML.NET,</em> the ONNX standard is widely regarded within the industry as a truly universal format across machine learning frameworks. In the next two sections, we will review what ONNX provides, in addition to the YOLO model that will drive our example in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing ONNX</h1>
                </header>
            
            <article>
                
<p><strong>ONNX</strong> <span>was created as a way for a less locked-down and free-flowing process when working with either pre-trained models or training models across frameworks. By providing an open format for frameworks to export to, ONNX allows interoperability, and thereby promotes experimentation that would have otherwise been prohibitive due to the nature of proprietary formats being used in almost every framework.</span></p>
<p>Currently, supported frameworks include TensorFlow, XGBoost, and PyTorch—in addition to ML.NET, of course.</p>
<div class="packt_infobox">If you want to deep dive into ONNX further, please check out their website: <a href="https://onnx.ai/index.html">https://onnx.ai/index.h</a><a href="https://onnx.ai/index.html">tml</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The YOLO ONNX model</h1>
                </header>
            
            <article>
                
<p>Building on the work that was performed in <a href="049e90c4-05b0-466d-af93-d56df861a843.xhtml">Chapter 12</a>, <em>TensorFlow with ML.NET</em>, in which we used the pre-trained Inception model, in this chapter, we are going to use the pre-trained YOLO model. This model provides very fast and accurate object detection, meaning it can find multiple objects within an image with a certain level of confidence. This differs from the last chapter's model that provided a pure image classification, such as water or food.</p>
<p class="CDPAlignLeft CDPAlign">To help visualize the difference between the two models, take the previous chapter's TensorFlow model that classified water, and compare that to this chapter's object detection of a car, as illustrated in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-594 image-border" src="assets/d1ab6cee-dd3b-4f28-88ee-079e9a8547ff.png" style="width:22.83em;height:9.08em;"/></p>
<p>Object detection within images (and video) has been increasing in demand due to the significantly increased amount of images on the internet and the need for security. Imagine a crowded environment such as a football stadium, in particular by the front gates. Security guards patrol and monitor this area; however, like you, they are only human and can only glance at so many people with a certain level of accuracy. Applying object detection with machine learning in real time to pick up on weapons or large bags could then be used to alert the security guards to go after a suspect.</p>
<p>The YOLO model itself comes in two main forms—a tiny and a full model. For the scope of this example, we will be using the smaller of the models (~60 MB) that can classify 20 objects found within an image. The tiny model is comprised of nine convolutional layers and six max-pooling layers. The full model can classify thousands of objects and, given the proper hardware (namely, <strong>graphics processing units</strong> (<strong>GPUs</strong>)), can run faster than real-time.</p>
<p>The following diagram depicts how the YOLO model works (and neural networks, to a degree):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-595 image-border" src="assets/72db35ce-7a4d-42ef-a52b-8b60d333a7a8.png" style="width:51.17em;height:21.42em;"/></p>
<p>Effectively, the image (or images) is converted to 3 x 416 x 416 images. The 3 component represents the <strong>Red-Green-Blue</strong> (<strong>RGB</strong>) values. The 416 values represent the width and height of the resized image. This input layer is then inputted into the hidden layers of the model. For the Tiny YOLO v2 model that we are using in this chapter, there are a total of 15 layers before outputting the layer.</p>
<div class="packt_tip">To deep dive further into the YOLO model, please read this paper: <a href="https://arxiv.org/pdf/1612.08242.pdf">https://arxiv.org/pdf/1612.08242.pdf</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the ONNX object detection application</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, the application we will be creating is an object detection application using a pre-trained ONNX model. Using the <span>application we developed in </span><a href="049e90c4-05b0-466d-af93-d56df861a843.xhtml">Chapter 12</a>, <em>Using TensorFlow with ML.NET</em> as a starting point, we will add in support for bounding boxes overlaid on top of the image when the model categorizes objects of which it is aware. The usefulness of this to the general public is in the various applications image object detection provides. Imagine that you are working on a project for the police or intelligence community, where they have images or videos and want to detect weapons. Utilizing the YOLO model with ML.NET, as we are going to show, would make that process very easy.</p>
<p>As with previous chapters, the completed project code, pre-trained model, and project files can be downloaded here: <a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-With-ML.NET/tree/master/chapter13">https://github.com/PacktPublishing/Hands-On-Machine-Learning-With-ML.NET/tree/master/chapter13</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the project architecture</h1>
                </header>
            
            <article>
                
<p>Building on the project architecture and code we created in previous chapters, the architecture we will be reviewing is enhanced to be more structured and usable by an end user.</p>
<p>As in some of the previous chapters, the following two additional NuGet packages are required if you want to utilize an ONNX model and perform object detection: </p>
<ul>
<li><kbd>Microsoft.ML.ImageAnalytics</kbd></li>
<li><kbd>Microsoft.ML.OnnxTransformer</kbd></li>
</ul>
<p>These NuGet packages are already referenced in the included sample code. Version 1.3.1 of these packages is used in both the included example on GitHub and throughout this chapter's deep dive.</p>
<p>In the following screenshot, you will find the Visual Studio Solution Explorer view of the project. There are several new additions to the solution, to facilitate the production use case we are targeting. We will review in detail each of the new files in the following solution screenshot later on in this chapter:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-596 image-border" src="assets/bda2449a-5aa4-4ac5-acf2-adb328a47f44.png" style="width:19.67em;height:32.58em;"/></p>
<p>Due to a current ML.NET limitation as of this writing, ONNX support is only provided for scoring using a pre-existing model. The pre-trained model included in this example can be found in the <kbd>assets/model</kbd> folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Diving into the code</h1>
                </header>
            
            <article>
                
<p>For this application, as noted in the previous section, we are building on top of the work completed in <a href="049e90c4-05b0-466d-af93-d56df861a843.xhtml">Chapter 12</a>, <em>Using TensorFlow with ML.NET</em>. While the <strong>user interface</strong> (<strong>UI</strong>) has not changed much, the underlying code to run an ONNX model has. For each file changed—as in previous chapters—we will review the changes made and the reasoning behind the changes.</p>
<p>Classes that were changed or added are as follows:</p>
<ul>
<li><kbd>DimensionsBase</kbd></li>
<li><kbd>BoundingBoxDimensions</kbd><span> </span></li>
<li><kbd>YoloBoundingBox</kbd></li>
<li><kbd>MainWindow.xaml</kbd></li>
<li><kbd>ImageClassificationPredictor</kbd></li>
<li><kbd>MainWindowViewModel</kbd></li>
</ul>
<p>There is one additional file, with the <kbd>YoloOutputParser</kbd> class contained within. This class is derived from the <strong>Massachusetts Institute of Technology</strong> (<strong>MIT</strong>) licensed interface for the <kbd>TinyYOLO</kbd> ONNX model. Due to the length of this class, we will not review it; however, the code does read easily, and if you wish to step through a prediction, the flow will be easy to follow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The DimensionsBase class</h1>
                </header>
            
            <article>
                
<p>The <kbd>DimensionsBase</kbd> class contains the coordinates along with the <kbd>Height</kbd> and <kbd>Width</kbd> properties, as illustrated in the following code block:</p>
<pre>public class DimensionsBase<br/>{<br/>    public float X { get; set; }<br/><br/>    public float Y { get; set; }<br/>    <br/>    public float Height { get; set; }<br/>    <br/>    public float Width { get; set; }<br/>}</pre>
<p>This base class is used by both the <kbd>YoloOutputParser</kbd> and <kbd>BoundingBoxDimensions</kbd> classes to reduce code duplication.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The YoloBoundingBox class</h1>
                </header>
            
            <article>
                
<p>The <kbd>YoloBoundingBox</kbd> class provides the container class for what is used to populate our bounding boxes when generating them for the overlay, as illustrated in the following code block:</p>
<pre>public class YoloBoundingBox<br/>{<br/>    public BoundingBoxDimensions Dimensions { get; set; }<br/><br/>    public string Label { get; set; }<br/><br/>    public float Confidence { get; set; }<br/><br/>    public RectangleF Rect =&gt; new RectangleF(Dimensions.X, Dimensions.Y, Dimensions.Width, Dimensions.Height);<br/><br/>    public Color BoxColor { get; set; }<br/>}</pre>
<p>In addition, also defined in this same class file is our <kbd>BoundingBoxDimensions</kbd> class, as shown in the following code block:</p>
<pre>public class BoundingBoxDimensions : DimensionsBase { }</pre>
<p>Again, this inheritance is used to reduce code duplication.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MainWindow.xaml file</h1>
                </header>
            
            <article>
                
<p>The <strong>Extensible Application Markup Language</strong> (<strong>XAML</strong>) view of our application has been simplified to just the button and the image controls, as illustrated in the following code block:</p>
<pre>&lt;Grid&gt;<br/>    &lt;Grid.RowDefinitions&gt;<br/>        &lt;RowDefinition Height="Auto" /&gt;<br/>        &lt;RowDefinition Height="*" /&gt;<br/>    &lt;/Grid.RowDefinitions&gt;<br/><br/>    &lt;Button Grid.Row="0" Margin="0,10,0,0" Width="200" Height="35" Content="Select Image File" HorizontalAlignment="Center" Click="btnSelectFile_Click" /&gt;<br/><br/>    &lt;Image Grid.Row="1" Margin="10,10,10,10" Source="{Binding SelectedImageSource}" /&gt;<br/>&lt;/Grid&gt;</pre>
<p>In addition, due to the nature of the bounding boxes and images you may select, the window has defaulted to <kbd>Maximized</kbd>, as can be seen in the following code block:</p>
<pre>&lt;Window x:Class="chapter13.wpf.MainWindow"<br/>        <br/>        <br/>        <br/>        <br/>        <br/>        mc:Ignorable="d"<br/>        ResizeMode="NoResize"<br/>        WindowStyle="SingleBorderWindow"<br/>        WindowState="Maximized"<br/>        WindowStartupLocation="CenterScreen"<br/>        Background="#1e1e1e"<br/>        Title="Chapter 13" Height="450" Width="800"&gt;</pre>
<p><span><span>With the XAML changes behind us, let us now dive into the revised <kbd>ImageClassificationPredictor</kbd> class.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ImageClassificationPredictor class</h1>
                </header>
            
            <article>
                
<p>The <kbd>ImageClassificationPredictor</kbd> class, much like that of <a href="049e90c4-05b0-466d-af93-d56df861a843.xhtml">Chapter 12</a>, <em>Using TensorFlow with ML.NET</em>, contains the methods to run our image prediction. In this chapter, we will need to make several additional class objects to support the running of an ONNX model, as follows:</p>
<ol>
<li>First, we define the <kbd>ImageNetSettings</kbd> struct that defines the height and width of our network. The YOLO model requires the use of 416 pixels by 416 pixels, as illustrated in the following code block:</li>
</ol>
<pre style="padding-left: 60px">public struct ImageNetSettings<br/>{<br/>    public const int imageHeight = 416;<br/>    public const int imageWidth = 416;<br/>}   </pre>
<ol start="2">
<li>Next, we define the <kbd>TinyYoloModelSettings</kbd> struct to be used with the ONNX model, as follows:</li>
</ol>
<pre style="padding-left: 60px">public struct TinyYoloModelSettings<br/>{<br/>    public const string ModelInput = "image";<br/><br/>    public const string ModelOutput = "grid";<br/>}</pre>
<ol start="3">
<li>Unlike the previous chapter, where the TensorFlow model was imported and then exported as an ML.NET model on the first run, ONNX, as of this writing, does not support that path. So, we must load the ONNX model in the <kbd>Initialize</kbd> method every time, as illustrated in the following code block:</li>
</ol>
<pre style="padding-left: 60px">public (bool Success, string Exception) Initialize()<br/>{<br/>    try<br/>    {<br/>        if (File.Exists(ML_NET_MODEL))<br/>        {<br/>            var data = MlContext.Data.LoadFromEnumerable(new List&lt;ImageDataInputItem&gt;());<br/><br/>            var pipeline = MlContext.Transforms.LoadImages(outputColumnName: "image", imageFolder: "", <br/>                    inputColumnName: nameof(ImageDataInputItem.ImagePath))<br/>                .Append(MlContext.Transforms.ResizeImages(outputColumnName: "image", <br/>                    imageWidth: ImageNetSettings.imageWidth, <br/>                    imageHeight: ImageNetSettings.imageHeight, <br/>                    inputColumnName: "image"))<br/>                .Append(MlContext.Transforms.ExtractPixels(outputColumnName: "image"))<br/>                .Append(MlContext.Transforms.ApplyOnnxModel(modelFile: ML_NET_MODEL, <br/>                    outputColumnNames: new[] { TinyYoloModelSettings.ModelOutput }, <br/>                    inputColumnNames: new[] { TinyYoloModelSettings.ModelInput }));<br/><br/>            _model = pipeline.Fit(data);<br/><br/>            return (true, string.Empty);<br/>        }<br/><br/>        return (false, string.Empty);<br/>    }<br/>    catch (Exception ex)<br/>    {<br/>        return (false, ex.ToString());<br/>    }<br/>}</pre>
<ol start="4">
<li>Next, we modify the <kbd>Predict</kbd> method extensively to support the <kbd>YoloParser</kbd> call, calling the <kbd>DrawBoundingBox</kbd> method to overlay the bounding boxes, and then returning the bytes of the updated image, as follows:</li>
</ol>
<pre style="padding-left: 60px">public byte[] Predict(string fileName)<br/>{<br/>    var imageDataView = MlContext.Data.LoadFromEnumerable(new List&lt;ImageDataInputItem&gt; { new ImageDataInputItem { ImagePath = fileName } });<br/><br/>    var scoredData = _model.Transform(imageDataView);<br/><br/>    var probabilities = scoredData.GetColumn&lt;float[]&gt;(TinyYoloModelSettings.ModelOutput);<br/><br/>    var parser = new YoloOutputParser();<br/><br/>    var boundingBoxes =<br/>        probabilities<br/>            .Select(probability =&gt; parser.ParseOutputs(probability))<br/>            .Select(boxes =&gt; parser.FilterBoundingBoxes(boxes, 5, .5F));<br/><br/>    return DrawBoundingBox(fileName, boundingBoxes.FirstOrDefault());<br/>}</pre>
<p>For brevity, the <kbd>DrawBoundingBox</kbd> method is not shown here. At a high level, the original image is loaded into memory, and the model's bounding boxes are then drawn on top of the image, along with the label and confidence. This updated image is then converted to a byte array and returned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MainWindowViewModel class</h1>
                </header>
            
            <article>
                
<p>Inside the <kbd>MainWindowViewModel</kbd> class, there are a couple of changes to be made due to the nature of the example. We look at them here:</p>
<ol>
<li>First, the <kbd>LoadImageBytes</kbd> method now simply takes the parsed image bytes and converts them to an <kbd>Image</kbd> object, like this:</li>
</ol>
<pre style="padding-left: 60px">private void LoadImageBytes(byte[] parsedImageBytes)<br/>{<br/>    var image = new BitmapImage();<br/><br/>    using (var mem = new MemoryStream(parsedImageBytes))<br/>    {<br/>        mem.Position = 0;<br/><br/>        image.BeginInit();<br/>        <br/>        image.CreateOptions = BitmapCreateOptions.PreservePixelFormat;<br/>        image.CacheOption = BitmapCacheOption.OnLoad;<br/>        image.UriSource = null;<br/>        image.StreamSource = mem;<br/>        <br/>        image.EndInit();<br/>    }<br/><br/>    image.Freeze();<br/><br/>    SelectedImageSource = image;<br/>}</pre>
<ol start="2">
<li>Lastly, we modify the <kbd>Classify</kbd> method to call the <kbd>LoadImageBytes</kbd> method upon successfully running the model, as follows:</li>
</ol>
<pre style="padding-left: 60px">public void Classify(string imagePath)<br/>{<br/>    var result = _prediction.Predict(imagePath);<br/><br/>    LoadImageBytes(result);<br/>}</pre>
<p>With the changes in place for the <kbd>Classify</kbd> method, that concludes the code changes required for this chapter's example. Now, let us run the application!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the application</h1>
                </header>
            
            <article>
                
<p>To run the application, the process is identical to the sample application in <a href="049e90c4-05b0-466d-af93-d56df861a843.xhtml">Chapter 12</a>, <em>Using TensorFlow with ML.NET</em>. To run the application from within Visual Studio, simply click the <em>play</em> icon found in the toolbar, as illustrated in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-597 image-border" src="assets/5fe7e40a-4da8-4dc1-8700-6fd1782f74bd.png" style="width:22.58em;height:2.33em;"/></p>
<p>After launching the application, just as in <a href="049e90c4-05b0-466d-af93-d56df861a843.xhtml">Chapter 12</a>, <em>Using TensorFlow with ML.NET</em>, select an image file, and the model will run. For example, I selected an image I took on a vacation to Germany (note the car's bounding boxes), shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-598 image-border" src="assets/be4ac682-f556-4270-a7e6-6dbf82838d48.png" style="width:69.42em;height:33.83em;"/></p>
<p>Feel free to try selecting images you have on your hard drive to see the confidence level of the detection and how well the bounding boxes are formed around the objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring additional production application enhancements</h1>
                </header>
            
            <article>
                
<p>Now that we have completed our deep dive, there are a couple of additional elements to further enhance the application. A few ideas are discussed in the upcoming sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logging</h1>
                </header>
            
            <article>
                
<p>As noted previously, the importance of logging cannot be stressed enough within desktop applications. Logging utilizing NLog (<a href="https://nlog-project.org/">https://nlog-project.org/</a>) or a similar open-source project is highly recommended as your application complexity increases. This will allow you to log to a file, console, or third-party logging solution such as Loggly, at varying levels. For instance, if you deploy this application to a customer, breaking down the error level to at least Debug, Warning, and Error will be helpful when debugging issues remotely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image scaling</h1>
                </header>
            
            <article>
                
<p>As you might have noticed, with images that are quite large (those exceeding your screen resolution), the text labeling of the bounding boxes and resizing within the image preview is not as easy to read as for, say, a 640 x 480 image. One area of improvement here would be to provide hover-over capabilities, resizing the image to the dimensions of the window or increasing the font size dynamically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Utilizing the full YOLO model</h1>
                </header>
            
            <article>
                
<p>In addition, another area of improvement for this sample would be to use the full YOLO model within an application. As previously noted with the Tiny YOLO model used within the example application, only 20 labels are provided. In a production application or one in which you wish to build on, using the larger, more complex model would be a good choice.</p>
<div class="packt_infobox">You can download the full YOLO model here: <a href="https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/yolov3">https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/yolov3</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Over the course of this chapter, we have deep dived into what goes into the ONNX format and what it offers to the community. In addition, we also created a brand new detection engine using the pre-trained Tiny YOLO model in ML.NET.</p>
<p>And with that, this concludes your deep dive into ML.NET. Between the first page of this book and this one, you have hopefully grown to understand the power that ML.NET offers in a very straightforward feature-rich abstraction. With ML.NET constantly evolving (much like .NET), there will be no doubt about the evolution of ML.NET's feature sets and deployment targets, ranging from embedded <strong>Internet of Things</strong> (<strong>IoT</strong>) devices to mobile devices. I hope this book was beneficial for your deep dive into ML.NET and machine learning. In addition, I hope that as you approach problems in the future, you will <span>first </span>think about whether the problem would benefit from utilizing ML.NET to solve the problem more efficiently and, potentially, better overall. Given the world's data continually growing at exponential rates, the necessity for using non-brute-force/traditional approaches will only continue to grow, therefore the skills garnered from this book should help you for years to come.</p>


            </article>

            
        </section>
    </body></html>