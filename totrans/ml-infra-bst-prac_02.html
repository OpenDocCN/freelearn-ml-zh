<html><head></head><body>
		<div id="_idContainer017">
			<h1 class="chapter-number"><a id="_idTextAnchor014"/>1</h1>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor015"/>Machine Learning Compared to Traditional Software</h1>
			<p>Machine learning software is a<a id="_idIndexMarker000"/> special kind of software that finds patterns in data, learns from them, and even recreates these patterns on new data. Developing the machine learning software is, therefore, focused on finding the right data, matching it with the appropriate algorithm, and evaluating its performance. Traditional software, on the contrary, is developed with the algorithm in mind. Based on software requirements, programmers develop algorithms that solve specific tasks and then test them. Data is secondary, although not completely unimportant. Both types of software can co-exist in the same software system, but the programmer must ensure compatibility <span class="No-Break">between them.</span></p>
			<p>In this chapter, we’ll explore where these two types of software systems are most appropriate. We’ll learn about the software development processes that programmers use to create both types of software. We’ll also learn about the four classical types of machine learning software – rule-based learning, supervised learning, unsupervised learning, and reinforcement learning. Finally, we’ll learn about the different roles of data in traditional and machine learning software – as input to pre-programmed algorithms in traditional software and input to training models in machine <span class="No-Break">learning software.</span></p>
			<p>The best practices introduced in this chapter provide practical guidance on when to choose each type of software and how to assess the advantages and disadvantages of these types. By exploring a few modern examples, we’ll understand how to create an entire software system with machine learning algorithms at <span class="No-Break">the center.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Machine learning is not a <span class="No-Break">traditional software</span></li>
				<li>Probability and software – how well do they <span class="No-Break">go together?</span></li>
				<li>Testing and validation – the same <span class="No-Break">but different</span></li>
			</ul>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>Machine learning is not traditional software</h1>
			<p>Although machine learning and artificial intelligence have been around since the 1950s, introduced by Alan Turing, they only became popular with the first MYCIN system and our understanding of machine learning systems changed over time. It was not until the 2010s that we started to perceive, design, and<a id="_idIndexMarker001"/> develop machine learning in the same way as we do today (in 2023). In my view, two pivotal moments shaped the landscape of machine learning as we see <span class="No-Break">it today.</span></p>
			<p>The first pivotal moment was the focus on big data in the late 2000s and early 2010s. With the introduction of smartphones, companies started to collect and process increasingly large quantities of data, mostly about our behavior online. One of the companies that perfected this was Google, which collected data about our searches, online behavior, and usage of Google’s operating system, Android. As the volume of the collected data increased (and its speed/velocity), so did its value and the need for its veracity – the five Vs. These five Vs – volume, velocity, value, veracity, and variety – required a new approach to working with data. The classical approach of relational databases (SQL) was no longer sufficient. Relational databases became too slow in handling high-velocity data streams, which gave way to map-reduce algorithms, distributed databases, and in-memory databases. The classical approach of relational schemas became too constraining for the variety of data, which gave way for non-SQL databases, which <span class="No-Break">stored documents.</span></p>
			<p>The second pivotal moment was the rise of modern machine learning algorithms – deep learning. Deep learning algorithms are designed to handle unstructured data such as text, images, or music (compared to structured data in the form of tables and matrices). Classical machine learning algorithms, such as regression, decision trees, or random forest, require data in a tabular form. Each row is a data point, and each column is one characteristic of it – a feature. The classical models are designed to handle relatively small datasets. Deep learning algorithms, on the other hand, can handle large datasets and find more complex patterns in the data because of the power of large neural networks and their <span class="No-Break">complex architectures.</span></p>
			<p>Machine learning is <a id="_idIndexMarker002"/>sometimes called <em class="italic">statistical learning</em> as it is based on statistical methods. The statistical methods calculate properties of data (such as mean values, standard deviations, and coefficients) and thus find patterns in the data. The core characteristic of machine learning is that it uses data to find patterns, learn from them, and then repeat these patterns on new data. We call this way of learning patterns training, and repeating these patterns as reasoning, or in machine learning language, <em class="italic">predicting</em>. The main benefits of using machine learning software come from the fact that we do not need to design the algorithms – we focus on the problem to be solved and the data that we use to solve the problem. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em> shows an example of how such a flowchart of machine learning software can <span class="No-Break">be realized.</span></p>
			<p>First, we import a generic machine learning model from a library. This generic model has all elements that are specific to it, but it is not trained to solve any tasks. An example of such a model is a decision tree <a id="_idIndexMarker003"/>model, which is designed to learn dependencies in data in the form of decisions (or data splits), which it uses later for new data. To make this model somewhat useful, we need to train it. For that, we need data, which we call the <span class="No-Break">training data.</span></p>
			<p>Second, we evaluate the trained model on new data, which we call the test data. The evaluation process uses the trained model and applies it to check whether its inferences are correct. To be precise, it checks to which degree the inferences are correct. The training data is in the same format as the test data, but the content of these datasets is different. No data point should be present <span class="No-Break">in both.</span></p>
			<p>In the third step, we use the model as part of a software system. We develop other non-machine learning components, and we connect them to the trained model. The entire software system usually consists of data procurement components, real-time validation components, data cleaning components, user interfaces, and business logic components. All these components, including the machine learning model, provide a specific functionality for the end user. Once the software system has been developed, it needs to be tested, which is where the input data comes into play. The input data is something that the end user inputs to the system, such as by filling in a form. The input data is designed in such a way that has both the input and expected output – to test whether the software system <span class="No-Break">works correctly.</span></p>
			<p>Finally, the last step is to deploy the entire system. The deployment can be very different, but most modern machine learning systems are organized into two parts – the onboard/edge algorithms for non-machine learning components and the user interface, and the offboard/cloud algorithms for machine learning inferences. Although it is possible to deploy all parts of the system on the target device (both machine learning and non-machine learning components), complex machine learning models require significant computational power for good performance and seamless user experience. The principle is simple – more data/complex data means more complex models, which means that more computational power <span class="No-Break">is needed:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer010">
					<img alt="Figure 1.1 – Typical flow of machine learning software development" src="image/B19548_01_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Typical flow of machine learning software development</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em>, one of the crucial elements of the machine learning software is the <em class="italic">model</em>, which is one of the generic <a id="_idIndexMarker004"/>machine learning models, such as a neural network, that’s been trained on specific data. Such a model is used to make predictions and inferences. In most systems, this kind of component – the model – is often prototyped and developed <span class="No-Break">in Python.</span></p>
			<p>Models are trained for different datasets and, therefore, the core characteristic of machine learning software is its dependence on that dataset. An example of such a model is a vision system, where we train a machine learning algorithm such as a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) to classify <a id="_idIndexMarker005"/>images of cats <span class="No-Break">and dogs.</span></p>
			<p>Since the models are trained on specific datasets, they perform best on similar datasets when making inferences. For example, if we train a model to recognize cats and dogs in 160 x 160-pixel grayscale images, the model can recognize cats and dogs in such images. However, the same model will <a id="_idIndexMarker006"/>perform very poorly (if at all!) if it needs to recognize cats and dogs in colorful images instead of grayscale images – the accuracy of the classification will be low (close <span class="No-Break">to 0).</span></p>
			<p>On the other hand, when we develop and design traditional software systems, we do not rely on data that much, as shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>. This figure provides an overview of a software development process for traditional, non-machine learning software. Although it is depicted as a flow, it is usually an iterative process where <em class="italic">Steps 1</em> to <em class="italic">3</em> are done in cycles, each one ending with new functionality added to <span class="No-Break">the product.</span></p>
			<p>The first step is developing the software system. This includes the development of all its components – user interface, business logic (processing), handling of data, and communication. The step does not involve much data unless the software engineer creates data for <span class="No-Break">testing purposes.</span></p>
			<p>The second step is system testing, where we use input data to validate the software system. In essence, this step is almost identical to testing machine learning software. The input data is complemented with the expected outcome data, which allows software testers to assess whether the software <span class="No-Break">works correctly.</span></p>
			<p>The third step is to deploy the software. The deployment can be done in many ways. However, if we consider traditional software that is<a id="_idIndexMarker007"/> similar in function to machine learning software, it is usually simpler. It usually does not require deployment on the cloud, just like machine <span class="No-Break">learning models:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer011">
					<img alt="Figure 1.2 - Typical flow of traditional software development" src="image/B19548_01_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 - Typical flow of traditional software development</p>
			<p>The main difference between traditional software and machine learning-based software is that we need to design, develop, and test all the <a id="_idIndexMarker008"/>elements of the traditional software. In machine learning-based software, we take an empty model, which <a id="_idIndexMarker009"/>contains all the necessary elements, and we use the data to train it. We do not need to develop the individual components of the machine learning model <span class="No-Break">from scratch.</span></p>
			<p>One of the main parts of traditional software is the <em class="italic">algorithm</em>, which is developed by software engineers from scratch, based on the requirements or user stories. The algorithm is usually written as a sequential set of steps that are implemented in a programming language. Naturally, all algorithms use data to operate on it, but they do it differently than machine learning systems. They do it based on the software engineer’s design – <em class="italic">if x, then y</em> or <span class="No-Break">something similar.</span></p>
			<p>We usually consider these traditional algorithms as deterministic, explainable, and traceable. This means that<a id="_idIndexMarker010"/> the software engineer’s design decisions are documented in the algorithm and the algorithm can be analyzed afterward. They are deterministic because they are programmed based on rules; there is no training from data or identifying patterns from data. They are explainable because they are designed by programmers and each line of the program has a predefined<a id="_idIndexMarker011"/> meaning. Finally, they are traceable as we can debug every step of <span class="No-Break">these programs.</span></p>
			<p>However, there is a drawback – the software engineer needs to thoroughly consider all corner cases and understand the problem very well. The data that the software engineer uses is only to support them in analyzing the algorithm, not <span class="No-Break">training it.</span></p>
			<p>An example of a system that can be implemented using both machine learning algorithms and traditional ones is one for reading passport information. Instead of using machine learning for image recognition, the software uses specific marks in the passport (usually the <strong class="source-inline">&lt;&lt;&lt;</strong> sequence of characters) to mark the beginning of the line or the beginning of the sequence of characters <a id="_idIndexMarker012"/>denoting a surname. These marks can be recognized quite quickly using rule-based <strong class="bold">optical character recognition</strong> (<strong class="bold">OCR</strong>) algorithms without the need for deep learning <span class="No-Break">or CNNs.</span></p>
			<p>Therefore, I would like to introduce the first <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #1</p>
			<p class="callout">Use machine learning algorithms <a id="_idIndexMarker013"/>when your problem is focused on data, not on <span class="No-Break">the algorithm.</span></p>
			<p>When selecting the right technology, we need to understand whether it is based on the classical approach, where the design of the algorithm is in focus, or whether we need to focus on handling data and finding patterns in it. It is usually beneficial to start with the <span class="No-Break">following guidelines.</span></p>
			<p>If the problem requires processing large quantities of data in raw format, use the machine learning approach. Examples of such systems are conversational bots, image recognition tools, text processing tools, or even <span class="No-Break">prediction systems.</span></p>
			<p>However, if the problem requires traceability and control, use the traditional approach. Examples of such systems are control software in cars (anti-lock braking, engine control, and so on) and <span class="No-Break">embedded systems.</span></p>
			<p>If the problem requires new data to be generated based on the existing data, a process known as <em class="italic">data manipulation</em>, use the machine<a id="_idIndexMarker014"/> learning approach. Examples of such systems are image manipulation programs (DALL-E), text generation programs, deep fake programs, and source code generation programs (<span class="No-Break">GitHub Copilot).</span></p>
			<p>If the problem requires adaptation over time and optimization, use machine learning software. Examples of such systems are power grid optimization software, non-playable character behavior components in computer games, playlist recommendation systems, and even GPS navigation systems in <span class="No-Break">modern cars.</span></p>
			<p>However, if the problem requires stability and traceability, use the traditional approach. Examples of such systems are systems to make diagnoses and recommendation systems in medicine, safety-critical systems in cars, planes, and trains, and infrastructure controlling and <span class="No-Break">monitoring systems.</span></p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor017"/>Supervised, unsupervised, and reinforcement learning – it is just the beginning</h2>
			<p>Now is a good time to mention that the field of machine learning is huge, and it is organized into three main areas – supervised learning, unsupervised learning, and reinforcement learning. Each of these areas has hundreds of different algorithms. For example, the area of supervised learning has over 1,000 algorithms, all of which can be automatically selected by meta-heuristic algorithms such <span class="No-Break">as AutoML:</span></p>
			<ul>
				<li><strong class="bold">Supervised learning</strong>: This is a group of <a id="_idIndexMarker015"/>algorithms that are trained based on annotated data. The data that’s used in these algorithms needs to have a <em class="italic">target</em> or a <em class="italic">label</em>. The label is used to tell the algorithm which pattern to look for. For example, such a label can be <em class="italic">cat</em> or <em class="italic">dog</em> for each image that the supervised learning model needs to recognize. Historically, supervised learning algorithms are the oldest ones as they come directly from statistical methods such as linear regression and multinomial regression. Modern algorithms are advanced and include methods such as deep learning neural networks, which can recognize objects in 3D images and segment them accordingly. The most advanced algorithms in this area are deep learning and multimodal models, which can process text and images at the <span class="No-Break">same time.</span><p class="list-inset">A sub-group of supervised learning algorithms is <strong class="bold">self-supervised models</strong>, which are often based on transformer<a id="_idIndexMarker016"/> architectures. These models do not require labels in the data, but they use the data itself as labels. The most prominent examples of these algorithms are translation models for natural languages and generative <a id="_idIndexMarker017"/>models for images or texts. Such algorithms are trained by masking words in the original texts and predicting them. For the generative models, these algorithms are trained by masking parts of their output to <span class="No-Break">predict it.</span></p></li>
				<li><strong class="bold">Unsupervised learning</strong>: This is a group of models that are applied to find patterns in data without any labels. These models are not trained, but they use statistical properties of the input <a id="_idIndexMarker018"/>data to find patterns. Examples of such algorithms are clustering algorithms and semantic map algorithms. The input data for these algorithms is not labeled and the goal of applying these algorithms is to find structure in the dataset according to similarities; these structures can then be used to add labels to this data. We encounter these algorithms daily when we get recommendations for products to buy, books to read, music to listen to, or films <span class="No-Break">to watch.</span></li>
				<li><strong class="bold">Reinforcement learning</strong>: This is a group of models that are applied to data to solve a particular task given a<a id="_idIndexMarker019"/> goal. For these models, we need to provide this goal in addition to the data. It is called the <em class="italic">reward function,</em> and it is an <a id="_idIndexMarker020"/>expression that defines when we achieve the goal. The model is trained based on this fitness function. Examples of such models are algorithms that play Go, Chess, or StarCraft. These algorithms are also used to solve hard programming problems (AlphaCode) or optimize <span class="No-Break">energy consumption.</span></li>
			</ul>
			<p>So, let me introduce the second <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #2</p>
			<p class="callout">Before you start developing a<a id="_idIndexMarker021"/> machine learning system, do due diligence and identify the right group of algorithms <span class="No-Break">to use.</span></p>
			<p>As each of these groups of models has different characteristics, solves different problems, and requires different data, a mistake in selecting the right algorithm can be costly. Supervised models are very good at solving problems related to predictions and classifications. The most powerful models in this area can compete with humans in selected areas – for example, GitHub Copilot can create programs that can pass as human-written. Unsupervised models are very powerful if we want to group entities and make recommendations. Finally, reinforcement learning models are the best when we want to have continuous optimization with the need to retrain models every time the data or the <span class="No-Break">environment changes.</span></p>
			<p>Although all these models are based on statistical learning, they are all components of larger systems to make them useful. Therefore, we need to understand how this probabilistic and statistical nature of machine learning goes with traditional, digital <span class="No-Break">software products.</span></p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor018"/>An example of traditional and machine learning software</h2>
			<p>To illustrate the difference between traditional software and machine learning software, let’s implement the same<a id="_idIndexMarker022"/> program using these two paradigms. We’ll implement a program that calculates a Fibonacci sequence using the traditional approach, which we have seen a million times in computer science courses. Then, we’ll implement the same program using machine learning models – or one model to be exact – that is, <span class="No-Break">logistic regression.</span></p>
			<p>The traditional implementation is presented here. It is based on one recursive function and a loop that <span class="No-Break">tests it:</span></p>
			<pre class="source-code">
# a recursive function to calculate the fibonacci number
# this is a standard solution that is used in almost all
# of computer science examples
def fibRec(n):
  if n &lt; 2:
      return n
  else:
      return fibRec(n-1) + fibRec(n-2)
# a short loop that uses the above function
for i in range(23):
  print(fibRec(i))</pre>			<p>The implementation is very simple and is based on the algorithm – in our case, the <strong class="source-inline">fibRec</strong> function. It is simplistic, but it has its limitations. The first one is its recursive implementation, which costs resources. Although it can be written as an iterative one, it still suffers from the second problem – it is focused on the calculations and not on <span class="No-Break">the data.</span></p>
			<p>Now, let’s see how the machine learning implementation is done. I’ll explain this by dividing it into two parts – data preparation and <span class="No-Break">model training/inference:</span></p>
			<pre class="source-code">
#predicting fibonacci with linear regression
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
# training data for the algorithm
# the first two columns are the numbers and the third column is the result
dfTrain = pd.DataFrame([[1, 1, 2],
                        [2, 1, 3],
                        [3, 2, 5],
                        [5, 3, 8],
                        [8, 5, 13]
])
# now, let's make some predictions
# we start the sequence as a list with the first two numbers
lstSequence = [0,1]
# we add the names of the columns to make it look better
dfTrain.columns = ['first number','second number','result']</pre>			<p>In the case of machine learning software, we prepare data to train the algorithm. In our case, this is the <strong class="source-inline">dfTrain</strong> DataFrame. It is a table that contains the numbers that the machine learning algorithm needs to find <span class="No-Break">the pattern.</span></p>
			<p>Please note that we prepared two datasets – <strong class="source-inline">dfTrain</strong>, which contains the numbers to train the algorithm, and <strong class="source-inline">lstSequence</strong>, which is the sequence of Fibonacci numbers that we’ll <span class="No-Break">find later.</span></p>
			<p>Now, let’s start training <span class="No-Break">the algorithm:</span></p>
			<pre class="source-code">
# algorithm to train
# here, we use linear regression
model = LinearRegression()
# now, the actual process of training the model
model.fit(dfTrain[['first number', 'second number']],
                               dfTrain['result'])
# printing the score of the model, i.e. how good the model is when trained
print(model.score(dfTrain[['first number', 'second number']], dfTrain['result']))</pre>			<p>The magic of the entire code fragment is in the bold-faced code – the <strong class="source-inline">model.fit</strong> method call. This method trains the logistic regression model based on the data we prepared for it. The model itself is created one line above, in the <strong class="source-inline">model = </strong><span class="No-Break"><strong class="source-inline">LinearRegression()</strong></span><span class="No-Break"> line.</span></p>
			<p>Now, we can make inferences or create new Fibonacci numbers using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# and loop through the newly predicted numbers
for k in range(23):
  # the line below is where the magic happens
  # it takes two numbers from the list
  # formats them to an array
  # and makes the prediction
  # since the model returns a float,
  # we need to convert it to it
  intFibonacci = int(model.predict(np.array([[lstSequence[k],lstSequence[k+1]]])))
  # add this new number to the list for the next iteration
  lstSequence.append(intFibonacci)
  # and print it
  print(intFibonacci)</pre>			<p>This code fragment contains a similar line to the previous one – <strong class="source-inline">model.predict()</strong>. This line uses the previously created model to make an inference. Since the Fibonacci sequence is recursive, we need to add the newly created number to the list before we can make the new inference, which is done in the <span class="No-Break"><strong class="source-inline">lstSequence.append()</strong></span><span class="No-Break"> line.</span></p>
			<p>Now, it is very important to emphasize the difference between these two ways of solving the same problem. The traditional implementation <em class="italic">exposes</em> the algorithm used to create the numbers. We do not see the Fibonacci sequence there, but we can see how it is calculated. The machine learning implementation <em class="italic">exposes</em> the data used to create the numbers. We see the first sequence as training data, but we never see how the model creates that sequence. We do not know whether that model is always correct – we would need to test it against the real sequence – simply because we do not know how the algorithm works. This takes us to the next part, which is about just that – <span class="No-Break">probabilities.</span></p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor019"/>Probability and software – how well they go together</h1>
			<p>The fundamental characteristic that makes machine learning software different from traditional software is the fact that the core of machine learning models is statistics. This statistical learning means that the <a id="_idIndexMarker023"/>output of the machine learning model is a probability and, as such, it is not as clear as in traditional <span class="No-Break">software systems.</span></p>
			<p>The probability, which is the result of the <a id="_idIndexMarker024"/>model, means that the answer we receive is a probability of something. For example, if we classify an image to check whether it contains a dog or a cat, the result of this classification is a probability – for example, there is a 93% probability that the image contains a dog and a 7% probability that it contains a cat. This is illustrated in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer012">
					<img alt="Figure 1.3 – Probabilistic nature of machine learning software" src="image/B19548_01_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Probabilistic nature of machine learning software</p>
			<p>To use these probabilistic results in other parts of the software, or other systems, the machine learning software usually uses thresholds (for example, if <strong class="source-inline">x&lt;0.5</strong>) to provide only one result. Such thresholds specify which probability is acceptable to be able to consider the results to belong to a specific class. For our example of image classification, this probability would be 50% – if the probability of identifying a dog in the image is larger than 50%, then the model states that the image contains a dog (without <span class="No-Break">the probability).</span></p>
			<p>Changing these probabilistic results to digital ones, as we did in the previous example, is often correct, but not always. Especially in corner cases, such as when the probability is close to the threshold’s lower bound, the classification can lead to errors and thus to software failures. Such failures<a id="_idIndexMarker025"/> are often negligible, but not always. In safety-critical systems, there should be<a id="_idIndexMarker026"/> no mistakes as they can lead to unnecessary hazards with potentially <span class="No-Break">catastrophic consequences.</span></p>
			<p>In contexts where the probabilistic nature of machine learning software is problematic, but we still need machine learning for its other benefits, we can construct mechanisms that mitigate the consequences of mispredictions, misclassifications, and sub-optimizations. These mechanisms can guard the machine learning models and prevent them from suggesting wrong recommendations. For example, when we use machine learning image classification in the safety system of a car, we construct a so-called <em class="italic">safety cage</em> around the model. This safety cage is a non-machine learning component that uses rules to check whether a specific recommendation, classification, or prediction is plausible in the specific context. It can, for instance, prevent a car from suddenly stopping for a non-existent traffic light signal on a highway, which is a consequence of a misclassification of a camera feed from the <span class="No-Break">front camera.</span></p>
			<p>Therefore, let’s look at another best practice that encourages the use of machine learning software even in <span class="No-Break">safety-critical systems.</span></p>
			<p class="callout-heading">Best practice #3</p>
			<p class="callout">If your software is safety-critical, make<a id="_idIndexMarker027"/> sure that you can design mechanisms to prevent hazards caused by the probabilistic nature of <span class="No-Break">machine learning.</span></p>
			<p>Although this best practice is formulated toward safety-critical systems, it is more general than that. Even for mission-critical or business-critical systems, we can construct mechanisms that can gatekeep the machine learning models and prevent erroneous behavior of the entire software system. An example of how such a cage can be constructed is shown in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em>, where the gatekeeper component provides an additional signal that the model’s prediction cannot <span class="No-Break">be trusted/used:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer013">
					<img alt="Figure 1.4 – Gatekeeping of machine learning models" src="image/B19548_01_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Gatekeeping of machine learning models</p>
			<p>In this figure, the<a id="_idIndexMarker028"/> additional component is placed as the last one in this processing pipeline to ensure that the result is always binary (for this case). In other cases, such a gatekeeper can be placed in parallel to the machine learning<a id="_idIndexMarker029"/> model and can act as a parallel processing flow, where data quality is checked rather than the <span class="No-Break">classification model.</span></p>
			<p>Such gatekeeper models are used quite frequently, such as when detecting objects in perception systems – the model detects objects in individual images, while the gatekeeper checks that the same object is identified consistently over sequences of consecutive images. They can form redundant processing channels and pipelines. They can form feasibility-checking components, or they can correct out-of-bounds results into proper values. Finally, they can also disconnect machine learning components from the pipeline and adapt these pipelines to other components of the software, usually algorithms that make decisions – thus forming self-adaptive or self-healing <span class="No-Break">software systems.</span></p>
			<p>This probabilistic nature of machine learning software means that pre-deployment activities are different from the traditional software. In particular, the process of testing machine learning and traditional software <span class="No-Break">is different.</span></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor020"/>Testing and evaluation – the same but different</h1>
			<p>Every machine learning model needs to be validated, which means that the model needs to be able to provide correct inferences for a dataset that the model did not see before. The goal is to assess whether the model has learned patterns in the data, the data itself, or neither. The typical measures of correctness in classification problems are accuracy (the quotient of correctly <a id="_idIndexMarker030"/>inferred instances to all <a id="_idIndexMarker031"/>classified instances), <strong class="bold">Area Under Curve/Receiver Operation Characteristics</strong> (<strong class="bold">AUROC</strong>), and<a id="_idIndexMarker032"/> the <strong class="bold">true positive ratio</strong> (<strong class="bold">TPR</strong>) and <strong class="bold">false positive </strong><span class="No-Break"><strong class="bold">ratio</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FPR</strong></span><span class="No-Break">).</span></p>
			<p>For prediction problems, the quality of the model is<a id="_idIndexMarker033"/> measured in the mispredictions, such as the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). These measures quantify the errors in predictions – the smaller <a id="_idIndexMarker034"/>the values, the better the model. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em> shows the process for the most common form of <span class="No-Break">supervised learning:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer014">
					<img alt="Figure 1.5 – Model evaluation process for supervised learning" src="image/B19548_01_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Model evaluation process for supervised learning</p>
			<p>In this process, the model is subjected to different data for every iteration of training, after which it is used to make inferences (classifications or regression) on the same test data. The test data is set aside before training, and it is used as input to the model only when validating, never <span class="No-Break">during training.</span></p>
			<p>Finally, some models are reinforcement learning models, where the quality is assessed by the ability of the model to optimize the output according to a predefined function (reward function). These measures allow the algorithm to optimize its operations and find the optimal solution – for example, in genetic algorithms, self-driving cars, or energy grid operations. The challenge with these models is that there is no single metric that can measure performance – it depends on the scenario, the function, and the amount of training that the model received. One famous example of such training is the algorithm from the <em class="italic">War Games</em> movie (from 1983), where the main supercomputer plays millions of tic-tac-toe <a id="_idIndexMarker035"/>games to understand that there is no strategy to win – the game has <span class="No-Break">no winner.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.6</em> presents the process of training a reinforcement <span class="No-Break">system graphically:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer015">
					<img alt="Figure 1.6 – Reinforcement learning training process" src="image/B19548_01_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Reinforcement learning training process</p>
			<p>We could get the<a id="_idIndexMarker036"/> impression that training, testing, and validating machine learning models are all we need when developing machine learning software. This is far from being true. The models are parts of larger systems, which means that they need to be integrated with other components; these components are not validated in the process of validation described in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em> and <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
			<p>Every software system needs to undergo rigorous testing before it can be released. The goal of this testing is to find and remove as many defects as possible so that the user of the software experiences the best possible quality. Typically, the process of testing software is a process that comprises multiple phases. The process of testing follows the process of software development and aligns with that. In the beginning, software engineers (or testers) use unit tests to verify the correctness of <span class="No-Break">their components.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.7</em> presents how these three types of testing are related to one another. In unit testing, the focus is on algorithms. Often, this means that the software engineers must test individual functions and modules. Integration testing focuses <a id="_idIndexMarker037"/>on the connections between modules and how they can conduct tasks together. Finally, system testing and acceptance testing focus on the entire software product. The<a id="_idIndexMarker038"/> testers imitate real users to check that the software fulfills the requirements of <span class="No-Break">the users:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer016">
					<img alt="Figure 1.7 – Three types of software testing – unit testing (left), integration testing (middle), and system and acceptance testing (right)" src="image/B19548_01_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Three types of software testing – unit testing (left), integration testing (middle), and system and acceptance testing (right)</p>
			<p>The software testing process is very<a id="_idIndexMarker039"/> different than the process of model validation. Although we could<a id="_idIndexMarker040"/> mistake unit testing for model validation, this is not entirely the case. The output from the model validation process is one of the metrics (for example, accuracy), whereas the output from the unit test is <strong class="source-inline">true/false</strong> – whether the software produces the expected output or not. No known defects (equivalent to the false test results) are acceptable for a <span class="No-Break">software company.</span></p>
			<p>In traditional software testing, software engineers prepare a set of test cases to check whether their software works according to the specification. In machine learning software, the process of testing is based on setting aside part of the dataset (the test set) and checking how well the trained model (on the train set) works on <span class="No-Break">that data.</span></p>
			<p>Therefore, here is my fourth best practice for testing machine <span class="No-Break">learning systems.</span></p>
			<p class="callout-heading">Best practice #4</p>
			<p class="callout">Test the machine learning <a id="_idIndexMarker041"/>software as an addition to the typical train-validation-evaluation process of machine learning <span class="No-Break">model development.</span></p>
			<p>Testing the entire system is very important as the entire software system contains mechanisms to cope with the probabilistic<a id="_idIndexMarker042"/> nature of machine learning components. One such mechanism is the safety cage mechanism, where we can monitor the behavior of the machine learning components and prevent them from providing low-quality signals to the rest of the system (in the case of corner cases, close to the decision boundaries, in the <span class="No-Break">inference process).</span></p>
			<p>When we test the software, we also learn about the limitations of the machine learning components and our ability to handle the corner cases. Such knowledge is important for deploying the system when we need to specify the operational environment for the software. We need to understand the limitations related to the requirements and the specification of the software – the use cases for our software. Even more importantly, we need to understand the implications of the use of the software in terms of ethics <span class="No-Break">and trustworthiness.</span></p>
			<p>We’ll discuss ethics in <a href="B19548_15.xhtml#_idTextAnchor179"><span class="No-Break"><em class="italic">Chapter 15</em></span></a> and <a href="B19548_16.xhtml#_idTextAnchor187"><span class="No-Break"><em class="italic">Chapter 16</em></span></a>, but it is important to understand that we need to consider ethics from the very beginning. If we don’t, we risk that our system makes potentially harmful mistakes, such as the ones made by large artificial intelligence hiring systems, face recognition systems, or self-driving vehicles. These harmful mistakes entail monetary costs, but more importantly, they entail loss of trust in the product and even <span class="No-Break">missed opportunities.</span></p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor021"/>Summary</h1>
			<p>Machine learning and traditional software are often perceived as two alternatives. However, they are more like siblings – one cannot function without the other. Machine learning models are very good at solving constrained problems, but they require traditional software for data collection, preparation, <span class="No-Break">and presentation.</span></p>
			<p>The probabilistic nature of machine learning models requires additional elements to make them useful in the context of complete software products. Therefore, we need to embrace this nature and use it to our advantage. Even for safety-critical systems, we could (and should) use machine learning when we know how to design safety mechanisms to prevent <span class="No-Break">hazardous consequences.</span></p>
			<p>In this chapter, we explored the differences between machine learning software and traditional software while focusing on how to design software that can contain both parts. We also showed that there is much more to machine learning software than just training, testing, and evaluating the model – we showed that rigorous testing makes sense and is necessary for deploying <span class="No-Break">reliable software.</span></p>
			<p>Now, it is time to move on to the next chapter, where we’ll open up the black box of machine learning software and explore what we need to develop a complete machine learning software product – starting from data acquisition and ending with <span class="No-Break">user interaction.</span></p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>References</h1>
			<ul>
				<li><em class="italic">Shortliffe, E.H., et al., Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system. Computers and biomedical research, 1975. 8(4): </em><span class="No-Break"><em class="italic">p. 303-320.</em></span></li>
				<li><em class="italic">James, G., et al., An introduction to statistical learning. Vol. 112. </em><span class="No-Break"><em class="italic">2013: Springer.</em></span></li>
				<li><em class="italic">Saleh, H., Machine Learning Fundamentals: Use Python and scikit-learn to get up and running with the hottest developments in machine learning. 2018: Packt </em><span class="No-Break"><em class="italic">Publishing Ltd.</em></span></li>
				<li><em class="italic">Raschka, S. and V. Mirjalili, Python machine learning: Machine learning and deep learning with Python, scikit-learn, and TensorFlow 2. 2019: Packt </em><span class="No-Break"><em class="italic">Publishing Ltd.</em></span></li>
				<li><em class="italic">Sommerville, I., Software engineering. 10th. Book Software Engineering. 10th, Series Software </em><span class="No-Break"><em class="italic">Engineering, 2015.</em></span></li>
				<li><em class="italic">Houpis, C.H., G.B. Lamont, and B. Lamont, Digital control systems: theory, hardware, software. 1985: McGraw-Hill </em><span class="No-Break"><em class="italic">New York.</em></span></li>
				<li><em class="italic">Sawhney, R., Can artificial intelligence make software development more productive? LSE Business </em><span class="No-Break"><em class="italic">Review, 2021.</em></span></li>
				<li><em class="italic">He, X., K. Zhao, and X. Chu, AutoML: A survey of the state-of-the-art. Knowledge-Based Systems.</em><em class="italic"> 2021. 212: </em><span class="No-Break"><em class="italic">p. 106622.</em></span></li>
				<li><em class="italic">Reed, S., et al., A generalist agent. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:2205.06175, 2022.</em></span></li>
				<li><em class="italic">Floridi, L. and M. Chiriatti, GPT-3: Its nature, scope, limits, and consequences. Minds and Machines, 2020. 30(4): </em><span class="No-Break"><em class="italic">p. 681-694.</em></span></li>
				<li><em class="italic">Creswell, A., et al., Generative adversarial networks: An overview. IEEE signal processing magazine, 2018. 35(1): </em><span class="No-Break"><em class="italic">p. 53-65.</em></span></li>
				<li><em class="italic">Celebi, M.E. and K. Aydin, Unsupervised learning algorithms. </em><span class="No-Break"><em class="italic">2016: Springer.</em></span></li>
				<li><em class="italic">Chen, J.X., The evolution of computing: AlphaGo. Computing in Science &amp; Engineering, 2016. 18(4): </em><span class="No-Break"><em class="italic">p. 4-7.</em></span></li>
				<li><em class="italic">Ko, J.-S., J.-H. Huh, and J.-C. Kim, Improvement of energy efficiency and control performance of cooling system fan applied to Industry 4.0 data center. Electronics, 2019. 8(5): </em><span class="No-Break"><em class="italic">p. 582.</em></span></li>
				<li><em class="italic">Dastin, J., Amazon scraps secret AI recruiting tool that showed bias against women. In Ethics of Data and Analytics. 2018, Auerbach Publications. </em><span class="No-Break"><em class="italic">p. 296-299.</em></span></li>
				<li><em class="italic">Castelvecchi, D., Is facial recognition too biased to be let loose? Nature, 2020. 587(7834): </em><span class="No-Break"><em class="italic">p. 347-350.</em></span></li>
				<li><em class="italic">Siddiqui, F., R. Lerman, and J.B. Merrill, Teslas running Autopilot involved in 273 crashes reported since last year. In The Washington </em><span class="No-Break"><em class="italic">Post. 2022.</em></span></li>
			</ul>
		</div>
	</body></html>