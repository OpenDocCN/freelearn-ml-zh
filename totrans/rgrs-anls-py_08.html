<html><head></head><body>
<div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Regression Methods"><div class="book" id="1ENBI2-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Advanced Regression Methods</h1></div></div></div><p class="calibre8">In this chapter, we will introduce some advanced regression methods. Since many of them are very complex, we will skip most of the mathematical formulations, providing the readers instead with the ideas underneath the techniques and some practical advice, such as explaining when and when not to use the technique. We will illustrate:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Least Angle Regression (LARS)</li><li class="listitem">Bayesian regression</li><li class="listitem">SGD classification with hinge loss (note that this is not a regressor, it's a classifier)</li><li class="listitem">Regression trees</li><li class="listitem">Ensemble of regressors (bagging and boosting)</li><li class="listitem">Gradient Boosting Regressor with Least Angle Deviation</li></ul></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Regression Methods">
<div class="book" title="Least Angle Regression"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec43" class="calibre1"/>Least Angle Regression</h1></div></div></div><a id="id564" class="calibre1"/><p class="calibre8">Although very similar to Lasso (seen in <a class="calibre1" title="Chapter 6. Achieving Generalization" href="part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6">Chapter 6</a>, <span class="strong"><em class="calibre9">Achieving Generalization</em></span>), Least Angle Regression, or simply LARS, is a regression algorithm that, in a fast and smart way, selects the best features to use in the model, even though they're very closely correlated to each other. LARS is an evolution of the Forward Selection (also called Forward Stepwise Regression) algorithm and of the Forward Stagewise Regression algorithm.</p><p class="calibre8">Here is how the Forward Selection algorithm<a id="id565" class="calibre1"/> works, based on the hypothesis that all the variables, including the target one, have been previously normalized:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Of all the possible predictors for a problem, the one with the largest absolute correlation with the target variable <span class="strong"><em class="calibre9">y</em></span> is selected (that is, the one with the most explanatory capability). Let's call it <span class="strong"><em class="calibre9">p<sub class="calibre20">1</sub></em></span>.</li><li class="listitem" value="2">All the other predictors are now projected onto <span class="strong"><em class="calibre9">p<sub class="calibre20">1</sub></em></span> Least Angle Regression, and the projection is removed, creating a vector of residuals orthogonal to <span class="strong"><em class="calibre9">p<sub class="calibre20">1</sub></em></span>.</li><li class="listitem" value="3">Step 1 is repeated on the residual vectors, and the most correlated predictor is again selected. Let's name it <span class="strong"><em class="calibre9">p<sup class="calibre21">2</sup></em></span> Apply subscript.</li><li class="listitem" value="4">Step 2 is repeated, using <span class="strong"><em class="calibre9">p<sub class="calibre20">2</sub></em></span>, creating a vector of residuals orthogonal to <span class="strong"><em class="calibre9">p<sub class="calibre20">2</sub></em></span> (and also <span class="strong"><em class="calibre9">p<sub class="calibre20">1</sub></em></span>).</li><li class="listitem" value="5">This <a id="id566" class="calibre1"/>process continues until the prediction is satisfying, or when the largest absolute correlation falls below a set threshold. After each iteration, a new predictor is added to the list of predictors, and the residual is orthogonal to all of them.</li></ol><div class="calibre18"/></div><p class="calibre8">This method <a id="id567" class="calibre1"/>is not very popular because it has a serious limitation due to its extremely greedy approach; however, it's fairly quick. Let's now consider that we have a regression problem with two highly correlated variables. Forward Selection, on this dataset, will select the predictor on the basis of the first or the second variable, and then, since the residual will be very low, will reconsider the other variable in a far later step (eventually, never). This fact will lead to overfitting problems on the model. Wouldn't it be better if the two highly correlated variables were selected together, balancing the new predictor? That's practically the core idea of the <a id="id568" class="calibre1"/>Forward Stagewise Regression algorithm, where, in each step, the best predictor is partially added to the model. Let's provide the details here:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">In the model, every feature has an associate weight of zero—that is, <span class="strong"><em class="calibre9">w<sub class="calibre20">i</sub> = 0</em></span> for each feature <span class="strong"><em class="calibre9">i</em></span>.</li><li class="listitem" value="2">Of all the possible predictors for a problem, the one with the largest (absolute) correlation with the target variable <span class="strong"><em class="calibre9">y</em></span> is partially added to the model—that is, in the model, the weight of <span class="strong"><em class="calibre9">w<sub class="calibre20">i</sub></em></span> is increased by <span class="strong"><em class="calibre9">ε</em></span>.</li><li class="listitem" value="3">Repeat step 2, until the exploratory power is below a predefined threshold.</li></ol><div class="calibre18"/></div><p class="calibre8">This method represents a great improvement on the Forward Selected because, in the case of correlated features, both of them will be in the final model with a similar weight. The result is very good, but the enormous number of iterations needed to create the model is the really big problem with this algorithm. Again, the method becomes impractical because of its running time.</p><p class="calibre8">The<a id="id569" class="calibre1"/> LARS algorithm instead operates as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">In the model, every feature has an associate weight of zero—that is, <span class="strong"><em class="calibre9">w<sub class="calibre20">i</sub> = 0</em></span> for each feature <span class="strong"><em class="calibre9">i</em></span>.</li><li class="listitem" value="2">Of the possible predictors for a problem, the one with the largest (absolute) correlation with the target variable <span class="strong"><em class="calibre9">y</em></span> is partially added to the model—that is, in the model, the weight of <span class="strong"><em class="calibre9">w<sub class="calibre20">i</sub></em></span> is increased by <span class="strong"><em class="calibre9">ε</em></span>.</li><li class="listitem" value="3">Keep increasing <span class="strong"><em class="calibre9">w<sub class="calibre20">i</sub></em></span> till any other predictor (let's say <span class="strong"><em class="calibre9">j</em></span>) has as much correlation with the residual vector as the current predictor has.</li><li class="listitem" value="4">Increase <span class="strong"><em class="calibre9">w<sub class="calibre20">i</sub></em></span> and <span class="strong"><em class="calibre9">w<sub class="calibre20">j</sub></em></span> simultaneously until another predictor has as much correlation with the residual vector as the current predictors have.</li><li class="listitem" value="5">Keep adding predictors and weights until all the predictors are in the model or it meets another termination criterion, such as the number of iterations.</li></ol><div class="calibre18"/></div><p class="calibre8">This <a id="id570" class="calibre1"/>solution is able to compose the best pieces of Forward Selection and Stagewise Regression, creating a solution that is stable, not so prone to overfitting, and fast. Before getting to the examples, you may wonder why it is named Least Angle Regression. The answer is very simple: if the features and output are represented as vectors in the Cartesian space, at every iteration LARS includes in the model the variable most correlated with the residual vector, which is the one that generates the least angle with the residual. Actually, the whole process can be expressed visually.</p></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Regression Methods">
<div class="book" title="Least Angle Regression">
<div class="book" title="Visual showcase of LARS"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec80" class="calibre1"/>Visual showcase of LARS</h2></div></div></div><div class="mediaobject"><img src="../images/00120.jpeg" alt="Visual showcase of LARS" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here<a id="id571" class="calibre1"/> is the visual situation: two predictors (<span class="strong"><strong class="calibre2">x1</strong></span> and <span class="strong"><strong class="calibre2">x2</strong></span>), not necessarily orthogonal, and the target (<span class="strong"><strong class="calibre2">y</strong></span>). Note that, at the beginning, the residual corresponds to the target. Our model starts at <span class="strong"><strong class="calibre2">u0</strong></span> (where all the weights are <span class="strong"><em class="calibre9">0</em></span>).</p><p class="calibre8">Then, since <span class="strong"><strong class="calibre2">x2</strong></span> makes a smaller angle with the residual compared to <span class="strong"><strong class="calibre2">x1</strong></span>, we start <span class="strong"><em class="calibre9">walking</em></span> in the direction of <span class="strong"><strong class="calibre2">x2</strong></span>, while we keep computing the residual vector. Now, a question: where should we stop?</p><div class="mediaobject"><img src="../images/00121.jpeg" alt="Visual showcase of LARS" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We should <a id="id572" class="calibre1"/>stop at <span class="strong"><strong class="calibre2">u1</strong></span>, where the angle between the residual and <span class="strong"><strong class="calibre2">x1</strong></span> is the same as the angle between the residual and <span class="strong"><strong class="calibre2">x2</strong></span>. We then walk in the direction of the composition <span class="strong"><strong class="calibre2">x1</strong></span> and <span class="strong"><strong class="calibre2">x2</strong></span>, reaching <span class="strong"><strong class="calibre2">y</strong></span>.</p><div class="mediaobject"><img src="../images/00122.jpeg" alt="Visual showcase of LARS" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Regression Methods">
<div class="book" title="Least Angle Regression">
<div class="book" title="A code example"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec81" class="calibre1"/>A code example</h2></div></div></div><p class="calibre8">Let's now<a id="id573" class="calibre1"/> see LARS in action in Python on the Diabetic dataset, which consists of 10 numerical variables (age, sex, weight, blood pressure, and so on) measured on 442 patients, and an indication of disease progression after one year. First, we want to visualize the path of the weights of the coefficients. To do so, the <code class="email">lars_path()</code> class comes to our help (especially if its training is verbose):</p><div class="informalexample"><pre class="programlisting">In:
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

from sklearn import linear_model
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

diabetes = datasets.load_diabetes()
X = StandardScaler().fit_transform(diabetes.data)
y = StandardScaler(with_mean=True, with_std=False) \
        .fit_transform(diabetes.target)

alphas, _, coefs = linear_model.lars_path(X, y, verbose=2)

xx = np.sum(np.abs(coefs.T), axis=1)
xx /= xx[-1]

plt.plot(xx, coefs.T)
ymin, ymax = plt.ylim()
plt.vlines(xx, ymin, ymax, linestyle='dashed')
plt.xlabel('|coef| / max|coef|')
plt.ylabel('Coefficients')
plt.axis('tight')
plt.show()

Out:</pre></div><div class="mediaobject"><img src="../images/00123.jpeg" alt="A code example" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00124.jpeg" alt="A code example" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the <a id="id574" class="calibre1"/>output table, you can see that the first feature inserted in the model is the number 2, followed by the number 8 and so on. In the image, instead, you can simultaneously see the values of the coefficients (colored lines) and the steps (dotted lines). Remember that, at every step, one coefficient becomes non-zero, and all the coefficients in the model are updated linearly. On the right side of the image, you can find the final values of the weights.</p><p class="calibre8">This is the graphical way to see the LARS coefficients; if we only need a regressor (exactly as we've seen in the previous chapters), we can just use the <code class="email">Lars</code> class:</p><div class="informalexample"><pre class="programlisting">In:
regr = linear_model.Lars()

regr.fit(X, y)

print("Coefficients are:", regr.coef_)
Out:
Coefficients are: 
[ -0.47623169 -11.40703082  24.72625713  15.42967916 -37.68035801
  22.67648701   4.80620008   8.422084    35.73471316   3.21661161]</pre></div><p class="calibre8">As you <a id="id575" class="calibre1"/>may expect, the regressor object can be fitted with the method <code class="email">.fit</code>, and its weights (coefficients) are exactly the ones shown in the previous screenshot. To get the quality of the model, in a similar fashion to the other regressors, you can use the method score. In respect of the training data, here's the scoring output:</p><div class="informalexample"><pre class="programlisting">In:
print("R2 score is", regr.score(X,y)) Out:
R2 score is 0.517749425413</pre></div></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Advanced Regression Methods">
<div class="book" title="Least Angle Regression">
<div class="book" title="LARS wrap up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec82" class="calibre1"/>LARS wrap up</h2></div></div></div><p class="calibre8">Pros:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The<a id="id576" class="calibre1"/> smart way in which coefficients are updated produces low overfitting</li><li class="listitem">The model is intuitive and easily interpretable</li><li class="listitem">The training is as fast as Forward Selection</li><li class="listitem">It is great when the number of features is comparable with, or greater than, the number of observations</li></ul></div><p class="calibre8">Cons:</p><div class="book"><ul class="itemizedlist"><li class="listitem">It might <a id="id577" class="calibre1"/>not work very well when the number of features is very large—that is, where the number of features is far greater than the number of observations, since in such an occurrence it's very probable you'll find spurious correlations</li><li class="listitem">It won't work with very noisy features</li></ul></div></div></div></div>

<div class="book" title="Bayesian regression" id="1FLS41-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec44" class="calibre1"/>Bayesian regression</h1></div></div></div><p class="calibre8">Bayesian regression<a id="id578" class="calibre1"/> is similar to linear regression, as seen in <a class="calibre1" title="Chapter 3. Multiple Regression in Action" href="part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 3</a>, <span class="strong"><em class="calibre9">Multiple Regression in Action</em></span>, but, instead of predicting a value, it predicts its probability distribution. Let's start with an example: given <code class="email">X</code>, the training observation matrix, and <code class="email">y</code>, the target vector, linear regression creates a model (that is a series of coefficients) that fits the line that has the minimal error with the training points. Then, when a new observation arrives, the model is applied to that point, and a predicted value is outputted. That's the only output from linear regression, and no conclusions can be made as to whether the prediction, for that specific point, is accurate or not. Let's take a very simple <a id="id579" class="calibre1"/>example in code: the observed phenomenon has only one feature, and the number of observations is just <code class="email">10</code>:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.datasets import make_classification
from sklearn.datasets import make_regression

X, y = make_regression(n_samples=10, n_features=1, n_informative=1, noise=3, random_state=1)</pre></div><p class="calibre8">Now, let's fit a <span class="strong"><em class="calibre9">classic</em></span> linear regression model, and let's try to predict the regression value for a point outside the training support (in this simple example, we predict the value for a point whose <code class="email">x</code> value is double the max of the training values):</p><div class="informalexample"><pre class="programlisting">In:
regr = linear_model.LinearRegression()
regr.fit(X, y)

test_x = 2*np.max(X)
pred_test_x = regr.predict(test_x)
pred_test_x
Out:
array([ 10.79983753])</pre></div><p class="calibre8">Let's now plot the training points, the fitted line, and the predicted test point (on the extreme right of the image):</p><div class="informalexample"><pre class="programlisting">In:
plt.scatter(X, y)
x_bounds = np.array([1.2*np.min(X), 1.2*np.max(X)]).reshape(-1, 1)
plt.plot(x_bounds, regr.predict(x_bounds) , 'r-')
plt.plot(test_x, pred_test_x, 'g*')
plt.show()
Out:</pre></div><div class="mediaobject"><img src="../images/00125.jpeg" alt="Bayesian regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To have a <a id="id580" class="calibre1"/>probability density function of the predicted value, we should start from the beginning and change a hypothesis and some steps in the linear regressor. Since this is an advanced algorithm, the math involved is very heavy and we prefer to communicate the idea underlying the methods, instead of exposing pages and pages of math formulation.</p><p class="calibre8">First, we are only able to infer a distribution on the predicted value if every variable is modeled as a distribution. In fact, weights in this model are treated as random variables with a normal distribution, centered in zero (that is, a spherical Gaussian) and having an unknown variance (learnt from the data). The regularization imposed by this algorithm is very similar to the one set by Ridge regression.</p><p class="calibre8">The output of a prediction is a value (exactly as in linear regression) and a variance value. Using the value as the mean, and the variance as an actual variance, we can then represent the probability distribution of the output:</p><div class="informalexample"><pre class="programlisting">In:
regr = linear_model.BayesianRidge()
regr.fit(X, y)
Out:
BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, 
              copy_X=True, fit_intercept=True, lambda_1=1e-06, 
              lambda_2=1e-06, n_iter=300, normalize=False, 
              tol=0.001, verbose=False)
In:
from matplotlib.mlab import normpdf

mean = regr.predict(test_x)
stddev = regr.alpha_
plt_x = np.linspace(mean-3*stddev, mean+3*stddev,100)
plt.plot(plt_x, normpdf(plt_x, mean, stddev))
plt.show()
Out:</pre></div><div class="mediaobject"><img src="../images/00126.jpeg" alt="Bayesian regression" class="calibre10"/></div><p class="calibre11"> </p></div>

<div class="book" title="Bayesian regression" id="1FLS41-a2faae6898414df7b4ff4c9a487a20c6">
<div class="book" title="Bayesian regression wrap up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec83" class="calibre1"/>Bayesian regression wrap up</h2></div></div></div><p class="calibre8">Pros:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Robustness<a id="id581" class="calibre1"/> to Gaussian noise</li><li class="listitem">Great if the number of features is comparable to the number of observations</li></ul></div><p class="calibre8">Cons:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Time-consuming</li><li class="listitem">The <a id="id582" class="calibre1"/>hypotheses imposed on the variables are often far from real</li></ul></div></div></div>

<div class="book" title="SGD classification with hinge loss"><div class="book" id="1GKCM2-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec45" class="calibre1"/>SGD classification with hinge loss</h1></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 4. Logistic Regression" href="part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 4</a>, <span class="strong"><em class="calibre9">Logistic Regression</em></span> we explored a classifier based on a regressor, logistic regression. Its <a id="id583" class="calibre1"/>goal was to fit the best probabilistic function associated with the probability of one point to be classified with a label. Now, the core function of the algorithm considers all the training points of the dataset: what if it's only built on the boundary ones? That's exactly the case with the linear <a id="id584" class="calibre1"/>
<span class="strong"><strong class="calibre2">Support Vector Machine</strong></span> (SVM) classifier, where a linear decision plane is drawn by only considering the points close to the separation boundary itself.</p><p class="calibre8">Beyond working on the support vectors (the closest points to the boundary), SVM uses a new decision loss, called<a id="id585" class="calibre1"/> <span class="strong"><strong class="calibre2">hinge</strong></span>. Here's its formulation:</p><div class="mediaobject"><img src="../images/00127.jpeg" alt="SGD classification with hinge loss" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Where t is <a id="id586" class="calibre1"/>the intended label of the point x and w the set of weights in the classifier. The hinge loss is also sometimes called <a id="id587" class="calibre1"/>
<span class="strong"><strong class="calibre2">softmax</strong></span>, because it's actually a clipped max. In this formula, just the boundary points (that is, the support vectors) are used.</p><p class="calibre8">In the first instance, this function, although convex, is non differentiable, so approaches based on stochastic gradient descent (SGD) are theoretically invalid. In practical terms, since it's a continuous function, it has a piecewise derivative. This leads to the fact that SGD can be actively used in this technique to derive a quick and approximate solution.</p><p class="calibre8">Here's an example in Python: let's use the <code class="email">SGDClassifier</code> class (as seen in <a class="calibre1" title="Chapter 4. Logistic Regression" href="part0029_split_000.html#RL0A2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 4</a>, <span class="strong"><em class="calibre9">Logistic Regression</em></span>) with the <code class="email">hinge</code> loss, applied on a dataset of <code class="email">100</code> points drawn from <code class="email">2</code> classes. With this piece of code, we're interested in seeing the decision boundary and the support vectors chosen by the classifier:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.linear_model import SGDClassifier

# we create 50 separable points
X, y = make_classification(n_samples=100, n_features=2, 
                           n_informative=2, n_redundant=0,
                           n_clusters_per_class=1, class_sep=2, 
                           random_state=101)

# fit the model
clf = SGDClassifier(loss="hinge", n_iter=500, random_state=101, 
                    alpha=0.001)
clf.fit(X, y)

# plot the line, the points, and the nearest vectors to the plane
xx = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 10)
yy = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 10)

X1, X2 = np.meshgrid(xx, yy)
Z = np.empty(X1.shape)
for (i, j), val in np.ndenumerate(X1):
    x1 = val
    x2 = X2[i, j]
    p = clf.decision_function([[x1, x2]])
    Z[i, j] = p[0]
levels = [-1.0, 0.0, 1.0]
linestyles = ['dashed', 'solid', 'dashed']
plt.contour(X1, X2, Z, levels, colors='k', linestyles=linestyles)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)

plt.show()

Out:</pre></div><div class="mediaobject"><img src="../images/00128.jpeg" alt="SGD classification with hinge loss" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The image presents <a id="id588" class="calibre1"/>the points belonging to the two classes' points (the dots on the right and left) and the decision boundary (the solid line between the classes). In addition, it contains two dotted lines, which connect the support vectors for each class (that is, points on these lines are support vectors). The decision boundary is, simply, the line at the same distance between them.</p></div>

<div class="book" title="SGD classification with hinge loss">
<div class="book" title="Comparison with logistic regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec84" class="calibre1"/>Comparison with logistic regression</h2></div></div></div><p class="calibre8">The <a id="id589" class="calibre1"/>logistic regression learner is intended to make use of all the input points of the training set, and emit a probability as output. SGD with hinge loss, instead, directly produces a label, and only uses the points on the boundary to improve the model. How are their performances? Let's make a test with an artificial dataset with 20 features (of them, 5 are informative, 5 redundant, and 10 random) and 10,000 observations. Then, we split the data into 70/30 as training set and test set and we train two SGD classifiers: one with the hinge loss function and the <a id="id590" class="calibre1"/>second with the logistic loss function. Finally, we compare the accuracy of their predictions on their test set:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.cross_validation import train_test_split
from sklearn.metrics import accuracy_score

X, y = make_classification(n_samples=10000, n_features=20, 
                           n_informative=5, n_redundant=5,
                           n_clusters_per_class=2, class_sep=1,
                           random_state=101)


X_train, X_test, y_train, y_test =  train_test_split(
    X, y, test_size=0.3, random_state=101)

clf_1 = SGDClassifier(loss="hinge", random_state=101)
clf_1.fit(X_train, y_train)

clf_2 = SGDClassifier(loss="log", random_state=101)
clf_2.fit(X_train, y_train)

print('SVD            : ', accuracy_score(y_test, clf_1.predict(X_test)))
print('Log. Regression: ', accuracy_score(y_test, clf_2.predict(X_test)))
Out:
SVD            :  0.814333333333
Log. Regression:  0.756666666667</pre></div><p class="calibre8">As a rule of thumb, SVM is generically more accurate than logistic regression, but its performance is not extraordinary. SVM, though, is slower during the training process; in fact, with regard to training times, logistic regression is more than 30% faster than SVM.</p><div class="informalexample"><pre class="programlisting">In:
%timeit clf_1.fit(X_train, y_train)  
Out:
100 loops, best of 3: 3.16 ms per loop
In:
%timeit clf_2.fit(X_train, y_train)
Out:
100 loops, best of 3: 4.86 ms per loop</pre></div></div></div>

<div class="book" title="SGD classification with hinge loss">
<div class="book" title="SVR"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec85" class="calibre1"/>SVR</h2></div></div></div><p class="calibre8">As for <a id="id591" class="calibre1"/>linear regressor/logistic regression, even SVM has a regression counterpart, called <a id="id592" class="calibre1"/>
<span class="strong"><strong class="calibre2">Support Vector Regressor</strong></span> (SVR). Its math formulation is very long and beyond the scope of this book. However, since it's very effective, we believe it is important to depict how it works in practice, as applied to the Boston dataset and compared with a linear regressor model:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.svm import SVR
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.datasets import load_boston


boston = load_boston()
X = StandardScaler().fit_transform(boston['data'])
y = boston['target']


X_train, X_test, y_train, y_test =  train_test_split(
    X, y, test_size=0.3, random_state=101)

regr_1 = SVR(kernel='linear')
regr_1.fit(X_train, y_train)

regr_2 = SGDRegressor(random_state=101)
regr_2.fit(X_train, y_train)

print('SVR            : ', mean_absolute_error(y_test, regr_1.predict(X_test)))
print('Lin. Regression: ', mean_absolute_error(y_test, regr_2.predict(X_test)))
Out:
SVR            :  3.67434988716
Lin. Regression:  3.7487663498</pre></div></div></div>

<div class="book" title="SGD classification with hinge loss">
<div class="book" title="SVM wrap up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec86" class="calibre1"/>SVM wrap up</h2></div></div></div><p class="calibre8">The<a id="id593" class="calibre1"/> pros <a id="id594" class="calibre1"/>are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Can use SGD to speed up the processing</li><li class="listitem">Output is usually more accurate than logistic regression (since only boundary points are in the formula)</li></ul></div><p class="calibre8">The cons <a id="id595" class="calibre1"/>are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">It works very well if the points of the two classes are linearly separable, although an extension for non-linearly separable classes is available. In this case, though complexity is very high, results are still usually great.</li><li class="listitem">As for <a id="id596" class="calibre1"/>logistic regression, it can be used for two-class problems.</li></ul></div></div></div>

<div class="book" title="Regression trees (CART)" id="1HIT81-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec46" class="calibre1"/>Regression trees (CART)</h1></div></div></div><p class="calibre8">A very <a id="id597" class="calibre1"/>common learner, recently used very much due to its speed, is the regression tree. It's a non-linear learner, can work with both categorical and numerical features, and can be used alternately for classification or regression; that's why it's often called <span class="strong"><strong class="calibre2">Classification and Regression Tree</strong></span> (CART). Here, in this section, we will see how regression trees work.</p><p class="calibre8">A tree is composed of a series of nodes that split the branch into two children. Each branch, then, can go in another node, or remain a leaf with the predicted value (or class).</p><p class="calibre8">Starting from the root (that is, the whole dataset):</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">The best feature with which to split the dataset, <span class="strong"><em class="calibre9">F1</em></span>, is identified as well as the best splitting value. If the feature is numerical, the splitting value is a threshold <span class="strong"><em class="calibre9">T1</em></span>: in this case, the left child branch will be the set of observations where <span class="strong"><em class="calibre9">F1</em></span> is below <span class="strong"><em class="calibre9">T1</em></span>, and the right one is the set of observations where <span class="strong"><em class="calibre9">F1</em></span> is greater than, or equal to, <span class="strong"><em class="calibre9">T1</em></span>. If the feature is categorical, the splitting is done on a subset of levels <span class="strong"><em class="calibre9">S1</em></span>: observations where the <span class="strong"><em class="calibre9">F1</em></span> feature is one of these levels compose the left branch child, all the others compose the right branch child.</li><li class="listitem" value="2">This operation is then run again (independently) for each branch, recursively, until there's no more chance to split.</li><li class="listitem" value="3">When the splits are completed, a leaf is created. Leaves denote output values.</li></ol><div class="calibre18"/></div><p class="calibre8">You can immediately see that making the prediction is immediate: you just need to traverse the tree from the root to the leaves and, in each node, check whether a feature is below (or not) a threshold or, alternatively, has a value inside (or outside) a set.</p><p class="calibre8">As a concluding remark, we discuss how to define the best feature to split. What about the best value or subset? Well, for regression trees, we use the criteria of the variance reduction: in each node, an extensive search is run among all features and among all values or levels in that feature. The combination that achieves the best possible variance in both the right branch and left branches, compared with the input set, is selected and marked as <span class="strong"><em class="calibre9">best</em></span>.</p><p class="calibre8">Note that regression trees decide, for each node, the optimal split. Such a local optimization approach unfortunately leads to a suboptimal result. In addition, it is advisable that the regression tree should be pruned; that is, you should remove some leaves to prevent overfitting (for example, by setting a minimum threshold to the variance reduction measure). Such <a id="id598" class="calibre1"/>are the drawbacks of regression trees. On the other hand, they are somehow accurate and relatively quick to train and test.</p><p class="calibre8">In the code, regression trees are as easy as the other regressors:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.tree import DecisionTreeRegressor

regr = DecisionTreeRegressor(random_state=101)
regr.fit(X_train, y_train)

mean_absolute_error(y_test, regr.predict(X_test))
Out:
3.2842105263157895</pre></div></div>

<div class="book" title="Regression trees (CART)" id="1HIT81-a2faae6898414df7b4ff4c9a487a20c6">
<div class="book" title="Regression tree wrap up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec87" class="calibre1"/>Regression tree wrap up</h2></div></div></div><p class="calibre8">Pros:</p><div class="book"><ul class="itemizedlist"><li class="listitem">They <a id="id599" class="calibre1"/>can model non-linear behaviors</li><li class="listitem">Great for categorical features and numerical features, without normalization</li><li class="listitem">Same approach for classification and regression</li><li class="listitem">Fast training, fast prediction time, and small memory fingerprint</li></ul></div><p class="calibre8">Cons:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Greedy<a id="id600" class="calibre1"/> algorithm: it doesn't optimize the full solution, just the best choice.</li><li class="listitem">It doesn't work very well when the number of features is significant.</li><li class="listitem">Leaves can be very specific. In this case, we need to "prune the tree", removing some nodes.</li></ul></div></div></div>

<div class="book" title="Bagging and boosting"><div class="book" id="1IHDQ2-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec47" class="calibre1"/>Bagging and boosting</h1></div></div></div><p class="calibre8">Bagging <a id="id601" class="calibre1"/>and boosting are two techniques used to combine learners. These techniques are classified under the generic name of<a id="id602" class="calibre1"/> <span class="strong"><strong class="calibre2">ensembles</strong></span> (or meta-algorithm) because <a id="id603" class="calibre1"/>the ultimate goal is actually to ensemble <span class="strong"><em class="calibre9">weak</em></span> learners to create a more sophisticated, but more accurate, model. There is no formal definition of a weak learner, but ideally it's a fast, sometimes linear model that not necessarily produces excellent results (it suffices that they are just better than a random guess). The final ensemble is typically a non-linear learner whose performance increases with the number of weak learners in the model (note that the relation is strictly non-linear). Let's now see how they work.</p></div>

<div class="book" title="Bagging and boosting">
<div class="book" title="Bagging"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec88" class="calibre1"/>Bagging</h2></div></div></div><p class="calibre8">Bagging <a id="id604" class="calibre1"/>stands <a id="id605" class="calibre1"/>for <span class="strong"><strong class="calibre2">Bootstrap Aggregating</strong></span>, and its ultimate goal is to reduce variance by averaging weak learners' results. Let's now see the code; we will explain how it works. As a dataset, we will reuse the Boston dataset (and its validation split) from the previous example:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.ensemble import BaggingRegressor
bagging = BaggingRegressor(SGDRegressor(), n_jobs=-1,
                           n_estimators=1000, random_state=101,
                           max_features=0.8)
bagging.fit(X_train, y_train)
mean_absolute_error(y_test, bagging.predict(X_test))
Out:
3.8345485952100629</pre></div><p class="calibre8">The<a id="id606" class="calibre1"/> <code class="email">BaggingRegressor</code> class, from the <code class="email">submodule</code> ensemble of Scikit-learn, is the base class to create bagging regressors. It requires the weak learner (in the example, it's a <code class="email">SGDRegressor</code>), the total number of regressors (1,000), and the maximum number of features to be used in each regressor (80% of the total number). Then, the bagging learner is trained as with the other learners seen so far, with the method fit. At this point, for each weak learner:</p><div class="book"><ul class="itemizedlist"><li class="listitem">80% of the features composing the <span class="strong"><em class="calibre9">X</em></span> train dataset are selected at random</li><li class="listitem">The weak learner is trained just on the selected features on a bootstrap with a replacement set of observations in the training set</li></ul></div><p class="calibre8">At the end, the bagging model contains 1,000 trained <code class="email">SGDRegressors</code>. When a prediction is requested from the ensemble, each of the 1,000 weak learners makes its prediction, then the results are averaged, producing the ensemble prediction.</p><p class="calibre8">Please note that both training and prediction operations are per-weak learner; therefore they can be parallelized on multiple CPUs (that's why <code class="email">n_jobs</code> is <code class="email">-1</code> in the example; that is, we use all the cores).</p><p class="calibre8">The final result, in terms of MAE, should be better than a single <code class="email">SGDRegressor</code>; on the other hand, the model is about 1,000 times more complex.</p><p class="calibre8">Typically, ensembles are associated with decision or regression trees. In that case, the name of the regression ensemble changes to Random Forest Regressor (that is, a forest, composed of multiple trees). Since this technique is often used as the <span class="strong"><em class="calibre9">default</em></span> bagging ensemble, there is an ad hoc class in Scikit-learn:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.ensemble import RandomForestRegressor

regr = RandomForestRegressor(n_estimators=100, 
                             n_jobs=-1, random_state=101)
regr.fit(X_train, y_train)
mean_absolute_error(y_test, regr.predict(X_test))
Out:
2.6412236842105261</pre></div><p class="calibre8">One <a id="id607" class="calibre1"/>additional feature of Random Forests is their ability to rank feature importance in the model (that is, they detect which features produce the highest variation of the predicted variable). Here's the code; always remember to normalize the feature matrix first (we've already done it in the previous section):</p><div class="informalexample"><pre class="programlisting">In:
sorted(zip(regr.feature_importances_, boston['feature_names']),
       key=lambda x: -x[0])
Out:</pre></div><div class="mediaobject"><img src="../images/00129.jpeg" alt="Bagging" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The list is sorted from the most important feature to the least important (for this ensemble). If you change the weak learner, or any other parameter, this list may change.</p></div></div>

<div class="book" title="Bagging and boosting">
<div class="book" title="Boosting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec89" class="calibre1"/>Boosting</h2></div></div></div><p class="calibre8">Boosting is <a id="id608" class="calibre1"/>a way to combine (ensemble) weak learners, primarily to reduce prediction bias. Instead of creating a pool of predictors, as in bagging, boosting produces a cascade of them, where each output is the input for the following learner. We'll<a id="id609" class="calibre1"/> start with an example, exactly as we've done in the previous sub-section:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.ensemble import AdaBoostRegressor
booster = AdaBoostRegressor(SGDRegressor(), random_state=101,
                            n_estimators=100, learning_rate=0.01)

booster.fit(X_train, y_train)
mean_absolute_error(y_test, booster.predict(X_test))
Out:
3.8621128094354349</pre></div><p class="calibre8">The <code class="email">AdaBoostRegressor</code> class, from<a id="id610" class="calibre1"/> the <code class="email">submodule</code> ensemble of Scikit-learn, is the base class to create a Boosted Regressor. As for the bagging, it requires the weak learner (an <code class="email">SGDRegressor</code>), the total number of regressors (100), and the learning rate (0.01). Starting from an unfitted ensemble, for each weak learner the training is:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Given the training set, the cascade of already-fit learners produces a prediction</li><li class="listitem">The error between the actual values and the predicted ones, multiplied by the learning rate, is computed</li><li class="listitem">A new weak learner is trained on that error set, and inserted as the last stage in the cascade of already trained learners</li></ul></div><p class="calibre8">At the end of the training stage, the ensemble contains 100 trained <code class="email">SGDRegressors</code> organized in a cascade. When a prediction is requested from the ensemble, the final value is a recursive operation: starting from the last stage, the output value is the value predicted by the previous stage plus the learning rate multiplied by the prediction of the current stage.</p><p class="calibre8">The learning rate is similar to the one from the stochastic gradient descent. A smaller learning rate will require more steps to approach the results, but the granularity of the output will be better. A bigger rate will require fewer steps, but will probably approach a less accurate result.</p><p class="calibre8">Please note here that training and testing cannot be done independently on each weak learner, since to train a model you need the chain of outputs of the previous ones. This fact limits the CPU usage to only one, limiting the length of the cascade.</p><p class="calibre8">In the case of boosting with Decision/Regression Trees, the Scikit-learn package offers a pre-build class called <code class="email">GradientBoostingRegressor</code>. A short code snippet should suffice to demonstrate how it works:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.ensemble import GradientBoostingRegressor

regr = GradientBoostingRegressor(n_estimators=500, 
                                 learning_rate=0.01, 
                                 random_state=101)
regr.fit(X_train, y_train)
mean_absolute_error(y_test, regr.predict(X_test))
Out:
2.6148878419996806</pre></div><p class="calibre8">Even with <a id="id611" class="calibre1"/>Boosting, it is possible to rank feature importance. In fact, it's the very same method:</p><div class="informalexample"><pre class="programlisting">In:
sorted(zip(regr.feature_importances_, boston['feature_names']),
       key=lambda x: -x[0])
Out:</pre></div><div class="mediaobject"><img src="../images/00130.jpeg" alt="Boosting" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Bagging and boosting">
<div class="book" title="Ensemble wrap up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec90" class="calibre1"/>Ensemble wrap up</h2></div></div></div><p class="calibre8">The <a id="id612" class="calibre1"/>pros are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Strong learners based on weak learners</li><li class="listitem">They enable stochastic learning</li><li class="listitem">The randomness of the process creates a robust solution</li></ul></div><p class="calibre8">The cons <a id="id613" class="calibre1"/>are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Training time is considerable, as well as the memory footprint</li><li class="listitem">The learning step (in the boosted ensemble) can be very tricky to properly set, similar to the update step (alpha) in the stochastic gradient descent</li></ul></div></div></div>

<div class="book" title="Gradient Boosting Regressor with LAD" id="1JFUC1-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec48" class="calibre1"/>Gradient Boosting Regressor with LAD</h1></div></div></div><p class="calibre8">More than a <a id="id614" class="calibre1"/>new technique, this is an ensemble of technologies already seen in this book, with a new loss function, the <span class="strong"><strong class="calibre2">Least Absolute Deviations</strong></span> (LAD). With respect to the least square function, seen in the previous chapter, with LAD the L1 norm of the error is computed.</p><p class="calibre8">Regressor learners <a id="id615" class="calibre1"/>based on LAD are typically robust but unstable, because of the multiple minima of the loss function (leading therefore to multiple best solutions). Alone, this loss function seems to bear little value, but paired with gradient boosting, it creates a very stable regressor, due to the fact that boosting overcomes LAD regression limitations. With the code, this is very simple to achieve:</p><div class="informalexample"><pre class="programlisting">In:
from sklearn.ensemble import GradientBoostingRegressor

regr = GradientBoostingRegressor('lad',
                                 n_estimators=500, 
                                 learning_rate=0.1, 
                                 random_state=101)
regr.fit(X_train, y_train)
mean_absolute_error(y_test, regr.predict(X_test))
Out:
2.6216986613160258</pre></div><p class="calibre8">Remember to specify to use the <code class="email">'lad'</code> loss, otherwise the default least square (L<sup class="calibre21">2</sup>) is used. In addition, another loss function, <code class="email">huber</code>, combines the least square loss and the least absolute deviation loss to create a loss function even more robust. To try it, just insert the string value <code class="email">'huber'</code> instead of <code class="email">'lad'</code> in the last run piece of code.</p></div>

<div class="book" title="Gradient Boosting Regressor with LAD" id="1JFUC1-a2faae6898414df7b4ff4c9a487a20c6">
<div class="book" title="GBM with LAD wrap up"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec91" class="calibre1"/>GBM with LAD wrap up</h2></div></div></div><p class="calibre8">The pros are<a id="id616" class="calibre1"/> that it combines the strength of a boosted ensemble to the LAD loss, producing a very stable and robust learner and the cons are that training time is very high (exactly the same as training N consecutive LAD learners, one after the other).</p></div></div>
<div class="book" title="Summary" id="1KEEU1-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec49" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter concludes the long journey around regression methods we have taken throughout this book. We have seen how to deal with different kinds of regression modeling, how to pre-process data, and how to evaluate the results. In the present chapter, we glanced at some cutting-edge techniques. In the next, and last, chapter of the book, we apply regression in real-world examples and invite you to experiment with some concrete examples.</p></div></body></html>