- en: Neural Networks for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于机器学习的神经网络
- en: This chapter is the introduction to the world of deep learning, whose methods
    make it possible to achieve the state-of-the-art performance in many classification
    and regression fields often considered extremely difficult to manage (such as
    image segmentation, automatic translation, voice synthesis, and so on). The goal
    is to provide the reader with the basic instruments to understand the structure
    of a fully connected neural network and model it using the Python tool Keras (employing
    all the modern techniques to speed the training process and prevent overfitting).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是深度学习世界的介绍，其方法使得在许多通常被认为难以管理的分类和回归领域（如图像分割、自动翻译、语音合成等）实现最先进的性能成为可能。目标是向读者提供理解全连接神经网络结构的基本工具，并使用Python工具Keras（采用所有现代技术以加速训练过程并防止过拟合）对其进行建模。
- en: 'In particular, the topics covered in the chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，本章涵盖的主题如下：
- en: The structure of a basic artificial neuron
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本人工神经元的结构
- en: Perceptrons, linear classifiers, and their limitations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器、线性分类器和它们的局限性
- en: Multilayer perceptrons with the most important activation functions (such as
    ReLU)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有最重要的激活函数（如ReLU）的多层感知器
- en: Back-propagation algorithms based on **stochastic gradient descent** (**SGD**)
    optimization method
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于**随机梯度下降**（**SGD**）优化方法的反向传播算法
- en: Optimized SGD algorithms (Momentum, RMSProp, Adam, AdaGrad, and AdaDelta)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化的SGD算法（动量、RMSProp、Adam、AdaGrad和AdaDelta）
- en: Regularization and dropout
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化和dropout
- en: Batch normalization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化
- en: The basic artificial neuron
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本人工神经元
- en: 'The building block of a neural network is the abstraction of a biological neuron,
    a quite simplistic but powerful computational unit that was proposed for the first
    time by F. Rosenblatt in 1957, to make up the simplest neural architecture, called
    a perceptron, that we are going to analyze in the next section. Contrary to Hebbian
    Learning, which is more biologically plausible but has some strong limitations,
    the artificial neuron has been designed with a pragmatic viewpoint and, of course,
    only its structure is based on a few elements characterizing a biological cell.
    However, recent deep learning research activities have unveiled the enormous power
    of this kind of architecture. Even if there are more complex and specialized computational
    cells, the basic artificial neuron can be summarized as the conjunction of two
    blocks, which are clearly shown in the following diagram:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本构建块是对生物神经元的抽象，这是一个相当简单但功能强大的计算单元，由F. Rosenblatt于1957年首次提出，用于构成最简单的神经网络架构，即感知器，我们将在下一节中分析。与更符合生物学原理但有一些强烈限制的赫布学习法相反，人工神经元的设计具有实用主义观点，当然，其结构仅基于一些表征生物细胞的元素。然而，最近深度学习研究活动揭示了这种架构的巨大力量。即使存在更复杂和专门的计算单元，基本的人工神经元也可以概括为两个块的结合，这在以下图中可以清楚地看到：
- en: '![](img/cac7847b-851e-4a7e-81a8-6bb81174669a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cac7847b-851e-4a7e-81a8-6bb81174669a.png)'
- en: 'The input of a neuron is a real-valued vector *x ∈ ℜ^n*, while the output is
    a scalar *y ∈ ℜ*. The first operation is linear:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的输入是一个实值向量 *x ∈ ℜ^n*，而输出是一个标量 *y ∈ ℜ*。第一个操作是线性的：
- en: '![](img/0963dba1-7a8a-43f1-9f8c-00dbc8866620.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0963dba1-7a8a-43f1-9f8c-00dbc8866620.png)'
- en: 'The vector *w ∈ ℜ^n* is called **weight-vector** (or **synaptic weight vector**,
    because, analogously to a biological neuron, it reweights the input values), while
    the scalar term *b ∈ ℜ* is a constant called **bias**. In many cases, it''s easier
    to consider only the weight vector. It''s possible to get rid of the bias by adding
    an extra input feature equal to 1 and a corresponding weight:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 *w ∈ ℜ^n* 被称为**权重向量**（或**突触权重向量**，因为它类似于生物神经元，重新加权输入值），而标量项 *b ∈ ℜ* 是一个称为**偏差**的常数。在许多情况下，考虑权重向量更容易。可以通过添加一个等于1的额外输入特征及其相应的权重来消除偏差：
- en: '![](img/5cf0597d-6932-4e7b-b990-94e3981c2441.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5cf0597d-6932-4e7b-b990-94e3981c2441.png)'
- en: In this way, the only element that must be learned is the weight vector. The
    following block is called an **activation function**, and it's responsible for
    remapping the input into a different subset. If the function is *f[a](z) = z*,
    the neuron is called linear and the transformation can be omitted. The first experiments
    were based on linear neurons that are much less powerful than non-linear ones,
    and this was a reason that led many researchers to consider the perceptron as
    a failure, but, at the same time, this limitation opened the door for a new architecture
    that, instead, showed its excellent abilities. Let's now start this analysis with
    the first neural network ever proposed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，唯一需要学习的是权重向量。以下块被称为**激活函数**，它负责将输入重新映射到不同的子集。如果函数是*f[a](z) = z*，则该神经元被称为线性神经元，变换可以被省略。最初的实验基于线性神经元，这些神经元的强大程度远低于非线性神经元，这也是许多研究人员认为感知机失败的原因之一，但与此同时，这种限制为一种新的架构打开了大门，这种架构反而展示了其卓越的能力。现在，让我们从这个最初提出的神经网络开始分析。
- en: Perceptron
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机
- en: '**Perceptron** was the name that Frank Rosenblatt gave to the first neural
    model in 1957\. A perceptron is a neural network with a single layer of input
    linear neurons, followed by an output unit based on the *sign(•)* function (alternatively,
    it''s possible to consider a bipolar unit whose output is -1 and 1). The architecture
    of a perceptron is shown in the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机**是弗兰克·罗森布拉特在1957年给第一个神经网络模型起的名字。感知机是一个单层输入线性神经元组成的神经网络，后面跟着一个基于*sign(•)*函数的输出单元（或者，也可以考虑一个输出为-1和1的双极性单元）。感知机的架构在以下图表中展示：'
- en: '![](img/45010c23-8b99-4016-ad8d-cc0490489977.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45010c23-8b99-4016-ad8d-cc0490489977.png)'
- en: 'Even if the diagram can appear as quite complex, a perceptron can be summarized
    by the following equation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 即使图表看起来可能相当复杂，感知机可以用以下方程来概括：
- en: '![](img/49fbeabc-88f8-4bfa-bc16-2a1efb7a151c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49fbeabc-88f8-4bfa-bc16-2a1efb7a151c.png)'
- en: All the vectors are conventionally column-vectors; therefore, the dot product
    *w^Tx[i]* transforms the input into a scalar, then the bias is added, and the
    binary output is obtained using the step function, which outputs 1 when *z > 0*
    and 0 otherwise. At this point, a reader could object that the step function is
    non-linear; however, a non-linearity applied to the output layer is only a filtering
    operation that has no effect on the actual computation. Indeed, the output is
    already decided by the linear block, while the step function is employed only
    to impose a binary threshold. Moreover, in this analysis, we are considering only
    single-value outputs (even if there are multi-class variants) because our goal
    is to show the dynamics and also the limitations, before moving to more generic
    architectures that can be used to solve extremely complex problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所有向量都是惯例的列向量；因此，点积*w^Tx[i]*将输入转换为一个标量，然后加上偏差，使用步进函数获得二进制输出，当*z > 0*时输出1，否则输出0。此时，一个读者可能会反对说步进函数是非线性的；然而，应用于输出层的非线性只是一种过滤操作，对实际计算没有影响。确实，输出已经由线性块决定，而步进函数仅用于施加二进制阈值。此外，在这个分析中，我们只考虑单值输出（即使有多类变体），因为我们的目标是展示动态和局限性，然后再转向可以用来解决极其复杂问题的更通用的架构。
- en: 'A perceptron can be trained with an online algorithm (even if the dataset is
    finite) but it''s also possible to employ an offline approach that repeats for
    a fixed number of iterations or until the total error becomes smaller than a predefined
    threshold. The procedure is based on the squared error loss function (remember
    that, conventionally, the term *loss* is applied to single samples, while the
    term *cost* refers to the sum/average of every single loss):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机可以用在线算法（即使数据集是有限的）进行训练，但也可以采用离线方法，该方法重复固定次数的迭代，或者直到总误差小于预定义的阈值。该过程基于平方误差损失函数（记住，传统上，术语*loss*应用于单个样本，而术语*cost*指的是每个单个损失的求和/平均值）：
- en: '![](img/2bf91875-302e-4805-8124-d040e18149ee.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bf91875-302e-4805-8124-d040e18149ee.png)'
- en: 'When a sample is presented, the output is computed, and if it is wrong, a weight
    correction is applied (otherwise the step is skipped). For simplicity, we don''t
    consider the bias, as it doesn''t affect the procedure. Our goal is to correct
    the weights so as to minimize the loss. This can be achieved by computing the
    partial derivatives with respect to *w[i]*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个样本被呈现时，会计算输出，如果输出错误，则应用权重校正（否则跳过该步骤）。为了简化，我们不考虑偏差，因为它不影响过程。我们的目标是校正权重以最小化损失。这可以通过计算相对于
    *w[i]* 的偏导数来实现：
- en: '![](img/ef339462-311a-4e70-8e75-2577230ae0d7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef339462-311a-4e70-8e75-2577230ae0d7.png)'
- en: 'Let''s suppose that *w^((0)) = (0, 0)* (ignoring the bias) and the sample,
    *x = (1, 1)*, has *y = 1*. The perceptron misclassifies the sample, because *sign(w^Tx)
    = 0*. The partial derivatives are both equal to -1; therefore, if we subtract
    them from the current weights, we obtain *w^((1))* *= (1, 1)* and now the sample
    is correctly classified because *sign(w^Tx) = 1*. Therefore, including a learning
    rate *η*, the weight update rule becomes as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *w^((0)) = (0, 0)*（忽略偏差）和样本 *x = (1, 1)*，其 *y = 1*。感知器错误地分类了该样本，因为 *sign(w^Tx)
    = 0*。偏导数都等于 -1；因此，如果我们从当前权重中减去它们，我们得到 *w^((1))* *= (1, 1)*，现在样本被正确分类，因为 *sign(w^Tx)
    = 1*。因此，包括学习率 *η*，权重更新规则如下：
- en: '![](img/5c35fd06-e3ac-406f-be6b-786b945c9a02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5c35fd06-e3ac-406f-be6b-786b945c9a02.png)'
- en: When a sample is misclassified, the weights are corrected proportionally to
    the difference between actual linear output and true label. This is a variant
    of a learning rule called the **delta rule**, which represented the first step
    toward the most famous training algorithm, employed in almost any supervised deep
    learning scenario (we're going to discuss it in the next sections). The algorithm
    has been proven to converge to a stable solution in a finite number of states
    as the dataset is linearly separable. The formal proof is quite tedious and very
    technical, but the reader who is interested can find it in *Perceptrons, Minsky
    M. L., Papert S. A., The MIT Press*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个样本被错误分类时，权重会根据实际线性输出和真实标签之间的差异成比例地校正。这是称为 **delta规则** 的学习规则的一个变体，它代表了最著名的训练算法的第一步，该算法在几乎任何监督深度学习场景中都被使用（我们将在下一节中讨论它）。该算法已被证明在数据集线性可分的情况下，在有限的状态数内收敛到稳定解。形式证明相当繁琐且非常技术性，但感兴趣的读者可以在
    *《感知器》，Minsky M. L.，Papert S. A.，麻省理工学院出版社* 中找到。
- en: In this chapter, the role of the learning rate becomes more and more important,
    in particular when the update is performed after the evaluation of a single sample
    (like in a perceptron) or a small batch. In this case, a high learning rate (that
    is, one greater than 1.0) can cause an instability in the convergence process
    because of the magnitude of the single corrections. When working with neural networks,
    it's preferable to use a small learning rate and repeat the training session for
    a fixed number of epochs. In this way, the single corrections are limited, and
    only if they are *confirmed* by the majority of samples/batches, they can become
    stable, driving the network to converge to an optimal solution. If, instead, the
    correction is the consequence of an outlier, a small learning rate can limit its
    action, avoiding destabilizing the whole network only for a few noisy samples.
    We are going to discuss this problem in the next sections.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，学习率的作用变得越来越重要，特别是在单个样本（如感知器）或小批量评估之后执行更新时。在这种情况下，高学习率（即大于 1.0 的值）可能会因为单个校正的幅度而导致收敛过程中的不稳定性。在处理神经网络时，最好使用较小的学习率并重复固定数量的训练周期。这样，单个校正被限制在较小的范围内，并且只有当它们被大多数样本/批量
    *确认* 时，它们才能变得稳定，推动网络收敛到最佳解。相反，如果校正是异常值的结果，较小的学习率可以限制其作用，避免仅因为几个噪声样本而破坏整个网络。我们将在下一节中讨论这个问题。
- en: 'Now, we can describe the full perceptron algorithm and close the paragraph
    with some important considerations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以描述完整的感知器算法，并在一些重要考虑下结束本段：
- en: Select a value for the learning rate *η* (such as `0.1`).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个学习率 *η* 的值（例如 `0.1`）。
- en: Append a constant column (set to `1.0`) to the sample vector *X*. Therefore
    *X[b] ∈ ℜ^(M × (n+1))*.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个常数列（设置为 `1.0`）添加到样本向量 *X* 中。因此 *X[b] ∈ ℜ^(M × (n+1))*。
- en: Initialize the weight vector *w ∈ ℜ^(n+1)* with random values sampled from a
    normal distribution with a small variance (such as `0.05`).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用具有小方差（例如 `0.05`）的正态分布随机值初始化权重向量 *w ∈ ℜ^(n+1)*。
- en: Set an error threshold `Thr` (such as `0.0001`).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个错误阈值 `Thr`（例如 `0.0001`）。
- en: Set a maximum number of iterations *N[i]*.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置最大迭代次数 *N[i]*。
- en: Set `i = 0`.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `i = 0`。
- en: Set `e = 1.0`.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `e = 1.0`。
- en: 'While *i < N[i]* and *e > Thr*:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *i < N[i]* 且 *e > Thr*：
- en: Set `e = 0.0`.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `e = 0.0`。
- en: 'For *k=1* to *M*:'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *k=1* 到 *M*：
- en: Compute the linear output *l[k] = w^Tx[k]* and the threshold one *t[k] = sign(l[k])*.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算线性输出 *l[k] = w^Tx[k]* 和阈值输出 *t[k] = sign(l[k])*。
- en: If *t[k] != y*[*k*:]
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t[k] != y[k]*：
- en: Compute *Δw[j] = η(l[k] - y[k])x[k]^((j))*.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *Δw[j] = η(l[k] - y[k])x[k]^((j))*。
- en: Update the weight vector.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重向量。
- en: Set *e += (l[k] - y[k])²* (alternatively it's possible to use the absolute value
    *|l[k] - y[k]|*).
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *e += (l[k] - y[k])²*（或者也可以使用绝对值 *|l[k] - y[k]|*）。
- en: Set `e /= M`.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `e /= M`。
- en: 'The algorithm is very simple, and the reader should have noticed an analogy
    with a logistic regression. Indeed, this method is based on a structure that can
    be considered as a perceptron with a sigmoid output activation function (that
    outputs a real value that can be considered as a probability). The main difference
    is the training strategy—in a logistic regression, the correction is performed
    after the evaluation of a cost function based on the negative log likelihood:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法非常简单，读者应该已经注意到了它与逻辑回归的类比。事实上，这种方法基于一个可以被认为是具有sigmoid输出激活函数的感知器结构（该函数输出一个可以被认为是概率的实数值）。主要区别在于训练策略——在逻辑回归中，校正是在基于负对数似然度的损失函数评估之后进行的：
- en: '![](img/2476673b-93c2-4fba-a649-9af13799e415.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2476673b-93c2-4fba-a649-9af13799e415.png)'
- en: This cost function is the well-known cross-entropy and, in the first chapter,
    we showed that minimizing it is equivalent to reducing the Kullback-Leibler divergence
    between the true and predicted distribution. In almost all deep learning classification
    tasks, we are going to employ it, thanks to its robustness and convexity (this
    is a convergence guarantee in a logistic regression, but unfortunately the property
    is normally lost in more complex architectures).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数是众所周知的交叉熵，在第一章中，我们展示了最小化它是等同于减少真实分布和预测分布之间的Kullback-Leibler散度。在几乎所有深度学习分类任务中，我们将利用它，归功于其鲁棒性和凸性（这是逻辑回归中的收敛保证，但不幸的是，在更复杂的架构中，这个属性通常会被丢失）。
- en: Example of a perceptron with Scikit-Learn
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn的感知器示例
- en: 'Even if the algorithm is very simple to implement from scratch, I prefer to
    employ the Scikit-Learn implementation `Perceptron`, so as to focus the attention
    on the limitations that led to non-linear neural networks. The *historical* problem
    that showed the main weakness of a perceptron is based on the XOR dataset. Instead
    of explaining, it''s better to build it and visualize the structure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使从零开始实现这个算法非常简单，我还是更喜欢使用Scikit-Learn的`Perceptron`实现，以便将注意力集中在导致非线性神经网络局限性的问题上。展示感知器主要弱点的*历史性*问题基于XOR数据集。与其解释，不如构建它并可视化其结构：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The plot showing the true labels is shown in the following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显示真实标签的图表如下所示：
- en: '![](img/f047a4fc-fff1-4018-b6ea-566443d4ca3e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f047a4fc-fff1-4018-b6ea-566443d4ca3e.png)'
- en: Example of XOR dataset
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: XOR数据集示例
- en: 'As it''s possible to see, the dataset is split into four blocks that are organized
    as the output of a logical XOR operator. Considering that the separation hypersurface
    of a two-dimensional perceptron (as well as the one of a logistic regression)
    is a line; it''s easy to understand that any possible final configuration can
    achieve an accuracy that is about 50% (a random guess). To have a confirmation,
    let''s try to solve this problem:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，数据集被分为四个块，这些块的组织方式类似于逻辑异或运算符的输出。考虑到二维感知器（以及逻辑回归）的分离超曲面是一条线；很容易理解任何可能的最终配置都能达到大约50%的准确率（一个随机猜测）。为了确认这一点，让我们尝试解决这个问题：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Scikit-Learn implementation offers the possibility to add a regularization
    term (see [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine Learning
    Models Fundamentals*) through the parameter `penalty` (it can be `''l1''`, `''l2''`
    or `''elasticnet''`) to avoid overfitting and improve the convergence speed (the
    strength can be specified using the parameter `alpha`). This is not always necessary,
    but as the algorithm is offered in a production-ready package, the designers decided
    to add this feature. Nevertheless, the average cross-validation accuracy is slightly
    higher than 0.5 (the reader is invited to test any other possible hyperparameter
    configuration). The corresponding plot (that can change with different random
    states or subsequent experiments) is shown in the following diagram:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn实现提供了通过参数`penalty`（可以是`'l1'`、`'l2'`或`'elasticnet'`）添加正则化项的可能性（见[第1章](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml)，*机器学习模型基础*），以避免过拟合并提高收敛速度（强度可以使用参数`alpha`指定）。这并不总是必要的，但由于该算法提供的是一个生产就绪的包，设计者决定添加此功能。尽管如此，平均交叉验证准确率略高于0.5（读者被邀请测试任何其他可能的超参数配置）。相应的图（可能会因不同的随机状态或后续实验而改变）如下所示：
- en: '![](img/b082166a-bf90-4f69-95d5-3102b88ebf13.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b082166a-bf90-4f69-95d5-3102b88ebf13.png)'
- en: XOR dataset labeled using a perceptron
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用感知器标记的XOR数据集
- en: It's obvious that a perceptron is another linear model without specific peculiarities,
    and its employment is discouraged in favor of other algorithms like logistic regression
    or SVM. After 1957, for a few years, many researchers didn't hide their delusion
    and considered the neural network like a promise never fulfilled. It was necessary
    to wait until a simple modification to the architecture, together with a powerful
    learning algorithm, opened officially the door of a new fascinating machine learning
    branch (later called **deep learning**).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，感知器是另一个没有特定特性的线性模型，并且其使用被劝阻，以其他算法（如逻辑回归或SVM）为优先。1957年之后，在几年里，许多研究人员并没有隐藏他们的幻想，将神经网络视为一个从未实现的承诺。必须等到对架构的简单修改，以及一个强大的学习算法，才正式打开了通往一个新迷人机器学习分支的大门（后来称为**深度学习**）。
- en: In Scikit-Learn > 0.19, the class `Perceptron` allows adding `max_iter` or `tol`
    (tolerance) parameters. If not specified, a warning will be issued to inform the
    reader about the future behavior. This piece of information doesn't affect the
    actual results.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn > 0.19中，`Perceptron`类允许添加`max_iter`或`tol`（容差）参数。如果没有指定，将发出警告，通知读者未来的行为。这条信息不会影响实际结果。
- en: Multilayer perceptrons
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'The main limitation of a perceptron is its linearity. How is it possible to
    exploit this kind of architecture by removing such a constraint? The solution
    is easier than any speculation. Adding at least a non-linear layer between input
    and output leads to a highly non-linear combination, parametrized with a larger
    number of variables. The resulting architecture, called **Mu****ltilayer Perceptron**
    (**MLP**) and containing a single (only for simplicity) **Hidden Layer***,* is
    shown in the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的主要限制是其线性。如何通过去除这种约束来利用这种架构？解决方案比任何猜测都简单。在输入和输出之间添加至少一个非线性层会导致一个高度非线性的组合，使用更多的变量进行参数化。这种架构称为**多层感知器**（**MLP**），包含一个（为了简单起见）**隐藏层**，如下所示：
- en: '![](img/7a6cfef0-a339-4c00-8abb-e469a2fb4b31.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a6cfef0-a339-4c00-8abb-e469a2fb4b31.png)'
- en: This is a so-called **feed-forward network**, meaning that the flow of information
    begins in the first layer, proceeds always in the same direction and ends at the
    output layer. Architectures that allow a partial feedback (for example, in order
    to implement a local memory) are called **recurrent** **networks** and will be
    analyzed in the next chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个所谓的**前馈网络**，意味着信息的流动从第一层开始，始终沿同一方向进行，并在输出层结束。允许部分反馈（例如，为了实现局部记忆）的架构称为**循环****网络**，将在下一章进行分析。
- en: 'In this case, there are two weight matrices, *W* and *H*, and two corresponding
    bias vectors, *b* and *c*. If there are *m* hidden neurons, *x[i] ∈ ℜ^(n × 1)*
    (column vector), and *y[i] ∈ ℜ^(k × 1)*, the dynamics are defined by the following
    transformations:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有两个权重矩阵，*W* 和 *H*，以及两个相应的偏置向量，*b* 和 *c*。如果有 *m* 个隐藏神经元，*x[i] ∈ ℜ^(n ×
    1)*（列向量），和 *y[i] ∈ ℜ^(k × 1)*，动态由以下转换定义：
- en: '![](img/a844411b-2b60-42ba-9292-5d9fde341e6e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a844411b-2b60-42ba-9292-5d9fde341e6e.png)'
- en: A fundamental condition for any MLP is that at least one hidden-layer activation
    function *f[h](•)* is non-linear. It's straightforward to prove that *m* linear
    hidden layers are equivalent to a single linear network and, hence, an MLP falls
    back into the case of a standard perceptron. Conventionally, the activation function
    is fixed for a given layer, but there are no limitations in their combinations.
    In particular, the output activation is normally chosen to meet a precise requirement
    (such as multi-label classification, regression, image reconstruction, and so
    on). That's why the first step of this analysis concerns the most common activation
    functions and their features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何多层感知器（MLP）来说，一个基本条件是至少有一个隐藏层激活函数 *f[h](•)* 是非线性的。证明 m 个线性隐藏层等价于单个线性网络是直接的，因此，MLP
    会退化为标准感知器的情形。传统上，给定层的激活函数是固定的，但它们的组合没有限制。特别是，输出激活通常被选择以满足精确的要求（如多标签分类、回归、图像重建等）。这就是为什么这一分析的第一步关注最常用的激活函数及其特性。
- en: Activation functions
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: In general, any continuous and differentiable function could be employed as
    activation; however, some of them have particular properties that allow achieving
    a good accuracy while improving the learning process speed. They are commonly
    used in the state-of-the-art models, and it's important to understand their properties
    in order to make the most reasonable choice.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，任何连续且可微分的函数都可以用作激活函数；然而，其中一些具有特定的性质，可以在提高学习过程速度的同时实现良好的精度。它们在最新的模型中普遍使用，理解它们的特性对于做出最合理的选择至关重要。
- en: Sigmoid and hyperbolic tangent
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid 和双曲正切
- en: 'These two activations are very similar but with an important difference. Let''s
    start defining them:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种激活函数非常相似，但有一个重要的区别。让我们先来定义它们：
- en: '![](img/818ed54f-f955-4666-aa7c-ee6feb633b3c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/818ed54f-f955-4666-aa7c-ee6feb633b3c.png)'
- en: 'The corresponding plots are shown in the following diagram:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的图示如下所示：
- en: '![](img/33cd940d-83ba-4471-b926-c6159dfcf26b.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33cd940d-83ba-4471-b926-c6159dfcf26b.png)'
- en: Sigmoid and hyperbolic tangent plots
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 和双曲正切函数图
- en: A sigmoid *σ(x)* is bounded between 0 and 1, with two asymptotes (*σ(x) → 0
    when x → -∞* and *σ(x) → 1 when x → ∞*). Similarly, the hyperbolic tangent (`tanh`)
    is bounded between -1 and 1 with two asymptotes corresponding to the extreme values.
    Analyzing the two plots, we can discover that both functions are almost linear
    in a short range (about *[-2, 2]*), and they become almost flat immediately after.
    This means that the gradient is high and about constant when *x* has small values
    around 0 and it falls down to about 0 for larger absolute values. A sigmoid perfectly
    represents a probability or a set of weights that must be bounded between 0 and
    1, and therefore, it can be a good choice for some output layers. However, the
    hyperbolic tangent is completely symmetric, and, for optimization purposes, it's
    preferable because the performances are normally superior. This activation function
    is often employed in intermediate layers, whenever the input is normally small.
    The reason will be clear when the back-propagation algorithm is analyzed; however,
    it's obvious that large absolute inputs lead to almost constant outputs, and as
    the gradient is about 0, the weight correction can become extremely slow (this
    problem is formally known as **vanishing gradients**). For this reason, in many
    real-world applications, the next family of activation functions is often employed.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数 *σ(x)* 被限制在 0 和 1 之间，有两个渐近线（当 x → -∞ 时 *σ(x) → 0*，当 x → ∞ 时 *σ(x)
    → 1*）。同样，双曲正切（`tanh`）被限制在 -1 和 1 之间，有两个渐近线对应于极值。分析这两个图，我们可以发现，这两个函数在短范围内几乎呈线性（大约在
    *[-2, 2]*），并且立即变得几乎平坦。这意味着当 *x* 在 0 附近的值较小时，梯度很高且大致恒定，而对于较大的绝对值，梯度下降到大约 0。Sigmoid
    函数完美地表示一个概率或一组必须在 0 和 1 之间有界的权重，因此，它可以是一些输出层的良好选择。然而，双曲正切函数完全对称，并且，出于优化的目的，它更可取，因为性能通常更优越。这种激活函数通常在输入通常较小的情况下用于中间层。当分析反向传播算法时，原因将变得清晰；然而，显然，大的绝对输入会导致几乎恒定的输出，并且由于梯度大约为
    0，权重校正可以变得极其缓慢（这个问题正式称为**梯度消失**）。因此，在许多实际应用中，通常采用下一系列的激活函数。
- en: Rectifier activation functions
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩形激活函数
- en: 'These functions are all linear (or quasi-linear for Swish) when *x > 0*, while
    they differ when *x < 0*. Even if some of them are differentiable when *x = 0*,
    the derivative is set to `0` in this case. The most common functions are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数在*x > 0*时都是线性的（或Swish的准线性），而当*x < 0*时则不同。即使其中一些在*x = 0*时是可微的，但在此情况下导数被设置为`0`。最常见的函数如下：
- en: '![](img/87aa3fd6-522b-4ebb-9acd-2325e25a7102.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87aa3fd6-522b-4ebb-9acd-2325e25a7102.png)'
- en: 'The corresponding plots are shown in the following diagram:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的图表如下所示：
- en: '![](img/da31e807-da0a-4604-bf32-c89f3ac08674.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da31e807-da0a-4604-bf32-c89f3ac08674.png)'
- en: The basic function (and also the most commonly employed) is the ReLU, which
    has a constant gradient when *x > 0*, while it is null for *x < 0*. This function
    is very often employed in visual processing when the input is normally greater
    than 0 and has the extraordinary advantage to mitigate the vanishing gradient
    problem, as a correction based on the gradient is always possible. On the other
    side, ReLU is null (together with its first derivative) when *x < 0*, therefore
    every negative input doesn't allow any modification. In general, this is not an
    issue, but there are some deep networks that perform much better when a small
    negative gradient was allowed. This consideration drove to the other variants,
    which are characterized by the presence of the hyperparameter *α*, that controls
    the strength of the *negative tail*. Common values between 0.01 and 0.1 allow
    a behavior that is almost identical to ReLU, but with the possibility of a small
    weight update when *x < 0*. The last function, called Swish and proposed in *Searching
    for Activation Functions, Ramachandran P., Zoph P., Le V. L., arXiv:1710.05941
    [cs.NE]*, is based on the sigmoid and offers the extra advantage to converge to
    0 when *x → 0*, so the *non-null effect* is limited to a short region bounded
    between *[-b, 0]* with *b > 0*. This function can improve the performance of some
    particular visual processing deep networks, as discussed in the aforementioned
    paper. However, I always suggest starting the analysis with ReLU (that is very
    robust and computationally inexpensive) and switch to an alternative only if no
    other techniques can improve the performance of a model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 基本功能（同时也是最常用的）是ReLU，当*x > 0*时具有恒定的梯度，而当*x < 0*时则为零。这个函数在视觉处理中非常常用，因为输入通常大于0，并且具有减轻梯度消失问题的非凡优势，因为基于梯度的校正总是可能的。另一方面，当*x
    < 0*时，ReLU（及其一阶导数）为零，因此每个负输入都不允许任何修改。一般来说，这不是一个问题，但有些深度网络在允许小的负梯度时表现更好。这种考虑导致了其他变体，这些变体以存在超参数*α*为特征，该参数控制负尾的强度。介于0.01和0.1之间的常见值允许几乎与ReLU相同的行为，但*x
    < 0*时允许小的权重更新。最后一个函数称为Swish，由*Searching for Activation Functions, Ramachandran
    P., Zoph P., Le V. L., arXiv:1710.05941 [cs.NE]*提出，基于sigmoid，并提供了当*x → 0*时收敛到0的额外优势，因此*非零效应*被限制在由*[-b,
    0]*（其中*b > 0*）界定的短区域内。这个函数可以提高某些特定视觉处理深度网络的性能，如上述论文中所述。然而，我总是建议从ReLU（非常稳健且计算成本低）开始分析，只有在没有其他技术可以提高模型性能的情况下才切换到替代方案。
- en: Softmax
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax
- en: 'This function characterized the output layer of almost all classification networks,
    as it can immediately represent a discrete probability distribution. If there
    are *k* outputs *y[i]*, the softmax is computed as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数几乎定义了所有分类网络的输出层，因为它可以立即表示一个离散概率分布。如果有*k*个输出*y[i]*，softmax的计算如下：
- en: '![](img/1271b390-d7d7-4eee-a438-565e3ddf6ee6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1271b390-d7d7-4eee-a438-565e3ddf6ee6.png)'
- en: 'In this way, the output of a layer containing *k* neurons is normalized so
    that the sum is always 1\. It goes without saying that, in this case, the best
    cost function is the cross-entropy. In fact, if all true labels are represented
    with a one-hot encoding, they implicitly become probability vectors with 1 corresponding
    to the true class. The goal of the classifier is hence to reduce the discrepancy
    between the training distribution of its output by minimizing the function (see
    [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine Learning Models
    Fundamentals*, for further information):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，包含*k*个神经元的层的输出被归一化，使得总和始终为1。不言而喻，在这种情况下，最好的损失函数是交叉熵。事实上，如果所有真实标签都使用one-hot编码表示，它们隐式地成为概率向量，其中1对应于真实类别。因此，分类器的目标是通过最小化函数来减少其输出的训练分布之间的差异（有关更多信息，请参阅[第1章](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml)，*机器学习模型基础*）：
- en: '![](img/2f10de73-0460-4cde-9e68-370467df39b4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f10de73-0460-4cde-9e68-370467df39b4.png)'
- en: Back-propagation algorithm
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: We can now discuss the training approach employed in an MLP (and almost all
    other neural networks). This algorithm is more a methodology than an actual one;
    therefore I prefer to define the main concepts without focusing on a particular
    case. The reader who is interested in implementing it will be able to apply the
    same techniques to different kinds of networks with minimal effort (assuming that
    all requirements are met).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以讨论在多层感知器（以及几乎所有其他神经网络）中使用的训练方法。这个算法与其说是实际算法，不如说是方法论；因此，我更喜欢定义主要概念，而不专注于特定案例。对实现感兴趣的读者将能够以最小的努力将相同的技巧应用于不同类型的网络（假设所有要求都已满足）。
- en: 'The goal of a training process using a deep learning model is normally achieved
    by minimizing a cost function. Let''s suppose to have a network parameterized
    with a global vector θ, the cost function (using the same notation for loss and
    cost but with different parameters to disambiguate) is defined as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习模型进行训练的过程的目标通常是通过最小化成本函数来实现的。假设我们有一个用全局向量 θ 参数化的网络，成本函数（使用与损失相同的符号，但使用不同的参数来消除歧义）定义如下：
- en: '![](img/5d0a5dfb-1bf8-4371-816c-06620a147ce3.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d0a5dfb-1bf8-4371-816c-06620a147ce3.png)'
- en: 'We have already explained that the minimization of the previous expression
    (which is the empirical risk) is a way to minimize the real expected risk and,
    therefore, to maximize the accuracy. Our goal is, hence, to find an optimal parameter
    set so that the following applies:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释过，最小化前面的表达式（这是经验风险）是减少实际预期风险的一种方法，因此可以最大化准确性。因此，我们的目标是找到一个最优参数集，使得以下成立：
- en: '![](img/d6088201-8503-45cc-939a-ebb7c87122b0.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6088201-8503-45cc-939a-ebb7c87122b0.png)'
- en: 'If we consider a single loss function (associated with a sample x[i] and a
    true label *y[i]*), we know that such a function can be expressed with an explicit
    dependence on the predicted value:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑一个单一的损失函数（与样本 x[i] 和真实标签 *y[i]* 相关），我们知道这样的函数可以用对预测值的显式依赖来表示：
- en: '![](img/2ed57317-d234-4ff0-baec-8270e920a653.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ed57317-d234-4ff0-baec-8270e920a653.png)'
- en: 'Now, the parameters have been *embedded* into the prediction. From calculus
    (without an excessive mathematical rigor that can be found in many books about
    optimization techniques), we know that the gradient of *L*, a scalar function,
    computed at any point (we are assuming the *L* is differentiable) is a vector
    with components:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，参数已经被嵌入到预测中。从微积分（不需要像许多关于优化技术书籍中所找到的那样过度的数学严谨性）我们知道，*L* 的梯度，一个标量函数，在任意点（我们假设
    *L* 是可微的）计算出的梯度是一个具有以下分量的向量：
- en: '![](img/3686bcc1-3894-41fd-88ef-97a7cb63df4f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3686bcc1-3894-41fd-88ef-97a7cb63df4f.png)'
- en: 'As *∇L* points always in the direction of the closest maximum, so the negative
    gradient points in the direction of the closest minimum. Hence, if we compute
    the gradient of *L*, we have a ready-to-use piece of information that can be used
    to minimize the cost function. Before proceeding, it''s useful to expose an important
    mathematical property called the **chain rule of derivatives**:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *∇L* 总是指向最近的极大值方向，因此负梯度指向最近的极小值方向。因此，如果我们计算 *L* 的梯度，我们就得到了一个可以直接用于最小化成本函数的信息。在继续之前，揭示一个重要的数学性质——**导数的链式法则**是有用的：
- en: '![](img/12aa74d3-62b3-489d-ae4e-da19eace1537.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/12aa74d3-62b3-489d-ae4e-da19eace1537.png)'
- en: 'Now, let''s consider a single step in an MLP (starting from the bottom) and
    let''s exploit the chain rule:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个多层感知器（MLP）的单步（从底部开始）并利用链式法则：
- en: '![](img/9bfe13f3-ac2c-4200-bf7e-3043d56a51cb.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9bfe13f3-ac2c-4200-bf7e-3043d56a51cb.png)'
- en: 'Each component of the vector *y* is independent of the others, so we can simplify
    the example by considering only an output value:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 *y* 的每个分量都与其他分量独立，因此我们可以通过只考虑一个输出值来简化示例：
- en: '![](img/2707a46b-ef56-45ca-9655-91d2b2494f8b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2707a46b-ef56-45ca-9655-91d2b2494f8b.png)'
- en: 'In the previous expression (discarding the bias), there are two important elements—the
    weights, *h[j]* (that are the columns of *H*), and the expression, *z[j]*, which
    is a function of the previous weights. As *L* is, in turn, a function of all predictions,
    *y[i]*, applying the chain rule (using the variable *t* as the generic argument
    of the activation functions), we get the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中（忽略偏差），有两个重要元素——权重，*h[j]*（它们是 *H* 的列），以及表达式，*z[j]*，它是先前权重的函数。由于 *L*
    又是所有预测值 *y[i]* 的函数，通过应用链式法则（使用变量 *t* 作为激活函数的通用参数），我们得到以下结果：
- en: '![](img/977e1cdb-f888-4a18-80a7-e78f83a6f59c.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/977e1cdb-f888-4a18-80a7-e78f83a6f59c.png)'
- en: 'As we normally cope with vectorial functions, it''s easier to express this
    concept using the gradient operator. Simplifying the transformations performed
    by a generic layer, we can express the relations (with respect to a row of *H*,
    so to a weight vector *h[i]* corresponding to a hidden unit, *z[i]*) as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通常处理向量函数，使用梯度算子表达这个概念更容易。简化通用层执行的转换，我们可以将关系（相对于 *H* 的一行，因此相对于对应于隐藏单元 *z[i]*
    的权重向量 *h[i]*）表示如下：
- en: '![](img/907957cb-c2f1-427c-bfdd-b5e7cd4eb261.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/907957cb-c2f1-427c-bfdd-b5e7cd4eb261.png)'
- en: 'Employing the gradient and considering the vectorial output *y* can be written
    as *y = (y[1], y[2], ..., y[m])*, we can derive the following expression:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度并考虑向量输出 *y* 可以表示为 *y = (y[1], y[2], ..., y[m])*，我们可以推导出以下表达式：
- en: '![](img/cc908237-c2dc-4d74-ae2c-b2cf8669681e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cc908237-c2dc-4d74-ae2c-b2cf8669681e.png)'
- en: 'In this way we get all the components of the gradient of *L* computed with
    respect to the weightvectors, *h[i]*. If we move back, we can derive the expression
    of *z[j]*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就得到了相对于权重向量 *h[i]* 的 *L* 的梯度的所有分量。如果我们回溯，我们可以推导出 *z[j]* 的表达式：
- en: '![](img/edac1e1f-6f2d-46e9-9e4f-d3ffe2230e4e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/edac1e1f-6f2d-46e9-9e4f-d3ffe2230e4e.png)'
- en: 'Reapplying the chain rule, we can compute the partial derivative of *L* with
    respect to *w[pj]* (to avoid confusion, the argument of the prediction *y[i]*
    is called *t[1]*, while the argument of *z[j]* is called *t[2]*):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 重新应用链式法则，我们可以计算 *L* 对 *w[pj]* 的偏导数（为了避免混淆，预测 *y[i]* 的参数称为 *t[1]*，而 *z[j]* 的参数称为
    *t[2]*）：
- en: '![](img/351d0b2a-0865-4cf6-b84b-71a637a5c261.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/351d0b2a-0865-4cf6-b84b-71a637a5c261.png)'
- en: Observing this expression (that can be easily rewritten using the gradient)
    and comparing it with the previous one, it's possible to understand the philosophy
    of the **back-propagation algorithm**, presented for the first time in *Learning
    representations by back-propagating errors, Rumelhart D. E., Hinton G. E., Williams
    R. J., Nature 323/1986*. The samples are fed into the network and the cost function
    is computed. At this point, the process starts from the bottom, computing the
    gradients with respect to the closest weights and reusing a part of the calculation
    δ[i] (proportional to the error) to move back until the first layer is reached.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个表达式（可以使用梯度轻松重写）并与前面的一个进行比较，可以理解**反向传播算法**的哲学，该算法首次在 *通过反向传播错误学习表示，Rumelhart
    D. E.，Hinton G. E.，Williams R. J.，Nature 323/1986* 中提出。样本被输入到网络中，并计算损失函数。在这个时候，过程从底部开始，计算相对于最近权重的梯度，并重用计算
    δ[i]（与误差成比例）的一部分来回溯，直到达到第一层。
- en: The correction is indeed propagated from the source (the cost function) to the
    origin (the input layer), and the effect is proportional to the responsibility
    of each different weight (and bias).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，修正是从源（损失函数）传播到起源（输入层），并且效果与每个不同权重的责任（和偏差）成比例。
- en: Considering all the possible different architectures, I think that writing all
    the equations for a single example is useless. The methodology is conceptually
    simple, and it's purely based on the chain rule of derivatives. Moreover, all
    existing frameworks, such as Tensorflow, Caffe, CNTK, PyTorch, Theano, and so
    on, can compute the gradients for all weights of a complete network with a single
    operation, so as to allow the user to focus attention on more pragmatic problems
    (like finding the best way to avoid overfitting and improving the training process).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑所有可能的不同架构，我认为为单个示例写出所有方程是无用的。方法论在概念上是简单的，它纯粹基于导数的链式法则。此外，所有现有的框架，如Tensorflow、Caffe、CNTK、PyTorch、Theano等，都可以通过单个操作计算整个网络的全部权重的梯度，从而使用户能够关注更实际的问题（如找到避免过拟合和改进训练过程的最佳方法）。
- en: 'A very important phenomenon that is worth considering was already outlined
    in the previous section and now it should be clearer: the chain rule is based
    on multiplications; therefore, when the gradients start to become smaller than
    1, the multiplication effect forces the last values to be close to 0\. This problem
    is known as **vanishing gradients** and can really stop the training process of
    very deep models that use saturating activation functions (like `sigmoid` or `tanh`).
    Rectifier units provide a good solution to many specific issues, but sometimes
    when functions like hyperbolic tangent are necessary, other methods, like normalization,
    must be employed to mitigate the phenomenon. We are going to discuss some specific
    techniques in this chapter and in the next one, but a generic best practice is
    to work always with normalized datasets and, if necessary, also testing the effect
    of whitening.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个章节中已经概述的一个重要现象值得考虑，现在应该更清晰了：链式法则基于乘法；因此，当梯度开始变得小于1时，乘法效应会迫使最后值接近0。这个问题被称为**梯度消失**，并且真的会停止使用饱和激活函数（如`sigmoid`或`tanh`）的非常深层的模型的训练过程。整流单元为许多特定问题提供了良好的解决方案，但有时当需要像双曲正切这样的函数时，必须采用其他方法，如归一化，以减轻这种现象。我们将在本章和下一章讨论一些具体技术，但一个通用的最佳实践是始终使用归一化数据集，并在必要时也测试白化的效果。
- en: Stochastic gradient descent
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Once the gradients have been computed, the cost function can be *moved* in the
    direction of its minimum. However, in practice, it is better to perform an update
    after the evaluation of a fixed number of training samples (batch). Indeed, the
    algorithms that are normally employed don't compute the global cost for the whole
    dataset, because this operation could be very computationally expensive. An approximation
    is obtained with partial steps, limited to the experience accumulated with the
    evaluation of a small subset. According to some literature, the expression **stochastic
    gradient descent** (**SGD**) should be used only when the update is performed
    after every single sample. When this operation is carried out on every *k* sample,
    the algorithm is also known as **mini-batch gradient descent**; however, conventionally
    SGD is referred to all batches containing *k ≥ 1* samples, and we are going to
    use this expression from now on.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出了梯度，成本函数就可以*移动*到其最小值的方向。然而，在实践中，在评估了一定数量的训练样本（批次）之后进行更新会更好。确实，通常使用的算法不会计算整个数据集的全局成本，因为这个操作可能非常计算量大。通过部分步骤获得近似值，这些步骤仅限于通过评估小部分数据集积累的经验。根据一些文献，**随机梯度下降**（**SGD**）应该只在每次更新后对每个单独的样本执行时使用。当这个操作在每*k*个样本上执行时，该算法也被称为**小批量梯度下降**；然而，传统上SGD指的是包含*k
    ≥ 1*个样本的所有批次，并且我们现在将使用这个表达式。
- en: 'The process can be expressed considering a partial cost function computed using
    a batch containing *k* samples:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过考虑使用包含*k*个样本的批次计算的部分成本函数来表示这个过程：
- en: '![](img/dea3d8af-284f-4464-b8e0-17fa793615dd.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dea3d8af-284f-4464-b8e0-17fa793615dd.png)'
- en: 'The algorithm performs a gradient descent by updating the weights according
    to the following rule:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 算法通过根据以下规则更新权重来执行梯度下降：
- en: '![](img/d787dbc1-4a87-49bc-b7d7-6ecdfc897b97.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d787dbc1-4a87-49bc-b7d7-6ecdfc897b97.png)'
- en: 'If we start from an initial configuration *θ[start]*, the stochastic gradient
    descent process can be imagined like the path shown in the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从初始配置*θ[start]*开始，随机梯度下降过程可以想象成以下图中显示的路径：
- en: '![](img/b8c4f4f1-9db4-40be-8714-64da4b095753.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8c4f4f1-9db4-40be-8714-64da4b095753.png)'
- en: The weights are moved towards the minimum *θ*[*opt*,] with many subsequent corrections
    that could also be wrong considering the whole dataset. For this reason, the process
    must be repeated several times (epochs), until the validation accuracy reaches
    its maximum. In a perfect scenario, with a convex cost function L, this simple
    procedure converges to the optimal configuration. Unfortunately, a deep network
    is a very complex and non-convex function where plateaus and saddle points are
    quite common (see [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine
    Learning Models Fundamentals*). In such a scenario, a *vanilla* SGD wouldn't be
    able to find the global optimum and, in many cases, could not even find a close
    point. For example, in flat regions, the gradients can become so small (also considering
    the numerical imprecisions) as to slow down the training process until no change
    is possible (so *θ^((t+1)) ≈ θ^((t))*). In the next section, we are going to present
    some common and powerful algorithms that have been developed to mitigate this
    problem and dramatically accelerate the convergence of deep models.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 权重被移动到最小 *θ*[*opt*,]，随后会有许多后续的修正，考虑到整个数据集，这些修正也可能出错。因此，这个过程必须重复多次（epoch），直到验证准确率达到最大。在理想场景下，具有凸成本函数
    L，这个简单的程序会收敛到最优配置。不幸的是，深度网络是一个非常复杂且非凸的函数，其中平台和鞍点相当常见（参见[第1章](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml)，*机器学习模型基础*）。在这种情况下，普通的
    SGD 不会找到全局最优解，在许多情况下甚至找不到接近的点。例如，在平坦区域，梯度可以变得非常小（也考虑到数值不精确），以至于会减慢训练过程，直到没有变化可能（所以
    *θ^((t+1)) ≈ θ^((t))*）。在下一节中，我们将介绍一些常见的强大算法，这些算法已被开发出来以减轻这个问题并显著加速深度模型的收敛。
- en: Before moving on, it's important to mark two important elements. The first one
    concerns the learning rate, *η*. This hyperparameter plays a fundamental role
    in the learning process. As also shown in the figure, the algorithm proceeds jumping
    from a point to another one (which is not necessarily closer to the optimum).
    Together with the optimization algorithms, it's absolutely important to correctly
    tune up the learning rate. A high value (such as 1.0) could move the weights too
    rapidly increasing the instability. In particular, if a batch contains a few outliers
    (or simply non-dominant samples), a large *η* will consider them as representative
    elements, correcting the weights so to minimize the error. However, subsequent
    batches could better represent the data generating process, and, therefore, the
    algorithm must partially *revert* its modifications in order to compensate the
    wrong update. For this reason, the learning rate is usually quite small with common
    values bounded between 0.0001 and 0.01 (in some particular cases, *η = 0.1* can
    be also a valid choice). On the other side, a very small learning rate leads to
    minimum corrections, slowing down the training process. A good trade-off, which
    is often the best practice, is to let the learning rate decay as a function of
    the epoch. In the beginning, *η* can be higher, because the probability to be
    close to the optimum is almost null; so, larger jumps can be easily adjusted.
    While the training process goes on, the weights are progressively moved towards
    their final configuration and, hence, the corrections become smaller and smaller.
    In this case, large jumps should be avoided, preferring a fine-tuning. That's
    why the learning rate is decayed. Common techniques include the exponential decay
    or a linear one. In both cases, the initial and final values must be chosen according
    to the specific problem (testing different configurations) and the optimization
    algorithm. In many cases, the ratio between the start and end value is about 10
    or even larger.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，标记两个重要元素是很重要的。第一个与学习率*η*有关。这个超参数在学习过程中起着根本的作用。如图所示，算法从一个点跳到另一个点（这并不一定更接近最优解）。与优化算法一起，正确调整学习率至关重要。高值（如1.0）可能会使权重变化过快，增加不稳定性。特别是，如果一批数据包含一些异常值（或简单地是非主导样本），大的*η*会将它们视为代表性元素，通过调整权重以最小化误差。然而，后续批次可能更好地代表数据生成过程，因此，算法必须部分*撤销*其修改以补偿错误的更新。因此，学习率通常很小，常见值介于0.0001和0.01之间（在某些特定情况下，*η
    = 0.1*也可以是一个有效的选择）。另一方面，非常小的学习率会导致最小的修正，减慢训练过程。一个好的权衡，通常是最佳实践，是让学习率随着epoch的变化而衰减。一开始，*η*可以更高，因为接近最优解的概率几乎为零；因此，更大的跳跃可以轻松调整。当训练过程继续进行时，权重逐渐移动到它们的最终配置，因此，修正变得越来越小。在这种情况下，应避免大的跳跃，而偏好微调。这就是为什么学习率会衰减。常见的技术包括指数衰减或线性衰减。在这两种情况下，初始值和最终值必须根据具体问题（测试不同的配置）和优化算法来选择。在许多情况下，起始值和结束值之间的比率约为10或更大。
- en: Another important hyperparameter is the batch size. There are no silver bullets
    that allow us to automatically make the right choice, but some considerations
    can be made. As SGD is an approximate algorithm, larger batches drive to corrections
    that are probably more similar to the ones obtained considering the whole dataset.
    However, when the number of samples is extremely high, we don't expect the deep
    model to map them with a one-to-one association, but instead our efforts are directed
    to improving the generalization ability. This feature can be re-expressed saying
    that the network must learn a smaller number of abstractions and reuse them in
    order to classify new samples. A batch, if sampled correctly, contains a part
    of these *abstract elements* and part of the corrections automatically improve
    the evaluation of a subsequent batch. You can imagine a waterfall process, where
    a new training step never starts from scratch. However, the algorithm is also
    called mini-batch gradient descent, because the usual batch size normally ranges
    from 16 to 512 (larger sizes are uncommon, but always possible), which are values
    smaller than the number of total samples (in particular in deep learning contexts).
    A reasonable default value could be 32 samples, but I always invite the reader
    to test larger values, comparing the performances in terms of training speed and
    final accuracy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的超参数是批量大小。没有银弹能让我们自动做出正确的选择，但我们可以考虑一些因素。由于SGD是一个近似算法，较大的批量会导致更接近整个数据集得到的校正。然而，当样本数量极高时，我们并不期望深度模型能够将它们一一对应，相反，我们的努力是致力于提高泛化能力。这一特性也可以重新表述为，网络必须学习更少的抽象，并重复使用它们来对新样本进行分类。如果正确采样，一个批量将包含这些*抽象元素*的一部分，并且自动校正的一部分会提高后续批量的评估。你可以想象一个瀑布过程，其中新的训练步骤永远不会从头开始。然而，该算法也被称为小批量梯度下降，因为通常的批量大小通常在16到512之间（较大的尺寸不常见，但总是可能的），这些值小于总样本数（特别是在深度学习环境中）。一个合理的默认值可能是32个样本，但我总是建议读者测试更大的值，比较训练速度和最终准确性的性能。
- en: When working with deep neural networks, all the values (number of neurons in
    a layer, batch size, and so on) are normally powers of two. This is not a constraint,
    but only an optimization tip (above all when using GPUs), as the memory can be
    more efficiently filled when the blocks are based on a *2^N* elements. However,
    this is only a suggestion, whose benefits could also be negligible; so, don't
    be afraid to test architectures with different values. For example, in many papers,
    the batch size is 100 or some layers have 1,000 neurons.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当与深度神经网络一起工作时，所有的值（例如，一个层中的神经元数量、批量大小等）通常是2的幂。这并不是一个约束，而只是一个优化提示（尤其是在使用GPU时），因为当块基于*2^N*元素时，内存可以更有效地被填充。然而，这只是一个建议，其好处也可能是微不足道的；所以，不要害怕测试具有不同值的架构。例如，在许多论文中，批量大小是100，或者某些层有1,000个神经元。
- en: Weight initialization
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重初始化
- en: A very important element is the initial configuration of a neural network. How
    should the weights be initialized? Let's imagine we that have set them all to
    zero. As all neurons in a layer receive the same input, if the weights are 0 (or
    any other common, constant number), the output will be equal. When applying the
    gradient correction, all neurons will be treated in the same way; so, the network
    is equivalent to a sequence of single neuron layers. It's clear that the initial
    weights must be different to achieve a goal called symmetry breaking, but which
    is the best choice?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常重要的元素是神经网络的初始配置。权重应该如何初始化？让我们假设我们将它们都设置为0。由于一个层中的所有神经元都接收相同的输入，如果权重是0（或任何其他常见的常数），输出将是相等的。当应用梯度校正时，所有神经元都将被同等对待；因此，网络相当于一系列的单神经元层。很明显，初始权重必须不同才能实现一个称为对称破缺的目标，但最好的选择是什么？
- en: If we knew (also approximately) the final configuration, we could set them to
    easily reach the optimal point in a few iterations, but, unfortunately, we have
    no idea where the minimum is located. Therefore, some empirical strategies have
    been developed and tested, with the goal of minimizing the training time (obtaining
    state-of-the-art accuracies). A general rule of thumb is that the weights should
    be small (compared to the input sample variance). Large values lead to large outputs
    that negatively impact on saturating functions (such as `tanh` and `sigmoid`),
    while small values can be more easily optimized because the corresponding gradients
    are larger and the corrections have a stronger effect. The same is true also for
    rectifier units because the maximum efficiency is achieved by working in a segment
    crossing the origin (where the non-linearity is actually *located*). For example,
    when coping with images, if the values are positive and large, a ReLU neuron becomes
    almost a linear unit, losing a lot of its advantages (that's why images are normalized,
    so as to bound each pixel value between 0 and 1 or -1 and 1).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们（也大致地）知道最终的配置，我们就可以在几次迭代中轻松地将它们设置到最优点，但不幸的是，我们不知道最小值在哪里。因此，已经开发并测试了一些经验策略，目标是尽量减少训练时间（获得最先进的准确率）。一个普遍的规则是，权重应该很小（与输入样本方差相比）。大值会导致大的输出，这会负面影响饱和函数（如`tanh`和`sigmoid`），而小值则更容易优化，因为相应的梯度更大，校正效果更强。对于整流器单元也是如此，因为最大效率是在穿过原点的区间内工作（非线性实际上就位于那里）。例如，当处理图像时，如果值是正的且很大，ReLU神经元几乎变成了线性单元，失去了很多优势（这就是为什么图像要归一化，以便将每个像素值限制在0到1或-1到1之间）。
- en: At the same time, ideally, the activation variances should remain almost constant
    throughout the network, as well as the weight variances after every back-propagation
    step. These two conditions are fundamental in order to improve the convergence
    process and to avoid the vanishing and exploding gradient problems (the latter,
    which is the opposite of vanishing gradients, will be discussed in the section
    dedicated to recurrent network architectures).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，理想情况下，激活方差应该在整个网络中保持几乎恒定，以及在每个反向传播步骤之后的权重方差。这两个条件对于改善收敛过程和避免梯度消失和梯度爆炸问题（后者是梯度消失的对立面，将在关于循环网络架构的章节中讨论）是基本的。
- en: 'A very common strategy considers the number of neurons in a layer and initializes
    the weights as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见的策略是考虑层中的神经元数量，并按以下方式初始化权重：
- en: '![](img/84f592fa-d6d8-402f-811c-a1c7fd37492b.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84f592fa-d6d8-402f-811c-a1c7fd37492b.png)'
- en: 'This method is called **variance scaling** and can be applied using the number
    of input units (Fan-In), the number of output units (Fan-Out), or their average.
    The idea is very intuitive: if the number of incoming or outgoing connections
    is large, the weights must be smaller, so as to avoid large outputs. In the degenerate
    case of a single neuron, the variance is set to `1.0`, which is the maximum value
    allowed(in general, all methods keep the initial values for the biases equal to
    0.0 because it''s not necessary to initialize them with a random value).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**方差缩放**，可以使用输入单元数（Fan-In）、输出单元数（Fan-Out）或它们的平均值来应用。这个想法非常直观：如果输入或输出的连接数很大，权重必须更小，以避免大的输出。在单神经元退化的情况下，方差设置为`1.0`，这是允许的最大值（通常，所有方法都将偏差的初始值保持为0.0，因为不需要用随机值初始化它们）。
- en: 'Other variations have been proposed, even if they all share the same basic
    ideas. **LeCun** proposed initializing the weights as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 还提出了其他变体，尽管它们都共享相同的基本思想。**LeCun**提出了以下初始化权重的建议：
- en: '![](img/b7564827-d12d-451a-9d80-a0de48e25214.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7564827-d12d-451a-9d80-a0de48e25214.png)'
- en: 'Another method called **Xavier initialization** (presented in *Understanding
    the difficulty of training deep feedforward neural networks, Glorot X., Bengio
    Y., Proceedings of the 13th International Conference on Artificial Intelligence
    and Statistics*), is similar to **LeCun initialization**, but it''s based on the
    average between the number of units of two consecutive layers (to mark the sequentiality,
    we have substituted the terms Fan-In and Fan-Out with explicit indices):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种称为**Xavier初始化**（在*Understanding the difficulty of training deep feedforward
    neural networks, Glorot X., Bengio Y., Proceedings of the 13th International Conference
    on Artificial Intelligence and Statistics*）的方法与**LeCun初始化**类似，但它基于两个连续层的单元数的平均值（为了标记顺序，我们将Fan-In和Fan-Out术语替换为显式索引）：
- en: '![](img/8a4ca55c-0f06-4619-8a46-940e4083553c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8a4ca55c-0f06-4619-8a46-940e4083553c.png)'
- en: This is a more robust variant, as it considers both the incoming connections
    and also the outgoing ones (which are in turn incoming connections). The goal
    (widely discussed by the authors in the aforementioned papers) is trying to meet
    the two previously presented requirements. The first one is to avoid oscillations
    in the variance of the activations of each layer (ideally, this condition can
    avoid saturation). The second one is strictly related to the back-propagation
    algorithm, and it's based on the observation that, when employing a variance scaling
    (or an equivalent uniform distribution), the variance of a weight matrix is proportional
    to the reciprocal of *3n[k]*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个更稳健的变体，因为它考虑了输入连接和输出连接（反过来也是输入连接）。目标（作者在上述论文中广泛讨论）是试图满足之前提出的两个要求。第一个要求是避免每层激活方差中的振荡（理想情况下，这个条件可以避免饱和）。第二个要求与反向传播算法严格相关，其基于观察，当采用方差缩放（或等效的均匀分布）时，权重矩阵的方差与*3n[k]*的倒数成正比。
- en: Therefore, the averages of Fan-In and Fan-Out are multiplied by three, trying
    to avoid large variations in the weights after the updates. Xavier initialization
    has been proven to be very effective in many deep architectures, and it's often
    the default choice.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Fan-In和Fan-Out的平均值乘以三，试图避免更新后权重的大幅变化。Xavier初始化在许多深度架构中已被证明非常有效，并且通常是默认选择。
- en: 'Other methods are based on a different way to measure the variance during both
    the feed-forward and back-propagation phases and trying to correct the values
    to minimize residual oscillations in specific contexts. For example, He, Zhang,
    Ren, and Sun (in *Delving Deep into Rectifiers: Surpassing Human-Level Performance
    on ImageNet Classification, He K., Zhang X., Ren S., Sun J., arXiv:1502.01852
    [cs.CV]*) analyzed the initialization problem in the context of convolutional
    networks (we are going to discuss them in the next chapter) based on ReLU or variable
    Leaky-ReLU activations (also known as PReLU, parametric ReLU), deriving an optimal
    criterion (often called the **He initializer**), which is slightly different from
    the Xavier initializer:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '其他方法基于在正向传播和反向传播阶段以不同方式测量方差，并尝试校正这些值以最小化特定环境中的残差振荡。例如，He、Zhang、Ren 和 Sun（在
    *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification, He K., Zhang X., Ren S., Sun J., arXiv:1502.01852 [cs.CV]*) 基于ReLU或可变Leaky-ReLU激活（也称为PReLU，参数化ReLU）分析了卷积网络（我们将在下一章讨论）的初始化问题，推导出一个最优标准（通常称为**He初始化器**），它与Xavier初始化器略有不同：'
- en: '![](img/1562931a-8ac3-493b-b49b-a82aa418792e.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1562931a-8ac3-493b-b49b-a82aa418792e.png)'
- en: All these methods share some common principles and, in many cases, they are
    interchangeable. As already mentioned, Xavier is one of the most robust and, in
    the majority of real-life problems, there's no need to look for other methods;
    however, the reader should be always aware that the complexity of deep models
    must be often faced using empirical methods based on sometimes simplistic mathematical
    assumptions. Only the validation with real dataset can confirm if a hypothesis
    is correct or it's better to continue the investigation in another direction.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都共享一些共同原则，并且在许多情况下可以互换。如前所述，Xavier是最稳健的之一，在大多数现实问题中，没有必要寻找其他方法；然而，读者应始终意识到，深度模型的复杂性必须经常使用基于有时简单数学假设的经验方法来面对。只有通过真实数据集的验证，才能确认一个假设是正确的，或者是否需要继续在其他方向上进行研究。
- en: Example of MLP with Keras
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的MLP示例
- en: Keras ([https://keras.io](https://keras.io)) is a powerful Python toolkit that
    allows modeling and training complex deep learning architectures with minimum
    effort. It relies on low-level frameworks, such as Tensorflow, Theano, or CNTK,
    and provides high-level blocks to build the single layers of a model. In this
    book, we need to be very pragmatic because there's no room for a complete explanation;
    however, all the examples will be structured to allow the reader to try different
    configurations and options without a full knowledge (for further details, I suggest
    the book *Deep Learning with Keras, Gulli A, Pal S., Packt Publishing*).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Keras（[https://keras.io](https://keras.io)）是一个强大的Python工具包，它允许以最小的努力建模和训练复杂的深度学习架构。它依赖于底层框架，如Tensorflow、Theano或CNTK，并提供高级块来构建模型的单个层。在这本书中，我们需要非常实用，因为没有空间进行完整的解释；然而，所有示例都将结构化，以便读者在没有全面知识的情况下尝试不同的配置和选项（对于更详细的信息，我建议阅读书籍《使用Keras的深度学习》，作者A.
    Gulli和S. Pal，Packt出版社）。
- en: 'In this example, we want to build a small MLP with a single hidden layer to
    solve the XOR problem (the dataset is the same created in the previous example).
    The simplest and most common way is to instantiate the class `Sequential`, which
    defines an *empty container* for an indefinite model. In this initial part, the
    fundamental method is `add()`, which allows adding a layer to the model. For our
    example, we want to employ four hidden layers with hyperbolic tangent activation
    and two softmax output layers. The following snippet defines the MLP:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们想要构建一个包含单个隐藏层的小型MLP来解决XOR问题（数据集与上一个示例中创建的相同）。最简单和最常见的方法是实例化`Sequential`类，它定义了一个*空容器*，用于不定型的模型。在这个初始部分，基本方法是`add()`，它允许向模型添加一个层。对于我们的示例，我们想要使用四个具有双曲正切激活的隐藏层和两个softmax输出层。以下代码片段定义了MLP：
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `Dense` class defines a fully connected layer (a *classical* MLP layer),
    and the first parameter is used to declare the number of desired units. The first
    layer must declare the `input_shape` or `input_dim`, which specify the dimensions
    (or the shape) of a single sample (the batch size is omitted as it's dynamically
    set by the framework). All the subsequent layers compute the dimensions automatically.
    One of the strengths of Keras is the possibility to avoid setting many parameters
    (like weight initializers), as they will be automatically configured using the
    most appropriate default values (for example, the default weight initializer is
    Xavier). In the next examples, we are going to explicitly set some of them, but
    I suggest that the reader checks the official documentation to get acquainted
    with all the possibilities and features. The other layer involved in this experiment
    is `Activation`, which specifies the desired activation function (it's also possible
    to declare it using the parameter `activation` implemented by almost all layers,
    but I prefer to decouple the operations to emphasize the single roles, and also
    because some techniques—such as batch normalization—are normally applied to the
    linear output, before the activation).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dense`类定义了一个全连接层（一个*经典*的多层感知器层），第一个参数用于声明所需的单元数。第一层必须声明`input_shape`或`input_dim`，它们指定单个样本的维度（或形状）（批处理大小被省略，因为它由框架动态设置）。所有后续层都会自动计算维度。Keras的一个优点是能够避免设置许多参数（如权重初始化器），因为它们将自动使用最合适的默认值进行配置（例如，默认权重初始化器是Xavier）。在接下来的示例中，我们将明确设置其中的一些，但我建议读者查阅官方文档，以了解所有可能性和功能。另一个参与此实验的层是`Activation`，它指定所需的激活函数（也可以通过几乎所有层实现的参数`activation`来声明，但我更喜欢解耦操作以强调单一角色，并且也因为一些技术——例如批量归一化——通常在激活之前应用于线性输出）。'
- en: 'At this point, we must ask Keras to compile the model (using the preferred
    backend):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们必须要求Keras编译模型（使用首选的后端）：
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The parameter `optimizer` defines the stochastic gradient descent algorithm
    that we want to employ. Using `optimizer=''sgd''`, it''s possible to implement
    a standard version (as described in the previous paragraph). In this case, we
    are employing Adam (with the default parameters), which is a much more performant
    variant that will be discussed in the next section. The parameter `loss` is used
    to define the cost function (in this case, cross-entropy) and `metrics` is a list
    of all the evaluation score we want to be computed (`''accuracy''` is enough for
    many classification tasks). Once the model is compiled, it''s possible to train
    it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`optimizer`定义了我们想要使用的随机梯度下降算法。使用`optimizer='sgd'`，可以实现标准版本（如前一段所述）。在这种情况下，我们使用的是Adam（默认参数），这是一个性能更优的变体，将在下一节讨论。参数`loss`用于定义成本函数（在这种情况下，是交叉熵），而`metrics`是一个包含我们想要计算的评估分数的列表（对于许多分类任务，`'accuracy'`就足够了）。一旦模型被编译，就可以对其进行训练：
- en: '[PRE4]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The operations are quite simple. We have split the dataset into training and
    test/validation sets (in deep learning, cross-validation is seldom employed) and,
    then, we have trained the model setting `batch_size=32` and `epochs=100`. The
    dataset is automatically shuffled at the beginning of each epoch, unless setting
    `shuffle=False`. In order to convert the discrete labels into one-hot encoding,
    we have used the utility function `to_categorical`. In this case, the label 0
    becomes (1, 0) and the label 1 (0, 1). The model converges before reaching 100
    epochs; therefore, I invite the reader to optimize the parameters as an exercise.
    However, at the end of the process, the training accuracy is about 0.999 and the
    validation accuracy is 1.0.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 操作相当简单。我们将数据集分为训练集和测试/验证集（在深度学习中，很少使用交叉验证），然后我们设置了`batch_size=32`和`epochs=100`来训练模型。在每个epoch的开始，数据集会自动打乱，除非设置`shuffle=False`。为了将离散标签转换为one-hot编码，我们使用了`to_categorical`实用函数。在这种情况下，标签0变为(1,
    0)，标签1变为(0, 1)。模型在达到100个epoch之前就收敛了；因此，我邀请读者将参数优化作为练习。然而，在过程结束时，训练准确率约为0.999，验证准确率为1.0。
- en: 'The final classification plot is shown in the following diagram:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的分类图如下所示：
- en: '![](img/e3be515d-06ce-43dc-ba43-ac43ff49332c.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e3be515d-06ce-43dc-ba43-ac43ff49332c.png)'
- en: MLP classification of the XOR dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: XOR数据集的MLP分类
- en: 'Only three points have been misclassified, but it''s clear that the MLP successfully
    separated the XOR dataset. To have a confirmation of the generalization ability,
    we''ve plotted the decision surfaces for a hyperbolic tangent hidden layer and
    ReLU one:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 只有三个点被错误分类，但很明显，MLP成功地将XOR数据集分离。为了确认泛化能力，我们绘制了双曲正切隐藏层和ReLU隐藏层的决策表面：
- en: '![](img/d8bf3ec2-26b9-470b-8f3e-bfa3bcbcce76.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d8bf3ec2-26b9-470b-8f3e-bfa3bcbcce76.png)'
- en: MLP decision surfaces with Tanh (left) and ReLU (right) hidden layer
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Tanh（左侧）和ReLU（右侧）隐藏层的MLP决策表面
- en: In both cases, the MLPs delimited the areas in a reasonable way. However, while
    a `tanh` hidden layer seems to be overfitted (this is not true in our case, as
    the dataset represents exactly the data generating process), the ReLU layer generates
    less smooth boundaries with an apparent lower variance (in particular for considering
    the outliers of a class). We know that the final validation accuracies confirm
    an almost perfect fit, and the decision plots (which is easy to create with two
    dimensions) show in both cases acceptable boundaries, but this simple exercise
    is useful to understand the complexity and the sensitivity of a deep model. For
    this reason, it's absolutely necessary to select a valid training set (representing
    the ground-truth) and employ all possible techniques to avoid the overfitting
    (as we're going to discuss later). The easiest way to detect such a situation
    is checking the validation loss. A good model should reduce both training and
    validation loss after each epoch, reaching a plateau for the latter. If, after
    *n* epochs, the validation loss (and, consequently, the accuracy) begins to increase,
    while the training loss keeps decreasing, it means that the model is overfitting
    the training set.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，MLPs都以合理的方式划定了区域。然而，虽然`tanh`隐藏层似乎过度拟合（在我们的案例中并非如此，因为数据集正好代表了数据生成过程），ReLU层生成的边界则不够平滑，方差明显较低（特别是对于考虑类中的异常值）。我们知道最终的验证准确率证实了几乎完美的拟合，决策图（在两个维度上容易创建）在两种情况下都显示了可接受的边界，但这个简单的练习有助于理解深度模型的复杂性和敏感性。因此，绝对有必要选择一个有效的训练集（代表真实情况）并采用所有可能的技巧来避免过度拟合（正如我们稍后将要讨论的）。检测这种情况的最简单方法就是检查验证损失。一个好的模型应该在每个epoch后减少训练和验证损失，后者的损失达到平台期。如果在*n*个epoch后，验证损失（以及随之而来的准确率）开始增加，而训练损失继续下降，这意味着模型正在过度拟合训练集。
- en: Another empirical indicator that the training process is evolving correctly
    is that, at least at the beginning, the validation accuracy should be higher than
    the training one. This can seem strange, but we need to consider that the validation
    set is slightly smaller and less complex than the training set; therefore, if
    the capacity of the model is not saturated with training samples, the probability
    of misclassification is higher for the training set than for the validation set.
    When this trend is inverted, the model is very likely to overfit after a few epochs.
    To verify these concepts, I invite the reader to repeat the exercise using a large
    number of hidden neurons (so as to increase dramatically the capacity), but they
    will be clearer when working with much more complex and unstructured datasets.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个表明训练过程正在正确演变的经验指标是，至少在开始时，验证准确率应该高于训练准确率。这看起来可能有些奇怪，但我们需要考虑验证集比训练集略小且复杂度更低；因此，如果模型的容量没有因训练样本而饱和，训练集的误分类概率将高于验证集。当这种趋势逆转时，模型在经过几个epoch后很可能过度拟合。为了验证这些概念，我邀请读者使用大量隐藏神经元（以显著增加容量）重复此练习，但在处理更加复杂和无结构的数据集时，这些概念将更加清晰。
- en: 'Keras can be installed using the command `pip install -U keras`. The default
    framework is Theano with CPU support. In order to use other frameworks (such as
    Tensorflow GPU), I suggest reading the instructions reported on the home page
    [https://keras.io](https://keras.io). As also suggested by the author, the best
    backend is Tensorflow, which is available for Linux, Mac OSX, and Windows. To
    install it (together with all dependencies), please follow the instructions on
    the following page: [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用命令`pip install -U keras`安装Keras。默认框架是支持CPU的Theano。为了使用其他框架（如Tensorflow GPU），我建议阅读主页上报告的说明[https://keras.io](https://keras.io)。正如作者所建议的，最好的后端是Tensorflow，它适用于Linux、Mac
    OSX和Windows。要安装它（以及所有依赖项），请遵循以下页面上的说明：[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)
- en: Optimization algorithms
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化算法
- en: When discussing the back-propagation algorithm, we have shown how the SGD strategy
    can be easily employed to train deep networks with large datasets. This method
    is quite robust and effective; however, the function to optimize is generally
    non-convex and the number of parameters is extremely large. These conditions increase
    dramatically the probability to find saddle points (instead of local minima) and
    can slow down the training process when the surface is almost flat.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论反向传播算法时，我们展示了如何将 SGD 策略轻松地应用于训练大型数据集的深度网络。这种方法相当稳健且有效；然而，要优化的函数通常是非凸的，参数数量极其庞大。这些条件大大增加了找到鞍点（而不是局部最小值）的概率，当表面几乎平坦时，这可能会减慢训练过程。
- en: 'A common result of applying a *vanilla* SGD algorithm to these systems is shown
    in the following diagram:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *vanilla* SGD 算法应用于这些系统的常见结果如下所示：
- en: '![](img/762c2102-e0ea-48f5-8b10-13ea786aea2e.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/762c2102-e0ea-48f5-8b10-13ea786aea2e.png)'
- en: Instead of reaching the optimal configuration, *θ[opt]*, the algorithm reaches
    a sub-optimal parameter configuration, *θ*[*subopt*,] and loses the ability to
    perform further corrections. To mitigate all these problems and their consequences,
    many SGD optimization algorithms have been proposed, with the purpose of speeding
    up the convergence (also when the gradients become extremely small) and avoiding
    the instabilities of ill-conditioned systems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 算法不是达到最优配置 *θ[opt]*，而是达到次优参数配置 *θ*[*subopt*,]，并失去了执行进一步修正的能力。为了减轻所有这些问题及其后果，已经提出了许多
    SGD 优化算法，目的是加快收敛速度（即使在梯度变得非常小的情况下）并避免不良条件系统的稳定性问题。
- en: Gradient perturbation
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度扰动
- en: 'A common problem arises when the hypersurface is flat (plateaus) the gradients
    become close to zero. A very simple way to mitigate this problem is based on adding
    a small homoscedastic noise component to the gradients:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当超曲面平坦（平台期）时，梯度接近零，这会出现一个常见问题。缓解这个问题的一个非常简单的方法是在梯度中添加一个小的同质噪声成分：
- en: '![](img/5322d2e3-a850-48af-a649-b73657a318e8.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5322d2e3-a850-48af-a649-b73657a318e8.png)'
- en: The covariance matrix is normally diagonal with all elements set to *σ²(t)*,
    and this value is decayed during the training process to avoid perturbations when
    the corrections are very small. This method is conceptually reasonable, but its
    implicit randomness can yield undesired effects when the noise component is dominant.
    As it's very difficult to tune up the variances in deep models, other (more deterministic)
    strategies have been proposed.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵通常是正交的，所有元素都设置为 *σ²(t)*，这个值在训练过程中会衰减，以避免在修正非常小的时候产生扰动。这个方法在概念上是合理的，但当噪声成分占主导地位时，其隐含的随机性可能会产生不期望的效果。由于在深度模型中很难调整方差，因此已经提出了其他（更确定性的）策略。
- en: Momentum and Nesterov momentum
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量和Nesterov动量
- en: 'A more robust way to improve the performance of SGD when plateaus are encountered
    is based on the idea of momentum (analogously to physical momentum). More formally,
    a momentum is obtained employing the weighted moving average of subsequent gradient
    estimations instead of the punctual value:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当遇到平台期时，一种更稳健的方法是基于动量的想法（类似于物理动量）。更正式地说，动量是通过使用后续梯度估计的加权移动平均而不是瞬时值来获得的：
- en: '![](img/fa994b7f-3b44-4c19-9ad7-8a5e5086d2a7.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fa994b7f-3b44-4c19-9ad7-8a5e5086d2a7.png)'
- en: 'The new vector *v*^(*(t)*,) contains a component which is based on the past
    history (and weighted using the parameter *μ* which is a forgetting factor) and
    a term referred to the current gradient estimation (multiplied by the learning
    rate). With this approach, abrupt changes become more difficult, and when the
    exploration leaves a sloped region to enter a plateau, the momentum doesn''t become
    immediately null (but for a time proportional to *μ*) a portion of the previous
    gradients will be kept, making it possible to traverse flat regions. The value
    assigned to the hyperparameter μ is normally bounded between 0 and 1\. Intuitively,
    small values imply a short memory as the first term decays very quickly, while
    values close to 1.0 (for example, 0.9) allow a longer memory, less influenced
    by local oscillations. Like for many other hyperparameters, μ needs to be tuned
    according to the specific problem, considering that a high momentum is not always
    the best choice. High values could slow down the convergence when very small adjustments
    are needed, but, at the same time, values close to 0.0 are normally ineffective
    because the memory contribution decays too early. Using momentum, the update rule
    becomes as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 新向量*v*^(*(t)*,)包含一个基于过去历史（并使用参数*μ*加权，*μ*是一个遗忘因子）的分量，以及一个与当前梯度估计相关的项（乘以学习率）。采用这种方法，突兀的变化变得更加困难，当探索离开斜坡区域进入平台期时，动量不会立即变为零（但与*μ*成比例的一段时间内），部分先前梯度将被保留，这使得穿越平坦区域成为可能。分配给超参数μ的值通常介于0和1之间。直观地说，较小的值意味着短期记忆，因为第一项衰减非常快，而接近1.0（例如，0.9）的值允许更长的记忆，受局部振荡的影响较小。像许多其他超参数一样，μ需要根据具体问题进行调整，考虑到高动量并不总是最佳选择。当需要非常小的调整时，高值可能会减慢收敛速度，但与此同时，接近0.0的值通常无效，因为记忆贡献衰减得太早。使用动量，更新规则变为如下：
- en: '![](img/2c0a39b1-8da8-4ade-9ec5-7906ffdcde6b.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c0a39b1-8da8-4ade-9ec5-7906ffdcde6b.png)'
- en: 'A variant is provided by **Nesterov momentum**, which is based on the results
    obtained in the field of mathematical optimization by Nesterov that have been
    proven to speed up the convergence of many algorithms. The idea is to determine
    a temporary parameter update based on the current momentum and then apply the
    gradient to this vector to determine the next momentum (it can be interpreted
    as a *look-ahead* gradient evaluation aimed to mitigate the risk of a wrong correction
    considering the moving history of each parameter):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nesterov动量**提供了一种变体，它基于Nesterov在数学优化领域的成果，这些成果已被证明可以加速许多算法的收敛。其思路是根据当前动量确定一个临时参数更新，然后将梯度应用到这个向量上以确定下一个动量（可以解释为一种*前瞻性*梯度评估，旨在减轻考虑每个参数移动历史时的错误校正风险）：'
- en: '![](img/2a57de8e-bed1-45a2-8559-e80c36c48a2a.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a57de8e-bed1-45a2-8559-e80c36c48a2a.png)'
- en: This algorithm showed a performance improvement in several deep models; however,
    its usage is still limited because the next algorithms very soon outperformed
    the standard SGD with momentum, and they became the first choice in almost any
    real-life task.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法在几个深度模型中显示出性能提升；然而，由于其使用仍然有限，因为接下来的算法很快超过了标准的带有动量的SGD，并且它们几乎成为任何实际任务的首选。
- en: SGD with momentum in Keras
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的带有动量的SGD
- en: 'When using Keras, it''s possible to customize the SGD optimizer by directly
    instantiating the `SGD` class and using it while compiling the model:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Keras时，可以通过直接实例化`SGD`类并在编译模型时使用它来自定义SGD优化器：
- en: '[PRE5]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The class SGD accepts the parameter `lr` (the learning rate *η* with a default
    set to `0.01`), `momentum` (the parameter *μ*), `nesterov` (a boolean indicating
    whether employing the Nesterov momentum), and an optional `decay` parameter to
    indicate whether the learning rate must be decayed over the updates with the following
    formula:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`SGD`类接受参数`lr`（学习率*η*，默认设置为`0.01`），`momentum`（参数*μ*），`nesterov`（一个布尔值，表示是否采用Nesterov动量），以及一个可选的`decay`参数，以指示学习率是否需要在更新过程中衰减，使用以下公式：'
- en: '![](img/62f05832-fcb8-4175-b271-6729257cf8cd.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62f05832-fcb8-4175-b271-6729257cf8cd.png)'
- en: RMSProp
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RMSProp
- en: '**RMSProp** was proposed by Hinton as an adaptive algorithm, partially based
    on the concept of momentum. Instead of considering the whole gradient vector,
    it tries to optimize each parameter separately to increase the corrections of
    slowly changing weights (that probably need more drastic modifications) and decreasing
    the update magnitudes of quickly changing ones (which are normally the more unstable).
    The algorithm computes the exponentially weighted moving average of the *changing
    speed* of every parameter considering the square of the gradient (which is insensitive
    to the sign):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**RMSProp**是由Hinton提出的自适应算法，部分基于动量的概念。它不是考虑整个梯度向量，而是试图分别优化每个参数，以增加缓慢变化的权重的校正（可能需要更剧烈的修改）并减少快速变化的更新幅度（通常是更不稳定的）。该算法计算每个参数的*变化速度*的指数加权移动平均，考虑梯度的平方（对符号不敏感）：'
- en: '![](img/c15efc5f-3c65-472d-a5dd-a893063b7008.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c15efc5f-3c65-472d-a5dd-a893063b7008.png)'
- en: 'The weight update is then performed, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照以下方式执行权重更新：
- en: '![](img/dfd09daa-c05f-4729-9c29-e9469ea11fd2.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dfd09daa-c05f-4729-9c29-e9469ea11fd2.png)'
- en: 'The parameter *δ* is a small constant (such as 10^(-6)) that is added to avoid
    numerical instabilities when the changing speed becomes null. The previous expression
    could be rewritten in a more compact way:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 参数*δ*是一个小的常数（例如10^(-6)），在变化速度变为零时添加以避免数值不稳定性。前面的表达式可以以更紧凑的方式重写：
- en: '![](img/9e4d44d1-dec1-44c9-bafc-fd5f783de5e5.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9e4d44d1-dec1-44c9-bafc-fd5f783de5e5.png)'
- en: Using this notation, it is clear that the role of RMSProp is adapting the learning
    rate for every parameter so it can increase it when necessary (almost *frozen*
    weights) and decrease it when the risk of oscillations is higher. In a practical
    implementation, the learning rate is always decayed over the epochs using an exponential
    or linear function.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种表示法，可以清楚地看出RMSProp的作用是调整每个参数的学习率，以便在必要时增加它（几乎*冻结*的权重）并在振荡风险更高时降低它。在实际实现中，学习率总是通过指数或线性函数在epoch中衰减。
- en: RMSProp with Keras
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RMSProp与Keras
- en: 'The following snippet shows the usage of RMSProp with Keras:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了在Keras中使用RMSProp的方法：
- en: '[PRE6]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The learning rate and decay are the same as SGD. The parameter `rho` corresponds
    to the exponential moving average weight, μ, and `epsilon` is the constant added
    to the changing speed to improve the stability. As with any other algorithm, if
    the user wants to use the default values, it's possible to declare the optimizer
    without instantiating the class (for example, `optimizer='rmsprop'`).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率和衰减与SGD相同。参数`rho`对应于指数移动平均权重μ，而`epsilon`是添加到变化速度中的常数，以提高稳定性。与任何其他算法一样，如果用户想使用默认值，可以在不实例化类的情况下声明优化器（例如，`optimizer='rmsprop'`）。
- en: Adam
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam
- en: '**Adam** (the contraction of Adaptive Moment Estimation) is an algorithm proposed
    by Kingma and Ba (in *Adam: A Method for Stochastic Optimization, Kingma D. P.,
    Ba J., arXiv:1412.6980 [cs.LG]**)* to further improve the performance of RMSProp.
    The algorithm determines an adaptive learning rate by computing the exponentially
    weighted averages of both the gradient and its square for every parameter:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adam**（自适应矩估计的缩写）是由Kingma和Ba（在*Adam: A Method for Stochastic Optimization,
    Kingma D. P., Ba J., arXiv:1412.6980 [cs.LG]**）提出的算法，旨在进一步提高RMSProp的性能。该算法通过计算每个参数的梯度和其平方的指数加权平均来确定自适应学习率：'
- en: '![](img/d1e00807-fed7-4348-8cff-503e85644b7e.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1e00807-fed7-4348-8cff-503e85644b7e.png)'
- en: 'In the aforementioned paper, the authors suggest to unbias the two estimations
    (which concern the first and second moment) by dividing them by *1 - μ[i]*, so
    the new moving averages become as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述论文中，作者建议通过除以*1 - μ[i]*来消除两个估计（涉及第一和第二矩）的偏差，因此新的移动平均变为以下形式：
- en: '![](img/88d60b62-7a70-4dcb-b672-472d1614e2c1.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88d60b62-7a70-4dcb-b672-472d1614e2c1.png)'
- en: 'The weight update rule for Adam is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Adam的权重更新规则如下：
- en: '![](img/56b028cf-f88d-48ad-aadd-19d3cd2e7bd0.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/56b028cf-f88d-48ad-aadd-19d3cd2e7bd0.png)'
- en: Analyzing the previous expression, it is possible to understand why this algorithm
    is often called RMSProp with momentum. In fact, the term *g(•)* acts just like
    the standard momentum, computing the moving average of the gradient for each parameter
    (with all the advantages of this procedure), while the denominator acts as an
    adaptive term with the same exact semantics of RMSProp. For this reason, Adam
    is very often one of the most widely employed algorithms, even if, in many complex
    tasks, its performances are comparable to a standard RMSProp. The choice must
    be made considering the extra complexity due to the presence of two forgetting
    factors. In general, the default values (0.9) are acceptable, but sometimes it's
    better to perform an analysis of several scenarios before deciding on a specific
    configuration. Another important element to remember is that all momentum based
    methods can lead to instabilities (oscillations) when training some deep architectures.
    That's why RMSProp is very diffused in almost any research paper; however, don't
    consider this statement as a limitation, because Adam has shown outstanding performances
    in many tasks. It's helpful to remember that, whenever the training process seems
    unstable also with low learning rates, it's preferable to employ methods that
    are not based on momentum (the inertial term, in fact, can slow down the fast
    modifications necessary to avoid oscillations).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 分析前面的表达式，可以理解为什么这个算法通常被称为带有动量的 RMSProp。实际上，项 *g(•)* 就像标准的动量一样，计算每个参数的梯度移动平均值（具有此过程的全部优点），而分母则充当具有与
    RMSProp 相同精确语义的自适应项。因此，Adam 非常经常是应用最广泛的算法之一，尽管在许多复杂任务中，其性能与标准 RMSProp 相当。选择必须考虑到由于存在两个遗忘因子而带来的额外复杂性。一般来说，默认值（0.9）是可以接受的，但有时在决定特定配置之前进行几个场景的分析会更好。另一个需要记住的重要元素是，所有基于动量的方法都可能导致训练某些深度架构时的不稳定性（振荡）。这就是为什么
    RMSProp 几乎在所有研究论文中都广泛使用；然而，不要将此声明视为限制，因为 Adam 在许多任务中已经显示出卓越的性能。记住，当训练过程似乎不稳定，即使学习率较低时，最好使用不基于动量的方法（惯性项实际上会减慢避免振荡所需的快速修改）。
- en: Adam with Keras
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 的 Adam
- en: 'The following snippet shows the usage of Adam with Keras:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在使用 Keras 时使用 Adam：
- en: '[PRE7]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The forgetting factors, *μ[1]* and *μ*[*2*,] are represented by the parameters
    `beta_1` and `beta_2`. All the other elements are the same as the other algorithms.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘因子 *μ[1]* 和 *μ*[*2*,] 分别由参数 `beta_1` 和 `beta_2` 表示。所有其他元素与其他算法相同。
- en: AdaGrad
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaGrad
- en: 'This algorithm has been proposed by Duchi, Hazan, and Singer (in *Adaptive
    Subgradient Methods for Online Learning and Stochastic Optimizatioln, Duchi J.,
    Hazan E., Singer Y., Journal of Machine Learning Research 12/2011).* The idea
    is very similar to RMSProp, but, in this case, the whole history of the squared
    gradients is taken into account:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法由 Duchi、Hazan 和 Singer 提出（在 *Adaptive Subgradient Methods for Online Learning
    and Stochastic Optimization, Duchi J., Hazan E., Singer Y., Journal of Machine
    Learning Research 12/2011)*。这个想法与 RMSProp 非常相似，但在这个情况下，考虑了平方梯度的整个历史：
- en: '![](img/16165cf0-6ea2-4abc-9eee-0767a05d6e2a.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16165cf0-6ea2-4abc-9eee-0767a05d6e2a.png)'
- en: 'The weights are updated exactly like in RMSProp:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的更新方式与 RMSProp 完全相同：
- en: '![](img/be1c1511-841d-4fa0-8be7-7f3ede17e84f.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be1c1511-841d-4fa0-8be7-7f3ede17e84f.png)'
- en: However, as the squared gradients are non-negative, the implicit sum *v^((t))(•)
    → ∞* when *t → ∞*. As the growth continues until the gradients are non-null, there's
    no way to keep the contribution stable while the training process proceeds. The
    effect is normally quite strong at the beginning, but vanishes after a limited
    number of epochs, yielding a null learning rate. **AdaGrad** keeps on being a
    powerful algorithm when the number of epochs is very limited, but it cannot be
    a first-choice solution for the majority of deep models (the next algorithm has
    been proposed to solve this problem).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于平方梯度是非负的，当 *t → ∞* 时，隐含的和 *v^((t))(•) → ∞*。由于增长会持续到梯度非零，所以在训练过程中无法保持贡献的稳定性。通常在开始时效果非常明显，但在有限的几个epoch之后就会消失，导致学习率为零。**AdaGrad**
    在 epoch 数量非常有限时仍然是一个强大的算法，但它不能成为大多数深度模型的首选解决方案（下一个算法已经被提出以解决这个问题）。
- en: AdaGrad with Keras
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 的 AdaGrad
- en: 'The following snippet shows the use of AdaGrad with Keras:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了在使用 Keras 时使用 AdaGrad 的用法：
- en: '[PRE8]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The AdaGrad implementation has no other parameters but the common ones.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 的实现没有其他参数，只有常见的那些。
- en: AdaDelta
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaDelta
- en: '**AdaDelta** is an algorithm (proposed in *ADADELTA: An Adaptive Learning Rate
    Method, Zeiler M. D., arXiv:1212.5701 [cs.LG]*) in order to address the main issue
    of AdaGrad, which arises to managing the whole squared gradient history. First
    of all, instead of the accumulator, AdaDelta employs an exponentially weighted
    moving average, like RMSProp:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaDelta** 是一个算法（由 Zeiler M. D. 提出，发表在 *ADADELTA: An Adaptive Learning Rate
    Method, Zeiler M. D., arXiv:1212.5701 [cs.LG]*)，旨在解决 AdaGrad 的主要问题，即管理整个平方梯度历史。首先，AdaDelta
    不是使用累加器，而是使用指数加权移动平均，类似于 RMSProp：'
- en: '![](img/33648fdf-1fcd-4796-b41e-6f551e2e8172.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/33648fdf-1fcd-4796-b41e-6f551e2e8172.png)'
- en: 'However, the main difference with RMSProp is based on the analysis of the update
    rule. When we consider the operation *x + Δx*, we assume that both terms have
    the same unit; however, the author noticed that the adaptive learning rate *η(θ[i])*
    obtained with RMSProp (as well as AdaGrad) is unitless (instead of having the
    unit of *θ[i]*). In fact, as the gradient is split into partial derivatives that
    can be approximated as *ΔL/Δθ[i]* and the cost function *L* is assumed to be unitless,
    we obtain the following relations:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 RMSProp 的主要区别在于对更新规则的分析。当我们考虑操作 *x + Δx* 时，我们假设这两个项具有相同的单位；然而，作者注意到，使用
    RMSProp（以及 AdaGrad）获得的自适应学习率 *η(θ[i]*) 是无单位的（而不是具有 *θ[i]* 的单位）。事实上，由于梯度被分解为可以近似为
    *ΔL/Δθ[i]* 的偏导数，并且假设成本函数 *L* 是无单位的，我们得到以下关系：
- en: '![](img/7346ef49-3082-45b3-bc08-aaee221b0bf7.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7346ef49-3082-45b3-bc08-aaee221b0bf7.png)'
- en: 'Therefore, Zeiler proposed to apply a correction term proportional to the unit
    of each weight *θ[i]*. This factor is obtained by considering the exponentially
    weighted moving average of every squared difference:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Zeiler 提出应用一个与每个权重单位 *θ[i]* 成比例的校正项。这个因子是通过考虑每个平方差的指数加权移动平均得到的：
- en: '![](img/042bebb2-417b-46a5-9539-5353244a5832.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/042bebb2-417b-46a5-9539-5353244a5832.png)'
- en: 'The resulting updated rule hence becomes as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更新的规则因此变为如下：
- en: '![](img/e911cca3-2e71-43ef-b075-221043212d02.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e911cca3-2e71-43ef-b075-221043212d02.png)'
- en: This approach is indeed more similar to RMSProp than AdaGrad, but the boundaries
    between the two algorithms are very thin, in particular when the history is limited
    to a finite sliding window. AdaDelta is a powerful algorithm, but it can outperform
    Adam or RMSProp only in very particular tasks. My suggestion is to employ a method
    and, before moving to another one, try to optimize the hyperparameters until the
    accuracy reaches its maximum. If the performances keep on being bad and the model
    cannot be improved in any other way, it's a good idea to test other optimization
    algorithms.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法确实与 RMSProp 更相似，但两种算法之间的界限非常微妙，尤其是在历史记录仅限于有限滑动窗口的情况下。AdaDelta 是一个强大的算法，但它只能在非常特定的任务中优于
    Adam 或 RMSProp。我的建议是采用一种方法，并且在转向另一种方法之前，尝试优化超参数，直到准确率达到最大。如果性能持续不佳，并且模型无法以任何其他方式改进，那么测试其他优化算法是一个好主意。
- en: AdaDelta with Keras
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaDelta 与 Keras
- en: 'The following snippet shows the usage of AdaDelta with Keras:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了在 Keras 中使用 AdaDelta 的用法：
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The forgetting factor, *μ*, is represented by the parameter `rho`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记因子，*μ*，由参数 `rho` 表示。
- en: Regularization and dropout
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化和 dropout
- en: Overfitting is a common issue in deep models. Their extremely high capacity
    can often become problematic even with very large datasets because the ability
    to learn the structure of the training set is not always related to the ability
    to generalize. A deep neural network can easily become an associative memory,
    but the final internal configuration couldn't be the most suitable to manage samples
    belonging to the same distribution but was never presented during the training
    process. It goes without saying that this behavior is proportional to the complexity
    of the separation hypersurface. A linear classifier has a minimum chance to overfit,
    and a polynomial classifier is incredibly more prone to do it. A combination of
    hundreds, thousands, or more non-linear functions yields a separation hypersurface,
    which is beyond any possible analysis. In 1991, Hornik (in *Approximation Capabilities
    of Multilayer Feedforward Networks,Hornik K., Neural Networks, 4/2*) generalized
    a very important result obtained two years before by the mathematician Cybenko
    (and published in *Approximations by Superpositions of Sigmoidal Functions, Cybenko
    G., Mathematics of Control, Signals, and Systems, 2 /4*). Without any mathematical
    detail (which is, however, not very complex), the theorem states that an MLP (not
    the most complex architecture!) can approximate any function that is continuous
    in a compact subset of *ℜ^n*. It's clear that such a result formalized what almost
    any researcher already intuitively knew, but its *power* goes beyond the first
    impact, because the MLP is a finite system (not a mathematical series) and the
    theorem assumes a finite number of layers and neurons. Obviously, the precision
    is proportional to the complexity; however, there are no unacceptable limitations
    for almost any problem. However, our goal is not learning an existing continuous
    function, but managing samples drawn from an unknown data generating process with
    the purpose to maximize the accuracy when a new sample is presented. There are
    no guarantees that the function is continuous or that the domain is a compact
    subset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 过度拟合是深度模型中常见的问题。即使数据集非常大，它们的极高容量也可能成为问题，因为学习训练集结构的能力并不总是与泛化能力相关。深度神经网络可以轻易地成为一个联想记忆，但最终的内部配置可能并不是最适合管理属于同一分布但从未在训练过程中出现过的样本。不言而喻，这种行为与分离超面的复杂性成正比。线性分类器过度拟合的可能性最小，而多项式分类器则极其容易过度拟合。数百、数千或更多非线性函数的组合产生一个分离超面，这超出了任何可能的分析。1991年，Hornik在《多层前馈网络的逼近能力》（Hornik
    K., Neural Networks, 4/2）中推广了数学家Cybenko两年前获得的一个非常重要的结果（发表在《通过Sigmoid函数的叠加逼近》（Cybenko
    G., Mathematics of Control, Signals, and Systems, 2 /4）上）。无需任何数学细节（尽管这并不复杂），该定理表明MLP（不是最复杂的架构！）可以逼近在*ℜ^n*的紧子集上连续的任何函数。很明显，这样的结果形式化了几乎所有研究人员已经直觉上知道的事情，但其*力量*超越了最初的影响，因为MLP是一个有限系统（而不是数学级数），并且该定理假设有限层数和神经元数。显然，精度与复杂性成正比；然而，对于几乎所有问题都没有不可接受的限制。然而，我们的目标不是学习现有的连续函数，而是管理从未知数据生成过程中抽取的样本，目的是在新的样本出现时最大化准确性。无法保证函数是连续的，或者域是紧子集。
- en: 'In [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine Learning
    Models Fundamentals*, we have presented the main regularization techniques based
    on a slightly modified cost function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第一章](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml)《机器学习模型基础》中，我们介绍了基于略微修改的成本函数的主要正则化技术：
- en: '![](img/c2be5c48-adfd-4fd3-87c8-d3f7a8a19ce1.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2be5c48-adfd-4fd3-87c8-d3f7a8a19ce1.png)'
- en: The additional term *g(θ)* is a non-negative function of the weights (such as
    L2 norm) that forces the optimization process to keep the parameters as small
    as possible. When working with saturating functions (such as `tanh`), regularization
    methods based on the L2 norm try to limit the operating range of the function
    to the linear part, reducing *de facto* its capacity. Of course, the final configuration
    won't be the optimal one (that could be the result of an overfitted model) but
    the suboptimal trade-off between training and validation accuracy (alternatively,
    we can say between bias and variance). A system with a bias close to 0 (and a
    training accuracy close to 1.0) could be extremely rigid in the classification,
    succeeding only when the samples are very similar to ones evaluated during the
    training process. That's why this *price* is often paid considering the advantages
    obtained when working with new samples. L2 regularization can be employed with
    any kind of activation function, but the effect could be different. For example,
    ReLU units have an increased probability to become linear (or constantly null)
    when the weights are very large. Trying to keep them close to 0.0 means forcing
    the function to exploit its non-linearity without the risk of extremely large
    outputs (that can negatively affect very deep architectures). This result can
    sometimes be more useful, because it allows training bigger models in a smoother
    way, obtaining better final performances. In general, it's almost impossible to
    decide whether a regularization can improve the result without several tests,
    but there are some scenarios where it's very common to introduce a dropout (we
    discuss this approach in the next paragraph) and tune up its hyperparameter. This
    is more an empirical choice than a precise architectural decision because many
    real-life examples (including state-of-the-art models) obtained outstanding results
    employing this regularization technique. I suggest the reader prefer a rational
    skepticism to blind trust and double-checking its models before picking a specific
    solution. Sometimes, an extremely high-performing network turns to being ineffective
    when a different (but analogous) dataset is chosen. That's why testing different
    alternatives can provide the best experience in order to solve specific problem
    classes.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的项 *g(θ)* 是权重（例如 L2 范数）的非负函数，它迫使优化过程尽可能保持参数尽可能小。当与饱和函数（例如 `tanh`）一起工作时，基于
    L2 范数的正则化方法试图限制函数的操作范围在线性部分，从而实际上减少了其容量。当然，最终的配置不会是最佳配置（这可能是一个过拟合模型的产物），而是在训练和验证准确率（或者说是偏差和方差）之间的次优权衡。一个偏差接近
    0（且训练准确率接近 1.0）的系统在分类中可能会非常僵硬，只有在样本与训练过程中评估的样本非常相似时才能成功。这就是为什么在考虑使用新样本获得的优势时，这种
    *代价* 通常会被支付。L2 正则化可以与任何类型的激活函数一起使用，但效果可能不同。例如，当权重非常大时，ReLU 单元有更大的概率变得线性（或始终为空）。试图将它们保持在
    0.0 附近意味着迫使函数利用其非线性，而不存在产生极端大输出的风险（这可能会对非常深的架构产生负面影响）。有时，这种结果可能更有用，因为它允许以更平滑的方式训练更大的模型，并获得更好的最终性能。一般来说，几乎不可能在没有进行多次测试的情况下决定正则化是否可以改进结果，但有一些场景中，引入
    dropout（我们将在下一段讨论这种方法）并调整其超参数是非常常见的。这更多的是一种经验选择，而不是精确的架构决策，因为许多现实生活中的例子（包括最先进的模型）通过使用这种正则化技术获得了出色的结果。我建议读者在选择特定解决方案之前，对模型进行理性怀疑和双重检查。有时，一个性能极高的网络在选择了不同的（但类似）数据集后，可能会变得无效。这就是为什么测试不同的替代方案可以为解决特定问题类别提供最佳体验。
- en: 'Before moving on, I want to show how it''s possible to implement an L1 (useful
    to enforce sparsity), L2, or ElasticNet (the combination of L1 and L2) regularization
    using Keras. The framework provides a fine-grained approach that allows imposing
    a different constraint to each layer. For example, the following snippet shows
    how to add a `l2` constraint with the strength parameter set to `0.05` to a generic
    fully connected layer:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我想展示如何使用 Keras 实现一个 L1（用于强制稀疏性）、L2 或 ElasticNet（L1 和 L2 的组合）正则化。该框架提供了一种细粒度方法，允许对每个层施加不同的约束。例如，以下代码片段展示了如何将强度参数设置为
    `0.05` 的 `l2` 约束添加到一个通用的全连接层：
- en: '[PRE10]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `keras.regularizers` package contains the functions `l1()`, `l2()`, and
    `l1_l2()`, which can be applied to `Dense` and convolutional layers (we're going
    to discuss them in the next chapter). These layers allow us to impose a regularization
    on the weights (`kernel_regularizer`), on the bias (`bias_regularizer`), and on
    the activation output (`activation_regularizer`), even if the first one is normally
    the most widely employed.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras.regularizers`包包含`l1()`、`l2()`和`l1_l2()`函数，这些函数可以应用于`Dense`和卷积层（我们将在下一章中讨论它们）。这些层允许我们对权重（`kernel_regularizer`）、偏差（`bias_regularizer`）和激活输出（`activation_regularizer`）施加正则化，尽管第一个通常是最广泛使用的。'
- en: 'Alternatively, it''s possible to impose specific constraints on the weights
    and biases that in a more selective way. The following snippet shows how to set
    a maximum norm (equal to `1.5`) on the weights of a layer:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也可以对权重和偏差施加特定的约束，以更选择性的方式。以下代码片段展示了如何设置一个层的权重最大范数（等于`1.5`）：
- en: '[PRE11]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Keras, in the `keras.constraints` package, provides some functions that can
    be used to impose a maximum norm on the weights or biases `maxnorm()`, a unit
    norm along an axis `unit_norm()`, non-negativity `non_neg()`, and upper and lower
    bounds for the norm `min_max_norm()`. The difference between this approach and
    regularization is that it is applied only if necessary. Considering the previous
    example, imposing an L2 regularization always has an effect, while a constraint
    on the maximum norm is inactive until the value is lower than the predefined threshold.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的`keras.constraints`包提供了一些函数，可以用来对权重或偏差施加最大范数`maxnorm()`，沿轴的单位范数`unit_norm()`，非负性`non_neg()`，以及范数的上下限`min_max_norm()`。这种方法与正则化的区别在于它仅在必要时应用。考虑到前面的例子，施加L2正则化始终有效，而最大范数约束在值低于预定义阈值之前是无效的。
- en: Dropout
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: This method has been proposed by Hinton and co. (in *Improving neural networks
    by preventing co-adaptation of feature detectors, Hinton G. E., Srivastava N.,
    Krizhevsky A., Sutskever I., Salakhutdinov R. R., arXiv:1207.0580 [cs.NE]*) as
    an alternative to prevent overfitting and allow bigger networks to explore more
    regions of the sample space. The idea is rather simple—during every training step,
    given a predefined percentage *n[d]*, a **dropout** layer randomly selects *n[d]N*
    incoming units and sets them to `0.0` (the operation is only active during the
    training phase, while it's completely removed when the model is employed for new
    predictions).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是由Hinton及其同事（在《通过防止特征检测器的共适应改进神经网络，Hinton G. E.，Srivastava N.，Krizhevsky
    A.，Sutskever I.，Salakhutdinov R. R.，arXiv:1207.0580 [cs.NE]》中提出）作为防止过拟合和允许更大网络探索样本空间更多区域的替代方案。这个想法相当简单——在每一步训练中，给定一个预定义的百分比
    *n[d]*，一个**dropout**层随机选择 *n[d]N* 个输入单元并将它们设置为 `0.0`（该操作仅在训练阶段有效，当模型用于新预测时则完全移除）。
- en: This operation can be interpreted in many ways. When more dropout layers are
    employed, the result of their selection is a sub-network with a reduced capacity
    that can, with more difficultly, overfit the training set. The overlap of many
    trained sub-networks makes up an implicit ensemble whose prediction is an average
    over all models. If the dropout is applied on input layers, it works like a weak
    data augmentation, by adding a random noise to the samples (setting a few units
    to zero can lead to potential corrupted patterns). At the same time, employing
    several dropout layers allows exploring several potential configurations that
    are continuously combined and refined.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作可以有多种解释。当使用更多的dropout层时，它们选择的结果是一个容量减少的子网络，它可能更难以过拟合训练集。许多训练子网络的交集构成一个隐式集成，其预测是所有模型平均的结果。如果dropout应用于输入层，它就像是一种弱数据增强，通过向样本添加随机噪声（将一些单元设置为零可能导致潜在的损坏模式）。同时，使用多个dropout层允许探索几个潜在配置，这些配置不断组合和优化。
- en: This strategy is clearly probabilistic, and the result can be affected by many
    factors that are impossible to anticipate; however, several tests confirmed that
    the employment of a dropout is a good choice when the networks are very deep because
    the resulting sub-networks have a residual capacity that allows them to model
    a wide portion of the samples, without driving the whole network to *freeze* its
    configuration overfitting the training set. On the other hand, this method is
    not very effective when the networks are shallow or contain a small number of
    neurons (in these cases, L2 regularization is probably a better choice).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略显然是概率性的，结果可能会受到许多难以预料的因素的影响；然而，几个测试证实，当网络非常深时，使用dropout是一个不错的选择，因为由此产生的子网络具有残余能力，允许它们模拟大量样本，而不会使整个网络在训练集上过度拟合而“冻结”其配置。另一方面，当网络较浅或包含较少的神经元时（在这些情况下，L2正则化可能是一个更好的选择），这种方法并不非常有效。
- en: According to the authors, dropout layers should be used in conjunction with
    high learning rates and maximum norm constraints on the weights. In this way,
    in fact, the model can easily learn more potential configurations that would be
    avoided when the learning rate is kept very small. However, this is not an absolute
    rule because many state-of-the-art models use a dropout together with optimization
    algorithms, such as RMSProp or Adam, and not excessively high learning rates.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 根据作者的说法，dropout层应该与高学习率和权重上的最大范数约束一起使用。实际上，这样模型可以轻松地学习更多潜在的配置，这些配置在保持非常小的学习率时会被避免。然而，这并不是一个绝对规则，因为许多最先进的模型使用dropout与优化算法（如RMSProp或Adam）一起使用，而不是过高的学习率。
- en: The main drawback of a dropout is that it slows down the training process and
    can lead to an unacceptable sub-optimality. The latter problem can be mitigated
    by adjusting the percentages of dropped units, but, in general, it's very difficult
    to solve it completely. For this reason, some new image-recognition models (like
    residual networks) avoid the dropout and employ more sophisticated techniques
    to train very deep convolutional networks that overfit both training and validation
    sets.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: dropout的主要缺点是它会减慢训练过程，并可能导致不可接受的次优化。后者的问题可以通过调整丢弃单元的百分比来缓解，但通常很难完全解决这个问题。因此，一些新的图像识别模型（如残差网络）避免了dropout，并采用更复杂的技巧来训练非常深的卷积网络，这些网络会过度拟合训练集和验证集。
- en: Example of dropout with Keras
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中dropout的示例
- en: We cannot test the effectiveness of the dropout with a more challenging classification
    problem. The dataset is the *classical* MNIST handwritten digits, but Keras allows
    downloading and working with the original version that is made up of 70 thousand
    (60 thousand training and 10 thousand test) 28 × 28 grayscale images. Even if
    this is not the best strategy, because a convolutional network should be the first
    choice to manage images, we want to try to classify the digits considering them
    as flattened 784-dimensional arrays.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法用一个更具挑战性的分类问题来测试dropout的有效性。数据集是*经典*的MNIST手写数字数据集，但Keras允许下载并使用由70,000个（60,000个训练和10,000个测试）28
    × 28灰度图像组成的原始版本。即使这不是最佳策略，因为卷积网络应该是处理图像的首选，我们仍想尝试将数字视为784维的展平数组来对它们进行分类。
- en: 'The first step is loading and normalizing the dataset so that each value becomes
    a float bounded between 0 and 1:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载和归一化数据集，使得每个值都成为一个介于0和1之间的浮点数：
- en: '[PRE12]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At this point, we can start testing a model without dropout. The structure,
    which is common to all experiments, is based on three fully connected ReLU layers
    (2048-1024-1024) followed by a softmax layer with 10 units. Considering the problem,
    we can try to train the model using an Adam optimizer with *η = 0.0001* and a
    decay set to *10^(-6)*:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以开始测试一个没有dropout的模型。所有实验共有的结构是基于三个全连接ReLU层（2048-1024-1024），随后是一个有10个单位的softmax层。考虑到这个问题，我们可以尝试使用Adam优化器来训练模型，其中*η
    = 0.0001*，并且将衰减设置为*10^(-6)*：
- en: '[PRE13]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model is trained for `200` epochs with a batch size of `256` samples:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 模型以`256`个样本的批次大小训练了`200`个epoch：
- en: '[PRE14]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Even without a further analysis, we can immediately notice that the model is
    overfitted. After 200 epochs, the training accuracy is 1.0 with a loss close to
    0.0, while the validation accuracy is reasonably high, but with a validation loss
    slightly lower than the one obtained at the end of the second epoch.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有进一步的分析，我们也可以立即注意到模型过拟合了。在200个epoch后，训练准确率为1.0，损失接近0.0，而验证准确率合理，但验证损失略低于第二个epoch结束时的损失。
- en: 'To better understand what happened, it''s useful to plot both accuracy and
    loss during the training process:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解发生了什么，在训练过程中绘制准确率和损失是有用的：
- en: '![](img/61e814cc-f0c0-4603-b83b-5cbf451d941c.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61e814cc-f0c0-4603-b83b-5cbf451d941c.png)'
- en: 'As it''s possible to see, the validation loss reached a minimum during the
    first 10 epochs and immediately restarted to grow (this is sometimes called a
    U-curve because of its shape). At the same moment, the training accuracy reached
    1.0\. From that epoch on, the model started overfitting, learning a perfect structure
    of the training set, but losing the generalization ability. In fact, even if the
    final validation accuracy is rather high, the loss function indicates a lack of
    robustness when new samples are presented. As the loss is a categorical cross-entropy,
    the result can be interpreted as saying that the model has learned a distribution
    that partially mismatches the validation set one. As our goal is to use the model
    to predict new samples, this configuration could not be acceptable. Therefore,
    we try again, using some dropout layers. As suggested by the authors, we also
    increment the learning rate to 0.1 (switching to a Momentum SGD optimizer in order
    to avoid *explosions* due to adaptivity of RMSProp or Adam), initialize the weight
    with a uniform distribution (*-0.05, 0.05*), and impose a maximum norm constraint
    set to 2.0\. This choice allows the exploration of more sub-configurations without
    the risk of excessively high weights. The dropout is applied to the 25% of input
    units and to all ReLU fully connected layers with a percentage set to 50%:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，验证损失在最初的10个epoch达到了最小值，并立即开始增长（由于其形状有时被称为U曲线）。在同一时刻，训练准确率达到了1.0。从那个epoch开始，模型开始过拟合，学习训练集的完美结构，但失去了泛化能力。实际上，即使最终的验证准确率相当高，损失函数表明当呈现新的样本时，缺乏鲁棒性。由于损失是分类交叉熵，结果可以解释为模型学习了一个部分不匹配验证集的分布。由于我们的目标是使用模型来预测新的样本，这种配置是不可接受的。因此，我们再次尝试，使用一些dropout层。正如作者所建议的，我们还增加了学习率到0.1（切换到动量SGD优化器以避免由于RMSProp或Adam的自适应性导致的*爆炸*），使用均匀分布（*-0.05,
    0.05*）初始化权重，并施加最大范数约束，设置为2.0。这个选择允许探索更多的子配置，而不存在权重过高的风险。dropout应用于25%的输入单元和所有ReLU全连接层，百分比设置为50%：
- en: '[PRE15]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The training process is performed with the same parameters:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程使用相同的参数进行：
- en: '[PRE16]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The final condition is dramatically changed. The model is no longer overfitted
    (even if it''s possible to improve it in order to increase the validation accuracy)
    and the validation loss is lower than the initial one. To have a confirmation,
    let''s analyze the accuracy/loss plots:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最终条件发生了戏剧性的变化。模型不再过拟合（即使有可能通过提高验证准确率来改进它）并且验证损失低于初始值。为了确认这一点，让我们分析准确率/损失图：
- en: '![](img/bfd4857e-5576-4626-98a0-f628019f210a.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bfd4857e-5576-4626-98a0-f628019f210a.png)'
- en: The result shows some imperfections because the validation loss is almost flat
    for many epochs; however, the same model, with a higher learning rate and a weaker
    algorithm achieved a better final performance (0.988 validation accuracy) and
    a superior generalization ability. State-of-the-art models can also reach a validation
    accuracy equal to 0.995, but our goal was to show the effect of dropout layers
    in preventing the overfitting and, moreover, yielding a final configuration that
    is much more robust to new samples or noisy ones. I invite the reader to repeat
    the experiment with different parameters, bigger or smaller networks, and other
    optimization algorithms, trying to further reduce the final validation loss.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了一些不完美之处，因为验证损失在许多epoch中几乎呈平坦状态；然而，具有更高学习率和较弱算法的相同模型实现了更好的最终性能（0.988验证准确率）和更优越的泛化能力。最先进的模型也可以达到0.995的验证准确率，但我们的目标是展示dropout层在防止过拟合以及产生对新的样本或噪声样本具有更多鲁棒性的最终配置的效果。我邀请读者用不同的参数、更大或更小的网络和其他优化算法重复实验，以进一步降低最终的验证损失。
- en: 'Keras also implements two additional dropout layers: `GaussianDropout`, which
    multiplies the input samples by a Gaussian noise:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7859906-f6f3-4ce4-b4ff-c748763d1f5b.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: The value for the constant ρ can be set through the parameter `rate` (bounded
    between 0 and 1). When *ρ → 1*, *σ² → ∞*, while small values yield a null effect
    as *n ≈ 1*. This layer can be very useful as input one, in order to simulate a
    random data augmentation process. The other class is `AlphaDropout`, which works
    like the previous one, but renormalizing the output to keep the original mean
    and variance (this effect is very similar to the one obtained employing the technique
    described in the next paragraph together with noisy layers).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: When working with probabilistic layers (such as dropout), I always suggest setting
    the random seed (`np.random.seed(...)` and `tf.set_random_seed(...)` when Tensorflow
    backend is used). In this way, it's possible to repeat the experiments comparing
    the results without any bias. If the random seed is not explicitly set, every
    new training process will be different and it's not easy to compare the performances,
    for example, after a fixed number of epochs.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a mini-batch of *k* samples:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ffd163f-345d-4367-bb5c-8051d63d3911.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Before traversing the network, we can measure a mean and a variance:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87aea2d8-f5ec-4982-84f2-032efed1a32b.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
- en: 'After the first layer (for simplicity, let''s suppose that the activation function,
    *f(•)*, is the always the same), the batch is transformed into the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ebe382c-ae47-4a90-b01c-cccfd04593d3.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'In general, there''s no guarantee that the new mean and variance are the same.
    On the contrary, it''s easy to observe a modification that increases throughout
    the network. This phenomenon is called **covariate shift**, and it''s responsible
    for a progressive training speed decay due to the different adaptations needed
    in each layer. Ioffe and Szegedy (in *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift, Ioffe S., Szegedy C., arXiv:1502.03167
    [cs.LG]*) proposed a method to mitigate this problem, which has been called **batch
    normalization** (**BN**).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to renormalize the linear output of a layer (before or after applying
    the activation function), so that the batch has null mean and unit variance. Therefore,
    the first task of a BN layer is to compute:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0de4a53-1db8-4ac3-a6be-c0728afdeb5a.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: 'Then each sample is transformed into a normalized version (the parameter *δ*
    is included to improve the numerical stability):'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f936eb0-339a-41fd-9364-4dd0e875d1fc.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: 'However, as the batch normalization has no computational purposes other than
    speeding up the training process, the transformation must always be an identity
    (in order to avoid to distort and bias the data); therefore, the actual output
    will be obtained by applying the linear operation:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/129065a7-5bb8-4131-b0e2-290e84efd55c.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: 'The two parameters *α^((j))* and *β^((j))* are variables optimized by the SGD
    algorithm; therefore, each transformation is guaranteed not to alter the scale
    and the position of data. These layers are active only during the training phase
    (like dropout), but, contrary to other algorithms, they cannot be simply discarded
    when the model is used to make predictions on new samples because the output would
    be constantly biased. To avoid this problem, the authors suggest approximating
    both mean and variance of *X* by averaging over the batches (assuming that there
    are *N[b]* batches with *k* samples):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4d8ac24-285f-4aeb-8fc3-6e0c6da16b89.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'Using these values, the batch normalization layers can be transformed into
    the following linear operations:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b298eb1-53a3-440c-9bae-1cdf0d9622bb.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: It's not difficult to prove that this approximation becomes more and more accurate
    when the number of batches increases and that the error is normally negligible.
    However, when the batch size is very small, the statistics can be quite inaccurate;
    therefore, this method should be used considering the *representativeness* of
    a batch. If the data generating process is simple, even a small batch can be enough
    to describe the actual distribution. When, instead, *p[data]* is more complex,
    batch normalization requires larger batches to avoid wrong adjustments (a feasible
    strategy is to compare global mean and variance with the ones computed sampling
    some batches and trying to set the batch size that minimizes the discrepancy).
    However, this simple process can dramatically reduce the covariate shift and improve
    the convergence speed of very deep networks (including the famous residual networks).
    Moreover, it allows employing higher learning rates as the layers are implicitly
    *saturated* and can never *explode*. Additionally, it has been proven that batch
    normalization has also a secondary regularization effect even if it doesn't work
    on the weights. The reason is not very different from the one proposed for L2,
    but, in this case, there's a residual effect due to the transformation itself
    (partially caused by the variability of the parameters *α^((j))* and *β**^((j))*)
    that can encourage the exploration of different regions of the sample space. However,
    this is not the primary effect, and it's not a good practice employing this method
    as a regularizer.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Example of batch normalization with Keras
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to show the feature of this technique, let''s repeat the previous
    example using an MLP without dropout but applying a batch normalization after
    each fully connected layer before the ReLU activation. The example is very similar
    to the first one, but, in this case, we increase the Adam learning rate to 0.001
    keeping the same decay:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now train using the same parameters again:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The model is again overfitted, but now the final validation accuracy is only
    slightly higher than the one achieved using the dropout layers. Let''s plot accuracy
    and loss to better analyze the training process:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 模型再次过拟合，但现在最终的验证准确率仅略高于使用dropout层所达到的准确率。让我们绘制准确率和损失曲线以更好地分析训练过程：
- en: '![](img/be30b058-1319-423c-8c2e-9a3dde35d229.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/be30b058-1319-423c-8c2e-9a3dde35d229.png)'
- en: The effect of the batch normalization improved the performances and slowed down
    the overfitting. At the same time, the elimination of the covariate shift avoided
    the U-curve keeping a quite low validation loss. Moreover, the model reached a
    validation accuracy of about 0.99 during the epochs 135-140 with a residual positive
    trend. Analogously to the previous example, this solution is imperfect, but it's
    a good starting point for further optimization. It would be a good idea to continue
    the training process for a larger number of epochs, monitoring both the validation
    loss and accuracy. Moreover, it's possible to mix dropout and batch normalization
    or experiment with the Keras AlphaDropout layer. However, if, in the first example
    (without dropout), the climax of training accuracy was associated with a starting
    positive trend for the validation loss, in this case, the learned distribution
    doesn't seem to be very different from the validation set one. In other words,
    batch normalization is not preventing overfitting the training set, but it's avoiding
    a decay in the generalization ability (observed when there was no batch normalization).
    I suggest repeating the test with other hyperparameter and architectural configurations
    in order to decide whether this model can be used for prediction purposes or it's
    better to look for other solutions.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 批标准化（batch normalization）的效果提高了性能并减缓了过拟合。同时，消除协变量偏移避免了U型曲线，保持了相当低的验证损失。此外，模型在135-140个epoch期间达到了大约0.99的验证准确率，并呈现出残余的正趋势。与之前的例子类似，这个解决方案并不完美，但它是进一步优化的良好起点。继续进行更多epoch的训练过程，同时监控验证损失和准确率将是一个好主意。此外，可以混合使用dropout和批标准化，或者尝试Keras的AlphaDropout层。然而，如果在第一个例子（没有dropout）中，训练准确率的峰值与验证损失的正趋势相关联，那么在这种情况下，学习到的分布似乎与验证集的分布没有太大区别。换句话说，批标准化并没有防止训练集过拟合，但它避免了泛化能力下降（在没有批标准化的情况下观察到）。我建议使用其他超参数和架构配置重复测试，以决定这个模型是否可用于预测目的，或者是否需要寻找其他解决方案。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started the exploration of the deep learning world by introducing
    the basic concepts that led the first researchers to improve the algorithms until
    they achieved the top results we have nowadays. The first part explained the structure
    of a basic artificial neuron, which combines a linear operation followed by an
    optional non-linear scalar function. A single layer of linear neurons was initially
    proposed as the first neural network, with the name of the perceptron.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过介绍引领第一代研究人员改进算法直至达到如今顶级结果的初步概念，开始了对深度学习世界的探索。第一部分解释了基本人工神经元结构，它结合了一个线性操作，随后是一个可选的非线性标量函数。最初提出单层线性神经元作为第一个神经网络，命名为感知器。
- en: Even though it was quite powerful for many problems, this model soon showed
    its limitations when working with non-linear separable datasets. A perceptron
    is not very different from a logistic regression, and there's no concrete reason
    to employ it. Nevertheless, this model opened the doors to a family of extremely
    powerful models obtained combining multiple non-linear layers. The multilayer
    perceptron, which has been proven to be a universal approximator, is able to manage
    almost any kind of dataset, achieving high-level performances when other methods
    fail.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个模型在许多问题上都非常强大，但当与非线性可分的数据集一起工作时，它很快便显示了其局限性。感知器与逻辑回归并没有太大的区别，也没有具体的原因去使用它。尽管如此，这个模型为通过结合多个非线性层获得的一族极其强大的模型打开了大门。多层感知器已被证明是一个通用逼近器，能够处理几乎任何类型的数据集，在其它方法失败时也能实现高级别的性能。
- en: In the next section, we analyzed the building bricks of an MLP. We started with
    the activation functions, describing their structure and features, and focusing
    on the reasons they lead the choice for specific problems. Then, we discussed
    the training process, considering the basic idea behind the back-propagation algorithm
    and how it can be implemented using the stochastic gradient descent method. Even
    if this approach is quite effective, it can be slow when the complexity of the
    network is very high. For this reason, many optimization algorithms were proposed.
    In this chapter, we analyzed the role of momentum and how it's possible to manage
    adaptive corrections using RMSProp. Then, we combined both, momentum and RMSProp
    to derive a very powerful algorithm called Adam. In order to provide a complete
    vision, we also presented two slightly different adaptive algorithms, called AdaGrad
    and AdaDelta.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们分析了多层感知器（MLP）的构建块。我们从激活函数开始，描述了它们的结构和特性，并重点讨论了它们为何能针对特定问题做出选择。然后，我们讨论了训练过程，考虑了反向传播算法背后的基本思想以及如何使用随机梯度下降法实现它。尽管这种方法相当有效，但当网络复杂度非常高时，它可能会很慢。因此，提出了许多优化算法。在本章中，我们分析了动量（momentum）的作用以及如何使用
    RMSProp 管理自适应校正。然后，我们将动量和 RMSProp 结合起来，推导出一个非常强大的算法，称为 Adam。为了提供一个完整的视角，我们还介绍了两种略有不同的自适应算法，称为
    AdaGrad 和 AdaDelta。
- en: In the next sections, we discussed the regularization methods and how they can
    be plugged into a Keras model. An important section was dedicated to a very diffused
    technique called dropout, which consists in setting to zero (dropping) a fixed
    percentage of samples through a random selection. This method, although very simple,
    prevents the overfitting of very deep networks and encourages the exploration
    of different regions of the sample space,  obtaining a result not very dissimilar
    to the ones analyzed in [Chapter 8](78baef9c-5391-4898-91bf-8df25330a163.xhtml),
    *Ensemble Learning*. The last topic was the batch normalization technique, which
    is a method to reduce the mean and variance shift (called covariate shift) caused
    by subsequent neural transformations. This phenomenon can slow down the training
    process as each layer requires different adaptations and it's more difficult to
    move all the weights in the best direction. Applying batch normalization means
    very deep networks can be trained in a shorter time, thanks also to the possibility
    of employing higher learning rates.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们讨论了正则化方法以及它们如何集成到 Keras 模型中。一个重要的部分是关于一种非常普遍的技术——dropout，它通过随机选择将固定百分比的样本设置为零（丢弃）。这种方法虽然非常简单，但可以防止非常深层网络的过拟合，并鼓励探索样本空间的各个不同区域，得到的结果与第
    8 章[集成学习](78baef9c-5391-4898-91bf-8df25330a163.xhtml)中分析的结果不太相似。最后一个主题是批量归一化技术，这是一种减少由后续神经网络变换引起的均值和方差偏移（称为协变量偏移）的方法。这种现象可能会减慢训练过程，因为每一层都需要不同的调整，并且更难将所有权重移动到最佳方向。应用批量归一化意味着非常深的网络可以在更短的时间内训练，这也要归功于可以使用更高的学习率。
- en: In the next chapter, we are going to continue this exploration, analyzing very
    important advanced layers like convolutions (that achieve extraordinary performances
    in image-oriented tasks) and recurrent units (for the processing of time series)
    and discussing some practical applications that can be experimented on and readapted
    using Keras and Tensorflow.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续这一探索，分析非常重要的高级层，如卷积（在面向图像的任务中实现非凡的性能）和循环单元（用于处理时间序列），并讨论一些可以使用 Keras
    和 Tensorflow 进行实验和重新调整的实际应用。
