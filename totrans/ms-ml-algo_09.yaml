- en: Neural Networks for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is the introduction to the world of deep learning, whose methods
    make it possible to achieve the state-of-the-art performance in many classification
    and regression fields often considered extremely difficult to manage (such as
    image segmentation, automatic translation, voice synthesis, and so on). The goal
    is to provide the reader with the basic instruments to understand the structure
    of a fully connected neural network and model it using the Python tool Keras (employing
    all the modern techniques to speed the training process and prevent overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the topics covered in the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The structure of a basic artificial neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perceptrons, linear classifiers, and their limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer perceptrons with the most important activation functions (such as
    ReLU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back-propagation algorithms based on **stochastic gradient descent** (**SGD**)
    optimization method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized SGD algorithms (Momentum, RMSProp, Adam, AdaGrad, and AdaDelta)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization and dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic artificial neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The building block of a neural network is the abstraction of a biological neuron,
    a quite simplistic but powerful computational unit that was proposed for the first
    time by F. Rosenblatt in 1957, to make up the simplest neural architecture, called
    a perceptron, that we are going to analyze in the next section. Contrary to Hebbian
    Learning, which is more biologically plausible but has some strong limitations,
    the artificial neuron has been designed with a pragmatic viewpoint and, of course,
    only its structure is based on a few elements characterizing a biological cell.
    However, recent deep learning research activities have unveiled the enormous power
    of this kind of architecture. Even if there are more complex and specialized computational
    cells, the basic artificial neuron can be summarized as the conjunction of two
    blocks, which are clearly shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cac7847b-851e-4a7e-81a8-6bb81174669a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The input of a neuron is a real-valued vector *x ∈ ℜ^n*, while the output is
    a scalar *y ∈ ℜ*. The first operation is linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0963dba1-7a8a-43f1-9f8c-00dbc8866620.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The vector *w ∈ ℜ^n* is called **weight-vector** (or **synaptic weight vector**,
    because, analogously to a biological neuron, it reweights the input values), while
    the scalar term *b ∈ ℜ* is a constant called **bias**. In many cases, it''s easier
    to consider only the weight vector. It''s possible to get rid of the bias by adding
    an extra input feature equal to 1 and a corresponding weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cf0597d-6932-4e7b-b990-94e3981c2441.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, the only element that must be learned is the weight vector. The
    following block is called an **activation function**, and it's responsible for
    remapping the input into a different subset. If the function is *f[a](z) = z*,
    the neuron is called linear and the transformation can be omitted. The first experiments
    were based on linear neurons that are much less powerful than non-linear ones,
    and this was a reason that led many researchers to consider the perceptron as
    a failure, but, at the same time, this limitation opened the door for a new architecture
    that, instead, showed its excellent abilities. Let's now start this analysis with
    the first neural network ever proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Perceptron** was the name that Frank Rosenblatt gave to the first neural
    model in 1957\. A perceptron is a neural network with a single layer of input
    linear neurons, followed by an output unit based on the *sign(•)* function (alternatively,
    it''s possible to consider a bipolar unit whose output is -1 and 1). The architecture
    of a perceptron is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45010c23-8b99-4016-ad8d-cc0490489977.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even if the diagram can appear as quite complex, a perceptron can be summarized
    by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49fbeabc-88f8-4bfa-bc16-2a1efb7a151c.png)'
  prefs: []
  type: TYPE_IMG
- en: All the vectors are conventionally column-vectors; therefore, the dot product
    *w^Tx[i]* transforms the input into a scalar, then the bias is added, and the
    binary output is obtained using the step function, which outputs 1 when *z > 0*
    and 0 otherwise. At this point, a reader could object that the step function is
    non-linear; however, a non-linearity applied to the output layer is only a filtering
    operation that has no effect on the actual computation. Indeed, the output is
    already decided by the linear block, while the step function is employed only
    to impose a binary threshold. Moreover, in this analysis, we are considering only
    single-value outputs (even if there are multi-class variants) because our goal
    is to show the dynamics and also the limitations, before moving to more generic
    architectures that can be used to solve extremely complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'A perceptron can be trained with an online algorithm (even if the dataset is
    finite) but it''s also possible to employ an offline approach that repeats for
    a fixed number of iterations or until the total error becomes smaller than a predefined
    threshold. The procedure is based on the squared error loss function (remember
    that, conventionally, the term *loss* is applied to single samples, while the
    term *cost* refers to the sum/average of every single loss):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bf91875-302e-4805-8124-d040e18149ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When a sample is presented, the output is computed, and if it is wrong, a weight
    correction is applied (otherwise the step is skipped). For simplicity, we don''t
    consider the bias, as it doesn''t affect the procedure. Our goal is to correct
    the weights so as to minimize the loss. This can be achieved by computing the
    partial derivatives with respect to *w[i]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef339462-311a-4e70-8e75-2577230ae0d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s suppose that *w^((0)) = (0, 0)* (ignoring the bias) and the sample,
    *x = (1, 1)*, has *y = 1*. The perceptron misclassifies the sample, because *sign(w^Tx)
    = 0*. The partial derivatives are both equal to -1; therefore, if we subtract
    them from the current weights, we obtain *w^((1))* *= (1, 1)* and now the sample
    is correctly classified because *sign(w^Tx) = 1*. Therefore, including a learning
    rate *η*, the weight update rule becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c35fd06-e3ac-406f-be6b-786b945c9a02.png)'
  prefs: []
  type: TYPE_IMG
- en: When a sample is misclassified, the weights are corrected proportionally to
    the difference between actual linear output and true label. This is a variant
    of a learning rule called the **delta rule**, which represented the first step
    toward the most famous training algorithm, employed in almost any supervised deep
    learning scenario (we're going to discuss it in the next sections). The algorithm
    has been proven to converge to a stable solution in a finite number of states
    as the dataset is linearly separable. The formal proof is quite tedious and very
    technical, but the reader who is interested can find it in *Perceptrons, Minsky
    M. L., Papert S. A., The MIT Press*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, the role of the learning rate becomes more and more important,
    in particular when the update is performed after the evaluation of a single sample
    (like in a perceptron) or a small batch. In this case, a high learning rate (that
    is, one greater than 1.0) can cause an instability in the convergence process
    because of the magnitude of the single corrections. When working with neural networks,
    it's preferable to use a small learning rate and repeat the training session for
    a fixed number of epochs. In this way, the single corrections are limited, and
    only if they are *confirmed* by the majority of samples/batches, they can become
    stable, driving the network to converge to an optimal solution. If, instead, the
    correction is the consequence of an outlier, a small learning rate can limit its
    action, avoiding destabilizing the whole network only for a few noisy samples.
    We are going to discuss this problem in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can describe the full perceptron algorithm and close the paragraph
    with some important considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a value for the learning rate *η* (such as `0.1`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append a constant column (set to `1.0`) to the sample vector *X*. Therefore
    *X[b] ∈ ℜ^(M × (n+1))*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the weight vector *w ∈ ℜ^(n+1)* with random values sampled from a
    normal distribution with a small variance (such as `0.05`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set an error threshold `Thr` (such as `0.0001`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of iterations *N[i]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `i = 0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `e = 1.0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *i < N[i]* and *e > Thr*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `e = 0.0`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *k=1* to *M*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the linear output *l[k] = w^Tx[k]* and the threshold one *t[k] = sign(l[k])*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *t[k] != y*[*k*:]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *Δw[j] = η(l[k] - y[k])x[k]^((j))*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weight vector.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *e += (l[k] - y[k])²* (alternatively it's possible to use the absolute value
    *|l[k] - y[k]|*).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `e /= M`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The algorithm is very simple, and the reader should have noticed an analogy
    with a logistic regression. Indeed, this method is based on a structure that can
    be considered as a perceptron with a sigmoid output activation function (that
    outputs a real value that can be considered as a probability). The main difference
    is the training strategy—in a logistic regression, the correction is performed
    after the evaluation of a cost function based on the negative log likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2476673b-93c2-4fba-a649-9af13799e415.png)'
  prefs: []
  type: TYPE_IMG
- en: This cost function is the well-known cross-entropy and, in the first chapter,
    we showed that minimizing it is equivalent to reducing the Kullback-Leibler divergence
    between the true and predicted distribution. In almost all deep learning classification
    tasks, we are going to employ it, thanks to its robustness and convexity (this
    is a convergence guarantee in a logistic regression, but unfortunately the property
    is normally lost in more complex architectures).
  prefs: []
  type: TYPE_NORMAL
- en: Example of a perceptron with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even if the algorithm is very simple to implement from scratch, I prefer to
    employ the Scikit-Learn implementation `Perceptron`, so as to focus the attention
    on the limitations that led to non-linear neural networks. The *historical* problem
    that showed the main weakness of a perceptron is based on the XOR dataset. Instead
    of explaining, it''s better to build it and visualize the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot showing the true labels is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f047a4fc-fff1-4018-b6ea-566443d4ca3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of XOR dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As it''s possible to see, the dataset is split into four blocks that are organized
    as the output of a logical XOR operator. Considering that the separation hypersurface
    of a two-dimensional perceptron (as well as the one of a logistic regression)
    is a line; it''s easy to understand that any possible final configuration can
    achieve an accuracy that is about 50% (a random guess). To have a confirmation,
    let''s try to solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Scikit-Learn implementation offers the possibility to add a regularization
    term (see [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine Learning
    Models Fundamentals*) through the parameter `penalty` (it can be `''l1''`, `''l2''`
    or `''elasticnet''`) to avoid overfitting and improve the convergence speed (the
    strength can be specified using the parameter `alpha`). This is not always necessary,
    but as the algorithm is offered in a production-ready package, the designers decided
    to add this feature. Nevertheless, the average cross-validation accuracy is slightly
    higher than 0.5 (the reader is invited to test any other possible hyperparameter
    configuration). The corresponding plot (that can change with different random
    states or subsequent experiments) is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b082166a-bf90-4f69-95d5-3102b88ebf13.png)'
  prefs: []
  type: TYPE_IMG
- en: XOR dataset labeled using a perceptron
  prefs: []
  type: TYPE_NORMAL
- en: It's obvious that a perceptron is another linear model without specific peculiarities,
    and its employment is discouraged in favor of other algorithms like logistic regression
    or SVM. After 1957, for a few years, many researchers didn't hide their delusion
    and considered the neural network like a promise never fulfilled. It was necessary
    to wait until a simple modification to the architecture, together with a powerful
    learning algorithm, opened officially the door of a new fascinating machine learning
    branch (later called **deep learning**).
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-Learn > 0.19, the class `Perceptron` allows adding `max_iter` or `tol`
    (tolerance) parameters. If not specified, a warning will be issued to inform the
    reader about the future behavior. This piece of information doesn't affect the
    actual results.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main limitation of a perceptron is its linearity. How is it possible to
    exploit this kind of architecture by removing such a constraint? The solution
    is easier than any speculation. Adding at least a non-linear layer between input
    and output leads to a highly non-linear combination, parametrized with a larger
    number of variables. The resulting architecture, called **Mu****ltilayer Perceptron**
    (**MLP**) and containing a single (only for simplicity) **Hidden Layer***,* is
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a6cfef0-a339-4c00-8abb-e469a2fb4b31.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a so-called **feed-forward network**, meaning that the flow of information
    begins in the first layer, proceeds always in the same direction and ends at the
    output layer. Architectures that allow a partial feedback (for example, in order
    to implement a local memory) are called **recurrent** **networks** and will be
    analyzed in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, there are two weight matrices, *W* and *H*, and two corresponding
    bias vectors, *b* and *c*. If there are *m* hidden neurons, *x[i] ∈ ℜ^(n × 1)*
    (column vector), and *y[i] ∈ ℜ^(k × 1)*, the dynamics are defined by the following
    transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a844411b-2b60-42ba-9292-5d9fde341e6e.png)'
  prefs: []
  type: TYPE_IMG
- en: A fundamental condition for any MLP is that at least one hidden-layer activation
    function *f[h](•)* is non-linear. It's straightforward to prove that *m* linear
    hidden layers are equivalent to a single linear network and, hence, an MLP falls
    back into the case of a standard perceptron. Conventionally, the activation function
    is fixed for a given layer, but there are no limitations in their combinations.
    In particular, the output activation is normally chosen to meet a precise requirement
    (such as multi-label classification, regression, image reconstruction, and so
    on). That's why the first step of this analysis concerns the most common activation
    functions and their features.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, any continuous and differentiable function could be employed as
    activation; however, some of them have particular properties that allow achieving
    a good accuracy while improving the learning process speed. They are commonly
    used in the state-of-the-art models, and it's important to understand their properties
    in order to make the most reasonable choice.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid and hyperbolic tangent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These two activations are very similar but with an important difference. Let''s
    start defining them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/818ed54f-f955-4666-aa7c-ee6feb633b3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding plots are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33cd940d-83ba-4471-b926-c6159dfcf26b.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid and hyperbolic tangent plots
  prefs: []
  type: TYPE_NORMAL
- en: A sigmoid *σ(x)* is bounded between 0 and 1, with two asymptotes (*σ(x) → 0
    when x → -∞* and *σ(x) → 1 when x → ∞*). Similarly, the hyperbolic tangent (`tanh`)
    is bounded between -1 and 1 with two asymptotes corresponding to the extreme values.
    Analyzing the two plots, we can discover that both functions are almost linear
    in a short range (about *[-2, 2]*), and they become almost flat immediately after.
    This means that the gradient is high and about constant when *x* has small values
    around 0 and it falls down to about 0 for larger absolute values. A sigmoid perfectly
    represents a probability or a set of weights that must be bounded between 0 and
    1, and therefore, it can be a good choice for some output layers. However, the
    hyperbolic tangent is completely symmetric, and, for optimization purposes, it's
    preferable because the performances are normally superior. This activation function
    is often employed in intermediate layers, whenever the input is normally small.
    The reason will be clear when the back-propagation algorithm is analyzed; however,
    it's obvious that large absolute inputs lead to almost constant outputs, and as
    the gradient is about 0, the weight correction can become extremely slow (this
    problem is formally known as **vanishing gradients**). For this reason, in many
    real-world applications, the next family of activation functions is often employed.
  prefs: []
  type: TYPE_NORMAL
- en: Rectifier activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These functions are all linear (or quasi-linear for Swish) when *x > 0*, while
    they differ when *x < 0*. Even if some of them are differentiable when *x = 0*,
    the derivative is set to `0` in this case. The most common functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87aa3fd6-522b-4ebb-9acd-2325e25a7102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding plots are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da31e807-da0a-4604-bf32-c89f3ac08674.png)'
  prefs: []
  type: TYPE_IMG
- en: The basic function (and also the most commonly employed) is the ReLU, which
    has a constant gradient when *x > 0*, while it is null for *x < 0*. This function
    is very often employed in visual processing when the input is normally greater
    than 0 and has the extraordinary advantage to mitigate the vanishing gradient
    problem, as a correction based on the gradient is always possible. On the other
    side, ReLU is null (together with its first derivative) when *x < 0*, therefore
    every negative input doesn't allow any modification. In general, this is not an
    issue, but there are some deep networks that perform much better when a small
    negative gradient was allowed. This consideration drove to the other variants,
    which are characterized by the presence of the hyperparameter *α*, that controls
    the strength of the *negative tail*. Common values between 0.01 and 0.1 allow
    a behavior that is almost identical to ReLU, but with the possibility of a small
    weight update when *x < 0*. The last function, called Swish and proposed in *Searching
    for Activation Functions, Ramachandran P., Zoph P., Le V. L., arXiv:1710.05941
    [cs.NE]*, is based on the sigmoid and offers the extra advantage to converge to
    0 when *x → 0*, so the *non-null effect* is limited to a short region bounded
    between *[-b, 0]* with *b > 0*. This function can improve the performance of some
    particular visual processing deep networks, as discussed in the aforementioned
    paper. However, I always suggest starting the analysis with ReLU (that is very
    robust and computationally inexpensive) and switch to an alternative only if no
    other techniques can improve the performance of a model.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This function characterized the output layer of almost all classification networks,
    as it can immediately represent a discrete probability distribution. If there
    are *k* outputs *y[i]*, the softmax is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1271b390-d7d7-4eee-a438-565e3ddf6ee6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this way, the output of a layer containing *k* neurons is normalized so
    that the sum is always 1\. It goes without saying that, in this case, the best
    cost function is the cross-entropy. In fact, if all true labels are represented
    with a one-hot encoding, they implicitly become probability vectors with 1 corresponding
    to the true class. The goal of the classifier is hence to reduce the discrepancy
    between the training distribution of its output by minimizing the function (see
    [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine Learning Models
    Fundamentals*, for further information):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f10de73-0460-4cde-9e68-370467df39b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Back-propagation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now discuss the training approach employed in an MLP (and almost all
    other neural networks). This algorithm is more a methodology than an actual one;
    therefore I prefer to define the main concepts without focusing on a particular
    case. The reader who is interested in implementing it will be able to apply the
    same techniques to different kinds of networks with minimal effort (assuming that
    all requirements are met).
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of a training process using a deep learning model is normally achieved
    by minimizing a cost function. Let''s suppose to have a network parameterized
    with a global vector θ, the cost function (using the same notation for loss and
    cost but with different parameters to disambiguate) is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d0a5dfb-1bf8-4371-816c-06620a147ce3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have already explained that the minimization of the previous expression
    (which is the empirical risk) is a way to minimize the real expected risk and,
    therefore, to maximize the accuracy. Our goal is, hence, to find an optimal parameter
    set so that the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6088201-8503-45cc-939a-ebb7c87122b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we consider a single loss function (associated with a sample x[i] and a
    true label *y[i]*), we know that such a function can be expressed with an explicit
    dependence on the predicted value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ed57317-d234-4ff0-baec-8270e920a653.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the parameters have been *embedded* into the prediction. From calculus
    (without an excessive mathematical rigor that can be found in many books about
    optimization techniques), we know that the gradient of *L*, a scalar function,
    computed at any point (we are assuming the *L* is differentiable) is a vector
    with components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3686bcc1-3894-41fd-88ef-97a7cb63df4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As *∇L* points always in the direction of the closest maximum, so the negative
    gradient points in the direction of the closest minimum. Hence, if we compute
    the gradient of *L*, we have a ready-to-use piece of information that can be used
    to minimize the cost function. Before proceeding, it''s useful to expose an important
    mathematical property called the **chain rule of derivatives**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12aa74d3-62b3-489d-ae4e-da19eace1537.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s consider a single step in an MLP (starting from the bottom) and
    let''s exploit the chain rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bfe13f3-ac2c-4200-bf7e-3043d56a51cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each component of the vector *y* is independent of the others, so we can simplify
    the example by considering only an output value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2707a46b-ef56-45ca-9655-91d2b2494f8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression (discarding the bias), there are two important elements—the
    weights, *h[j]* (that are the columns of *H*), and the expression, *z[j]*, which
    is a function of the previous weights. As *L* is, in turn, a function of all predictions,
    *y[i]*, applying the chain rule (using the variable *t* as the generic argument
    of the activation functions), we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/977e1cdb-f888-4a18-80a7-e78f83a6f59c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we normally cope with vectorial functions, it''s easier to express this
    concept using the gradient operator. Simplifying the transformations performed
    by a generic layer, we can express the relations (with respect to a row of *H*,
    so to a weight vector *h[i]* corresponding to a hidden unit, *z[i]*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/907957cb-c2f1-427c-bfdd-b5e7cd4eb261.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Employing the gradient and considering the vectorial output *y* can be written
    as *y = (y[1], y[2], ..., y[m])*, we can derive the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc908237-c2dc-4d74-ae2c-b2cf8669681e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this way we get all the components of the gradient of *L* computed with
    respect to the weightvectors, *h[i]*. If we move back, we can derive the expression
    of *z[j]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edac1e1f-6f2d-46e9-9e4f-d3ffe2230e4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reapplying the chain rule, we can compute the partial derivative of *L* with
    respect to *w[pj]* (to avoid confusion, the argument of the prediction *y[i]*
    is called *t[1]*, while the argument of *z[j]* is called *t[2]*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/351d0b2a-0865-4cf6-b84b-71a637a5c261.png)'
  prefs: []
  type: TYPE_IMG
- en: Observing this expression (that can be easily rewritten using the gradient)
    and comparing it with the previous one, it's possible to understand the philosophy
    of the **back-propagation algorithm**, presented for the first time in *Learning
    representations by back-propagating errors, Rumelhart D. E., Hinton G. E., Williams
    R. J., Nature 323/1986*. The samples are fed into the network and the cost function
    is computed. At this point, the process starts from the bottom, computing the
    gradients with respect to the closest weights and reusing a part of the calculation
    δ[i] (proportional to the error) to move back until the first layer is reached.
  prefs: []
  type: TYPE_NORMAL
- en: The correction is indeed propagated from the source (the cost function) to the
    origin (the input layer), and the effect is proportional to the responsibility
    of each different weight (and bias).
  prefs: []
  type: TYPE_NORMAL
- en: Considering all the possible different architectures, I think that writing all
    the equations for a single example is useless. The methodology is conceptually
    simple, and it's purely based on the chain rule of derivatives. Moreover, all
    existing frameworks, such as Tensorflow, Caffe, CNTK, PyTorch, Theano, and so
    on, can compute the gradients for all weights of a complete network with a single
    operation, so as to allow the user to focus attention on more pragmatic problems
    (like finding the best way to avoid overfitting and improving the training process).
  prefs: []
  type: TYPE_NORMAL
- en: 'A very important phenomenon that is worth considering was already outlined
    in the previous section and now it should be clearer: the chain rule is based
    on multiplications; therefore, when the gradients start to become smaller than
    1, the multiplication effect forces the last values to be close to 0\. This problem
    is known as **vanishing gradients** and can really stop the training process of
    very deep models that use saturating activation functions (like `sigmoid` or `tanh`).
    Rectifier units provide a good solution to many specific issues, but sometimes
    when functions like hyperbolic tangent are necessary, other methods, like normalization,
    must be employed to mitigate the phenomenon. We are going to discuss some specific
    techniques in this chapter and in the next one, but a generic best practice is
    to work always with normalized datasets and, if necessary, also testing the effect
    of whitening.'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the gradients have been computed, the cost function can be *moved* in the
    direction of its minimum. However, in practice, it is better to perform an update
    after the evaluation of a fixed number of training samples (batch). Indeed, the
    algorithms that are normally employed don't compute the global cost for the whole
    dataset, because this operation could be very computationally expensive. An approximation
    is obtained with partial steps, limited to the experience accumulated with the
    evaluation of a small subset. According to some literature, the expression **stochastic
    gradient descent** (**SGD**) should be used only when the update is performed
    after every single sample. When this operation is carried out on every *k* sample,
    the algorithm is also known as **mini-batch gradient descent**; however, conventionally
    SGD is referred to all batches containing *k ≥ 1* samples, and we are going to
    use this expression from now on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process can be expressed considering a partial cost function computed using
    a batch containing *k* samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dea3d8af-284f-4464-b8e0-17fa793615dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm performs a gradient descent by updating the weights according
    to the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d787dbc1-4a87-49bc-b7d7-6ecdfc897b97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we start from an initial configuration *θ[start]*, the stochastic gradient
    descent process can be imagined like the path shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8c4f4f1-9db4-40be-8714-64da4b095753.png)'
  prefs: []
  type: TYPE_IMG
- en: The weights are moved towards the minimum *θ*[*opt*,] with many subsequent corrections
    that could also be wrong considering the whole dataset. For this reason, the process
    must be repeated several times (epochs), until the validation accuracy reaches
    its maximum. In a perfect scenario, with a convex cost function L, this simple
    procedure converges to the optimal configuration. Unfortunately, a deep network
    is a very complex and non-convex function where plateaus and saddle points are
    quite common (see [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine
    Learning Models Fundamentals*). In such a scenario, a *vanilla* SGD wouldn't be
    able to find the global optimum and, in many cases, could not even find a close
    point. For example, in flat regions, the gradients can become so small (also considering
    the numerical imprecisions) as to slow down the training process until no change
    is possible (so *θ^((t+1)) ≈ θ^((t))*). In the next section, we are going to present
    some common and powerful algorithms that have been developed to mitigate this
    problem and dramatically accelerate the convergence of deep models.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on, it's important to mark two important elements. The first one
    concerns the learning rate, *η*. This hyperparameter plays a fundamental role
    in the learning process. As also shown in the figure, the algorithm proceeds jumping
    from a point to another one (which is not necessarily closer to the optimum).
    Together with the optimization algorithms, it's absolutely important to correctly
    tune up the learning rate. A high value (such as 1.0) could move the weights too
    rapidly increasing the instability. In particular, if a batch contains a few outliers
    (or simply non-dominant samples), a large *η* will consider them as representative
    elements, correcting the weights so to minimize the error. However, subsequent
    batches could better represent the data generating process, and, therefore, the
    algorithm must partially *revert* its modifications in order to compensate the
    wrong update. For this reason, the learning rate is usually quite small with common
    values bounded between 0.0001 and 0.01 (in some particular cases, *η = 0.1* can
    be also a valid choice). On the other side, a very small learning rate leads to
    minimum corrections, slowing down the training process. A good trade-off, which
    is often the best practice, is to let the learning rate decay as a function of
    the epoch. In the beginning, *η* can be higher, because the probability to be
    close to the optimum is almost null; so, larger jumps can be easily adjusted.
    While the training process goes on, the weights are progressively moved towards
    their final configuration and, hence, the corrections become smaller and smaller.
    In this case, large jumps should be avoided, preferring a fine-tuning. That's
    why the learning rate is decayed. Common techniques include the exponential decay
    or a linear one. In both cases, the initial and final values must be chosen according
    to the specific problem (testing different configurations) and the optimization
    algorithm. In many cases, the ratio between the start and end value is about 10
    or even larger.
  prefs: []
  type: TYPE_NORMAL
- en: Another important hyperparameter is the batch size. There are no silver bullets
    that allow us to automatically make the right choice, but some considerations
    can be made. As SGD is an approximate algorithm, larger batches drive to corrections
    that are probably more similar to the ones obtained considering the whole dataset.
    However, when the number of samples is extremely high, we don't expect the deep
    model to map them with a one-to-one association, but instead our efforts are directed
    to improving the generalization ability. This feature can be re-expressed saying
    that the network must learn a smaller number of abstractions and reuse them in
    order to classify new samples. A batch, if sampled correctly, contains a part
    of these *abstract elements* and part of the corrections automatically improve
    the evaluation of a subsequent batch. You can imagine a waterfall process, where
    a new training step never starts from scratch. However, the algorithm is also
    called mini-batch gradient descent, because the usual batch size normally ranges
    from 16 to 512 (larger sizes are uncommon, but always possible), which are values
    smaller than the number of total samples (in particular in deep learning contexts).
    A reasonable default value could be 32 samples, but I always invite the reader
    to test larger values, comparing the performances in terms of training speed and
    final accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: When working with deep neural networks, all the values (number of neurons in
    a layer, batch size, and so on) are normally powers of two. This is not a constraint,
    but only an optimization tip (above all when using GPUs), as the memory can be
    more efficiently filled when the blocks are based on a *2^N* elements. However,
    this is only a suggestion, whose benefits could also be negligible; so, don't
    be afraid to test architectures with different values. For example, in many papers,
    the batch size is 100 or some layers have 1,000 neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very important element is the initial configuration of a neural network. How
    should the weights be initialized? Let's imagine we that have set them all to
    zero. As all neurons in a layer receive the same input, if the weights are 0 (or
    any other common, constant number), the output will be equal. When applying the
    gradient correction, all neurons will be treated in the same way; so, the network
    is equivalent to a sequence of single neuron layers. It's clear that the initial
    weights must be different to achieve a goal called symmetry breaking, but which
    is the best choice?
  prefs: []
  type: TYPE_NORMAL
- en: If we knew (also approximately) the final configuration, we could set them to
    easily reach the optimal point in a few iterations, but, unfortunately, we have
    no idea where the minimum is located. Therefore, some empirical strategies have
    been developed and tested, with the goal of minimizing the training time (obtaining
    state-of-the-art accuracies). A general rule of thumb is that the weights should
    be small (compared to the input sample variance). Large values lead to large outputs
    that negatively impact on saturating functions (such as `tanh` and `sigmoid`),
    while small values can be more easily optimized because the corresponding gradients
    are larger and the corrections have a stronger effect. The same is true also for
    rectifier units because the maximum efficiency is achieved by working in a segment
    crossing the origin (where the non-linearity is actually *located*). For example,
    when coping with images, if the values are positive and large, a ReLU neuron becomes
    almost a linear unit, losing a lot of its advantages (that's why images are normalized,
    so as to bound each pixel value between 0 and 1 or -1 and 1).
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, ideally, the activation variances should remain almost constant
    throughout the network, as well as the weight variances after every back-propagation
    step. These two conditions are fundamental in order to improve the convergence
    process and to avoid the vanishing and exploding gradient problems (the latter,
    which is the opposite of vanishing gradients, will be discussed in the section
    dedicated to recurrent network architectures).
  prefs: []
  type: TYPE_NORMAL
- en: 'A very common strategy considers the number of neurons in a layer and initializes
    the weights as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84f592fa-d6d8-402f-811c-a1c7fd37492b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This method is called **variance scaling** and can be applied using the number
    of input units (Fan-In), the number of output units (Fan-Out), or their average.
    The idea is very intuitive: if the number of incoming or outgoing connections
    is large, the weights must be smaller, so as to avoid large outputs. In the degenerate
    case of a single neuron, the variance is set to `1.0`, which is the maximum value
    allowed(in general, all methods keep the initial values for the biases equal to
    0.0 because it''s not necessary to initialize them with a random value).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other variations have been proposed, even if they all share the same basic
    ideas. **LeCun** proposed initializing the weights as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7564827-d12d-451a-9d80-a0de48e25214.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another method called **Xavier initialization** (presented in *Understanding
    the difficulty of training deep feedforward neural networks, Glorot X., Bengio
    Y., Proceedings of the 13th International Conference on Artificial Intelligence
    and Statistics*), is similar to **LeCun initialization**, but it''s based on the
    average between the number of units of two consecutive layers (to mark the sequentiality,
    we have substituted the terms Fan-In and Fan-Out with explicit indices):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a4ca55c-0f06-4619-8a46-940e4083553c.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a more robust variant, as it considers both the incoming connections
    and also the outgoing ones (which are in turn incoming connections). The goal
    (widely discussed by the authors in the aforementioned papers) is trying to meet
    the two previously presented requirements. The first one is to avoid oscillations
    in the variance of the activations of each layer (ideally, this condition can
    avoid saturation). The second one is strictly related to the back-propagation
    algorithm, and it's based on the observation that, when employing a variance scaling
    (or an equivalent uniform distribution), the variance of a weight matrix is proportional
    to the reciprocal of *3n[k]*.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the averages of Fan-In and Fan-Out are multiplied by three, trying
    to avoid large variations in the weights after the updates. Xavier initialization
    has been proven to be very effective in many deep architectures, and it's often
    the default choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other methods are based on a different way to measure the variance during both
    the feed-forward and back-propagation phases and trying to correct the values
    to minimize residual oscillations in specific contexts. For example, He, Zhang,
    Ren, and Sun (in *Delving Deep into Rectifiers: Surpassing Human-Level Performance
    on ImageNet Classification, He K., Zhang X., Ren S., Sun J., arXiv:1502.01852
    [cs.CV]*) analyzed the initialization problem in the context of convolutional
    networks (we are going to discuss them in the next chapter) based on ReLU or variable
    Leaky-ReLU activations (also known as PReLU, parametric ReLU), deriving an optimal
    criterion (often called the **He initializer**), which is slightly different from
    the Xavier initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1562931a-8ac3-493b-b49b-a82aa418792e.png)'
  prefs: []
  type: TYPE_IMG
- en: All these methods share some common principles and, in many cases, they are
    interchangeable. As already mentioned, Xavier is one of the most robust and, in
    the majority of real-life problems, there's no need to look for other methods;
    however, the reader should be always aware that the complexity of deep models
    must be often faced using empirical methods based on sometimes simplistic mathematical
    assumptions. Only the validation with real dataset can confirm if a hypothesis
    is correct or it's better to continue the investigation in another direction.
  prefs: []
  type: TYPE_NORMAL
- en: Example of MLP with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras ([https://keras.io](https://keras.io)) is a powerful Python toolkit that
    allows modeling and training complex deep learning architectures with minimum
    effort. It relies on low-level frameworks, such as Tensorflow, Theano, or CNTK,
    and provides high-level blocks to build the single layers of a model. In this
    book, we need to be very pragmatic because there's no room for a complete explanation;
    however, all the examples will be structured to allow the reader to try different
    configurations and options without a full knowledge (for further details, I suggest
    the book *Deep Learning with Keras, Gulli A, Pal S., Packt Publishing*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we want to build a small MLP with a single hidden layer to
    solve the XOR problem (the dataset is the same created in the previous example).
    The simplest and most common way is to instantiate the class `Sequential`, which
    defines an *empty container* for an indefinite model. In this initial part, the
    fundamental method is `add()`, which allows adding a layer to the model. For our
    example, we want to employ four hidden layers with hyperbolic tangent activation
    and two softmax output layers. The following snippet defines the MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `Dense` class defines a fully connected layer (a *classical* MLP layer),
    and the first parameter is used to declare the number of desired units. The first
    layer must declare the `input_shape` or `input_dim`, which specify the dimensions
    (or the shape) of a single sample (the batch size is omitted as it's dynamically
    set by the framework). All the subsequent layers compute the dimensions automatically.
    One of the strengths of Keras is the possibility to avoid setting many parameters
    (like weight initializers), as they will be automatically configured using the
    most appropriate default values (for example, the default weight initializer is
    Xavier). In the next examples, we are going to explicitly set some of them, but
    I suggest that the reader checks the official documentation to get acquainted
    with all the possibilities and features. The other layer involved in this experiment
    is `Activation`, which specifies the desired activation function (it's also possible
    to declare it using the parameter `activation` implemented by almost all layers,
    but I prefer to decouple the operations to emphasize the single roles, and also
    because some techniques—such as batch normalization—are normally applied to the
    linear output, before the activation).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we must ask Keras to compile the model (using the preferred
    backend):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameter `optimizer` defines the stochastic gradient descent algorithm
    that we want to employ. Using `optimizer=''sgd''`, it''s possible to implement
    a standard version (as described in the previous paragraph). In this case, we
    are employing Adam (with the default parameters), which is a much more performant
    variant that will be discussed in the next section. The parameter `loss` is used
    to define the cost function (in this case, cross-entropy) and `metrics` is a list
    of all the evaluation score we want to be computed (`''accuracy''` is enough for
    many classification tasks). Once the model is compiled, it''s possible to train
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The operations are quite simple. We have split the dataset into training and
    test/validation sets (in deep learning, cross-validation is seldom employed) and,
    then, we have trained the model setting `batch_size=32` and `epochs=100`. The
    dataset is automatically shuffled at the beginning of each epoch, unless setting
    `shuffle=False`. In order to convert the discrete labels into one-hot encoding,
    we have used the utility function `to_categorical`. In this case, the label 0
    becomes (1, 0) and the label 1 (0, 1). The model converges before reaching 100
    epochs; therefore, I invite the reader to optimize the parameters as an exercise.
    However, at the end of the process, the training accuracy is about 0.999 and the
    validation accuracy is 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final classification plot is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3be515d-06ce-43dc-ba43-ac43ff49332c.png)'
  prefs: []
  type: TYPE_IMG
- en: MLP classification of the XOR dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Only three points have been misclassified, but it''s clear that the MLP successfully
    separated the XOR dataset. To have a confirmation of the generalization ability,
    we''ve plotted the decision surfaces for a hyperbolic tangent hidden layer and
    ReLU one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8bf3ec2-26b9-470b-8f3e-bfa3bcbcce76.png)'
  prefs: []
  type: TYPE_IMG
- en: MLP decision surfaces with Tanh (left) and ReLU (right) hidden layer
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the MLPs delimited the areas in a reasonable way. However, while
    a `tanh` hidden layer seems to be overfitted (this is not true in our case, as
    the dataset represents exactly the data generating process), the ReLU layer generates
    less smooth boundaries with an apparent lower variance (in particular for considering
    the outliers of a class). We know that the final validation accuracies confirm
    an almost perfect fit, and the decision plots (which is easy to create with two
    dimensions) show in both cases acceptable boundaries, but this simple exercise
    is useful to understand the complexity and the sensitivity of a deep model. For
    this reason, it's absolutely necessary to select a valid training set (representing
    the ground-truth) and employ all possible techniques to avoid the overfitting
    (as we're going to discuss later). The easiest way to detect such a situation
    is checking the validation loss. A good model should reduce both training and
    validation loss after each epoch, reaching a plateau for the latter. If, after
    *n* epochs, the validation loss (and, consequently, the accuracy) begins to increase,
    while the training loss keeps decreasing, it means that the model is overfitting
    the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Another empirical indicator that the training process is evolving correctly
    is that, at least at the beginning, the validation accuracy should be higher than
    the training one. This can seem strange, but we need to consider that the validation
    set is slightly smaller and less complex than the training set; therefore, if
    the capacity of the model is not saturated with training samples, the probability
    of misclassification is higher for the training set than for the validation set.
    When this trend is inverted, the model is very likely to overfit after a few epochs.
    To verify these concepts, I invite the reader to repeat the exercise using a large
    number of hidden neurons (so as to increase dramatically the capacity), but they
    will be clearer when working with much more complex and unstructured datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras can be installed using the command `pip install -U keras`. The default
    framework is Theano with CPU support. In order to use other frameworks (such as
    Tensorflow GPU), I suggest reading the instructions reported on the home page
    [https://keras.io](https://keras.io). As also suggested by the author, the best
    backend is Tensorflow, which is available for Linux, Mac OSX, and Windows. To
    install it (together with all dependencies), please follow the instructions on
    the following page: [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When discussing the back-propagation algorithm, we have shown how the SGD strategy
    can be easily employed to train deep networks with large datasets. This method
    is quite robust and effective; however, the function to optimize is generally
    non-convex and the number of parameters is extremely large. These conditions increase
    dramatically the probability to find saddle points (instead of local minima) and
    can slow down the training process when the surface is almost flat.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common result of applying a *vanilla* SGD algorithm to these systems is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/762c2102-e0ea-48f5-8b10-13ea786aea2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of reaching the optimal configuration, *θ[opt]*, the algorithm reaches
    a sub-optimal parameter configuration, *θ*[*subopt*,] and loses the ability to
    perform further corrections. To mitigate all these problems and their consequences,
    many SGD optimization algorithms have been proposed, with the purpose of speeding
    up the convergence (also when the gradients become extremely small) and avoiding
    the instabilities of ill-conditioned systems.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient perturbation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common problem arises when the hypersurface is flat (plateaus) the gradients
    become close to zero. A very simple way to mitigate this problem is based on adding
    a small homoscedastic noise component to the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5322d2e3-a850-48af-a649-b73657a318e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The covariance matrix is normally diagonal with all elements set to *σ²(t)*,
    and this value is decayed during the training process to avoid perturbations when
    the corrections are very small. This method is conceptually reasonable, but its
    implicit randomness can yield undesired effects when the noise component is dominant.
    As it's very difficult to tune up the variances in deep models, other (more deterministic)
    strategies have been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Momentum and Nesterov momentum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A more robust way to improve the performance of SGD when plateaus are encountered
    is based on the idea of momentum (analogously to physical momentum). More formally,
    a momentum is obtained employing the weighted moving average of subsequent gradient
    estimations instead of the punctual value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa994b7f-3b44-4c19-9ad7-8a5e5086d2a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The new vector *v*^(*(t)*,) contains a component which is based on the past
    history (and weighted using the parameter *μ* which is a forgetting factor) and
    a term referred to the current gradient estimation (multiplied by the learning
    rate). With this approach, abrupt changes become more difficult, and when the
    exploration leaves a sloped region to enter a plateau, the momentum doesn''t become
    immediately null (but for a time proportional to *μ*) a portion of the previous
    gradients will be kept, making it possible to traverse flat regions. The value
    assigned to the hyperparameter μ is normally bounded between 0 and 1\. Intuitively,
    small values imply a short memory as the first term decays very quickly, while
    values close to 1.0 (for example, 0.9) allow a longer memory, less influenced
    by local oscillations. Like for many other hyperparameters, μ needs to be tuned
    according to the specific problem, considering that a high momentum is not always
    the best choice. High values could slow down the convergence when very small adjustments
    are needed, but, at the same time, values close to 0.0 are normally ineffective
    because the memory contribution decays too early. Using momentum, the update rule
    becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c0a39b1-8da8-4ade-9ec5-7906ffdcde6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A variant is provided by **Nesterov momentum**, which is based on the results
    obtained in the field of mathematical optimization by Nesterov that have been
    proven to speed up the convergence of many algorithms. The idea is to determine
    a temporary parameter update based on the current momentum and then apply the
    gradient to this vector to determine the next momentum (it can be interpreted
    as a *look-ahead* gradient evaluation aimed to mitigate the risk of a wrong correction
    considering the moving history of each parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a57de8e-bed1-45a2-8559-e80c36c48a2a.png)'
  prefs: []
  type: TYPE_IMG
- en: This algorithm showed a performance improvement in several deep models; however,
    its usage is still limited because the next algorithms very soon outperformed
    the standard SGD with momentum, and they became the first choice in almost any
    real-life task.
  prefs: []
  type: TYPE_NORMAL
- en: SGD with momentum in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using Keras, it''s possible to customize the SGD optimizer by directly
    instantiating the `SGD` class and using it while compiling the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The class SGD accepts the parameter `lr` (the learning rate *η* with a default
    set to `0.01`), `momentum` (the parameter *μ*), `nesterov` (a boolean indicating
    whether employing the Nesterov momentum), and an optional `decay` parameter to
    indicate whether the learning rate must be decayed over the updates with the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62f05832-fcb8-4175-b271-6729257cf8cd.png)'
  prefs: []
  type: TYPE_IMG
- en: RMSProp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**RMSProp** was proposed by Hinton as an adaptive algorithm, partially based
    on the concept of momentum. Instead of considering the whole gradient vector,
    it tries to optimize each parameter separately to increase the corrections of
    slowly changing weights (that probably need more drastic modifications) and decreasing
    the update magnitudes of quickly changing ones (which are normally the more unstable).
    The algorithm computes the exponentially weighted moving average of the *changing
    speed* of every parameter considering the square of the gradient (which is insensitive
    to the sign):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c15efc5f-3c65-472d-a5dd-a893063b7008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight update is then performed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfd09daa-c05f-4729-9c29-e9469ea11fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The parameter *δ* is a small constant (such as 10^(-6)) that is added to avoid
    numerical instabilities when the changing speed becomes null. The previous expression
    could be rewritten in a more compact way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e4d44d1-dec1-44c9-bafc-fd5f783de5e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this notation, it is clear that the role of RMSProp is adapting the learning
    rate for every parameter so it can increase it when necessary (almost *frozen*
    weights) and decrease it when the risk of oscillations is higher. In a practical
    implementation, the learning rate is always decayed over the epochs using an exponential
    or linear function.
  prefs: []
  type: TYPE_NORMAL
- en: RMSProp with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following snippet shows the usage of RMSProp with Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The learning rate and decay are the same as SGD. The parameter `rho` corresponds
    to the exponential moving average weight, μ, and `epsilon` is the constant added
    to the changing speed to improve the stability. As with any other algorithm, if
    the user wants to use the default values, it's possible to declare the optimizer
    without instantiating the class (for example, `optimizer='rmsprop'`).
  prefs: []
  type: TYPE_NORMAL
- en: Adam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Adam** (the contraction of Adaptive Moment Estimation) is an algorithm proposed
    by Kingma and Ba (in *Adam: A Method for Stochastic Optimization, Kingma D. P.,
    Ba J., arXiv:1412.6980 [cs.LG]**)* to further improve the performance of RMSProp.
    The algorithm determines an adaptive learning rate by computing the exponentially
    weighted averages of both the gradient and its square for every parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1e00807-fed7-4348-8cff-503e85644b7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the aforementioned paper, the authors suggest to unbias the two estimations
    (which concern the first and second moment) by dividing them by *1 - μ[i]*, so
    the new moving averages become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88d60b62-7a70-4dcb-b672-472d1614e2c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight update rule for Adam is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56b028cf-f88d-48ad-aadd-19d3cd2e7bd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing the previous expression, it is possible to understand why this algorithm
    is often called RMSProp with momentum. In fact, the term *g(•)* acts just like
    the standard momentum, computing the moving average of the gradient for each parameter
    (with all the advantages of this procedure), while the denominator acts as an
    adaptive term with the same exact semantics of RMSProp. For this reason, Adam
    is very often one of the most widely employed algorithms, even if, in many complex
    tasks, its performances are comparable to a standard RMSProp. The choice must
    be made considering the extra complexity due to the presence of two forgetting
    factors. In general, the default values (0.9) are acceptable, but sometimes it's
    better to perform an analysis of several scenarios before deciding on a specific
    configuration. Another important element to remember is that all momentum based
    methods can lead to instabilities (oscillations) when training some deep architectures.
    That's why RMSProp is very diffused in almost any research paper; however, don't
    consider this statement as a limitation, because Adam has shown outstanding performances
    in many tasks. It's helpful to remember that, whenever the training process seems
    unstable also with low learning rates, it's preferable to employ methods that
    are not based on momentum (the inertial term, in fact, can slow down the fast
    modifications necessary to avoid oscillations).
  prefs: []
  type: TYPE_NORMAL
- en: Adam with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following snippet shows the usage of Adam with Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The forgetting factors, *μ[1]* and *μ*[*2*,] are represented by the parameters
    `beta_1` and `beta_2`. All the other elements are the same as the other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This algorithm has been proposed by Duchi, Hazan, and Singer (in *Adaptive
    Subgradient Methods for Online Learning and Stochastic Optimizatioln, Duchi J.,
    Hazan E., Singer Y., Journal of Machine Learning Research 12/2011).* The idea
    is very similar to RMSProp, but, in this case, the whole history of the squared
    gradients is taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16165cf0-6ea2-4abc-9eee-0767a05d6e2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weights are updated exactly like in RMSProp:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be1c1511-841d-4fa0-8be7-7f3ede17e84f.png)'
  prefs: []
  type: TYPE_IMG
- en: However, as the squared gradients are non-negative, the implicit sum *v^((t))(•)
    → ∞* when *t → ∞*. As the growth continues until the gradients are non-null, there's
    no way to keep the contribution stable while the training process proceeds. The
    effect is normally quite strong at the beginning, but vanishes after a limited
    number of epochs, yielding a null learning rate. **AdaGrad** keeps on being a
    powerful algorithm when the number of epochs is very limited, but it cannot be
    a first-choice solution for the majority of deep models (the next algorithm has
    been proposed to solve this problem).
  prefs: []
  type: TYPE_NORMAL
- en: AdaGrad with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following snippet shows the use of AdaGrad with Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The AdaGrad implementation has no other parameters but the common ones.
  prefs: []
  type: TYPE_NORMAL
- en: AdaDelta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AdaDelta** is an algorithm (proposed in *ADADELTA: An Adaptive Learning Rate
    Method, Zeiler M. D., arXiv:1212.5701 [cs.LG]*) in order to address the main issue
    of AdaGrad, which arises to managing the whole squared gradient history. First
    of all, instead of the accumulator, AdaDelta employs an exponentially weighted
    moving average, like RMSProp:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33648fdf-1fcd-4796-b41e-6f551e2e8172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the main difference with RMSProp is based on the analysis of the update
    rule. When we consider the operation *x + Δx*, we assume that both terms have
    the same unit; however, the author noticed that the adaptive learning rate *η(θ[i])*
    obtained with RMSProp (as well as AdaGrad) is unitless (instead of having the
    unit of *θ[i]*). In fact, as the gradient is split into partial derivatives that
    can be approximated as *ΔL/Δθ[i]* and the cost function *L* is assumed to be unitless,
    we obtain the following relations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7346ef49-3082-45b3-bc08-aaee221b0bf7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, Zeiler proposed to apply a correction term proportional to the unit
    of each weight *θ[i]*. This factor is obtained by considering the exponentially
    weighted moving average of every squared difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/042bebb2-417b-46a5-9539-5353244a5832.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The resulting updated rule hence becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e911cca3-2e71-43ef-b075-221043212d02.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach is indeed more similar to RMSProp than AdaGrad, but the boundaries
    between the two algorithms are very thin, in particular when the history is limited
    to a finite sliding window. AdaDelta is a powerful algorithm, but it can outperform
    Adam or RMSProp only in very particular tasks. My suggestion is to employ a method
    and, before moving to another one, try to optimize the hyperparameters until the
    accuracy reaches its maximum. If the performances keep on being bad and the model
    cannot be improved in any other way, it's a good idea to test other optimization
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: AdaDelta with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following snippet shows the usage of AdaDelta with Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The forgetting factor, *μ*, is represented by the parameter `rho`.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization and dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overfitting is a common issue in deep models. Their extremely high capacity
    can often become problematic even with very large datasets because the ability
    to learn the structure of the training set is not always related to the ability
    to generalize. A deep neural network can easily become an associative memory,
    but the final internal configuration couldn't be the most suitable to manage samples
    belonging to the same distribution but was never presented during the training
    process. It goes without saying that this behavior is proportional to the complexity
    of the separation hypersurface. A linear classifier has a minimum chance to overfit,
    and a polynomial classifier is incredibly more prone to do it. A combination of
    hundreds, thousands, or more non-linear functions yields a separation hypersurface,
    which is beyond any possible analysis. In 1991, Hornik (in *Approximation Capabilities
    of Multilayer Feedforward Networks,Hornik K., Neural Networks, 4/2*) generalized
    a very important result obtained two years before by the mathematician Cybenko
    (and published in *Approximations by Superpositions of Sigmoidal Functions, Cybenko
    G., Mathematics of Control, Signals, and Systems, 2 /4*). Without any mathematical
    detail (which is, however, not very complex), the theorem states that an MLP (not
    the most complex architecture!) can approximate any function that is continuous
    in a compact subset of *ℜ^n*. It's clear that such a result formalized what almost
    any researcher already intuitively knew, but its *power* goes beyond the first
    impact, because the MLP is a finite system (not a mathematical series) and the
    theorem assumes a finite number of layers and neurons. Obviously, the precision
    is proportional to the complexity; however, there are no unacceptable limitations
    for almost any problem. However, our goal is not learning an existing continuous
    function, but managing samples drawn from an unknown data generating process with
    the purpose to maximize the accuracy when a new sample is presented. There are
    no guarantees that the function is continuous or that the domain is a compact
    subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 1](acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml), *Machine Learning
    Models Fundamentals*, we have presented the main regularization techniques based
    on a slightly modified cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2be5c48-adfd-4fd3-87c8-d3f7a8a19ce1.png)'
  prefs: []
  type: TYPE_IMG
- en: The additional term *g(θ)* is a non-negative function of the weights (such as
    L2 norm) that forces the optimization process to keep the parameters as small
    as possible. When working with saturating functions (such as `tanh`), regularization
    methods based on the L2 norm try to limit the operating range of the function
    to the linear part, reducing *de facto* its capacity. Of course, the final configuration
    won't be the optimal one (that could be the result of an overfitted model) but
    the suboptimal trade-off between training and validation accuracy (alternatively,
    we can say between bias and variance). A system with a bias close to 0 (and a
    training accuracy close to 1.0) could be extremely rigid in the classification,
    succeeding only when the samples are very similar to ones evaluated during the
    training process. That's why this *price* is often paid considering the advantages
    obtained when working with new samples. L2 regularization can be employed with
    any kind of activation function, but the effect could be different. For example,
    ReLU units have an increased probability to become linear (or constantly null)
    when the weights are very large. Trying to keep them close to 0.0 means forcing
    the function to exploit its non-linearity without the risk of extremely large
    outputs (that can negatively affect very deep architectures). This result can
    sometimes be more useful, because it allows training bigger models in a smoother
    way, obtaining better final performances. In general, it's almost impossible to
    decide whether a regularization can improve the result without several tests,
    but there are some scenarios where it's very common to introduce a dropout (we
    discuss this approach in the next paragraph) and tune up its hyperparameter. This
    is more an empirical choice than a precise architectural decision because many
    real-life examples (including state-of-the-art models) obtained outstanding results
    employing this regularization technique. I suggest the reader prefer a rational
    skepticism to blind trust and double-checking its models before picking a specific
    solution. Sometimes, an extremely high-performing network turns to being ineffective
    when a different (but analogous) dataset is chosen. That's why testing different
    alternatives can provide the best experience in order to solve specific problem
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, I want to show how it''s possible to implement an L1 (useful
    to enforce sparsity), L2, or ElasticNet (the combination of L1 and L2) regularization
    using Keras. The framework provides a fine-grained approach that allows imposing
    a different constraint to each layer. For example, the following snippet shows
    how to add a `l2` constraint with the strength parameter set to `0.05` to a generic
    fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `keras.regularizers` package contains the functions `l1()`, `l2()`, and
    `l1_l2()`, which can be applied to `Dense` and convolutional layers (we're going
    to discuss them in the next chapter). These layers allow us to impose a regularization
    on the weights (`kernel_regularizer`), on the bias (`bias_regularizer`), and on
    the activation output (`activation_regularizer`), even if the first one is normally
    the most widely employed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, it''s possible to impose specific constraints on the weights
    and biases that in a more selective way. The following snippet shows how to set
    a maximum norm (equal to `1.5`) on the weights of a layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Keras, in the `keras.constraints` package, provides some functions that can
    be used to impose a maximum norm on the weights or biases `maxnorm()`, a unit
    norm along an axis `unit_norm()`, non-negativity `non_neg()`, and upper and lower
    bounds for the norm `min_max_norm()`. The difference between this approach and
    regularization is that it is applied only if necessary. Considering the previous
    example, imposing an L2 regularization always has an effect, while a constraint
    on the maximum norm is inactive until the value is lower than the predefined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method has been proposed by Hinton and co. (in *Improving neural networks
    by preventing co-adaptation of feature detectors, Hinton G. E., Srivastava N.,
    Krizhevsky A., Sutskever I., Salakhutdinov R. R., arXiv:1207.0580 [cs.NE]*) as
    an alternative to prevent overfitting and allow bigger networks to explore more
    regions of the sample space. The idea is rather simple—during every training step,
    given a predefined percentage *n[d]*, a **dropout** layer randomly selects *n[d]N*
    incoming units and sets them to `0.0` (the operation is only active during the
    training phase, while it's completely removed when the model is employed for new
    predictions).
  prefs: []
  type: TYPE_NORMAL
- en: This operation can be interpreted in many ways. When more dropout layers are
    employed, the result of their selection is a sub-network with a reduced capacity
    that can, with more difficultly, overfit the training set. The overlap of many
    trained sub-networks makes up an implicit ensemble whose prediction is an average
    over all models. If the dropout is applied on input layers, it works like a weak
    data augmentation, by adding a random noise to the samples (setting a few units
    to zero can lead to potential corrupted patterns). At the same time, employing
    several dropout layers allows exploring several potential configurations that
    are continuously combined and refined.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy is clearly probabilistic, and the result can be affected by many
    factors that are impossible to anticipate; however, several tests confirmed that
    the employment of a dropout is a good choice when the networks are very deep because
    the resulting sub-networks have a residual capacity that allows them to model
    a wide portion of the samples, without driving the whole network to *freeze* its
    configuration overfitting the training set. On the other hand, this method is
    not very effective when the networks are shallow or contain a small number of
    neurons (in these cases, L2 regularization is probably a better choice).
  prefs: []
  type: TYPE_NORMAL
- en: According to the authors, dropout layers should be used in conjunction with
    high learning rates and maximum norm constraints on the weights. In this way,
    in fact, the model can easily learn more potential configurations that would be
    avoided when the learning rate is kept very small. However, this is not an absolute
    rule because many state-of-the-art models use a dropout together with optimization
    algorithms, such as RMSProp or Adam, and not excessively high learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: The main drawback of a dropout is that it slows down the training process and
    can lead to an unacceptable sub-optimality. The latter problem can be mitigated
    by adjusting the percentages of dropped units, but, in general, it's very difficult
    to solve it completely. For this reason, some new image-recognition models (like
    residual networks) avoid the dropout and employ more sophisticated techniques
    to train very deep convolutional networks that overfit both training and validation
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: Example of dropout with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We cannot test the effectiveness of the dropout with a more challenging classification
    problem. The dataset is the *classical* MNIST handwritten digits, but Keras allows
    downloading and working with the original version that is made up of 70 thousand
    (60 thousand training and 10 thousand test) 28 × 28 grayscale images. Even if
    this is not the best strategy, because a convolutional network should be the first
    choice to manage images, we want to try to classify the digits considering them
    as flattened 784-dimensional arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is loading and normalizing the dataset so that each value becomes
    a float bounded between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can start testing a model without dropout. The structure,
    which is common to all experiments, is based on three fully connected ReLU layers
    (2048-1024-1024) followed by a softmax layer with 10 units. Considering the problem,
    we can try to train the model using an Adam optimizer with *η = 0.0001* and a
    decay set to *10^(-6)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is trained for `200` epochs with a batch size of `256` samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Even without a further analysis, we can immediately notice that the model is
    overfitted. After 200 epochs, the training accuracy is 1.0 with a loss close to
    0.0, while the validation accuracy is reasonably high, but with a validation loss
    slightly lower than the one obtained at the end of the second epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand what happened, it''s useful to plot both accuracy and
    loss during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61e814cc-f0c0-4603-b83b-5cbf451d941c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As it''s possible to see, the validation loss reached a minimum during the
    first 10 epochs and immediately restarted to grow (this is sometimes called a
    U-curve because of its shape). At the same moment, the training accuracy reached
    1.0\. From that epoch on, the model started overfitting, learning a perfect structure
    of the training set, but losing the generalization ability. In fact, even if the
    final validation accuracy is rather high, the loss function indicates a lack of
    robustness when new samples are presented. As the loss is a categorical cross-entropy,
    the result can be interpreted as saying that the model has learned a distribution
    that partially mismatches the validation set one. As our goal is to use the model
    to predict new samples, this configuration could not be acceptable. Therefore,
    we try again, using some dropout layers. As suggested by the authors, we also
    increment the learning rate to 0.1 (switching to a Momentum SGD optimizer in order
    to avoid *explosions* due to adaptivity of RMSProp or Adam), initialize the weight
    with a uniform distribution (*-0.05, 0.05*), and impose a maximum norm constraint
    set to 2.0\. This choice allows the exploration of more sub-configurations without
    the risk of excessively high weights. The dropout is applied to the 25% of input
    units and to all ReLU fully connected layers with a percentage set to 50%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The training process is performed with the same parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The final condition is dramatically changed. The model is no longer overfitted
    (even if it''s possible to improve it in order to increase the validation accuracy)
    and the validation loss is lower than the initial one. To have a confirmation,
    let''s analyze the accuracy/loss plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfd4857e-5576-4626-98a0-f628019f210a.png)'
  prefs: []
  type: TYPE_IMG
- en: The result shows some imperfections because the validation loss is almost flat
    for many epochs; however, the same model, with a higher learning rate and a weaker
    algorithm achieved a better final performance (0.988 validation accuracy) and
    a superior generalization ability. State-of-the-art models can also reach a validation
    accuracy equal to 0.995, but our goal was to show the effect of dropout layers
    in preventing the overfitting and, moreover, yielding a final configuration that
    is much more robust to new samples or noisy ones. I invite the reader to repeat
    the experiment with different parameters, bigger or smaller networks, and other
    optimization algorithms, trying to further reduce the final validation loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras also implements two additional dropout layers: `GaussianDropout`, which
    multiplies the input samples by a Gaussian noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7859906-f6f3-4ce4-b4ff-c748763d1f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: The value for the constant ρ can be set through the parameter `rate` (bounded
    between 0 and 1). When *ρ → 1*, *σ² → ∞*, while small values yield a null effect
    as *n ≈ 1*. This layer can be very useful as input one, in order to simulate a
    random data augmentation process. The other class is `AlphaDropout`, which works
    like the previous one, but renormalizing the output to keep the original mean
    and variance (this effect is very similar to the one obtained employing the technique
    described in the next paragraph together with noisy layers).
  prefs: []
  type: TYPE_NORMAL
- en: When working with probabilistic layers (such as dropout), I always suggest setting
    the random seed (`np.random.seed(...)` and `tf.set_random_seed(...)` when Tensorflow
    backend is used). In this way, it's possible to repeat the experiments comparing
    the results without any bias. If the random seed is not explicitly set, every
    new training process will be different and it's not easy to compare the performances,
    for example, after a fixed number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a mini-batch of *k* samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ffd163f-345d-4367-bb5c-8051d63d3911.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before traversing the network, we can measure a mean and a variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87aea2d8-f5ec-4982-84f2-032efed1a32b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the first layer (for simplicity, let''s suppose that the activation function,
    *f(•)*, is the always the same), the batch is transformed into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ebe382c-ae47-4a90-b01c-cccfd04593d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In general, there''s no guarantee that the new mean and variance are the same.
    On the contrary, it''s easy to observe a modification that increases throughout
    the network. This phenomenon is called **covariate shift**, and it''s responsible
    for a progressive training speed decay due to the different adaptations needed
    in each layer. Ioffe and Szegedy (in *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift, Ioffe S., Szegedy C., arXiv:1502.03167
    [cs.LG]*) proposed a method to mitigate this problem, which has been called **batch
    normalization** (**BN**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to renormalize the linear output of a layer (before or after applying
    the activation function), so that the batch has null mean and unit variance. Therefore,
    the first task of a BN layer is to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0de4a53-1db8-4ac3-a6be-c0728afdeb5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then each sample is transformed into a normalized version (the parameter *δ*
    is included to improve the numerical stability):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f936eb0-339a-41fd-9364-4dd0e875d1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, as the batch normalization has no computational purposes other than
    speeding up the training process, the transformation must always be an identity
    (in order to avoid to distort and bias the data); therefore, the actual output
    will be obtained by applying the linear operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/129065a7-5bb8-4131-b0e2-290e84efd55c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The two parameters *α^((j))* and *β^((j))* are variables optimized by the SGD
    algorithm; therefore, each transformation is guaranteed not to alter the scale
    and the position of data. These layers are active only during the training phase
    (like dropout), but, contrary to other algorithms, they cannot be simply discarded
    when the model is used to make predictions on new samples because the output would
    be constantly biased. To avoid this problem, the authors suggest approximating
    both mean and variance of *X* by averaging over the batches (assuming that there
    are *N[b]* batches with *k* samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4d8ac24-285f-4aeb-8fc3-6e0c6da16b89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using these values, the batch normalization layers can be transformed into
    the following linear operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b298eb1-53a3-440c-9bae-1cdf0d9622bb.png)'
  prefs: []
  type: TYPE_IMG
- en: It's not difficult to prove that this approximation becomes more and more accurate
    when the number of batches increases and that the error is normally negligible.
    However, when the batch size is very small, the statistics can be quite inaccurate;
    therefore, this method should be used considering the *representativeness* of
    a batch. If the data generating process is simple, even a small batch can be enough
    to describe the actual distribution. When, instead, *p[data]* is more complex,
    batch normalization requires larger batches to avoid wrong adjustments (a feasible
    strategy is to compare global mean and variance with the ones computed sampling
    some batches and trying to set the batch size that minimizes the discrepancy).
    However, this simple process can dramatically reduce the covariate shift and improve
    the convergence speed of very deep networks (including the famous residual networks).
    Moreover, it allows employing higher learning rates as the layers are implicitly
    *saturated* and can never *explode*. Additionally, it has been proven that batch
    normalization has also a secondary regularization effect even if it doesn't work
    on the weights. The reason is not very different from the one proposed for L2,
    but, in this case, there's a residual effect due to the transformation itself
    (partially caused by the variability of the parameters *α^((j))* and *β**^((j))*)
    that can encourage the exploration of different regions of the sample space. However,
    this is not the primary effect, and it's not a good practice employing this method
    as a regularizer.
  prefs: []
  type: TYPE_NORMAL
- en: Example of batch normalization with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to show the feature of this technique, let''s repeat the previous
    example using an MLP without dropout but applying a batch normalization after
    each fully connected layer before the ReLU activation. The example is very similar
    to the first one, but, in this case, we increase the Adam learning rate to 0.001
    keeping the same decay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now train using the same parameters again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is again overfitted, but now the final validation accuracy is only
    slightly higher than the one achieved using the dropout layers. Let''s plot accuracy
    and loss to better analyze the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be30b058-1319-423c-8c2e-9a3dde35d229.png)'
  prefs: []
  type: TYPE_IMG
- en: The effect of the batch normalization improved the performances and slowed down
    the overfitting. At the same time, the elimination of the covariate shift avoided
    the U-curve keeping a quite low validation loss. Moreover, the model reached a
    validation accuracy of about 0.99 during the epochs 135-140 with a residual positive
    trend. Analogously to the previous example, this solution is imperfect, but it's
    a good starting point for further optimization. It would be a good idea to continue
    the training process for a larger number of epochs, monitoring both the validation
    loss and accuracy. Moreover, it's possible to mix dropout and batch normalization
    or experiment with the Keras AlphaDropout layer. However, if, in the first example
    (without dropout), the climax of training accuracy was associated with a starting
    positive trend for the validation loss, in this case, the learned distribution
    doesn't seem to be very different from the validation set one. In other words,
    batch normalization is not preventing overfitting the training set, but it's avoiding
    a decay in the generalization ability (observed when there was no batch normalization).
    I suggest repeating the test with other hyperparameter and architectural configurations
    in order to decide whether this model can be used for prediction purposes or it's
    better to look for other solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started the exploration of the deep learning world by introducing
    the basic concepts that led the first researchers to improve the algorithms until
    they achieved the top results we have nowadays. The first part explained the structure
    of a basic artificial neuron, which combines a linear operation followed by an
    optional non-linear scalar function. A single layer of linear neurons was initially
    proposed as the first neural network, with the name of the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Even though it was quite powerful for many problems, this model soon showed
    its limitations when working with non-linear separable datasets. A perceptron
    is not very different from a logistic regression, and there's no concrete reason
    to employ it. Nevertheless, this model opened the doors to a family of extremely
    powerful models obtained combining multiple non-linear layers. The multilayer
    perceptron, which has been proven to be a universal approximator, is able to manage
    almost any kind of dataset, achieving high-level performances when other methods
    fail.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we analyzed the building bricks of an MLP. We started with
    the activation functions, describing their structure and features, and focusing
    on the reasons they lead the choice for specific problems. Then, we discussed
    the training process, considering the basic idea behind the back-propagation algorithm
    and how it can be implemented using the stochastic gradient descent method. Even
    if this approach is quite effective, it can be slow when the complexity of the
    network is very high. For this reason, many optimization algorithms were proposed.
    In this chapter, we analyzed the role of momentum and how it's possible to manage
    adaptive corrections using RMSProp. Then, we combined both, momentum and RMSProp
    to derive a very powerful algorithm called Adam. In order to provide a complete
    vision, we also presented two slightly different adaptive algorithms, called AdaGrad
    and AdaDelta.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we discussed the regularization methods and how they can
    be plugged into a Keras model. An important section was dedicated to a very diffused
    technique called dropout, which consists in setting to zero (dropping) a fixed
    percentage of samples through a random selection. This method, although very simple,
    prevents the overfitting of very deep networks and encourages the exploration
    of different regions of the sample space,  obtaining a result not very dissimilar
    to the ones analyzed in [Chapter 8](78baef9c-5391-4898-91bf-8df25330a163.xhtml),
    *Ensemble Learning*. The last topic was the batch normalization technique, which
    is a method to reduce the mean and variance shift (called covariate shift) caused
    by subsequent neural transformations. This phenomenon can slow down the training
    process as each layer requires different adaptations and it's more difficult to
    move all the weights in the best direction. Applying batch normalization means
    very deep networks can be trained in a shorter time, thanks also to the possibility
    of employing higher learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to continue this exploration, analyzing very
    important advanced layers like convolutions (that achieve extraordinary performances
    in image-oriented tasks) and recurrent units (for the processing of time series)
    and discussing some practical applications that can be experimented on and readapted
    using Keras and Tensorflow.
  prefs: []
  type: TYPE_NORMAL
