<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;6.&#xA0;Classification (II) &#x2013; Neural Network and SVM"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Classification (II) – Neural Network and SVM</h1></div></div></div><p class="calibre7">In this chapter, we will cover the following recipes:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Classifying data with a support vector machine </li><li class="listitem">Choosing the cost of a support vector machine</li><li class="listitem">Visualizing an SVM fit</li><li class="listitem">Predicting labels based on a model trained by a support vector machine</li><li class="listitem">Tuning a support vector machine</li><li class="listitem">Training a neural network with neuralnet</li><li class="listitem">Visualizing a neural network trained by neuralnet</li><li class="listitem">Predicting labels based on a model trained by neuralnet</li><li class="listitem">Training a neural network with nnet</li><li class="listitem">Predicting labels based on a model trained by nnet</li></ul></div></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Classification (II) &#x2013; Neural Network and SVM">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec66" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">Most research has shown that <span class="strong"><strong class="calibre2">support vector machines</strong></span> (<span class="strong"><strong class="calibre2">SVM</strong></span>) and <span class="strong"><strong class="calibre2">neural networks</strong></span> (<span class="strong"><strong class="calibre2">NN</strong></span>) are <a id="id521" class="calibre1"/>powerful classification tools, which can be applied to several different areas. Unlike tree-based or probabilistic-based methods that were mentioned in the previous chapter, the process of how<a id="id522" class="calibre1"/> support vector machines and neural networks transform from input to output is less clear and can be hard to interpret. As a result, both support vector machines and neural networks are referred to as black box methods.</p><p class="calibre7">The development of a neural network is inspired by human brain activities. As such, this type of network is a computational model that mimics the pattern of the human mind. In contrast to this, support vector machines first map input data into a high dimension feature space defined by the kernel function, and find the optimum hyperplane that separates the training data by the maximum margin. In short, we can think of support vector machines as a linear algorithm in a high dimensional space.</p><p class="calibre7">Both these <a id="id523" class="calibre1"/>methods have advantages and disadvantages in solving <a id="id524" class="calibre1"/>classification problems. For example, support vector machine solutions are the global optimum, while neural networks may suffer from multiple local optimums. Thus, choosing between either depends on the characteristics of the dataset source. In this chapter, we will illustrate the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">How to train a support vector machine</li><li class="listitem">Observing how the choice of cost can affect the SVM classifier</li><li class="listitem">Visualizing the SVM fit</li><li class="listitem">Predicting the labels of a testing dataset based on the model trained by SVM</li><li class="listitem">Tuning the SVM</li></ul></div><p class="calibre7">In the neural network section, we will cover:</p><div class="book"><ul class="itemizedlist"><li class="listitem">How to train a neural network</li><li class="listitem">How to visualize a neural network model</li><li class="listitem">Predicting the labels of a testing dataset based on a model trained by <code class="email">neuralnet</code></li><li class="listitem">Finally, we will show how to train a neural network with <code class="email">nnet</code>, and how to use it to predict the labels of a testing dataset</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with a support vector machine"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec67" class="calibre1"/>Classifying data with a support vector machine</h1></div></div></div><p class="calibre7">The two most <a id="id525" class="calibre1"/>well known and popular support vector machine tools<a id="id526" class="calibre1"/> are <code class="email">libsvm</code> and <code class="email">SVMLite</code>. For R users, you can find the implementation <a id="id527" class="calibre1"/>of <code class="email">libsvm</code> in the <code class="email">e1071</code> package and <code class="email">SVMLite</code> in the <code class="email">klaR</code> package. Therefore, you can use the implemented function of these two packages to train support vector machines. In this recipe, we will focus on using the <code class="email">svm</code> function (the <code class="email">libsvm</code> implemented version) from the <code class="email">e1071</code> package to train a support vector machine based on the telecom customer churn data training dataset.</p></div>

<div class="book" title="Classifying data with a support vector machine">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec221" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom churn dataset as the input data source to train the support vector machine. For those who have not prepared the dataset, please refer to <a class="calibre1" title="Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic" href="part0060_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre8">Classification (I) – Tree, Lazy, and Probabilistic</em></span>, for details.</p></div></div>

<div class="book" title="Classifying data with a support vector machine">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec222" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to train the SVM:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Load the <code class="email">e1071</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(e1071)</strong></span>
</pre></div></li><li class="listitem" value="2">Train the <a id="id528" class="calibre1"/>support vector machine using the <code class="email">svm</code> function with <code class="email">trainset</code> as the input dataset, and use <code class="email">churn</code> as the classification category:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; model  = svm(churn~., data = trainset, kernel="radial", cost=1, gamma = 1/ncol(trainset))</strong></span>
</pre></div></li><li class="listitem" value="3">Finally, you can obtain overall information about the built model with <code class="email">summary</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(model)</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">svm(formula = churn ~ ., data = trainset, kernel = "radial", cost = 1, gamma = 1/ncol(trainset))</strong></span>


<span class="strong"><strong class="calibre2">Parameters:</strong></span>
<span class="strong"><strong class="calibre2">   SVM-Type:  C-classification </strong></span>
<span class="strong"><strong class="calibre2"> SVM-Kernel:  radial </strong></span>
<span class="strong"><strong class="calibre2">       cost:  1 </strong></span>
<span class="strong"><strong class="calibre2">      gamma:  0.05882353 </strong></span>

<span class="strong"><strong class="calibre2">Number of Support Vectors:  691</strong></span>

<span class="strong"><strong class="calibre2"> ( 394 297 )</strong></span>


<span class="strong"><strong class="calibre2">Number of Classes:  2 </strong></span>

<span class="strong"><strong class="calibre2">Levels: </strong></span>
<span class="strong"><strong class="calibre2"> yes no</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with a support vector machine">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec223" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The support vector machine constructs a hyperplane (or set of hyperplanes) that maximize the margin width between two classes in a high dimensional space. In these, the cases that <a id="id529" class="calibre1"/>define the hyperplane are support vectors, as shown in the following figure:</p><div class="mediaobject"><img src="../images/00112.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 1: Support Vector Machine</p></div></div><p class="calibre10"> </p><p class="calibre7">Support vector machine starts from constructing a hyperplane that maximizes the margin width. Then, it extends the definition to a nonlinear separable problem. Lastly, it maps the data to a high dimensional space where the data can be more easily separated with a linear boundary.</p><p class="calibre7">The advantage<a id="id530" class="calibre1"/> of using SVM is that it builds a highly accurate model through an engineering problem-oriented kernel. Also, it makes use of the regularization term to avoid over-fitting. It also does not suffer from local optimal and multicollinearity. The main limitation of SVM is its speed and size in the training and testing time. Therefore, it is not suitable or efficient enough to construct classification models for data that is large in size. Also, since it is hard to interpret SVM, how does the determination of the kernel take place? Regularization is another problem that we need tackle. </p><p class="calibre7">In this recipe, we continue to use the telecom <code class="email">churn</code> dataset as our example data source. We begin training a support vector machine using <code class="email">libsvm</code> provided in the <code class="email">e1071</code> package. Within the training function, <code class="email">svm</code>, one can specify the <code class="email">kernel</code> function, cost, and the <code class="email">gamma</code> function. For the <code class="email">kernel</code> argument, the default value is radial, and one can specify the kernel to a linear, polynomial, radial basis, and sigmoid. As for the <code class="email">gamma</code> argument, the default value is equal to (1/data dimension), and it controls the shape of the separating hyperplane. Increasing the <code class="email">gamma</code> argument usually increases the number of support vectors. </p><p class="calibre7">As for the cost, the default value is set to 1, which indicates that the regularization term is constant, and the larger the value, the smaller the margin is. We will<a id="id531" class="calibre1"/> discuss more on how the cost can affect the SVM classifier in the next recipe. Once the support vector machine is built, the <code class="email">summary</code> function can be used to obtain information, such as calls, parameters, number of classes, and the types of label.</p></div></div>

<div class="book" title="Classifying data with a support vector machine">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec224" class="calibre1"/>See also</h2></div></div></div><p class="calibre7">Another popular <a id="id532" class="calibre1"/>support vector machine tool is <code class="email">SVMLight</code>. Unlike the <code class="email">e1071</code> package, which provides the full implementation of <code class="email">libsvm</code>, the <code class="email">klaR</code> package simply provides an interface to <code class="email">SVMLight</code> only. To use <code class="email">SVMLight</code>, one can perform the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Install the <code class="email">klaR</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("klaR")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(klaR)</strong></span>
</pre></div></li><li class="listitem" value="2">Download <a id="id533" class="calibre1"/>the <code class="email">SVMLight</code> source code and binary for your platform from <a class="calibre1" href="http://svmlight.joachims.org/">http://svmlight.joachims.org/</a>. For example, if your guest OS is Windows 64-bit, you should download the file from <a class="calibre1" href="http://download.joachims.org/svm_light/current/svm_light_windows64.zip">http://download.joachims.org/svm_light/current/svm_light_windows64.zip</a>.</li><li class="listitem" value="3">Then, you should unzip the file and put the workable binary in the working directory; you may check your working directory by using the <code class="email">getwd</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; getwd()</strong></span>
</pre></div></li><li class="listitem" value="4">Train the support vector machine using the <code class="email">svmlight</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; model.light  = svmlight(churn~., data = trainset, kernel="radial", cost=1, gamma = 1/ncol(trainset))</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Choosing the cost of a support vector machine"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec68" class="calibre1"/>Choosing the cost of a support vector machine</h1></div></div></div><p class="calibre7">The support vector machines create an optimum hyperplane that separates the training data by the <a id="id534" class="calibre1"/>maximum margin. However, sometimes we would like to allow some misclassifications while separating categories. The SVM model has a cost function, which controls training errors and margins. For example, a small cost creates a large margin (a soft margin) and allows more misclassifications. On the other hand, a large cost creates a narrow margin (a hard margin) and permits fewer misclassifications. In this recipe, we will illustrate how the large and small cost will affect the SVM classifier.</p></div>

<div class="book" title="Choosing the cost of a support vector machine">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec225" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this<a id="id535" class="calibre1"/> recipe, we will use the <code class="email">iris</code> dataset as our example data source.</p></div></div>

<div class="book" title="Choosing the cost of a support vector machine">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec226" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to generate two different classification examples with different costs:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Subset the <code class="email">iris</code> dataset with columns named as <code class="email">Sepal.Length</code>, <code class="email">Sepal.Width</code>, <code class="email">Species</code>, with species in <code class="email">setosa</code> and <code class="email">virginica</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; iris.subset = subset(iris, select=c("Sepal.Length", "Sepal.Width", "Species"), Species %in% c("setosa","virginica"))</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you can generate a scatter plot with <code class="email">Sepal.Length</code> as the x-axis and the <code class="email">Sepal.Width</code> as the y-axis:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(x=iris.subset$Sepal.Length,y=iris.subset$Sepal.Width, col=iris.subset$Species, pch=19)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00113.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 2: Scatter plot of Sepal.Length and Sepal.Width with subset of iris dataset</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Next, you can train SVM based on <code class="email">iris.subset</code> with the cost equal to 1:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.model = svm(Species ~ ., data=iris.subset, kernel='linear', cost=1, scale=FALSE)</strong></span>
</pre></div></li><li class="listitem" value="4">Then, we <a id="id536" class="calibre1"/>can circle the support vector with blue circles:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; points(iris.subset[svm.model$index,c(1,2)],col="blue",cex=2)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00114.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 3: Circling support vectors with blue ring</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">Lastly, we can add a separation line on the plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; w = t(svm.model$coefs) %*% svm.model$SV</strong></span>
<span class="strong"><strong class="calibre2">&gt; b = -svm.model$rho</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="red", lty=5)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00115.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 4: Add separation line to scatter plot</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">In addition<a id="id537" class="calibre1"/> to this, we create another SVM classifier where <code class="email">cost = 10,000</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(x=iris.subset$Sepal.Length,y=iris.subset$Sepal.Width, col=iris.subset$Species, pch=19)</strong></span>
<span class="strong"><strong class="calibre2">&gt; svm.model = svm(Species ~ ., data=iris.subset, type='C-classification', kernel='linear', cost=10000, scale=FALSE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; points(iris.subset[svm.model$index,c(1,2)],col="blue",cex=2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; w = t(svm.model$coefs) %*% svm.model$SV</strong></span>
<span class="strong"><strong class="calibre2">&gt; b = -svm.model$rho</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="red", lty=5)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00116.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 5: A classification example with large cost</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Choosing the cost of a support vector machine">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec227" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id538" class="calibre1"/> recipe, we demonstrate how different costs can affect the SVM classifier. First, we create an iris subset with the columns, <code class="email">Sepal.Length</code>, <code class="email">Sepal.Width</code>, and <code class="email">Species</code> containing the species, <code class="email">setosa</code> and <code class="email">virginica</code>. Then, in order to create a soft margin and allow some misclassification, we use an SVM with small cost (where <code class="email">cost = 1</code>) to train the support of the vector machine. Next, we circle the support vectors with blue circles and add the separation line. As per <span class="strong"><em class="calibre8">Figure 5</em></span>, one of the green points (<code class="email">virginica</code>) is misclassified (it is classified to <code class="email">setosa</code>) to the other side of the separation line due to the choice of the small cost.</p><p class="calibre7">In addition to this, we would like to determine how a large cost can affect the SVM classifier. Therefore, we choose a large cost (where <code class="email">cost = 10,000</code>). From Figure 5, we can see that the margin created is narrow (a hard margin) and no misclassification cases are present. As a result, the two examples show that the choice of different costs may affect the margin created and also affect the possibilities of misclassification.</p></div></div>

<div class="book" title="Choosing the cost of a support vector machine">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec228" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The idea of soft margin, which allows misclassified examples, was suggested by Corinna Cortes and Vladimir N. Vapnik in 1995 in the following paper: Cortes, C., and Vapnik, V. (1995). <span class="strong"><em class="calibre8">Support-vector networks. Machine learning</em></span>, 20(3), 273-297.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Visualizing an SVM fit"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec69" class="calibre1"/>Visualizing an SVM fit</h1></div></div></div><p class="calibre7">To visualize the built model, one can first use the plot function to generate a scatter plot of data input and the SVM fit. In this plot, support vectors and classes are highlighted through the<a id="id539" class="calibre1"/> color symbol. In addition to this, one can draw a contour filled plot of the class regions to easily identify misclassified samples from the plot.</p></div>

<div class="book" title="Visualizing an SVM fit">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec229" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use two datasets: the <code class="email">iris</code> dataset and the telecom <code class="email">churn</code> dataset. For the telecom <code class="email">churn</code> dataset, one needs to have completed the previous recipe by training a support vector machine with SVM, and to have saved the SVM fit model.</p></div></div>

<div class="book" title="Visualizing an SVM fit">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec230" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to visualize the SVM fit object:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Use SVM to train the support vector machine based on the iris dataset, and use the <code class="email">plot</code> function to visualize the fitted model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; data(iris)</strong></span>
<span class="strong"><strong class="calibre2">&gt; model.iris  = svm(Species~., iris)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(model.iris, iris, Petal.Width ~ Petal.Length, slice = list(Sepal.Width = 3, Sepal.Length = 4))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00117.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 6: The SVM classification plot of trained SVM fit based on iris dataset</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">Visualize the SVM fit object, <code class="email">model</code>, using the <code class="email">plot</code> function with the dimensions of <code class="email">total_day_minutes</code> and <code class="email">total_intl_charge</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(model, trainset, total_day_minutes ~ total_intl_charge)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00118.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 7: The SVM classification plot of trained SVM fit based on churn dataset</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Visualizing an SVM fit">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec231" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id540" class="calibre1"/> recipe, we demonstrate how to use the <code class="email">plot</code> function to visualize the SVM fit. In the first plot, we train a support vector machine using the <code class="email">iris</code> dataset. Then, we use the <code class="email">plot</code> function to visualize the fitted SVM.</p><p class="calibre7">In the argument list, we specify the fitted model in the first argument and the dataset (this should be the same data used to build the model) as the second parameter. The third parameter indicates the dimension used to generate the classification plot. By default, the <code class="email">plot</code> function can only generate a scatter plot based on two dimensions (for the x-axis and y-axis). Therefore, we select the variables, <code class="email">Petal.Length</code> and <code class="email">Petal.Width</code> as the two dimensions to generate the scatter plot.</p><p class="calibre7">From <span class="strong"><em class="calibre8">Figure 6</em></span>, we find <code class="email">Petal.Length</code> assigned to the x-axis, <code class="email">Petal.Width</code> assigned to the y-axis, and data points with <code class="email">X</code> and <code class="email">O</code> symbols scattered on the plot. Within the scatter plot, the <code class="email">X</code> symbol shows the support vector and the <code class="email">O</code> symbol represents the data points. These two symbols can be altered through the configuration of the <code class="email">svSymbol</code> and <code class="email">dataSymbol</code> options. Both the support vectors and true classes are highlighted and colored depending on their label (green refers to viginica, red refers to versicolor, and black <a id="id541" class="calibre1"/>refers to setosa). The last argument, <code class="email">slice</code>, is set when there are more than two variables. Therefore, in this example, we use the additional variables, <code class="email">Sepal.width</code> and <code class="email">Sepal.length</code>, by assigning a constant of <code class="email">3</code> and <code class="email">4</code>.</p><p class="calibre7">Next, we take the same approach to draw the SVM fit based on customer churn data. In this example, we use <code class="email">total_day_minutes</code> and <code class="email">total_intl_charge</code> as the two dimensions used to plot the scatterplot. As per <span class="strong"><em class="calibre8">Figure 7</em></span>, the support vectors and data points in red and black are scattered closely together in the central region of the plot, and there is no simple way to separate them.</p></div></div>

<div class="book" title="Visualizing an SVM fit">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec232" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">There are other parameters, such as <code class="email">fill</code>, <code class="email">grid</code>, <code class="email">symbolPalette</code>, and so on, that can be configured to change the layout of the plot. You can use the <code class="email">help</code> function to view the following document for further information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?svm.plot</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting labels based on a model trained by a support vector machine"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec70" class="calibre1"/>Predicting labels based on a model trained by a support vector machine</h1></div></div></div><p class="calibre7">In the previous recipe, we trained an SVM based on the training dataset. The training process finds<a id="id542" class="calibre1"/> the optimum hyperplane that <a id="id543" class="calibre1"/>separates the training data by the maximum margin. We can then utilize the SVM fit to predict the label (category) of new observations. In this recipe, we will demonstrate how to use the <code class="email">predict</code> function to predict values based on a model trained by SVM.</p></div>

<div class="book" title="Predicting labels based on a model trained by a support vector machine">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec233" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by generating a fitted SVM, and save the fitted model in model.</p></div></div>

<div class="book" title="Predicting labels based on a model trained by a support vector machine">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec234" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to predict the labels of the testing dataset:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Predict the label of the testing dataset based on the fitted SVM and attributes of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.pred = predict(model, testset[, !names(testset) %in% c("churn")])</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you can use the <code class="email">table</code> function to generate a classification table with the prediction result and labels of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.table=table(svm.pred, testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">&gt; svm.table</strong></span>
<span class="strong"><strong class="calibre2">        </strong></span>
<span class="strong"><strong class="calibre2">svm.pred yes  no</strong></span>
<span class="strong"><strong class="calibre2">     yes  70  12</strong></span>
<span class="strong"><strong class="calibre2">     no   71 865</strong></span>
</pre></div></li><li class="listitem" value="3">Next, you <a id="id544" class="calibre1"/>can use <code class="email">classAgreement</code> to calculate coefficients compared to the classification agreement:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; classAgreement(svm.table)</strong></span>
<span class="strong"><strong class="calibre2">$diag</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.9184676</strong></span>

<span class="strong"><strong class="calibre2">$kappa</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.5855903</strong></span>

<span class="strong"><strong class="calibre2">$rand</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.850083</strong></span>

<span class="strong"><strong class="calibre2">$crand</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.5260472</strong></span>
</pre></div></li><li class="listitem" value="4">Now, you <a id="id545" class="calibre1"/>can use <code class="email">confusionMatrix</code> to measure the prediction performance based on the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(caret)</strong></span>
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(svm.table)</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">        </strong></span>
<span class="strong"><strong class="calibre2">svm.pred yes  no</strong></span>
<span class="strong"><strong class="calibre2">     yes  70  12</strong></span>
<span class="strong"><strong class="calibre2">     no   71 865</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9185          </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.8999, 0.9345)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : 1.251e-08       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.5856          </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 1.936e-10       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.49645         </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.98632         </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.85366         </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.92415         </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.13851         </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.06876         </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.08055         </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.74139         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes             </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Predicting labels based on a model trained by a support vector machine">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec235" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we first used the <code class="email">predict</code> function to obtain the predicted labels of the testing<a id="id546" class="calibre1"/> dataset. Next, we used the <code class="email">table</code> function to generate the classification table based on the predicted labels of the testing<a id="id547" class="calibre1"/> dataset. So far, the evaluation procedure is very similar to the evaluation process mentioned in the previous chapter.</p><p class="calibre7">We then introduced a new function, <code class="email">classAgreement</code>, which computes several coefficients of agreement between the columns and rows of a two-way contingency table. The coefficients include diag, kappa, rand, and crand. The <code class="email">diag</code> coefficient represents the percentage of data points in the main diagonal of the classification table, <code class="email">kappa</code> refers to <code class="email">diag</code>, which is corrected for an agreement by a change (the probability of random agreements), <code class="email">rand</code> represents the Rand index, which measures the similarity between two data clusters, and <code class="email">crand</code> indicates the Rand index, which is adjusted for the chance grouping of elements.</p><p class="calibre7">Finally, we used <code class="email">confusionMatrix</code> from the <code class="email">caret</code> package to measure the performance of the classification model. The accuracy of 0.9185 shows that the trained support vector machine can correctly classify most of the observations. However, accuracy alone is not a good measurement of a classification model. One should also reference sensitivity and specificity.</p></div></div>

<div class="book" title="Predicting labels based on a model trained by a support vector machine">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec236" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Besides <a id="id548" class="calibre1"/>using SVM to predict the category<a id="id549" class="calibre1"/> of new observations, you can use SVM to predict continuous values. In other words, one can use SVM to perform regression analysis.</p><p class="calibre7">In the following example, we will show how to perform a simple regression prediction based on a fitted SVM with the type specified as <code class="email">eps-regression</code>.</p><p class="calibre7">Perform the following steps to train a regression model with SVM:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Train a support vector machine based on a <code class="email">Quartet</code> dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(car)</strong></span>
<span class="strong"><strong class="calibre2">&gt; data(Quartet)</strong></span>
<span class="strong"><strong class="calibre2">&gt; model.regression = svm(Quartet$y1~Quartet$x,type="eps-regression")</strong></span>
</pre></div></li><li class="listitem" value="2">Use the <code class="email">predict</code> function to obtain prediction results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predict.y = predict(model.regression, Quartet$x) </strong></span>
<span class="strong"><strong class="calibre2">&gt; predict.y</strong></span>
<span class="strong"><strong class="calibre2">       1        2        3        4        5        6        7        8 </strong></span>
<span class="strong"><strong class="calibre2">8.196894 7.152946 8.807471 7.713099 8.533578 8.774046 6.186349 5.763689 </strong></span>
<span class="strong"><strong class="calibre2">       9       10       11 </strong></span>
<span class="strong"><strong class="calibre2">8.726925 6.621373 5.882946 </strong></span>
</pre></div></li><li class="listitem" value="3">Plot the predicted points as squares and the training data points as circles on the same plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(Quartet$x, Quartet$y1, pch=19)</strong></span>
<span class="strong"><strong class="calibre2">&gt; points(Quartet$x, predict.y, pch=15, col="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00119.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 8: The scatter plot contains predicted data points and training data points</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Tuning a support vector machine"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec71" class="calibre1"/>Tuning a support vector machine</h1></div></div></div><p class="calibre7">Besides using different feature sets and the <code class="email">kernel</code> function in support vector machines, one trick that you can use to tune its performance is to adjust the gamma and cost configured<a id="id550" class="calibre1"/> in the argument. One possible approach to test the performance of different gamma and cost combination values is to write a <code class="email">for</code> loop to generate all the combinations of gamma and cost as inputs to train different support vector machines. Fortunately, SVM provides a tuning function, <code class="email">tune.svm</code>, which makes the tuning much easier. In this recipe, we will demonstrate how to tune a support vector machine through the use of <code class="email">tune.svm</code>.</p></div>

<div class="book" title="Tuning a support vector machine">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec237" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by preparing a training dataset, <code class="email">trainset</code>.</p></div></div>

<div class="book" title="Tuning a support vector machine">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec238" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to tune the support vector machine:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, tune the support vector machine using <code class="email">tune.svm</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; tuned = tune.svm(churn~., data = trainset, gamma = 10^(-6:-1), cost = 10^(1:2))</strong></span>
</pre></div></li><li class="listitem" value="2">Next, you can use the <code class="email">summary</code> function to obtain the tuning result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(tuned)</strong></span>

<span class="strong"><strong class="calibre2">Parameter tuning of 'svm':</strong></span>

<span class="strong"><strong class="calibre2">- sampling method: 10-fold cross validation </strong></span>

<span class="strong"><strong class="calibre2">- best parameters:</strong></span>
<span class="strong"><strong class="calibre2"> gamma cost</strong></span>
<span class="strong"><strong class="calibre2">  0.01  100</strong></span>

<span class="strong"><strong class="calibre2">- best performance: 0.08077885 </strong></span>

<span class="strong"><strong class="calibre2">- Detailed performance results:</strong></span>
<span class="strong"><strong class="calibre2">   gamma cost      error dispersion</strong></span>
<span class="strong"><strong class="calibre2">1  1e-06   10 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">2  1e-05   10 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">3  1e-04   10 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">4  1e-03   10 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">5  1e-02   10 0.09245223 0.02046032</strong></span>
<span class="strong"><strong class="calibre2">6  1e-01   10 0.09202306 0.01938475</strong></span>
<span class="strong"><strong class="calibre2">7  1e-06  100 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">8  1e-05  100 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">9  1e-04  100 0.14774780 0.02399512</strong></span>
<span class="strong"><strong class="calibre2">10 1e-03  100 0.11794484 0.02368343</strong></span>
<span class="strong"><strong class="calibre2">11 1e-02  100 0.08077885 0.01858195</strong></span>
<span class="strong"><strong class="calibre2">12 1e-01  100 0.12356135 0.01661508</strong></span>
</pre></div></li><li class="listitem" value="3">After<a id="id551" class="calibre1"/> retrieving the best performance parameter from tuning the result, you can retrain the support vector machine with the best performance parameter:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; model.tuned = svm(churn~., data = trainset, gamma = tuned$best.parameters$gamma, cost = tuned$best.parameters$cost)</strong></span>
<span class="strong"><strong class="calibre2">&gt; summary(model.tuned)</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">svm(formula = churn ~ ., data = trainset, gamma = 10^-2, cost = 100)</strong></span>


<span class="strong"><strong class="calibre2">Parameters:</strong></span>
<span class="strong"><strong class="calibre2">   SVM-Type:  C-classification </strong></span>
<span class="strong"><strong class="calibre2"> SVM-Kernel:  radial </strong></span>
<span class="strong"><strong class="calibre2">       cost:  100 </strong></span>
<span class="strong"><strong class="calibre2">      gamma:  0.01 </strong></span>

<span class="strong"><strong class="calibre2">Number of Support Vectors:  547</strong></span>

<span class="strong"><strong class="calibre2"> ( 304 243 )</strong></span>


<span class="strong"><strong class="calibre2">Number of Classes:  2 </strong></span>

<span class="strong"><strong class="calibre2">Levels: </strong></span>
<span class="strong"><strong class="calibre2"> yes no</strong></span>
</pre></div></li><li class="listitem" value="4">Then, you <a id="id552" class="calibre1"/>can use the <code class="email">predict</code> function to predict labels based on the fitted SVM:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.tuned.pred = predict(model.tuned, testset[, !names(testset) %in% c("churn")])</strong></span>
</pre></div></li><li class="listitem" value="5">Next, generate a classification table based on the predicted and original labels of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.tuned.table=table(svm.tuned.pred, testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">&gt; svm.tuned.table</strong></span>
<span class="strong"><strong class="calibre2">              </strong></span>
<span class="strong"><strong class="calibre2">svm.tuned.pred yes  no</strong></span>
<span class="strong"><strong class="calibre2">           yes  95  24</strong></span>
<span class="strong"><strong class="calibre2">           no   46 853</strong></span>
</pre></div></li><li class="listitem" value="6">Also, generate a class agreement to measure the performance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; classAgreement(svm.tuned.table)</strong></span>
<span class="strong"><strong class="calibre2">$diag</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.9312377</strong></span>

<span class="strong"><strong class="calibre2">$kappa</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.691678</strong></span>

<span class="strong"><strong class="calibre2">$rand</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.871806</strong></span>

<span class="strong"><strong class="calibre2">$crand</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.6303615</strong></span>
</pre></div></li><li class="listitem" value="7">Finally, you <a id="id553" class="calibre1"/>can use a confusion matrix to measure the performance of the retrained model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(svm.tuned.table)</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">              </strong></span>
<span class="strong"><strong class="calibre2">svm.tuned.pred yes  no</strong></span>
<span class="strong"><strong class="calibre2">           yes  95  24</strong></span>
<span class="strong"><strong class="calibre2">           no   46 853</strong></span>
<span class="strong"><strong class="calibre2">                                         </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9312         </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.9139, 0.946)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615         </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : 1.56e-12       </strong></span>
<span class="strong"><strong class="calibre2">                                         </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.6917         </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 0.01207        </strong></span>
<span class="strong"><strong class="calibre2">                                         </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.67376        </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.97263        </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.79832        </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.94883        </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.13851        </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.09332        </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.11690        </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.82320        </strong></span>
<span class="strong"><strong class="calibre2">                                         </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes            </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Tuning a support vector machine">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec239" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">To tune the<a id="id554" class="calibre1"/> support vector machine, you can use a trial and error method to find the best gamma and cost parameters. In other words, one has to generate a variety of combinations of gamma and cost for the purpose of training different support vector machines.</p><p class="calibre7">In this example, we generate different gamma values from <span class="strong"><em class="calibre8">10^-6</em></span> to <span class="strong"><em class="calibre8">10^-1</em></span>, and cost with a value of either 10 or 100. Therefore, you can use the tuning function, <code class="email">svm.tune</code>, to generate 12 sets of parameters. The function then makes 10 cross-validations and outputs the error dispersion of each combination. As a result, the combination with the least error dispersion is regarded as the best parameter set. From the summary table, we found that gamma with a value of 0.01 and cost with a value of 100 are the best parameters for the SVM fit.</p><p class="calibre7">After obtaining the best parameters, we can then train a new support vector machine with gamma equal to 0.01 and cost equal to 100. Additionally, we can obtain a classification table based on the predicted labels and labels of the testing dataset. We can also obtain a confusion matrix from the classification table. From the output of the confusion matrix, you can determine the accuracy of the newly trained model in comparison to the original model.</p></div></div>

<div class="book" title="Tuning a support vector machine">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec240" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For more information about how to tune SVM with <code class="email">svm.tune</code>, you can use the <code class="email">help</code> function to access this document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?svm.tune</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Training a neural network with neuralnet"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec72" class="calibre1"/>Training a neural network with neuralnet</h1></div></div></div><p class="calibre7">The neural network is constructed with an interconnected group of nodes, which involves the input, connected weights, processing element, and output. Neural networks can be applied to<a id="id555" class="calibre1"/> many areas, such as classification, clustering, and prediction. To train a neural network in R, you can use neuralnet, which<a id="id556" class="calibre1"/> is built to train multilayer perceptron in the context of regression analysis, and contains many flexible functions to train forward neural networks. In this recipe, we will introduce how to use neuralnet to train a neural network.</p></div>

<div class="book" title="Training a neural network with neuralnet">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec241" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use an <code class="email">iris</code> dataset as our example dataset. We will first split the <code class="email">iris</code> dataset into a training and testing datasets, respectively.</p></div></div>

<div class="book" title="Training a neural network with neuralnet">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec242" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to train a neural network with neuralnet:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First<a id="id557" class="calibre1"/> load the <code class="email">iris</code> dataset <a id="id558" class="calibre1"/>and split the data into training and testing datasets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; data(iris)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ind = sample(2, nrow(iris), replace = TRUE, prob=c(0.7, 0.3))</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset = iris[ind == 1,]</strong></span>
<span class="strong"><strong class="calibre2">&gt; testset = iris[ind == 2,]</strong></span>
</pre></div></li><li class="listitem" value="2">Then, install and load the <code class="email">neuralnet</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("neuralnet")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(neuralnet)</strong></span>
</pre></div></li><li class="listitem" value="3">Add the columns versicolor, setosa, and virginica based on the name matched value in the <code class="email">Species</code> column:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; trainset$setosa = trainset$Species == "setosa"</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset$virginica = trainset$Species == "virginica"</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset$versicolor = trainset$Species == "versicolor"</strong></span>
</pre></div></li><li class="listitem" value="4">Next, train the neural network with the <code class="email">neuralnet</code> function with three hidden neurons in each layer. Notice that the results may vary with each training, so you might not get the same result. However, you can use set.seed at the beginning, so you can get the same result in every training process<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; network = neuralnet(versicolor + virginica + setosa~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, trainset, hidden=3)</strong></span>
<span class="strong"><strong class="calibre2">&gt; network</strong></span>
<span class="strong"><strong class="calibre2">Call: neuralnet(formula = versicolor + virginica + setosa ~ Sepal.Length +     Sepal.Width + Petal.Length + Petal.Width, data = trainset,     hidden = 3)</strong></span>

<span class="strong"><strong class="calibre2">1 repetition was calculated.</strong></span>

<span class="strong"><strong class="calibre2">         Error Reached Threshold Steps</strong></span>
<span class="strong"><strong class="calibre2">1 0.8156100175    0.009994274769 11063</strong></span>
</pre></div></li><li class="listitem" value="5">Now, you can view the <code class="email">summary</code> information by accessing the <code class="email">result.matrix</code> attribute of the built neural network model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; network$result.matrix</strong></span>
<span class="strong"><strong class="calibre2">                                          1</strong></span>
<span class="strong"><strong class="calibre2">error                        0.815610017474</strong></span>
<span class="strong"><strong class="calibre2">reached.threshold            0.009994274769</strong></span>
<span class="strong"><strong class="calibre2">steps                    11063.000000000000</strong></span>
<span class="strong"><strong class="calibre2">Intercept.to.1layhid1        1.686593311644</strong></span>
<span class="strong"><strong class="calibre2">Sepal.Length.to.1layhid1     0.947415215237</strong></span>
<span class="strong"><strong class="calibre2">Sepal.Width.to.1layhid1     -7.220058260187</strong></span>
<span class="strong"><strong class="calibre2">Petal.Length.to.1layhid1     1.790333443486</strong></span>
<span class="strong"><strong class="calibre2">Petal.Width.to.1layhid1      9.943109233330</strong></span>
<span class="strong"><strong class="calibre2">Intercept.to.1layhid2        1.411026063895</strong></span>
<span class="strong"><strong class="calibre2">Sepal.Length.to.1layhid2     0.240309549505</strong></span>
<span class="strong"><strong class="calibre2">Sepal.Width.to.1layhid2      0.480654059973</strong></span>
<span class="strong"><strong class="calibre2">Petal.Length.to.1layhid2     2.221435192437</strong></span>
<span class="strong"><strong class="calibre2">Petal.Width.to.1layhid2      0.154879347818</strong></span>
<span class="strong"><strong class="calibre2">Intercept.to.1layhid3       24.399329878242</strong></span>
<span class="strong"><strong class="calibre2">Sepal.Length.to.1layhid3     3.313958088512</strong></span>
<span class="strong"><strong class="calibre2">Sepal.Width.to.1layhid3      5.845670010464</strong></span>
<span class="strong"><strong class="calibre2">Petal.Length.to.1layhid3    -6.337082722485</strong></span>
<span class="strong"><strong class="calibre2">Petal.Width.to.1layhid3    -17.990352566695</strong></span>
<span class="strong"><strong class="calibre2">Intercept.to.versicolor     -1.959842102421</strong></span>
<span class="strong"><strong class="calibre2">1layhid.1.to.versicolor      1.010292389835</strong></span>
<span class="strong"><strong class="calibre2">1layhid.2.to.versicolor      0.936519720978</strong></span>
<span class="strong"><strong class="calibre2">1layhid.3.to.versicolor      1.023305801833</strong></span>
<span class="strong"><strong class="calibre2">Intercept.to.virginica      -0.908909982893</strong></span>
<span class="strong"><strong class="calibre2">1layhid.1.to.virginica      -0.009904635231</strong></span>
<span class="strong"><strong class="calibre2">1layhid.2.to.virginica       1.931747950462</strong></span>
<span class="strong"><strong class="calibre2">1layhid.3.to.virginica      -1.021438938226</strong></span>
<span class="strong"><strong class="calibre2">Intercept.to.setosa          1.500533827729</strong></span>
<span class="strong"><strong class="calibre2">1layhid.1.to.setosa         -1.001683936613</strong></span>
<span class="strong"><strong class="calibre2">1layhid.2.to.setosa         -0.498758815934</strong></span>
<span class="strong"><strong class="calibre2">1layhid.3.to.setosa         -0.001881935696</strong></span>
</pre></div></li><li class="listitem" value="6">Lastly, you<a id="id559" class="calibre1"/> can view the <a id="id560" class="calibre1"/>generalized weight by accessing it in the network:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; head(network$generalized.weights[[1]])</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Training a neural network with neuralnet">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec243" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The neural network is a network made up of artificial neurons (or nodes). There are three types of neurons within the network: input neurons, hidden neurons, and output neurons. In the <a id="id561" class="calibre1"/>network, neurons are connected; the connection strength between neurons is called weights. If the weight is greater<a id="id562" class="calibre1"/> than zero, it is in an excitation status. Otherwise, it is in an inhibition status. Input neurons receive the input information; the higher the input value, the greater the activation. Then, the activation value is passed through the network in regard to weights and transfer functions in the graph. The hidden neurons (or output neurons) then sum up the activation values and modify the summed values with the transfer function. The activation value then flows through hidden neurons and stops when it reaches the output nodes. As a result, one can use the output value from the output neurons to classify the data.</p><div class="mediaobject"><img src="../images/00120.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 9: Artificial Neural Network</p></div></div><p class="calibre10"> </p><p class="calibre7">The <a id="id563" class="calibre1"/>advantages of a neural network are: first, it can detect nonlinear relationships between the dependent and independent variable. Second, one can efficiently train large datasets using the parallel architecture. Third, it is a nonparametric model so that one can eliminate errors in the estimation of parameters. The main disadvantages of a neural network are that it often converges to the local minimum rather than the global minimum. Also, it might over-fit when the training process goes on for too long.</p><p class="calibre7">In this recipe, we demonstrate how to train a neural network. First, we split the <code class="email">iris</code> dataset into training and testing datasets, and then install the <code class="email">neuralnet</code> package and load the library into an R session. Next, we add the columns <code class="email">versicolor</code>, <code class="email">setosa</code>, and <code class="email">virginica</code> based on the name matched value in the <code class="email">Species</code> column, respectively. We then use the <code class="email">neuralnet</code> function to train the network model. Besides specifying the label (the column where the name equals to versicolor, virginica, and setosa) and training attributes in the function, we also configure the number of hidden neurons (vertices) as three in <a id="id564" class="calibre1"/>each layer.</p><p class="calibre7">Then, we<a id="id565" class="calibre1"/> examine the basic information about the training process and the trained network saved in the network. From the output message, it shows the training process needed 11,063 steps until all the absolute partial derivatives of the error function were lower than 0.01 (specified in the threshold). The <a id="id566" class="calibre1"/>error refers to the likelihood of calculating <span class="strong"><strong class="calibre2">Akaike Information Criterion</strong></span> (<span class="strong"><strong class="calibre2">AIC</strong></span>). To see detailed information on this, you can access the <code class="email">result.matrix</code> of the built neural network to see the estimated weight. The output reveals that the estimated weight ranges from -18 to 24.40; the intercepts of the first hidden layer are 1.69, 1.41 and 24.40, and the two weights leading to the first hidden neuron are estimated as 0.95 (<code class="email">Sepal.Length</code>), -7.22 (<code class="email">Sepal.Width</code>), 1.79 (<code class="email">Petal.Length</code>), and 9.94 (<code class="email">Petal.Width</code>). We can lastly determine that the trained neural network information includes generalized weights, which express the effect of each covariate. In this recipe, the model generates 12 generalized weights, which are the combination of four covariates (<code class="email">Sepal.Length</code>, <code class="email">Sepal.Width</code>, <code class="email">Petal.Length</code>, <code class="email">Petal.Width</code>) to three responses (<code class="email">setosa</code>, <code class="email">virginica</code>, <code class="email">versicolor</code>).</p></div></div>

<div class="book" title="Training a neural network with neuralnet">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec244" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For a more detailed introduction on neuralnet, one can refer to the following paper: Günther, F., and Fritsch, S. (2010). <span class="strong"><em class="calibre8">neuralnet: Training of neural networks</em></span>. <span class="strong"><em class="calibre8">The R journal</em></span>, 2(1), 30-38.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Visualizing a neural network trained by neuralnet"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec73" class="calibre1"/>Visualizing a neural network trained by neuralnet</h1></div></div></div><p class="calibre7">The <a id="id567" class="calibre1"/>package, <code class="email">neuralnet</code>, provides <a id="id568" class="calibre1"/>the <code class="email">plot</code> function to visualize a built neural network and the <code class="email">gwplot</code> function to visualize generalized weights. In following recipe, we will cover how to use these two functions.</p></div>

<div class="book" title="Visualizing a neural network trained by neuralnet">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec245" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by training a neural network and have all basic information saved in the network.</p></div></div>

<div class="book" title="Visualizing a neural network trained by neuralnet">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec246" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the <a id="id569" class="calibre1"/>following steps to visualize<a id="id570" class="calibre1"/> the neural network and the generalized weights:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can visualize the trained neural network with the <code class="email">plot</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(network)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00121.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 10: The plot of the trained neural network</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">Furthermore, you can use <code class="email">gwplot</code> to visualize the generalized weights:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; par(mfrow=c(2,2))</strong></span>
<span class="strong"><strong class="calibre2">&gt; gwplot(network,selected.covariate="Petal.Width")</strong></span>
<span class="strong"><strong class="calibre2">&gt; gwplot(network,selected.covariate="Sepal.Width")</strong></span>
<span class="strong"><strong class="calibre2">&gt; gwplot(network,selected.covariate="Petal.Length")</strong></span>
<span class="strong"><strong class="calibre2">&gt; gwplot(network,selected.covariate="Petal.Width")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00122.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 11: The plot of generalized weights</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Visualizing a neural network trained by neuralnet">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec247" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id571" class="calibre1"/> recipe, we demonstrate how to<a id="id572" class="calibre1"/> visualize the trained neural network and the generalized weights of each trained attribute. As per <span class="strong"><em class="calibre8">Figure 10</em></span>, the plot displays the network topology of the trained neural network. Also, the plot includes the estimated weight, intercepts and basic information about the training process. At the bottom of the figure, one can find the overall error and number of steps required to converge.</p><p class="calibre7">
<span class="strong"><em class="calibre8">Figure 11</em></span> presents the generalized weight plot in regard to <code class="email">network$generalized.weights</code>. The four plots in <span class="strong"><em class="calibre8">Figure 11</em></span> display the four covariates: <code class="email">Petal.Width</code>, <code class="email">Sepal.Width</code>, <code class="email">Petal.Length</code>, and <code class="email">Petal.Width</code>, in regard to the versicolor response. If all the generalized weights are close to zero on the plot, it means the covariate has little effect. However, if the overall variance is greater than one, it means the covariate has a nonlinear effect.</p></div></div>

<div class="book" title="Visualizing a neural network trained by neuralnet">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec248" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For more information about <code class="email">gwplot</code>, one can use the <code class="email">help</code> function to access the following document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?gwplot</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting labels based on a model trained by neuralnet"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec74" class="calibre1"/>Predicting labels based on a model trained by neuralnet</h1></div></div></div><p class="calibre7">Similar to <a id="id573" class="calibre1"/>other classification methods, we can predict the labels of new observations based on trained neural<a id="id574" class="calibre1"/> networks. Furthermore, we can validate the performance of these networks through the use of a confusion matrix. In the following recipe, we will introduce how to use the <code class="email">compute</code> function in a neural network to obtain a probability matrix of the testing dataset labels, and use a table and confusion matrix to measure the prediction performance.</p></div>

<div class="book" title="Predicting labels based on a model trained by neuralnet">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec249" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by generating the training dataset, <code class="email">trainset</code>, and the testing dataset, <code class="email">testset</code>. The trained neural network needs to be saved in the network.</p></div></div>

<div class="book" title="Predicting labels based on a model trained by neuralnet">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec250" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to measure the prediction performance of the trained neural network:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, generate a prediction probability matrix based on a trained neural network and the testing dataset, <code class="email">testset</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; net.predict = compute(network, testset[-5])$net.result</strong></span>
</pre></div></li><li class="listitem" value="2">Then, obtain other possible labels by finding the column with the greatest probability:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; net.prediction = c("versicolor", "virginica", "setosa")[apply(net.predict, 1, which.max)]</strong></span>
</pre></div></li><li class="listitem" value="3">Generate a classification table based on the predicted labels and the labels of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predict.table = table(testset$Species, net.prediction)</strong></span>
<span class="strong"><strong class="calibre2">&gt; predict.table</strong></span>
<span class="strong"><strong class="calibre2">            prediction</strong></span>
<span class="strong"><strong class="calibre2">             setosa versicolor virginica</strong></span>
<span class="strong"><strong class="calibre2">  setosa         20          0         0</strong></span>
<span class="strong"><strong class="calibre2">  versicolor      0         19         1</strong></span>
<span class="strong"><strong class="calibre2">  virginica       0          2        16</strong></span>
</pre></div></li><li class="listitem" value="4">Next, generate <code class="email">classAgreement</code> from the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; classAgreement(predict.table)</strong></span>
<span class="strong"><strong class="calibre2">$diag</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.9444444444</strong></span>

<span class="strong"><strong class="calibre2">$kappa</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.9154488518</strong></span>

<span class="strong"><strong class="calibre2">$rand</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.9224318658</strong></span>

<span class="strong"><strong class="calibre2">$crand</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.8248251737</strong></span>
</pre></div></li><li class="listitem" value="5">Finally, use<a id="id575" class="calibre1"/> <code class="email">confusionMatrix</code> to <a id="id576" class="calibre1"/>measure the prediction performance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(predict.table)</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">            prediction</strong></span>
<span class="strong"><strong class="calibre2">             setosa versicolor virginica</strong></span>
<span class="strong"><strong class="calibre2">  setosa         20          0         0</strong></span>
<span class="strong"><strong class="calibre2">  versicolor      0         19         1</strong></span>
<span class="strong"><strong class="calibre2">  virginica       0          2        16</strong></span>

<span class="strong"><strong class="calibre2">Overall Statistics</strong></span>
<span class="strong"><strong class="calibre2">                                                  </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9482759               </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.8561954, 0.9892035)  </strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.362069                </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022204</strong></span>
<span class="strong"><strong class="calibre2">                                                  </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.922252                </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : NA                      </strong></span>

<span class="strong"><strong class="calibre2">Statistics by Class:</strong></span>

<span class="strong"><strong class="calibre2">                     Class: setosa Class: versicolor Class: virginica</strong></span>
<span class="strong"><strong class="calibre2">Sensitivity              1.0000000         0.9047619        0.9411765</strong></span>
<span class="strong"><strong class="calibre2">Specificity              1.0000000         0.9729730        0.9512195</strong></span>
<span class="strong"><strong class="calibre2">Pos Pred Value           1.0000000         0.9500000        0.8888889</strong></span>
<span class="strong"><strong class="calibre2">Neg Pred Value           1.0000000         0.9473684        0.9750000</strong></span>
<span class="strong"><strong class="calibre2">Prevalence               0.3448276         0.3620690        0.2931034</strong></span>
<span class="strong"><strong class="calibre2">Detection Rate           0.3448276         0.3275862        0.2758621</strong></span>
<span class="strong"><strong class="calibre2">Detection Prevalence     0.3448276         0.3448276        0.3103448</strong></span>
<span class="strong"><strong class="calibre2">Balanced Accuracy        1.0000000         0.9388674        0.9461980</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Predicting labels based on a model trained by neuralnet">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec251" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id577" class="calibre1"/> recipe, we demonstrate<a id="id578" class="calibre1"/> how to predict labels based on a model trained by neuralnet. Initially, we use the <code class="email">compute</code> function to create an output probability matrix based on the trained neural network and the testing dataset. Then, to convert the probability matrix to class labels, we use the <code class="email">which.max</code> function to determine the class label by selecting the column with the maximum probability within the row. Next, we use a table to generate a classification matrix based on the labels of the testing dataset and the predicted labels. As we have created the classification table, we can employ a confusion matrix to measure the prediction performance of the built neural network.</p></div></div>

<div class="book" title="Predicting labels based on a model trained by neuralnet">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec252" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">In this recipe, we use the <code class="email">net.result</code> function, which is the overall result of the neural network, used to predict the labels of the testing dataset. Apart from examining the overall result by accessing <code class="email">net.result</code>, the <code class="email">compute</code> function also generates the output from neurons in each layer. You can examine the output of neurons to get a better understanding of how <code class="email">compute</code> works:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; compute(network, testset[-5])</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Training a neural network with nnet"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec75" class="calibre1"/>Training a neural network with nnet</h1></div></div></div><p class="calibre7">The <code class="email">nnet</code> package is another package that can deal with artificial neural networks. This package <a id="id579" class="calibre1"/>provides the functionality<a id="id580" class="calibre1"/> to train feed-forward <a id="id581" class="calibre1"/>neural networks with traditional back propagation. As you can find most of the neural network function implemented in the <code class="email">neuralnet</code> package, in this recipe we provide a short overview of how to train neural networks with <code class="email">nnet</code>.</p></div>

<div class="book" title="Training a neural network with nnet">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec253" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we do not use the <code class="email">trainset</code> and <code class="email">trainset</code> generated from the previous step; please reload the <code class="email">iris</code> dataset again.</p></div></div>

<div class="book" title="Training a neural network with nnet">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec254" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to train the neural network with <code class="email">nnet</code>:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">nnet</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("nnet")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(nnet)</strong></span>
</pre></div></li><li class="listitem" value="2">Next, split the dataset into training and testing datasets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; data(iris)</strong></span>
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ind = sample(2, nrow(iris), replace = TRUE, prob=c(0.7, 0.3))</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset = iris[ind == 1,]</strong></span>
<span class="strong"><strong class="calibre2">&gt; testset = iris[ind == 2,]</strong></span>
</pre></div></li><li class="listitem" value="3"> Then, train the neural network with <code class="email">nnet</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; iris.nn = nnet(Species ~ ., data = trainset, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)</strong></span>
<span class="strong"><strong class="calibre2"># weights:  19</strong></span>
<span class="strong"><strong class="calibre2">initial  value 165.086674 </strong></span>
<span class="strong"><strong class="calibre2">iter  10 value 70.447976</strong></span>
<span class="strong"><strong class="calibre2">iter  20 value 69.667465</strong></span>
<span class="strong"><strong class="calibre2">iter  30 value 69.505739</strong></span>
<span class="strong"><strong class="calibre2">iter  40 value 21.588943</strong></span>
<span class="strong"><strong class="calibre2">iter  50 value 8.691760</strong></span>
<span class="strong"><strong class="calibre2">iter  60 value 8.521214</strong></span>
<span class="strong"><strong class="calibre2">iter  70 value 8.138961</strong></span>
<span class="strong"><strong class="calibre2">iter  80 value 7.291365</strong></span>
<span class="strong"><strong class="calibre2">iter  90 value 7.039209</strong></span>
<span class="strong"><strong class="calibre2">iter 100 value 6.570987</strong></span>
<span class="strong"><strong class="calibre2">iter 110 value 6.355346</strong></span>
<span class="strong"><strong class="calibre2">iter 120 value 6.345511</strong></span>
<span class="strong"><strong class="calibre2">iter 130 value 6.340208</strong></span>
<span class="strong"><strong class="calibre2">iter 140 value 6.337271</strong></span>
<span class="strong"><strong class="calibre2">iter 150 value 6.334285</strong></span>
<span class="strong"><strong class="calibre2">iter 160 value 6.333792</strong></span>
<span class="strong"><strong class="calibre2">iter 170 value 6.333578</strong></span>
<span class="strong"><strong class="calibre2">iter 180 value 6.333498</strong></span>
<span class="strong"><strong class="calibre2">final  value 6.333471 </strong></span>
<span class="strong"><strong class="calibre2">converged</strong></span>
</pre></div></li><li class="listitem" value="4">Use<a id="id582" class="calibre1"/> the <code class="email">summary</code> to <a id="id583" class="calibre1"/>obtain information about the trained neural network:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(iris.nn)</strong></span>
<span class="strong"><strong class="calibre2">a 4-2-3 network with 19 weights</strong></span>
<span class="strong"><strong class="calibre2">options were - softmax modelling  decay=0.0005</strong></span>
<span class="strong"><strong class="calibre2"> b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 </strong></span>
<span class="strong"><strong class="calibre2"> -0.38  -0.63  -1.96   3.13   1.53 </strong></span>
<span class="strong"><strong class="calibre2"> b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 </strong></span>
<span class="strong"><strong class="calibre2">  8.95   0.52   1.42  -1.98  -3.85 </strong></span>
<span class="strong"><strong class="calibre2"> b-&gt;o1 h1-&gt;o1 h2-&gt;o1 </strong></span>
<span class="strong"><strong class="calibre2">  3.08 -10.78   4.99 </strong></span>
<span class="strong"><strong class="calibre2"> b-&gt;o2 h1-&gt;o2 h2-&gt;o2 </strong></span>
<span class="strong"><strong class="calibre2"> -7.41   6.37   7.18 </strong></span>
<span class="strong"><strong class="calibre2"> b-&gt;o3 h1-&gt;o3 h2-&gt;o3 </strong></span>
<span class="strong"><strong class="calibre2">  4.33   4.42 -12.16 </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Training a neural network with nnet">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec255" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we demonstrate steps to train a neural network model with the <code class="email">nnet</code> package. We first use <code class="email">nnet</code> to train the neural network. With this function, we can set the classification formula, source of data, number of hidden units in the <code class="email">size</code> parameter, initial random weight in the <code class="email">rang</code> parameter, parameter for weight decay in the <code class="email">decay</code> parameter, and the maximum iteration in the <code class="email">maxit</code> parameter. As we set <code class="email">maxit</code> to 200, the training process repeatedly runs till the value of the fitting criterion plus the decay term converge. Finally, we use the <code class="email">summary</code> function to obtain information about the built <a id="id584" class="calibre1"/>neural network, which reveals <a id="id585" class="calibre1"/>that the model is built with 4-2-3 networks with 19 weights. Also, the model shows a list of weight transitions from one node to another at the bottom of the printed message.</p></div></div>

<div class="book" title="Training a neural network with nnet">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec256" class="calibre1"/>See also</h2></div></div></div><p class="calibre7">For those who are interested in the background theory of <code class="email">nnet</code> and how it is made, please refer to the following articles:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Ripley, B. D. (1996) <span class="strong"><em class="calibre8">Pattern Recognition and Neural Networks</em></span>. Cambridge</li><li class="listitem">Venables, W. N., and Ripley, B. D. (2002). <span class="strong"><em class="calibre8">Modern applied statistics with S. Fourth edition</em></span>. Springer</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting labels based on a model trained by nnet"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec76" class="calibre1"/>Predicting labels based on a model trained by nnet</h1></div></div></div><p class="calibre7">As we<a id="id586" class="calibre1"/> have trained a neural<a id="id587" class="calibre1"/> network with <code class="email">nnet</code> in the previous recipe, we can now predict the labels of the testing dataset based on the trained neural network. Furthermore, we can assess the model with a confusion matrix adapted <a id="id588" class="calibre1"/>from the <code class="email">caret</code> package.</p></div>

<div class="book" title="Predicting labels based on a model trained by nnet">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec257" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by generating the training dataset, <code class="email">trainset</code>, and the testing dataset, <code class="email">testset</code>, from the <code class="email">iris</code> dataset. The trained neural network also needs to be saved as <code class="email">iris.nn</code>.</p></div></div>

<div class="book" title="Predicting labels based on a model trained by nnet">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec258" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to predict labels based on the trained neural network:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Generate the predictions of the testing dataset based on the model, <code class="email">iris.nn</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; iris.predict = predict(iris.nn, testset, type="class")</strong></span>
</pre></div></li><li class="listitem" value="2">Generate a classification table based on the predicted labels and labels of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; nn.table = table(testset$Species, iris.predict)</strong></span>
<span class="strong"><strong class="calibre2">            iris.predict</strong></span>
<span class="strong"><strong class="calibre2">             setosa versicolor virginica</strong></span>
<span class="strong"><strong class="calibre2">  setosa         17          0         0</strong></span>
<span class="strong"><strong class="calibre2">  versicolor      0         14         0</strong></span>
<span class="strong"><strong class="calibre2">  virginica       0          1        14</strong></span>
</pre></div></li><li class="listitem" value="3">Lastly, generate <a id="id589" class="calibre1"/>a <a id="id590" class="calibre1"/>confusion matrix based on the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(nn.table)</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">            iris.predict</strong></span>
<span class="strong"><strong class="calibre2">             setosa versicolor virginica</strong></span>
<span class="strong"><strong class="calibre2">  setosa         17          0         0</strong></span>
<span class="strong"><strong class="calibre2">  versicolor      0         14         0</strong></span>
<span class="strong"><strong class="calibre2">  virginica       0          1        14</strong></span>

<span class="strong"><strong class="calibre2">Overall Statistics</strong></span>
<span class="strong"><strong class="calibre2">                                                  </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9782609               </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.8847282, 0.9994498)  </strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.3695652               </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022204</strong></span>
<span class="strong"><strong class="calibre2">                                                  </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.9673063               </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : NA                      </strong></span>

<span class="strong"><strong class="calibre2">Statistics by Class:</strong></span>

<span class="strong"><strong class="calibre2">                     Class: setosa Class: versicolor</strong></span>
<span class="strong"><strong class="calibre2">Sensitivity              1.0000000         0.9333333</strong></span>
<span class="strong"><strong class="calibre2">Specificity              1.0000000         1.0000000</strong></span>
<span class="strong"><strong class="calibre2">Pos Pred Value           1.0000000         1.0000000</strong></span>
<span class="strong"><strong class="calibre2">Neg Pred Value           1.0000000         0.9687500</strong></span>
<span class="strong"><strong class="calibre2">Prevalence               0.3695652         0.3260870</strong></span>
<span class="strong"><strong class="calibre2">Detection Rate           0.3695652         0.3043478</strong></span>
<span class="strong"><strong class="calibre2">Detection Prevalence     0.3695652         0.3043478</strong></span>
<span class="strong"><strong class="calibre2">Balanced Accuracy        1.0000000         0.9666667</strong></span>
<span class="strong"><strong class="calibre2">                     Class: virginica</strong></span>
<span class="strong"><strong class="calibre2">Sensitivity                 1.0000000</strong></span>
<span class="strong"><strong class="calibre2">Specificity                 0.9687500</strong></span>
<span class="strong"><strong class="calibre2">Pos Pred Value              0.9333333</strong></span>
<span class="strong"><strong class="calibre2">Neg Pred Value              1.0000000</strong></span>
<span class="strong"><strong class="calibre2">Prevalence                  0.3043478</strong></span>
<span class="strong"><strong class="calibre2">Detection Rate              0.3043478</strong></span>
<span class="strong"><strong class="calibre2">Detection Prevalence        0.3260870</strong></span>
<span class="strong"><strong class="calibre2">Balanced Accuracy           0.9843750</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Predicting labels based on a model trained by nnet">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec259" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Similar<a id="id591" class="calibre1"/> to other classification <a id="id592" class="calibre1"/>methods, one can also predict labels based on the neural networks trained by <code class="email">nnet</code>. First, we use the <code class="email">predict</code> function to generate the predicted labels based on a testing dataset, <code class="email">testset</code>. Within the <code class="email">predict</code> function, we specify the <code class="email">type</code> argument to the class, so the output will be class labels instead of a probability matrix. Next, we use the <code class="email">table</code> function to generate a classification table based on predicted labels and labels written in the testing dataset. Finally, as we have created the classification table, we can employ a confusion matrix from the <code class="email">caret</code> package to measure the prediction performance of the trained neural network.</p></div></div>

<div class="book" title="Predicting labels based on a model trained by nnet">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec260" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For the <code class="email">predict</code> function, if the <code class="email">type</code> argument to <code class="email">class</code> is not specified, by default, it will generate a probability matrix as a prediction result, which is very similar to <code class="email">net.result</code> generated from the <code class="email">compute</code> function within the <code class="email">neuralnet</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; head(predict(iris.nn, testset))</strong></span>
</pre></div></li></ul></div></div></div></body></html>