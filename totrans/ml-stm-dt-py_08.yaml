- en: '*Chapter 6*: Online Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, you were introduced to some basic notions of classification.
    You first saw a use case in which online classification models in River were used
    to build a model that can identify an iris species based on a number of characteristics
    of a plant. This iris dataset is one of the best-known datasets in the world and
    is a very common starting point for classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: After that, you looked at anomaly detection. We discussed how classification
    models can be used for anomaly detection for those cases where we can label anomalies
    as one class and non-anomalies as another class. Specific anomaly detection models
    are often better at the task, as they strive to understand only the non-anomalies.
    Classification models will strive to understand each of the classes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll go much deeper into classification. The chapter will
    start by posing definitions of what classification is and what it can be used
    for. You will then see a number of classification models, of which you'll learn
    the differences between their online and offline counterparts. You will also implement
    multiple examples in Python using the River package. This will, in the end, result
    in a model benchmarking study for the use case that will be introduced later on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Defining classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying use cases of classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification algorithms in River
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Go to the link of the repository.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the green **Code** button.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Download ZIP**.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you download the ZIP file, unzip it in your local environment, and you
    will be able to access the code through your preferred Python editor.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Python environment
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: If you are not yet familiar with Python environments, I would advise you to
    check out Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with Jupyter Notebook and JupyterLab, which are both great for executing
    notebooks. It also comes with Spyder and VSCode for editing scripts and programs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup to do.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Defining classification
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will discover classification. Classification is a supervised
    machine learning task in which a model is constructed that assigns observations
    to a category.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The simplest types of classification models that everybody tends to know are
    decision trees. Let's consider a super simple example of how a decision tree could
    be used for classification.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we have a dataset in which we have observations about five humans
    and five animals. The goal is to use this data to build a decision tree that can
    be used on any new, unseen animal or human.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The data can be imported as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-1
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The data is shown in the following figure:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The data'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_1.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – The data
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to construct the decision tree, you would generally use machine learning,
    as that is far more efficient than constructing the tree by hand. Yet, for this
    example, let''s do a simple decision tree that works as the following graph indicates:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – The example decision tree'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_2.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – The example decision tree
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is a model, so it is only a partial representation of the truth.
    It works quite well for the current dataset of 10 observations, but with more
    data points, you would encounter all types of anomalies, so you'd need more variables.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'You could code this model for a `human` versus `not human` classification in
    Python as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result is the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – The predicted outcomes'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_3.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – The predicted outcomes
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The general idea behind this is that a classification model is any machine learning
    model that uses the data to generate decision rules to assign observations to
    specific classes. In the next section, we'll be going into some use cases of classification
    to get a better idea of what it can be used for in practice.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Identifying use cases of classification
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use cases of classification are huge; it is a very commonly used method
    in many projects. Still, let's see some examples to get a better idea of the different
    types of use cases that can benefit from classification methods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Use case 1 – email spam classification
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first use case that is generally built on classification is **spam detection**
    in email. Spam emails have been around for a long time. The business model of
    sending fake emails to generally steal people's money is a big problem, and receiving
    many spam emails can negatively impact your emailing experience.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Email service providers have come a long way in detecting spam emails automatically
    and sending them to your spam/junk box. Nowadays, this is all done automatically
    and relies heavily on machine learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: If you compare this to our super-small classification example, you could imagine
    that the decision tree (or any other model) can take several information types
    about every received email and use that to decide whether or not the email should
    be classified as spam. This has to be done in real time, as nobody wants to wait
    for a spam detection service to finally send their email through.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about this use case in the following resources:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.sciencedirect.com/science/article/pii/S2405844018353404](https://www.sciencedirect.com/science/article/pii/S2405844018353404)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning](https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case 2 – face detection in phone camera
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second example of classification is face detection when you want to unlock
    your phone. Your phone has to make a split-second decision whether the face it's
    seeing is the face of its owner or not.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'This decision is a classification decision, as it comes down to a yes/no decision:
    it *is* the owner, or it is *not* the owner. This decision will generally be made
    by machine learning, as the rules would be very complex and hard to write down
    as `if`/`else` statements. Machine learning algorithms are, nowadays, relatively
    good at such use cases.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'For other more detailed examples of this use case, you can check out the following
    links:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.xfinity.com/hub/mobile/facial-recognition-on-phone](https://www.xfinity.com/hub/mobile/facial-recognition-on-phone)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nytimes.com/wirecutter/blog/how-facial-recognition-works/](https://www.nytimes.com/wirecutter/blog/how-facial-recognition-works/)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case 3 – online marketing ad selection
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A final example to add to the previous two is online marketing ad selection.
    Many websites nowadays display personalized ads. This means that you will see
    an advertisement that matches you as a customer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Personalized ad systems do not invent ads though; they have to make a decision
    and choose between multiple available ads to know which one fits you best. In
    this way, it is a classification, as it has to decide between multiple choices.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As you can understand, page loads have to be fast and, therefore, ad selection
    has to be done in a split second as well. Real-time responses are key for the
    model to provide any value at all.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'The following links talk in more depth about this use case:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.owox.com/blog/articles/machine-learning-in-marketing/](https://www.owox.com/blog/articles/machine-learning-in-marketing/)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/watson-advertising/thought-leadership/benefits-of-machine-learning-in-advertising](https://www.ibm.com/watson-advertising/thought-leadership/benefits-of-machine-learning-in-advertising)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you'll see a more practical side to doing classification,
    as you will discover several classification algorithms in the River Python library.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Overview of classification algorithms in River
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a large number of online classification models available in the River
    online machine learning package.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'A selection of relevant ones is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegression`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Perceptron`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AdaptiveRandomForestClassifier`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ALMAClassifier`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PAClassifier`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification algorithm 1 – LogisticRegression
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is one of the most basic statistical classification models.
    It models a dependent variable (target variable) that has two classes (1 or 0)
    and can use multiple independent variables to make the prediction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The model combines each of the independent variables as log-odds; you can see
    this as the coefficients in linear regression, except that they are log-odds for
    each variable. The split in the model is based on the logistic function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see a simplified schematic of the idea as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The logistic curve'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_4.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – The logistic curve
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression in River
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For online logistic regression, you can use the `LogisticRegression` class
    in River''s `linear_model` section. Let''s now see an example of that:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can start by making a classification dataset using sklearn''s inbuilt
    `make_blobs` function, which makes classification datasets. You can use the following
    code for this:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-3
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To see what this dataset looks like, it is important to make a plot. You can
    use the following `matplotlib` code for this:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-4
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should obtain the following plot, or something resembling it:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The data'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_5.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – The data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure that your model evaluation will be fair, it is important to make
    a train-test split in the data. You can do this with sklearn''s `train_test_split`,
    as shown here:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-5
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s now move on to the application of the logistic regression model. The
    following code shows how to fit the model one data point at a time. Note that
    you should be using a JSON conversion of the input data for `x`, as this is required
    by River:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-6
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The printed data will look something like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The output of Code Block 6-6'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_6.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – The output of Code Block 6-6
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do predictions one by one as well, or you can use `predict_many` to
    make all the predictions on the test set at once. There will not be any difference
    in the result. In the following code, `predict_many` is used:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-7
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To get a quality metric on this prediction, let''s use the accuracy score by
    `scikit-learn`. As you can see in the following code block, the model has obtained
    100% accuracy on the blob data example. It must be stated that this blob data
    example is a simple prediction task as the data is perfectly separable by a straight
    line, as can be seen in the plot shown earlier:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-8
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This should result in the following output:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The output of Code Block 6-8'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_7.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – The output of Code Block 6-8
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 2 – Perceptron
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron is another algorithm for supervised learning on classification
    problems. It takes inputs, multiplies them by weights, and puts the sum of those
    through an activation function. The output is the resulting classification. The
    following graph shows an example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Schematic overview of a perceptron'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_8.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Schematic overview of a perceptron
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron in River
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like logistic regression, the perceptron is a commonly used offline model that
    has been reworked into an online model for River. In River, the perceptron has
    been implemented as a special case of logistic regression.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the perceptron just like logistic regression. You can use the same
    code example as in the previous case, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-9
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The result is `1.0`, which is, unsurprisingly, the same as the logistic regression
    result.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 3 – AdaptiveRandomForestClassifier
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the introduction, you already saw the general idea behind a decision tree.
    Random Forests are an ensemble model that improves decision trees.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind Random Forests is that they reduce the error of single decision
    trees by making a large number of slightly different decision trees. The most
    common prediction among a large number of decision trees is retained as the final
    prediction.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The decision trees are made slightly differently by fitting each of them on
    a slightly different dataset, which is created by resampling the observations.
    There is also a subset of variables used for creating the decision tree splits.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest in River
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For online learning, the data needs to be fitted one by one into the Random
    Forest, which is not an easy task. River''s implementation is based on the two
    key elements of Random Forests, which are the resampling and the variable subsets.
    They have also added drift detection for each single decision tree:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use an alternative data creation function, which creates data that is
    harder to separate than the blobs. This function from `sklearn` is called `make_classification`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-10
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The data is shown in the following figure:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – The new data'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_9.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – The new data
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a total of 20 variables generated by default, of which a number are
    automatically made more relevant and some are mostly irrelevant. Let''s do a train-test
    split just like before:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-11
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Using this train-test split, we can move on to building the model:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-12
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now that the model is fit, we can make predictions on the test set. There is
    no `predict_many` function here, so it is necessary to do a loop with `predict_one`
    repeatedly:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-13
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As a final step, let''s compute the accuracy of this model:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-14
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The result is `0.86`. Of course, the dataset was more difficult to predict,
    so that is not to be mistaken for a bad score. As an additional metric, we can
    look at the classification report for more information:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-15
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The result is shown in the following figure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The output of Code Block 6-15'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_10.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – The output of Code Block 6-15
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In this classification report, you see that the precision and recall and the
    scores for positives and negatives are all relatively equal. This shows that there
    is no imbalance in the classifier, which is important when relying on the accuracy
    score.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 4 – ALMAClassifier
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have seen some commonly used machine learning models for classification
    in a way adapted to accommodate online learning, it is time to see some more specific
    models as well. The first of these is the ALMA classifier.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The **approximate large margin algorithm** (**ALMA**) classifier is an incremental
    implementation of **support vector machines** (**SVMs**), a commonly used machine
    learning model for classification.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'You saw the adaptation of SVMs in the previous chapter: a one-class SVM is
    often used for anomaly detection. For classification, you''d use a regular (two-class)
    SVM.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: ALMAClassifier in River
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s see how ALMAClassifier compares to the adaptive Random Forest, by executing
    it on the same data:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by applying the same code that we already defined before:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-16
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The result is `0.77`, not as good as the Random Forest. Let''s also check the
    classification report to see whether anything changed there:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-17
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The result is shown in the following figure:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11 – The output of Code Block 6-17'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_11.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – The output of Code Block 6-17
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: There is a little more variation here, but nothing that seems too shocking.
    In general, the Random Forest was just better overall for this data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 5 – PAClassifier
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **passive-aggressive** (**PA**) classifier is an online machine learning
    model that is not related to any existing offline model. It is based on the idea
    of updating the model at each step and thereby solving the following problem:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '*The update of the classifier is performed by solving a constrained optimization
    problem: we would like the new classifier to remain as close as possible to the
    current one while achieving at least a unit margin on the most recent example.*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'This quote has been taken from the following paper on PA algorithms, which
    is also an interesting reference for further reading: [https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf](https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The name *passive-aggressive* comes from the idea that an algorithm that learns
    too quickly from each new data point is considered too aggressive. PA is less
    aggressive.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: PAClassifier in River
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s see how the PA classifier performs on the same task as the two previous
    models:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-18
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The obtained score is `0.85`. The following section summarizes all the scores
    that we have obtained.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating benchmark results
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This leaves us with the following accuracy scores for the past three models:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.1 – The table with the results'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_Table_01.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 – The table with the results
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The best result was obtained by AdaptiveRandomForest and PAClassifier came in
    second place. ALMAClassifier was less performant with a score of `0.77`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have first seen a general overview of classification and
    its use cases. You have understood how it is different from anomaly detection,
    but how it can sometimes still be applied to anomaly detection use cases.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: You have learned about five models for online classification of which some are
    mainly adaptations of offline models, and others are specifically designed for
    working in an online manner. Both types exist, and it is important to have the
    tools to benchmark model performance before making a choice for a final model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The model benchmark that you executed in Python was done in such a way as to
    find the best model in terms of the accuracy of the model on a test set. You have
    seen clear differences between the benchmarked models, and this is a great showcase
    for the importance of model benchmarking.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, you will do the same type of model benchmarking exercise,
    but this time, you will be focusing on a regression use case, which has a goal
    that is fundamentally different from classification. This comes with some changes
    with respect to measuring errors and benchmarking, but from a high-level perspective,
    also has a lot in common with the classification benchmarking use case that you
    worked with in this chapter.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*LogisticRegression*: [https://riverml.xyz/latest/api/linear-model/LogisticRegression/](https://riverml.xyz/latest/api/linear-model/LogisticRegression/)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Perceptron*: [https://riverml.xyz/latest/api/linear-model/Perceptron/](https://riverml.xyz/latest/api/linear-model/Perceptron/)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AdaptiveRandomForestClassifier*: [https://riverml.xyz/latest/api/ensemble/AdaptiveRandomForestClassifier/](https://riverml.xyz/latest/api/ensemble/AdaptiveRandomForestClassifier/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ALMA*: [https://riverml.xyz/latest/api/linear-model/ALMAClassifier/](https://riverml.xyz/latest/api/linear-model/ALMAClassifier/)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ALMA*: [https://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf](https://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '*PAClassifier*: [https://riverml.xyz/latest/api/linear-model/PAClassifier/](https://riverml.xyz/latest/api/linear-model/PAClassifier/'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '*PAClassifier*: [https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf](https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '*make_classification*: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.htm](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.htm'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '*make_blobs*: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*make_blobs*: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)'
