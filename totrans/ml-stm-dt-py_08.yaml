- en: '*Chapter 6*: Online Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, you were introduced to some basic notions of classification.
    You first saw a use case in which online classification models in River were used
    to build a model that can identify an iris species based on a number of characteristics
    of a plant. This iris dataset is one of the best-known datasets in the world and
    is a very common starting point for classification.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you looked at anomaly detection. We discussed how classification
    models can be used for anomaly detection for those cases where we can label anomalies
    as one class and non-anomalies as another class. Specific anomaly detection models
    are often better at the task, as they strive to understand only the non-anomalies.
    Classification models will strive to understand each of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll go much deeper into classification. The chapter will
    start by posing definitions of what classification is and what it can be used
    for. You will then see a number of classification models, of which you'll learn
    the differences between their online and offline counterparts. You will also implement
    multiple examples in Python using the River package. This will, in the end, result
    in a model benchmarking study for the use case that will be introduced later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying use cases of classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification algorithms in River
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the link of the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the green **Code** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Download ZIP**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you download the ZIP file, unzip it in your local environment, and you
    will be able to access the code through your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: Python environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not yet familiar with Python environments, I would advise you to
    check out Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with Jupyter Notebook and JupyterLab, which are both great for executing
    notebooks. It also comes with Spyder and VSCode for editing scripts and programs.
  prefs: []
  type: TYPE_NORMAL
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup to do.
  prefs: []
  type: TYPE_NORMAL
- en: Defining classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will discover classification. Classification is a supervised
    machine learning task in which a model is constructed that assigns observations
    to a category.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest types of classification models that everybody tends to know are
    decision trees. Let's consider a super simple example of how a decision tree could
    be used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we have a dataset in which we have observations about five humans
    and five animals. The goal is to use this data to build a decision tree that can
    be used on any new, unseen animal or human.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data can be imported as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – The data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to construct the decision tree, you would generally use machine learning,
    as that is far more efficient than constructing the tree by hand. Yet, for this
    example, let''s do a simple decision tree that works as the following graph indicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – The example decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – The example decision tree
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is a model, so it is only a partial representation of the truth.
    It works quite well for the current dataset of 10 observations, but with more
    data points, you would encounter all types of anomalies, so you'd need more variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'You could code this model for a `human` versus `not human` classification in
    Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – The predicted outcomes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – The predicted outcomes
  prefs: []
  type: TYPE_NORMAL
- en: The general idea behind this is that a classification model is any machine learning
    model that uses the data to generate decision rules to assign observations to
    specific classes. In the next section, we'll be going into some use cases of classification
    to get a better idea of what it can be used for in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying use cases of classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The use cases of classification are huge; it is a very commonly used method
    in many projects. Still, let's see some examples to get a better idea of the different
    types of use cases that can benefit from classification methods.
  prefs: []
  type: TYPE_NORMAL
- en: Use case 1 – email spam classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first use case that is generally built on classification is **spam detection**
    in email. Spam emails have been around for a long time. The business model of
    sending fake emails to generally steal people's money is a big problem, and receiving
    many spam emails can negatively impact your emailing experience.
  prefs: []
  type: TYPE_NORMAL
- en: Email service providers have come a long way in detecting spam emails automatically
    and sending them to your spam/junk box. Nowadays, this is all done automatically
    and relies heavily on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: If you compare this to our super-small classification example, you could imagine
    that the decision tree (or any other model) can take several information types
    about every received email and use that to decide whether or not the email should
    be classified as spam. This has to be done in real time, as nobody wants to wait
    for a spam detection service to finally send their email through.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about this use case in the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.sciencedirect.com/science/article/pii/S2405844018353404](https://www.sciencedirect.com/science/article/pii/S2405844018353404)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning](https://www.enjoyalgorithms.com/blog/email-spam-and-non-spam-filtering-using-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case 2 – face detection in phone camera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second example of classification is face detection when you want to unlock
    your phone. Your phone has to make a split-second decision whether the face it's
    seeing is the face of its owner or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'This decision is a classification decision, as it comes down to a yes/no decision:
    it *is* the owner, or it is *not* the owner. This decision will generally be made
    by machine learning, as the rules would be very complex and hard to write down
    as `if`/`else` statements. Machine learning algorithms are, nowadays, relatively
    good at such use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For other more detailed examples of this use case, you can check out the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.xfinity.com/hub/mobile/facial-recognition-on-phone](https://www.xfinity.com/hub/mobile/facial-recognition-on-phone)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nytimes.com/wirecutter/blog/how-facial-recognition-works/](https://www.nytimes.com/wirecutter/blog/how-facial-recognition-works/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case 3 – online marketing ad selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A final example to add to the previous two is online marketing ad selection.
    Many websites nowadays display personalized ads. This means that you will see
    an advertisement that matches you as a customer.
  prefs: []
  type: TYPE_NORMAL
- en: Personalized ad systems do not invent ads though; they have to make a decision
    and choose between multiple available ads to know which one fits you best. In
    this way, it is a classification, as it has to decide between multiple choices.
  prefs: []
  type: TYPE_NORMAL
- en: As you can understand, page loads have to be fast and, therefore, ad selection
    has to be done in a split second as well. Real-time responses are key for the
    model to provide any value at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following links talk in more depth about this use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.owox.com/blog/articles/machine-learning-in-marketing/](https://www.owox.com/blog/articles/machine-learning-in-marketing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/watson-advertising/thought-leadership/benefits-of-machine-learning-in-advertising](https://www.ibm.com/watson-advertising/thought-leadership/benefits-of-machine-learning-in-advertising)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you'll see a more practical side to doing classification,
    as you will discover several classification algorithms in the River Python library.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of classification algorithms in River
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a large number of online classification models available in the River
    online machine learning package.
  prefs: []
  type: TYPE_NORMAL
- en: 'A selection of relevant ones is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegression`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Perceptron`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AdaptiveRandomForestClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ALMAClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PAClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification algorithm 1 – LogisticRegression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is one of the most basic statistical classification models.
    It models a dependent variable (target variable) that has two classes (1 or 0)
    and can use multiple independent variables to make the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The model combines each of the independent variables as log-odds; you can see
    this as the coefficients in linear regression, except that they are log-odds for
    each variable. The split in the model is based on the logistic function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see a simplified schematic of the idea as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The logistic curve'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – The logistic curve
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression in River
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For online logistic regression, you can use the `LogisticRegression` class
    in River''s `linear_model` section. Let''s now see an example of that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can start by making a classification dataset using sklearn''s inbuilt
    `make_blobs` function, which makes classification datasets. You can use the following
    code for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To see what this dataset looks like, it is important to make a plot. You can
    use the following `matplotlib` code for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should obtain the following plot, or something resembling it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – The data
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure that your model evaluation will be fair, it is important to make
    a train-test split in the data. You can do this with sklearn''s `train_test_split`,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now move on to the application of the logistic regression model. The
    following code shows how to fit the model one data point at a time. Note that
    you should be using a JSON conversion of the input data for `x`, as this is required
    by River:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The printed data will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The output of Code Block 6-6'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – The output of Code Block 6-6
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do predictions one by one as well, or you can use `predict_many` to
    make all the predictions on the test set at once. There will not be any difference
    in the result. In the following code, `predict_many` is used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a quality metric on this prediction, let''s use the accuracy score by
    `scikit-learn`. As you can see in the following code block, the model has obtained
    100% accuracy on the blob data example. It must be stated that this blob data
    example is a simple prediction task as the data is perfectly separable by a straight
    line, as can be seen in the plot shown earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The output of Code Block 6-8'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – The output of Code Block 6-8
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 2 – Perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron is another algorithm for supervised learning on classification
    problems. It takes inputs, multiplies them by weights, and puts the sum of those
    through an activation function. The output is the resulting classification. The
    following graph shows an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Schematic overview of a perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Schematic overview of a perceptron
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron in River
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like logistic regression, the perceptron is a commonly used offline model that
    has been reworked into an online model for River. In River, the perceptron has
    been implemented as a special case of logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the perceptron just like logistic regression. You can use the same
    code example as in the previous case, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The result is `1.0`, which is, unsurprisingly, the same as the logistic regression
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 3 – AdaptiveRandomForestClassifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the introduction, you already saw the general idea behind a decision tree.
    Random Forests are an ensemble model that improves decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind Random Forests is that they reduce the error of single decision
    trees by making a large number of slightly different decision trees. The most
    common prediction among a large number of decision trees is retained as the final
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The decision trees are made slightly differently by fitting each of them on
    a slightly different dataset, which is created by resampling the observations.
    There is also a subset of variables used for creating the decision tree splits.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest in River
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For online learning, the data needs to be fitted one by one into the Random
    Forest, which is not an easy task. River''s implementation is based on the two
    key elements of Random Forests, which are the resampling and the variable subsets.
    They have also added drift detection for each single decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use an alternative data creation function, which creates data that is
    harder to separate than the blobs. This function from `sklearn` is called `make_classification`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – The new data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – The new data
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a total of 20 variables generated by default, of which a number are
    automatically made more relevant and some are mostly irrelevant. Let''s do a train-test
    split just like before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this train-test split, we can move on to building the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the model is fit, we can make predictions on the test set. There is
    no `predict_many` function here, so it is necessary to do a loop with `predict_one`
    repeatedly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As a final step, let''s compute the accuracy of this model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is `0.86`. Of course, the dataset was more difficult to predict,
    so that is not to be mistaken for a bad score. As an additional metric, we can
    look at the classification report for more information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The output of Code Block 6-15'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – The output of Code Block 6-15
  prefs: []
  type: TYPE_NORMAL
- en: In this classification report, you see that the precision and recall and the
    scores for positives and negatives are all relatively equal. This shows that there
    is no imbalance in the classifier, which is important when relying on the accuracy
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 4 – ALMAClassifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have seen some commonly used machine learning models for classification
    in a way adapted to accommodate online learning, it is time to see some more specific
    models as well. The first of these is the ALMA classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The **approximate large margin algorithm** (**ALMA**) classifier is an incremental
    implementation of **support vector machines** (**SVMs**), a commonly used machine
    learning model for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'You saw the adaptation of SVMs in the previous chapter: a one-class SVM is
    often used for anomaly detection. For classification, you''d use a regular (two-class)
    SVM.'
  prefs: []
  type: TYPE_NORMAL
- en: ALMAClassifier in River
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s see how ALMAClassifier compares to the adaptive Random Forest, by executing
    it on the same data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by applying the same code that we already defined before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is `0.77`, not as good as the Random Forest. Let''s also check the
    classification report to see whether anything changed there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 6-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11 – The output of Code Block 6-17'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – The output of Code Block 6-17
  prefs: []
  type: TYPE_NORMAL
- en: There is a little more variation here, but nothing that seems too shocking.
    In general, the Random Forest was just better overall for this data.
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithm 5 – PAClassifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **passive-aggressive** (**PA**) classifier is an online machine learning
    model that is not related to any existing offline model. It is based on the idea
    of updating the model at each step and thereby solving the following problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The update of the classifier is performed by solving a constrained optimization
    problem: we would like the new classifier to remain as close as possible to the
    current one while achieving at least a unit margin on the most recent example.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This quote has been taken from the following paper on PA algorithms, which
    is also an interesting reference for further reading: [https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf](https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The name *passive-aggressive* comes from the idea that an algorithm that learns
    too quickly from each new data point is considered too aggressive. PA is less
    aggressive.
  prefs: []
  type: TYPE_NORMAL
- en: PAClassifier in River
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s see how the PA classifier performs on the same task as the two previous
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 6-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The obtained score is `0.85`. The following section summarizes all the scores
    that we have obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating benchmark results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This leaves us with the following accuracy scores for the past three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.1 – The table with the results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_06_Table_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 – The table with the results
  prefs: []
  type: TYPE_NORMAL
- en: The best result was obtained by AdaptiveRandomForest and PAClassifier came in
    second place. ALMAClassifier was less performant with a score of `0.77`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have first seen a general overview of classification and
    its use cases. You have understood how it is different from anomaly detection,
    but how it can sometimes still be applied to anomaly detection use cases.
  prefs: []
  type: TYPE_NORMAL
- en: You have learned about five models for online classification of which some are
    mainly adaptations of offline models, and others are specifically designed for
    working in an online manner. Both types exist, and it is important to have the
    tools to benchmark model performance before making a choice for a final model.
  prefs: []
  type: TYPE_NORMAL
- en: The model benchmark that you executed in Python was done in such a way as to
    find the best model in terms of the accuracy of the model on a test set. You have
    seen clear differences between the benchmarked models, and this is a great showcase
    for the importance of model benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, you will do the same type of model benchmarking exercise,
    but this time, you will be focusing on a regression use case, which has a goal
    that is fundamentally different from classification. This comes with some changes
    with respect to measuring errors and benchmarking, but from a high-level perspective,
    also has a lot in common with the classification benchmarking use case that you
    worked with in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*LogisticRegression*: [https://riverml.xyz/latest/api/linear-model/LogisticRegression/](https://riverml.xyz/latest/api/linear-model/LogisticRegression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Perceptron*: [https://riverml.xyz/latest/api/linear-model/Perceptron/](https://riverml.xyz/latest/api/linear-model/Perceptron/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AdaptiveRandomForestClassifier*: [https://riverml.xyz/latest/api/ensemble/AdaptiveRandomForestClassifier/](https://riverml.xyz/latest/api/ensemble/AdaptiveRandomForestClassifier/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ALMA*: [https://riverml.xyz/latest/api/linear-model/ALMAClassifier/](https://riverml.xyz/latest/api/linear-model/ALMAClassifier/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ALMA*: [https://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf](https://www.jmlr.org/papers/volume2/gentile01a/gentile01a.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*PAClassifier*: [https://riverml.xyz/latest/api/linear-model/PAClassifier/](https://riverml.xyz/latest/api/linear-model/PAClassifier/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*PAClassifier*: [https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf](https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*make_classification*: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.htm](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.htm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*make_blobs*: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
