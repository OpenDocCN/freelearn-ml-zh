<html><head></head><body>
<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer161">
</div>
</div>
<div class="Content" id="_idContainer162">
<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Section 4 â€“ Modeling Dichotomous and Multiclass Targets with Supervised Learning</h1>
<p>There are a good number of high performing algorithms for predicting categorical targets. We will examine the most popular classification algorithms in this part. We will also consider why we might choose one algorithm over any of the others given the attributes our data and our domain knowledge.</p>
<p>We are as concerned with underfitting and overfitting with classification models as we were with regression models in the previous part. When the relationship between features and the target is complicated, we need to use an algorithm that can capture that complexity. But there is often a non-trivial risk of overfitting. We will discuss strategies for modeling complexity without overfitting in the chapters in this part. This usually involves some form of regularization for logistic regression models, limits on tree depth for decision trees, and adjusting the tolerance for margin violations with support vector classification.</p>
<p>If we are trying to model complexity without overfitting we have to be prepared to spend a good chunk of time doing hyperparameter tuning. We will definitely spend a fair bit of time on that in these chapters. Related to that, we also get really good at cross validation and generating and interpreting evaluation metrics. We will discuss accuracy, precision, sensitivity, and specificity in each of the next five chapters. We will also get very used to staring at a confusion matrix.</p>
<p>We will also examine how these algorithms can be extended to multiclass targets. This is straightforward with k-nearest neighbors and decision tress, but requires extension to the algorithm for logistic regression and support vector regression. We go over that in these chapters.</p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a>, <em class="italic">Logistic Regression</em></li>
<li><a href="B17978_11_ePub.xhtml#_idTextAnchor135"><em class="italic">Chapter 11</em></a>, <em class="italic">Decision Trees and Random Forest Classification</em></li>
<li><a href="B17978_12_ePub.xhtml#_idTextAnchor144"><em class="italic">Chapter 12</em></a>, <em class="italic">K-Nearest Neighbors for Classification</em></li>
<li><a href="B17978_13_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 13</em></a>, <em class="italic">Support Vector Machine Classification</em></li>
<li><a href="B17978_14_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Naive Bayes Classification</em></li>
</ul>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer163">
</div>
</div>
</div>
</body></html>