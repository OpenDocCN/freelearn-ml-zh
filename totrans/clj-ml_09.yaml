- en: Chapter 9. Large-scale Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore a few methodologies for handling large volumes
    of data to train machine learning models. In the latter section of this chapter,
    we will also demonstrate how to use cloud-based services for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Using MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data-processing methodology that is often encountered in the context of data
    parallelism is **MapReduce**. This technique is inspired by the **map** and **reduce**
    functions from functional programming. Although these functions serve as a basis
    to understand the algorithm, actual implementations of MapReduce focus more on
    scaling and distributing the processing of data. There are currently several active
    implementations of MapReduce, such as Apache Hadoop and Google Bigtable.
  prefs: []
  type: TYPE_NORMAL
- en: A MapReduce engine or program is composed of a function that performs some processing
    over a given record in a potentially large dataset (for more information, refer
    to "Map-Reduce for Machine Learning on Multicore"). This function represents the
    `Map()` step of the algorithm. This function is applied to all the records in
    the dataset and the results are then combined. The latter step of extracting the
    results is termed as the `Reduce()` step of the algorithm. In order to scale this
    process over huge datasets, the input data provided to the `Map()` step is first
    partitioned and then processed on different computing nodes. These nodes may or
    may not be on separate machines, but the processing performed by a given node
    is independent from that of the other nodes in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Some systems follow a different design in which the code or query is sent to
    nodes that contain the data, instead of the other way around. This step of partitioning
    the input data and then forwarding the query or data to different nodes is called
    the `Partition()` step of the algorithm. To summarize, this method of handling
    a large dataset is quite different from traditional methods of iterating over
    the entire data as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce scales better than other methods because the partitions of the input
    data can be processed independently on physically different machines and then
    combined later. This gain in scalability is not only because the input is divided
    among several nodes, but because of an intrinsic reduction in complexity. An NP-hard
    problem cannot be solved for a large problem space, but can be solved if the problem
    space is smaller.
  prefs: []
  type: TYPE_NORMAL
- en: For problems with an algorithmic complexity of ![Using MapReduce](img/4351OS_09_01.jpg)
    or ![Using MapReduce](img/4351OS_09_02.jpg), partitioning the problem space will
    actually increase the time needed to solve the given problem. However, if the
    algorithmic complexity is ![Using MapReduce](img/4351OS_09_03.jpg), where ![Using
    MapReduce](img/4351OS_09_04.jpg), partitioning the problem space will reduce the
    time needed to solve the problem. In case of NP-hard problems, ![Using MapReduce](img/4351OS_09_05.jpg).
    Thus, MapReduce decreases the time needed to solve NP-hard problems by partitioning
    the problem space (for more information, refer to *Evaluating MapReduce for Multi-core
    and Multiprocessor Systems*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MapReduce algorithm can be illustrated using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using MapReduce](img/4351OS_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous diagram, the input data is first partitioned, and each partition
    is independently processed in the `Map()` step. Finally, the results are combined
    in the `Reduce()` step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can concisely define the MapReduce algorithm in Clojure pseudo-code, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `map-reduce` function defined in the previous code distributes the application
    of the function `f` among several processors (or threads) using the standard `pmap`
    (abbreviation for parallel map) function. The input data, represented by the collection
    `coll`, is first partitioned using the `partition-all` function, and the function
    `f` is then applied to each partition in parallel using the `pmap` function. The
    results of this `Map()` step are then combined using a composition of the standard
    `reduce` and `concat` functions. Note that this is possible in Clojure due to
    the fact the each partition of data is a sequence, and the `pmap` function will
    thus return a sequence of partitions that can be joined or concatenated into a
    single sequence to produce the result of the computation.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is only a theoretical explanation of the core of the MapReduce
    algorithm. Actual implementations tend to focus more on distributing the processing
    among several machines, rather than among several processors or threads as shown
    in the `map-reduce` function defined in the previous code.
  prefs: []
  type: TYPE_NORMAL
- en: Querying and storing datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with large datasets, it's useful to be able to query the data based
    on some arbitrary conditions. Also, it's more reliable to store the data in a
    database rather than in a flat file or as an in-memory resource. The Incanter
    library provides us with several useful functions to perform these operations,
    as we will demonstrate in the code example that will follow.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Incanter library and the MongoDB driver used in the upcoming example can
    be added to a Leiningen project by adding the following dependency to the `project.clj`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Also, this example requires MongoDB to be installed and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will use the Iris dataset, which can be fetched using
    the `get-dataset` function from the `incanter.datasets` namespace. The code is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the previous code, we simply bind the Iris dataset to a variable
    `iris`. We can perform various operations on this dataset using the `with-data`
    function. To view the data, we can use the `view` function along with the `with-data`
    function to provide a tabular representation of the dataset, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `$data` variable is a special binding that can be used to represent the
    entire dataset within the scope of the `with-data` function. In the previous code,
    we add an extra column to represent the row number of a record to the data using
    a composition of the `conj-cols`, `nrows`, and `range` functions. The data is
    then displayed in a spreadsheet-like table using the `view` function. The previous
    code produces the following table that represents the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Querying and storing datasets](img/4351OS_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also select columns we are interested in from the original dataset using
    the `$` function within the scope of the `with-data` function, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `$` function selects the `:Species` and `:Sepal.Length` columns from the
    `iris` dataset in the code example shown previously. We can also filter the data
    based on a condition using the `$where` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example queries the `iris` dataset for records with the `:Sepal.Length`
    column equal to `7.7` using the `$where` function. We can also specify the lower
    or upper bound of the value to compare a column to using the `:$gt` and `:$lt`
    symbols in a map passed to `$where` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example checks for records that have a `:Sepal.Length` attribute
    with a value greater than `7`. To check whether a column''s value lies within
    a given range, we can specify both the `:$gt` and `:$lt` keys in the map passed
    to the `$where` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous example checks for records that have a `:Sepal.Length` attribute
    within the range of `7.0` and `7.5`. We can also specify a discrete set of values
    using the `$:in` key, such as in the expression `{:$in #{7.2 7.3 7.5}}`. The Incanter
    library also provides several other functions such as `$join` and `$group-by`
    that can be used to express more complex queries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Incanter library provides functions to operate with MongoDB to persist
    and fetch datasets. MongoDB is a nonrelational document database that allows for
    storage of JSON documents with dynamic schemas. To connect to a MongoDB instance,
    we use the `mongo!` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, the database name `sampledb` is specified as a keyword
    argument with the key `:db` to the `mongo!` function. We can also specify the
    hostname and port of the instance to connect to using the `:host` and `:post`
    keyword arguments, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can store datasets in the connected MongoDB instance using the `insert-dataset`
    function from the `incanter.mongodb` namespace. Unfortunately, MongoDB does not
    support the use of the dot character (.) as column names, and so we must change
    the names of the columns in the `iris` dataset in order to successfully store
    it using the `insert-dataset` function. Replacing the column names can be done
    using the `col-names` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The previous code stores the `iris` dataset in the MongoDB instance after replacing
    the dot characters in the column names.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the dataset will be stored in a collection named `iris` in the `sampledb`
    database. Also, MongoDB will assign a hash-based ID to each record in the dataset
    that was stored in the database. This column can be referred to using the `:_id`
    keyword.
  prefs: []
  type: TYPE_NORMAL
- en: To fetch the dataset back from the database, we use the `fetch-dataset` function,
    as shown in the following code. The value returned by this function can be directly
    used by the `with-data` function to query and view the dataset fetched.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also inspect the database after storing our dataset, using the `mongo`
    client, as shown in the following code. As we mentioned our database name is `sampledb`,
    we must select this database using the `use` command, as shown in the following
    terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can view all collections in the database using the `show collections` command.
    The queries can be executed using the `find()` function on the appropriate property
    in the variable `db` instance, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To conclude, the Incanter library provides us with a sufficient set of tools
    for querying and storing datasets. Also, MongoDB can be easily used to store datasets
    via the Incanter library.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the modern day of web-based and cloud services, it is also possible to persist
    both datasets and machine learning models to online cloud storage. This is a great
    solution when dealing with enormous amounts of data, since cloud solutions take
    care of both the storage and processing of huge amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: '**BigML** ([http://bigml.com/](http://bigml.com/)) is a cloud provider for
    machine learning resources. BigML internally uses **Classification and Regression
    Trees** (**CARTs**), which are a specialization of decision trees (for more information,
    refer to *Top-down induction of decision trees classifiers-a survey*), as a machine
    learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: BigML provides developers with a simple REST API that can be used to work with
    the service from any language or platform that can send HTTP requests. The service
    supports several file formats such as **CSV** (**comma-separated values**), Excel
    spreadsheet, and the Weka library's ARFF format, and also supports a variety of
    data compression formats such as TAR and GZIP. This service also takes a white-box
    approach, in the sense that models can be downloaded for local use, apart from
    the use of the models for predictions through the online web interface.
  prefs: []
  type: TYPE_NORMAL
- en: There are bindings for BigML in several languages, and we will demonstrate a
    Clojure client library for BigML in this section. Like other cloud services, users
    and developers of BigML must first register for an account. They can then use
    this account and a provided API key to access BigML from a client library. A new
    BigML account provides a few example datasets to experiment with, including the
    Iris dataset that we've frequently encountered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard of a BigML account provides a simple web-based user interface
    for all the resources available to the account.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning in the cloud](img/4351OS_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: BigML resources include sources, datasets, models, predictions, and evaluations.
    We will discuss each of these resources in the upcoming code example.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The BigML Clojure library can be added to a Leiningen project by adding the
    following dependency to the `project.clj` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Firstly, we will have to provide authentication details for the BigML service.
    This is done using the `make-connection` function from the `bigml.api` namespace.
    We must provide a username, an API key, and a flag indicating whether we are using
    development or production datasets to the `make-connection` function, as shown
    in the following code. Note that this username and API key will be shown on your
    BigML account page.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the connection `default-connection` defined in the previous code, we
    must use the `with-connection` function. We can avoid repeating the use of the
    `with-connection` function with the `default-connection` variable by use of a
    simple macro, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In effect, using `with-default-connection` is as good as using the `with-connection`
    function with the `default-connection` binding, thus helping us avoid repeating
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'BigML has the notion of sources to represent resources that can be converted
    to training data. BigML supports local files, remote files, and inline code resources
    as sources, and also supports multiple data types. To create a resource, we can
    use the `create` function from the `bigml.source` namespace, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we define a source using some inline data. The data is
    actually a set of features of various car models, such as their year of manufacture
    and total weight. The last feature is the mileage or MPG of the car model. By
    convention, BigML sources treat the last column as the output or objective variable
    of the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: We must now convert the source to a BigML dataset, which is a structured and
    indexed representation of the raw data from a source. Each feature in the data
    is assigned a unique integer ID in a dataset. This dataset can then be used to
    train a machine learning CART model, which is simply termed as a model in BigML
    jargon. We can create a dataset and a model using the `dataset/create` and `model/create`
    functions, respectively, as shown in the following code. Also, we will have to
    use the `api/get-final` function to finalize a resource that has been sent to
    the BigML cloud service for processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'BigML also provides an interactive visualization of a trained CART model. For
    our training data, the following visualization is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning in the cloud](img/4351OS_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now use the trained model to predict the value of the output variable.
    Each prediction is stored in the BigML cloud service, and is shown in the **Predictions**
    tab of the dashboard. This is done using the `create` function from the `bigml.prediction`
    namespace, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we attempt to predict the MPG (miles per gallon, a measure
    of mileage) of a car model by providing values for the year of manufacture and
    the weight of the car to the `prediction/create` function. The value returned
    by this function is a map, which contains a key `:prediction` among other things,
    that represents the predicted value of the output variable. The value of this
    key is another map that contains column IDs as keys and their predicted values
    as values in the map, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The MPG column, which has the ID `000004`, is predicted to have a value of
    `33` from the trained model, as shown in the previous code. The `prediction/create`
    function creates an online, or remote, prediction, and sends data to the BigML
    service whenever it is called. Alternatively, we can download a function from
    the BigML service that we can use to perform predictions locally using the `prediction/predictor`
    function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the downloaded function, `default-local-predictor`, to perform
    local predictions, as shown in the following REPL output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the previous code, the local prediction function predicts the MPG
    of a car manufactured in `1983` as `22.4`. We can also pass the `:details` keyword
    argument to the `default-local-predictor` function to provide more information
    about the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: BigML also allows us to evaluate trained CART models. We will now train a model
    using the Iris dataset and then cross-validate it. The `evaluation/create` function
    from the BigML library will create an evaluation using a trained model and some
    cross-validation data. This function returns a map that contains all cross-validation
    information about the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous code snippets, we used the `api/get-final` function in almost
    all stages of training a model. In the following code example, we will attempt
    to avoid repeated use of this function by using a macro. We first define a function
    to apply the `api/get-final` and `with-default-connection` functions to an arbitrary
    function that takes any number of arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `final-with-default-connection` function defined in the previous
    code, we can define a macro that will map it to a list of values, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get-final->` macro defined in the previous code basically uses the `->>`
    threading macro to pass the value in the `head` argument through the functions
    in the `body` argument. Also, the previous macro interleaves application of the
    `final-with-default-connection` function to finalize the values returned by functions
    in the `body` argument. We can now use the `get-final->` macro to create a source,
    dataset, and model in a single expression, and then evaluate the model using the
    `evaluation/create` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code snippet, we use a remote file that contains the Iris sample
    data as a source, and pass it to the `source/create`, `dataset/create`, and `model/create`
    functions in sequence using the `get-final->` macro we previously defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formulated model is then evaluated using a composition of the `api/get-final`
    and `evaluation/create` functions, and the result is stored in the variable `iris-evaluation`.
    Note that we use the training data itself to cross-validate the model, which doesn''t
    really achieve anything useful. In practice, however, we should use unseen data
    to evaluate a trained machine learning model. Obviously, as we use the training
    data to cross-validate the model, the accuracy of the model is found to be a 100
    percent or 1, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The BigML dashboard will also provide a visualization (as shown in the following
    diagram) of the model formulated from the data in the previous example. This illustration
    depicts the CART decision tree that was formulated from the Iris sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning in the cloud](img/4351OS_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To conclude, the BigML cloud service provides us with several flexible options
    to estimate CARTs from large datasets in a scalable and platform-independent manner.
    BigML is just one of the many machine learning services available online, and
    the reader is encouraged to explore other cloud service providers of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored a few useful techniques to deal with huge amounts
    of sample data. We also described how we can use machine learning models through
    online services such as BigML, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We described MapReduce and how it is used to process large volumes of data using
    parallel and distributed computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explored how we can query and persist datasets using the Incanter library
    and MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We briefly studied the BigML cloud service provider and how we can use this
    service to formulate and evaluate CARTs from sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, we described several techniques and tools that can be used to
    implement machine learning systems in this book. Clojure helps us build these
    systems in a simple and scalable manner by leveraging the power of the JVM and
    equally powerful libraries. We also studied how we can evaluate and improve machine
    learning systems. Programmers and architects can use these tools and techniques
    to model and learn from their users' data, as well as build machine learning systems
    that provide users with a better experience.
  prefs: []
  type: TYPE_NORMAL
- en: You can explore the academia and research in machine learning through the various
    citations and references that have been used in this book. New academic papers
    and articles on machine learning provide even more insight into the cutting-edge
    of machine learning, and you are encouraged to find and explore them.
  prefs: []
  type: TYPE_NORMAL
