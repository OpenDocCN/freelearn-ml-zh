<html><head></head><body>
		<div id="_idContainer076">
			<h1 class="chapter-number" id="_idParaDest-34"><a id="_idTextAnchor075"/>2</h1>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor076"/>Data Processing</h1>
			<p>One of the essential things in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) is the data that we use for training. We can gather training <a id="_idIndexMarker129"/>data from the processes we work with, or we can take already prepared training data from third-party sources. In any case, we have to store training data in a file format that should satisfy our development requirements. These requirements depend on the task we solve, as well as the data-gathering process. Sometimes, we need to transform data stored in one format to another to satisfy our needs. Examples of such needs are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Increasing human readability to ease communication <span class="No-Break">with engineers</span></li>
				<li>The existence of compression possibility to allow data to occupy less space on <span class="No-Break">secondary storage</span></li>
				<li>The use of data in the binary form to speed up the <span class="No-Break">parsing process</span></li>
				<li>Supporting complex relations between different parts of data to make precise mirroring of a <span class="No-Break">specific domain</span></li>
				<li>Platform independence to be able to use the dataset in different development and <span class="No-Break">production environments</span></li>
			</ul>
			<p>Today, there exists a variety of file formats that are used for storing different kinds of information. Some of these are very specific, and some of them are general-purpose. There are software libraries that allow us to manipulate these file formats. There is rarely a need to develop a new format and parser from scratch. Using existing software for reading a format can significantly reduce development and testing time, which allows us to focus on <span class="No-Break">particular tasks.</span></p>
			<p>This chapter discusses<a id="_idIndexMarker130"/> how to process popular file formats that we use for storing <a id="_idIndexMarker131"/>data. It shows what libraries exist for <a id="_idIndexMarker132"/>working with <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>), <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>), and <strong class="bold">Hierarchical Data Format v5</strong> (<strong class="bold">HDF5</strong>) formats. This chapter also introduces the basic operations required to load and process image data with the <strong class="source-inline">OpenCV</strong> and <strong class="source-inline">Dlib</strong> libraries and how to convert the data format used in these libraries to data types used in linear algebra libraries. It also describes data normalization techniques such as feature scaling and standardization procedures to deal with <span class="No-Break">heterogeneous data.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Parsing data formats to C++ <span class="No-Break">data structures</span></li>
				<li>Initializing matrix and tensor objects from C++ <span class="No-Break">data structures</span></li>
				<li>Manipulating images with the <strong class="source-inline">OpenCV</strong> and <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> libraries</span></li>
				<li>Transforming images into matrix and tensor objects of <span class="No-Break">various libraries</span></li>
				<li><span class="No-Break">Normalizing <a id="_idTextAnchor077"/>d<a id="_idTextAnchor078"/>ata</span></li>
			</ul>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor079"/>Technical requirements</h1>
			<p>The required technologies and installations for this chapter are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Modern C++ compiler with <span class="No-Break">C++17/C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.22</span></li>
				<li><strong class="source-inline">Dlib</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">mlpack</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">Flashlight</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">Eigen</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">hdf5lib</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">HighFive</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">nlohmann-json</strong> <span class="No-Break">library installation</span></li>
				<li><strong class="source-inline">Fast-CPP-CSV-Parser</strong> <span class="No-Break">library installation</span></li>
			</ul>
			<p>The code for this chapter can be found at the following GitHub <span class="No-Break">repo: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edi<span id="_idTextAnchor080"/>t<span id="_idTextAnchor081"/>ion</span></a></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor082"/>Parsing data formats to C++ data structures</h1>
			<p>The most popular format for representing<a id="_idIndexMarker133"/> structured data is called <strong class="bold">CSV</strong>. This format is just a text file with a<a id="_idIndexMarker134"/> two-dimensional table in it whereby<a id="_idIndexMarker135"/> values in a row are separated with commas, and rows are placed on every new line. It looks <span class="No-Break">like this:</span></p>
			<pre class="source-code">
1, 2, 3, 4
5, 6, 7, 8
9, 10, 11, 12</pre>			<p>The advantages of this file format are that it has a straightforward structure, many software tools can process it, it is human-readable, and it is supported on a variety of computer platforms. Disadvantages are a lack of support for multidimensional data and data with complex structuring, as well as slow parsing speed in comparison with <span class="No-Break">binary formats.</span></p>
			<p>Another widely used format is <strong class="bold">JSON</strong>. Although the<a id="_idIndexMarker136"/> format contains JavaScript in its abbreviation, we can use it with almost all programming languages. This is a file format with name-value pairs and arrays of such pairs. It has rules on how to group such pairs into distinct objects and array declarations, and there are rules on how to define values of different types. The following code sample shows a file in <span class="No-Break">JSON format:</span></p>
			<pre class="source-code">
{
  "name": "Bill",
  "age": 25,
  "phones": [
    {
      "type": "home",
      "number": 43534590
    },
    {
      "type": "work",
      "number": 56985468
    }
  ]
}</pre>			<p>The advantages of this format are human readability, software support on many computer platforms, and the possibility to store hierarchical and nested data structures. Disadvantages are its slow parsing speed in comparison with binary formats and the fact it is not very useful for representing numerical matrices. In terms of character reading, binary formats offer more direct access to the underlying data structure, allowing for faster and more precise character<a id="_idIndexMarker137"/> extraction. With text formats, additional steps may be required to convert the<a id="_idIndexMarker138"/> characters into their numerical representations, potentially introducing additional <span class="No-Break">processing overhead.</span></p>
			<p>Often, we use a combination of file formats to represent a complex dataset. For example, we can describe object relations with JSON, and data/numerical data in the binary form can be stored in a folder structure on the filesystem with references to it in <span class="No-Break">JSON files.</span></p>
			<p><strong class="bold">HDF5</strong> is a specialized file format<a id="_idIndexMarker139"/> for storing scientific data. This file format was developed to store heterogeneous multidimensional data with a complex structure. It provides fast access to single elements because it has optimized data structures for using secondary storage. Furthermore, HDF5 supports data compression. In general, this file format consists of named groups that contain multidimensional arrays of multitype data. Each element of this file format can contain metadata, as illustrated in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer063">
					<img alt="Figure 2.1 – HDF5 format structure" src="image/B19849_02_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – HDF5 format structure</p>
			<p>The advantages of this format are its high read-and-write speed, fast access to distinct elements, and its ability to support data with a complex structure and various types of data. Disadvantages are the <a id="_idIndexMarker140"/>requirement of specialized tools for editing and viewing by users, the limited<a id="_idIndexMarker141"/> support of type conversions among different platforms, and using a single file for the whole dataset. The last issue makes data restoration almost impossible in the event of file corruption. So, it makes sense to regularly back up your data to prevent data loss in case of hardware failure or <span class="No-Break">accidental deletion.</span></p>
			<p>There are a lot of other formats for representing datasets for ML, but we found the ones mentioned to be the <a id="_idTextAnchor083"/><span class="No-Break">m<a id="_idTextAnchor084"/>ost useful.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor085"/>Reading CSV files with the Fast-CPP-CSV-Parser library</h2>
			<p>Consider how to deal with CSV format in <a id="_idIndexMarker142"/>C++. There are many different libraries for parsing CSV format with C++. They have different sets of functions and different ways to integrate <a id="_idIndexMarker143"/>them into applications. The easiest way to use C++ libraries is to use<a id="_idIndexMarker144"/> header-only libraries because this eliminates the need to build and link them. We propose to use the <strong class="source-inline">Fast-CPP-CSV-Parser</strong> library because it is a small single-file header-only library with the minimal required functionality, which can be easily integrated into a development code base. It also provides a fast and efficient way to read and write <span class="No-Break">CSV data.</span></p>
			<p>As an example of a CSV file format, we use the <strong class="source-inline">Iris</strong> dataset, which describes three different types of iris plants (<em class="italic">Iris setosa</em>, <em class="italic">Iris versicolor</em>, and <em class="italic">Iris virginica</em>) and was conceived by R.A. Fisher. Each row in the file contains the following fields: sepal length, sepal width, petal<a id="_idIndexMarker145"/> length, petal width, and a string with a class name. This dataset is used for examples of how to<a id="_idIndexMarker146"/> classify an unknown iris flower based on these <span class="No-Break">four features.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The reference to the Iris dataset is the following: <em class="italic">Dua, D.</em> and <em class="italic">Graff, C.</em> (<em class="italic">2019</em>). <em class="italic">UCI Machine Learning Repository</em> [<a href="https://archive.ics.uci.edu/static/public/53/iris.zip">https://archive.ics.uci.edu/static/public/53/iris.zip</a>]. <em class="italic">Irvine, CA: University of California, School of Information and </em><span class="No-Break"><em class="italic">Computer Science</em></span><span class="No-Break">.</span></p>
			<p>To read this dataset with the <strong class="source-inline">Fast-CPP-CSV-Parser</strong> library, we need to include a single header file, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#include &lt;csv.h&gt;</pre>			<p>Then, we define an object of the type <strong class="source-inline">io::CSVReader</strong>. We must define the number of columns as a<a id="_idIndexMarker147"/> template parameter. This parameter is one of the library limitations because we need to be aware of the CSV file structure. The code for this is illustrated in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
const uint32_t columns_num = 5;
io::CSVReader&lt;columns_num&gt; csv_reader(file_path);</pre>			<p>Next, we define containers for storing the values we read, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;std::string&gt; categorical_column;
std::vector&lt;double&gt; values;</pre>			<p>Then, to make our code more generic and gather all information about column types in one place, we introduce the following helper types and functions. We define a tuple object that describes values for a row, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
using RowType = 
  std::tuple&lt;double, double, double, double, std::string&gt;;
RowType row;</pre>			<p>The reason for using a tuple is that we can easily iterate it with metaprogramming techniques. Then, we define two helper functions. One is for reading a row from a file, and it uses the <strong class="source-inline">read_row()</strong> method of the <strong class="source-inline">io::CSVReader</strong> class. The <strong class="source-inline">read_row()</strong> method takes a<a id="_idIndexMarker148"/> variable number of parameters of different types. Our <strong class="source-inline">RowType</strong> type describes these <a id="_idIndexMarker149"/>values. We do automatic parameter filling by using the <strong class="source-inline">std::index_sequence</strong> type with the <strong class="source-inline">std::get</strong> function, as illustrated in the <a id="_idIndexMarker150"/>following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
template &lt;std::size_t... Idx, typename T, typename R&gt;
bool read_row_help(std::index_sequence&lt;Idx...&gt;, T&amp; row, R&amp; r) {
  return r.read_row(std::get&lt;Idx&gt;(row)...);
}</pre>			<p>The second helper function uses a similar technique for transforming a row tuple object to our value vectors, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
template &lt;std::size_t... Idx, typename T&gt;
void fill_values(std::index_sequence&lt;Idx...&gt;,
                 T&amp; row,
                 std::vector&lt;double&gt;&amp; data) {
    data.insert(data.end(), {std::get&lt;Idx&gt;(row)...});
}</pre>			<p>Now, we can put all the parts together. We define a loop where we continuously read row values and move them to our containers. After we read a row, we check the return value of the <strong class="source-inline">read_row()</strong> method, which tells us if the read was successful or not. A <strong class="source-inline">false</strong> return value means that we have reached the end of the file. In the case of a parsing error, we catch an exception from the <strong class="source-inline">io::error</strong> namespace. There are exception types for different parsing failures. In the following example, we handle number <span class="No-Break">parsing errors:</span></p>
			<pre class="source-code">
try {
  bool done = false;
  while (!done) {
    done = !read_row_help(
      std::make_index_sequence&lt;
        std::tuple_size&lt;RowType&gt;::value&gt;{},
      row, csv_reader);
    if (!done) {
      categorical_column.push_back(std::get&lt;4&gt;(row));
      fill_values(
        std::make_index_sequence&lt;columns_num - 1&gt;{},
        row, values);
    }
  }
}
} catch (const io::error::no_digit&amp; err) {
    // Ignore badly formatted samples
    std::cerr &lt;&lt; err.what() &lt;&lt; std::endl;
}</pre>			<p>Also, notice that we moved <a id="_idIndexMarker151"/>only four values to our vector of doubles because the last column contains string objects that we put into another vector of <span class="No-Break">categorical</span><span class="No-Break"><a id="_idIndexMarker152"/></span><span class="No-Break"> values.</span></p>
			<p>In this code<a id="_idIndexMarker153"/> sample, we saw how to parse the particular dataset with string and numerical values into two containers: <strong class="source-inline">std::vector&lt;std::string&gt; categorical_column</strong> and <span class="No-Break"><strong class="source-inline">std<a id="_idTextAnchor086"/>::<a id="_idTextAnchor087"/>vector&lt;double&gt; values</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor088"/>Preprocessing CSV files</h2>
			<p>Sometimes, the data <a id="_idIndexMarker154"/>we have comes in a format that’s incompatible with the libraries we want to use. For example, the Iris dataset file contains a column that contains strings. Many ML libraries cannot read <a id="_idIndexMarker155"/>such values because they assume that CSV files contain only numerical values that can be directly loaded into an internal <span class="No-Break">matrix representation.</span></p>
			<p>So, before using such datasets, we need to preprocess them. In the case of the Iris dataset, we need to replace the <strong class="source-inline">categorical</strong> column containing string labels with numeric encoding. In the following code sample, we replace strings with distinct numbers, but in general, such an approach is a bad idea, especially for classification tasks. ML algorithms usually learn only numerical relations, so a more suitable approach would be to use specialized encoding—for example, one-hot encoding. One-hot encoding is a method used in ML to represent categorical data as numerical values. It involves creating a binary vector for each unique value in the categorical feature, where only one element in the vector is set to <strong class="source-inline">1</strong> and all others are set to <strong class="source-inline">0</strong>. The code can be seen in the <span class="No-Break">following block:</span></p>
			<pre class="source-code">
#include &lt;fstream&gt;
#include &lt;regex&gt;
...
std::ifstream data_stream("iris.data");
std::string data_string(
    (std::istreambuf_iterator&lt;char&gt;(data_stream)),
    std::istreambuf_iterator&lt;char&gt;()
);
data_string = std::regex_replace(data_string,
                                 std::regex("Irissetosa"),
                                 "1");
data_string = std::regex_replace(data_string,
                                 std::regex("Irisversicolor"),
                                 "2");
data_string = std::regex_replace(data_string,
                                 std::regex("Irisvirginica"),
                                 "3");
std::ofstream out_stream("iris_fix.csv");
out_stream &lt;&lt; data_string;</pre>			<p>We read the CSV file content to the <strong class="source-inline">std::string</strong> object with the <strong class="source-inline">std::ifstream</strong> object. Also, we use <strong class="source-inline">std::regex</strong> routines to replace string class names with numbers. Using <strong class="source-inline">regex</strong> functions allows us to<a id="_idIndexMarker156"/> reduce code size and make it more expressive in comparison with the loop approach, which<a id="_idIndexMarker157"/> typically uses the <strong class="source-inline">std::string::find()</strong> and <strong class="source-inline">std::string::replace()</strong> methods. After replacing all categorical class names in the file, we create a new file wi<a id="_idTextAnchor089"/>t<a id="_idTextAnchor090"/>h the <span class="No-Break"><strong class="source-inline">std::ofstream</strong></span><span class="No-Break"> object.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor091"/>Reading CSV files with the mlpack library</h2>
			<p>Many ML frameworks already have<a id="_idIndexMarker158"/> routines for reading the CSV file format to their internal representations. In the following code sample, we show how to load a CSV file with the <strong class="source-inline">mlpack</strong> library into the <strong class="source-inline">matrix</strong> object. The CSV parser in this library can automatically create numerical <a id="_idIndexMarker159"/>mappings for <a id="_idIndexMarker160"/>non-numeric values, so we easily load the Iris dataset without <span class="No-Break">additional preprocessing.</span></p>
			<p>To read a CSV file with the <strong class="source-inline">mlpack</strong> library, we have to include the corresponding headers, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#include &lt;mlpack/core.hpp&gt;
using namespace mlpack;</pre>			<p>We can use the <strong class="source-inline">data::Load</strong> function to load the CSV data from a file, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
arma::mat dataset;
data::DatasetInfo info;
data::Load(file_name,
           dataset,
           info,
           /*fail with error*/ true);</pre>			<p>Notice that the <strong class="source-inline">data::Load</strong> function takes the <strong class="source-inline">dataset</strong> matrix object to load data and the <strong class="source-inline">info</strong> object of the <strong class="source-inline">DatasetInfo</strong> type that can be used to get additional information about the loaded file. Also, the<a id="_idIndexMarker161"/> last Boolean <strong class="source-inline">true</strong> parameter was used to make the function throw an<a id="_idIndexMarker162"/> exception in the loading error case. For example, we can get the number of columns<a id="_idIndexMarker163"/> and an available mapping for non-numeric values, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::cout &lt;&lt; "Number of dimensions: " &lt;&lt; info.Dimensionality() 
          &lt;&lt; std::endl;
std::cout &lt;&lt; "Number of classes: " &lt;&lt; <a id="_idTextAnchor092"/>info.NumMappings(4) 
          &lt;&lt; std::endl;</pre>			<p>Due to the fact that data is loaded as is, there are no automatic assumptions about dataset structure. So, to extract labels, we need to manually divide the loaded matrix object <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
arma::Row&lt;size_t&gt; labels;
labels = arma::conv_to&lt;arma::Row&lt;size_t&gt;&gt;::from(
    dataset.row(dataset.n_rows - 1));
dataset.shed_row(dataset.n_rows – 1);</pre>			<p>We used the <strong class="source-inline">arma::conv_to</strong> function to create the standalone <strong class="source-inline">arma::Row</strong> object from the dataset row. Then, we deleted the last row from the <a id="_idTextAnchor093"/>dataset with the <span class="No-Break"><strong class="source-inline">shed_row</strong></span><span class="No-Break"> method.</span></p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor094"/>Reading CSV files with the Dlib library</h2>
			<p>The <strong class="source-inline">Dlib</strong> library can load CSV<a id="_idIndexMarker164"/> files directly to its matrix type, as the <strong class="source-inline">mlpack</strong> library does. For this operation, we<a id="_idIndexMarker165"/> can use a simple C++ streaming operator and a standard <span class="No-Break"><strong class="source-inline">std::ifstream</strong></span><span class="No-Break"> object.</span></p>
			<p>As a first step, we make the <a id="_idIndexMarker166"/>necessary <strong class="source-inline">include</strong> statements, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
include &lt;Dlib/matrix.h&gt;
using namespace Dlib;</pre>			<p>Then, we define a <strong class="source-inline">matrix</strong> object and load data from the file, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
matrix&lt;double&gt; data;
std::ifstream file("iris_fix.csv");
file &gt;&gt; data;
std::cout &lt;&lt; data &lt;&lt; std::endl;</pre>			<p>In the <strong class="source-inline">Dlib</strong> library, <strong class="source-inline">matrix</strong> objects are<a id="_idIndexMarker167"/> used for training ML algorithms directly without the need to transform<a id="_idTextAnchor095"/> <a id="_idTextAnchor096"/>them into intermediate <span class="No-Break">dataset types.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor097"/>Reading JSON files with the nlohmann-json library</h2>
			<p>Some datasets come with<a id="_idIndexMarker168"/> structured annotations and can contain multiple files and folders. An example of such a complex dataset<a id="_idIndexMarker169"/> is the <strong class="bold">Common Objects in Context</strong> (<strong class="bold">COCO</strong>) dataset. This dataset<a id="_idIndexMarker170"/> contains a text file with annotations for describing relations between objects and their structural parts. This widely known dataset is used to train models for<a id="_idIndexMarker171"/> segmentation, object detection, and classification tasks. Annotations in this dataset are defined in the JSON file format. JSON is a widely used file format for objects’ (<span class="No-Break">entities’) representations.</span></p>
			<p>It is just a text file with special notations for describing relations between objects and their parts. In the following code samples, we show how to work with this file format using the <strong class="source-inline">nlohmann-json</strong> library. This library provides a simple and intuitive interface for working with JSON, making it <a id="_idIndexMarker172"/>easy to convert between JSON strings and C++ data structures such as maps, vectors, and custom classes. It also supports various features such as automatic<a id="_idIndexMarker173"/> type conversion, pretty printing, and error handling. However, we are going to use a more straightforward dataset that defines paper reviews. The authors of this dataset are Keith, B., Fuentes, E., and Meneses, C., and they made this dataset for their work titled <em class="italic">A Hybrid Approach for Sentiment Analysis Applied to Paper </em><span class="No-Break"><em class="italic">Reviews</em></span><span class="No-Break"> (2017).</span></p>
			<p>The following sample<a id="_idIndexMarker174"/> shows a reduced part of this <span class="No-Break">JSON-based dataset:</span></p>
			<pre class="source-code">
{
  "paper": [
    {
      "id": 1,
      "preliminary_decision": "accept",
      "review": [
        {
          "confidence": "4",
          "evaluation": "1",
          "id": 1,
          "lan": "es",
          "orientation": "0",
          "remarks": "",
          "text" : "- El artículo aborda un problema contingente\n 
            y muy relevante, e incluye tanto un diagnóstico\n 
            nacional de uso de buenas prácticas como una solución\n
            (buenas prácticas concretas)... ",
          "timespan": "2010-07-05"
        },
        {
          "confidence": "4",
          "evaluation": "1",
          "id": 2,
          "lan": "es",
          "orientation": "1",
          "remarks": "",
          "text" : "El artículo presenta recomendaciones\n 
         prácticas para el desarrollo de software seguro... ",
          "timespan": "2010-07-05"
        },
        {
          "confidence": "5",
          "evaluation": "1",
          "id": 3,
          "lan": "es",
          "orientation": "1",
          "remarks": "",
          "text" : "- El tema es muy interesante y puede ser de\n 
            mucha ayuda una guía para incorporar prácticas de\n 
            seguridad... ",
          "timespan": "2010-07-05"
        }
      ]
    }
  ]
}</pre>			<p>There are two main <a id="_idIndexMarker175"/>approaches to parsing and processing JSON files, which are listed <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The first approach <a id="_idIndexMarker176"/>assumes the parsing of whole <a id="_idIndexMarker177"/>files at once and creating a <strong class="bold">Document Object Model</strong> (<strong class="bold">DOM</strong>). The DOM is a<a id="_idIndexMarker178"/> hierarchical structure of objects that represents entities stored in files. It is usually stored in computer memory, and, in the case of large files, it can occupy a significant amount <span class="No-Break">of memory.</span></li>
				<li>Another approach is to parse the file continuously and provide an <strong class="bold">application program interface</strong> (<strong class="bold">API</strong>) for a user to <a id="_idIndexMarker179"/>handle and process each event related to the file-parsing process. This second approach is usually<a id="_idIndexMarker180"/> called <strong class="bold">Simple API for XML</strong> (<strong class="bold">SAX</strong>). Despite its name, it’s a general approach that is used with non-XML data too. SAX is faster than DOM for parsing large XML files because it doesn’t build a complete tree representation of the entire document in memory. However, it can be more difficult to use for complex operations that require accessing specific parts of <span class="No-Break">the document.</span></li>
			</ul>
			<p>Using a DOM for working with<a id="_idIndexMarker181"/> training datasets usually requires a lot of memory for structures that are useless for ML algorithms. So, in many cases, it is preferable to use the SAX interface. It allows us to filter irrelevant data and initialize structures that we can use directly in our algorithms. In the following code sample, we use <span class="No-Break">this approach.</span></p>
			<p>As a preliminary step, we define types for <strong class="source-inline">paper</strong>/<strong class="source-inline">review</strong> entities, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
...
struct Paper {
  uint32_t id{0};
  std::string preliminary_decision;
  std::vector&lt;Review&gt; reviews;
};
using Papers = std::vector&lt;Paper&gt;;
...
struct Review {
  std::string confidence;
  std::string evaluation;
  uint32_t id{0};
  std::string language;
  std::string orientation;
  std::string remarks;
  std::string text;
  std::string timespan;
};</pre>			<p>Then, we declare a type for<a id="_idIndexMarker182"/> the object, which will be used by the parser <a id="_idIndexMarker183"/>to handle parsing events. This type should be inherited from the <strong class="source-inline">nlohmann::json::json_sax_t</strong> base class, and we need to override virtual handler functions that the<a id="_idIndexMarker184"/> parser will call when a particular parsing event occurs, as illustrated in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
#include &lt;nlohmann/json.hpp&gt;
using json = nlohmann::json;
...
struct ReviewsHandler
: public json::json_sax_t {
  ReviewsHandler(Papers* papers) : papers_(papers) {}
  bool null() override;
  bool boolean(bool) override;
  bool number_integer(number_integer_t) override;
  bool number_float(number_float_t, const string_t&amp;) override;
  bool binary(json::binary_t&amp;) override;
  bool parse_error(std::size_t, const std::string&amp;, 
                                const json::exception&amp; ex) override;
  bool number_unsigned(number_unsigned_t u) override;
  bool string(string_t&amp; str) override ;
  bool key(string_t&amp; str) override;
  bool start_object(std::size_t) override;
  bool end_object() override;
  bool start_array(std::size_t)override;
  bool end_array() override;
  Paper paper_;
  Review review_;
  std::string key_;
  Papers* papers_{nullptr};
  HandlerState state_{HandlerState::None};
};</pre>			<p>We have to override<a id="_idIndexMarker185"/> all methods, but we can provide real handler implementations only for objects, arrays parsing events, and events for parsing unsigned <strong class="source-inline">int</strong>/<strong class="source-inline">string</strong> values. Other methods can have trivial implementations <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
bool number_float(number_float_t, const string_t&amp;) override {
  return true;
}</pre>			<p>Now, we can<a id="_idIndexMarker186"/> use the <strong class="source-inline">nlohmann::json::sax_parse</strong> method to load a JSON file; this method takes the <strong class="source-inline">std::istream</strong> object and a <strong class="source-inline">handler</strong> object as the second argument. The following code<a id="_idIndexMarker187"/> block shows how to <span class="No-Break">use it:</span></p>
			<pre class="source-code">
std::ifstream file(filename);
if (file) {
  // define papers data container to be filled
  Papers papers;
  // define object with SAX handlers
  ReviewsHandler handler(&amp;papers);
  // parse file
  bool result = json::sax_parse(file, &amp;handler);
  // check parsing result
  if (!result) {
    throw std::runtime_error(handler.error_);
  }
  return papers;
} else {
  throw std::invalid_argument("File can't be opened " + filename);
}</pre>			<p>When there are no parsing errors, we will have an initialized array of <strong class="source-inline">Paper</strong> type objects. Consider, more <a id="_idIndexMarker188"/>precisely, the event handler’s implementation details. Our event handler works <a id="_idIndexMarker189"/>as a state machine. In one state, we populate it with the <strong class="source-inline">Review</strong> objects, and in another one, with the <strong class="source-inline">Paper</strong> objects, and there are states for other events, as<a id="_idIndexMarker190"/> shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
enum class HandlerState {
  None,
  Global,
  PapersArray,
  Paper,
  ReviewArray,
  Review
};</pre>			<p>We parse the unsigned <strong class="source-inline">unit</strong> values only for the <strong class="source-inline">Id</strong> attributes of the <strong class="source-inline">Paper</strong> and the <strong class="source-inline">Review</strong> objects, and <a id="_idIndexMarker191"/>we update these values according to the current state and the previously parsed key, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
bool number_unsigned(number_unsigned_t u) override {
  bool res{true};
  try {
    if (state_ == HandlerState::Paper &amp;&amp; key_ == "id") {
      paper_.id = u;
    } else if (state_ == HandlerState::Review &amp;&amp; key_ == "id") {
      review_.id = u;
    } else {
      res = false;
    }
  } catch (...) {
    res = false;
  }
  key_.clear();
  return res;
}</pre>			<p>String values<a id="_idIndexMarker192"/> also exist in<a id="_idIndexMarker193"/> both types of objects, so we do the same checks to update corresponding <a id="_idIndexMarker194"/>values, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
bool string(string_t&amp; str) override {
  bool res{true};
  try {
    if (state_ == HandlerState::Paper &amp;&amp;
        key_ == "preliminary_decision") {
      paper_.preliminary_decision = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "confidence") {
      review_.confidence = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "evaluation") {
      review_.evaluation = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "lan") {
      review_.language = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "orientation") {
      review_.orientation = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "remarks") {
      review_.remarks = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "text") {
      review_.text = str;
    } else if (state_ == HandlerState::Review &amp;&amp;
               key_ == "timespan") {
      review_.timespan = str;
    } else {
      res = false;
    }
  } catch (...) {
    res = false;
  }
  key_.clear();
  return res;
}</pre>			<p>The event <a id="_idIndexMarker195"/>handler for the JSON <strong class="source-inline">key</strong> attribute <a id="_idIndexMarker196"/>stores the <strong class="source-inline">key</strong> value to the appropriate variable, which we use<a id="_idIndexMarker197"/> to identify a current object in the parsing process, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
bool key(string_t&amp; str) override  {
  key_ = str;
  return true;
}</pre>			<p>The <strong class="source-inline">start_object</strong> event handler switches states according to the current <strong class="source-inline">key</strong> value and the previous <strong class="source-inline">state</strong> value. We base the current implementation on the knowledge of the structure of the current JSON file: there is no array of <strong class="source-inline">Paper</strong> objects, and each <strong class="source-inline">Paper</strong> object includes an<a id="_idIndexMarker198"/> array of reviews. It is one of the limitations of the SAX interface—we need to <a id="_idIndexMarker199"/>know the structure of the document to implement all event handlers correctly. The code can be seen in the <span class="No-Break">following block:</span></p>
			<pre class="source-code">
bool start_object(std::size_t) override {
  if (state_ == HandlerState::None &amp;&amp; key_.empty()) {
      state_ = HandlerState::Global;
  } else if (state_ == HandlerState::PapersArray &amp;&amp; key_.empty()) {
      state_ = HandlerState::Paper;
  } else if (state_ == HandlerState::ReviewArray &amp;&amp; key_.empty()) {
      state_ = HandlerState::Review;
  } else {
      return false;
  }
  return true;
}</pre>			<p>In the <strong class="source-inline">end_object</strong> event handler, we populate arrays of <strong class="source-inline">Paper</strong> and <strong class="source-inline">Review</strong> objects according to the current<a id="_idIndexMarker200"/> state. Also, we switch the current state back to the previous one by <a id="_idIndexMarker201"/>running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
bool end_object() override {
  if (state_ == HandlerState::Global) {
      state_ = HandlerState::None;
  } else if (state_ == HandlerState::Paper) {
      state_ = HandlerState::PapersArray;
      papers_-&gt;push_back(paper_);
      paper_ = Paper();
  } else if (state_ == HandlerState::Review) {
      state_ = HandlerState::ReviewArray;
      paper_.reviews.push_back(review_);
  } else {
      return false;
  }
  return true;
}</pre>			<p>In the <strong class="source-inline">start_array</strong> event <a id="_idIndexMarker202"/>handler, we switch the current state to a new one according to the current <strong class="source-inline">state</strong> value by <a id="_idIndexMarker203"/>running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
bool start_array(std::size_t) override {
  if (state_ == HandlerState::Global &amp;&amp; key_ == "paper") {
      state_ = HandlerState::PapersArray;
      key_.clear();
  } else if (state_ == HandlerState::Paper &amp;&amp; key_ == "review") {
      state_ = HandlerState::ReviewArray;
      key_.clear();
  } else {
      return false;
  }
  return true;
}</pre>			<p>In the <strong class="source-inline">end_array</strong> event handler, we<a id="_idIndexMarker204"/> switch the current state to the previous one based on our knowledge of <a id="_idIndexMarker205"/>the document structure by running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
bool end_array() override {
  if (state_ == HandlerState::ReviewArray) {
      state_ = HandlerState::Paper;
  } else if (state_ == HandlerState::PapersArray) {
      state_ = HandlerState::Global;
  } else {
      return false;
  }
  return true;
}</pre>			<p>The vital thing in this approach is to clear the current <strong class="source-inline">key</strong> value after object processing. This helps us to debug parsing errors, and we always have actual information about the currently <span class="No-Break">processed entity.</span></p>
			<p>For small files, using the<a id="_idIndexMarker206"/> DOM app<a id="_idTextAnchor098"/>r<a id="_idTextAnchor099"/>oach can be preferable because it leads to less code and <span class="No-Break">cleaner algorithms.</span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor100"/>Writing and reading HDF5 files with the HighFive library</h2>
			<p>HDF5 is a highly efficient file format for storing<a id="_idIndexMarker207"/> datasets and scientific values. The <strong class="source-inline">HighFive</strong> library provides a higher-level C++ interface for the C library provided by the HDF Group. In this example, we <a id="_idIndexMarker208"/>propose to look at its interface by transforming the dataset used in the previous section to <span class="No-Break">HDF5 format.</span></p>
			<p>The main concepts of the HDF5 format <a id="_idIndexMarker209"/>are groups and datasets. Each <a id="_idIndexMarker210"/>group can contain other groups and have attributes of different types. Also, each group can contain a set of dataset entries. Each dataset is a multidimensional array of values of the same type, which also<a id="_idIndexMarker211"/> can have attributes of<a id="_idIndexMarker212"/> <span class="No-Break">different types.</span></p>
			<p>Let’s start with including the required headers, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#include &lt;highfive/H5DataSet.hpp&gt;
#include &lt;highfive/H5DataSpace.hpp&gt;
#include &lt;highfive/H5File.hpp&gt;</pre>			<p>Then, we have to create a <strong class="source-inline">file</strong> object where we will write our dataset, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
HighFive::File file(file_name, 
  HighFive::File::ReadWrite | HighFive::File::Create | 
  HighFive::File::Truncate);</pre>			<p>After we have a <strong class="source-inline">file</strong> object, we can start creating groups. We define a group of papers that should hold all <strong class="source-inline">paper</strong> objects, <span class="No-Break">as</span><span class="No-Break"><a id="_idIndexMarker213"/></span><span class="No-Break"> follows:</span></p>
			<pre class="source-code">
auto papers_group = file.createGroup("papers");</pre>			<p>Then, we iterate through an array of <a id="_idIndexMarker214"/>papers (as shown in the previous section) and create a group for each <strong class="source-inline">paper</strong> object with two attributes: the numerical <strong class="source-inline">id</strong> attribute <a id="_idIndexMarker215"/>and the <strong class="source-inline">preliminary_decision</strong> attribute of the <strong class="source-inline">string</strong> type, as illustrated in the<a id="_idIndexMarker216"/> following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
for (const auto&amp; paper : papers) {
  auto paper_group = papers_group.createGroup(
    "paper_" + std::to_string(paper.id));
  
  std::vector&lt;uint32_t&gt; id = {paper.id};
  auto id_attr = paper_group.createAttribute&lt;uint32_t&gt;(
    "id", HighFive::DataSpace::From(id));
  id_attr.write(id);
  auto dec_attr = paper_group.createAttribute&lt;std::string&gt;(
    "preliminary_decision",
    HighFive::DataSpace::From(paper.preliminary_decision));
  dec_attr.write(paper.preliminary_decision);
}</pre>			<p>After we have created an<a id="_idIndexMarker217"/> attribute, we have to put in its value with the <strong class="source-inline">write()</strong> method. Notice that the <strong class="source-inline">HighFive::DataSpace::From</strong> function automatically detects the size of the attribute value. The size is the amount of<a id="_idIndexMarker218"/> memory required to hold the attribute’s value. Then, for each <strong class="source-inline">paper_group</strong> object, we create a corresponding group of<a id="_idIndexMarker219"/> reviews, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
 auto reviews_group = paper_group.createGroup("reviews");</pre>			<p>We insert into each <strong class="source-inline">reviews_group</strong> object a dataset of numerical values of <strong class="source-inline">confidence</strong>, <strong class="source-inline">evaluation</strong>, and <strong class="source-inline">orientation</strong> fields. For the dataset, we define <strong class="source-inline">DataSpace</strong> (the number of <a id="_idIndexMarker220"/>elements in the dataset) of size <strong class="source-inline">3</strong> and define a storage type as a 32-bit<a id="_idIndexMarker221"/> integer, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;size_t&gt; dims = {3};
std::vector&lt;int32_t&gt; values(3);
for (const auto&amp; r : paper.reviews) {
     auto dataset = reviews_group.createDataSet&lt;int32_t&gt;(
       std::to_string(r.id), HighFive::DataSpace(dims));
     values[0] = std::stoi(r.confidence);
     values[1] = std::stoi(r.evaluation);
     values[2] = std::stoi(r.orientation);
     dataset.write(values);
 }
}</pre>			<p>After we have created <a id="_idIndexMarker222"/>and initialized all objects, the Papers/Reviews dataset in HDF5 format is ready. When the <strong class="source-inline">file</strong> object leaves the scope, its destructor <a id="_idIndexMarker223"/>saves everything to the <span class="No-Break">secondary storage.</span></p>
			<p>Having the file in the <a id="_idIndexMarker224"/>HDF5 format, we can consider the <strong class="source-inline">HighFive</strong> library interface for <span class="No-Break">file reading.</span></p>
			<p>As the first step, we again create a <strong class="source-inline">HighFive::File</strong> object, but with attributes for reading, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
HighFive::File file(file_name, HighFive::File::ReadOnly);</pre>			<p>Then, we use the <strong class="source-inline">getGroup()</strong> method to get the top-level <strong class="source-inline">papers_group</strong> object, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto papers_group = file.getGroup("papers");</pre>			<p>The <strong class="source-inline">getGroup</strong> method allows us to get a specific group by its name, so it’s a type of navigation through the HDF5 <span class="No-Break">file structure.</span></p>
			<p>We need to get a list of all nested <a id="_idIndexMarker225"/>objects in this group because we can access objects only by their names. We can <a id="_idIndexMarker226"/>do this by running the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
auto papers_names = papers_group.listObjectNames();</pre>			<p>Using a loop, we iterate<a id="_idIndexMarker227"/> over all <strong class="source-inline">papers_group</strong> objects<a id="_idIndexMarker228"/> in the <strong class="source-inline">papers_group</strong> container, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
for (const auto&amp; pname : papers_names) {
    auto paper_group = papers_group.getGroup(pname);
...
}</pre>			<p>For each <strong class="source-inline">paper</strong> object, we<a id="_idIndexMarker229"/> read its attributes and the memory space required for the attribute value. Also, because each attribute can be <a id="_idIndexMarker230"/>multidimensional, we should take care of it and allocate an appropriate container, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;uint32_t&gt; id;
paper_group.getAttribute("id").read(id);
std::cout &lt;&lt; id[0];
std::string decision;
paper_group.getAttribute("preliminary_decision").read(decision);
std::cout &lt;&lt; " " &lt;&lt; decision &lt;&lt; std::endl;</pre>			<p>For reading datasets, we can use the same approach: get the <strong class="source-inline">reviews</strong> group, then get a list of dataset names, and, finally, read <a id="_idIndexMarker231"/>each dataset in a loop, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto reviews_group = paper_group.getGroup("reviews");
auto reviews_names = reviews_group.listObjectNames();
std::vector&lt;int32_t&gt; values(2);
for (const auto&amp; rname : reviews_names) {
    std::cout &lt;&lt; "\t review: " &lt;&lt; rname &lt;&lt; std::endl;
    auto dataset = reviews_group.getDataSet(rname);
    auto selection = dataset.select({1}, {2}); 
  // or use just dataset.read method to get whole data
    selection.read(values);
    std::cout &lt;&lt; "\t\t evaluation: " &lt;&lt; values[0] &lt;&lt; std::endl;
    std::cout &lt;&lt; "\t\t orientation: " &lt;&lt; values[1] &lt;&lt; std::endl;
}</pre>			<p>Notice that we use the <strong class="source-inline">select()</strong> method<a id="_idIndexMarker232"/> for the dataset, which allows<a id="_idIndexMarker233"/> us to read only a part of the dataset. We define this part with ranges given as arguments. There is the <strong class="source-inline">read()</strong> method in the <strong class="source-inline">dataset</strong> type to <a id="_idIndexMarker234"/>read a whole dataset <span class="No-Break">at once.</span></p>
			<p>Using these techniques, we can read and transform any HDF5 dataset. This file format allows us to work only with <a id="_idIndexMarker235"/>part of the required data and not to load the whole file to the memory. Also, because this is a binary format, its reading is more <a id="_idIndexMarker236"/>efficient than reading large text files. Other useful features of HDF5 are <span class="No-Break">the following:</span></p>
			<ul>
				<li>Compression options for datasets and attributes, reducing storage space and <span class="No-Break">transfer time.</span></li>
				<li>Parallelization of I/O operations, enabling multiple threads or processes to access the file simultaneously. This can greatly increase throughput and reduce <span class="No-Break">processing time.</span></li>
			</ul>
			<p>In this section, we saw how to<a id="_idIndexMarker237"/> load different file formats with data into C++ data structures provided by <a id="_idIndexMarker238"/>various C++ librari<a id="_idTextAnchor101"/>es. Especially we learned how to fill matrix and tensor objects that will be <a id="_idIndexMarker239"/>used in different <a id="_idIndexMarker240"/>ML algorithms. In the following section, we will see how to initialize the same data struc<a id="_idTextAnchor102"/>tures<a id="_idIndexMarker241"/> with values from regular C++ containers, which<a id="_idIndexMarker242"/> can be important when you implement your own <span class="No-Break">data loader.</span></p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor103"/>Initializing matrix and tensor objects from C++ data structures</h1>
			<p>There are a variety of file formats used for datasets, and not all of them might be supported by libraries. For <a id="_idIndexMarker243"/>using data from unsupported formats, we might need to write custom parsers. After we read values to regular C++ containers, we usually need to convert them into object types used in t<a id="_idTextAnchor104"/>he ML framework we use. As an e<a id="_idTextAnchor105"/>xample, let’s consider the case of reading matrix data from files into <span class="No-Break">C++ objects.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor106"/>Working with the Eigen library</h2>
			<p>Using the <strong class="source-inline">Eigen</strong> library, we can <a id="_idIndexMarker244"/>wrap a C++ array into an <strong class="source-inline">Eigen::Matrix</strong> object with the <strong class="source-inline">Eigen::Map</strong> type. The wrapped object will behave as a standard <strong class="source-inline">Eigen</strong> matrix. We have to parametrize the <strong class="source-inline">Eigen::Map</strong> type with the type of matrix that has the required behavior. Also, when we create the <strong class="source-inline">Eigen::Map</strong> object, it<a id="_idIndexMarker245"/> takes as arguments a pointer to the C++ array and matrix<a id="_idIndexMarker246"/> dimensions, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::vector&lt;double&gt; values;
...
auto x_data = Eigen::Map&lt;Eigen::Matrix&lt;double,
                                       Eigen::Dynamic,
                                       Eigen::Dynamic,
                                       Eigen::RowMajor&gt;&gt;(
    values.data(),
    rows_num,
    columns_num);</pre>			<h2 id="_idParaDest-46"><a id="_idTextAnchor107"/>Working with the Blaze library</h2>
			<p>The <strong class="source-inline">Blaze</strong> library has special classes that can be used to create wrappers for C++ arrays. To wrap a C++ container <a id="_idIndexMarker247"/>with objects of these classes, we have to pass a pointer to the data and corresponding dimensions as arguments, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::array&lt;int, 4&gt; data = {1, 2, 3, 4};
blaze::CustomVector&lt;int,
                    blaze::unaligned,
                    blaze::unpadded,
                    blaze::rowMajor&gt;
    v2(data.data(), data.size());
std::vector&lt;float&gt; mdata = {1, 2, 3, 4, 5, 6, 7, 8, 9};
blaze::CustomMatrix&lt;float,
                    blaze::unaligned,
                    blaze::unpadded,
                    blaze::rowMajor&gt;
    a2(mdata.data(), 3UL, 3UL);</pre>			<p>Notice that ad<a id="_idTextAnchor108"/>ditional template parameters were used to specify memory layout, alignment, <span class="No-Break">and padding.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor109"/>Working with the Dlib library</h2>
			<p>The <strong class="source-inline">Dlib</strong> library has the <strong class="source-inline">Dlib::mat()</strong> function for wrapping C++ containers into the <strong class="source-inline">Dlib</strong> matrix object. It also takes a pointer to the<a id="_idIndexMarker248"/> data and matrix dimensions as arguments, as illustrated in the follo<a id="_idTextAnchor110"/>wing <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
double data[] = {1, 2, 3, 4, 5, 6};
auto m2 = Dlib::mat(data, 2, 3); // create matrix with size 2x3</pre>			<p>The <strong class="source-inline">Dlib::mat</strong> func<a id="_idTextAnchor111"/>tion has other overloads that can take other types of containers to create <span class="No-Break">a matrix.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor112"/>Working with the ArrayFire library</h2>
			<p>The <strong class="source-inline">ArrayFire</strong> library has a <a id="_idIndexMarker249"/>single technique to initialize the array object with an external memory pointer. It can be used <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
float host_data[] = {0, 1, 2, 3, 4, 5};
array A(2, 3, host_data);</pre>			<p>In this example, we initialize the <strong class="source-inline">2x3</strong> matrix with the data from the <strong class="source-inline">C</strong> array object. We used the <strong class="source-inline">array</strong> type constructor. The first two arguments are matrix row and column numbers, and the last one<a id="_idIndexMarker250"/> is the pointer to the data. We can initialize the <strong class="source-inline">array</strong> type object with th<a id="_idTextAnchor113"/>e CUDA pointer in the same, but the fourth argument should be the <span class="No-Break"><strong class="source-inline">afDevice</strong></span><span class="No-Break"> specification.</span></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor114"/>Working with the mlpack library</h2>
			<p>The <strong class="source-inline">mlpack</strong> framework uses the <strong class="source-inline">Armadillo</strong> library for linear algebra objects. So, to wrap a C++ container into the <strong class="source-inline">arma::mat</strong> object, we can use the corresponding constructor that takes a <a id="_idIndexMarker251"/>pointer to the data and matrix dimensions, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
std::vector&lt;double&gt; values;
...
arma::mat(values.data(), n_rows, n_cols, /*copy_aux_mem*/ false);</pre>			<p>If the fourth parameter named <strong class="source-inline">copy_aux_mem</strong> is set to <strong class="source-inline">false</strong>, the data will be not copied into the matrix’s <span class="No-Break">internal buffer.</span></p>
			<p>Notice that all of these functions only make a wrapper for the original C++ array where the data is stored and don’t copy the values into a new location. If we want to copy values from a C++ array to a <strong class="source-inline">matrix</strong> object, we usually need to call a <strong class="source-inline">clone()</strong> method or an analog of it for the <span class="No-Break">wrapper object.</span></p>
			<p>After we have a matrix object for an ML framework we use, we can initialize other specialized objects for training ML algorithms. Examples of such abstractions are the <strong class="source-inline">fl::TensorDataset</strong> class in the <strong class="source-inline">Flashlight</strong> library or the <strong class="source-inline">torch::data::Dataset</strong> class in the <span class="No-Break"><strong class="source-inline">libtorch</strong></span><span class="No-Break"> library.</span></p>
			<p>In this section, we learned<a id="_idIndexMarker252"/> how to initialize matrix and tensor objects<a id="_idTextAnchor115"/> <a id="_idTextAnchor116"/>with regular C++ containers and pointers. The following section will move to another<a id="_idIndexMarker253"/> important topic: <span class="No-Break">manipulation images.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor117"/>Manipulating images with the OpenCV and Dlib libraries</h1>
			<p>Many ML algorithms are related to <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>) problems. Examples of such tasks are object detection in<a id="_idIndexMarker254"/> images, segmentation, image classification, and others. To be able to deal with <a id="_idIndexMarker255"/>such tasks, we need instruments for working with images. We usually need routines to load images to computer memory, as well as routines for image processing. For example, the standard operation is image scaling, because many ML algorithms are trained only on images of a specific size. This limitation follows from the algorithm structure or is a<a id="_idIndexMarker256"/> hardware requirement. For example, we cannot load large images to the <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) memory because of its <span class="No-Break">limited size.</span></p>
			<p>Also, hardware requirements can lead to a limited range of numeric types that our hardware supports, so we will need to change the initial image representation to one that our hardware can efficiently process. Also, ML algorithms usually assume a predefined layout of image channels, which can be different from the layout in the original <span class="No-Break">image file.</span></p>
			<p>Another type of image-processing task is the creation of training datasets. In many cases, we have a limited number of available images for a specific task. However, to make a machine algorithm train well, we usually need more training images. So, the typical approach is to augment existing images. Augmentation can be done with operations such as random scaling, cropping parts of images, rotations, and other operations that can be used to make different images from the <span class="No-Break">existing set.</span></p>
			<p>In this section, we show how to use two of the most popular libraries for image processing for C++. <strong class="source-inline">OpenCV</strong> is a framework for solving CV problems that includes many ready-to-use implementations of CV algorithms. Also, it has many functions for image processing. <strong class="source-inline">D<a id="_idTextAnchor118"/>l<a id="_idTextAnchor119"/>ib</strong> is a CV and <a id="_idIndexMarker257"/>ML framework with a large number of implemented algorithms, as well as a rich set of <span class="No-Break">image-processing routines.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor120"/>Using OpenCV</h2>
			<p>In the <strong class="source-inline">OpenCV</strong> library, an image is treated as a multidimensional matrix of values. There is a special <strong class="source-inline">cv::Mat</strong> type for this <a id="_idIndexMarker258"/>purpose. There are two base functions: the <strong class="source-inline">cv::imread()</strong> function loads the image, and the <strong class="source-inline">cv::imwrite()</strong> function writes the image to a file, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
#include &lt;opencv2/opencv.hpp&gt;
..
cv::Mat img = cv::imread(file_name);
cv::imwrite(new_file_name, img);</pre>			<p>Also, there are functions to manage images located in a memory buffer. The <strong class="source-inline">cv::imdecode()</strong> function loads an image from the memory buffer, and the <strong class="source-inline">cv::imencode()</strong> function writes an image to the <span class="No-Break">memory buffer.</span></p>
			<p>Scaling operations in the <strong class="source-inline">OpenCV</strong> library can be done with the <strong class="source-inline">cv::resize()</strong> function. This function takes an input image, an output image, the output image size or scale factors, and an interpolation type as arguments. The interpolation type governs how the output image will look after the scaling. General recommendations are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Use <strong class="source-inline">cv::INTER_AREA</strong> <span class="No-Break">for shrinking</span></li>
				<li>Use <strong class="source-inline">cv::INTER_CUBIC</strong> (slow) or <strong class="source-inline">cv::INTER_LINEAR</strong> <span class="No-Break">for zooming</span></li>
				<li>Use <strong class="source-inline">cv::INTER_LINEAR</strong> for all resizing purposes because it <span class="No-Break">is fast</span></li>
			</ul>
			<p>The main difference between linear and cubic scaling lies in their approach to scaling pixels. Linear scaling preserves the aspect ratio and is simpler, while cubic scaling attempts to maintain details and transitions. The choice between the two depends on the specific requirements of your project and the <span class="No-Break">desired outcome.</span></p>
			<p>The following code sample shows how to scale <span class="No-Break">an image:</span></p>
			<pre class="source-code">
cv::resize(img,
           img,
           {img.cols / 2, img.rows / 2},
           0,
           0,
           cv::INTER_AREA);
cv::resize(img, img, {}, 1.5, 1.5, cv::INTER_CUBIC);</pre>			<p>There is no special function for image cropping in the <strong class="source-inline">OpenCV</strong> library, but the <strong class="source-inline">cv::Mat</strong> type overrides the <strong class="source-inline">operator()</strong> method, which takes a cropping rectangle as an argument and returns a new <strong class="source-inline">cv::Mat</strong> object with part of the image surrounded by the specified rectangle. Also, note that this object will share the same memory with the original image, so its modification will change the original image too. To make a deep copy of the <strong class="source-inline">cv::Mat</strong> object, we need to use the <strong class="source-inline">clone()</strong> method, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
img = img(cv::Rect(0, 0, img.cols / 2, img.rows / 2));</pre>			<p>Sometimes, we need to<a id="_idIndexMarker259"/> move or rotate an image. The <strong class="source-inline">OpenCV</strong> library supports translation and rotation operations for images through affine transformations. We have to manually—or with helper functions—create a matrix of 2D affine transformations and then apply it to our image. For the move (the translation), we can create such a matrix manually and then apply it to an image with the <strong class="source-inline">cv::wrapAffine()</strong> function, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cv::Mat trm = (cv::Mat_&lt;double&gt;(2, 3) &lt;&lt; 1, 0, -50, 0, 1, -50);
cv::wrapAffine(img, img, trm, {img.cols, img.rows});</pre>			<p>We can create a rotation matrix with the <strong class="source-inline">cv::getRotationMatrix2D()</strong> function. This takes a point of origin and the rotation angle in degrees, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
auto rotm = cv::getRotationMatrix2D({img.cols / 2, img.rows / 2},
                                     45,
                                     1);
cv::wrapAffine(img, img, rotm, {img.cols, img.rows});</pre>			<p>Another useful operation is extending an image size without scaling but with added borders. There is the <strong class="source-inline">cv::copyMakeBorder()</strong> function in the <strong class="source-inline">OpenCV</strong> library for this purpose. This function has different options on how to create borders. It takes an input image, an output image, border sizes for the top, the bottom, the left, and the right sides, the type of the<a id="_idIndexMarker260"/> border, and the border color. Border types can be one of <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">BORDER_CONSTANT</strong>:Make function fill borders with a <span class="No-Break">single color</span></li>
				<li><strong class="source-inline">BORDER_REPLICATE</strong>: Make function fill borders with copies of the last pixel values on each side (for <span class="No-Break">example, </span><span class="No-Break"><em class="italic">aaaaaa|abcdefgh|hhhhhhh</em></span><span class="No-Break">)</span></li>
				<li><strong class="source-inline">BORDER_REFLECT</strong>: Make function fill borders with copies of opposite pixel values on each side (for <span class="No-Break">example, </span><span class="No-Break"><em class="italic">fedcba|abcdefgh|hgfedcb</em></span><span class="No-Break">)</span></li>
				<li><strong class="source-inline">BORDER_WRAP</strong>: Make function fill borders by simulating the image duplication (for <span class="No-Break">example, </span><span class="No-Break"><em class="italic">cdefgh|abcdefgh|abcdefg</em></span><span class="No-Break">)</span></li>
			</ul>
			<p>The following example shows how to use <span class="No-Break">this function:</span></p>
			<pre class="source-code">
int top = 50; // px
int bottom = 20; // px
int left = 150; // px
int right = 5; // px
cv::copyMakeBorder(img,img,top,bottom,left,right,
                   cv::BORDER_CONSTANT | cv::BORDER_ISOLATED,
                   cv::Scalar(255, 0, 0));</pre>			<p>When we are using this function, we should take care of the origin of the source image. The <strong class="source-inline">OpenCV</strong> documentation says: “<em class="italic">If the source image is a part of a bigger image, the function will try to use the pixels outside of the ROI (short for region of interest) to form a border. To disable this feature and always do extrapolation, as if the source image was not a part of another image, use border </em><span class="No-Break"><em class="italic">type</em></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">BORDER_ISOLATED</strong></span><span class="No-Break">.”</span></p>
			<p>The function described previously is very helpful when we need to adapt training images of different sizes to the one standard image size used in some ML algorithms because, with this function, we do not distort target <span class="No-Break">image content.</span></p>
			<p>There is the <strong class="source-inline">cv::cvtColor()</strong> function to convert different color spaces in the <strong class="source-inline">OpenCV</strong> library. The function takes an input image, an output image, and a conversion scheme type. For example, in the<a id="_idIndexMarker261"/> following code sample, we convert the <strong class="bold">red, gree<a id="_idTextAnchor121"/>n, and blue</strong> (<strong class="bold">RGB</strong>) color space to<a id="_idIndexMarker262"/> a <span class="No-Break">grayscale<a id="_idTextAnchor122"/> one:</span></p>
			<pre class="source-code">
cv::cvtColor(img, img, cv::COLOR_RGB2GRAY);
// now pixels values are in range 0-1</pre>			<p>This can be very handy in <span class="No-Break">certain scenarios.</span></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor123"/>Using Dlib</h2>
			<p><strong class="source-inline">Dlib</strong> is another popular library for image<a id="_idIndexMarker263"/> processing. This library has different functions and classes for math routines and image processing. The library documentation recommends using the <strong class="source-inline">Dlib::array2d</strong> type for images. The <strong class="source-inline">Dlib::array2d</strong> type is a template type that has to be parametrized with a pixel type. Pixel types in the <strong class="source-inline">Dlib</strong> library are defined with pixel-type traits. There are the following predefined pixel types: <strong class="source-inline">rgb_pixel</strong>, <strong class="source-inline">bgr_pixel</strong>, <strong class="source-inline">rgb_alpha_pixel</strong>, <strong class="source-inline">hsi_pixel</strong>, <strong class="source-inline">lab_pixel</strong>, and any scalar type can be used for grayscaled <span class="No-Break">pixels’ representation.</span></p>
			<p>We can use the <strong class="source-inline">load_image()</strong> function to load an image from disk, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#include &lt;Dlib/image_io.h&gt;
#include &lt;Dlib/image_transforms.h&gt;
using namespace Dlib;
...
array2d&lt;rgb_pixel&gt; img;
load_image(img, file_path);</pre>			<p>For a scaling operation, there is the <strong class="source-inline">Dlib::resize_image()</strong> function. This function has two different overloads. One takes a single scale factor and a reference to an image object. The second one takes an input image, an output image, the desired size, and an interpolation type. To specify the interpolation type in the <strong class="source-inline">Dlib</strong> library, we should call special functions: the <strong class="source-inline">interpolate_nearest_neighbor()</strong>, the <strong class="source-inline">interpolate_quadratic()</strong>, and the <strong class="source-inline">interpolate_bilinear()</strong> functions. The criteria for choosing one of them are the same as for the ones that we discussed in the <em class="italic">Using OpenCV</em> section. Notice that the output image for the <strong class="source-inline">resize_image()</strong> function should be already preallocated, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
array2d&lt;rgb_pixel&gt; img2(img.nr() / 2, img.nc() / 2);
resize_image(img, img2, interpolate_nearest_neighbor());
resize_image(1.5, img); // default interpolate_bilinear</pre>			<p>To crop an image with <strong class="source-inline">Dlib</strong>, we<a id="_idIndexMarker264"/> can use the <strong class="source-inline">Dlib::extract_image_chips()</strong> function. This function takes an original image, rectangle-defined bounds, and an output image. Also, there are overloads of this function that take an array of rectangle bounds and an array of output images, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
extract_image_chip(
    img,
    rectangle(0, 0, img.nc() / 2, img.nr() / 2),
    img2);</pre>			<p>The <strong class="source-inline">Dlib</strong> library supports image transformation operations through affine transformations. There is the <strong class="source-inline">Dlib::transform_image()</strong> function, which takes an input image, an output image, and an affine transformation object. An example of the transformation object could be an instance of the <strong class="source-inline">Dlib::point_transform_affine</strong> class, which defines the affine transformation with a rotation matrix and a translation vector. Also, the <strong class="source-inline">Dlib::transform_image()</strong> function can take an interpolation type as the last parameter, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
transform_image(img,img2,interpolate_bilinear(),
                point_transform_affine(
                    identity_matrix&lt;double&gt;(2),
                    Dlib::vector&lt;double, 2&gt;(-50, -50)));</pre>			<p>In case we only need to do a rotation, <strong class="source-inline">Dlib</strong> has the <strong class="source-inline">Dlib::rotate_image()</strong> function. The <strong class="source-inline">Dlib::rotate_image()</strong> function takes an input image, an output image, a rotation angle in degrees, and an interpolation type, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
rotate_image(img, img2, -45, interpolate_bilinear());</pre>			<p>There is no complete analog of a function for adding borders to images in the <strong class="source-inline">Dlib</strong> library. There are two functions: <strong class="source-inline">Dlib::assign_border_pixels()</strong> and <strong class="source-inline">Dlib::zero_border_pixels()</strong> for filling image borders with specified values. Before using these routines, we should resize the image and place the content in the right position. The new image size should include the borders’ widths. We can use the <strong class="source-inline">Dlib::transform_image()</strong> function<a id="_idIndexMarker265"/> to move the image content into the right place. The following code sample shows how to add borders to <span class="No-Break">an image:</span></p>
			<pre class="source-code">
int top = 50; // px
int bottom = 20; // px
int left = 150; // px
int right = 5; // px
img2.set_size(img.nr() + top + bottom, img.nc() + left + right);
transform_image(
  img, img2, interpolate_bilinear(),
  point_transform_affine(
    identity_matrix&lt;double&gt;(2),
    Dlib::vector&lt;double, 2&gt;(-left/2, -top/2)
  )
);</pre>			<p>For color-space conversions, there exists the <strong class="source-inline">Dlib::assign_image()</strong> function in the <strong class="source-inline">Dlib</strong> library. This function uses color-type information from pixel-type traits we used for the image definition. So, to convert an image to another color space, we should define a new image with the desired type of pixels and pass it to this function. The following example shows <a id="_idIndexMarker266"/>how to convert the RGB image to a <strong class="bold">blue, green, red</strong> (<span class="No-Break"><strong class="bold">BGR</strong></span><span class="No-Break">) one:</span></p>
			<pre class="source-code">
array2d&lt;bgr_pixel&gt; img_bgr;
assign_image(img_bgr, img);</pre>			<p>To make a grayscale image, we can define an image with the <strong class="source-inline">unsigned char</strong> pixel type, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
array2d&lt;unsigned char&gt; img_gray;
assign_image(img_gray, img);</pre>			<p>In this section, we learned how to load and preprocess images with the <strong class="source-inline">OpenCV</strong> and <strong class="source-inline">Dlib</strong> libraries. The next important step is to<a id="_idTextAnchor124"/> convert images into matrix or tensor structures to be able to use them in ML algorithms; it will be described in the <span class="No-Break">following section.</span></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor125"/>Transforming images into matrix or tensor objects of various libraries</h1>
			<p>In most cases, images are<a id="_idIndexMarker267"/> represented in computer memory in an interleaved format, which means that pixel values are placed one by one in linear order. Each pixel value consists of several numbers representing a color. For example, for the RGB format, there will be three values placed together. So, in the memory, we will see the following layout for a <span class="No-Break">4x4 image:</span></p>
			<pre class="source-code">
rgb rgb rgb rgb
rgb rgb rgb rgb
rgb rgb rgb rgb
rgb rgb rgb rgb</pre>			<p>For image-processing libraries, such a value layout is not a problem, but many ML algorithms require different ordering. For example, it’s a common approach for <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) to take image channels<a id="_idIndexMarker268"/> separately ordered, one by one. The following example shows how such a layout is usually placed <span class="No-Break">in memory:</span></p>
			<pre class="source-code">
r r r r g g g g b b b b
r r r r g g g g b b b b
r r r r g g g g b b b b
r r r r g g g g b b b b</pre>			<p>So, often, we need to deinterleave image representation before passing it to some ML algorithm. It means that we need to extract color channels into <span class="No-Break">separate vectors.</span></p>
			<p>Moreover, we usually need to convert a color’s value data type too. For example, <strong class="source-inline">OpenCV</strong> library users often use floating-point formats, which allows them to preserve more color information in image transformations and processing routines. The opposite case is when we use a 256-bit type for color-channel information, but then we need to conv<a id="_idTextAnchor126"/>e<a id="_idTextAnchor127"/>rt it to a floating-point type. So, in many cases, we need to convert the underlying data type to another<a id="_idIndexMarker269"/> one more suitable for <span class="No-Break">our needs.</span></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor128"/>Deinterleaving in OpenCV</h2>
			<p>By default, when we load <a id="_idIndexMarker270"/>an image with the <strong class="source-inline">OpenCV</strong> library, it loads the image in the BGR format and with <strong class="source-inline">char</strong> as the underlying data type. So, we need to convert it to the RGB format, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
cv::cvtColor(img, img, cv::COLOR_BGR2RGB);</pre>			<p>Then, we can convert the underlying data type to the <strong class="source-inline">float</strong> type, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
img.convertTo(img, CV_32FC3, 1/255.0);</pre>			<p>Next, to deinterleave channels, we need to split them with the <strong class="source-inline">cv::split()</strong> function, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
cv::Mat bgr[3];
cv::split(img, bgr);</pre>			<p>Then, we can place channels back to the <strong class="source-inline">cv::Mat</strong> object in the order we need with the <strong class="source-inline">cv::vconcat()</strong> function, which concatenates matrices vertically, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cv::Mat ordered_channels;
cv::vconcat(bgr[2], bgr[1], ordered_channels);
cv::vconcat(ordered_channels, bgr[0], ordered_channels);</pre>			<p>There is a useful method in the <strong class="source-inline">cv::Mat</strong> type named <strong class="source-inline">isContinuous</strong> that allows us to check if the matrix’s data is placed in m<a id="_idTextAnchor129"/>e<a id="_idTextAnchor130"/>mory with a single contiguous block. If that is <strong class="source-inline">true</strong>, we can copy this block of memory or pass it to routines that work with plain <span class="No-Break">C arrays.</span></p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor131"/>Deinterleaving in Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library <a id="_idIndexMarker271"/>uses the <strong class="source-inline">unsigned char</strong> type for pixel color representation, and we can use floating-point types only for grayscaled images. The <strong class="source-inline">Dlib</strong> library stores pixels in row-major order with interleaved channels, and data is placed in memory continuously with a single block. There are no special functions in the <strong class="source-inline">Dlib</strong> library to manage image channels, so we cannot deinterleave them or mix them. However, we can use raw pixel data to manage color values manually. Two functions in the <strong class="source-inline">Dlib</strong> library can help us: the <strong class="source-inline">image_data()</strong> function to access raw pixel data, and the <strong class="source-inline">width_step()</strong> function to get the <span class="No-Break">padding value.</span></p>
			<p>The most straightforward approach to deinterleave the <strong class="source-inline">Dlib</strong> image object is using a loop over all pixels. In such a loop, we can split each pixel value into <span class="No-Break">separate colors.</span></p>
			<p>As a first step, we <a id="_idIndexMarker272"/>define containers for each of the channels, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto channel_size = static_cast&lt;size_t&gt;(img.nc() * img.nr());
std::vector&lt;unsigned char&gt; ch1(channel_size);
std::vector&lt;unsigned char&gt; ch2(channel_size);
std::vector&lt;unsigned char&gt; ch3(channel_size);</pre>			<p>Then, we read color values for each pixel with two nested loops over image rows and columns, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
size_t i{0};
for (long r = 0; r &lt; img.nr(); ++r) {
  for (long c = 0; c &lt; img.nc(); ++c) {
    ch1[i] = img[r][c].red;
    ch2[i] = img[r][c].green;
    ch3[i] = img[r][c].blue;
    ++i;
  }
}</pre>			<p>The result is three containers with color-channel values, which we can use separately. They are suitable to initialize grayscaled images for use in image-processing routines. Alternatively, we can use them to initialize a matrix-type object that we can process with linear <span class="No-Break">algebra routines.</span></p>
			<p>We saw how to load and prepare images for use in linear algebra abstractions and ML algorithms. In the <a id="_idIndexMarker273"/>next section, we will lea<a id="_idTextAnchor132"/>r<a id="_idTextAnchor133"/>n general methods to prepare data for use in ML algorithms. Such methods will help us make learning procedures more stable and <span class="No-Break">converge faster.</span></p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor134"/>Normalizing data</h1>
			<p>Data normalization is a crucial preprocessing step in ML. In general, data normalization is a process that transforms multiscale data to the same scale. Feature values in a dataset can have very different scales—for example, the height can be given in centimeters with small values, but the income can have large-value amounts. This fact has a significant impact on many <span class="No-Break">ML algorithms.</span></p>
			<p>For example, if some feature<a id="_idIndexMarker274"/> values differ from values of other features several times, then this feature will dominate over others in classification algorithms based on the Euclidean distance. Some algorithms have a strong requirement for normalization of input data; an example of such an algorithm is the <strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>) algorithm. NNs <a id="_idIndexMarker275"/>also usually require normalized<a id="_idIndexMarker276"/> input data. Also, data normalization has an impact on optimization algorithms. For example, optimizers based on the <strong class="bold">gradient descent</strong> (<strong class="bold">GD</strong>) approach can converge much quicker if data has the <span class="No-Break">same scale.</span></p>
			<p>There are several methods of normalization, but from our point of view, the most popular are the standardization, the min-max, and the mean <span class="No-Break">normalization methods.</span></p>
			<p><strong class="bold">Standardization</strong> is a process of making <a id="_idIndexMarker277"/>data have a zero mean and a standard deviation equal to 1. The formula for standardized vector is <img alt="" role="presentation" src="image/B19849_Formula_01.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_02.png"/> is an original vector, <img alt="" role="presentation" src="image/B19849_Formula_03.png"/> is an average value of <img alt="" role="presentation" src="image/B19849_Formula_04.png"/> calculated with the formula <img alt="" role="presentation" src="image/B19849_Formula_05.png"/>, and <img alt="" role="presentation" src="image/B19849_Formula_06.png"/> is the standard deviation of <img alt="" role="presentation" src="image/B19849_Formula_07.png"/> calculated with the <span class="No-Break">formula <img alt="" role="presentation" src="image/B19849_Formula_08.png"/>.</span></p>
			<p><strong class="bold">Min-max normalization</strong> or <strong class="bold">rescaling</strong> is a process<a id="_idIndexMarker278"/> of making data fit the range of <strong class="source-inline">[0, 1]</strong>. We can do<a id="_idIndexMarker279"/> rescaling with the <span class="No-Break">following formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer072">
					<img alt="" role="presentation" src="image/B19849_Formula_091.jpg"/>
				</div>
			</div>
			<p>Min-max scaling is useful when there are significant differences in the scale of different features in your dataset. It helps to make the features comparable, which is important for many <span class="No-Break">ML models.</span></p>
			<p><strong class="bold">Mean normalization</strong> is used to fit <a id="_idIndexMarker280"/>data into the range <strong class="source-inline">[-1, 1]</strong> so that its mean becomes zero. We can use the following formula to do <span class="No-Break">mean normalization:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer073">
					<img alt="" role="presentation" src="image/B19849_Formula_101.jpg"/>
				</div>
			</div>
			<p>This transformation helps to make the data more easily interpretable and improves the performance of some ML algorithms by reducing the impact of outliers and ensuring that all features are on a<a id="_idIndexMarker281"/> similar scale. Consider how we can implement these normalization techniques and which ML framework functions can be used to <span class="No-Break">calculate them.</span></p>
			<p>W<a id="_idTextAnchor135"/>e<a id="_idTextAnchor136"/> assume that each row of this matrix <img alt="" role="presentation" src="image/B19849_Formula_11.png"/> is one training sample, and the value in each column is the value of one feature of the <span class="No-Break">current sample.</span></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor137"/>Normalizing with Eigen</h2>
			<p>There are no functions for data<a id="_idIndexMarker282"/> normalization in the <strong class="source-inline">Eigen</strong> library. However, we<a id="_idIndexMarker283"/> can implement them according to the <span class="No-Break">provided formulas.</span></p>
			<p>For standardization, we first have to calculate the standard deviation, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Eigen::Array&lt;double, 1, Eigen::Dynamic&gt; std_dev =
  ((x.rowwise() - x.colwise().mean())
    .array()
    .square()
    .colwise()
    .sum() /
    (x_data.rows() - 1))
  .sqrt();</pre>			<p>Notice that some reduction functions in the <strong class="source-inline">Eigen</strong> library work only with array representation; examples are the <strong class="source-inline">sum()</strong> and the <strong class="source-inline">sqrt()</strong> functions. We have also calculated the mean for each feature—we used the <strong class="source-inline">x.colwise().mean()</strong> function combination, which<a id="_idIndexMarker284"/> returns a vector of <strong class="source-inline">mean</strong>. We can use the same <a id="_idIndexMarker285"/>approach for other feature <span class="No-Break">statistics’ calculations.</span></p>
			<p>Having the standard deviation value, the rest of the formula for standardization will look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic&gt; x_std =
    (x.rowwise() - x.colwise().mean()).array().rowwise() / std_dev;</pre>			<p>Implementation of <strong class="source-inline">min-max</strong> normalization is very straightforward and does not require intermediate values, as illustrated in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic&gt; x_min_max =
    (x.rowwise() - x.colwise().minCoeff()).array().rowwise() /
    (x.colwise().maxCoeff() - x.colwise().minCoeff()).array();</pre>			<p>We implement the mean normalization in the same way, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic&gt; x_avg =
  (x.rowwise() - x.colwise().mean()).array().rowwise() /
  (x.colwise().maxCoeff() - x.colwise().minCoeff()).array();</pre>			<p>Notice that we implement formulas in a vectorized way without loops; this approach is more computationally efficient<a id="_idTextAnchor138"/> <a id="_idTextAnchor139"/><a id="_idIndexMarker286"/>because it can be compiled for<a id="_idIndexMarker287"/> execution on a GPU or the <strong class="bold">central processing unit’s </strong>(<strong class="bold">CPU’s</strong>) <strong class="bold">Single Instruction Multiple Data</strong> (<span class="No-Break"><strong class="bold">SIMD</strong></span><span class="No-Break">) instructions.</span></p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor140"/>Normalizing with mlpack</h2>
			<p>There are different <a id="_idIndexMarker288"/>classes for feature scaling in the <strong class="source-inline">mlpack</strong> library. The most interesting for us are <strong class="source-inline">data::data::MinMaxScaler</strong>, which implements min-max normalization (or rescaling), and <strong class="source-inline">mlpack::data::StandardScaler</strong>, which implements data standardization. We can reuse objects<a id="_idIndexMarker289"/> of those classes for scaling different data with the same learned statistics. It can be useful in cases when we train an ML algorithm on one data format with applied rescaling, and then we use the algorithm for predictions on new data. To make this algorithm work as we want, we have to rescale new data in the same way as we did in the training process, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#include &lt;mlpack/core.hpp&gt;
...
arma::mat features;
arma::Row&lt;size_t&gt; labels;
data::MinMaxScaler min_max_scaler;
min_max_scaler.Fit(features); // learn statistics
arma::mat scaled_dataset;
min_max_scaler.Transform(features, scaled_data<a id="_idTextAnchor141"/>set);</pre>			<p>To learn statistics values, we use the <strong class="source-inline">Fit()</strong> method, and for feature modification, we use the <strong class="source-inline">Transform()</strong> method of the <span class="No-Break"><strong class="source-inline">MinMaxScaler</strong></span><span class="No-Break"> class.</span></p>
			<p>The <strong class="source-inline">StandardScaler</strong> class can be used in the same manner, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
data::StandardScaler standard_scaler;
standard_scaler.Fit(features);
standard_scaler.Transform(features, scaled_dataset);</pre>			<p>To print the matrix object in the <strong class="source-inline">mlpack</strong> library, the standard streaming operators ca<a id="_idTextAnchor142"/>n be used <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::cout &lt;&lt; scaled_dataset &lt;&lt; std::endl;</pre>			<p>Also, to revert the<a id="_idIndexMarker290"/> applied scaling, these classes<a id="_idIndexMarker291"/> have the <span class="No-Break"><strong class="source-inline">InverseTransform</strong></span><span class="No-Break"> method.</span></p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor143"/>Normalizing with Dlib</h2>
			<p>The <strong class="source-inline">Dlib</strong> library provides functionality for feature<a id="_idIndexMarker292"/> standardization with the <strong class="source-inline">Dlib::vector_normalizer</strong> class. There is one limitation to using <a id="_idIndexMarker293"/>this class—we cannot use it with one big matrix containing all training samples. Alternatively, we should represent each sample with a separate vector object and put them into the C++ <strong class="source-inline">std::vector</strong> container, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;matrix&lt;double&gt;&gt; samples;
...
vector_normalizer&lt;matrix&lt;double&gt;&gt; normalizer;
samples normalizer.train(samples);
samples = normalizer(samples);</pre>			<p>We see that the object of this class can be reused, but it should be trained first. The train method implementation can look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
matrix&lt;double&gt; m(mean(mat(samples)));
matrix&lt;double&gt; sd(reciprocal(stddev(mat(samples))));
for (size_t i = 0; i &lt; samples.size(); ++i)
    samples[i] = pointwise_multiply(samples[i] - m, sd);</pre>			<p>Notice that the <strong class="source-inline">Dlib::mat()</strong> function has different overloads for matrix creation from different sources. Also, we use the <strong class="source-inline">reciprocal()</strong> function that makes the <img alt="" role="presentation" src="image/B19849_Formula_121.png"/> matrix if <em class="italic">m</em> is the <span class="No-Break">input matrix.</span></p>
			<p>Printing matrices for debugging purposes in<a id="_idTextAnchor144"/> the <strong class="source-inline">Dlib</strong> library can be done with the simple streaming operator, as illustrated <a id="_idIndexMarker294"/>in the following <span class="No-Break">code sni<a id="_idTextAnchor145"/>ppet:</span></p>
			<pre class="source-code">
std::cout &lt;&lt; mat(samples) &lt;&lt; std::endl;</pre>			<p>We can see that the <strong class="source-inline">Dlib</strong> library<a id="_idIndexMarker295"/> provides a rich interface for data preprocessing that can be <span class="No-Break">easily used.</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor146"/>Normalizing with Flashlight</h2>
			<p>The <strong class="source-inline">Flashlight</strong> library doesn’t have particular classes to perform feature scaling. But it has functions to calculate basic <a id="_idIndexMarker296"/>statistics easily, so we can <a id="_idIndexMarker297"/>implement feature scaling algorithms <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
fl::Tensor x;
...
// min-max scaling
auto x_min = fl::amin(x, {1});
auto x_max = fl::amax(x, {1});
auto x_min_max = (x - x_min) / (x_max - x_min);
// normalization(z-score)
auto x_mean = fl::mean(x, {1});
auto x_std = fl::std(x, {1});
auto x_norm = (x - x_mean) / x_std;</pre>			<p>The <strong class="source-inline">fl::amin</strong> and <strong class="source-inline">fl::amax</strong> functions find the minimum and maximum values. The <strong class="source-inline">fl::mean</strong> and <strong class="source-inline">fl::std</strong> functions calculate the mean and standard deviation correspondingly. All these functions do their calculation along a specified dimension that comes as the second parameter. It means that we scale each <strong class="source-inline">x</strong> feature in the<strong class="source-inline"> </strong><span class="No-Break">dataset separately.</span></p>
			<p>We can print a <strong class="source-inline">fl::Tensor</strong> object with the standard C++ streaming operator, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::cout &lt;&lt; dataset &lt;&lt; <a id="_idTextAnchor147"/>std::endl;</pre>			<p>We saw that<a id="_idIndexMarker298"/> despite the <strong class="source-inline">FlashLight</strong> library not providing special <a id="_idIndexMarker299"/>classes for data preprocessing, we can build them with linear <span class="No-Break">algebra routines.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor148"/>Summary</h1>
			<p>In this chapter, we considered how to load data from CSV, JSON, and HDF5 formats. CSV is easy to read and write, making it suitable for small to medium-sized datasets. CSV files are often used for tabular data, such as customer information, sales records, or financial transactions. JSON is a lightweight data interchange format that is human-readable and easy to parse. It is commonly used for representing structured data, including objects, arrays, and key-value pairs. In ML, JSON can be used to store data for training models, such as feature vectors, labels, and metadata. HDF5 is a high-performance file format designed for scientific data storage and analysis. It supports large datasets with complex structures, allowing for efficient storage of multidimensional arrays and tables. HDF5 files are commonly used in applications where large amounts of data need to be stored and <span class="No-Break">accessed efficiently.</span></p>
			<p>We saw how to convert the loaded data into objects suitable for use in different ML frameworks. We used the libraries’ APIs to convert raw C++ arrays into matrices and higher-level dataset objects for <span class="No-Break">ML algorithms.</span></p>
			<p>We looked at how to load and process images with the <strong class="source-inline">OpenCV</strong> and <strong class="source-inline">Dlib</strong> libraries. These libraries offer a wide range of functions and algorithms that can be used in various applications for CV. The libraries can be used for basic image preprocessing, as well as for more complicated systems that use ML for solving industry-important tasks such as face detection and recognition, which can be used to build security systems, and for access control or facial authentication. Object detection can be used for tasks such as counting objects in an image, detecting defects in products, identifying specific objects, or tracking their movement. This is useful in industrial automation, surveillance systems to identify suspicious activities, and autonomous vehicles. Image segmentation allows users to extract specific parts of an image for further analysis. This is essential for diagnosing diseases in medical imaging analysis. Motion tracking of objects over time is also used for sports analytics, traffic monitoring, <span class="No-Break">and surveillance.</span></p>
			<p>We became familiar with the data normalization process, which is very important for many ML algorithms. Also, we saw which normalization techniques are available in ML libraries, and we implemented some normalization approaches with linear algebra functions from the <span class="No-Break"><strong class="source-inline">Eigen</strong></span><span class="No-Break"> library.</span></p>
			<p>In the following chapter, we will see how to measure a model’s performance on different types of data. We will look at special techniques that help us to understand how the model describes the training dataset well and how it performs on new data. Al<a id="_idTextAnchor149"/>s<a id="_idTextAnchor150"/>o, we will learn the different types of parameters ML models depend on and see how to select the best combination of them to improve the <span class="No-Break">model’s performance.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor151"/>Further reading</h1>
			<ul>
				<li>The HDF5® library and file <span class="No-Break">format: </span><a href="https://www.hdfgroup.org/solutions/hdf5/"><span class="No-Break">https://www.hdfgroup.org/solutions/hdf5/</span></a></li>
				<li>GitHub link for Fast-CPPCSV <span class="No-Break">Parser: </span><a href="https://github.com/ben-strasser/Fast-CPP-CSV-Parser"><span class="No-Break">https://github.com/ben-strasser/Fast-CPP-CSV-Parser</span></a></li>
				<li><span class="No-Break"><strong class="source-inline">OpenCV</strong></span><span class="No-Break">: </span><a href="https://opencv.org/"><span class="No-Break">https://opencv.org/</span></a></li>
				<li><strong class="source-inline">Dlib</strong> C++ <span class="No-Break">library: </span><a href="http://Dlib.net/"><span class="No-Break">http://Dlib.net/</span></a></li>
				<li><strong class="source-inline">Flashlight</strong> <span class="No-Break">documentation: </span><a href="https://fl.readthedocs.io/en/latest/index.html"><span class="No-Break">https://fl.readthedocs.io/en/latest/index.html</span></a></li>
				<li><strong class="source-inline">nlohmann-json</strong> <span class="No-Break">documentation: </span><a href="https://json.nlohmann.me/"><span class="No-Break">https://json.nlohmann.me/</span></a></li>
				<li><strong class="source-inline">mlpack</strong> <span class="No-Break">documentation: </span><a href="https://mlpack.org/doc/index.html"><span class="No-Break">https://mlpack.org/doc/index.html</span></a></li>
				<li><em class="italic">A Hybrid Approach for Sentiment Analysis Applied to Paper Reviews</em> <span class="No-Break">dataset: </span><a href="https://archive.ics.uci.edu/static/public/410/paper+reviews.zip"><span class="No-Break">https://archive.ics.uci.edu/static/public/410/paper+reviews.zip</span></a></li>
			</ul>
		</div>
	</body></html>