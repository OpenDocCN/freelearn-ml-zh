- en: <st c="0">7</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Performing Feature Scaling</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="28">Many machine learning algorithms are sensitive to the variable scale.</st>
    <st c="99">For example, the coefficients of linear models depend on the scale
    of the feature – that is, changing the feature scale will change the coefficient’s
    value.</st> <st c="256">In linear models, as well as in algorithms that depend
    on distance calculations such as clustering and principal component analysis,
    features with larger value ranges tend to dominate over features with smaller
    ranges.</st> <st c="475">Therefore, having features on a similar scale allows
    us to compare feature importance and may help algorithms converge faster, improving
    performance and</st> <st c="628">training times.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="643">Scaling techniques, in general, divide the variables by some constant;
    therefore, it is important to highlight that the shape of the variable distribution
    does not change when we rescale the variables.</st> <st c="846">If you want to
    change the distribution shape, check out</st> [*<st c="902">Chapter 3</st>*](B22396_03.xhtml#_idTextAnchor351)<st
    c="911">,</st> *<st c="913">Transforming</st>* *<st c="926">Numerical Variables</st>*<st
    c="945">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="946">In this chapter, we will describe different methods to set features
    on a</st> <st c="1020">similar scale.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1034">This chapter will cover the</st> <st c="1063">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1081">Standardizing</st> <st c="1096">the features</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1108">Scaling to the maximum and</st> <st c="1136">minimum values</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1150">Scaling with the median</st> <st c="1175">and quantiles</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1188">Performing</st> <st c="1200">mean normalization</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1218">Implementing maximum</st> <st c="1240">absolute scaling</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1256">Scaling to vector</st> <st c="1275">unit length</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1286">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1309">The main libraries that we use in this chapter are scikit-learn
    (</st>`<st c="1375">sklearn</st>`<st c="1383">) for scaling,</st> `<st c="1399">pandas</st>`
    <st c="1405">to handle the data, and</st> `<st c="1430">matplotlib</st>` <st c="1440">for
    plotting.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1454">S</st><st c="1456">tandardizing the features</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1481">Standardization</st> <st c="1497">is the process of centering the
    variable at</st> `<st c="1542">0</st>` <st c="1543">and standardizing the variance
    to</st> `<st c="1578">1</st>`<st c="1579">. To</st> <st c="1583">standardize features,
    we subtract the mean from each observation and then divide the result by the</st>
    <st c="1683">standard deviation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/25.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="1733">The result of the preceding transformation is called</st> <st c="1785">the</st>
    **<st c="1790">z-score</st>** <st c="1797">and represents how many standard deviations
    a given observation</st> *<st c="1862">deviates</st>* <st c="1870">from</st> <st
    c="1876">the mean.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1885">Standardization is generally useful when models require the variables
    to be centered at zero and data is not sparse (centering sparse data will destroy
    its sparse nature).</st> <st c="2058">On the downside, standardization is sensitive
    to outliers and the z-score does not keep the symmetric properties if the variables
    are highly skewed, as we discuss in the</st> <st c="2228">following section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2246">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="2260">With standardization, the variable distribution does not change;
    what changes is the magnitude of their values, as we see in the</st> <st c="2390">following
    figure:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Distribution of a normal and skewed variable before and after
    standardization.](img/B22396_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="2777">Figure 7.1 – Distribution of a normal and skewed variable before
    and after standardization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2868">The z-score (</st>*<st c="2882">x</st>* <st c="2884">axis in the
    bottom panels) indicates</st> <st c="2921">how many standard deviations an observation
    deviates from the mean.</st> <st c="2990">When the z-score is</st> `<st c="3010">1</st>`<st
    c="3011">, the observation lies 1 standard deviation to the right of the mean,
    whereas when the z-score is</st> `<st c="3109">-1</st>`<st c="3111">, the sample
    is 1 standard deviation to the left of</st> <st c="3163">the mean.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3172">In normally distributed variables, we can estimate the probability
    of a value being greater or smaller than a given z-score, and this probability
    distribution is symmetric.</st> <st c="3346">The probability of an observation
    being smaller than a z-score of</st> `<st c="3412">-1</st>` <st c="3414">is equivalent
    to the probability of a value being greater than</st> `<st c="3478">1</st>` <st
    c="3479">(horizontal line in the bottom-left panel).</st> <st c="3524">This symmetry
    is fundamental to many statistical tests.</st> <st c="3580">In skewed distributions,
    this symmetry does not hold.</st> <st c="3634">As illustrated in the bottom-right
    panel of</st> *<st c="3678">Figure 7</st>**<st c="3686">.1</st>* <st c="3688">(horizontal
    lines), the probability of a value being smaller than</st> `<st c="3755">-1</st>`
    <st c="3757">is different from that of being greater</st> <st c="3798">than</st>
    `<st c="3803">1</st>`<st c="3804">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3805">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3810">The mean and the standard deviation are sensitive to outliers;
    therefore, the features may scale differently from each other in the presence
    of outliers when</st> <st c="3969">using standardization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3991">In practice, we often apply standardization ignoring the shape
    of the distribution.</st> <st c="4076">However, keep in mind that if the models
    or tests you are using make assumptions about the data’s distribution, you might
    benefit from transforming the variables before standardization, or trying a different</st>
    <st c="4284">scaling metho</st><st c="4297">d.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4300">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="4316">In this recipe, we’ll apply</st> <st c="4345">standardization to
    the variables of the California</st> <st c="4396">housing dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4412">Let’s begin by importing the required Python packages, classes,</st>
    <st c="4477">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4667">Let’s load the California housing dataset from scikit-learn into
    a DataFrame and drop the</st> `<st c="4758">Latitude</st>` <st c="4766">and</st>
    `<st c="4771">Longitude</st>` <st c="4780">variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4919">Now, let’s divide the data into train and</st> <st c="4962">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5062">Next, we’ll set up the</st> `<st c="5086">StandardScaler()</st>`
    <st c="5102">function from scikit-learn and fit it</st> <st c="5140">to the train
    set so that it learns each variable’s mean and</st> <st c="5201">standard deviation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5298">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5303">Scikit-learn scalers, like any scikit-learn transformer, return
    NumPy arrays by default.</st> <st c="5393">To return</st> `<st c="5403">pandas</st>`
    <st c="5409">or</st> `<st c="5413">polars</st>` <st c="5419">DataFrames, we need
    to specify the output container with the</st> `<st c="5481">set_output()</st>`
    <st c="5493">method, as we did in</st> *<st c="5515">Step 4</st>*<st c="5521">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5522">Now, let’s</st> <st c="5534">standardize the train and test sets
    with the</st> <st c="5579">trained scaler:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`<st c="5678">StandardScaler()</st>` <st c="5695">stores the mean and standard
    deviation learned from the training set during</st> `<st c="5772">fit()</st>`<st
    c="5777">. Let’s visualize the</st> <st c="5799">learned parameters.</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="5818">First, we’ll print the mean values that were learned</st> <st c="5872">by</st>
    `<st c="5875">scaler</st>`<st c="5881">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5896">We see the mean values of each variable in the</st> <st c="5944">following
    output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.scale_
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="6227">array([1.89109236e+00, 1.25962585e+01, 2.28754018e+00,                          4.52736275e-01,
    1.14954037e+03, 6.86792905e+00])</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6416">Let’s print</st> <st c="6428">the descriptive statistics from the
    original variables in the</st> <st c="6491">test set:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6518">In the following output, we see that the variables’ mean values
    are different from zero and the</st> <st c="6615">variance varies:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Descriptive statistical parameters of the variables before scaling](img/B22396_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="7205">Figure 7.2 – Descriptive statistical parameters of the variables
    before scaling</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7284">Let’s now print</st> <st c="7300">the descriptive statistical values
    from the</st> <st c="7345">transformed variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7392">In the following output, we see that the variables’ mean is now
    centered at</st> `<st c="7469">0</st>` <st c="7470">and the variance is</st> <st
    c="7491">approximately</st> `<st c="7505">1</st>`<st c="7506">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Descriptive statistical parameters of the scaled variables showing
    a mean of 0 and variance of approximately 1](img/B22396_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8073">Figure 7.3 – Descriptive statistical parameters of the scaled variables
    showing a mean of 0 and variance of approximately 1</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8196">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8201">The</st> `<st c="8206">AveRooms</st>`<st c="8214">,</st> `<st c="8216">AveBedrms</st>`<st
    c="8225">, and</st> `<st c="8231">AveOccup</st>` <st c="8239">variables are highly
    skewed, which can lead to observed values in the test set that are much greater
    or much smaller than those in the training set, and hence we see that the variance
    deviates from</st> `<st c="8438">1</st>`<st c="8439">. This is to be expected
    because standardization is sensitive to outliers and very</st> <st c="8522">skewed
    distributions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8543">We mentioned, in</st> <st c="8560">the</st> *<st c="8565">Getting
    ready</st>* <st c="8578">section, that the shape of the distribution does not
    change with standardization.</st> <st c="8661">Go ahead and corroborate that by
    executing</st> `<st c="8704">X_test.hist()</st>` <st c="8717">and then</st> `<st
    c="8727">X_test_scaled.hist()</st>` <st c="8747">to compare the variables’ distribution
    before and after</st> <st c="8804">the transformati</st><st c="8820">on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8824">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8840">In this recipe, we standardized the variables of the California
    housing dataset by utilizing scikit-learn.</st> <st c="8948">We split the data
    into train and test sets because the parameters for the standardization should
    be learned from the train set.</st> <st c="9076">This is to avoid leaking data
    from the test to the train set during the preprocessing steps and to ensure the
    test set remains naïve to all feature</st> <st c="9224">transformation processes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9249">To standardize these features, we used scikit-learn’s</st> `<st
    c="9304">StandardScaler()</st>` <st c="9320">function, which is able to learn
    and store the parameters utilized in the transformation.</st> <st c="9411">Using</st>
    `<st c="9417">fit()</st>`<st c="9422">, the scaler learned each variable’s mean
    and standard deviation and stored them in its</st> `<st c="9510">mean_</st>` <st
    c="9515">and</st> `<st c="9520">scale_</st>` <st c="9526">attributes.</st> <st
    c="9539">Using</st> `<st c="9545">transform()</st>`<st c="9556">, the scaler standardized
    the variables in the train and test sets.</st> <st c="9624">The default output
    of</st> `<st c="9646">StandardScaler()</st>` <st c="9662">is a NumPy array, but
    through the</st> `<st c="9697">set_output()</st>` <st c="9709">parameter, we can
    change the output</st> <st c="9746">container to a</st> `<st c="9761">pandas</st>`
    <st c="9767">DataFrame, as we did in</st> *<st c="9792">Step 4</st>*<st c="9798">,
    or to</st> `<st c="9806">polars</st>`<st c="9812">, by</st> <st c="9817">setting</st>
    `<st c="9825">transform="polars"</st>`<st c="9843">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9844">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="9849">StandardScaler()</st>` <st c="9866">will subtract the mean and
    divide it by the standard deviation by default.</st> <st c="9942">If we want to
    just center the distributions without standardizing the variance, we can do so
    by setting</st> `<st c="10046">with_std=False</st>` <st c="10060">when initializing
    the transformer.</st> <st c="10096">If we want to set the variance to</st> `<st
    c="10130">1</st>`<st c="10131">, without cantering the distribution, we can do
    so by setting</st> `<st c="10193">with_mean=False</st>` <st c="10208">in</st>
    *<st c="10212">Step</st> <st c="10216">4</st>*<st c="10218">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10219">Scaling to the maximum and minimum values</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="10261">Scaling</st> <st c="10270">to the minimu</st><st c="10283">m and
    maximum values squeezes the values of</st> <st c="10328">the variables between</st>
    `<st c="10350">0</st>` <st c="10351">and</st> `<st c="10356">1</st>`<st c="10357">.
    To implement this scaling method, we subtract the minimu</st><st c="10415">m value
    from all the observations and divide the result by the value range – that is,
    the difference between the maximum and</st> <st c="10541">minimum values:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>max</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>−</mo><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/26.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="10591">Scaling to the minimum and maximum is suitable for variables with
    very small standard deviations, when the models do not require data to be centered
    at zero, and when we want to preserve zero entries in sparse data, such as in
    one-hot encoded variables.</st> <st c="10845">On the downside, it is sensitive</st>
    <st c="10878">to outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10890">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10904">Scaling to the minimum and maximum value does not change the distribution
    of the variables, as illustrated in the</st> <st c="11019">following figure:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Distribution of a normal and skewed variable before and after
    scaling to the minimum and maximum value](img/B22396_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11270">Figure 7.4 – Distribution of a normal and skewed variable before
    and after scaling to the minimum and maximum value</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11385">This</st> <st c="11391">scaling method standardizes the maximum
    value of the</st> <st c="11443">variables to a unit size.</st> <st c="11470">Scaling
    to the minimum and maximum value tends to be the preferred alternative to standardization,
    and it is suitable for variables with very small standard deviations and when
    we want to preserve zero entries in sparse data, such as in one-hot encoded variables,
    or variables derived from counts, such as bag of words.</st> <st c="11790">However,
    this procedure does not center the variables at zero, so if the algorithm has
    that requirement, this method might not be the</st> <st c="11924">best choice.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11936">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11941">Scaling to the minimum and maximum values is sensitive to outliers.</st>
    <st c="12010">If outliers are present in the training set, the scaling will squeeze
    the values toward one of the tails.</st> <st c="12116">If, on the contrary, outliers
    are in the test set, the variable will show values greater than</st> `<st c="12210">1</st>`
    <st c="12211">or smaller than</st> `<st c="12228">0</st>` <st c="12229">after
    scaling, depending on whether the outlier is on the left or</st> <st c="12296">righ</st><st
    c="12300">t tail.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12308">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="12324">In this recipe, we’ll scale</st> <st c="12353">the variables of
    the California housing dataset</st> <st c="12401">to values between</st> `<st
    c="12419">0</st>` <st c="12420">and</st> `<st c="12424">1</st>`<st c="12425">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12426">Let’s start by importing</st> `<st c="12451">pandas</st>` <st
    c="12457">and the required classes</st> <st c="12483">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12671">Let’s load the</st> <st c="12687">California housing dataset from
    scikit-learn into a</st> `<st c="12739">pandas</st>` <st c="12745">DataFrame,
    dropping the</st> `<st c="12770">Latitude</st>` <st c="12778">and</st> `<st c="12783">Longitude</st>`
    <st c="12792">variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12932">Let’s divide the data into training and</st> <st c="12973">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13073">Let’s set up the scaler and then fit it to the train set so that
    it learns each variable’s minimum and maximum values and the</st> <st c="13200">value
    range:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13289">Finally, let’s</st> <st c="13304">scale the variables in the train
    and test sets with the</st> <st c="13361">trained scaler:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13460">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="13465">MinMaxScale</st><st c="13477">r()</st>` <st c="13481">stores
    the maximum and minimum values and the value ranges in its</st> `<st c="13548">data_max_</st>`<st
    c="13557">,</st> `<st c="13559">min_</st>`<st c="13563">, and</st> `<st c="13569">data_range_</st>`
    <st c="13580">attributes, respectively.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13606">We can</st> <st c="13613">corroborate the minimum values of the</st>
    <st c="13651">transformed variables by executing</st> `<st c="13687">X_test_scaled.min()</st>`<st
    c="13706">, which will return the</st> <st c="13730">following output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="13968">MedInc           1.000000</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13983">HouseAge        1.000000</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14001">AveRooms        1.071197</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14019">AveBedrms      0.750090</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14038">Population     0.456907</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14058">AveOccup        2.074553</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="14076">dtype: float64</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import RobustScaler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop(labels=[     "Latitude", "Longitude"], axis=1,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: inplace=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = RobustScaler().set_output(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: transform="pandas")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler.transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.center_
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '<st c="18097">RobustScaler()</st>:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18294">This scaling procedure does not change the variable distributions.</st>
    <st c="18362">Go ahead and compare the distribution of the variables before and
    after the transformation</st> <st c="18452">by</st> <st c="18456">using histograms.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop(labels=[
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Latitude", "Longitude"], axis=1, inplace=True)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: means = X_train.mean(axis=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21106">MedInc           3.866667</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21122">HouseAge        28.618702</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21141">AveRooms         5.423404</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21159">AveBedrms        1.094775</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21178">Population    1425.157323</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21201">AveOccup         3.040518</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="21219">dtype: float</st><st c="21232">64</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: ranges = X_train.max(axis=0)-X_train.min(axis=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21443">MedInc</st> <st c="21450">14.500200</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21460">HouseAge         51.000000</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21479">AveRooms        131.687179</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21499">AveBedrms        33.733333</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21519">Population    35679.000000</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="21543">AveOccup        598.964286</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '<st c="21563">dtype: float64</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = (X_train - means) / ranges
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = (X_test - means) / ranges
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: from sklearn.preprocessing import (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: StandardScaler, RobustScaler
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_mean = StandardScaler(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_mean=True, with_std=False,
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ).set_output(transform="pandas")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_minmax = RobustScaler(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_centering=False,
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: with_scaling=True,
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: quantile_range=(0, 100)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ).set_output(transform="pandas")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_mean.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler_minmax.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler_minmax.transform(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler_mean.transform(X_train)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler_minmax.transform(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler_mean.transform(X_test)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import matplotlib.pyplot as plt
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import MaxAbsScaler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data = pd.read_csv("bag_of_words.csv")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = MaxAbsScaler().set_output(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: transform="pandas")
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler.fit(data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data_scaled = scaler.transform(data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.max_abs_
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="27715">array([ 7.,  6.,  2.,  2., 11.,  4.,  3.,  6., 52.,  2.])</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data.hist(bins=20, figsize=(20, 20))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: plt.show()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data_scaled.hist(bins=20, figsize=(20, 20))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: plt.show()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MaxAbsScaler, StandardScaler)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.pipeline import Pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop( labels=[ "Latitude",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Longitude"], axis=1, inplace=True)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_mean = StandardScaler(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_mean=True, with_std=False)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_maxabs = MaxAbsScaler()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = Pipeline([
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ("scaler_mean", scaler_mean),
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ("scaler_max", scaler_maxabs),
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: ']).set_output(transform="pandas")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler.transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import numpy as np
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import Normalizer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop(labels=[
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Latitude", "Longitude"], axis=1, inplace=True)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = Normalizer(norm='l1')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler.fit_transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: np.round(np.linalg.norm(X_train, ord=1, axis=1), 1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="35330">array([ 255.3,  889.1, 1421.7, ...,  744.6, 1099.5,</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="35380">1048.9])</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: np.round(np.linalg.norm(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_scaled, ord=1, axis=1), 1)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: array([1., 1., 1., ..., 1., 1., 1.])
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
