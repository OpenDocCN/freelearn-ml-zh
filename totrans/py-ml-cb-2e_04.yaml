- en: Clustering with Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering data using the k-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing an image using vector quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping data using agglomerative clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the number of clusters using the **Density-Based Spatial Clustering
    of Applications with Noise** (**DBSCAN**) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding patterns in stock market data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a customer segmentation model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using autoencoders to reconstruct handwritten digit images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you will need the following files (which
    are available on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`kmeans.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_multivar.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_quantization.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flower_image.jpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`agglomerative.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`performance.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_perf.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`estimate_clusters.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stock_market.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`symbol_map.json`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stock_market_data.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`customer_segmentation.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wholesale.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AutoencMnist.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Unsupervised learning** is a paradigm in machine learning where we build
    models without relying on labeled training data. Up to this point, we have dealt
    with data that was labeled in some way. This means that learning algorithms can
    look at this data and learn to categorize it them based on labels. In the world
    of unsupervised learning, we don''t have this opportunity! These algorithms are
    used when we want to find subgroups within datasets using a similarity metric.'
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, information from the database is automatically extracted.
    All this takes place without prior knowledge of the content to be analyzed. In
    unsupervised learning, there is no information on the classes that the examples
    belong to, or on the output corresponding to a given input. We want a model that
    can discover interesting properties, such as groups with similar characteristics,
    which happens in **clustering**. An example of the application of these algorithms
    is a search engine. These applications are able to create a list of links related
    to our search, starting from one or more keywords.
  prefs: []
  type: TYPE_NORMAL
- en: These algorithms work by comparing data and looking for similarities or differences. The
    validity of these algorithms depends on the usefulness of the information they
    can extract from the database. Available data only concerns the set of features
    that describe each example.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common methods is clustering. You will have heard this term
    being used quite frequently; we mainly use it for data analysis when we want to
    find clusters in our data. These clusters are usually found by using a certain
    kind of similarity measure, such as the Euclidean distance. Unsupervised learning
    is used extensively in many fields, such as data mining, medical imaging, stock
    market analysis, computer vision, and market segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering data using the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm is one of the most popular clustering algorithms. This
    algorithm is used to divide the input data into *k* subgroups using various attributes
    of the data. Grouping is achieved using an optimization technique where we try
    to minimize the sum of squares of distances between the datapoints and the corresponding
    centroid of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the k-means algorithm to group the data into four
    clusters identified by the relative centroid. We will also be able to trace the
    boundaries to identify the areas of relevance of each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to perform a clustering data analysis using the k-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `kmeans.py` file that has already
    been provided to you. Now let''s take a look at how it''s built. Create a new
    Python file, and import the following packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s load the input data and define the number of clusters. We will use
    the `data_multivar.txt` file that has already been provided to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to see what the input data looks like. Let''s go ahead and add the
    following lines of code to the Python file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f52083d0-7d25-42dd-ad1c-8a8e09e83f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now ready to train the model. Let''s initialize the `kmeans` object
    and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is trained, we need to visualize the boundaries. Let''s go
    ahead and add the following lines of code to the Python file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We just evaluated the model across a grid of points. Let''s plot these results
    to view the boundaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s overlay `centroids` on top of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c419f851-27a7-4d45-adca-58f674a4ca79.png)'
  prefs: []
  type: TYPE_IMG
- en: The four centroids and their boundaries are sufficiently highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means was developed by James MacQueen, who, in 1967, designed it for the purpose
    of dividing groups of objects into *k* partitions based on their attributes. It
    is a variation of the **expectation-maximization** (**EM**) algorithm, whose objective
    is to determine the groups of k data generated by Gaussian distributions. The
    difference between the two algorithms lies in the Euclidean distance calculation
    method. In k-means, it is assumed that the attributes of the object can be represented
    as vectors, and thus form a vector space. The goal is to minimize the total intra-cluster
    variance (or standard deviation). Each cluster is identified by a centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm follows an iterative procedure, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the number of *k* clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initially, create *k* partitions and assign each entry partition either randomly,
    or by using some heuristic information
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the centroid of each group
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distance between each observation and each cluster centroid
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, construct a new partition by associating each entry point with the cluster
    whose centroid is closer to it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The centroid for new clusters is recalculated
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 4 to 6 until the algorithm converges
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of the algorithm is to locate *k* centroids, one for each cluster.
    The position of each centroid is of particular importance as different positions
    cause different results. The best choice is to put them as far apart as possible
    from each other. When this is done, you must associate each object with the nearest
    centroid. In this way, we will get a first grouping. After finishing the first
    cycle, we go to the next one by recalculating the new *k* centroids as the cluster's
    barycenter using the previous one. Once you locate these new *k* centroids, you
    need to make a new connection between the same dataset and the new closest centroid.
    At the end of these operations, a new cycle is performed. Due to this cycle, we
    can note that the *k* centroids change their position step by step until they
    are modified. So, the centroid no longer moves.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `sklearn.cluster.KMeans` function: [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *MATLAB for Machine Learning,* Giuseppe Ciaburro, Packt Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *K Means* (from Stanford University): [http://stanford.edu/~cpiech/cs221/handouts/kmeans.html](http://stanford.edu/~cpiech/cs221/handouts/kmeans.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *K-means and Hierarchical Clustering* (by Andrew Moore): [https://www.autonlab.org/tutorials/kmeans.html](https://www.autonlab.org/tutorials/kmeans.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressing an image using vector quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main applications of k-means clustering is **vector quantization**.
    Simply speaking, vector quantization is the *N*-dimensional version of rounding
    off. When we deal with one-dimensional data, such as numbers, we use the rounding-off
    technique to reduce the memory needed to store that value. For example, instead
    of storing 23.73473572, we just store 23.73 if we want to be accurate up to the
    second decimal place. Or, we can just store 24 if we don't care about decimal
    places. It depends on our needs and the trade-off that we are willing to make.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when we extend this concept to *N*-dimensional data, it becomes vector
    quantization. Of course, there are more nuances to it! Vector quantization is
    popularly used in image compression where we store each pixel using fewer bits
    than the original image to achieve compression.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use a sample image and then we will compress the image
    further by reducing the number of bits.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to compress an image using vector quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `vector_quantization.py` file
    that has already been provided to you. Let''s take a look at how it''s built.
    We''ll start by importing the required packages. Create a new Python file, and
    add the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a function to parse the input arguments. We will be able to pass
    the image and the number of bits per pixel as input arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a function to compress the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we compress the image, we need to see how it affects the quality. Let''s
    define a function to plot the output image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to use all these functions. Let''s define the main function
    that takes the input arguments, processes them, and extracts the output image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compress this image using the input argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to run the code; run the following command on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The input image looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6181237a-5430-43f0-a546-4ee71d70d93d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You should get a compressed image as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49e1ef5c-cf08-4c49-ac10-998b7b994ed2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compress the image further by reducing the number of bits to `2`. Run
    the following command on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following compressed image as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd4dc96c-a349-4d91-a4f6-09dff2fc2014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you reduce the number of bits to `1`, you can see that it will become a
    binary image with black and white as the only two colors. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce68373c-4432-44be-bee8-62aa506869df.png)'
  prefs: []
  type: TYPE_IMG
- en: We have seen how, by compressing the image further, the quality of the image
    has undergone considerable downsizing.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector quantization is an algorithm used for signal compression, image coding,
    and speech. We use geometric criteria (the Euclidean distance) to find clusters.
    It is, therefore, an example of unsupervised training. It is a technique that
    allows the modeling of probability density functions through the distribution
    of prototype vectors. Vector quantization divides a large set of points (vectors)
    into clusters by using a similar number of points closer to them. Each cluster
    is illustrated by its centroid point (as in k-means).
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The vector quantization algorithm can be used to divide a dataset into a number
    of clusters. The algorithm is based on the calculation of the Euclidean distance
    for the allocation of the samples to the cluster, to which it belongs. The algorithm
    consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning, all the vectors are assigned to the same cluster, whose centroid
    is calculated as the mean value of all the vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each centroid, a perturbation is introduced that generates two new cluster
    centers. The old representative is discarded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each carrier is reassigned to one of the new clusters according to the minimum
    distance criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new representatives are calculated as the average value of the vectors assigned
    to each cluster. These will be the new centers of the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the end criterion is met, the algorithm terminates. If not, return to step
    2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `sklearn.cluster.KMeans` function: [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Image Compression Using Vector Quantization Algorithms: A Review*:
    [https://pdfs.semanticscholar.org/24d2/db6db81f1000b74246d22641e83390fb1065.pdf](https://pdfs.semanticscholar.org/24d2/db6db81f1000b74246d22641e83390fb1065.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the *Argparse Tutorial*: [https://docs.python.org/2/howto/argparse.html](https://docs.python.org/2/howto/argparse.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `scipy.misc.imread` function: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping data using agglomerative clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we talk about agglomerative clustering, we need to understand **hierarchical
    clustering**. Hierarchical clustering refers to a set of clustering algorithms
    that creates tree-like clusters by consecutively splitting or merging them, and
    they are represented using a tree. Hierarchical clustering algorithms can be either
    bottom-up or top-down. Now, what does this mean? In bottom-up algorithms, each
    datapoint is treated as a separate cluster with a single object. These clusters
    are then successively merged until all the clusters are merged into a single giant
    cluster. This is called **agglomerative clustering**. On the other hand, top-down
    algorithms start with a giant cluster and successively split these clusters until
    individual datapoints are reached.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In hierarchical clustering, we construct clusters by partitioning the instances recursively using a
    top-down or bottom-up fashion. We can divide these methods as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agglomerative algorithm** (bottom-up): Here, we obtain the solution from
    individual statistical units. At each iteration, we aggregate the most closely-related
    statistical units and the procedure ends when a single cluster is formed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive algorithm** (top-down): Here, all units are in the same class and
    the unit that is not similar to others is added to a new cluster for each subsequent
    iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both methods result in a dendrogram. This represents a nested group of objects,
    and the similarity levels at which the groups change. By cutting the dendrogram
    at the desired similarity level, we can get a clustering of data objects. The
    merging or division of clusters is performed using a similarity measure, which optimizes
    a criterion.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to group data using agglomerative clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `agglomerative.py` file that''s
    provided to you. Now let''s look at how it''s built. Create a new Python file,
    and import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the function that we need to perform agglomerative clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the labels and specify the shapes of the markers for the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through the datapoints and plot them accordingly using different markers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to demonstrate the advantage of agglomerative clustering, we need
    to run it on datapoints that are linked spatially, but also located close to each
    other in space. We want the linked datapoints to belong to the same cluster, as
    opposed to datapoints that are just spatially close to each other. Let''s, now
    define a function to get a set of datapoints on a spiral:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous function, we added some noise to the curve because it adds
    some uncertainty. Let''s define this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s define another function to get datapoints located on a rose curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to add more variety, let''s also define a `hypotrochoid` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to define the main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output if we don''t use any
    connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1dce003-053b-48c0-a77a-37baa6c1acef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second output diagram looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9229b70e-c4ed-468d-b2f1-f545c25cf904.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, using the connectivity feature enables us to group the datapoints
    that are linked to each other as opposed to clustering them, based on their spatial
    locations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In agglomerative clustering, each observation begins in its cluster and the
    clusters are subsequently combined. The strategies for joining the clusters are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ward clustering minimizes the sum of squared differences within all the clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum or complete linkage is used to minimize the maximum distance between
    observations of pairs of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average linkage is used to minimize the average of the distances between all
    observations of pairs of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single linkage is used to minimize the distance between the closest observations
    of pairs of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To decide what clusters must be combined, it is necessary to define a measure
    of dissimilarity between the clusters. In most hierarchical clustering methods,
    specific metrics are used to quantify the distance between two pairs of elements,
    and a linking criterion that defines the dissimilarity of two sets of elements
    (clusters) as a function of the distance between pairs of elements in the two
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'These common metrics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The uniform rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mahalanobis distance, which corrects data by different scales and correlations
    in variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The angle between the two vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hamming distance, which measures the minimum number of substitutions required
    to change one member into another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.cluster.AgglomerativeClustering`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Hierarchical agglomerative clustering* (from Stanford University):
    [https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have built different clustering algorithms, but haven't measured
    their performance. In supervised learning, the predicted values with the original
    labels are compared to calculate their accuracy. In contrast, in unsupervised
    learning, we have no labels, so we need to find a way to measure the performance
    of our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good way to measure a clustering algorithm is by seeing how well the clusters
    are separated. Are the clusters well separated? Are the datapoints in a cluster
    that is tight enough? We need a metric that can quantify this behavior. We will
    use a metric called the **silhouette coefficient** score. This score is defined
    for each datapoint; this coefficient is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5ad7de6-612e-4fb8-ba61-d236a35d0e99.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ***x*** is the average distance between the current datapoint and all
    the other datapoints in the same cluster, and ***y*** is the average distance
    between the current datapoint and all the datapoints in the next nearest cluster.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to evaluate the performance of clustering algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `performance.py` file that has
    already been provided to you. Now let''s look at how it''s built. Create a new
    Python file, and import the following packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the input data from the `data_perf.txt` file that has already been
    provided to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to determine the optimal number of clusters, let''s iterate through
    a range of values and see where it peaks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s plot the graph to see where it peaked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The bar graph looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b2c8314-3f41-4f14-a81c-ef837e330f53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As with these scores, the best configuration is five clusters. Let''s see what
    the data actually looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/590cfb57-6f65-4c5a-a6bb-9dcf70fbb065.png)'
  prefs: []
  type: TYPE_IMG
- en: We can visually confirm that the data, in fact, has five clusters. We just took
    the example of a small dataset that contains five distinct clusters. This method
    becomes very useful when you are dealing with a huge dataset that contains high-dimensional
    data that cannot be visualized easily.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sklearn.metrics.silhouette_score` function computes the mean silhouette
    coefficient of all the samples. For each sample, two distances are calculated:
    the mean intra-cluster distance (***x***), and the mean nearest-cluster distance
    (***y***). The silhouette coefficient for a sample is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29e29ff3-2b2f-40df-b587-5f0a192a6547.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, **y** is the distance between a sample and the nearest cluster
    that does not include the sample.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best value is 1, and the worst value is -1\. 0 represents clusters that
    overlap, while values of less than 0 mean that that particular sample has been
    attached to the wrong cluster.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.metrics.silhouette_score`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Silhouette* (from Wikipedia):[https://en.wikipedia.org/wiki/Silhouette_(clustering)](https://en.wikipedia.org/wiki/Silhouette_(clustering))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the number of clusters using the DBSCAN algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we discussed the k-means algorithm, we saw that we had to give the number
    of clusters as one of the input parameters. In the real world, we won't have this
    information available. We can definitely sweep the parameter space to find out
    the optimal number of clusters using the silhouette coefficient score, but this
    will be an expensive process! A method that returns the number of clusters in
    our data will be an excellent solution to the problem. DBSCAN does just that for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will perform a DBSCAN analysis using the `sklearn.cluster.DBSCAN`
    function. We will use the same data that we used in the previous *Evaluating the
    performance of clustering algorithms* (`data_perf.txt`) recipe, to compare the
    two methods used.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to automatically estimate the number of clusters using the DBSCAN
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `estimate_clusters.py` file that
    has already been provided to you. Now let''s look at how it''s built. Create a
    new Python file, and import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the input data from the `data_perf.txt` file. This is the same file that
    we used in the previous recipe, which will help us to compare the methods on the
    same dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to find the best parameter, so let''s initialize a few variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s sweep the parameter space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For each iteration, we need to extract the performance metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to store the best score and its associated epsilon value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now plot the bar graph, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s store the best models and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Some datapoints may remain unassigned. We need to identify them, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the number of clusters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to extract all the core samples, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the resultant clusters. We will start by extracting the set
    of unique labels and specifying different markers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s iterate through the clusters and plot the datapoints using different
    markers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following bar graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2c69ed2-a6e1-4d57-ab31-461187a0346c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the labeled datapoints, along with unassigned datapoints
    marked by solid points in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de991d06-5c2c-464e-a4d3-d4529a12031c.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DBSCAN works by treating datapoints as groups of dense clusters. If a point
    belongs to a cluster, then there should be a lot of other points that belong to
    the same cluster. One of the parameters that we can control is the maximum distance
    of this point from other points. This is called **epsilon**. No two points in
    a given cluster should be further away than epsilon. One of the main advantages
    of this method is that it can deal with outliers. If there are some points located
    alone in a low-density area, DBSCAN will detect these points as outliers as opposed
    to forcing them into a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DBSCAN presents the following pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: It does not require to know the number of a priori clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can find clusters of arbitrary forms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It requires only two parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: The quality of clustering depends on its distance measurement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not able to classify datasets with large differences in density.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.cluster.DBSCAN` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *DBSCAN* (from Wikipedia):[https://en.wikipedia.org/wiki/DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Density-based Methods* (from the University at Buffalo): [https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf](https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding patterns in stock market data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can use unsupervised learning for stock market analysis. Since
    we don't know how many clusters there are, we'll use an algorithm called **affinity
    propagation** (**AP**) on the cluster. It tries to find a representative datapoint
    for each cluster in our data, along with measures of similarity between pairs
    of datapoints, and considers all our datapoints as potential representatives,
    also called **exemplars**, of their respective clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will analyze the stock market variations of companies over
    a specified duration. Our goal is to then find out what companies behave similarly
    in terms of their quotes over time.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to find patterns in stock market data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `stock_market.py` file that has
    already been provided to you. Now let''s look at how it''s built. Create a new
    Python file, and import the following packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a file that contains all the symbols and the associated names. This
    information is located in the `symbol_map.json` file provided to you. Let''s load
    this, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read the data from the `symbol_map.json` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s load the data. We will use an Excel file (`stock_market_data.xlsx`);
    this is a multisheet file, one for each symbol:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'As we need some feature points for analysis, we will use the difference between
    the opening and closing quotes every day to analyze the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build a graph model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to standardize the data before we use it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s train the model using this data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to build the clustering model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Eight clusters are identified. From an initial analysis, we can see that the
    grouped companies seem to treat the same products: IT, banks, engineering, detergents,
    and computers.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AP is a clustering algorithm based on the concept of passing messages between
    points (item). Unlike clustering algorithms such as k-means, AP does not require
    the cluster number to be defined a priori. AP searches for representative members
    (exemplars) of the set of inputs, which are, in fact, representative of the individual
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The central point of the AP algorithm is the identification of a subset of exemplars.
    In the input, a matrix of similarity is taken between pairs of data. The data
    exchanges real values as messages until suitable specimens emerge, and consequently,
    good clusters are obtained.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform AP clustering, the `sklearn.cluster.affinity_propagation()` function
    was used. In the case of training samples with similar similarities and preferences,
    the assignment of cluster centers and labels depends on preference. If the preference
    is less than the similarities, a single cluster center and a 0 label for each
    sample will be returned. Otherwise, each training sample becomes its cluster center
    and a unique mark is assigned.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the `sklearn.cluster.affinity_propagation()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.affinity_propagation.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.affinity_propagation.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *AFFINITY PROPAGATION: CLUSTERING DATA BY PASSING MESSAGES* (from
    Toronto University): [http://www.cs.columbia.edu/~delbert/docs/DDueck-thesis_small.pdf](http://www.cs.columbia.edu/~delbert/docs/DDueck-thesis_small.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a customer segmentation model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main applications of unsupervised learning is market segmentation.
    This is when we don't have labeled data available all the time, but it's important
    to segment the market so that people can target individual groups. This is very
    useful in advertising, inventory management, implementing strategies for distribution,
    and mass media. Let's go ahead and apply unsupervised learning to one such use case
    to see how it can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be dealing with a wholesale vendor and his customers. We will be using
    the data available at [https://archive.ics.uci.edu/ml/datasets/Wholesale+customers](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers).
    The spreadsheet contains data regarding the consumption of different types of
    items by their customers and our goal is to find clusters so that they can optimize
    their sales and distribution strategy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build a customer segmentation model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `customer_segmentation.py` file
    that has already been provided to you. Now let''s look at how it''s built. Create
    a new Python file, and import the following packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the input data from the `wholesale.csv` file that''s already provided
    to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build a mean shift model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print the centroids of clusters that we obtained, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize a couple of features to get a sense of the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a543761c-e0ec-4210-9eb4-6e34a1859023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will get the following output that depicts the centroids for the features, *milk*
    and *groceries,* where milk is on the *x* axis and groceries is on the *y* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8db5b4ab-144e-41b1-99ed-35e5e3204271.png)'
  prefs: []
  type: TYPE_IMG
- en: In this output, the eight centroids of the identified clusters are clearly represented.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we have faced a clustering problem by using the mean shift algorithm.
    It is a clustering type that assigns datapoints to clusters in an iterative manner
    by moving points to the mode. The mode is the value that appears most frequently.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm assigns iteratively each data point to the centroid of the nearest
    cluster. The centroid of the nearest cluster is determined by where most of the
    neighboring points are located. Thus, at each iteration, each data point approaches
    the point where the greatest number of points is located, which is, or will lead
    to, the cluster center. When the algorithm stops, each point is assigned to a
    cluster. Unlike the k-means algorithm, the mean shift algorithm is not required
    in advance to specify the number of clusters; this is determined automatically
    by the algorithm. The mean shift algorithm is widely used in the field of image
    processing and artificial vision.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform the mean shift clustering, a `sklearn.cluster.MeanShift()` function
    was used. This function carries out a mean shift clustering using a flat kernel.
    The mean shift clustering allows us to identify point aggregates in a uniform
    density of samples. Candidates for the centroids are updated with the average
    of points within a given region. These points are then filtered in a postprocessing
    phase to eliminate possible duplicates to form the final set of centroids.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.cluster.MeanShift()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn-cluster-meanshift](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn-cluster-meanshift)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Mean Shift: A Robust Approach Toward Feature Space Analysis* (by
    Dorin Comaniciu , Peter Meer): [https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf](https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using autoencoders to reconstruct handwritten digit images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An autoencoder is a neural network whose purpose is to code its input into
    small dimensions, and for the result that is obtained to be able to reconstruct
    the input itself. Autoencoders are made up by the union of the following two subnets:
    encoder and decoder. A loss function is added to these functions and it is calculated
    as the distance between the amount of information loss between the compressed
    representation of the data and the decompressed representation. The encoder and
    the decoder will be differentiable with respect to the distance function, so the
    parameters of the encoding and decoding functions can be optimized to minimize
    the loss of reconstruction, using the gradient stochastic.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Handwriting recognition** (**HWR**) is widely used in modern technology.
    The written text image can be taken offline from a piece of paper by optical scanning
    (**optical character recognition**, **OCR**), or intelligent word recognition.
    Calligraphy recognition shows the ability of a computer to receive and interpret
    input that can be understood by hand from sources such as paper documents, touchscreens,
    photographs, and other devices. HWR consists of various techniques that generally
    require OCR. However, a complete script recognition system also manages formatting,
    carries out correct character segmentation, and finds the most plausible words.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Modified National Institute of Standards and Technology** (**MNIST**)
    is a large database of handwritten digits. It has a set of 70,000 examples of
    data. It is a subset of MNIST's larger dataset. The digits are of 28 x 28 pixel
    resolution and are stored in a matrix of 70,000 rows and 785 columns; 784 columns
    form each pixel value from the 28 x 28 matrix, and one value is the actual digit.
    The digits have been size-normalized and centered in a fixed-size image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to build autoencoders to reconstruct handwritten digit images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this recipe is given in the `AutoencMnist.py` file that has
    already been provided to you. Let''s look at how it''s built. Create a new Python
    file, and import the following package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the MNIST dataset, the following code must be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing the dataset, we have printed the shape of the data, and the
    following results are returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The 70,000 items in the database were divided into 60,000 items for training,
    and 10,000 items for testing. The data output is represented by integers in the
    range 0 to 9\. Let''s check it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'It may be useful to analyze the distribution of the two values in the available
    arrays. To start, we count the number of occurrences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see it in a graph, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the results obtained on both output datasets (`YTrain` and `YTest`),
    two histograms were traced and displayed side by side, as shown in the following
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0a15b2ac-7b88-48a3-85ec-65d2e28bcc5e.png)'
  prefs: []
  type: TYPE_IMG
- en: From the analysis of the previous output, we can see that in both datasets,
    the 10 digits are represented in the same proportions. In fact, the bars seem
    to have the same dimensions, even if the vertical axis has different ranges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we have to normalize all values between 0 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'To reduce the dimensionality, we will flatten the 28 x 28 images into vectors
    of size 784:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will build the model using the Keras functional API. Let''s start importing
    the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can build the Keras model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac5cbb23-0a56-4f4f-a28c-1f5537a38961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we have to configure the model for training. To do this, we will use the
    `compile` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can train the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is now ready, so we can use it to rebuild the handwritten digits
    automatically. To do this, we will use the `predict()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now finished; the model has been trained and will later be used to
    make predictions. So, we can just print the starting handwritten digits and those
    that were reconstructed from our model. Of course, we will do it only for some
    of the 60,000 digits contained in the dataset. In fact, we will limit ourselves
    to displaying the first five; we will also use the `matplotlib` library in this
    case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8aa1ffc4-1688-4590-9ec6-94b3f059c7f7.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding output, the result is very close to the original,
    meaning that the model works well.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An autoencoder is a neural network whose purpose is to code its input into small
    dimensions and the result obtained so as to be able to reconstruct the input itself.
    Autoencoders are made up of a union of the following two subnets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have an encoder that calculates the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f384c7f-ab17-490d-affa-84c1b17d111b.png)'
  prefs: []
  type: TYPE_IMG
- en: Given an *x* input, the encoder encodes it in a z variable, which is also called
    a latent variable. *z* usually has much smaller dimensions than *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, we have a decoder that calculates the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/defee6c0-e241-476b-bfdb-ad8bbf6ac0c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *z* is the code of *x* produced by the encoder, the decoder must decode
    it so that *x'* is similar to *x*. The training of autoencoders is intended to
    minimize the mean squared error between the input and the result.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is a Python library that provides a simple and clean way to create a range
    of deep learning models. The Keras code was released under the MIT license. Keras
    has been structured based on austerity and simplicity, and it provides a programming
    model without ornaments to maximize readability. It allows neural networks to
    be expressed in a very modular way, considering models as a sequence or a single
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the Keras library: [https://keras.io/](https://keras.io/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Keras 2.x Projects*, Giuseppe Ciaburro, Packt Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
