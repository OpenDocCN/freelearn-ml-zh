["```py\nimport cv2\n\ngray_img = cv2.cvtColor(cv2.imread('example.png'), cv2.COLOR_RGB2GRAY)\n\ncascade_clf = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\nfaces = cascade_clf.detectMultiScale(gray_img,\n                                     scaleFactor=1.1,\n                                     minNeighbors=3,\n                                     flags=cv2.CASCADE_SCALE_IMAGE)\n```", "```py\nfor (x, y, w, h) in faces: \n    # draw bounding box on frame \n    cv2.rectangle(frame, (x, y), (x + w, y + h), (100, 255, 0), \n                  thickness=2) \n```", "```py\nimport cv2 \nimport numpy as np \n\nclass FaceDetector:\n\n    def __init__(self, *,\n                 face_cascade='params/haarcascade_frontalface_default.xml',\n                 eye_cascade='params/haarcascade_lefteye_2splits.xml',\n                 scale_factor=4):\n```", "```py\n        # load pre-trained cascades\n        self.face_clf = cv2.CascadeClassifier(face_cascade)\n        if self.face_clf.empty():\n            raise ValueError(f'Could not load face cascade \n            \"{face_cascade}\"')\n```", "```py\n        self.eye_clf = cv2.CascadeClassifier(eye_cascade)\n        if self.eye_clf.empty():\n            raise ValueError(\n                f'Could not load eye cascade \"{eye_cascade}\"')\n```", "```py\nself.scale_factor = scale_factor \n```", "```py\n    def detect_face(self, rgb_img, *, outline=True):\n        frameCasc = cv2.cvtColor(cv2.resize(rgb_img, (0, 0),\n                                            fx=1.0 / \n                                            self.scale_factor,\n                                            fy=1.0 / \n                                            self.scale_factor),\n                                 cv2.COLOR_RGB2GRAY)\n```", "```py\n    faces = self.face_clf.detectMultiScale(\n            frameCasc,\n            scaleFactor=1.1,\n            minNeighbors=3,\n            flags=cv2.CASCADE_SCALE_IMAGE) * self.scale_factor\n```", "```py\n     for (x, y, w, h) in faces:\n            if outline:\n                cv2.rectangle(rgb_img, (x, y), (x + w, y + h), \n                              (100, 255, 0), thickness=2)\n```", "```py\n        head = cv2.cvtColor(rgb_img[y:y + h, x:x + w],\n                            cv2.COLOR_RGB2GRAY)\n```", "```py\n         return True, rgb_img, head, (x, y)\n```", "```py\n        return False, rgb_img, None, (None, None)\n```", "```py\n    def align_head(self, head):\n        desired_eye_x = 0.25\n        desired_eye_y = 0.2\n        desired_img_width = desired_img_height = 200\n```", "```py\n        try:\n            eye_centers = self.eye_centers(head)\n        except RuntimeError:\n            return False, head\n```", "```py\n        if eye_centers[0][0] < eye_centers[0][1]:\n            left_eye, right_eye = eye_centers\n        else:\n            right_eye, left_eye = eye_centers\n```", "```py\n        eye_angle_deg = 180 / np.pi * np.arctan2(right_eye[1] \n                                                 - left_eye[1],\n                                                 right_eye[0] \n                                                 - left_eye[0])\n```", "```py\n        eye_dist = np.linalg.norm(left_eye - right_eye)\n        eye_size_scale = (1.0 - desired_eye_x * 2) * \n        desired_img_width / eye_dist\n```", "```py\n        eye_midpoint = (left_eye + right_eye) / 2\n        rot_mat = cv2.getRotationMatrix2D(tuple(eye_midpoint), \n                                          eye_angle_deg,\n                                          eye_size_scale)\n```", "```py\n        rot_mat[0, 2] += desired_img_width * 0.5 - eye_midpoint[0]\n        rot_mat[1, 2] += desired_eye_y * desired_img_height - \n        eye_midpoint[1]\n```", "```py\n        res = cv2.warpAffine(head, rot_mat, (desired_img_width,\n                                             desired_img_width))\n        return True, res\n```", "```py\n$ python chapter8.py collect\n```", "```py\n$ python chapter8.py demo\n```", "```py\ndef run_layout(layout_cls, **kwargs):\n    # open webcam\n    capture = cv2.VideoCapture(0)\n    # opening the channel ourselves, if it failed to open.\n    if not(capture.isOpened()):\n        capture.open()\n\n    capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n\n    # start graphical user interface\n    app = wx.App()\n    layout = layout_cls(capture, **kwargs)\n    layout.Center()\n    layout.Show()\n    app.MainLoop()\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('mode', choices=['collect', 'demo'])\n    parser.add_argument('--classifier')\n    args = parser.parse_args()\n```", "```py\n    if args.mode == 'collect':\n        run_layout(DataCollectorLayout, title='Collect Data')\n    elif args.mode == 'demo':\n        assert args.svm is not None, 'you have to provide --svm'\n        run_layout(FacialExpressionRecognizerLayout,\n                   title='Facial Expression Recognizer',\n                   classifier_path=args.classifier)\n```", "```py\nimport wx\nfrom wx_gui import BaseLayout\n\nclass DataCollectorLayout(BaseLayout):\n```", "```py\n    def __init__(self, *args,\n                 training_data='data/cropped_faces.csv',\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n```", "```py\n        self.face_detector = FaceDetector(\n            face_cascade='params/haarcascade_frontalface_default.xml',\n            eye_cascade='params/haarcascade_eye.xml')\n        self.training_data = training_data\n```", "```py\n    def augment_layout(self):\n        pnl2 = wx.Panel(self, -1)\n        self.neutral = wx.RadioButton(pnl2, -1, 'neutral', (10, 10),\n                                      style=wx.RB_GROUP)\n        self.happy = wx.RadioButton(pnl2, -1, 'happy')\n        self.sad = wx.RadioButton(pnl2, -1, 'sad')\n        self.surprised = wx.RadioButton(pnl2, -1, 'surprised')\n        self.angry = wx.RadioButton(pnl2, -1, 'angry')\n        self.disgusted = wx.RadioButton(pnl2, -1, 'disgusted')\n        hbox2 = wx.BoxSizer(wx.HORIZONTAL)\n        hbox2.Add(self.neutral, 1)\n        hbox2.Add(self.happy, 1)\n        hbox2.Add(self.sad, 1)\n        hbox2.Add(self.surprised, 1)\n        hbox2.Add(self.angry, 1)\n        hbox2.Add(self.disgusted, 1)\n        pnl2.SetSizer(hbox2)\n```", "```py\n        # create horizontal layout with single snapshot button\n        pnl3 = wx.Panel(self, -1)\n        self.snapshot = wx.Button(pnl3, -1, 'Take Snapshot')\n        self.Bind(wx.EVT_BUTTON, self._on_snapshot, self.snapshot)\n        hbox3 = wx.BoxSizer(wx.HORIZONTAL)\n        hbox3.Add(self.snapshot, 1)\n        pnl3.SetSizer(hbox3)\n```", "```py\n        # arrange all horizontal layouts vertically\n        self.panels_vertical.Add(pnl2, flag=wx.EXPAND | wx.BOTTOM, \n                                 border=1)\n        self.panels_vertical.Add(pnl3, flag=wx.EXPAND | wx.BOTTOM, \n                                 border=1)\n```", "```py\n    def process_frame(self, frame_rgb: np.ndarray) -> np.ndarray:\n        _, frame, self.head, _ = self.face_detector.detect_face(frame_rgb)\n        return frame\n```", "```py\n    def _on_snapshot(self, evt):\n        \"\"\"Takes a snapshot of the current frame\n\n           This method takes a snapshot of the current frame, preprocesses\n           it to extract the head region, and upon success adds the data\n           sample to the training set.\n        \"\"\"\n```", "```py\n        if self.neutral.GetValue():\n            label = 'neutral'\n        elif self.happy.GetValue():\n            label = 'happy'\n        elif self.sad.GetValue():\n            label = 'sad'\n        elif self.surprised.GetValue():\n            label = 'surprised'\n        elif self.angry.GetValue():\n            label = 'angry'\n        elif self.disgusted.GetValue():\n            label = 'disgusted'\n```", "```py\n        if self.head is None:\n            print(\"No face detected\")\n        else:\n            success, aligned_head = \n            self.face_detector.align_head(self.head)\n```", "```py\n            if success:\n                save_datum(self.training_data, label, aligned_head)\n                print(f\"Saved {label} training datum.\")\n            else:\n                print(\"Could not align head (eye detection \n                                             failed?)\")\n```", "```py\ndef save_datum(path, label, img):\n    with open(path, 'a', newline='') as outfile:\n        writer = csv.writer(outfile)\n        writer.writerow([label, img.tolist()])\n```", "```py\ndef load_collected_data(path):\n    data, targets = [], []\n    with open(path, 'r', newline='') as infile:\n        reader = csv.reader(infile)\n        for label, sample in reader:\n            targets.append(label)\n            data.append(json.loads(sample))\n    return data, targets\n```", "```py\ndef pca_featurize(data) -> (np.ndarray, List)\n```", "```py\ndef _pca_featurize(data, center, top_vecs):\n    return np.array([np.dot(top_vecs, np.array(datum).flatten() - center)\n                     for datum in data]).astype(np.float32)\n```", "```py\ndef pca_featurize(training_data) -> (np.ndarray, List)\n```", "```py\nx_arr = np.array(training_data).reshape((len(training_data), -1)).astype(np.float32)\n```", "```py\n    mean, eigvecs = cv2.PCACompute(x_arr, mean=None)\n```", "```py\n    # Take only first num_components eigenvectors.\n    top_vecs = eigvecs[:num_components]\n```", "```py\n    center = mean.flatten()\n```", "```py\n    args = (center, top_vecs)\n    return _pca_featurize(training_data, *args), args\n```", "```py\n    mlp = cv2.ml.ANN_MLP_create()\n```", "```py\n    mlp.setLayerSizes(np.array([20, 10, 6], dtype=np.uint8)\n```", "```py\n    mlp.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP, 0.1)\n    mlp.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM)\n```", "```py\n    mlp.setTermCriteria((cv2.TERM_CRITERIA_COUNT | \n                         cv2.TERM_CRITERIA_EPS, 30, 0.000001 ))\n```", "```py\ndef train_test_split(n, train_portion=0.8):\n```", "```py\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n```", "```py\n    N = int(n * train_portion)\n```", "```py\n    return indices[:N], indices[N:]\n```", "```py\n    train, test = train_test_split(len(data), 0.8)\n    x_train, pca_args = pca_featurize(np.array(data)[train])\n```", "```py\n    encoded_targets, index_to_label = one_hot_encode(targets)\n    y_train = encoded_targets[train]\n    mlp.train(x_train, cv2.ml.ROW_SAMPLE, y_train)\n```", "```py\ndef one_hot_encode(all_labels) -> (np.ndarray, Callable):\n    unique_lebels = list(sorted(set(all_labels)))\n```", "```py\n    y = np.zeros((len(all_labels), len(unique_lebels))).astype(np.float32)\n```", "```py\n    index_to_label = dict(enumerate(unique_lebels))\n    label_to_index = {v: k for k, v in index_to_label.items()\n```", "```py\n    for i, label in enumerate(all_labels):\n        y[i, label_to_index[label]] = 1\n```", "```py\n    return y, index_to_label\n```", "```py\n     x_test = _pca_featurize(np.array(data)[test], *pca_args)\n```", "```py\n_, predicted = mlp.predict(x_test)\n    y_hat = np.array([index_to_label[np.argmax(y)] for y \n    in predicte\n```", "```py\n    y_true = np.array(targets)[test]\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', required=True)\n    parser.add_argument('--save', type=Path)\n    args = parser.parse_args()\n```", "```py\n    data, targets = load_collected_data(args.data)\n\n    mlp = cv2.ml.ANN_MLP_create()\n    ...\n    mlp.train(...\n```", "```py\n    if args.save:\n        print('args.save')\n        x_all, pca_args = pca_featurize(np.array(data))\n        mlp.train(x_all, cv2.ml.ROW_SAMPLE, encoded_targets)\n        mlp.save(str(args.save / 'mlp.xml'))\n        pickle_dump(index_to_label, args.save / 'index_to_label')\n        pickle_dump(pca_args, args.save / 'pca_args')\n```", "```py\n $ python chapter8.py demo --classifier data/clf1\n```", "```py\nclass FacialExpressionRecognizerLayout(BaseLayout):\n    def __init__(self, *args,\n                 clf_path=None,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n```", "```py\nself.clf = cv2.ml.ANN_MLP_load(str(clf_path / 'mlp.xml'))\n```", "```py\n        self.index_to_label = pickle_load(clf_path \n                                          / 'index_to_label')\n        self.pca_args = pickle_load(clf_path / 'pca_args')\n```", "```py\n        self.face_detector = FaceDetector(\n            face_cascade='params/\n            haarcascade_frontalface_default.xml',\n            eye_cascade='params/haarcascade_lefteye_2splits.xml')\n```", "```py\n   def process_frame(self, frame_rgb: np.ndarray) -> np.ndarray:\n        success, frame, self.head, (x, y) = \n        self.face_detector.detect_face(frame_rgb)\n```", "```py\n        if not success:\n            return frame\n```", "```py\n        success, head = self.face_detector.align_head(self.head)\n        if not success:\n            return frame\n```", "```py\n        _, output = self.clf.predict(self.featruize_head(head))\n        label = self.index_to_label[np.argmax(output)]\n```", "```py\n        cv2.putText(frame, label, (x, y - 20),\n                    cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n\n        return frame\n```", "```py\n    def featurize_head(self, head):\n        return _pca_featurize(head[None], *self.pca_args)\n```"]