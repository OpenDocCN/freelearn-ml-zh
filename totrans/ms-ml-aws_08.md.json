["```py\npoi_df = spark.read.csv(SRC_PATH + 'data-sigir17/poiList-sigir17', \n                        header=True, inferSchema=True, sep=';')\n```", "```py\nvisits_path = SRC_PATH+'data-sigir17/userVisits-sigir17'\nvisits_df = spark.read.csv(visits_path, \n                           header=True,\n                           inferSchema=True, sep=';')\n```", "```py\nsample_df = visits_df.limit(1000).toPandas()\nsample_df.describe()\n```", "```py\ncount 1000\n unique 36\n top 10182842@N08\n freq 365\n```", "```py\npoi_df.createOrReplaceTempView('points')\nvisits_df.createOrReplaceTempView('visits')\n```", "```py\nspark.sql('select distinct poiID from visits').count()\n31\n```", "```py\nspark.sql('select nsid,count(distinct poiID) as cnt from visits group by nsid').describe().show()\n```", "```py\nspark.sql('select nsid,poiID,count(*) from visits group by nsid,poiID').describe().show()\n```", "```py\ntrain_df = spark.sql('select hash(nsid) as user_hash_id, poiID, count(*) as pictures_taken from visits group by 1,2')\n```", "```py\nfrom pyspark.ml.recommendation import ALS\n\nrecommender = ALS(userCol=\"user_hash_id\", \n                  itemCol=\"poi_hash_id\", \n                  ratingCol=\"pictures_taken\", \n                  coldStartStrategy=\"drop\")\n\nmodel = recommender.fit(train_df)\n```", "```py\nrecommendations = model.recommendForAllUsers(10)\n```", "```py\nrow_list = spark.sql('select distinct p.poiName, p.poiID from visits v join points p on (p.poiID=v.poiID) ').collect()\nid_to_poi_name = dict(map(lambda x: (x.poiID, x.poiName), row_list))\n```", "```py\n{1: 'Test Track',\n 10: 'Golden Zephyr',\n 19: \"Tarzan's Treehouse\",\n 22: 'Country Bear Jamboree'\n ....\n }\n```", "```py\ndef poi_names(recommendations, visited_pois):\n   visited_set = set([id_to_poi_name[poi] for poi in visited_pois])\n   recommended = str([(id_to_poi_name[poi], weight) \\\n                      for (poi,weight) in recommendations\n                      if id_to_poi_name[poi] not in visited_set])\n   return \"recommended: %s ; visited: %s \"%(recommended, visited_set)\n\nspark.udf.register(\"poi_names\", poi_names)\n```", "```py\nrecommendations.createOrReplaceTempView('recommendations')\n\nrecommendation_sample = spark.sql('select user_hash_id, collect_list(poiID), poi_names(max(recommendations), collect_list(poiID)) as recommendation from recommendations r join visits v on (r.user_hash_id = hash(v.nsid)) group by 1')\\\n   .sample(fraction=0.1, withReplacement=False) \\\n   .collect()\n\n```", "```py\nprint(recommendation_sample[0].recommendation)\n\nrecommended: [(\"It's A Small World\", 31.352962493896484), ('Walt Disney World Railroad', 23.464025497436523), ('Pirates of the Caribbean', 21.36219596862793), ('Buzz Lightyear Astro Blasters', 17.21680450439453), ('Haunted Mansion', 15.873616218566895), ('Country Bear Jamboree', 9.63521957397461), ('Astro Orbiter', 9.164801597595215), ('The Great Movie Ride', 8.167647361755371)] ; visited: {\"California Screamin'\", 'Sleeping Beauty Castle Walkthrough', 'Voyage of The Little Mermaid', \"Tarzan's Treehouse\", 'Main Street Cinema', 'The Many Adventures of Winnie the Pooh', 'Jungle Cruise', 'Tom Sawyer Island', 'Test Track', 'The Twilight Zone Tower of Terror'}\n```", "```py\nprint(recommendation_sample[200].recommendation)\n\nrecommended: [('Splash Mountain', 0.9785523414611816), ('Sleeping Beauty Castle Walkthrough', 0.8383632302284241), (\"Pinocchio's Daring Journey\", 0.7456990480422974), ('Journey Into Imagination With Figment', 0.4501221477985382), (\"California Screamin'\", 0.44446268677711487), ('Tom Sawyer Island', 0.41949236392974854), (\"It's A Small World\", 0.40130260586738586), ('Astro Orbiter', 0.37899214029312134), ('The Twilight Zone Tower of Terror', 0.3728359639644623)] ; visited: {\"Snow White's Scary Adventures\"}\n\nprint(recommendation_sample[600].recommendation)\n\nrecommended: [('Fantasmic!', 20.900590896606445), ('Pirates of the Caribbean', 9.25596809387207), (\"It's A Small World\", 8.825133323669434), ('Buzz Lightyear Astro Blasters', 5.474684715270996), ('Main Street Cinema', 5.1001691818237305), ('Country Bear Jamboree', 4.3145904541015625), (\"California Screamin'\", 3.717888832092285), (\"It's A Small World\", 3.6027705669403076), ('The Many Adventures of Winnie the Pooh', 3.429044246673584)] ; visited: {'Haunted Mansion', 'The Twilight Zone Tower of Terror', 'Journey Into Imagination With Figment'}\n```", "```py\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import QuantileDiscretizer\nfrom pyspark.ml.feature import VectorAssembler\n\npipeline = Pipeline(stages = [\n   StringIndexer(inputCol='user_hash_id', \n                 outputCol=\"user_hash_id_index\", \n                 handleInvalid='keep'),\n   OneHotEncoder(inputCol='user_hash_id_index', \n                 outputCol='user_hash_id_encoded'),\n   StringIndexer(inputCol='poiID', \n                 outputCol='poi_id_indexed', \n                 handleInvalid='keep'),\n   OneHotEncoder(inputCol='poi_id_indexed', \n                 outputCol='poi_id_encoded'),\n   QuantileDiscretizer(numBuckets=5, \n                       inputCol='pictures_taken', \n                       outputCol='interest_level'),\n   VectorAssembler(inputCols=['poi_id_encoded', 'user_hash_id_encoded'],\n                   outputCol='features'),\n])\n\nmodel = pipeline.fit(train_df)\n```", "```py\nsparse_df = model.transform(train_df)\n```", "```py\nsagemaker_train_df.write.format(\"sagemaker\") \\\n  .option(\"labelColumnName\", \"interest_level\") \\\n  .option(\"featuresColumnName\", \"features\") \\\n  .save(\"s3://mastering-ml-aws/chapter6/train-data\")\n```", "```py\nfrom scipy.sparse import csr_matrix\nimport numpy as np\nimport boto3\nimport io\nimport numpy as np\nimport scipy.sparse as sp\nimport sagemaker.amazon.common as smac\n```", "```py\ndef spark_vector_to_sparse_matrix(row):\n   vect = row['features']\n   return csr_matrix((vect.values, vect.indices, np.array([0, vect.values.size])),\n                     (1, vect.size), \n                      dtype=np.float32)\n\ndef upload_matrices_to_s3(dataframe, dataset_name):\n   features_matrices = \n        dataframe.select(\"features\") \\\n                 .rdd.map(spark_vector_to_sparse_matrix).collect()\n   interest_levels = \n        dataframe.select(\"interest_level\") \\\n                 .rdd.map(lambda r: r['interest_level']).collect()\n\n   interest_level_vector = np.array(interest_levels, dtype=np.float32)\n   buffer = io.BytesIO()\n   smac.write_spmatrix_to_sparse_tensor(buffer, \\\n                                        sp.vstack(features_matrices), \\\n                                        interest_level_vector)\n   buffer.seek(0)\n   bucket = boto3.resource('s3').Bucket('mastering-ml-aws')\n   bucket.Object('chapter6/%s-data.protobuf'%dataset_name).upload_fileobj(buffer)\n```", "```py\nupload_matrices_to_s3(sagemaker_train_df, 'train')\nupload_matrices_to_s3(sagemaker_test_df, 'test')\n```", "```py\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport json\nimport boto3\n\nsess = sagemaker.Session()\nrole = get_execution_role()\ncontainer = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', \n     \"factorization-machines\", \n     \"latest\")\n\ns3_train_data = 's3://mastering-ml-aws/chapter6/train-data.protobuf'\ns3_test_data = 's3://mastering-ml-aws/chapter6/train-data.protobuf'\ns3_output_location = 's3://mastering-ml-aws/chapter6/sagemaker/output/'\n```", "```py\nfrom sagemaker.session import s3_input\n\nrecommender = sagemaker.estimator.Estimator(container,\n                                            role,\n                                            train_instance_count=1,\n                                            train_instance_type='ml.c4.xlarge',\n                                            output_path=s3_output_location,\n                                            sagemaker_session=sess)\n\nrecommender.set_hyperparameters(predictor_type='regressor',\n                                feature_dim=8934,\n                                epochs=200,\n                                mini_batch_size=100,\n                                num_factors=128)\n\nrecommender.fit({'train': s3_input(s3_train_data), \\\n                 'test': s3_input(s3_test_data)})\n```", "```py\n[02/23/2019 22:01:02 INFO 140697667364672] #test_score (algo-1) : ('rmse', 0.19088356774389661)\n2019-02-23 22:01:11 Uploading - Uploading generated training model\n 2019-02-23 22:01:11 Completed - Training job completed\n```", "```py\npredictor = recommender.deploy(instance_type='ml.c5.xlarge', initial_instance_count=1)\n```", "```py\nprint(recommendation_sample[1].user_hash_id)\n-525385694\n```", "```py\n\nsagemaker_test_df.select('features').where('user_hash_id=-525385694') \\\n                 .rdd.map(build_request).collect()\n\n[{'data': {'features': {'shape': [8934],\n   'keys': [4, 3297],\n   'values': [1.0, 1.0]}}}]\n```", "```py\ndef build_request(row):\n   vect = row['features']\n   return {'data':{ 'features': {'shape':[int(vect.size)], \n                                 'keys':list(map(int,vect.indices)),\n                                 'values':list(vect.values)}}}\n```", "```py\nimport json\n\npredictor.content_type = 'application/json'\npredictor.predict(json.dumps({'instances': [\n    {'data': {'features': {'shape': [8934], 'keys': [4, 3297], \n              'values': [1, 1]}}}]}))\n```", "```py\n{'predictions': [{'score': 0.8006305694580078}]}\n```", "```py\ndef predict_poi(poi_position):\n   prediction = predictor.predict( \n           json.dumps({'instances': [{'data': \n                        {'features': {'shape': [8934], \n                                      'keys': [poi_position, 3297], \n                                      'values': [1, 1]}}}]}))\n   return prediction['predictions'][0]['score']\n\npredictions = [(poi_position, predict_poi(poi_position)) for poi_position in range(0,31)]\npredictions.sort(key=lambda x:x[1], reverse=True)\n```", "```py\nuser_visited_pois = \n     [id_to_poi_name[x] for x in set(recommendation_sample[1]['collect_list(poiID)'])]\n\nfor (poi_position, score) in predictions[:10]:\n  recommended_poi = id_to_poi_name[int(model.stages[2].labels[poi_position])]\n  if recommended_poi not in user_visited_pois:\n       print(recommended_poi)\n```", "```py\nTest Track\n Walt Disney World Railroad\n Main Street Cinema\n Tom Sawyer Island\n Tarzan's Treehouse\n Mark Twain Riverboat\n Sleeping Beauty Castle Walkthrough\n Snow White's Scary Adventures\n```", "```py\nprint(recommendation_sample[1].recommendation)\nrecommended: [(\"Pinocchio's Daring Journey\", 3.278768539428711), ('Tom Sawyer Island', 2.78713321685791), ('Splash Mountain', 2.114530324935913), (\"Tarzan's Treehouse\", 2.06896710395813), ('Fantasmic!', 1.9648514986038208), (\"Snow White's Scary Adventures\", 1.8940000534057617), ('Main Street Cinema', 1.6671074628829956), ('Mark Twain Riverboat', 1.314055323600769), ('Astro Orbiter', 1.3135600090026855)] ; visited: {'The Many Adventures of Winnie the Pooh', 'Rose & Crown Pub Musician', 'Golden Zephyr', \"It's A Small World\"}\n```"]