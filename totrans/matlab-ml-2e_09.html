<html><head></head><body>
<div id="_idContainer098">
<h1 class="chapter-number" id="_idParaDest-180"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-181"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.2.1">Time Series Analysis and Forecasting with MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Time series data constitutes a sequence of measurements gathered over a certain period. </span><span class="koboSpan" id="kobo.3.2">These measurements, which are tied to a specific variable, occur at regular intervals. </span><span class="koboSpan" id="kobo.3.3">An essential characteristic of time series data lies in the significance of its order; the arrangement of observations on a timeline conveys meaningful patterns. </span><span class="koboSpan" id="kobo.3.4">Altering this order can completely reshape the data’s meaning. </span><span class="koboSpan" id="kobo.3.5">Sequential data is a broader concept that encompasses any data presented in a sequential manner, which includes time series data. </span><span class="koboSpan" id="kobo.3.6">In this chapter, we will delve into the fundamental concepts surrounding sequential data, elucidating how to construct models that capture patterns within time series or any </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">sequential data.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Exploring the basic concepts of time </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">series data</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Extracting statistics from </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">sequential data</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Implementing a model to predict stock </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">market data</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Dealing with imbalanced datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">in MATLAB</span></span></li>
</ul>
<h1 id="_idParaDest-182"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.15.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.16.1">In this chapter, we will introduce basic machine learning concepts. </span><span class="koboSpan" id="kobo.16.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.16.3">You will also require a working knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">of MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">To work with the MATLAB code in this chapter, you will need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.20.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.21.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.22.1">Nile.csv</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.23.1">TSLA.csv</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">TeslaStockForecasting.m</span></strong></span></li>
</ul>
<h1 id="_idParaDest-183"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.25.1">Exploring the basic concepts of time series data</span></h1>
<p><span class="koboSpan" id="kobo.26.1">A time series </span><a id="_idIndexMarker976"/><span class="koboSpan" id="kobo.27.1">represents a chronological sequence of recorded observations, such as monthly revenue, daily stock prices, weekly interest rates, annual profits, and more. </span><span class="koboSpan" id="kobo.27.2">The primary objective of time series analysis is to examine the historical progression of a phenomenon over time to anticipate its future trajectory. </span><span class="koboSpan" id="kobo.27.3">This predictive insight is derived from the assumption that recurring patterns observed in the past will continue to manifest in the future.  </span><span class="koboSpan" id="kobo.27.4">We will explore the concepts of predictive forecasting and various forecasting methodologies, providing a detailed description </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">of both.</span></span></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.29.1">Understanding predictive forecasting</span></h2>
<p><span class="koboSpan" id="kobo.30.1">Predicting the</span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.31.1"> trajectory of variables is immensely </span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.32.1">significant when it comes to formulating plans and policies for any endeavor. </span><span class="koboSpan" id="kobo.32.2">For instance, when planning a company’s production strategy, it isn’t sufficient to merely understand whether the demand for products or services is on the rise or declining. </span><span class="koboSpan" id="kobo.32.3">It’s imperative to forecast the future trends in product demand, pricing dynamics, and the costs of raw materials. </span><span class="koboSpan" id="kobo.32.4">Collectively, these elements wield substantial influence over </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">production activities.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">Forecasts hold a pivotal position at the core of the entire decision-making procedure. </span><span class="koboSpan" id="kobo.34.2">When forecasts are imprecise or insufficient, there exists a substantial risk of rendering the conclusions drawn from intricate decision models ineffective. </span><span class="koboSpan" id="kobo.34.3">The phrase </span><em class="italic"><span class="koboSpan" id="kobo.35.1">forecasting process</span></em><span class="koboSpan" id="kobo.36.1"> encompasses a range of intricate activities, whether overt or covert, that ultimately culminate in the creation of </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">a forecast.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">The terms </span><em class="italic"><span class="koboSpan" id="kobo.39.1">forecast</span></em><span class="koboSpan" id="kobo.40.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.41.1">prediction</span></em><span class="koboSpan" id="kobo.42.1"> are frequently used interchangeably, but it’s beneficial to differentiate their meanings. </span><span class="koboSpan" id="kobo.42.2">Forecasting involves associating probabilities with future events or defining confidence intervals to estimate the range of values likely to occur in the future. </span><span class="koboSpan" id="kobo.42.3">On the contrary, prediction entails identifying the exact value a measurable quantity will take on in the future. </span><span class="koboSpan" id="kobo.42.4">Consequently, it’s straightforward to connect forecasts with the predictions made by employing traditional inferential statistical tools to derive corresponding confidence intervals. </span><span class="koboSpan" id="kobo.42.5">The primary goal of all forecasting models is to determine an estimation of the anticipated value, coupled with an estimation of the potential error the forecasting model </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">may generate.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">The forecast horizon is a crucial factor that distinguishes the forecasting process significantly. </span><span class="koboSpan" id="kobo.44.2">Forecasts can pertain to the immediate future, spanning up to 12 months, and serving as a foundation for operational decision-making. </span><span class="koboSpan" id="kobo.44.3">This might involve predicting product demand for the next 2 months. </span><span class="koboSpan" id="kobo.44.4">Alternatively, forecasts may extend into the medium term, ranging from 12 to 24 months, supporting decisions related to production planning. </span><span class="koboSpan" id="kobo.44.5">In the third scenario, forecasts are aimed at a more distant future, beyond the 24-month mark. </span><span class="koboSpan" id="kobo.44.6">In this case, they are crafted to underpin managerial decisions concerning company development plans. </span><span class="koboSpan" id="kobo.44.7">Across these three scenarios, each characterized by a short, medium, or long prediction horizon, the objectives of decision-makers seeking to utilize these forecasts differ significantly. </span><span class="koboSpan" id="kobo.44.8">Likewise, the level of precision and detail</span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.45.1"> required for these respective </span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.46.1">forecasts </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">also varies.</span></span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.48.1">Introducing forecasting methodologies</span></h2>
<p><span class="koboSpan" id="kobo.49.1">Forecasting </span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.50.1">methodologies refer to the various</span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.51.1"> techniques and approaches that are used to predict future values, trends, or events based on historical data and patterns. </span><span class="koboSpan" id="kobo.51.2">These methodologies are essential in a wide range of fields, including economics, finance, business, and meteorology, among others. </span><span class="koboSpan" id="kobo.51.3">Forecasting methodologies vary primarily depending on the characteristics and intended purposes of the decisions they support. </span><span class="koboSpan" id="kobo.51.4">Factors such as the time horizon’s duration, the quality and consistency of historical data, and the specific attributes of the product being forecasted, such as its stage in the life cycle, play significant roles in determining the appropriate </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">forecasting method.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">In essence, forecasting methods can be categorized into two major groups: </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">quantitative methods</span></strong><span class="koboSpan" id="kobo.55.1"> and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.56.1">qualitative methods</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.58.1">Explaining quantitative forecasting methods</span></h3>
<p><span class="koboSpan" id="kobo.59.1">Quantitative </span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.60.1">methods are employed when there is sufficient quantitative data available, enabling predictions of future outcomes based on historical data. </span><span class="koboSpan" id="kobo.60.2">Typically, these methods find application in making short - to intermediate-term decisions. </span><span class="koboSpan" id="kobo.60.3">Here are some examples of quantitative </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">prediction methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Time series analysis</span></strong><span class="koboSpan" id="kobo.63.1">: In this method, the phenomenon to be predicted is treated as an opaque </span><a id="_idIndexMarker984"/><span class="koboSpan" id="kobo.64.1">entity, and the focus is not on identifying influencing factors. </span><span class="koboSpan" id="kobo.64.2">The objective of this approach is to recognize past patterns in the phenomenon’s evolution and extend those patterns into the future to make predictions. </span><span class="koboSpan" id="kobo.64.3">In simpler terms, predictions are based on the historical behavior of the phenomenon over time rather than being linked to explanatory variables. </span><span class="koboSpan" id="kobo.64.4">For example, this approach is often used in analyzing sales trends, GDP trends, and </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">similar data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Regression analysis</span></strong><span class="koboSpan" id="kobo.67.1">: Regression models quantify the correlation between one or more predictor variables and a response variable. </span><span class="koboSpan" id="kobo.67.2">These methods operate under the assumption that the variable to be forecasted can be associated with one or more independent or explanatory variables. </span><span class="koboSpan" id="kobo.67.3">This method is suitable for predicting numerical outcomes and is commonly used in economics, finance, and social sciences. </span><span class="koboSpan" id="kobo.67.4">For instance, the demand for consumer goods in a household is believed to be influenced by factors such as income, the ages of household members, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">related variables.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.69.1">Exponential smoothing</span></strong><span class="koboSpan" id="kobo.70.1">: Exponential smoothing techniques allocate exponentially decreasing weights to previous observations, prioritizing the significance of more recent data points. </span><span class="koboSpan" id="kobo.70.2">This helps in smoothing out noise and identifying trends in time </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">series data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Machine learning</span></strong><span class="koboSpan" id="kobo.73.1">: Machine learning algorithms, including decision trees, random forests, and neural networks, find application in forecasting tasks, especially when handling extensive datasets or intricate relationships. </span><span class="koboSpan" id="kobo.73.2">They can capture non-linear patterns and are increasingly employed in various </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">forecasting applications.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.75.1">These methods are applicable under the </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">following conditions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.77.1">Sufficient historical </span><a id="_idIndexMarker985"/><span class="koboSpan" id="kobo.78.1">data is accessible regarding the phenomenon’s </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">previous patterns</span></span></li>
<li><span class="koboSpan" id="kobo.80.1">It is possible to quantify this </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">historical information</span></span></li>
<li><span class="koboSpan" id="kobo.82.1">It can be reasonably assumed that the attributes characterizing past trends will persist into the future, allowing for </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">accurate forecasting</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.84.1">In essence, quantitative </span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.85.1">methods are employed when there is an ample supply of quantifiable </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">historical data.</span></span></p>
<h3><span class="koboSpan" id="kobo.87.1">Describing qualitative forecasting methodologies</span></h3>
<p><span class="koboSpan" id="kobo.88.1">Qualitative methods</span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.89.1"> primarily rely on judgments, making them dependent on the viewpoints and assessments of consumers and experts. </span><span class="koboSpan" id="kobo.89.2">These methods come into play when quantitative data is scarce, yet ample qualitative information is available. </span><span class="koboSpan" id="kobo.89.3">Here are a few instances illustrating the application of </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">qualitative methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.91.1">Expert judgment</span></strong><span class="koboSpan" id="kobo.92.1">: Qualitative forecasting often relies on the expertise and opinions of subject matter experts. </span><span class="koboSpan" id="kobo.92.2">The Delphi method and expert panels are examples of techniques that gather input from </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">knowledgeable individuals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.94.1">Market research</span></strong><span class="koboSpan" id="kobo.95.1">: Qualitative research methods, such as surveys, focus groups, and customer interviews, are used to gather information about consumer preferences, behaviors, and market trends. </span><span class="koboSpan" id="kobo.95.2">This data can be valuable for predicting future </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">market conditions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Scenario analysis</span></strong><span class="koboSpan" id="kobo.98.1">: Qualitative forecasting can involve the creation of multiple scenarios or narratives about possible future events. </span><span class="koboSpan" id="kobo.98.2">Decision-makers then consider these scenarios to make </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">informed decisions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.100.1">Historical analogy</span></strong><span class="koboSpan" id="kobo.101.1">: This method involves making predictions based on similarities with past events or situations. </span><span class="koboSpan" id="kobo.101.2">It can be useful when dealing with unique or </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">novel situations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.103.1">Expert systems</span></strong><span class="koboSpan" id="kobo.104.1">: Expert systems and AI-driven tools can assist in qualitative forecasting by aggregating and interpreting data from various sources to provide insights </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">and predictions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.106.1">The choice between quantitative and qualitative methods for forecasting depends on various factors, including the nature of the data, the availability of historical information, the specific forecasting goals, and the level of subjectivity involved. </span><span class="koboSpan" id="kobo.106.2">In many cases, a combination of both quantitative and qualitative methods is used to improve the accuracy and </span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.107.1">robustness of forecasts, especially in complex </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">decision-making contexts.</span></span></p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.109.1">Time series analysis</span></h2>
<p><span class="koboSpan" id="kobo.110.1">A time series represents</span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.111.1"> a collection of observations related to a phenomenon, recorded at consecutive time points or intervals. </span><span class="koboSpan" id="kobo.111.2">These intervals are typically, though not always, evenly spaced or of uniform duration. </span><span class="koboSpan" id="kobo.111.3">Instances of time series data include diverse elements such as trends in commodity prices, stock market indices, the BTP/BUND spread, and </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">unemployment rates.</span></span></p>
<p><span class="koboSpan" id="kobo.113.1">Unlike classical statistics, which frequently assumes that independent observations stem from a single random variable, time series analysis presupposes </span><em class="italic"><span class="koboSpan" id="kobo.114.1">n</span></em><span class="koboSpan" id="kobo.115.1"> observations derived from a multitude of dependent random variables. </span><span class="koboSpan" id="kobo.115.2">Consequently, the analysis of a time series entails a procedure aimed at deciphering the underlying generating process behind the observed data, as opposed to treating each observation as independent and </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">identically distributed.</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">Based on the data collected, time series data can be divided into two </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">main types:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.119.1">Continuous time series</span></strong><span class="koboSpan" id="kobo.120.1">: Data is</span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.121.1"> recorded continuously over a specific period without any gaps or interruptions. </span><span class="koboSpan" id="kobo.121.2">This type of time series is common in applications such as sensor data, where measurements are taken at very short intervals, often in real time. </span><span class="koboSpan" id="kobo.121.3">Examples include temperature readings recorded every second, stock prices updated every minute, or heart rate monitoring over </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">continuous intervals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.123.1">Discrete time series</span></strong><span class="koboSpan" id="kobo.124.1">: Data is recorded at specific, discrete time intervals, which may not necessarily be evenly spaced. </span><span class="koboSpan" id="kobo.124.2">These intervals can be daily, weekly, monthly, annually, or any other predetermined period. </span><span class="koboSpan" id="kobo.124.3">Discrete time series data is often used in economic, financial, and social sciences to analyze trends and patterns over time. </span><span class="koboSpan" id="kobo.124.4">Examples include monthly sales figures, annual GDP growth rates, or weekly website </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">traffic statistics.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.126.1">The choice between continuous and discrete time series depends on the nature of the data being collected and the specific objectives of the analysis. </span><span class="koboSpan" id="kobo.126.2">Each type of time series has its own set of statistical techniques and tools that are used for modeling, forecasting, and extracting </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">meaningful insights.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">Time series data can also be categorized into two main types based on the underlying nature of the </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">data-generating process:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.130.1">Deterministic time series</span></strong><span class="koboSpan" id="kobo.131.1">: This type is characterized by a clear and predictable pattern</span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.132.1"> or relationship between data points. </span><span class="koboSpan" id="kobo.132.2">The data values in a deterministic time series follow a specific mathematical function or rule. </span><span class="koboSpan" id="kobo.132.3">Deterministic patterns can be expressed as mathematical equations or formulas and may include periodic or seasonal variations, trends, or specific mathematical relationships. </span><span class="koboSpan" id="kobo.132.4">Deterministic time series often exhibit little or no randomness, making them highly predictable. </span><span class="koboSpan" id="kobo.132.5">Examples of this type of time series are a sine wave representing daily temperature variations over a year, a linear trend in stock prices, where prices consistently increase or decrease over time, and monthly sales figures influenced by a known seasonal pattern, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">holiday sales.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.134.1">Stochastic time series</span></strong><span class="koboSpan" id="kobo.135.1">: This is characterized by randomness, and the data points are not governed by a specific deterministic rule. </span><span class="koboSpan" id="kobo.135.2">Instead, they are influenced by random or probabilistic factors. </span><span class="koboSpan" id="kobo.135.3">Stochastic time series exhibit inherent randomness, making it challenging to predict future values with certainty. </span><span class="koboSpan" id="kobo.135.4">They often involve elements of uncertainty, noise, and randomness that cannot be fully explained by a deterministic model. </span><span class="koboSpan" id="kobo.135.5">Statistical techniques, such as </span><strong class="bold"><span class="koboSpan" id="kobo.136.1">Autoregressive Integrated Moving Average</span></strong><span class="koboSpan" id="kobo.137.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.138.1">ARIMA</span></strong><span class="koboSpan" id="kobo.139.1">) models or state-space </span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.140.1">models, are commonly used to analyze and forecast stochastic time series. </span><span class="koboSpan" id="kobo.140.2">Examples of this type of time series are stock prices, which are influenced by a multitude of unpredictable factors and exhibit daily fluctuations that cannot be precisely predicted, daily weather conditions, where variables such as rainfall or temperature can vary randomly, and daily website traffic, which is influenced by user behavior and external factors, resulting in </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">random fluctuations.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.142.1">In practice, many real-world time series data contains a combination of deterministic and stochastic components. </span><span class="koboSpan" id="kobo.142.2">Analyzing and modeling time series data effectively often requires identifying and separating these components to gain insights and make accurate forecasts or predictions. </span><span class="koboSpan" id="kobo.142.3">Deterministic models are useful when the underlying patterns are well defined and stable, while stochastic models are employed when randomness plays a significant role in </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.144.1">Indeed, when dealing with time series data that exhibits both deterministic and stochastic components, it is common to represent the series as the sum of these two contributions. </span><span class="koboSpan" id="kobo.144.2">This approach allows for a more comprehensive understanding of the data and facilitates</span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.145.1"> modeling and analysis. </span><span class="koboSpan" id="kobo.145.2">Mathematically, it can be expressed </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.147.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.148.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.149.1">t</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.150.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.151.1">f</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.152.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.153.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.154.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.155.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.156.1">w</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.157.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.158.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.159.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.160.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.162.1">Y</span></em><em class="italic"><span class="koboSpan" id="kobo.163.1">t</span></em><span class="koboSpan" id="kobo.164.1">: This represents the observed time series data at </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">time </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.166.1">t</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.167.1">f(t)</span></em><span class="koboSpan" id="kobo.168.1">: This represents the deterministic component or contribution at </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">time </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.170.1">t</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.171.1">w(t)</span></em><span class="koboSpan" id="kobo.172.1">: This represents the stochastic (random) component or contribution at </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">time </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.174.1">t</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.175.1">By decomposing the time series into its deterministic and stochastic parts, analysts and researchers can do </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.177.1">Analyze trends</span></strong><span class="koboSpan" id="kobo.178.1">: The deterministic component often includes trends, seasonality, and other structured patterns that can be analyzed separately to understand the </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">underlying dynamics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.180.1">Model stochastic behavior</span></strong><span class="koboSpan" id="kobo.181.1">: The stochastic component captures the random fluctuations and noise in the data. </span><span class="koboSpan" id="kobo.181.2">Models </span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.182.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.183.1">ARIMA</span></strong><span class="koboSpan" id="kobo.184.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.185.1">Generalized Autoregressive Conditional Heteroskedasticity</span></strong><span class="koboSpan" id="kobo.186.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.187.1">GARCH</span></strong><span class="koboSpan" id="kobo.188.1">) can be applied to</span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.189.1"> this component for forecasting or </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">risk assessment.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.191.1">This decomposition approach enhances the ability to make forecasts, identify anomalies, and gain insights into the factors influencing the time </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">series data.</span></span></p>
<p><span class="koboSpan" id="kobo.193.1">Using time denoted as </span><em class="italic"><span class="koboSpan" id="kobo.194.1">t = 1 …. </span><span class="koboSpan" id="kobo.194.2">T</span></em><span class="koboSpan" id="kobo.195.1">, we represent the sequence, y</span><span class="subscript"><span class="koboSpan" id="kobo.196.1">t</span></span><span class="koboSpan" id="kobo.197.1">. </span><span class="koboSpan" id="kobo.197.2">Time serves as the critical parameter governing the sequence of events that must not be overlooked. </span><span class="koboSpan" id="kobo.197.3">Consequently, understanding the temporal dimension’s position of observation becomes essential. </span><span class="koboSpan" id="kobo.197.4">Typically, this information is depicted as the pair of values (</span><em class="italic"><span class="koboSpan" id="kobo.198.1">t</span></em><span class="koboSpan" id="kobo.199.1">, </span><em class="italic"><span class="koboSpan" id="kobo.200.1">yt</span></em><span class="koboSpan" id="kobo.201.1">) on a Cartesian graph, creating a continuous line graph that conveys the impression of continuous detection of the phenomenon. </span><span class="koboSpan" id="kobo.201.2">This graphical representation is commonly known as a time series plot (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.202.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.203.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">To understand how a series plot can be drawn, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.206.1">Nile.csv</span></strong><span class="koboSpan" id="kobo.207.1"> file in this book’s GitHub repository. </span><span class="koboSpan" id="kobo.207.2">This file provides data on the annual flow of the Nile river at Aswan from 1871 to 1970, measured in units of 10</span><span class="superscript"><span class="koboSpan" id="kobo.208.1">8</span></span><span class="koboSpan" id="kobo.209.1"> cubic meters. </span><span class="koboSpan" id="kobo.209.2">We can import this dataset into MATLAB using the </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.211.1">
Nile = readtable('Nile.csv');</span></pre> <p><span class="koboSpan" id="kobo.212.1">To draw a time series plot, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">following command:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.214.1">
plot(Nile.time,Nile.Nile)</span></pre> <p><span class="koboSpan" id="kobo.215.1">We used the </span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.216.1">dot format, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.217.1">time</span></strong><span class="koboSpan" id="kobo.218.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">Nile</span></strong><span class="koboSpan" id="kobo.220.1"> are the names of the two features selected. </span><span class="koboSpan" id="kobo.220.2">The following time series plot will </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">be drawn:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<span class="koboSpan" id="kobo.222.1"><img alt="Figure 9.1 – Measurements of the annual flow of the Nile ﻿river at Aswan" src="image/B21156_09_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.223.1">Figure 9.1 – Measurements of the annual flow of the Nile river at Aswan</span></p>
<p><span class="koboSpan" id="kobo.224.1">As shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.225.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.226.1">.1</span></em><span class="koboSpan" id="kobo.227.1">, there appears to be a significant change point in the data around 1898, suggesting a notable shift or change in the river’s flow characteristics during </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">that period.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">A time series plot provides an immediate insight into trends, recurring patterns, and other systematic behaviors that evolve. </span><span class="koboSpan" id="kobo.229.2">In the previous graph, we can see annual data exhibiting a consistent declining trend spanning a considerable time frame. </span><span class="koboSpan" id="kobo.229.3">Notably, there is a recurring zigzag pattern due to the data being recorded monthly, which is indicative of seasonality. </span><span class="koboSpan" id="kobo.229.4">It’s worth noting that the high peaks occur consistently during months when rainfall </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">is anticipated.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">Univariate analysis of time series aims to decipher the dynamic processes that underlie the series and predict future occurrences of the phenomenon. </span><span class="koboSpan" id="kobo.231.2">In this analysis, we focus exclusively on the data pairs (</span><em class="italic"><span class="koboSpan" id="kobo.232.1">t, yt</span></em><span class="koboSpan" id="kobo.233.1">), where </span><em class="italic"><span class="koboSpan" id="kobo.234.1">t = 1, …..,T</span></em><span class="koboSpan" id="kobo.235.1">. </span><span class="koboSpan" id="kobo.235.2">The crucial concept here is that both past and present data contain valuable information for forecasting the future development of </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">the phenomenon.</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">However, it’s essential to recognize that univariate analysis might be overly restrictive in some cases. </span><span class="koboSpan" id="kobo.237.2">Often, we possess information related to phenomena associated with the one under consideration, which should be suitably integrated to enhance the model’s predictive capabilities. </span><span class="koboSpan" id="kobo.237.3">Nevertheless, univariate analysis serves as a valuable baseline, allowing us to validate more sophisticated </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">modeling approaches.</span></span></p>
<p><span class="koboSpan" id="kobo.239.1">In a time series plot, four </span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.240.1">distinct patterns can be identified concerning the progression </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">of time:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.242.1">Horizontal pattern</span></strong><span class="koboSpan" id="kobo.243.1">: In this</span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.244.1"> scenario, the series fluctuates around a constant value, which is typically the series average. </span><span class="koboSpan" id="kobo.244.2">Such a series is referred to as being stationary on average. </span><span class="koboSpan" id="kobo.244.3">This pattern is commonly encountered in quality control, where processes are consistently maintained around an </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">average value.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.246.1">Seasonal pattern</span></strong><span class="koboSpan" id="kobo.247.1">: A seasonal pattern emerges when the series is influenced by recurring seasonal factors, such as monthly, semi-annual, or quarterly variations. </span><span class="koboSpan" id="kobo.247.2">Products such as ice cream, soft drinks, and electricity consumption often exhibit seasonal patterns. </span><span class="koboSpan" id="kobo.247.3">These series, which are influenced by seasonality, are also known as periodic series since the seasonal cycle repeats at fixed intervals. </span><span class="koboSpan" id="kobo.247.4">In the case of annual data, seasonality may not </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">be evident.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.249.1">Cyclic pattern</span></strong><span class="koboSpan" id="kobo.250.1">: The presence of a cyclic pattern is characterized by irregular increases and decreases in the series that do not follow a fixed period. </span><span class="koboSpan" id="kobo.250.2">This distinguishes cyclic fluctuations from seasonal ones. </span><span class="koboSpan" id="kobo.250.3">Furthermore, cyclic oscillations typically exhibit larger amplitudes compared to seasonal variations. </span><span class="koboSpan" id="kobo.250.4">In economic series, cyclic patterns are driven by economic expansions and contractions resulting from </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">speculative phenomena.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.252.1">Trend</span></strong><span class="koboSpan" id="kobo.253.1">: Trends are recognized by sustained, long-term increases or decreases in the series. </span><span class="koboSpan" id="kobo.253.2">For instance, the global population series exemplifies an increasing trend, while the monthly beer sales series may not display any discernible trend and instead </span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.254.1">possesses a stable horizontal </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">background pattern.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.256.1">Many time series exhibit a combination of these patterns, and it is precisely this kind of complexity that adds a high level of interest to the forecasting task. </span><span class="koboSpan" id="kobo.256.2">Forecasting methods must possess the capability to discern and replicate the various components of the series. </span><span class="koboSpan" id="kobo.256.3">This involves the assumption that past patterns will persist and continue to manifest their characteristic features in </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">the future.</span></span></p>
<p><span class="koboSpan" id="kobo.258.1">The classical approach to time series analysis involves dissecting the deterministic portion of the series into a set of signal components, which convey the structural information of the series while filtering out the negligible noise. </span><span class="koboSpan" id="kobo.258.2">In practical terms, our objective is to identify some of the previously mentioned patterns within the time </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">series trend.</span></span></p>
<p><span class="koboSpan" id="kobo.260.1">In time series analysis, three fundamental components are used to model and understand the underlying</span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.261.1"> patterns in the data: trend, seasonal, and residual components. </span><span class="koboSpan" id="kobo.261.2">These components help decompose a time series into its constituent parts, making it easier to analyze and forecast the </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">data accurately:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.263.1">Trend component</span></strong><span class="koboSpan" id="kobo.264.1">: It represents</span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.265.1"> the long-term, systematic, and often smooth movement or direction in the time series. </span><span class="koboSpan" id="kobo.265.2">It reflects the underlying, sustained changes or growth (increasing trend) or declines (decreasing trend) in the data over an extended period. </span><span class="koboSpan" id="kobo.265.3">The trend component aims to capture the overall behavior of the time series, making it essential for understanding its </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">fundamental trajectory.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.267.1">Seasonal component</span></strong><span class="koboSpan" id="kobo.268.1">: It accounts for the recurring patterns or fluctuations in the time series that follow a fixed and known period. </span><span class="koboSpan" id="kobo.268.2">These patterns repeat at regular intervals, such as daily, monthly, quarterly, or annually. </span><span class="koboSpan" id="kobo.268.3">Seasonality is often linked to external factors such as weather, holidays, or business cycles and tends to recur predictably </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">over time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.270.1">Residual component</span></strong><span class="koboSpan" id="kobo.271.1">: This component, also known as the error or noise, represents the random fluctuations or unexplained variability in the time series data. </span><span class="koboSpan" id="kobo.271.2">It includes all the information not accounted for by the trend and seasonal components. </span><span class="koboSpan" id="kobo.271.3">Analyzing the residual component helps identify irregularities, outliers, or </span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.272.1">unexpected events in </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">the data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.274.1">Mathematically, a time series (</span><em class="italic"><span class="koboSpan" id="kobo.275.1">Y</span></em><em class="italic"><span class="koboSpan" id="kobo.276.1">t</span></em><span class="koboSpan" id="kobo.277.1">) can be expressed as the sum of these three components. </span><span class="koboSpan" id="kobo.277.2">Decomposing a time series into these components is a fundamental step in time series analysis. </span><span class="koboSpan" id="kobo.277.3">Once separated, analysts can model each component individually, apply appropriate statistical techniques, and make more accurate forecasts. </span><span class="koboSpan" id="kobo.277.4">This decomposition helps in understanding the underlying dynamics of the data and can provide valuable</span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.278.1"> insights for decision-making </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">and forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">Let’s see the different approaches to time series modeling and how it is possible to extract knowledge from these types </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">of data.</span></span></p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.282.1">Extracting statistics from sequential data</span></h1>
<p><span class="koboSpan" id="kobo.283.1">Time series data represents a</span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.284.1"> sequence of measurements gathered over a certain period. </span><span class="koboSpan" id="kobo.284.2">These measurements are linked to a specific variable and are obtained at regular intervals. </span><span class="koboSpan" id="kobo.284.3">An essential characteristic of time series data is its inherent order, where the arrangement of observations along a timeline conveys significant information. </span><span class="koboSpan" id="kobo.284.4">Altering the sequence can completely change the data’s meaning. </span><span class="koboSpan" id="kobo.284.5">Sequential data, on a broader scale, encompasses any data presented sequentially, including time </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">series data.</span></span></p>
<p><span class="koboSpan" id="kobo.286.1">Our primary goal is to develop models that capture the underlying patterns within time series data or any sequential data. </span><span class="koboSpan" id="kobo.286.2">These models are instrumental in describing essential aspects of the time series patterns. </span><span class="koboSpan" id="kobo.286.3">They enable us to explore how past data influences the future, examine correlations between datasets, make future predictions, or control variables based on specific metrics. </span><span class="koboSpan" id="kobo.286.4">To visually represent time series data, we commonly employ line charts or bar graphs. </span><span class="koboSpan" id="kobo.286.5">Time series data analysis finds extensive use in various domains, including finance, signal processing, weather forecasting, trajectory prediction, earthquake prediction, and any field dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">temporal data.</span></span></p>
<p><span class="koboSpan" id="kobo.288.1">In the realm of time series and sequential data analysis, the models we construct must consider data </span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.289.1">ordering and extract relationships among adjacent data points. </span><span class="koboSpan" id="kobo.289.2">Now, let’s explore some MATLAB code examples for analyzing time series and </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">sequential data.</span></span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.291.1">Converting a dataset into a time series format in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.292.1">A time series </span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.293.1">represents a sequence</span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.294.1"> of observations of a phenomenon collected at consecutive moments or time intervals, typically evenly spaced or of consistent duration, although not necessarily so. </span><span class="koboSpan" id="kobo.294.2">It’s crucial to recognize that time plays a fundamental role in the analysis of a time series. </span><span class="koboSpan" id="kobo.294.3">To begin, we need to develop proficiency in handling data that portrays a prolonged observation of a specific phenomenon. </span><span class="koboSpan" id="kobo.294.4">Our initial step will involve learning how to transform a sequence of observations into time series data and then </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">visualizing it.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">In MATLAB, to adequately handle time series data, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">timetable</span></strong><span class="koboSpan" id="kobo.298.1"> object. </span><span class="koboSpan" id="kobo.298.2">A timetable is a specialized form of a table that’s designed for handling time series data. </span><span class="koboSpan" id="kobo.298.3">Similar to regular tables, timetables store data variables organized by columns, allowing for various data types and sizes, so long as they share the same number of rows. </span><span class="koboSpan" id="kobo.298.4">In addition to this flexibility, timetables offer specific functions tailored for time-related operations, enabling alignment, combination, and calculations involving timestamped data across one or </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">more timetables.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">The row times in a timetable consist of datetime or duration values that serve as labels for individual rows. </span><span class="koboSpan" id="kobo.300.2">You can access and manipulate a timetable by row time and variable. </span><span class="koboSpan" id="kobo.300.3">To access specific elements within a timetable, you can use round parentheses, </span><em class="italic"><span class="koboSpan" id="kobo.301.1">()</span></em><span class="koboSpan" id="kobo.302.1">, to retrieve a subtable or curly braces, </span><em class="italic"><span class="koboSpan" id="kobo.303.1">{}</span></em><span class="koboSpan" id="kobo.304.1">, to extract their contents. </span><span class="koboSpan" id="kobo.304.2">Furthermore, you can reference variables and the row times vector using their respective names. </span><span class="koboSpan" id="kobo.304.3">To identify and annotate events within a timetable, you can associate an event table with it. </span><span class="koboSpan" id="kobo.304.4">Event tables contain a record of event times, corresponding event labels, and additional </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">event-related details.</span></span></p>
<p><span class="koboSpan" id="kobo.306.1">To begin with, we will see how it is possible to create a timetable from such data. </span><span class="koboSpan" id="kobo.306.2">Suppose we have stored data about the flow of a river, measured at different times, in a timetable. </span><span class="koboSpan" id="kobo.306.3">In addition to archiving, schedules provide functions to synchronize data with specified times. </span><span class="koboSpan" id="kobo.306.4">In addition, we can note the time to add more information about the measurement conditions and other variables to be measured. </span><span class="koboSpan" id="kobo.306.5">Therefore, we can associate a time with the variables present in the workspace. </span><span class="koboSpan" id="kobo.306.6">The values present in the variable containing the times become the times of the object lines. </span><span class="koboSpan" id="kobo.306.7">All the other input arguments become time-dependent variables. </span><span class="koboSpan" id="kobo.306.8">Let’s learn how to format </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">timetable data:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.308.1">To begin, we must create the vector containing the variables, starting with the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">time vector:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.310.1">
TimeData = datetime({'01.01.1871';'01.01.1872';'01.01.1873';
'01.01.1874';'01.01.1875'});
TempData = [22.1;21.1;23.2;21.9;22.5];
RiverFlow = [1120;1160;963;1210;1160];</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.311.1">First, we have the time of the measurements, then the value of the temperatures measured in this place at this time, and finally the values of the river flow measured in units of 10</span><span class="superscript"><span class="koboSpan" id="kobo.312.1">8</span></span><span class="koboSpan" id="kobo.313.1"> cubic meters. </span><span class="koboSpan" id="kobo.313.2">To set the time format, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">datetime()</span></strong><span class="koboSpan" id="kobo.315.1"> function: a datetime array contains information about the year, month, day, hour, minute, and second components associated with each</span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.316.1"> recorded point in time, adhering </span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.317.1">to the proleptic ISO calendar system. </span><span class="koboSpan" id="kobo.317.2">Additionally, datetime arrays offer versatile formats for displaying output and parsing input text, support for storing fractional seconds with precision down to nanoseconds, and properties to handle considerations such as time zones, daylight saving time, and </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">leap seconds.</span></span></p></li> <li><span class="koboSpan" id="kobo.319.1">The data that’s used is corrected and added to the time vector in a format compatible with the type of data. </span><span class="koboSpan" id="kobo.319.2">Now, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">timetable()</span></strong><span class="koboSpan" id="kobo.321.1"> function to create </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">the object:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.323.1">
NileRiverFlow = timetable(TimeData, TempData, RiverFlow);</span></pre></li> <li><span class="koboSpan" id="kobo.324.1">A new timetable object was created in the MATLAB workspace. </span><span class="koboSpan" id="kobo.324.2">We can check the object’s type </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.326.1">
class(NileRiverFlow)
ans =
    'timetable'</span></pre></li> <li><span class="koboSpan" id="kobo.327.1">To understand how the </span><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">timetable()</span></strong><span class="koboSpan" id="kobo.329.1"> function handles the data, we will print the object that was created, </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.331.1">
NileRiverFlow
NileRiverFlow =
  5×2 timetable
     TimeData      TempData    RiverFlow
    ___________    ________    _________
    01-Jan-1871      22.1        1120
    01-Jan-1872      21.1        1160
    01-Jan-1873      23.2         963
    01-Jan-1874      21.9        1210
    01-Jan-1875      22.5        1160</span></pre></li> <li><span class="koboSpan" id="kobo.332.1">The timeline</span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.333.1"> can also be</span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.334.1"> constructed based on specific values of some parameters. </span><span class="koboSpan" id="kobo.334.2">For example, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">RiverFlow</span></strong><span class="koboSpan" id="kobo.336.1"> variable already available in the MATLAB workspace to associate each of its values with a specific time value defined in a specific timeline. </span><span class="koboSpan" id="kobo.336.2">For example, we can define a time step </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.338.1">
NileRiverFlow2 = timetable(RiverFlow,'TimeStep',years(1),
'StartTime',years(1870));</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.339.1">We set </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">TimeStep</span></strong><span class="koboSpan" id="kobo.341.1"> to one year and </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">StartTime</span></strong><span class="koboSpan" id="kobo.343.1"> to 1870. </span><span class="koboSpan" id="kobo.343.2">By doing this, one value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">RiverFlow</span></strong><span class="koboSpan" id="kobo.345.1"> is associated with a new year starting from 1870 with an increment of </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">one year.</span></span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.347.1">Understanding time series slicing</span></h2>
<p><span class="koboSpan" id="kobo.348.1">Slicing and dicing are</span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.349.1"> two expressions that are used in the context of datasets, signifying the process of partitioning a substantial dataset into smaller segments or examining it from various perspectives to gain deeper insights. </span><span class="koboSpan" id="kobo.349.2">These terms draw an analogy from culinary terminology, describing two fundamental knife techniques that chefs must proficiently execute. </span><span class="koboSpan" id="kobo.349.3">Slicing entails cutting, whereas dicing involves cutting food into finely uniform sections, often performed consecutively. </span><span class="koboSpan" id="kobo.349.4">In data analysis, the concept of slice and dice typically encompasses the systematic breakdown of a large dataset into more manageable portions to extract </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">additional information.</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">In MATLAB, a timetable is a specialized form of table that links a specific time with each row. </span><span class="koboSpan" id="kobo.351.2">You can efficiently extract time-related subsets of its data through various methods, such as identifying times within a specified range or matching recurring </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">time intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">After learning how to create a timetable object from variables in the workspace, we can learn how to import and manipulate an external file into MATLAB as a </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">timetable object:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.355.1">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.356.1">.csv</span></strong><span class="koboSpan" id="kobo.357.1"> file we used in the </span><em class="italic"><span class="koboSpan" id="kobo.358.1">Exploring the basic concepts of time series</span></em> <em class="italic"><span class="koboSpan" id="kobo.359.1">data</span></em><span class="koboSpan" id="kobo.360.1"> section. </span><span class="koboSpan" id="kobo.360.2">I’m referring to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">Nile.csv</span></strong><span class="koboSpan" id="kobo.362.1"> file; we can import this file using the </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.364.1">
NileRiverFlowData = readtimetable('Nile.csv');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.365.1">Now, let’s learn how to access this specific type of data. </span><span class="koboSpan" id="kobo.365.2">You can employ dot notation to retrieve the row times of a timetable. </span><span class="koboSpan" id="kobo.365.3">Furthermore, you can access specific variables by using dot notation individually or access all the data within a timetable using its name, as we </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">just did.</span></span></p></li> <li><span class="koboSpan" id="kobo.367.1">To display the first two lines of the time series object, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.369.1">
NileRiverFlowData(1:2,:)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.370.1">The following data </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">is printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.372.1">NileRiverFlowData(1:2,:)
ans =
  2×1 timetable
       time       Nile
    __________    ____
    01.01.1871    1120
    01.01.1872    1160</span></pre></li> <li><span class="koboSpan" id="kobo.373.1">To access only the data of one variable, we can use dot notation, </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.375.1">
NileRiverFlowData.Nile(1:2)</span></pre></li> <li><span class="koboSpan" id="kobo.376.1">Retrieve the entirety of the timetable data as a matrix using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.377.1">NileRiverFlowData.Variables</span></strong><span class="koboSpan" id="kobo.378.1"> syntax. </span><span class="koboSpan" id="kobo.378.2">This syntax relies on the second dimension name of the timetable and is functionally identical to accessing all contents through curly </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">brace indexing:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.380.1">
NileData = NileRiverFlowData{:,:};</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.381.1">However, it’s important to note that the resulting matrix does not incorporate row times as the </span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.382.1">row times vector is considered metadata within the timetable and not treated as a variable. </span><span class="koboSpan" id="kobo.382.2">In cases where the timetable data cannot be effectively concatenated into a matrix, an error message will </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">be generated.</span></span></p></li> <li><span class="koboSpan" id="kobo.384.1">To locate data within a particular range, you can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">timerange</span></strong><span class="koboSpan" id="kobo.386.1"> function, which establishes time-based subscripts for indexing. </span><span class="koboSpan" id="kobo.386.2">For example, you can set up a range that commences on January 1, 1880, and concludes on January 1, 1920. </span><span class="koboSpan" id="kobo.386.3">It’s important to note that by default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">timerange</span></strong><span class="koboSpan" id="kobo.388.1"> function defines a half-open interval that includes the left endpoint but excludes the </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">right endpoint:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.390.1">
TimeRange = timerange('01.01.1880','01.01.1920');</span></pre></li> <li><span class="koboSpan" id="kobo.391.1">Now, we can slice a portion of the dataset defined from this range, </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.393.1">
DataTimeRange = NileRiverFlowData(TimeRange,:);</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.394.1">The new dataset</span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.395.1"> contains data starting from January 1, 1880, and ending on January </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">1, 1919.</span></span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.397.1">Resampling time series data in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.398.1">Let’s see how it’s </span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.399.1">possible to resample time series data </span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.400.1">in MATLAB. </span><span class="koboSpan" id="kobo.400.2">This task involves changing the frequency or time intervals of the data while preserving the essential information. </span><span class="koboSpan" id="kobo.400.3">This can be useful for various purposes, such as aggregating data to a coarser time scale, interpolating data to a finer time scale, or aligning data from different sources with a common time grid. </span><span class="koboSpan" id="kobo.400.4">MATLAB provides several functions and methods for resampling time series data. </span><span class="koboSpan" id="kobo.400.5">Here’s a basic example of how to </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">do it.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">We start by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">retime()</span></strong><span class="koboSpan" id="kobo.404.1"> function, </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.406.1">
WeeklyData = retime(NileRiverFlowData,'weekly','spline');</span></pre> <p><span class="koboSpan" id="kobo.407.1">This function resamples or consolidates data within a timetable while addressing issues related to duplicate or irregular time entries. </span><span class="koboSpan" id="kobo.407.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">retime()</span></strong><span class="koboSpan" id="kobo.409.1"> function generates a timetable by incorporating variables from the initial data and enforces a consistent time step for the row times. </span><span class="koboSpan" id="kobo.409.2">This function resamples or consolidates data within the initial variables based on the specified method. </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">retime()</span></strong><span class="koboSpan" id="kobo.411.1"> can be applied to interpolate data values from initial values at different timestamps, aggregate data into time intervals, eliminate rows with duplicate timestamps in the timetable, and transform an irregular timetable into a regular one by imposing regularly spaced row times, as defined by the new </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.412.1">time step</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">In this example, we resampled the dataset by adding the weekly data starting from the monthly, but there are different ways to resample data. </span><span class="koboSpan" id="kobo.414.2">We performed data value interpolation based on the adjacent rows. </span><span class="koboSpan" id="kobo.414.3">It is essential that the input timetable has sorted and distinct row times. </span><span class="koboSpan" id="kobo.414.4">There are four different types of </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">interpolation available:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">linear</span></strong><span class="koboSpan" id="kobo.417.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">Linear interpolation</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">spline</span></strong><span class="koboSpan" id="kobo.420.1">: Piecewise cubic </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">spline interpolation</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">pchip</span></strong><span class="koboSpan" id="kobo.423.1">: Shape-preserving piecewise </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">cubic interpolation</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">makima</span></strong><span class="koboSpan" id="kobo.426.1">: Modified Akima cubic </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">Hermite interpolation</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.428.1">There are the possibilities to aggregate the time series data using the same function (</span><strong class="source-inline"><span class="koboSpan" id="kobo.429.1">retime()</span></strong><span class="koboSpan" id="kobo.430.1">). </span><span class="koboSpan" id="kobo.430.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">retime()</span></strong><span class="koboSpan" id="kobo.432.1"> function offers aggregation options, including the mean. </span><span class="koboSpan" id="kobo.432.2">For example, we can compute the mean values monthly for the </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">given data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.434.1">
MonthlyData = retime(NileRiverFlowData,'monthly','mean');</span></pre> <p><span class="koboSpan" id="kobo.435.1">In this way, we aggregated the previously resampled weekly data using the average as the</span><a id="_idIndexMarker1017"/> <span class="No-Break"><span class="koboSpan" id="kobo.436.1">aggregation </span></span><span class="No-Break"><a id="_idIndexMarker1018"/></span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">criterion.</span></span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.438.1">Moving average</span></h2>
<p><span class="koboSpan" id="kobo.439.1">A moving average is a </span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.440.1">commonly used method for analyzing time series data. </span><span class="koboSpan" id="kobo.440.2">It involves calculating the average of a set of data points within a specific window or time interval as it moves through the time series. </span><span class="koboSpan" id="kobo.440.3">This helps to smooth out fluctuations and highlight underlying trends in the data. </span><span class="koboSpan" id="kobo.440.4">There are different types of moving averages, such as </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.441.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">simple moving average</span></strong><span class="koboSpan" id="kobo.443.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.444.1">SMA</span></strong><span class="koboSpan" id="kobo.445.1">) and the </span><strong class="bold"><span class="koboSpan" id="kobo.446.1">exponential moving average</span></strong><span class="koboSpan" id="kobo.447.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.448.1">EMA</span></strong><span class="koboSpan" id="kobo.449.1">), each with its </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.450.1">characteristics and </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.452.1">SMA is a basic and widely used method for analyzing time series data. </span><span class="koboSpan" id="kobo.452.2">It is a statistical calculation that helps smooth out fluctuations in data to identify trends or patterns. </span><span class="koboSpan" id="kobo.452.3">Here’s how you </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">calculate SMA:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.454.1">Choose a specific period </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">or window.</span></span></li>
<li><span class="koboSpan" id="kobo.456.1">For each data point, take the average of the data points within that time window. </span><span class="koboSpan" id="kobo.456.2">This involves adding up the values over the chosen period and dividing by the number of data points in </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">that window.</span></span></li>
<li><span class="koboSpan" id="kobo.458.1">Move the window one time step forward and calculate the average again. </span><span class="koboSpan" id="kobo.458.2">Repeat this process for each </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">data point.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.460.1">The result is a series of averages that represent the trend in the data over the chosen period. </span><span class="koboSpan" id="kobo.460.2">SMAs are commonly used for various purposes, including identifying trends, smoothing out noise in time series data, and making forecasts. </span><span class="koboSpan" id="kobo.460.3">For example, if you are interested in calculating a 10-day SMA for daily stock prices, you would average the closing prices of the last 10 days to get a moving average value for each day. </span><span class="koboSpan" id="kobo.460.4">This moving average can help you identify the general direction of the stock’s price trend over those </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">10 days.</span></span></p>
<p><span class="koboSpan" id="kobo.462.1">In MATLAB, you can calculate an SMA for a time series using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">movmean()</span></strong><span class="koboSpan" id="kobo.464.1"> function, which is available in the Statistics and Machine Learning Toolbox. </span><span class="koboSpan" id="kobo.464.2">Let’s learn how to use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">movmean()</span></strong><span class="koboSpan" id="kobo.466.1"> function with a </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">practical example:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.468.1">We’ll start by defining the window size for </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">the SMA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.470.1">
windowSize = 5;</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.471.1">We defined a five-year window to calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">moving average.</span></span></p></li> <li><span class="koboSpan" id="kobo.473.1">Now, we can calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">moving average:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.475.1">
SMA = movmean(NileRiverFlowData.Nile,windowSize);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.476.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">movmean()</span></strong><span class="koboSpan" id="kobo.478.1"> function generates an array of local mean values for k-point neighborhoods. </span><span class="koboSpan" id="kobo.478.2">Each mean is computed by sliding a window with a length of </span><em class="italic"><span class="koboSpan" id="kobo.479.1">k</span></em><span class="koboSpan" id="kobo.480.1"> across adjacent elements of the array, </span><em class="italic"><span class="koboSpan" id="kobo.481.1">A</span></em><span class="koboSpan" id="kobo.482.1">. </span><span class="koboSpan" id="kobo.482.2">If </span><em class="italic"><span class="koboSpan" id="kobo.483.1">k</span></em><span class="koboSpan" id="kobo.484.1"> is an odd number, the window centers around the current position’s element. </span><span class="koboSpan" id="kobo.484.2">For even values of </span><em class="italic"><span class="koboSpan" id="kobo.485.1">k</span></em><span class="koboSpan" id="kobo.486.1">, the window centers around the current and preceding elements. </span><span class="koboSpan" id="kobo.486.2">The window size adjusts automatically at the array’s endpoints when there are insufficient elements to fill it. </span><span class="koboSpan" id="kobo.486.3">In such cases, the average is calculated based on the elements that fit within the window. </span><span class="koboSpan" id="kobo.486.4">The resulting array, denoted as </span><em class="italic"><span class="koboSpan" id="kobo.487.1">M</span></em><span class="koboSpan" id="kobo.488.1">, maintains the same dimensions as array </span><em class="italic"><span class="koboSpan" id="kobo.489.1">A</span></em><span class="koboSpan" id="kobo.490.1">. </span><span class="koboSpan" id="kobo.490.2">When </span><em class="italic"><span class="koboSpan" id="kobo.491.1">A</span></em><span class="koboSpan" id="kobo.492.1"> is a one-dimensional vector, </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">movmean()</span></strong><span class="koboSpan" id="kobo.494.1"> processes it along the vector’s length. </span><span class="koboSpan" id="kobo.494.2">For multidimensional arrays, </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">movmean()</span></strong><span class="koboSpan" id="kobo.496.1"> operates along the first dimension of </span><em class="italic"><span class="koboSpan" id="kobo.497.1">A</span></em><span class="koboSpan" id="kobo.498.1"> without a size </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">of 1.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.500.1">The EMA is a variant of </span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.501.1">the moving average, placing greater emphasis on recent data points. </span><span class="koboSpan" id="kobo.501.2">This characteristic renders it more responsive to short-term price fluctuations when compared to the SMA. </span><span class="koboSpan" id="kobo.501.3">To calculate an EMA, follow </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">these steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.503.1">Choose a specific period, typically represented </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">as </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.505.1">N</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.507.1">Calculate the multiplier, which is often referred to as the “smoothing factor” or “weighting multiplier.” </span><span class="koboSpan" id="kobo.507.2">This is usually calculated as </span><em class="italic"><span class="koboSpan" id="kobo.508.1">2 / (N + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.509.1">1)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.511.1">Start with the EMA for the first data point, which is typically the SMA for the first </span><em class="italic"><span class="koboSpan" id="kobo.512.1">N</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.513.1">data points.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.514.1">For the subsequent data points, calculate the EMA using the </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.516.1">E</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.517.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.518.1">A</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.519.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.520.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.521.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.522.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.523.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.524.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.525.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.526.1">t</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.527.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.528.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.529.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.530.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.531.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.532.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.533.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.534.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.535.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.536.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.537.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.538.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.539.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.540.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.541.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.542.1">t</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.543.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.544.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.545.1">E</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.546.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.547.1">A</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.548.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.549.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.550.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.551.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.552.1">v</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.553.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.554.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.555.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.556.1">s</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.557.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.558.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.559.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.560.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.561.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.562.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.563.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.564.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.565.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.566.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.567.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.568.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.569.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.570.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.571.1">E</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.572.1">M</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.573.1">A</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.574.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.575.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.576.1">r</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.577.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.578.1">v</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.579.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.580.1">o</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.581.1">u</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.582.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.583.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.584.1">Here, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">the following:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.586.1">EMA(current)</span></em><span class="koboSpan" id="kobo.587.1"> is the EMA for the current </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">data point</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.589.1">Price(current)</span></em><span class="koboSpan" id="kobo.590.1"> is the price of the asset at the </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">current time</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.592.1">EMA(previous)</span></em><span class="koboSpan" id="kobo.593.1"> is the EMA for the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">data point</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.595.1">Multiplier</span></em><span class="koboSpan" id="kobo.596.1"> is the smoothing factor calculated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.597.1">Step 2</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.598.1">The EMA assigns more weight to recent data points, and as a result, it reacts more quickly to price changes than the SMA. </span><span class="koboSpan" id="kobo.598.2">It’s commonly used in technical analysis to identify trends and potential reversal points in financial markets, such as stocks, currencies, and commodities. </span><span class="koboSpan" id="kobo.598.3">Traders and analysts often use different EMA periods to analyze short-term and </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">long-term trends.</span></span></p>
<p><span class="koboSpan" id="kobo.600.1">MATLAB, for instance, provides functions such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.601.1">movavg()</span></strong><span class="koboSpan" id="kobo.602.1"> to calculate various types of moving averages, including the EMA. </span><span class="koboSpan" id="kobo.602.2">You can use this function to compute EMAs in MATLAB. </span><span class="koboSpan" id="kobo.602.3">Remember that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">movavg()</span></strong><span class="koboSpan" id="kobo.604.1"> function requires the use of </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">Financial Toolbox.</span></span></p>
<p><span class="koboSpan" id="kobo.606.1">We start by defining the type of moving average, after which we define the window size and calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">the EMA:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.608.1">
type = 'exponential'
windowSize = 5;
EMA = movavg(NileRiverFlowData.Nile,type,windowSize)</span></pre> <p><span class="koboSpan" id="kobo.609.1">The EMA also calculates an average over a specific time window, but it assigns more weight to recent </span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.610.1">data points. </span><span class="koboSpan" id="kobo.610.2">It gives exponentially decreasing weight to older data points. </span><span class="koboSpan" id="kobo.610.3">The EMA is more responsive to recent changes in the data. </span><span class="koboSpan" id="kobo.610.4">It reacts quickly to price changes, making it suitable for short-term trend analysis and capturing </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">price reversals.</span></span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.612.1">Exponential smoothing</span></h2>
<p><span class="koboSpan" id="kobo.613.1">Exponential</span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.614.1"> smoothing is a time series forecasting </span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.615.1">method that is used to make predictions about future data points based on past observations. </span><span class="koboSpan" id="kobo.615.2">It is particularly useful for data with a trend or seasonal component. </span><span class="koboSpan" id="kobo.615.3">Exponential smoothing assigns exponentially decreasing weights to past data points, with the most recent observations receiving the highest weights. </span><span class="koboSpan" id="kobo.615.4">EMA and exponential smoothing are both techniques that are used in time series analysis to capture trends and patterns. </span><span class="koboSpan" id="kobo.615.5">However, they differ in their formulations and applications. </span><span class="koboSpan" id="kobo.615.6">EMA, a subtype of exponential smoothing, places higher emphasis on recent data points by assigning them greater weight in the calculation. </span><span class="koboSpan" id="kobo.615.7">The mathematical expression for the EMA explicitly includes a smoothing parameter (alpha), offering a straightforward approach to adjusting the impact of recent versus older observations. </span><span class="koboSpan" id="kobo.615.8">Conversely, exponential smoothing is a broader term that encompasses various smoothing techniques, with the EMA being a specific instance. </span><span class="koboSpan" id="kobo.615.9">While the EMA requires an initial value for the calculation, exponential smoothing, in general, demands an initial value that significantly influences the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">smoothing process.</span></span></p>
<p><span class="koboSpan" id="kobo.617.1">There are several variations of exponential smoothing, including </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Simple exponential smoothing</span></strong><span class="koboSpan" id="kobo.620.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.621.1">SES</span></strong><span class="koboSpan" id="kobo.622.1">): This is </span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.623.1">used for </span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.624.1">univariate time series data without a trend or seasonality. </span><span class="koboSpan" id="kobo.624.2">It uses a single smoothing parameter (alpha) to assign weights to past observations. </span><span class="koboSpan" id="kobo.624.3">The forecast for the next period is a weighted average of the most recent observation and the </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">previous forecast.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.626.1">Holt’s linear exponential smoothing</span></strong><span class="koboSpan" id="kobo.627.1">: This is used for data with a linear trend but no seasonality. </span><span class="koboSpan" id="kobo.627.2">It extends SES by introducing two smoothing parameters, alpha for the level and beta for the trend. </span><span class="koboSpan" id="kobo.627.3">It calculates the level and trend separately and uses them to </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">make forecasts.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.629.1">Holt-Winters exponential smoothing</span></strong><span class="koboSpan" id="kobo.630.1">: This is used for data with both a trend and seasonality. </span><span class="koboSpan" id="kobo.630.2">It extends Holt’s linear exponential smoothing by adding a third smoothing parameter, gamma, for the seasonal component. </span><span class="koboSpan" id="kobo.630.3">It models and forecasts the level, trend, and </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">seasonal components.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.632.1">The basic idea behind exponential smoothing is to assign exponentially decreasing weights to past observations, giving more importance to recent data, and making the method adaptive to changes in the data pattern. </span><span class="koboSpan" id="kobo.632.2">Exponential smoothing is widely used in time series forecasting applications, such as sales forecasting, demand forecasting, and financial </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">market predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.634.1">Double and triple smoothing are advanced techniques in time series analysis, building upon the foundation of exponential smoothing. </span><span class="koboSpan" id="kobo.634.2">Double smoothing incorporates an additional level of smoothing to address trends, introducing a trend-smoothing parameter alongside the data-smoothing parameter. </span><span class="koboSpan" id="kobo.634.3">This method enhances the model’s ability to capture and forecast trends effectively. </span><span class="koboSpan" id="kobo.634.4">Triple smoothing, or the Holt-Winters method, goes a step further by incorporating seasonality components. </span><span class="koboSpan" id="kobo.634.5">It includes parameters for data smoothing, trend smoothing, and seasonality smoothing, making it particularly robust for time series data with consistent patterns. </span><span class="koboSpan" id="kobo.634.6">These techniques provide nuanced tools for capturing and forecasting complex trends and </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">seasonal variations.</span></span></p>
<p><span class="koboSpan" id="kobo.636.1">In MATLAB, you can implement various types of exponential smoothing using functions and libraries related</span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.637.1"> to time series analysis and forecasting. </span><span class="koboSpan" id="kobo.637.2">These functions often provide tools to estimate the smoothing parameters and make forecasts based on your data. </span><span class="koboSpan" id="kobo.637.3">Let’s take a </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">closer look:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.639.1">We start by defining the smoothing </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">parameter (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">alpha</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.643.1">
alpha = 0.3;</span></pre></li> <li><span class="koboSpan" id="kobo.644.1">We can adjust </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">alpha</span></strong><span class="koboSpan" id="kobo.646.1"> based on our needs (</span><strong class="source-inline"><span class="koboSpan" id="kobo.647.1">0</span></strong><span class="koboSpan" id="kobo.648.1"> &lt; </span><strong class="source-inline"><span class="koboSpan" id="kobo.649.1">alpha</span></strong><span class="koboSpan" id="kobo.650.1"> &lt;= </span><strong class="source-inline"><span class="koboSpan" id="kobo.651.1">1</span></strong><span class="koboSpan" id="kobo.652.1">). </span><span class="koboSpan" id="kobo.652.2">After that, we must initialize the forecast with the first </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">data point:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.654.1">
forecast = NileRiverFlowData.Nile(1);</span></pre></li> <li><span class="koboSpan" id="kobo.655.1">Finally, we must calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">SES forecast:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.657.1">
for I = 2:length(NileRiverFlowData.Nile)
    forecast(i) = alpha * NileRiverFlowData.Nile (i) + (1–- alpha) * forecast(i–- 1);
end</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.658.1">Now, we can display </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">the forecasts:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.660.1">disp(forecast);</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.661.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">alpha</span></strong><span class="koboSpan" id="kobo.663.1"> parameter controls the smoothing level, with lower values making the forecast more responsive to recent data and higher values making </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">it smoother.</span></span></p>
<p><span class="koboSpan" id="kobo.665.1">In this section, we analyzed some statistical methods for extracting information from a time series. </span><span class="koboSpan" id="kobo.665.2">Now, let’s </span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.666.1">see a practical case in which we predict the performance of a company’s shares using </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">recurrent networks.</span></span></p>
<h1 id="_idParaDest-193"><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.668.1">Implementing a model to predict the stock market</span></h1>
<p><span class="koboSpan" id="kobo.669.1">Predicting stock</span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.670.1"> market movements is a</span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.671.1"> complex and challenging endeavor. </span><span class="koboSpan" id="kobo.671.2">It involves analyzing various factors and data points to forecast the future direction of stock prices. </span><span class="koboSpan" id="kobo.671.3">Fundamental analysts examine a company’s financial health, including its revenue, earnings, debt levels, and growth prospects. </span><span class="koboSpan" id="kobo.671.4">They also consider macroeconomic factors such as interest rates, inflation, and government policies that can impact the overall market. </span><span class="koboSpan" id="kobo.671.5">Technical analysts study historical price and volume data, looking for patterns and trends in stock charts. </span><span class="koboSpan" id="kobo.671.6">They use tools such as moving averages, support and resistance levels, and various technical indicators to make predictions. </span><span class="koboSpan" id="kobo.671.7">Identifying current market trends and understanding market cycles can provide insights into potential future movements. </span><span class="koboSpan" id="kobo.671.8">Bull markets, bear markets, and sideways markets can affect stock prices differently. </span><span class="koboSpan" id="kobo.671.9">Predictions are inherently uncertain, and risk management is crucial. </span><span class="koboSpan" id="kobo.671.10">Diversifying a portfolio, setting stop-loss orders, and using proper position sizing can help manage risks associated </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">with predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.673.1">Some traders and investors use machine learning </span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.674.1">and </span><strong class="bold"><span class="koboSpan" id="kobo.675.1">artificial intelligence</span></strong><span class="koboSpan" id="kobo.676.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.677.1">AI</span></strong><span class="koboSpan" id="kobo.678.1">) algorithms to analyze vast amounts of data and identify potential patterns or trends in the market. </span><span class="koboSpan" id="kobo.678.2">Stock market predictions can range from short-term (days or weeks) to long-term (years). </span><span class="koboSpan" id="kobo.678.3">The chosen will depend on the investment strategy and goals. </span><span class="koboSpan" id="kobo.678.4">It’s important to remember that past stock market performance is not a guarantee of future results. </span><span class="koboSpan" id="kobo.678.5">Market conditions can change rapidly and unpredictably. </span><span class="koboSpan" id="kobo.678.6">Stock market predictions involve a combination of financial analysis, technical analysis, sentiment analysis, and consideration of various external factors. </span><span class="koboSpan" id="kobo.678.7">While some investors and traders may use predictions as part of their decision-making process, it’s essential to approach them with caution and maintain a diversified portfolio to manage </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">risk effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.680.1">In this section, we will delve into the application of the </span><strong class="bold"><span class="koboSpan" id="kobo.681.1">L</span></strong><strong class="bold"><span class="koboSpan" id="kobo.682.1">ong short-term memory</span></strong><span class="koboSpan" id="kobo.683.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.684.1">LSTM</span></strong><span class="koboSpan" id="kobo.685.1">) model for forecasting the future stock price of a highly renowned corporation: Tesla Inc. </span><span class="koboSpan" id="kobo.685.2">This is an American </span><strong class="bold"><span class="koboSpan" id="kobo.686.1">electric</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.687.1">vehicle</span></strong><span class="koboSpan" id="kobo.688.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.689.1">EV</span></strong><span class="koboSpan" id="kobo.690.1">) and clean </span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.691.1">energy company founded in 2003 by Elon Musk, JB Straubel, Martin Eberhard, Marc Tarpenning, and Ian Wright. </span><span class="koboSpan" id="kobo.691.2">It is headquartered in Palo </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">Alto, California.</span></span></p>
<p><span class="koboSpan" id="kobo.693.1">To evaluate Tesla’s stock price performance, we will utilize data encompassing the stock prices from November 6, 2010, to October 5, 2023, sourced from the NASDAQ GS stock quotes. </span><span class="koboSpan" id="kobo.693.2">It is also</span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.694.1"> possible </span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.695.1">to adjust the timeframe if needed, beyond the default </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">interval provided.</span></span></p>
<p><span class="koboSpan" id="kobo.697.1">The data was acquired from the Yahoo! </span><span class="koboSpan" id="kobo.697.2">Finance </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">website: </span></span><a href="https://finance.yahoo.com/quote/TSLA"><span class="No-Break"><span class="koboSpan" id="kobo.699.1">https://finance.yahoo.com/quote/TSLA</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.700.1">.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.701.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.702.1">.2</span></em><span class="koboSpan" id="kobo.703.1"> shows a screenshot of the Yahoo! </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">Finance website:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<span class="koboSpan" id="kobo.705.1"><img alt="Figure 9.2 – The Yahoo! Finance website" src="image/B21156_09_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.706.1">Figure 9.2 – The Yahoo! </span><span class="koboSpan" id="kobo.706.2">Finance website</span></p>
<p><span class="koboSpan" id="kobo.707.1">This CSV file includes the </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">following attributes:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.709.1">Date</span></strong><span class="koboSpan" id="kobo.710.1">: Date </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">of quote</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.712.1">Open</span></strong><span class="koboSpan" id="kobo.713.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">Open price</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.715.1">High</span></strong><span class="koboSpan" id="kobo.716.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">High price</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.718.1">Low</span></strong><span class="koboSpan" id="kobo.719.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">Low price</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.721.1">Close</span></strong><span class="koboSpan" id="kobo.722.1">: Close price adjusted </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">for splits</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.724.1">Adj Close</span></strong><span class="koboSpan" id="kobo.725.1">: Close price adjusted for both dividends </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">and splits</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.727.1">Volume</span></strong><span class="koboSpan" id="kobo.728.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">Exchange volume</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.730.1">The information within the CSV file is labeled as </span><strong class="source-inline"><span class="koboSpan" id="kobo.731.1">TSLA.csv</span></strong><span class="koboSpan" id="kobo.732.1">. </span><span class="koboSpan" id="kobo.732.2">To start, let’s investigate the process of importing this data into the </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">MATLAB workspace:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.734.1">We will import the data in timetable format using the </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">following command:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.736.1">
TeslaData = readtimetable('TSLA.csv');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.737.1">As always, we should be in the folder containing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.738.1">TSLA.csv</span></strong><span class="koboSpan" id="kobo.739.1"> file or add this folder to the </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">MATLAB path.</span></span></p></li> <li><span class="koboSpan" id="kobo.741.1">Before embarking on data prediction using the LSTM method, we will commence with </span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.742.1">an </span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.743.1">exploratory analysis to glean insights into the data distribution and extract preliminary knowledge. </span><span class="koboSpan" id="kobo.743.2">To glean preliminary insights from the imported dataset, we can utilize the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.744.1">summary()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.745.1"> function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.746.1">
summary(TeslaData)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.747.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">are printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.749.1">summary(TeslaData)
RowTimes:
    Date: 157×1 datetime
        Values:
            Min         2010-11-01
            Median      2017-05-01
            Max         2023-10-06
Variables:
    Open: 157×1 double
        Values:
            Min        1.4627
            Median      16.87
            Max         386.9
    High: 157×1 double
        Values:
            Min        1.6993
            Median       18.8
            Max         414.5
    Low: 157×1 double
        Values:
            Min        1.4033
            Median     15.798
            Max         326.2
    Close: 157×1 double
        Values:
            Min        1.5927
            Median     16.795
            Max        381.59
    AdjClose: 157×1 double
        Values:
            Min        1.5927
            Median     16.795
            Max        381.59
    Volume: 157×1 double
        Values:
            Min       1.1783e+08
            Median    1.8456e+09
            Max       7.0888e+09</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.750.1">A summary of the data was returned with some statistics on all the variables in the dataset. </span><span class="koboSpan" id="kobo.750.2">For example, we can read the number of records (</span><strong class="source-inline"><span class="koboSpan" id="kobo.751.1">157</span></strong><span class="koboSpan" id="kobo.752.1">) and the min, median, and max values of all variables. </span><span class="koboSpan" id="kobo.752.2">Based on the preliminary analysis of the results, it is evident that Tesla’s stock prices have undergone a significant transformation over the past 13 years. </span><span class="koboSpan" id="kobo.752.3">Specifically, the minimum value stands at $1.5927, while the maximum value </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">reaches $381.59.</span></span></p></li> <li><span class="koboSpan" id="kobo.754.1">Upon reviewing</span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.755.1"> the</span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.756.1"> dataset’s content, our next step involves conducting an initial visual exploratory analysis, wherein we will generate a graphical representation of the stock prices over </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">the years:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.758.1">
plot(TeslaData.Date, TeslaData.Close)
xlabel('Time (years)')
ylabel('Stock price ($)')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.759.1">The following chart will be drawn (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.760.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.761.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer094">
<span class="koboSpan" id="kobo.763.1"><img alt="Figure 9.3 – Stock prices of Tesla Inc. from 2010 to 2023" src="image/B21156_09_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.764.1">Figure 9.3 – Stock prices of Tesla Inc. </span><span class="koboSpan" id="kobo.764.2">from 2010 to 2023</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.765.1">Examining the preceding graph reveals a substantial increase in prices over time. </span><span class="koboSpan" id="kobo.765.2">Commencing in 2020, this upsurge follows an exponential trend. </span><span class="koboSpan" id="kobo.765.3">Let’s delve deeper into understanding the fluctuations that the Tesla stock has documented </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">over time.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.767.1">Exploring the </span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.768.1">evolution of </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.769.1">a phenomenon becomes intriguing not only by examining its time series graph but also by drawing comparisons between the intensity of the phenomenon at distinct time points. </span><span class="koboSpan" id="kobo.769.2">This involves calculating variations in intensity from one period to another. </span><span class="koboSpan" id="kobo.769.3">Additionally, analyzing the trend of changes in the phenomenon between contiguous periods can offer </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">valuable insights.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.771.1">We represent a time series as </span><em class="italic"><span class="koboSpan" id="kobo.772.1">Y1</span></em><span class="koboSpan" id="kobo.773.1">, ..., </span><em class="italic"><span class="koboSpan" id="kobo.774.1">Yt</span></em><span class="koboSpan" id="kobo.775.1">, ..., </span><em class="italic"><span class="koboSpan" id="kobo.776.1">Yn</span></em><span class="koboSpan" id="kobo.777.1">. </span><span class="koboSpan" id="kobo.777.2">As explained in the </span><em class="italic"><span class="koboSpan" id="kobo.778.1">Time series analysis</span></em><span class="koboSpan" id="kobo.779.1"> section, a time series reflects the chronological recording of experimental observations of a variable. </span><span class="koboSpan" id="kobo.779.2">This variable can encompass various elements, such as price trends, stock market indices, spreads, and unemployment rates. </span><span class="koboSpan" id="kobo.779.3">Essentially, it functions as a sequence of time-ordered data points from which we endeavor to extract insights for characterizing the observed phenomenon and predicting </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">future values.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.781.1">To measure the variation between two distinct time points (referred to as </span><em class="italic"><span class="koboSpan" id="kobo.782.1">t</span></em><span class="koboSpan" id="kobo.783.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.784.1">t + 1</span></em><span class="koboSpan" id="kobo.785.1">), we can compute the </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">following ratio:</span></span></p><p class="list-inset"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.787.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.788.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.789.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.790.1">t</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.791.1">+</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.792.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.793.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.794.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.795.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.796.1">t</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.797.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.798.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.799.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.800.1">Y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.801.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.802.1">t</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.803.1"> </span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.804.1">This index, commonly known as “percentage change,” represents a percentage ratio. </span><span class="koboSpan" id="kobo.804.2">It specifically denotes the percentage rate of variation in the phenomenon, </span><em class="italic"><span class="koboSpan" id="kobo.805.1">Y</span></em><span class="koboSpan" id="kobo.806.1">, at time </span><em class="italic"><span class="koboSpan" id="kobo.807.1">t + 1</span></em><span class="koboSpan" id="kobo.808.1"> compared to the preceding time, </span><em class="italic"><span class="koboSpan" id="kobo.809.1">t</span></em><span class="koboSpan" id="kobo.810.1">. </span><span class="koboSpan" id="kobo.810.2">Utilizing the percentage change method provides a more accurate depiction of how data has evolved over a </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">specified timeframe.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.812.1">This approach is applied not only in monitoring the prices of individual securities and major market indices but also in comparing the values of various currencies. </span><span class="koboSpan" id="kobo.812.2">When preparing balance sheets with comparative financial statements, it is typical to include specific asset prices at different time junctures, accompanied by the</span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.813.1"> corresponding</span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.814.1"> percentage changes during </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">those periods.</span></span></p></li>
<li><span class="koboSpan" id="kobo.816.1">To calculate percentage changes in MATLAB, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.818.1">
PctCh=100*diff(TeslaData.Close(:,1))./TeslaData.Close(1:end-1,1);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.819.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">diff()</span></strong><span class="koboSpan" id="kobo.821.1"> function computes the differences between consecutive elements of an array along the first dimension where the size of that dimension is not equal </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">to 1.</span></span></p></li> <li><span class="koboSpan" id="kobo.823.1">We can plot the results obtained as a bar plot to see how the differences are distributed on </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">the time:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.825.1">
bar(TeslaData.Date(1:end-1,1),PctCh)
xlabel('Time (years)')
ylabel('Percentage changes ($)')</span></pre><p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.826.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.827.1">.4</span></em><span class="koboSpan" id="kobo.828.1"> shows the bar plot of percentage changes over </span><span class="No-Break"><span class="koboSpan" id="kobo.829.1">the years:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer095">
<span class="koboSpan" id="kobo.830.1"><img alt="Figure 9.4 – Percentage changes over the years" src="image/B21156_09_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.831.1">Figure 9.4 – Percentage changes over the years</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.832.1">Percentage changes in the stock market play a crucial role in providing valuable insights and serving various functions. </span><span class="koboSpan" id="kobo.832.2">For example, percentage changes allow investors and analysts to measure the performance of individual stocks, portfolios, or entire market</span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.833.1"> indices over specific </span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.834.1">periods. </span><span class="koboSpan" id="kobo.834.2">This helps in assessing whether investments have grown or declined in value. </span><span class="koboSpan" id="kobo.834.3">Understanding percentage changes helps investors gauge the volatility and risk associated with a particular stock or asset. </span><span class="koboSpan" id="kobo.834.4">Stocks with larger percentage fluctuations are generally considered </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">riskier investments.</span></span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.836.1">After calculating the percentage changes, we can move on to the return concept; return refers to the profit or loss that an investor realizes from their investments in stocks. </span><span class="koboSpan" id="kobo.836.2">Returns are typically expressed as a percentage and represent the change in the value of an investment over a </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">specific period.</span></span></li>
<li><span class="koboSpan" id="kobo.838.1">Calculating the return from stock prices in MATLAB involves taking the percentage change in prices over a specific period. </span><span class="koboSpan" id="kobo.838.2">We have just loaded stock price data into MATLAB as a matrix. </span><span class="koboSpan" id="kobo.838.3">We checked that the data was organized with prices in chronological order. </span><span class="koboSpan" id="kobo.838.4">After that, we calculated the percentage change in stock prices from one period to the next. </span><span class="koboSpan" id="kobo.838.5">Now, we can calculate various types of returns, such as price return (capital gain) or total return (including income). </span><span class="koboSpan" id="kobo.838.6">For price return, simply sum the percentage changes, </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.840.1">
PriceReturn = sum(PctCh);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.841.1">The following result </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">is obtained:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.843.1">fprintf('Price Return: %.2f%%\n', PriceReturn);
Price Return: 698.80%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.844.1">This is a great return from the Tesla stock market that’s excellent for those who have held stocks for the last </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">three years.</span></span></p></li> <li><span class="koboSpan" id="kobo.846.1">Our goal is to leverage the data within this dataset to forecast the Tesla stock price based on the </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.847.1">information </span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.848.1">provided in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.849.1">.csv</span></strong><span class="koboSpan" id="kobo.850.1"> file. </span><span class="koboSpan" id="kobo.850.2">To train the network, we require both input and output data. </span><span class="koboSpan" id="kobo.850.3">The input is defined by the data contained within the dataset. </span><span class="koboSpan" id="kobo.850.4">Consequently, we must generate our output. </span><span class="koboSpan" id="kobo.850.5">We will achieve this by assuming our objective is to forecast the Tesla stock price at time </span><em class="italic"><span class="koboSpan" id="kobo.851.1">t + 1</span></em><span class="koboSpan" id="kobo.852.1"> based on the information available at time </span><em class="italic"><span class="koboSpan" id="kobo.853.1">t</span></em><span class="koboSpan" id="kobo.854.1">. </span><span class="koboSpan" id="kobo.854.2">As a result, we will establish the </span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">following formula:</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.856.1">I</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.857.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.858.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.859.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.860.1">t</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.861.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.862.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.863.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.864.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.865.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.866.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.867.1">D</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.868.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.869.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.870.1">a</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.871.1">.</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.872.1">C</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.873.1">l</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.874.1">o</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.875.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.876.1">e</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.877.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.878.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.879.1">)</span></span></span></p><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.880.1">O</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.881.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.882.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.883.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.884.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.885.1">t</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.886.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.887.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.888.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.889.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.890.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.891.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.892.1">D</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.893.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.894.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.895.1">a</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.896.1">.</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.897.1">C</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.898.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.899.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.900.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.901.1">e</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.902.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.903.1">t</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.904.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.905.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.906.1">)</span></span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.907.1">This is a sequence-to-sequence regression problem, such as a deep learning approach where the goal is to predict a sequence of continuous values as an output, given a sequence of input data. </span><span class="koboSpan" id="kobo.907.2">It is commonly used for tasks such as time series forecasting, natural language processing, and any other problem where the output is a sequence of numerical values. </span><span class="koboSpan" id="kobo.907.3">In this context, the model learns to map an input sequence to an output sequence, and the length of the input and output sequences </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">can vary.</span></span></p></li>
<li><span class="koboSpan" id="kobo.909.1">A recurrent network possesses memory, and this memory is preserved by defining the time step. </span><span class="koboSpan" id="kobo.909.2">The time step determines how many steps into the past backpropagation are considered when computing gradients for weight updates during training. </span><span class="koboSpan" id="kobo.909.3">In this context, we establish </span><em class="italic"><span class="koboSpan" id="kobo.910.1">TimeStep = 1</span></em><span class="koboSpan" id="kobo.911.1">. </span><span class="koboSpan" id="kobo.911.2">Let’s prepare the data based on </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">this assumption:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.913.1">
XData=TeslaData.Close(1:end-1);
YData=TeslaData.Close(2:end);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.914.1">Within this code, </span><strong class="source-inline"><span class="koboSpan" id="kobo.915.1">XData</span></strong><span class="koboSpan" id="kobo.916.1"> represents the input variable, equivalent to data at time </span><em class="italic"><span class="koboSpan" id="kobo.917.1">t</span></em><span class="koboSpan" id="kobo.918.1">, while </span><strong class="source-inline"><span class="koboSpan" id="kobo.919.1">YData</span></strong><span class="koboSpan" id="kobo.920.1"> signifies the output value for the subsequent period, namely data at time </span><em class="italic"><span class="koboSpan" id="kobo.921.1">t + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.922.1">1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.924.1">To address this type of problem, we can use an LSTM network. </span><span class="koboSpan" id="kobo.924.2">LSTM is</span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.925.1"> a type of </span><strong class="bold"><span class="koboSpan" id="kobo.926.1">recurrent neural network</span></strong><span class="koboSpan" id="kobo.927.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.928.1">RNN</span></strong><span class="koboSpan" id="kobo.929.1">) architecture</span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.930.1"> that’s designed to address the vanishing gradient problem that can occur when</span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.931.1"> training </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.932.1">traditional RNNs. </span><span class="koboSpan" id="kobo.932.2">LSTM models are especially effective for processing and making predictions on sequences of data. </span><span class="koboSpan" id="kobo.932.3">They are built around the concept of memory cells, which can store information over long sequences. </span><span class="koboSpan" id="kobo.932.4">These memory cells possess the capability to learn and remember patterns, rendering them well suited for tasks involving sequential data. </span><span class="koboSpan" id="kobo.932.5">Such tasks include time series analysis, natural language processing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">speech recognition.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.934.1">The ideal lag value should be chosen based on the autocorrelation plot together with domain knowledge. </span><span class="koboSpan" id="kobo.934.2">To evaluate the performance of an LSTM model, long sequences (more lags) should be considered. </span><span class="koboSpan" id="kobo.934.3">Even an auto-regressive model would perform well </span><span class="No-Break"><span class="koboSpan" id="kobo.935.1">for </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.936.1">lag=1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.938.1">LSTM models have three fundamental gates that control the flow of information within </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">the network:</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.940.1">Forget gate</span></strong><span class="koboSpan" id="kobo.941.1">: This is responsible for deciding which information from the previous cell state should be either forgotten </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">or retained</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.943.1">Input gate</span></strong><span class="koboSpan" id="kobo.944.1">: This determines which new information should be stored in the </span><span class="No-Break"><span class="koboSpan" id="kobo.945.1">cell state</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.946.1">Output gate</span></strong><span class="koboSpan" id="kobo.947.1">: This regulates the information to be output from the </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">cell state</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.949.1">The cell state runs horizontally across the top of the LSTM and acts as a conveyor belt, allowing information to flow from one time step to another. </span><span class="koboSpan" id="kobo.949.2">The gates help regulate the flow of information into and out of the </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">cell state.</span></span></p></li> <li><span class="koboSpan" id="kobo.951.1">We start by setting </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">some parameters:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.953.1">
TrainLength = length(TeslaData.Close)-1;
numHiddenUnits = 200;</span></pre></li> <li><span class="koboSpan" id="kobo.954.1">After that, we have</span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.955.1"> to </span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.956.1">build the architecture of the </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">deep network:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.958.1">
layers = [ ...
</span><span class="koboSpan" id="kobo.958.2">    sequenceInputLayer(TrainLength)
    lstmLayer(numHiddenUnits,'OutputMode','sequence')
    fullyConnectedLayer(50)
    dropoutLayer(0.5)
    fullyConnectedLayer(TrainLength)
    regressionLayer];</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.959.1">Six layers </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">were used:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.961.1">sequenceInputLayer</span></strong><span class="koboSpan" id="kobo.962.1">: A sequence input layer receives sequential data and performs data normalization before feeding it into a </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">neural network.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.964.1">lstmLayer</span></strong><span class="koboSpan" id="kobo.965.1">: An LSTM layer, as part of an RNN, specializes </span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.966.1">in capturing long-term dependencies within time series and sequential data. </span><span class="koboSpan" id="kobo.966.2">This layer facilitates additive interactions, contributing to enhanced gradient flow over extended sequences during the training process. </span><span class="koboSpan" id="kobo.966.3">Two parameters were passed: the number of hidden units and the output mode as a sequence. </span><span class="koboSpan" id="kobo.966.4">One argument was supplied: the quantity of hidden units, also referred to as the hidden size, denoted as a positive integer. </span><span class="koboSpan" id="kobo.966.5">This parameter determines the volume of information retained by the layer across time steps, encapsulated in the hidden state. </span><span class="koboSpan" id="kobo.966.6">The hidden state can encompass data from all preceding time steps, irrespective of the</span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.967.1"> sequence</span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.968.1"> length. </span><span class="koboSpan" id="kobo.968.2">Oversizing the number of hidden units may lead to overfitting the training data. </span><span class="koboSpan" id="kobo.968.3">Additionally, the output mode is specified with one of two values: </span><strong class="source-inline"><span class="koboSpan" id="kobo.969.1">sequence</span></strong><span class="koboSpan" id="kobo.970.1">, which produces the entire sequence as output, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">last</span></strong><span class="koboSpan" id="kobo.972.1">, which generates output for the last time step of </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">the sequence.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.974.1">fullyConnectedLayer</span></strong><span class="koboSpan" id="kobo.975.1">: A fully connected layer multiplies the input with a weight matrix and subsequently incorporates a </span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">bias vector.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.977.1">dropoutLayer</span></strong><span class="koboSpan" id="kobo.978.1">: This is used for regularization during training. </span><span class="koboSpan" id="kobo.978.2">It helps prevent overfitting by randomly setting a fraction of the input units to 0 during each forward and backward pass. </span><span class="koboSpan" id="kobo.978.3">This encourages the network to learn more robust and </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">generalized features.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.980.1">fullyConnectedLayer</span></strong><span class="koboSpan" id="kobo.981.1">: The last fully connected layer is often used for making predictions based on the features learned by the preceding layers in </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">the network.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.983.1">regressionLayer</span></strong><span class="koboSpan" id="kobo.984.1">: This is a layer that’s used in deep learning for regression tasks. </span><span class="koboSpan" id="kobo.984.2">Regression tasks involve predicting continuous numerical values, as opposed to classification tasks, which involve predicting categories or classes. </span><strong class="source-inline"><span class="koboSpan" id="kobo.985.1">regressionLayer</span></strong><span class="koboSpan" id="kobo.986.1"> is specifically designed for regression problems, and it is commonly used in neural networks for tasks such as predicting stock prices, house prices, temperature forecasting, and various other numerical predictions. </span><strong class="source-inline"><span class="koboSpan" id="kobo.987.1">regressionLayer</span></strong><span class="koboSpan" id="kobo.988.1"> is responsible for calculating the loss between the predicted values generated by the neural network and the actual target values during training. </span><span class="koboSpan" id="kobo.988.2">It employs a loss function to gauge the dissimilarity between the predicted values and the ground truth values. </span><span class="koboSpan" id="kobo.988.3">Common loss functions employed in regression tasks encompass </span><strong class="bold"><span class="koboSpan" id="kobo.989.1">mean squared error</span></strong><span class="koboSpan" id="kobo.990.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.991.1">MSE</span></strong><span class="koboSpan" id="kobo.992.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.993.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.994.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.995.1">MAE</span></strong><span class="koboSpan" id="kobo.996.1">), and </span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.997.1">Huber loss, </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">among</span></span><span class="No-Break"><a id="_idIndexMarker1058"/></span><span class="No-Break"><span class="koboSpan" id="kobo.999.1"> others.</span></span></li></ul></li> <li><span class="koboSpan" id="kobo.1000.1">After that, we</span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.1001.1"> have </span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.1002.1">to specify the </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">training options:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1004.1">
maxEpochs = 600;
miniBatchSize = 20;
options = trainingOptions('adam', ...
</span><span class="koboSpan" id="kobo.1004.2">    'MaxEpochs',maxEpochs, ...
</span><span class="koboSpan" id="kobo.1004.3">    'MiniBatchSize',miniBatchSize, ...
</span><span class="koboSpan" id="kobo.1004.4">    'InitialLearnRate',0.01, ...
</span><span class="koboSpan" id="kobo.1004.5">    'GradientThreshold',1, ...
</span><span class="koboSpan" id="kobo.1004.6">    'Shuffle','never', ...
</span><span class="koboSpan" id="kobo.1004.7">    'Plots','training-progress',...
</span><span class="koboSpan" id="kobo.1004.8">    'Verbose',0);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1005.1">As the optimization algorithm, we set the Adam optimizer, a popular optimization algorithm used in training deep neural networks. </span><span class="koboSpan" id="kobo.1005.2">It is known for its ability to efficiently handle non-stationary objectives, high-dimensional parameter spaces, and noisy gradient information. </span><span class="koboSpan" id="kobo.1005.3">Adam combines the benefits of both the RMSprop optimizer and momentum-based methods. </span><span class="koboSpan" id="kobo.1005.4">We also set the number of training epochs: an epoch is one complete pass through the entire training dataset. </span><span class="koboSpan" id="kobo.1005.5">Training may stop early based on validation performance to avoid overfitting. </span><span class="koboSpan" id="kobo.1005.6">We set an appropriate learning rate for the optimizer. </span><span class="koboSpan" id="kobo.1005.7">You may need to experiment with different learning rates to find the one that works best for </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">your problem.</span></span></p></li> <li><span class="koboSpan" id="kobo.1007.1">Now, we can train </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">the network:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1009.1">
net = trainNetwork(XData,YData,layers,options);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1010.1">The following results are printed (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1011.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1012.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer096">
<span class="koboSpan" id="kobo.1014.1"><img alt="Figure 9.5 – LSTM training process" src="image/B21156_09_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1015.1">Figure 9.5 – LSTM training process</span></p>
<ol>
<li value="15"><span class="koboSpan" id="kobo.1016.1">Now, we can use the</span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.1017.1"> network</span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.1018.1"> to predict the performance of stocks. </span><span class="koboSpan" id="kobo.1018.2">Each value will be predicted starting from the previous one with a time step equal </span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">to 1:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1020.1">
YPred = predict(net,XData,'MiniBatchSize',1);</span></pre></li> <li><span class="koboSpan" id="kobo.1021.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1022.1">predict()</span></strong><span class="koboSpan" id="kobo.1023.1"> function forecasts outcomes by employing a well-trained deep learning neural network. </span><span class="koboSpan" id="kobo.1023.2">To evaluate the performance of the prediction, we can use </span><strong class="bold"><span class="koboSpan" id="kobo.1024.1">root mean square error</span></strong><span class="koboSpan" id="kobo.1025.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1026.1">RMSE</span></strong><span class="koboSpan" id="kobo.1027.1">), which</span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.1028.1"> is calculated </span><span class="No-Break"><span class="koboSpan" id="kobo.1029.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1030.1">
RMSE = sqrt(mean((YData-YPred).^2));</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1031.1">RMSE is a widely utilized metric in statistics and machine learning for quantifying the average magnitude of errors or differences between predicted and actual values. </span><span class="koboSpan" id="kobo.1031.2">It calculates the square root of the mean of the squared differences between predicted values and true values. </span><span class="koboSpan" id="kobo.1031.3">RMSE serves as a means to evaluate the accuracy of a predictive model or the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">of predictions.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1033.1">The following value </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">is returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1035.1">RMSE =
   13.9792</span></pre></li> <li><span class="koboSpan" id="kobo.1036.1">To appreciate the prediction capacity of the model based on LSTMs, let’s draw a graph of the </span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.1037.1">performance </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.1038.1">of the Tesla stock and compare it with that predicted by </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">our model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1040.1">
plot(TeslaData.Date(2:end), YData,TeslaData.Date(2:end), YPred,'--')
xlabel('Time (years)')
ylabel('Stock price ($)')
legend('ActualData', 'PredictedData')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1041.1">The following graph is printed (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1042.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1043.1">.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1044.1">):</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer097">
<span class="koboSpan" id="kobo.1045.1"><img alt="Figure 9.6 – Actual values versus predicted values" src="image/B21156_09_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1046.1">Figure 9.6 – Actual values versus predicted values</span></p>
<p><span class="koboSpan" id="kobo.1047.1">We can appreciate the goodness of the forecast in periods in which the situation does not undergo major variations. </span><span class="koboSpan" id="kobo.1047.2">On the contrary, a certain deviation is noted in correspondence with</span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.1048.1"> the </span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.1049.1">periods of greatest fluctuation of the security, demonstrating that it is necessary to consider further information for a </span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">correct prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.1051.1">Now, let’s see what we can do when we must work with </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">unbalanced data.</span></span></p>
<h1 id="_idParaDest-194"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.1053.1">Dealing with imbalanced datasets in MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.1054.1">Dealing with imbalanced </span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.1055.1">datasets is a common challenge in machine learning, particularly in classification tasks where one class significantly outnumbers the other(s). </span><span class="koboSpan" id="kobo.1055.2">Handling imbalanced datasets is crucial because models trained on such data may exhibit bias toward the majority class and perform poorly in predicting the </span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">minority class.</span></span></p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.1057.1">Understanding oversampling</span></h2>
<p><span class="koboSpan" id="kobo.1058.1">Oversampling is </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.1059.1">a method that’s employed</span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.1060.1"> to tackle class imbalance in a dataset by augmenting the number of instances belonging to the minority class. </span><span class="koboSpan" id="kobo.1060.2">The aim is to balance the class distribution and prevent machine learning models from being biased toward the majority class. </span><span class="koboSpan" id="kobo.1060.3">Oversampling is particularly useful when you have limited data for the minority class. </span><span class="koboSpan" id="kobo.1060.4">There are several methods for oversampling, including </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1062.1">Random oversampling</span></strong><span class="koboSpan" id="kobo.1063.1">: In random </span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.1064.1">oversampling, you</span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.1065.1"> randomly select and duplicate instances from the minority class until the class distribution is balanced. </span><span class="koboSpan" id="kobo.1065.2">This method is straightforward but may lead to overfitting as it introduces duplicate </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1067.1">Synthetic Minority Over-sampling Technique</span></strong><span class="koboSpan" id="kobo.1068.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1069.1">SMOTE</span></strong><span class="koboSpan" id="kobo.1070.1">): SMOTE is a popular </span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.1071.1">oversampling method that creates synthetic samples for the minority class. </span><span class="koboSpan" id="kobo.1071.2">It works by selecting a minority class instance and its </span><em class="italic"><span class="koboSpan" id="kobo.1072.1">k</span></em><span class="koboSpan" id="kobo.1073.1"> nearest neighbors. </span><span class="koboSpan" id="kobo.1073.2">Then, it generates synthetic data points along the line segments connecting the chosen instance and its neighbors. </span><span class="koboSpan" id="kobo.1073.3">SMOTE helps to prevent overfitting and can be more effective than </span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">random oversampling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1075.1">Adaptive Synthetic Sampling</span></strong><span class="koboSpan" id="kobo.1076.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1077.1">ADASYN</span></strong><span class="koboSpan" id="kobo.1078.1">): ADASYN is an extension of SMOTE that’s</span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.1079.1"> designed to generate additional synthetic samples for minority class instances that </span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.1080.1">pose greater difficulty in classification. </span><span class="koboSpan" id="kobo.1080.2">It considers the distribution of the data and assigns different weights to </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">different instances.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1082.1">Borderline-SMOTE</span></strong><span class="koboSpan" id="kobo.1083.1">: Borderline-SMOTE is a</span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.1084.1"> variation of SMOTE that specifically targets instances near the borderline between the minority and majority classes. </span><span class="koboSpan" id="kobo.1084.2">It generates synthetic samples for these borderline instances to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">classification performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1086.1">Minority Synthetic Over-sampling Technique</span></strong><span class="koboSpan" id="kobo.1087.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1088.1">MSMOTE</span></strong><span class="koboSpan" id="kobo.1089.1">): MSMOTE is an extension</span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.1090.1"> of SMOTE that takes into account the density of minority class instances. </span><span class="koboSpan" id="kobo.1090.2">It generates synthetic </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.1091.1">samples based on the local density of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1093.1">Random oversampling with noise</span></strong><span class="koboSpan" id="kobo.1094.1">: This technique involves adding a small amount of random noise to the duplicated instances during random oversampling. </span><span class="koboSpan" id="kobo.1094.2">This helps to </span><span class="No-Break"><span class="koboSpan" id="kobo.1095.1">reduce overfitting.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1096.1">Informed oversampling</span></strong><span class="koboSpan" id="kobo.1097.1">: Informed oversampling methods consider domain knowledge or specific characteristics of the data to guide the generation of </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">synthetic</span></span><span class="No-Break"><a id="_idIndexMarker1079"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1099.1"> samples.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1100.1">The choice of oversampling method depends on the characteristics of your dataset and the problem you are trying to solve. </span><span class="koboSpan" id="kobo.1100.2">Experimenting with different techniques and evaluating the impact on model performance using appropriate metrics (for example, F1 score, precision-recall curves, and so on) is often necessary to determine the most effective oversampling strategy for your specific use case. </span><span class="koboSpan" id="kobo.1100.3">It’s important to note that oversampling may not </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.1101.1">always be the best solution, and you should also consider other techniques, such as undersampling, cost-sensitive learning, and ensemble methods, to address class </span><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">imbalance effectively.</span></span></p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.1103.1">Exploring undersampling</span></h2>
<p><span class="koboSpan" id="kobo.1104.1">Undersampling is a</span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.1105.1"> technique that’s </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.1106.1">used to address class imbalance in a dataset by reducing the number of instances in the majority class. </span><span class="koboSpan" id="kobo.1106.2">The goal is to create a more balanced class distribution, which can help machine learning models avoid bias toward the majority class. </span><span class="koboSpan" id="kobo.1106.3">Undersampling can be a useful approach when you have a large amount of data in the majority class and want to avoid the potential issues of overfitting that may arise with oversampling the minority class. </span><span class="koboSpan" id="kobo.1106.4">Here are some key points to consider when </span><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">exploring undersampling:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1108.1">Random undersampling</span></strong><span class="koboSpan" id="kobo.1109.1">: In random undersampling, you randomly select and remove instances from the majority class until the class distribution is balanced. </span><span class="koboSpan" id="kobo.1109.2">This method is simple but may lead to the loss of </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">valuable data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1111.1">Cluster-centroid undersampling</span></strong><span class="koboSpan" id="kobo.1112.1">: Cluster-centroid undersampling involves clustering the majority class instances and selecting a representative centroid from each cluster to form a smaller dataset. </span><span class="koboSpan" id="kobo.1112.2">This method retains diversity within the majority class and is less likely to remove </span><span class="No-Break"><span class="koboSpan" id="kobo.1113.1">informative data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1114.1">Tomek links</span></strong><span class="koboSpan" id="kobo.1115.1">: Tomek links are pairs of instances from different classes that are close to each other but classified incorrectly. </span><span class="koboSpan" id="kobo.1115.2">Removing the majority class instance of a Tomek link can help improve the boundary </span><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">between classes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1117.1">Edited nearest neighbors</span></strong><span class="koboSpan" id="kobo.1118.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1119.1">ENN</span></strong><span class="koboSpan" id="kobo.1120.1">): ENN </span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.1121.1">identifies and removes instances from the majority class that do not agree with their neighbors in terms of class labels. </span><span class="koboSpan" id="kobo.1121.2">ENN can help in reducing noisy samples in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1122.1">majority class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1123.1">One-sided selection</span></strong><span class="koboSpan" id="kobo.1124.1">: One-sided selection combines both Tomek links and ENN. </span><span class="koboSpan" id="kobo.1124.2">It uses Tomek links to identify problematic instances and then uses ENN to </span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">remove them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1126.1">NearMiss undersampling</span></strong><span class="koboSpan" id="kobo.1127.1">: This method selects the majority class instances that are closest to minority class instances based on distance metrics. </span><span class="koboSpan" id="kobo.1127.2">NearMiss algorithms ensure that the selected majority class instances are close to the minority class, thus helping in improving </span><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">class separation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1129.1">Informed undersampling</span></strong><span class="koboSpan" id="kobo.1130.1">: Informed undersampling methods consider domain knowledge or specific characteristics of the data to guide the selection of instances from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1131.1">majority class.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1132.1">When exploring </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.1133.1">undersampling, it’s </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.1134.1">important to consider various aspects. </span><span class="koboSpan" id="kobo.1134.2">First, choose the undersampling method that is most appropriate for your dataset. </span><span class="koboSpan" id="kobo.1134.3">The method you choose should depend on the nature of your data, the problem you are trying to solve, and the potential impact on model performance. </span><span class="koboSpan" id="kobo.1134.4">Also, undersampling may lead to a loss of information, especially if the majority class contains important data points. </span><span class="koboSpan" id="kobo.1134.5">Be cautious when applying this technique, and consider whether you can obtain more data for the minority class or explore other methods such as oversampling, cost-sensitive learning, or ensemble methods. </span><span class="koboSpan" id="kobo.1134.6">Remember to evaluate your model’s performance using appropriate metrics, and experiment with different approaches to find the best solution for your specific use case. </span><span class="koboSpan" id="kobo.1134.7">For instance, suppose you have a dataset with 10,000 credit card transactions, out of which only 100 are fraudulent. </span><span class="koboSpan" id="kobo.1134.8">In this case, you might undersample the non-fraudulent transactions to create a more balanced dataset. </span><span class="koboSpan" id="kobo.1134.9">You could randomly select, say, 500 non-fraudulent transactions, resulting in</span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.1135.1"> a new dataset with 500 non-fraudulent</span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.1136.1"> and 100 </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">fraudulent transactions.</span></span></p>
<h3><span class="koboSpan" id="kobo.1138.1">Discovering cost-sensitive learning</span></h3>
<p><span class="koboSpan" id="kobo.1139.1">Cost-sensitive learning is a</span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.1140.1"> machine learning</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.1141.1"> approach that considers the varying costs of misclassifying different classes in a classification problem. </span><span class="koboSpan" id="kobo.1141.2">In traditional machine learning, all misclassifications are treated equally. </span><span class="koboSpan" id="kobo.1141.3">However, in many real-world applications, misclassifying one class may have more severe consequences or higher associated costs than misclassifying another class. </span><span class="koboSpan" id="kobo.1141.4">Cost-sensitive learning aims to optimize the model to minimize these </span><span class="No-Break"><span class="koboSpan" id="kobo.1142.1">specific costs.</span></span></p>
<p><span class="koboSpan" id="kobo.1143.1">Cost-sensitive learning is related to but different from handling class imbalance. </span><span class="koboSpan" id="kobo.1143.2">While class imbalance focuses on the unequal distribution of classes, cost-sensitive learning considers the associated costs of misclassification. </span><span class="koboSpan" id="kobo.1143.3">Cost-sensitive learning often involves the use of cost matrices, which specify the costs of different types of misclassifications. </span><span class="koboSpan" id="kobo.1143.4">These matrices assign different costs for false positives, false negatives, true positives, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1144.1">true negatives.</span></span></p>
<p><span class="koboSpan" id="kobo.1145.1">In cost-sensitive learning, the decision threshold for classification can be adjusted to minimize the expected cost. </span><span class="koboSpan" id="kobo.1145.2">This means that the model might be more or less conservative in its predictions, depending on the cost matrix. </span><span class="koboSpan" id="kobo.1145.3">Various machine learning algorithms can be adapted to handle cost-sensitive learning. </span><span class="koboSpan" id="kobo.1145.4">Some algorithms allow you to specify class-specific misclassification costs. </span><span class="koboSpan" id="kobo.1145.5">For example, in decision trees or ensemble methods, you can assign weights to classes based on </span><span class="No-Break"><span class="koboSpan" id="kobo.1146.1">their costs.</span></span></p>
<p><span class="koboSpan" id="kobo.1147.1">When working with cost-sensitive learning, it’s important to consider evaluation metrics that reflect the cost-effectiveness of the model, such as the weighted F1 score or cost-sensitive versions of precision and recall. </span><span class="koboSpan" id="kobo.1147.2">Cost-sensitive learning is often applied to anomaly detection problems. </span><span class="koboSpan" id="kobo.1147.3">In such cases, you want to detect rare and potentially costly events, such as fraud detection in </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">financial transactions.</span></span></p>
<p><span class="koboSpan" id="kobo.1149.1">Keep in mind that cost-sensitive learning involves trade-offs. </span><span class="koboSpan" id="kobo.1149.2">Reducing the cost of one type of error may increase the cost of another. </span><span class="koboSpan" id="kobo.1149.3">Careful consideration and domain knowledge are required when designing the cost matrix. </span><span class="koboSpan" id="kobo.1149.4">Cost-sensitive learning is a valuable approach in situations where misclassification costs are not uniform, and it can help optimize machine learning models for specific real-world applications. </span><span class="koboSpan" id="kobo.1149.5">It’s an essential technique</span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.1150.1"> for decision-making </span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.1151.1">systems where the consequences of different errors can be </span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">significantly different.</span></span></p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.1153.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1154.1">In this chapter, we explored several key concepts in the field of data analysis and predictive modeling. </span><span class="koboSpan" id="kobo.1154.2">We started by discussing the basics of time series data, which refers to data that is collected over a certain period and contains a sequential order. </span><span class="koboSpan" id="kobo.1154.3">The extraction of statistics from such sequential data is then highlighted as an important step in analyzing and understanding patterns within </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.1156.1">This chapter also emphasized the implementation of a model to predict stock market data. </span><span class="koboSpan" id="kobo.1156.2">This involves using various techniques and algorithms to analyze historical stock market data, identify patterns and trends, and make predictions about future </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">stock prices.</span></span></p>
<p><span class="koboSpan" id="kobo.1158.1">Lastly, this chapter addressed the challenge of dealing with imbalanced datasets in MATLAB. </span><span class="koboSpan" id="kobo.1158.2">An imbalanced dataset refers to a situation where the distribution of classes within the dataset is significantly skewed, making it difficult to train a model accurately. </span><span class="koboSpan" id="kobo.1158.3">We discussed methods and strategies to handle imbalanced datasets within the MATLAB </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">programming environment.</span></span></p>
<p><span class="koboSpan" id="kobo.1160.1">Overall, this chapter focused on the importance of understanding and analyzing time series data, extracting meaningful statistics, implementing predictive models for stock market data, and addressing the challenges of imbalanced datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">in MATLAB.</span></span></p>
<p><span class="koboSpan" id="kobo.1162.1">In the next chapter, we will understand the basic concepts of recommender systems, how to identify similar users in a dataset, and how to implement a practical case of recommender systems using MATLAB. </span><span class="koboSpan" id="kobo.1162.2">Finally, we will understand model compression, pruning, and quantization for efficient inference on </span><span class="No-Break"><span class="koboSpan" id="kobo.1163.1">edge devices.</span></span></p>
</div>
</body></html>