- en: 'Chapter 12: Model Serving and Monitoring'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will reflect on the need to serve and monitor **machine
    learning** (**ML**) models in production and explore different means of serving
    ML models for users or consumers of the model. Then, we will revisit the **Explainable
    Monitoring framework** from [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Key Principles for Monitoring Your ML System*, and implement it for the business
    use case we have been solving using MLOps to predict the weather. The implementation
    of an Explainable Monitoring framework is hands-on. We will infer the deployed
    API and monitor and analyze the inference data using **drifts** (such as data
    drift, feature drift, and model drift) to measure the performance of an ML system.
    Finally, we will look at several concepts to govern ML systems for the robust
    performance of ML systems to drive continuous learning and delivery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by reflecting on the need to monitor ML in production. Then, we
    will move on to explore the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Serving, monitoring, and maintaining models in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring different modes of serving ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Explainable Monitoring framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Governing your ML system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serving, monitoring, and maintaining models in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is no point in deploying a model or an ML system and not monitoring it.
    Monitoring performance is one of the most important aspects of an ML system. Monitoring
    enables us to analyze and map out the business impact an ML system offers to stakeholders
    in a qualitative and quantitative manner. In order to achieve maximum business
    impact, users of ML systems need to be served in the most convenient manner. After
    that, they can consume the ML system and generate value. In previous chapters,
    we developed and deployed an ML model to predict the weather conditions at a port
    as part of the business use case that we had been solving for practical implementation.
    In this chapter, we will revisit the Explainable Monitoring framework that we
    discussed in [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206), *Key
    Principles for Monitoring Your ML System*, and implement it within our business
    use case. In *Figure 12.1*, we can see the **Explainable Monitoring** framework
    and some of its components, as highlighted in green:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Components of the Explainable Monitoring framework to be implemented'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Components of the Explainable Monitoring framework to be implemented
  prefs: []
  type: TYPE_NORMAL
- en: 'We will implement Explainable Monitoring for these areas: *Data Integrity*,
    *Model Drift*, *Application Performance*, *Bias and Threat Detection*, *Local
    and Global Explanations*, *Alerts and Actions*, *Model QA and Control*, and *Model
    Auditing and Reports*. These components are the most significant, in our use case,
    to understand the implementation of Explainable Monitoring. We will leave out
    *Data Slicing* because we do not have much variety in terms of the demographics
    or samples within the data (for example, sex, age groups, and more). By using
    information from other components, we can assess the model''s performance and
    its fairness. In this chapter, we will implement components of the **Monitor**
    and **Analyze** modules: *Data Integrity*, *Model Drift*, *Application Performance*,
    *Bias and Threat Detection*, and *Local and Global Explanations*. The remaining
    component implementations will be covered in [*Chapter 13*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234),
    *Governing the ML System for Continual Learning*. Before we move on to the implementation
    process, let''s take a look at how models can be served for users to consume.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring different modes of serving ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will consider how a model can be served for users (both
    humans and machines) to consume the ML service efficiently. Model serving is a
    critical area, which an ML system needs to succeed at to fulfill its business
    impact, as any lag or bug in this area can be costly in terms of serving users.
    Robustness, availability, and convenience are key factors to keep in mind while
    serving ML models. Let''s take a look at some ways in which ML models can be served:
    this can be via batch service or on-demand mode (for instance, when a query is
    made on demand in order to get a prediction). A model can be served to either
    a machine or a human user in on-demand mode. Here is an example of serving a model
    to a user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Serving a model to users'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Serving a model to users
  prefs: []
  type: TYPE_NORMAL
- en: In a typical scenario (in on-demand mode), a model is served as a service for
    users to consume, as shown in *Figure 12.2*. Then, an external application on
    a machine or a human makes a query to the prediction or ML service using their
    data. The ML service, upon receiving a request, uses a load balancer to route
    the request to an available resource (such as a container or an application) within
    the ML application. The load balancer also manages resources within the ML service
    to orchestrate and generate new containers or resources on demand. The load balance
    redirects the query from the user to the model running in a container within the
    ML application to get the prediction. On getting the prediction, the load balance
    reverts back to the external application on a machine, or to a human who is making
    the request, or to the query within the model prediction. In this way, the ML
    service is able to serve its users. The ML system orchestrates with the model
    store or registry to keep itself updated with either the latest or best-performing
    models in order to serve the users in the best manner. In comparison to this typical
    scenario where users make a query, there is another use case where the model is
    served as a batch service.
  prefs: []
  type: TYPE_NORMAL
- en: Serving the model as a batch service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Batch processing or serving is applied to large quantities or batches of input
    data (that is, not single observations but bunches of observations together).
    In cases where there is a large bunch of data to be inferred, a model is normally
    served in batch mode. One example of this is when the model is used to process
    the data of all consumers or users of a product or service in one go. Alternatively,
    a batch of data from a factory for a fixed timeline might need to be processed
    to detect anomalies in the machines. Compared to on-demand mode, batch mode is
    more resource-efficient and is usually employed when some latency can be afforded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Batch Inference'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Batch inference
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of batch processing is that unlike a REST API-based
    service, a batch service might require lighter or less infrastructure. Writing
    a batch job is easier for a data scientist compared to deploying an online REST
    service. This is because the data scientist just needs to train a model or deserialize
    a trained model on a machine and perform batch inference on a batch of data. The
    results of batch inference can be stored in a database as opposed to sending responses
    to users or consumers. However, one major disadvantage is the high latency and
    it not being in real time. Typically, a batch service can process hundreds or
    thousands of features at once. A series of tests can be used to determine the
    optimal batch size to arrive at an acceptable latency for the use case. Typical
    batch sizes can be 32, 64, 128, or 518 to the power of 2\. Batch inference can
    be scheduled periodically and can serve many use cases where latency is not an
    issue. One such example is discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: A real-world example
  prefs: []
  type: TYPE_NORMAL
- en: One real-world example is a bank extracting information from batches of text
    documents. A bank receives thousands of documents a day from its partner institutions.
    It is not possible for a human agent to read through all of them and highlight
    any red flags in the operations listed in the documents. Batch inferencing is
    used to extract name entities and red flags from all the documents received by
    the bank in one go. The results of the batch inference or serving are then stored
    in a database.
  prefs: []
  type: TYPE_NORMAL
- en: Serving the model to a human user
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before processing a request from human users, it is essential to check whether
    the user has adequate permissions to use the model. Additionally, in most cases,
    it is helpful to know the context in which the request was made. Gathering the
    context of the request will enable the model to produce better predictions. After
    gathering the context, we can transform it into model-readable input and infer
    the model to get a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, here are the key steps in serving an on-demand model to human
    users:'
  prefs: []
  type: TYPE_NORMAL
- en: Validate or authorize the request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze and gather contextual information (for example, historic data, user
    experience data, or any other personal data of the user).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform any contextual information into a model-readable input or schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Infer the model with input data (with the request and contextual information)
    to make a prediction or get an output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret the output as per the context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Relay the output to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A real-world example
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Consider a chatbot serving human customers to book flight tickets. It performs
    contextual inference to serve human users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Serving the model to a machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can serve a machine or an external application using a `REST` API or a streaming
    service based on the use case. Typically, machine inference data requirements
    are either predetermined or within a standard schema. A well-defined topology
    and data schema in the form of a REST API or streaming service will work. Serving
    on demand to a machine or human varies from case to case, as, in some scenarios,
    demand may vary (for example, at a particular time of day when the demand for
    serving the user might be high, such as in the afternoon). To handle a high demand
    from the service, autoscaling (on the cloud) can help spawn more resources on
    demand and kill any idle resources to free up more resources. However, autoscaling
    is not a one-stop solution for scaling, as it cannot handle sudden or peculiar
    spikes in demand on its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Message Broker for on-demand serving'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Message broker for on-demand serving
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach shown in *Figure 12.4* is resource-efficient to handle high volume
    demand spikes. To handle sudden spikes, message brokers such as Apache Kafka or
    Spark can be useful. A message broker runs processes to write and read to a queue:
    one process to write messages in a queue and another process to read from that
    queue. The served model is periodically connected to the message broker to process
    batches of input data from the queue to make predictions for each element in the
    batch. After processing the input data batches and generating predictions, the
    predictions are written to the output queue, which is then pushed to the users
    as per their requests.'
  prefs: []
  type: TYPE_NORMAL
- en: A real-world example
  prefs: []
  type: TYPE_NORMAL
- en: Consider a social media company that has millions of users. The company uses
    a single or common ML model for the recommender system to recommend newsfeed articles
    or posts to users. As the volume of requests is high in order to serve many users,
    it cannot depend on a REST API-based ML system (as it is synchronous). A streaming
    solution is better as it provides asynchronous inference for the company to serve
    its users. When a user logs into their application or account hosted on a machine
    (such as a social media company server), the application running on their machine
    infers the ML model (that is, the recommender system) via a streaming service
    for recommendations for the user newsfeed. Likewise, thousands of other users
    log in at the same time. The streaming service can serve all of these users seamlessly.
    Note that this wouldn't have been possible with the REST API service. By using
    a streaming service for the recommender system model, the social media company
    is able to serve its high volume of users in real time, avoiding significant lags.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Explainable Monitoring framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement the Explainable Monitoring framework, it is worth doing a recap
    of what has been discussed so far, in terms of implementing hypothetical use cases.
    Here is a recap of what we did for our use case implementation, including the
    problem and solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem context**: You work as a data scientist in a small team with three
    other data scientists for a cargo shipping company based in the port of Turku
    in Finland. 90% of the goods imported into Finland arrive via cargo shipping at
    various ports across the country. For cargo shipping, weather conditions and logistics
    can be challenging at times. Rainy conditions can distort operations and logistics
    at the ports, which can affect supply chain operations. Forecasting rainy conditions
    in advance allows us to optimize resources such as human resources, logistics,
    and transport resources for efficient supply chain operations at ports. Business-wise,
    forecasting rainy conditions in advance enables ports to reduce their operational
    costs by up to approximately 20% by enabling the efficient planning and scheduling
    of human resources, logistics, and transport resources for supply chain operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task or solution**: You, as a data scientist, are tasked to develop an ML-driven
    solution to forecast weather conditions 4 hours in advance at the port of Turku
    in Finland. This will enable the port to optimize its resources, thereby enabling
    cost savings of up to 20%. To get started, you are provided with a historic weather
    dataset with a 10-year-timeline from the port of Turku (the dataset can be accessed
    in the Git repository of this book). Your task is to build a continuous learning-driven
    ML solution to optimize operations at the port of Turku.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have developed ML models and deployed them as REST API endpoints
    inside a Kubernetes cluster at [http://20.82.202.164:80/api/v1/service/weather-prod-service/score](http://20.82.202.164:80/api/v1/service/weather-prod-service/score)
    (the address of your endpoint will be different).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will replicate a real-life inference scenario for this endpoint. To
    do this, we will use the test dataset we had split and registered in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines*,
    in the *Data ingestion and feature engineering* section. Go to your Azure ML workspace
    and download the `test_data.csv` dataset (which was registered as `test_dataset`)
    from the **Datasets** section or the Blob storage that is connected to your workspace,
    as shown in *Figure 12.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Downloading the validation dataset (which was previously split
    and registered)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0051.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Downloading the validation dataset (which was previously split
    and registered)
  prefs: []
  type: TYPE_NORMAL
- en: 'Get ready to infer the `test_data.csv` data with the REST API endpoint or ML
    service. Go to the `12_Model_Serving_Monitoring` folder and place the downloaded
    dataset (`test_data.csv`) inside the folder. Next, access the `inference.` `py`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the `inference.py` file, begin by importing the necessary libraries, such
    as `json`, `requests`, and `pandas`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, import the dataset (`test_data.csv`) to use to infer with the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the unnecessary columns for inference, such as `Timestamp`, `Location`,
    and `Future_weather_condition` (we will predict this final column by querying
    the endpoint).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, point to the URL of the endpoint (you can find this by navigating to **Azure
    ML Workspace** | **Endpoints** | **Weather-prod-service** | **Consume**). For
    simplicity, since we did not have authentication or keys set up for the service,
    we have the header application/JSON with no keys or authentication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we will loop through the data array by inferring each element in the
    array with the endpoint. To run the script, simply replace `''url''` with your
    endpoint and run the following command in the Terminal (from the folder location)
    to execute the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The running script will take around 10–15 minutes to infer all of the elements
    of the inference data. After this, we can monitor the inference and analyze the
    results of the inferring data. Let's monitor and analyze this starting with data
    integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring your ML system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Monitor** module is dedicated to monitoring the application in production
    (that is, serving the ML model). The action monitor module has the following three
    functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data integrity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -To register the target dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -To create a data drift monitor
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -To perform data drift analysis
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: -To perform feature drift analysis
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's take a look at each of these functionalities in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Data integrity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To monitor data integrity for inference data, we need to monitor data drift
    and feature drift to see whether there are any anomalous changes in the incoming
    data or any new patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data drift**: This is when the properties of the independent variables change.
    For example, data changes can occur due to seasonality or the addition of new
    products or changes in consumer desires or habits, as it did during the COVID-19
    pandemic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature drift**: This is when properties of the feature(s) change over time.
    For example, the temperature is changing due to changing seasons or seasonality,
    that is, in summer, the temperature is warmer compared to temperatures during
    winter or autumn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To monitor drifts, we will measure the difference for the baseline dataset
    versus the target dataset. The first step is to define the baseline dataset and
    the target dataset. This depends on use case to use case; we will use the following
    datasets as the baseline and target datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Baseline dataset**: This is the training dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target dataset**: This is the inference dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use the training dataset that we previously used to train our models
    as the baseline dataset. This is because the model used in inference knows the
    patterns in the training dataset very well. The training dataset is ideal for
    comparing how inference data changes over time. We will compile all the inference
    data collected during inference into the inference dataset and compare these two
    datasets (that is, the baseline dataset and the target dataset) to gauge data
    and feature drifts for the target dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Registering the target dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training dataset was registered in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*, in the *Data ingestion and feature engineering*
    section. We need to register the inference dataset within the **Datasets** section
    of the Azure ML workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inference data is collected as a result of using the `azureml.monitoring` SDK
    (the `modelDataCollector` function). By enabling monitoring functions using the
    `modelDataCollector` function in your scoring file (in `score.py`, as we did in
    [*Chapter 6*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124), *Key Principles
    of Deploying Your ML System*), we store inference data in the form of a time-series
    dataset in the Blob storage. In the Blob storage connected to your Azure ML workspace,
    inference data is stored in the `modeldata` container. In the `modeldata` container,
    the inference data (including both inputs and outputs) is stored the form of CSV
    files that are partitioned inside folders. These are structured as per year, per
    month, and per day (when the inference data was recorded in the production). Inside
    the partitioned folders, inference data is stored in CSV files that are named
    `inputs.csv` and `outputs.csv`. We need to register these `input.csv` files to
    monitor data drift and feature drift. Follow these steps to register the `input.csv`
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Datasets** section and click on **Create dataset**. Then, select
    the **From datastore** option, as shown in *Figure 12.6*:![Figure 12.6 – Registering
    the inference dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.6 – Registering the inference dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Name the dataset (for example, `Inputs-Inference-Dataset`), select the dataset
    type as **Tabular**, and write an appropriate description in the **Description**
    field name by describing the purpose of your dataset. Click on **Next** to specify
    the datastore selection. Select the **modeldata** datastore, as shown in *Figure
    12.7*:![Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0071.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the `input.csv` file. You can find this in the folder of your
    `support vectorclassifier model`, which is inside the folder with your service
    name (for example, `prod-webservice`). Then, go into the subfolders (the default,
    inputs, and folders structured with dates), and go to the folder of your current
    date to find the `input.csv` file. Select the `input.csv` file, as shown in *Figure
    12.8*:![Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data
    registration)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data
    registration)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the `input.csv` file, click on the `/**/inputs*.csv` (as shown
    in *Figure 12.9*). This is an important step that will refer to all of the `input.csv`
    files in the `inputs` folder dynamically. Without referencing all of the `input.csv`
    files, we will confine the path to only one `input.csv` file (which was selected
    previously in *Figure 12.8*). By referring to all of the `input.csv` files, we
    will compile all of the input data (the `inputs.csv` files) into the target dataset
    (for example, `Inputs-Inference-Data`):![Figure 12.9 – Referencing the path to
    dynamically access all the input.csv files
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0091.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.9 – Referencing the path to dynamically access all the input.csv files
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Next** button to advance to **Settings and preview**:![Figure
    12.10 – Settings and preview (the inference dataset registration)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.10 – Settings and preview (the inference dataset registration)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As shown in *Figure 12.10*, we can configure the settings and preview the dataset.
    Point to the correct column names by selecting the **Column headers** dropdown
    and then selecting **Combine headers from all files**. Check for the correct column
    names (for example, **Temperature_C** and **Humidity**). After selecting the appropriate
    column names, click on the **Next** button to advance to the next window. Select
    the right schema by selecting all the columns you would like to monitor, along
    with their data types, as shown in *Figure 12.11*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Schema selection (the inference dataset registration)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image0111.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.11 – Schema selection (the inference dataset registration)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make sure that you select the **Timestamp** and **Date** properties in the **$aml_dc_scoring_timestamp**
    column as these contain the timestamps of the inference. This step is important.
    Only a time-series format dataset can be used to compute drift (by the Azure drift
    model); otherwise, we cannot compute drift. After selecting the right schema by
    selecting all of the columns, click on **Next** to confirm all of the necessary
    details (such as the name of the dataset, the dataset version, its path, and more).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on the **Create** button to create the dataset. When your dataset has
    been created successfully, you can view the dataset from the **Dataset** section
    in your Azure ML workspace. Go to the **Datasets** section to confirm your dataset
    has been created. Identify and click on your created dataset. Upon clicking, you
    will be able to view the details of your registered inference dataset, as shown
    in *Figure 12.12*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Viewing the registered inference dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Viewing the registered inference dataset
  prefs: []
  type: TYPE_NORMAL
- en: You can see all the essential attributes of your registered dataset in *Figure
    12.12*. It is important to note that the relative path is dynamic and it points
    to the referencing all of the `input.csv` files. The result of referencing all
    of the input files is shown in `input.csv` files will keep increasing as a new
    `input.csv` file is created in the datastore in Blob storage each day. Congratulations
    on registering the inference data. Next, we will configure the data drift monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the data drift monitor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To monitor data drift and feature drift, we will use built-in drift monitoring
    features from the Azure ML workspace as part of the `Data Drift Monitor` feature
    on our Azure ML workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to your workspace and access the **Datasets** section. Then, select **Dataset
    Monitors** (it is in preview mode at the moment, as this feature is still being
    tested). Click on **Create**, as shown in *Figure 12.13*:![Figure 12.13 – Creating
    the data drift monitor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0131.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.13 – Creating the data drift monitor
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon selecting the **Create** button, you will be prompted to create a new data
    drift monitor. Select the target dataset of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the *Registering the target dataset* section, we registered the `inputs.csv`
    files as `Input-InferenceData`. Select your inference dataset as the target dataset,
    as shown in *Figure 12.14*:![Figure 12.14 – Select target dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.14 – Select target dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting your target dataset, you will be prompted to point to your baseline
    dataset, which should be your training dataset (it was used to train your deployed
    ML model). Select your baseline dataset, as shown in *Figure 12.15*:![Figure 12.15
    – Select base dataset and configuring the monitor settings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0151.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.15 – Select the baseline dataset and configure the monitor settings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the baseline dataset, you will be prompted to set up monitor
    settings, such as the name of data drift monitor (for example, `weather-Data-Drift`),
    the compute target to run data drift jobs, the frequency of data drift jobs (for
    example, once a day), and the threshold for monitoring drift (for example, 60).
    You will also be asked to give an email of your choice to receive notifications
    when the data drift surpasses a set threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After configuring the settings, create a data drift monitor. Go to your newly
    created data drift (in the **Datasets** section, click on **Dataset Monitors**
    to view your drift monitors), as shown in *Figure 12.16*:![Figure 12.16 – Data
    drift overview (it is currently empty)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.16 – Data drift overview (it is currently empty)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you access your data drift monitor, you will see that there is no data.
    This is because we haven't computed any drift yet. In order to compute drift,
    we need a compute resource.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the **Compute** section, access the **Compute clusters** tab, and create
    a new compute resource (for example, **drift-compute – Standard_DS_V2 machine**),
    as shown in *Figure 12.17*:![Figure 12.17 – Creating a compute cluster to compute
    data drift
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0171.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.17 – Creating a compute cluster to compute data drift
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After creating the compute cluster, go back to your data drift monitor (for
    example, **Weather-Data-Drift**). Next, we will compute the data drift.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Analyze existing data** and submit a run to analyze any existing
    inference data, as shown in *Figure 12.18*:![Figure 12.18 – Submitting run to
    analyze any data drift
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image018.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.18 – Submitting run to analyze any data drift
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the start and end dates and the compute target (which was created previously,
    that is, **drift-compute**). Then, click on **Submit** to run drift computation.
    It will usually take around 10 minutes to analyze and compute data drift. You
    can track the progress of your runs in the **Experiments** section of your Azure
    ML workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data drift analysis**: After successfully finishing the run, data drift has
    been computed. Using the drift overview, as shown in *Figure 12.19*, we can monitor
    and analyze your ML model performance in production. We can view the data drift
    magnitude and drift distribution by features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Data Drift magnitude trend'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.19 – Data Drift magnitude trend
  prefs: []
  type: TYPE_NORMAL
- en: The way that model drift is measured by the Azure ML service is that it uses
    a separate drift model (maintained by Azure), which looks at the baseline and
    compares inference data. This comparison results in a simple statistical percentage
    or degree of change in data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 12.19*, the **Drift magnitude trend** suggests that we have had inferences
    made to the model on 3 days (that is, **03/23/21**, **04/03/21**, and **04/04/21**).
  prefs: []
  type: TYPE_NORMAL
- en: The analysis shows that the data drift on these three occasions is below the
    threshold of 70% (this is the red line, which indicates the threshold). The data
    drift on **03/23/21** is around 50%; on **04/03/21**, it is around 44%; and on
    **04/04/21**, it is 40%. This gives us an idea of the changing trend in the incoming
    inference data to the model. Likewise, we can monitor feature drift.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature drift analysis**: You can assess individual features and their drift
    by scrolling down to the **Feature details** section and selecting a feature of
    your choice. For example, we can see the **Temperature_C** distribution over time
    feature, as shown in *Figure 12.20*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.20 – Feature drift trend (Temperature_C)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.20 – Feature drift trend (Temperature_C)
  prefs: []
  type: TYPE_NORMAL
- en: 'To monitor the feature change over time, we can select some metrics of our
    choice for the feature. Metrics such as **Mean value**, **Min value**, **Max value**,
    **Euclidean distance**, or **Wasserstein distance** are available to analyze feature
    drift. Select a metric of your choice (for example, **Mean value**). We have selected
    the **Mean value** metric to assess the temperature drift, as shown in *Figure
    12.20*. The **Mean value** metric has changed from 14 to 8 as time has progressed;
    this shows the change of drift in the **Temperature_C** feature. Such a change
    is expected as seasonal changes give rise to changes in temperature. We can also
    monitor feature distribution change, as shown in *Figure 12.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.21 – The Feature distribution trend'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.21 – The Feature distribution trend
  prefs: []
  type: TYPE_NORMAL
- en: If the drift is drastic or anomalous, we need to check for the quality of input
    data being inferred into the system. Insights into feature drift enable us to
    understand the changing data and world around us. Likewise, we can monitor model
    drift to understand the model performance in accordance with the changing data
    and world.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Monitoring model drift enables us to keep a check on our model performance
    in production. Model drift is where the properties of dependent variables change.
    For example, in our case, this is the classification results of the weather (that
    is, rain or no rain). Just as we set up data drift in the *Creating the data drift
    monitor* section, we can also set up a model drift monitor to monitor model outputs.
    Here are the high-level steps to set up model drift:'
  prefs: []
  type: TYPE_NORMAL
- en: Register a new dataset (for example, `Outputs.csv` files. The **Outputs** dataset
    can be created from the **Datasets** section. When creating an outputs inference
    dataset, select the important columns (for example, **Future_Weather_Condition**)
    and change the dataset into tabular and time-series format (drift can only be
    computed in time-series data) by selecting a column with **Timestamp**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new monitor (for example, the model drift monitor) from the **Dataset**
    section, and click on **Dataset Monitor**. Select the feature to monitor (for
    example, **future weather condition**) and set a threshold that you want to monitor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Analyze the model drift in the overview (as shown in *Figure 12.22*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 12.22 – Submitting run to analyze the data drift'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.22 – Submitting a run to analyze the data drift
  prefs: []
  type: TYPE_NORMAL
- en: If your model drift has surpassed a set threshold, then that may be an indication
    that you should retrain or train your model comparison results in a simple statistical
    percentage or a degree of change in data. When the data drift has gone past the
    threshold (for example, 70%), we can notify the administrator or product owner
    via email or take actions such as deploying another model or retraining the existing
    model. Using smart actions, we can govern ML systems to produce maximum value.
    We will explore ways to govern ML systems in the next chapter ([*Chapter 13*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234),
    *Governing the ML System for Continual Learning*). So far, we have implemented
    the setting up data drift, feature drift, and model drift. Next, let's monitor
    the ML system's application performance.
  prefs: []
  type: TYPE_NORMAL
- en: Application performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have deployed the ML service in the form of REST API endpoints, which can
    be consumed by users. We can monitor these endpoints using Azure Application Insights
    (enabled by Azure Monitor). To monitor our application performance, access the
    Application Insights dashboard, as shown in *Figure 12.23*. Go to the **Endpoints**
    section in your Azure ML service workspace and select the REST API endpoint your
    ML model is deployed on. Click on **Application Insights url** to access the Application
    Insights endpoint connected to your REST API endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.23 – Application Insights Overview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.23 – Application Insights Overview
  prefs: []
  type: TYPE_NORMAL
- en: 'From the **Application Insights Overview** section, we can monitor and analyze
    critical application performance information for your ML service. Additionally,
    we can monitor information such as failed requests, server response time, server
    requests, and availability from the **Overview** section, as shown in *Figure
    12.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – Application Insights Overview'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.24 – Application Insights Overview
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these metrics and this information, we can monitor the application
    performance. Ideally, we should not have any failed requests or long server response
    times. To get deeper insights into the application performance, we can access
    the application dashboard (by clicking on the button at the top of the screen),
    as shown in *Figure 12.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.25 – Application dashboard with a more detailed performance assessment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.25 – Application dashboard with a more detailed performance assessment
  prefs: []
  type: TYPE_NORMAL
- en: From the application dashboard, we can monitor the application performance in
    more detail. For instance, we can monitor application usage, reliability, and
    other information. In terms of usage, **Unique sessions and users** is critical
    information to monitor the number of unique users the application is able to serve.
    Additionally, the **Average availability** information is useful to assess the
    availability of the service for our users. With this information, we can make
    scaling decisions if more resources are needed to serve users.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can monitor application reliability by assessing information such as the
    number of failed requests, server exceptions, and dependency failures. We can
    monitor responsiveness using information such as the average server response time
    and CPU utilization. Ideally, the application should not have any failures, and
    if there are any failures, we can take a deeper look into the application logs,
    as shown in *Figure 12.26*, by accessing **Transaction search** or logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.26 – Accessing the logs to understand any errors or failures'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.26 – Accessing the logs to understand any errors or failures
  prefs: []
  type: TYPE_NORMAL
- en: We can take a closer look into the logs of applications to understand any failures
    or errors in order to debug the application and maintain the healthy functioning
    of the application. A functional ML application results in satisfied users and
    maximum business impact. Therefore, monitoring applications can be rewarding to
    reveal potential failures and maintain the application in order to serve users
    in the most efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing your ML system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring and analyzing your ML system in production in real time is key to
    understanding the performance of your ML system and ensuring its robustness to
    produce maximized business value. Humans play a key role in analyzing model performance
    and detecting subtle anomalies and threats. We can analyze model performance to
    detect any biases or threats and to understand why the model makes decisions in
    a certain pattern. We can do this by applying advanced techniques, such as data
    slicing, adversarial attack prevention techniques, or by understanding local and
    global explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Data slicing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our use case, we will leave out data slicing as we do not have much variety
    in terms of demographics or samples within the data (for example, sex, age groups,
    and more). To measure the fairness of the model, we will focus on bias detection.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and threat detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To determine the model bias in production, we can use a bias-variance trade-off
    method. This makes it simple to monitor and analyze the model bias or any possible
    threat. It goes without saying that there might be better methods to monitor bias,
    but the idea here is to keep it simple, as, sometimes, simplicity is better and
    more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: The disparity between our model's average prediction and the right value we
    are attempting to predict is the bias. Variance is the variability of the estimation
    of the model for a given data point or a value that informs us of our data spread.
    Analyzing bias and variance for inference data for the deployed model reveals
    the bias to be 20.1 and variance to be 1.23 (you can read more on analyzing bias
    and variance at [https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/)).
    This means our model has high bias and low variance; therefore, it might be a
    good idea to train or retrain our model with inference data to balance the bias-variance.
  prefs: []
  type: TYPE_NORMAL
- en: Local and global explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local and global explanations offer different perspectives on model performance.
    Local explanation offers a justification for model prediction for a specific or
    individual input, whereas global explanation provides insights into the model's
    predictive process, independent of any particular input. We previously looked
    at global explanations while exploring monitoring drifts in *Figure 12.19*. We
    can further investigate feature distribution, as shown in *Figure 12.21*, to understand
    local explanations in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing your ML system for fairness, bias, and local and global explanations
    gives us key insights into model performance, and we can use this information
    to govern our ML system.
  prefs: []
  type: TYPE_NORMAL
- en: Governing your ML system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A great part of system governance involves quality assurance and control, model
    auditing, and reporting to have end-to-end trackability and compliance with regulations.
    The ML systems'' efficacy (that is, its ability to produce a desired or intended
    result) is dependent on the way it is governed to achieve maximum business value.
    So far, we have monitored and analyzed our deployed model for inference data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.27 – Components of governing your ML system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.27 – Components of governing your ML system
  prefs: []
  type: TYPE_NORMAL
- en: The efficacy of an ML system can be determined by using smart actions that are
    taken based on monitoring and alerting. In the next chapter, we will explore ML
    system governance in terms of alerts and actions, model QA and control, and model
    auditing and reports.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the key principles of serving ML models to
    our users and monitoring them to achieve maximized business value. We explored
    the different means of serving ML models for users or consumers of the model and
    implemented the Explainable Monitoring framework for a hypothetical business use
    case and deployed a model. We carried out this hands-on implementation of an Explainable
    Monitoring framework to measure the performance of ML systems. Finally, we discussed
    the need for governing ML systems to ensure the robust performance of ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: We will further explore the governance of ML systems and continual learning
    concepts in the next and final chapter!
  prefs: []
  type: TYPE_NORMAL
