- en: 'Chapter 12: Model Serving and Monitoring'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章：模型服务和监控
- en: In this chapter, we will reflect on the need to serve and monitor **machine
    learning** (**ML**) models in production and explore different means of serving
    ML models for users or consumers of the model. Then, we will revisit the **Explainable
    Monitoring framework** from [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Key Principles for Monitoring Your ML System*, and implement it for the business
    use case we have been solving using MLOps to predict the weather. The implementation
    of an Explainable Monitoring framework is hands-on. We will infer the deployed
    API and monitor and analyze the inference data using **drifts** (such as data
    drift, feature drift, and model drift) to measure the performance of an ML system.
    Finally, we will look at several concepts to govern ML systems for the robust
    performance of ML systems to drive continuous learning and delivery.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将反思在生产环境中服务和监控机器学习（ML）模型的需求，并探讨为模型用户或消费者提供不同服务ML模型的方法。然后，我们将重新审视第11章中讨论的**可解释监控框架**，即*监控您的ML系统的主要原则*，并将其应用于我们使用MLOps预测天气的业务用例中。可解释监控框架的实施是实践性的。我们将推断部署的API，并使用**漂移**（如数据漂移、特征漂移和模型漂移）来监控和分析推理数据，以衡量ML系统的性能。最后，我们将探讨一些概念，以管理ML系统，确保ML系统的稳健性能，以驱动持续学习和交付。
- en: 'Let''s start by reflecting on the need to monitor ML in production. Then, we
    will move on to explore the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从反思在生产环境中监控ML的需求开始。然后，我们将继续在本章中探讨以下主题：
- en: Serving, monitoring, and maintaining models in production
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中服务、监控和维护模型
- en: Exploring different modes of serving ML models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索服务机器学习模型的不同模式
- en: Implementing the Explainable Monitoring framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施解释监控框架
- en: Governing your ML system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理您的ML系统
- en: Serving, monitoring, and maintaining models in production
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中服务、监控和维护模型
- en: 'There is no point in deploying a model or an ML system and not monitoring it.
    Monitoring performance is one of the most important aspects of an ML system. Monitoring
    enables us to analyze and map out the business impact an ML system offers to stakeholders
    in a qualitative and quantitative manner. In order to achieve maximum business
    impact, users of ML systems need to be served in the most convenient manner. After
    that, they can consume the ML system and generate value. In previous chapters,
    we developed and deployed an ML model to predict the weather conditions at a port
    as part of the business use case that we had been solving for practical implementation.
    In this chapter, we will revisit the Explainable Monitoring framework that we
    discussed in [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206), *Key
    Principles for Monitoring Your ML System*, and implement it within our business
    use case. In *Figure 12.1*, we can see the **Explainable Monitoring** framework
    and some of its components, as highlighted in green:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个模型或ML系统而不监控它是没有意义的。监控性能是ML系统最重要的方面之一。监控使我们能够以定性和定量的方式分析和绘制ML系统对利益相关者的业务影响。为了实现最大的业务影响，ML系统的用户需要以最方便的方式提供服务。之后，他们可以消费ML系统并产生价值。在前几章中，我们开发和部署了一个ML模型来预测港口的天气条件，这是我们一直在解决的实际实施业务用例的一部分。在本章中，我们将重新审视我们在第11章中讨论的**可解释监控框架**，即*监控您的ML系统的主要原则*，并将其应用于我们的业务用例中。在*图12.1*中，我们可以看到**可解释监控**框架及其一些组件，如绿色高亮所示：
- en: '![Figure 12.1 – Components of the Explainable Monitoring framework to be implemented'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – 要实施的解释监控框架的组件'
- en: '](img/image0011.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0011.jpg)'
- en: Figure 12.1 – Components of the Explainable Monitoring framework to be implemented
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 要实施的解释监控框架的组件
- en: 'We will implement Explainable Monitoring for these areas: *Data Integrity*,
    *Model Drift*, *Application Performance*, *Bias and Threat Detection*, *Local
    and Global Explanations*, *Alerts and Actions*, *Model QA and Control*, and *Model
    Auditing and Reports*. These components are the most significant, in our use case,
    to understand the implementation of Explainable Monitoring. We will leave out
    *Data Slicing* because we do not have much variety in terms of the demographics
    or samples within the data (for example, sex, age groups, and more). By using
    information from other components, we can assess the model''s performance and
    its fairness. In this chapter, we will implement components of the **Monitor**
    and **Analyze** modules: *Data Integrity*, *Model Drift*, *Application Performance*,
    *Bias and Threat Detection*, and *Local and Global Explanations*. The remaining
    component implementations will be covered in [*Chapter 13*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234),
    *Governing the ML System for Continual Learning*. Before we move on to the implementation
    process, let''s take a look at how models can be served for users to consume.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为以下领域实现可解释监控：*数据完整性*、*模型漂移*、*应用性能*、*偏差和威胁检测*、*局部和全局解释*、*警报和操作*、*模型QA和控制*以及*模型审计和报告*。这些组件在我们使用案例中是最重要的，以理解可解释监控的实施。我们将省略*数据切片*，因为我们数据中的人口统计或样本（例如，性别、年龄组等）没有太多多样性。通过使用其他组件的信息，我们可以评估模型的表现及其公平性。在本章中，我们将实现**监控**和**分析**模块的组件：*数据完整性*、*模型漂移*、*应用性能*、*偏差和威胁检测*以及*局部和全局解释*。其余组件的实现将在[*第13章*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234)中介绍，*持续学习的ML系统治理*。在我们继续到实现过程之前，让我们看看模型如何为用户消费提供服务。
- en: Exploring different modes of serving ML models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索ML模型的不同服务模式
- en: 'In this section, we will consider how a model can be served for users (both
    humans and machines) to consume the ML service efficiently. Model serving is a
    critical area, which an ML system needs to succeed at to fulfill its business
    impact, as any lag or bug in this area can be costly in terms of serving users.
    Robustness, availability, and convenience are key factors to keep in mind while
    serving ML models. Let''s take a look at some ways in which ML models can be served:
    this can be via batch service or on-demand mode (for instance, when a query is
    made on demand in order to get a prediction). A model can be served to either
    a machine or a human user in on-demand mode. Here is an example of serving a model
    to a user:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑如何为用户（无论是人类还是机器）提供模型以高效地消费ML服务。模型服务是一个关键领域，ML系统需要成功实现以发挥其商业影响，因为在这个领域的任何延迟或错误都可能对服务用户造成高昂的成本。鲁棒性、可用性和便利性是在提供服务模型时需要考虑的关键因素。让我们看看ML模型可以以哪些方式提供服务：这可以是批量服务或按需模式（例如，当需要查询以获取预测时）。在按需模式下，模型可以服务于机器或人类用户。以下是将模型提供给用户的示例：
- en: '![Figure 12.2 – Serving a model to users'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.2 – 向用户提供服务模型'
- en: '](img/image002.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image002.jpg)'
- en: Figure 12.2 – Serving a model to users
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 向用户提供服务模型
- en: In a typical scenario (in on-demand mode), a model is served as a service for
    users to consume, as shown in *Figure 12.2*. Then, an external application on
    a machine or a human makes a query to the prediction or ML service using their
    data. The ML service, upon receiving a request, uses a load balancer to route
    the request to an available resource (such as a container or an application) within
    the ML application. The load balancer also manages resources within the ML service
    to orchestrate and generate new containers or resources on demand. The load balance
    redirects the query from the user to the model running in a container within the
    ML application to get the prediction. On getting the prediction, the load balance
    reverts back to the external application on a machine, or to a human who is making
    the request, or to the query within the model prediction. In this way, the ML
    service is able to serve its users. The ML system orchestrates with the model
    store or registry to keep itself updated with either the latest or best-performing
    models in order to serve the users in the best manner. In comparison to this typical
    scenario where users make a query, there is another use case where the model is
    served as a batch service.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型场景（按需模式）中，模型作为服务提供给用户消费，如图*图12.2*所示。然后，机器上的外部应用程序或人类使用他们的数据向预测或机器学习服务发出查询。机器学习服务在收到请求后，使用负载均衡器将请求路由到机器学习应用程序内的可用资源（如容器或应用程序）。负载均衡器还管理机器学习服务内的资源，以按需编排和生成新的容器或资源。负载均衡器将查询从用户重定向到机器学习应用程序内运行的容器中的模型以获取预测。获取预测后，负载均衡器将返回到机器上的外部应用程序，或请求查询的人类，或模型预测内的查询。这样，机器学习服务能够为其用户提供服务。机器学习系统与模型存储或注册表协同工作，以保持其与最新或性能最佳的模型同步，以便以最佳方式为用户提供服务。与用户进行查询的典型场景相比，还有另一种用例，即模型作为批量服务提供。
- en: Serving the model as a batch service
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型作为批量服务提供
- en: 'Batch processing or serving is applied to large quantities or batches of input
    data (that is, not single observations but bunches of observations together).
    In cases where there is a large bunch of data to be inferred, a model is normally
    served in batch mode. One example of this is when the model is used to process
    the data of all consumers or users of a product or service in one go. Alternatively,
    a batch of data from a factory for a fixed timeline might need to be processed
    to detect anomalies in the machines. Compared to on-demand mode, batch mode is
    more resource-efficient and is usually employed when some latency can be afforded:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理或服务应用于大量或批次的输入数据（即不是单个观察值，而是成组的观察值）。在存在大量数据需要推断的情况下，模型通常以批量模式提供服务。一个例子是当模型一次性处理产品或服务的所有消费者或用户的数据。或者，工厂在固定时间线上的数据批次可能需要被处理以检测机器中的异常。与按需模式相比，批量模式更节省资源，通常在可以承受一定延迟的情况下使用：
- en: '![Figure 12.3 – Batch Inference'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3 – 批量推断'
- en: '](img/image0031.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0031.jpg)'
- en: Figure 12.3 – Batch inference
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 批量推断
- en: One of the key advantages of batch processing is that unlike a REST API-based
    service, a batch service might require lighter or less infrastructure. Writing
    a batch job is easier for a data scientist compared to deploying an online REST
    service. This is because the data scientist just needs to train a model or deserialize
    a trained model on a machine and perform batch inference on a batch of data. The
    results of batch inference can be stored in a database as opposed to sending responses
    to users or consumers. However, one major disadvantage is the high latency and
    it not being in real time. Typically, a batch service can process hundreds or
    thousands of features at once. A series of tests can be used to determine the
    optimal batch size to arrive at an acceptable latency for the use case. Typical
    batch sizes can be 32, 64, 128, or 518 to the power of 2\. Batch inference can
    be scheduled periodically and can serve many use cases where latency is not an
    issue. One such example is discussed next.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理的一个关键优势是，与基于REST API的服务不同，批量服务可能需要更轻量或更少的基础设施。对于数据科学家来说，编写批量作业比部署在线REST服务更容易。这是因为数据科学家只需要在机器上训练一个模型或反序列化一个训练好的模型，并对一批数据进行批量推理。批量推理的结果可以存储在数据库中，而不是发送响应给用户或消费者。然而，一个主要的缺点是高延迟且不是实时。通常，批量服务可以一次处理数百或数千个特征。可以使用一系列测试来确定最佳批量大小，以达到可接受的延迟。典型的批量大小可以是32、64、128或2的518次方。批量推理可以定期安排，并可以服务于许多延迟不是问题的用例。以下将讨论一个这样的例子。
- en: A real-world example
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子
- en: One real-world example is a bank extracting information from batches of text
    documents. A bank receives thousands of documents a day from its partner institutions.
    It is not possible for a human agent to read through all of them and highlight
    any red flags in the operations listed in the documents. Batch inferencing is
    used to extract name entities and red flags from all the documents received by
    the bank in one go. The results of the batch inference or serving are then stored
    in a database.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子是银行从一批文本文档中提取信息。银行每天从其合作伙伴机构接收数千份文件。人类代理不可能阅读所有这些文件并突出显示文档中列出的操作中的任何红旗。批量推理用于一次性从银行接收的所有文档中提取命名实体和红旗。然后，批量推理或服务的成果存储在数据库中。
- en: Serving the model to a human user
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型提供给人类用户
- en: Before processing a request from human users, it is essential to check whether
    the user has adequate permissions to use the model. Additionally, in most cases,
    it is helpful to know the context in which the request was made. Gathering the
    context of the request will enable the model to produce better predictions. After
    gathering the context, we can transform it into model-readable input and infer
    the model to get a prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理来自人类用户的请求之前，检查用户是否有足够的权限使用模型至关重要。此外，在大多数情况下，了解请求的上下文也很有帮助。收集请求的上下文将使模型能够产生更好的预测。收集上下文后，我们可以将其转换为模型可读的输入，并推断模型以获得预测。
- en: 'In practice, here are the key steps in serving an on-demand model to human
    users:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，以下是向人类用户提供按需模型的关键步骤：
- en: Validate or authorize the request.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证或授权请求。
- en: Analyze and gather contextual information (for example, historic data, user
    experience data, or any other personal data of the user).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析和收集上下文信息（例如，历史数据、用户体验数据或任何其他用户个人数据）。
- en: Transform any contextual information into a model-readable input or schema.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将任何上下文信息转换为模型可读的输入或模式。
- en: Infer the model with input data (with the request and contextual information)
    to make a prediction or get an output.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用输入数据（包括请求和上下文信息）推断模型以做出预测或获取输出。
- en: Interpret the output as per the context.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据上下文解释输出结果。
- en: Relay the output to the user.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出传递给用户。
- en: A real-world example
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个现实世界的例子
- en: Consider a chatbot serving human customers to book flight tickets. It performs
    contextual inference to serve human users.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑一个聊天机器人服务于人类客户预订机票。它执行上下文推理以服务于人类用户。
- en: Serving the model to a machine
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型提供给机器
- en: 'We can serve a machine or an external application using a `REST` API or a streaming
    service based on the use case. Typically, machine inference data requirements
    are either predetermined or within a standard schema. A well-defined topology
    and data schema in the form of a REST API or streaming service will work. Serving
    on demand to a machine or human varies from case to case, as, in some scenarios,
    demand may vary (for example, at a particular time of day when the demand for
    serving the user might be high, such as in the afternoon). To handle a high demand
    from the service, autoscaling (on the cloud) can help spawn more resources on
    demand and kill any idle resources to free up more resources. However, autoscaling
    is not a one-stop solution for scaling, as it cannot handle sudden or peculiar
    spikes in demand on its own:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Message Broker for on-demand serving'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Message broker for on-demand serving
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach shown in *Figure 12.4* is resource-efficient to handle high volume
    demand spikes. To handle sudden spikes, message brokers such as Apache Kafka or
    Spark can be useful. A message broker runs processes to write and read to a queue:
    one process to write messages in a queue and another process to read from that
    queue. The served model is periodically connected to the message broker to process
    batches of input data from the queue to make predictions for each element in the
    batch. After processing the input data batches and generating predictions, the
    predictions are written to the output queue, which is then pushed to the users
    as per their requests.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: A real-world example
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Consider a social media company that has millions of users. The company uses
    a single or common ML model for the recommender system to recommend newsfeed articles
    or posts to users. As the volume of requests is high in order to serve many users,
    it cannot depend on a REST API-based ML system (as it is synchronous). A streaming
    solution is better as it provides asynchronous inference for the company to serve
    its users. When a user logs into their application or account hosted on a machine
    (such as a social media company server), the application running on their machine
    infers the ML model (that is, the recommender system) via a streaming service
    for recommendations for the user newsfeed. Likewise, thousands of other users
    log in at the same time. The streaming service can serve all of these users seamlessly.
    Note that this wouldn't have been possible with the REST API service. By using
    a streaming service for the recommender system model, the social media company
    is able to serve its high volume of users in real time, avoiding significant lags.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Explainable Monitoring framework
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement the Explainable Monitoring framework, it is worth doing a recap
    of what has been discussed so far, in terms of implementing hypothetical use cases.
    Here is a recap of what we did for our use case implementation, including the
    problem and solution:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem context**: You work as a data scientist in a small team with three
    other data scientists for a cargo shipping company based in the port of Turku
    in Finland. 90% of the goods imported into Finland arrive via cargo shipping at
    various ports across the country. For cargo shipping, weather conditions and logistics
    can be challenging at times. Rainy conditions can distort operations and logistics
    at the ports, which can affect supply chain operations. Forecasting rainy conditions
    in advance allows us to optimize resources such as human resources, logistics,
    and transport resources for efficient supply chain operations at ports. Business-wise,
    forecasting rainy conditions in advance enables ports to reduce their operational
    costs by up to approximately 20% by enabling the efficient planning and scheduling
    of human resources, logistics, and transport resources for supply chain operations.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task or solution**: You, as a data scientist, are tasked to develop an ML-driven
    solution to forecast weather conditions 4 hours in advance at the port of Turku
    in Finland. This will enable the port to optimize its resources, thereby enabling
    cost savings of up to 20%. To get started, you are provided with a historic weather
    dataset with a 10-year-timeline from the port of Turku (the dataset can be accessed
    in the Git repository of this book). Your task is to build a continuous learning-driven
    ML solution to optimize operations at the port of Turku.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we have developed ML models and deployed them as REST API endpoints
    inside a Kubernetes cluster at [http://20.82.202.164:80/api/v1/service/weather-prod-service/score](http://20.82.202.164:80/api/v1/service/weather-prod-service/score)
    (the address of your endpoint will be different).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will replicate a real-life inference scenario for this endpoint. To
    do this, we will use the test dataset we had split and registered in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines*,
    in the *Data ingestion and feature engineering* section. Go to your Azure ML workspace
    and download the `test_data.csv` dataset (which was registered as `test_dataset`)
    from the **Datasets** section or the Blob storage that is connected to your workspace,
    as shown in *Figure 12.5*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Downloading the validation dataset (which was previously split
    and registered)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0051.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Downloading the validation dataset (which was previously split
    and registered)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Get ready to infer the `test_data.csv` data with the REST API endpoint or ML
    service. Go to the `12_Model_Serving_Monitoring` folder and place the downloaded
    dataset (`test_data.csv`) inside the folder. Next, access the `inference.` `py`
    file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding code, we perform the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行以下步骤：
- en: In the `inference.py` file, begin by importing the necessary libraries, such
    as `json`, `requests`, and `pandas`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `inference.py` 文件中，首先导入必要的库，例如 `json`、`requests` 和 `pandas`。
- en: Next, import the dataset (`test_data.csv`) to use to infer with the endpoint.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入用于与端点推断的 `test_data.csv` 数据集。
- en: Drop the unnecessary columns for inference, such as `Timestamp`, `Location`,
    and `Future_weather_condition` (we will predict this final column by querying
    the endpoint).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除推理中不必要的列，如 `Timestamp`、`Location` 和 `Future_weather_condition`（我们将通过查询端点来预测这个最终列）。
- en: Next, point to the URL of the endpoint (you can find this by navigating to **Azure
    ML Workspace** | **Endpoints** | **Weather-prod-service** | **Consume**). For
    simplicity, since we did not have authentication or keys set up for the service,
    we have the header application/JSON with no keys or authentication.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，指向端点的 URL（您可以通过导航到 **Azure ML 工作区** | **端点** | **Weather-prod-service**
    | **消费**）来。为了简单起见，因为我们没有为该服务设置身份验证或密钥，所以我们有 application/JSON 的标题，没有密钥或身份验证。
- en: 'Finally, we will loop through the data array by inferring each element in the
    array with the endpoint. To run the script, simply replace `''url''` with your
    endpoint and run the following command in the Terminal (from the folder location)
    to execute the script:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将通过端点推断数组中的每个元素来遍历数据数组。要运行脚本，只需将 `'url'` 替换为您的端点，然后在终端（从文件夹位置）运行以下命令以执行脚本：
- en: '[PRE1]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The running script will take around 10–15 minutes to infer all of the elements
    of the inference data. After this, we can monitor the inference and analyze the
    results of the inferring data. Let's monitor and analyze this starting with data
    integrity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本将花费大约 10-15 分钟来推断推理数据中的所有元素。之后，我们可以监控推理并分析推断数据的结果。让我们从数据完整性开始监控和分析。
- en: Monitoring your ML system
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控您的机器学习系统
- en: 'The **Monitor** module is dedicated to monitoring the application in production
    (that is, serving the ML model). The action monitor module has the following three
    functionalities:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**监控**模块专门用于监控生产中的应用（即，提供机器学习模型）。动作监控模块具有以下三个功能：'
- en: 'Data integrity:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据完整性：
- en: -To register the target dataset
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 注册目标数据集'
- en: -To create a data drift monitor
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 创建数据漂移监控器'
- en: -To perform data drift analysis
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 执行数据漂移分析'
- en: -To perform feature drift analysis
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 执行特征漂移分析'
- en: Model drift
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型漂移
- en: Application performance
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用性能
- en: Let's take a look at each of these functionalities in more detail next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看这些功能。
- en: Data integrity
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据完整性
- en: 'To monitor data integrity for inference data, we need to monitor data drift
    and feature drift to see whether there are any anomalous changes in the incoming
    data or any new patterns:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控推理数据的数据完整性，我们需要监控数据漂移和特征漂移，以查看是否有任何异常变化或任何新的模式：
- en: '**Data drift**: This is when the properties of the independent variables change.
    For example, data changes can occur due to seasonality or the addition of new
    products or changes in consumer desires or habits, as it did during the COVID-19
    pandemic.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移**：这是指独立变量的属性发生变化的情况。例如，由于季节性或新产品的添加、消费者需求或习惯的变化，数据可能会发生变化，就像在 COVID-19
    大流行期间所发生的那样。'
- en: '**Feature drift**: This is when properties of the feature(s) change over time.
    For example, the temperature is changing due to changing seasons or seasonality,
    that is, in summer, the temperature is warmer compared to temperatures during
    winter or autumn.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征漂移**：这是指特征（的）属性随时间变化的情况。例如，由于季节变化或季节性，温度在夏季比冬季或秋季的温度要暖和。'
- en: 'To monitor drifts, we will measure the difference for the baseline dataset
    versus the target dataset. The first step is to define the baseline dataset and
    the target dataset. This depends on use case to use case; we will use the following
    datasets as the baseline and target datasets:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控漂移，我们将测量基线数据集与目标数据集之间的差异。第一步是定义基线数据集和目标数据集。这取决于用例；我们将使用以下数据集作为基线数据集和目标数据集：
- en: '**Baseline dataset**: This is the training dataset.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基线数据集**：这是训练数据集。'
- en: '**Target dataset**: This is the inference dataset.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标数据集**：这是推理数据集。'
- en: We will use the training dataset that we previously used to train our models
    as the baseline dataset. This is because the model used in inference knows the
    patterns in the training dataset very well. The training dataset is ideal for
    comparing how inference data changes over time. We will compile all the inference
    data collected during inference into the inference dataset and compare these two
    datasets (that is, the baseline dataset and the target dataset) to gauge data
    and feature drifts for the target dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前用于训练模型的训练数据集作为基线数据集。这是因为用于推理的模型非常了解训练数据集中的模式。训练数据集非常适合比较推理数据随时间的变化。我们将收集推理过程中收集的所有推理数据并将其编译到推理数据集中，并将这两个数据集（即基线数据集和目标数据集）进行比较，以评估目标数据集的数据和特征漂移。
- en: Registering the target dataset
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册目标数据集
- en: The training dataset was registered in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*, in the *Data ingestion and feature engineering*
    section. We need to register the inference dataset within the **Datasets** section
    of the Azure ML workspace.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集已在[*第 4 章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074)的*机器学习管道*部分注册，在*数据摄取和特征工程*部分。我们需要在
    Azure ML 工作区的**数据集**部分内注册推理数据集。
- en: 'Inference data is collected as a result of using the `azureml.monitoring` SDK
    (the `modelDataCollector` function). By enabling monitoring functions using the
    `modelDataCollector` function in your scoring file (in `score.py`, as we did in
    [*Chapter 6*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124), *Key Principles
    of Deploying Your ML System*), we store inference data in the form of a time-series
    dataset in the Blob storage. In the Blob storage connected to your Azure ML workspace,
    inference data is stored in the `modeldata` container. In the `modeldata` container,
    the inference data (including both inputs and outputs) is stored the form of CSV
    files that are partitioned inside folders. These are structured as per year, per
    month, and per day (when the inference data was recorded in the production). Inside
    the partitioned folders, inference data is stored in CSV files that are named
    `inputs.csv` and `outputs.csv`. We need to register these `input.csv` files to
    monitor data drift and feature drift. Follow these steps to register the `input.csv`
    files:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 推理数据是使用 `azureml.monitoring` SDK（`modelDataCollector` 函数）收集的结果。通过在评分文件（在 `score.py`
    中，正如我们在[*第 6 章*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124)的*部署您的 ML 系统的关键原则*中做的那样）中使用
    `modelDataCollector` 函数启用监控功能，我们将推理数据以时间序列数据集的形式存储在 Blob 存储中。在连接到您的 Azure ML 工作区的
    Blob 存储中，推理数据存储在 `modeldata` 容器中。在 `modeldata` 容器中，推理数据（包括输入和输出）以 CSV 文件的形式存储，这些文件被分在文件夹内。这些文件按照年份、月份和日期（在生产中记录推理数据时）进行结构化。在分区文件夹内，推理数据存储在名为
    `inputs.csv` 和 `outputs.csv` 的 CSV 文件中。我们需要注册这些 `input.csv` 文件以监控数据漂移和特征漂移。按照以下步骤注册
    `input.csv` 文件：
- en: Go to the **Datasets** section and click on **Create dataset**. Then, select
    the **From datastore** option, as shown in *Figure 12.6*:![Figure 12.6 – Registering
    the inference dataset
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往**数据集**部分，点击**创建数据集**。然后，选择**从数据存储**选项，如图 12.6 所示：![图 12.6 – 注册推理数据集
- en: '](img/image006.jpg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 006](img/image006.jpg)'
- en: Figure 12.6 – Registering the inference dataset
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.6 – 注册推理数据集
- en: Name the dataset (for example, `Inputs-Inference-Dataset`), select the dataset
    type as **Tabular**, and write an appropriate description in the **Description**
    field name by describing the purpose of your dataset. Click on **Next** to specify
    the datastore selection. Select the **modeldata** datastore, as shown in *Figure
    12.7*:![Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据集命名（例如，`Inputs-Inference-Dataset`），选择数据集类型为**表格型**，并在**描述**字段中写入适当的描述，描述数据集的目的。点击**下一步**以指定数据存储选择。选择**modeldata**数据存储，如图
    12.7 所示：![图 12.7 – 数据存储选择（输入-推理数据注册）
- en: '](img/image0071.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 0071](img/image0071.jpg)'
- en: Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.7 – 数据存储选择（输入-推理数据注册）
- en: After selecting the `input.csv` file. You can find this in the folder of your
    `support vectorclassifier model`, which is inside the folder with your service
    name (for example, `prod-webservice`). Then, go into the subfolders (the default,
    inputs, and folders structured with dates), and go to the folder of your current
    date to find the `input.csv` file. Select the `input.csv` file, as shown in *Figure
    12.8*:![Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data
    registration)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image008.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data
    registration)
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the `input.csv` file, click on the `/**/inputs*.csv` (as shown
    in *Figure 12.9*). This is an important step that will refer to all of the `input.csv`
    files in the `inputs` folder dynamically. Without referencing all of the `input.csv`
    files, we will confine the path to only one `input.csv` file (which was selected
    previously in *Figure 12.8*). By referring to all of the `input.csv` files, we
    will compile all of the input data (the `inputs.csv` files) into the target dataset
    (for example, `Inputs-Inference-Data`):![Figure 12.9 – Referencing the path to
    dynamically access all the input.csv files
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0091.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.9 – Referencing the path to dynamically access all the input.csv files
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the **Next** button to advance to **Settings and preview**:![Figure
    12.10 – Settings and preview (the inference dataset registration)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image010.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.10 – Settings and preview (the inference dataset registration)
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As shown in *Figure 12.10*, we can configure the settings and preview the dataset.
    Point to the correct column names by selecting the **Column headers** dropdown
    and then selecting **Combine headers from all files**. Check for the correct column
    names (for example, **Temperature_C** and **Humidity**). After selecting the appropriate
    column names, click on the **Next** button to advance to the next window. Select
    the right schema by selecting all the columns you would like to monitor, along
    with their data types, as shown in *Figure 12.11*:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Schema selection (the inference dataset registration)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/image0111.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.11 – Schema selection (the inference dataset registration)
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Make sure that you select the **Timestamp** and **Date** properties in the **$aml_dc_scoring_timestamp**
    column as these contain the timestamps of the inference. This step is important.
    Only a time-series format dataset can be used to compute drift (by the Azure drift
    model); otherwise, we cannot compute drift. After selecting the right schema by
    selecting all of the columns, click on **Next** to confirm all of the necessary
    details (such as the name of the dataset, the dataset version, its path, and more).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on the **Create** button to create the dataset. When your dataset has
    been created successfully, you can view the dataset from the **Dataset** section
    in your Azure ML workspace. Go to the **Datasets** section to confirm your dataset
    has been created. Identify and click on your created dataset. Upon clicking, you
    will be able to view the details of your registered inference dataset, as shown
    in *Figure 12.12*:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Viewing the registered inference dataset'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image012.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Viewing the registered inference dataset
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: You can see all the essential attributes of your registered dataset in *Figure
    12.12*. It is important to note that the relative path is dynamic and it points
    to the referencing all of the `input.csv` files. The result of referencing all
    of the input files is shown in `input.csv` files will keep increasing as a new
    `input.csv` file is created in the datastore in Blob storage each day. Congratulations
    on registering the inference data. Next, we will configure the data drift monitor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Creating the data drift monitor
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To monitor data drift and feature drift, we will use built-in drift monitoring
    features from the Azure ML workspace as part of the `Data Drift Monitor` feature
    on our Azure ML workspace:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Go to your workspace and access the **Datasets** section. Then, select **Dataset
    Monitors** (it is in preview mode at the moment, as this feature is still being
    tested). Click on **Create**, as shown in *Figure 12.13*:![Figure 12.13 – Creating
    the data drift monitor
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0131.jpg)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.13 – Creating the data drift monitor
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon selecting the **Create** button, you will be prompted to create a new data
    drift monitor. Select the target dataset of your choice.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the *Registering the target dataset* section, we registered the `inputs.csv`
    files as `Input-InferenceData`. Select your inference dataset as the target dataset,
    as shown in *Figure 12.14*:![Figure 12.14 – Select target dataset
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image014.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.14 – Select target dataset
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting your target dataset, you will be prompted to point to your baseline
    dataset, which should be your training dataset (it was used to train your deployed
    ML model). Select your baseline dataset, as shown in *Figure 12.15*:![Figure 12.15
    – Select base dataset and configuring the monitor settings
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0151.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.15 – Select the baseline dataset and configure the monitor settings
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the baseline dataset, you will be prompted to set up monitor
    settings, such as the name of data drift monitor (for example, `weather-Data-Drift`),
    the compute target to run data drift jobs, the frequency of data drift jobs (for
    example, once a day), and the threshold for monitoring drift (for example, 60).
    You will also be asked to give an email of your choice to receive notifications
    when the data drift surpasses a set threshold.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After configuring the settings, create a data drift monitor. Go to your newly
    created data drift (in the **Datasets** section, click on **Dataset Monitors**
    to view your drift monitors), as shown in *Figure 12.16*:![Figure 12.16 – Data
    drift overview (it is currently empty)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image016.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.16 – Data drift overview (it is currently empty)
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you access your data drift monitor, you will see that there is no data.
    This is because we haven't computed any drift yet. In order to compute drift,
    we need a compute resource.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the **Compute** section, access the **Compute clusters** tab, and create
    a new compute resource (for example, **drift-compute – Standard_DS_V2 machine**),
    as shown in *Figure 12.17*:![Figure 12.17 – Creating a compute cluster to compute
    data drift
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image0171.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.17 – Creating a compute cluster to compute data drift
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After creating the compute cluster, go back to your data drift monitor (for
    example, **Weather-Data-Drift**). Next, we will compute the data drift.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Analyze existing data** and submit a run to analyze any existing
    inference data, as shown in *Figure 12.18*:![Figure 12.18 – Submitting run to
    analyze any data drift
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/image018.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 12.18 – Submitting run to analyze any data drift
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the start and end dates and the compute target (which was created previously,
    that is, **drift-compute**). Then, click on **Submit** to run drift computation.
    It will usually take around 10 minutes to analyze and compute data drift. You
    can track the progress of your runs in the **Experiments** section of your Azure
    ML workspace.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data drift analysis**: After successfully finishing the run, data drift has
    been computed. Using the drift overview, as shown in *Figure 12.19*, we can monitor
    and analyze your ML model performance in production. We can view the data drift
    magnitude and drift distribution by features:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Data Drift magnitude trend'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image019.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.19 – Data Drift magnitude trend
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The way that model drift is measured by the Azure ML service is that it uses
    a separate drift model (maintained by Azure), which looks at the baseline and
    compares inference data. This comparison results in a simple statistical percentage
    or degree of change in data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 12.19*, the **Drift magnitude trend** suggests that we have had inferences
    made to the model on 3 days (that is, **03/23/21**, **04/03/21**, and **04/04/21**).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The analysis shows that the data drift on these three occasions is below the
    threshold of 70% (this is the red line, which indicates the threshold). The data
    drift on **03/23/21** is around 50%; on **04/03/21**, it is around 44%; and on
    **04/04/21**, it is 40%. This gives us an idea of the changing trend in the incoming
    inference data to the model. Likewise, we can monitor feature drift.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature drift analysis**: You can assess individual features and their drift
    by scrolling down to the **Feature details** section and selecting a feature of
    your choice. For example, we can see the **Temperature_C** distribution over time
    feature, as shown in *Figure 12.20*:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.20 – Feature drift trend (Temperature_C)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image020.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.20 – Feature drift trend (Temperature_C)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'To monitor the feature change over time, we can select some metrics of our
    choice for the feature. Metrics such as **Mean value**, **Min value**, **Max value**,
    **Euclidean distance**, or **Wasserstein distance** are available to analyze feature
    drift. Select a metric of your choice (for example, **Mean value**). We have selected
    the **Mean value** metric to assess the temperature drift, as shown in *Figure
    12.20*. The **Mean value** metric has changed from 14 to 8 as time has progressed;
    this shows the change of drift in the **Temperature_C** feature. Such a change
    is expected as seasonal changes give rise to changes in temperature. We can also
    monitor feature distribution change, as shown in *Figure 12.21*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.21 – The Feature distribution trend'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image021.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.21 – The Feature distribution trend
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: If the drift is drastic or anomalous, we need to check for the quality of input
    data being inferred into the system. Insights into feature drift enable us to
    understand the changing data and world around us. Likewise, we can monitor model
    drift to understand the model performance in accordance with the changing data
    and world.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Monitoring model drift enables us to keep a check on our model performance
    in production. Model drift is where the properties of dependent variables change.
    For example, in our case, this is the classification results of the weather (that
    is, rain or no rain). Just as we set up data drift in the *Creating the data drift
    monitor* section, we can also set up a model drift monitor to monitor model outputs.
    Here are the high-level steps to set up model drift:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Register a new dataset (for example, `Outputs.csv` files. The **Outputs** dataset
    can be created from the **Datasets** section. When creating an outputs inference
    dataset, select the important columns (for example, **Future_Weather_Condition**)
    and change the dataset into tabular and time-series format (drift can only be
    computed in time-series data) by selecting a column with **Timestamp**.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new monitor (for example, the model drift monitor) from the **Dataset**
    section, and click on **Dataset Monitor**. Select the feature to monitor (for
    example, **future weather condition**) and set a threshold that you want to monitor.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Analyze the model drift in the overview (as shown in *Figure 12.22*):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![ Figure 12.22 – Submitting run to analyze the data drift'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image022.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.22 – Submitting a run to analyze the data drift
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: If your model drift has surpassed a set threshold, then that may be an indication
    that you should retrain or train your model comparison results in a simple statistical
    percentage or a degree of change in data. When the data drift has gone past the
    threshold (for example, 70%), we can notify the administrator or product owner
    via email or take actions such as deploying another model or retraining the existing
    model. Using smart actions, we can govern ML systems to produce maximum value.
    We will explore ways to govern ML systems in the next chapter ([*Chapter 13*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234),
    *Governing the ML System for Continual Learning*). So far, we have implemented
    the setting up data drift, feature drift, and model drift. Next, let's monitor
    the ML system's application performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Application performance
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have deployed the ML service in the form of REST API endpoints, which can
    be consumed by users. We can monitor these endpoints using Azure Application Insights
    (enabled by Azure Monitor). To monitor our application performance, access the
    Application Insights dashboard, as shown in *Figure 12.23*. Go to the **Endpoints**
    section in your Azure ML service workspace and select the REST API endpoint your
    ML model is deployed on. Click on **Application Insights url** to access the Application
    Insights endpoint connected to your REST API endpoint:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.23 – Application Insights Overview'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image023.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.23 – Application Insights Overview
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'From the **Application Insights Overview** section, we can monitor and analyze
    critical application performance information for your ML service. Additionally,
    we can monitor information such as failed requests, server response time, server
    requests, and availability from the **Overview** section, as shown in *Figure
    12.24*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – Application Insights Overview'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image024.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.24 – Application Insights Overview
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these metrics and this information, we can monitor the application
    performance. Ideally, we should not have any failed requests or long server response
    times. To get deeper insights into the application performance, we can access
    the application dashboard (by clicking on the button at the top of the screen),
    as shown in *Figure 12.25*:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.25 – Application dashboard with a more detailed performance assessment'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image025.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.25 – Application dashboard with a more detailed performance assessment
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: From the application dashboard, we can monitor the application performance in
    more detail. For instance, we can monitor application usage, reliability, and
    other information. In terms of usage, **Unique sessions and users** is critical
    information to monitor the number of unique users the application is able to serve.
    Additionally, the **Average availability** information is useful to assess the
    availability of the service for our users. With this information, we can make
    scaling decisions if more resources are needed to serve users.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'We can monitor application reliability by assessing information such as the
    number of failed requests, server exceptions, and dependency failures. We can
    monitor responsiveness using information such as the average server response time
    and CPU utilization. Ideally, the application should not have any failures, and
    if there are any failures, we can take a deeper look into the application logs,
    as shown in *Figure 12.26*, by accessing **Transaction search** or logs:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.26 – Accessing the logs to understand any errors or failures'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image026.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.26 – Accessing the logs to understand any errors or failures
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: We can take a closer look into the logs of applications to understand any failures
    or errors in order to debug the application and maintain the healthy functioning
    of the application. A functional ML application results in satisfied users and
    maximum business impact. Therefore, monitoring applications can be rewarding to
    reveal potential failures and maintain the application in order to serve users
    in the most efficient way.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing your ML system
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring and analyzing your ML system in production in real time is key to
    understanding the performance of your ML system and ensuring its robustness to
    produce maximized business value. Humans play a key role in analyzing model performance
    and detecting subtle anomalies and threats. We can analyze model performance to
    detect any biases or threats and to understand why the model makes decisions in
    a certain pattern. We can do this by applying advanced techniques, such as data
    slicing, adversarial attack prevention techniques, or by understanding local and
    global explanations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Data slicing
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our use case, we will leave out data slicing as we do not have much variety
    in terms of demographics or samples within the data (for example, sex, age groups,
    and more). To measure the fairness of the model, we will focus on bias detection.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Bias and threat detection
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To determine the model bias in production, we can use a bias-variance trade-off
    method. This makes it simple to monitor and analyze the model bias or any possible
    threat. It goes without saying that there might be better methods to monitor bias,
    but the idea here is to keep it simple, as, sometimes, simplicity is better and
    more efficient.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The disparity between our model's average prediction and the right value we
    are attempting to predict is the bias. Variance is the variability of the estimation
    of the model for a given data point or a value that informs us of our data spread.
    Analyzing bias and variance for inference data for the deployed model reveals
    the bias to be 20.1 and variance to be 1.23 (you can read more on analyzing bias
    and variance at [https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/)).
    This means our model has high bias and low variance; therefore, it might be a
    good idea to train or retrain our model with inference data to balance the bias-variance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Local and global explanations
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local and global explanations offer different perspectives on model performance.
    Local explanation offers a justification for model prediction for a specific or
    individual input, whereas global explanation provides insights into the model's
    predictive process, independent of any particular input. We previously looked
    at global explanations while exploring monitoring drifts in *Figure 12.19*. We
    can further investigate feature distribution, as shown in *Figure 12.21*, to understand
    local explanations in detail.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing your ML system for fairness, bias, and local and global explanations
    gives us key insights into model performance, and we can use this information
    to govern our ML system.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Governing your ML system
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A great part of system governance involves quality assurance and control, model
    auditing, and reporting to have end-to-end trackability and compliance with regulations.
    The ML systems'' efficacy (that is, its ability to produce a desired or intended
    result) is dependent on the way it is governed to achieve maximum business value.
    So far, we have monitored and analyzed our deployed model for inference data:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.27 – Components of governing your ML system'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image027.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.27 – Components of governing your ML system
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The efficacy of an ML system can be determined by using smart actions that are
    taken based on monitoring and alerting. In the next chapter, we will explore ML
    system governance in terms of alerts and actions, model QA and control, and model
    auditing and reports.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the key principles of serving ML models to
    our users and monitoring them to achieve maximized business value. We explored
    the different means of serving ML models for users or consumers of the model and
    implemented the Explainable Monitoring framework for a hypothetical business use
    case and deployed a model. We carried out this hands-on implementation of an Explainable
    Monitoring framework to measure the performance of ML systems. Finally, we discussed
    the need for governing ML systems to ensure the robust performance of ML systems.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: We will further explore the governance of ML systems and continual learning
    concepts in the next and final chapter!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
