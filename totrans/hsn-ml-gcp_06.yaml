- en: Essential Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, in previous chapters, we went through the various ETL processes available
    in GCP. In this chapter, we will start our journey of machine learning and deep
    learning through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised and unsupervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of major machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the accuracy of a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between machine learning and deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning encompasses a set of techniques that learn from historical
    data. Based on the patterns learned from historical data, the machine learning
    technique predicts the probability of an event happening on a future dataset.
    Given the way in which machine learning works, there are multiple applications
    of the set of techniques. Let's explore some of them in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Financial services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some applications in the field of finance are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the riskiness of a loan/credit card applicant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the credit limit of a given customer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting whether a card transaction is a fraudulent transaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the customer segments that need to be targeted for a campaign
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting whether a customer is likely to default in the next few months
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending the right financial product that a customer should buy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retail industry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some applications of the different techniques of machine
    learning in the retail industry:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the next product that a customer is likely to buy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating the optimal price point for a given product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting the number of units a product will sell over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Targeting customers by bundling products for promotion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating a customer lifetime value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telecom industry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few applications of machine learning in the telecom industry:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the likelihood of a call drop before the start of a call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting if a customer is likely to churn in the next few months
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying add-ons to monthly usage that could be sold to a customer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the customers who are less likely to pay for postpaid services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workforce optimization for field force effectiveness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised and unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised machine learning constitutes the set of techniques that work towards
    building a model that approximate a function. The function takes a set of input
    variables, which are alternatively called independent variables, and tries to
    map the input variables to the output variable, alternatively called the dependent
    variable or the label.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we know the label (or the value) we are trying to predict, for a
    set of input variables, the technique becomes a supervised learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar manner, in an unsupervised learning problem, we do not have the
    output variable that we have to predict. However, in unsupervised learning, we
    try to group the data points so that they form logical groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'A distinction between supervised and unsupervised learning at a high level
    can be obtained as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c8d6266-8c0c-45ae-b8c1-2efa36c973b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, the supervised learning approach can distinguish
    between the two classes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac66330c-2d9a-48a1-8038-60bad3a57ace.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In supervised learning, there are two major objectives that can be achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict the probability of an event happening—classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate the value of the continuous dependent variable—regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The major methods that can help in classification are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with these (except logistic regression), linear regression also helps
    in estimating a continuous variable (regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'While these techniques help in estimating a continuous variable or in predicting
    the probability of an event happening (discrete variable prediction), unsupervised
    learning helps in grouping. Grouping can be either of rows (which is a typical
    clustering technique) or of columns (a dimensionality reduction technique). The
    major methods of row groupings are:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density-based clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The major methods of column groupings are:'
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row groupings result in identifying the segments of customers (observations)
    that are there in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Column groupings result in reducing the number of columns. This comes in handy
    when the number of independent variables is high. Typically when this is the case,
    there could be an issue in building the model, as the number of weights that need
    to be estimated could be high. Also, there could be an issue in interpreting the
    model, as some of the independent variables could be highly correlated with each
    other. Principal component analysis or t-SNE comes in handy in such a scenario,
    where we reduce the number of independent variables without losing too much of
    the information that is present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go through an overview of all the major machine
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of machine learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before going through an overview of the major machine learning techniques, let's
    go through the function that we would want to optimize in a regression technique
    or a classification technique.
  prefs: []
  type: TYPE_NORMAL
- en: Objective function in regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a regression exercise, we estimate the continuous variable value. In such
    a scenario, our predictions can be lower than the actual value or higher; that
    is, the error value could be either positive or negative. In such a scenario,
    the objective function translates to minimizing the sum of squared values of the
    difference between the actual and predicted values of each of the observations
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical terms, the preceding is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42d4aba6-3c58-462f-a211-fb58ffc73338.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the given equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*SSE* stands for the *sum of squared errors*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y* refers to the actual value of the dependent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y''* refers to the estimated value of the dependent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∑ refers to the summation of the squared errors across all the observations
    in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the objective function, let's understand how linear regression works at
    a high level.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In linear regression, we assume a linear relationship between the independent
    variables and the dependent variable. Linear regression is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c3f054d-cbcf-4906-9e71-32069d48bac5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the given equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y* is the dependent variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* is the weight associated with the independent variable *X*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is the intercept value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there are multiple independent variables (let''s say two independent variables,
    *x1* and *x2*), the equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e42934ac-8357-4e22-b5c4-380b299582dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the given equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w1* is the weight associated with variable *x1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w2* is the weight associated with variable *x2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A typical linear regression looks as follows, where the *x* axis is the independent
    variable and the *y* axis is the dependent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8eb8d1b7-4303-4ee3-88d2-5b99a27c004b.png)'
  prefs: []
  type: TYPE_IMG
- en: The straight line (with a certain slope and intercept) is the equation of linear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the line in the graph is the one that minimizes the overall squared
    error.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision tree is a technique that helps us in deriving rules from data. A rule-based
    technique is very helpful in explaining how the model is supposed to work in estimating
    a dependent variable value.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical decision tree looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12b92c36-714f-4dfe-a7de-4c0b3dd14e52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ROOT Node:** This represents the entire population or a sample, and it is
    further divided into two or more further nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Splitting**: A process of dividing a node into two or more subnodes based
    on a certain rule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Node:** When a subnode splits into further subnodes, it is called
    **decision node.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaf/Terminal Node:** The final node in a decision tree is a leaf or terminal
    node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning:** When we remove the subnodes of a decision node, this process is
    called **pruning**. You can say it is the opposite process of splitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Branch/Sub-Tree:** A subsection of the entire tree is called a **branch**
    or a **sub-tree**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parent and child node:** A node that is divided into subnodes is called the
    **parent node** of subnodes, whereas the subnodes are the children of the parent
    node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given a dependent variable and an independent variable value, we will go through
    how a decision tree works using the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **var2** | **response** |'
  prefs: []
  type: TYPE_TB
- en: '| `0.1` | `1996` |'
  prefs: []
  type: TYPE_TB
- en: '| `0.3` | `839` |'
  prefs: []
  type: TYPE_TB
- en: '| `0.44` | `2229` |'
  prefs: []
  type: TYPE_TB
- en: '| `0.51` | `2309` |'
  prefs: []
  type: TYPE_TB
- en: '| `0.75` | `815` |'
  prefs: []
  type: TYPE_TB
- en: '| `0.78` | `2295` |'
  prefs: []
  type: TYPE_TB
- en: '| `0.84` | `1590` |'
  prefs: []
  type: TYPE_TB
- en: In the preceding dataset, the variable `var2` is the input variable and the
    `response` variable is the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step of the decision tree, we sort the input variable from lowest
    to highest and test multiple rules, one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: In the first instance, all the observations of the dataset that have a `var2`
    value of less than `0.3` belong to the left node of a decision tree, and the other
    observations belong to the right node of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: In a regression exercise, the predicted value of the left node is the average
    of the `response` variable for all the observations that belong to the left node.
    Similarly, the predicted value of the right node is the average of `response`
    for all the observations that belong to the right node.
  prefs: []
  type: TYPE_NORMAL
- en: Given a predicted value for the left node and a different predicted value for
    the observations that belong to the right node, the squared error can be calculated
    for each of the left and right nodes. The overall error for a probable rule is
    the sum of squared error in both left and right nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The decision rule that is implemented is the rule that has the minimum squared
    error among all the possible rules.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is an extension of decision trees. It is a forest as it is a combination
    of multiple trees, and is random as we randomly sample different observations
    for each of the decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: A random forest works by averaging the prediction of each of the decision trees
    (which work on a sample of the original dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a random forest works better than a single decision tree, as the
    influence of outliers is reduced in it (because in some samples, outliers might
    not have occurred), whereas, in a decision tree, an outlier would have definitely
    occurred (if the original dataset contained an outlier).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While a random forest works in a framework where multiple parallel trees are
    built, gradient boosting takes a different approach—building a deep framework.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient in gradient boosting refers to the difference between actual and
    predicted values, and boosting refers to improvement, that is, improving the error
    over different iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient boosting also leverages the way in which decision trees work in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a decision tree to estimate the dependent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the error, that is, the difference between actual and predicted value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build another decision tree that predicts the error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the prediction by taking the prediction of error of the previous decision
    tree into account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, gradient boosting continuously builds a decision tree that predicts
    the error of the previous decision tree and thus a depth-based framework in gradient
    boosting.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network provides a way to approximate nonlinear functions. Nonlinearity
    is achieved by applying activation functions on top of the summation of weighted
    input variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/046bc137-9809-4623-95c1-3c16eda6b6d3.png)'
  prefs: []
  type: TYPE_IMG
- en: The input level contains the inputs and the hidden layer contains the summation
    of the weighted input values, where each connection is associated with a weight.
  prefs: []
  type: TYPE_NORMAL
- en: The nonlinearity is applied to the hidden layer. Typical non-linear activation
    functions could be sigmoid, tanh, or rectified linear unit.
  prefs: []
  type: TYPE_NORMAL
- en: The output level is associated with the summation of weights associated with
    each hidden unit. The optimal value of weights associated with each connection
    is obtained by adjusting the weights in such a way that the overall squared error
    value is minimized. More details of how a neural network works are provided in
    a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed before, logistic regression is used to classify a prediction to
    one class or another depending on the input dataset. Logistic regression uses
    the sigmoid function to attain the probability of an event happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid curve looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba322a10-423e-4a95-8e72-2a137a06009c.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the output is a high probability when the *x* axis value is greater
    than 3 and the output is a very low probability when the *x* axis value is less
    than 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression differs from linear regression in the usage of the activation
    function. While a linear regression equation would be *Y = a + b * X*, a logistic
    regression equation would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6aba77a4-c022-44f6-9801-87670cbd67b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Objective function in classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a regression technique, we minimize the overall squared error. However, in
    a classification technique, we minimize the overall cross-entropy error.
  prefs: []
  type: TYPE_NORMAL
- en: 'A binary cross-entropy error is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0336cc5f-d283-4c29-b27a-79ee9c4e4a73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the given equation:'
  prefs: []
  type: TYPE_NORMAL
- en: y is the actual dependent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* is the probability of an event happening'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a classification exercise, all the preceding algorithms work; it's just
    that the objective function changes to cross-entropy error minimization instead
    of squared error.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a decision tree, the variable that belongs to the root node is
    the variable that provides the highest information gain when compared to all the
    rest of the independent variables. Information gain is defined as the improvement
    in overall entropy when the tree is split by a given variable when compared to
    no splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Data splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the key problems that need to be addressed while working on any machine
    learning model is: *how accurate can this model be once it is implemented in production
    on a future dataset?*'
  prefs: []
  type: TYPE_NORMAL
- en: It is not possible to answer this question straight away. However, it is really
    important to obtain the buy-in from commercial teams that ultimately get benefited
    from the model build. Dividing the dataset into training and testing datasets
    comes in handy in such a scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset is the data that is used to build the model. The testing
    dataset is the dataset that is not seen by the model; that is, the data points
    are not used in building the model. Essentially, one can think of the testing
    dataset as the dataset that is likely to come in future. Hence, the accuracy that
    we see on the testing dataset is likely to be the accuracy of the model on the
    future dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, in regression, we deal with the problem of generalization/overfitting.
    The overfitting problem arises when the model is so complex that it perfectly
    fits all the data points—thus resulting in a minimal possible error rate. A typical
    example of an overfitted dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0795b03-7685-41c0-ae81-a4de6b6a53c2.png)'
  prefs: []
  type: TYPE_IMG
- en: From the graph dataset, one can observe that the line (colored in black) does
    not fit all the data points perfectly, while the curve (colored in blue) fits
    the points perfectly and hence has minimal error on the data points on which it
    is trained.
  prefs: []
  type: TYPE_NORMAL
- en: However, the line has a better chance of being more generalizable when compared
    to the curve on a new dataset. Thus, in practice, regression/classification is
    a trade-off between generalizability and complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The lower the generalizability of the model, the higher the error rate on unseen
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'This phenomenon can be observed in the following graph. As the complexity of
    the model increases, the error rate of unseen data points keeps reducing till
    a point, after which it starts increasing again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae560c6a-9894-4e43-a8f4-a4594c219b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: The curve colored in blue is the error rate on the training dataset, and the
    curve colored in red is the testing dataset error rate.
  prefs: []
  type: TYPE_NORMAL
- en: The validation dataset is used to obtain the optimal hyperparameters of the
    model. For example, in techniques such as random forest or GBM, the number of
    trees needed to build or the depth of a tree is a hyper parameter. As we keep
    changing the hyperparameter, the accuracy on unseen datasets changes.
  prefs: []
  type: TYPE_NORMAL
- en: However, we cannot go on varying the hyperparameter until the test dataset accuracy
    is the highest, as we would have seen the practically future dataset (testing
    dataset) in such a scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The validation dataset comes in handy in such scenarios, where we keep varying
    the hyperparameters on the training dataset until we see that the accuracy on
    the validation dataset is the highest. That would thus form the optimal hyperparameter
    combination for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the accuracy of a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The methods of evaluating the accuracy of a model differ between supervised
    learning and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical linear regression (where continuous values are predicted), there
    are a couple of ways of measuring the error of the model. Typically, error is
    measured on the validation and testing datasets, as measuring error on a training
    dataset (the dataset using which a model is built) is misleading. Hence, error
    is always measured on the dataset that is not used to build a model.
  prefs: []
  type: TYPE_NORMAL
- en: Absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Absolute error is defined as the absolute value of the difference between the
    forecast value and actual value. Let''s imagine a scenario as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Actual value** | **Predicted value** | **Error** | **Absolute error**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Data point 1** | 100 | 120 | 20 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| **Data point 2** | 100 | 80 | -20 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| **Overall** | 200 | 200 | 0 | 40 |'
  prefs: []
  type: TYPE_TB
- en: In the preceding scenario, we see that the overall error is 0 (as one error
    is +20 and the other is -20). If we assume that the overall error of the model
    is 0, we are missing out the fact that the model is not working well on individual
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, in order to avoid the issue of a positive error and negative error canceling
    each other out and thus resulting in minimal error, we consider the absolute error
    of a model, which in this case is 40; and the absolute error rate is 40/200 =
    20%.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean square error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another approach of solving the problem of inconsistent signs of error is to
    square the error (the square of a negative number is a positive number). The scenario
    discussed previously can be translated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Actual value** | **Predicted value** | **Error** | **Squared error**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Data point 1** | 100 | 120 | 20 | 400 |'
  prefs: []
  type: TYPE_TB
- en: '| **Data point 2** | 100 | 80 | -20 | 400 |'
  prefs: []
  type: TYPE_TB
- en: '| **Overall** | 200 | 200 | 0 | 800 |'
  prefs: []
  type: TYPE_TB
- en: In this case, the overall squared error is 800 and root mean squared error is
    the square root of (800/2), which is 20.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy in the case of a classification exercise is measured as follows:
    absolute error and RMSE are applicable when predicting continuous variables. However,
    predicting an event with discrete outcomes is a different process. Discrete event
    prediction happens in terms of probabilities; that is, the result of the model
    is a probability that certain event happens. In such cases, even though absolute
    error and RMSE can be theoretically used, there are other metrics of relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix counts the number of instances when the model predicted
    the outcome of an event and measures it against the actual values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted fraud** | **Predicted non-fraud** |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual fraud** | **True positive** (**TP**) | **False negative** (**FN**)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual non-fraud** | **False positive** (**FP**) | **True negative** (**TN**)
    |'
  prefs: []
  type: TYPE_TB
- en: '*Sensitivity or TP rate or recall = TP/ (total positives) = TP/ (TP+FN)**Specificity
    or TN rate = TN/ (total negative) = TP/(FP + TN)**Precision or positive predicted
    value = TP/(TP + FP)**Accuracy = (TP + TN)/(TP + FN + FP + TN)**F1 score = 2TP/
    (2TP + FP + FN)*'
  prefs: []
  type: TYPE_NORMAL
- en: A **receiver operating characteristic** (**ROC**) curve gives the relation between
    the true positive rate and false positive rate of various cutoffs. Let's say the
    model prediction is >0.8\. We assume that we should classify the prediction as
    positive. The 0.8 here is the cutoff point. Cutoffs come into the picture here
    as a model's prediction will always be a probability number—a value between 0
    and 1\. Hence, an analyst needs to bring his/her judgment in ascertaining the
    optimal cutoff.
  prefs: []
  type: TYPE_NORMAL
- en: An ROC curve is a curve where (1-specificity) is on the *x* axis and sensitivity
    is on the *y* axis. The curve is generated by plotting the various combinations
    of sensitivity and (1-specificity) by changing the cutoff, which decides whether
    the predicted value should be a 1 or a 0.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal scenario, where data can be clearly segregated and accuracy is 100%,
    there lies a cutoff of the probability, after which the predicted value is of
    one class; it belongs to the other class for values below the cutoff. In such
    a scenario, for certain values of cutoffs, the ROC curve would be on the *y* axis
    only, that is, specificity=1\. For the rest of its length, the curve is going
    to be parallel to the *x* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical example of an ROC curve looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e0d298b-6836-409b-a46c-2b08334744a8.png)'
  prefs: []
  type: TYPE_IMG
- en: An ROC curve is a measure of how much better the model's performance is over
    a random guess. A random guess is where in case of a churn of 5% customers, the
    random guesser guesses that for every twenty customers, one among them will be
    labeled as a potential churner. In such a scenario, the random guess is going
    to capture 20% of all churners after randomly labeling 20% of all the customers.
  prefs: []
  type: TYPE_NORMAL
- en: A model's predictive power is in being able to move as close to 100% accuracy
    as possible, that is, moving away from random guesses as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Area under the curve** (**AUC**) is a measure of the area between the model
    curve and the random guess curve. The higher the AUC, the higher the predictive
    accuracy of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between machine learning and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at how various machine learning algorithms work at a
    high level. In this section, we will understand how deep learning differs from
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key attributes of a machine learning task is that the inputs are
    given by the analyst or data scientist. Quite often, feature engineering plays
    a key role in improving the accuracy of the model. Moreover, if the input dataset
    is an unstructured one, feature engineering gets a lot more tricky. More often
    than not, it boils down to the knowledge of individual in deriving relevant features
    to build a more accurate model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s imagine a scenario where, given a set of words in a sentence,
    we are trying to predict the next word. In such a scenario, traditional machine
    learning algorithms work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encode each word in a sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represent the input sequence of words using the one-hot encoded vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represent the output word, also using a one-hot encoded vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a model to predict the output word vector given the set of input words
    by optimizing for the relevant loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the preceding method works, we face three major challenges in building
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimension of the one-hot encoded vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A piece of text is likely to have hundreds or thousands of unique words
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: High-dimensional data is likely to result in multiple issues—such as multicollinearity
    and the time taken to build a model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Order of words is missing in the input dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between two words is the same, irrespective of whether the words are
    similar to each other or not:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in a one-hot encoded vector scenario, the distance between king
    and prince would be the same as the distance between king and cheese
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep learning comes in handy in such a scenario. Using some of the techniques
    in deep learning (for example, Word2vec), we would be able to solve the following
    among the issues listed just now:'
  prefs: []
  type: TYPE_NORMAL
- en: Represent each word in a lower-dimensional space in such a way that words that
    are similar to each other have similar word vectors and words that are not similar
    to each other do not have similar vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, by representing a word in a lower-dimensional space (let’s say 100),
    we would have solved the problem of high dimensionality of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple variants of the Word2vec technique, such as the continuous
    bag-of-words model and the continuous skip-gram model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a CBOW model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be7b10e6-7f0d-40fa-af6d-da43a40231fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the input vector is the one-hot encoded version (as we would have
    used in a typical machine learning model). The hidden layer neurons ensure that
    we represent the 10,000-dimensional input vector in a 300-dimensional word vector.
  prefs: []
  type: TYPE_NORMAL
- en: The actual values in the output layer represent the one-hot encoded versions
    of the surrounding words (which form the context).
  prefs: []
  type: TYPE_NORMAL
- en: Another technique in deep learning that comes in handy to solve the preceding
    problem is the **recurrent neural network** (**RNN**). An RNN works towards solving
    the sequence-of-words problem that traditional machine learning faced in the scenario
    laid out previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN provides each word vector in order to predict the next word in the sequence.
    More details of how RNN works will be provided in a different chapter. The popular
    variants of the RNN technique are **long short-term memory** (**LSTM**) and **gated
    recurrent unit** (**GRU**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e155073-cb6b-4175-9799-111cc8f80cba.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram represents a typical RNN, where *x[(t-1)]*, *x[(t)]* and
    *x[(t+1)]* represent the words in each time period, *W* is the weightage associated
    with a previous word in predicting the next word, and *O[(t)]* is the output in
    time *t*.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM comes in handy when the weightage that needs to be associated with a word
    that occurred much earlier in sequence would have be high in predicting the next
    word.
  prefs: []
  type: TYPE_NORMAL
- en: A combination of Word2vec and RNN, which are variants of neural networks, helps
    in avoiding the challenge of feature engineering with the given text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solidify our understanding of the difference between machine learning
    and deep learning, let''s go through another example: predicting the label of
    an image.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use a classic example—the MNIST dataset (we will be using MNIST a lot
    more in future chapters).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MNIST dataset contains images of various digits, from zero to nine. Each
    image is 28 x 28 pixels in size. The task is to predict the label of the image
    by analyzing the various pixel values. A sample image in the MNIST dataset looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afa8a2ff-8930-4e86-9c80-dc690995f794.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Traditional machine learning solves the preceding problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Treat each pixel as a separate variable; that is, we have a total of 784 variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encode the label column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the probability of a label occurring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The challenge with the way in which we solve the preceding problem is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The model will not take pixel adjacencies into account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model will not account for translation or rotation of the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when the image is shifted appropriately, a zero could look like
    a six or vice versa. Similarly, if all the images are trained using a dataset
    that had all the numbers centered in the image but the test dataset has an image
    that is shifted slightly to the right or left, the prediction is likely to be
    inaccurate. This is because the model would have placed a weightage for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve the preceding problem, a deep learning technique named **convolutional
    neural network** (**CNN**) comes in handy. A CNN works in such a way that it assigns
    weightages at a region level rather than at a pixel level. Essentially, this forms
    the convolution part of convolutional neural networks. In this way, pixel adjacencies
    are taken into account by using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, translation of an image is accounted for by a technique called **max
    pooling** that is used in CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical architecture of a CNN looks as follows, and more details of it
    will be explained in a later chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e774be9a-699f-4669-aa70-65924337e3b8.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the input is the image that we consider. **conv1**
    is the output when a convolution is applied between filters and input. Given that
    we apply multiple filters, we would have multiple convolutions, and **pool1**
    is the output of applying pooling on the convolution output. The process of convolution
    and pooling is applied repeatedly until we obtain the final fully connected unit,
    which is then linked to the output.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we understood why deep learning shines over machine
    learning in some applications. Let''s go through some of the applications of deep
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Translation from one language to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech-to-text conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image analysis in multiple industries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying text present in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image and audio synthesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personalization to predict the next movie/product that a user is likely to watch/buy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting rare events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we understood the major difference between supervised and unsupervised
    learning and got an overview of the major machine learning algorithms. We also
    understood the areas where deep learning algorithms shine over traditional machine
    learning algorithms, through examples of text and image analysis.
  prefs: []
  type: TYPE_NORMAL
