<html><head></head><body>
		<div class="Content" id="_idContainer080">
			<h1 id="_idParaDest-78"><em class="italics"><a id="_idTextAnchor109"/>Chapter 3</em></h1>
		</div>
		<div class="Content" id="_idContainer081">
			<h1 id="_idParaDest-79"><a id="_idTextAnchor110"/>Probability Distributions</h1>
		</div>
		<div class="Content" id="_idContainer082">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Generate different distributions in R</li>
				<li class="bullets">Estimate probability distribution functions for new datasets in R</li>
				<li class="bullets">Compare the closeness of two different samples of the same distribution or different distributions</li>
			</ul>
			<p>In this chapter, we will learn how to use probability distributions as a form of unsupervised learning.</p>
		</div>
		<div class="Content" id="_idContainer128">
			<h2 id="_idParaDest-80"><a id="_idTextAnchor111"/>Introduction</h2>
			<p>In this chapter, we're going to study another aspect of unsupervised learning, called <strong class="bold">probability distributions</strong>. Probability distributions are part of classical statistics covered in many mathematical textbooks and courses. With the advent of big data, we've started using probability distributions in exploratory data analysis and other modeling applications as well. So, in this chapter, we're going to study how to use probability distributions in unsupervised learning.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor112"/>Basic Terminology of Probability Distributions</h2>
			<p>There are two families of methods in statistics: parametric and non-parametric methods. Non-parametric methods are meant to deal with data that could take any shape. Parametric methods, by contrast, make assumptions about the particular shape that data takes on. These assumptions are often encoded as parameters. The following are the two main parameters that you should be aware of:</p>
			<ul>
				<li><strong class="bold">Mean</strong>: This is the average of all values in the distribution.</li>
				<li><strong class="bold">Standard Deviation</strong>: This is the measure of the spread of values around the mean of a distribution. </li>
			</ul>
			<p>Most of the parametric methods in statistics depend in some way on those two parameters. The parametric distributions that we're going to study in this chapter are these:</p>
			<ul>
				<li>Uniform distributions</li>
				<li>Normal distributions</li>
				<li>Log-normal distributions.</li>
				<li>Binomial distributions</li>
				<li>Poisson distributions</li>
				<li>Pareto distributions</li>
			</ul>
			<h3 id="_idParaDest-82"><a id="_idTextAnchor113"/>Uniform Distribution</h3>
			<p>In the uniform distribution, all values between an interval, let's say [a,b], are equiprobable. Mathematically, it's defined as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="Figure 3.1: Mathematical formula for uniform distribution" src="image/C12628_03_01.jpg"/>
				</div>
			</div>
			<h6>Figure 3.1: Mathematical formula for a uniform distribution</h6>
			<p>It can be plotted on a graph as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer084">
					<img alt="Figure 3.2: Graph of uniform distribution" src="image/C12628_03_02.jpg"/>
				</div>
			</div>
			<h6>Figure 3.2: Graph of a uniform distribution</h6>
			<p>The uniform distribution is the simplest of the parametric distributions. There are many processes in the real world that follow uniform sampling:</p>
			<ul>
				<li>If it's raining in a very large area, the distribution of where raindrops will fall can be assumed to be uniform in a small area.</li>
				<li>The last digit of a social security number should follow a uniform distribution for any subset of people, such as for all the CEOs of start-ups in California. </li>
				<li>Uniform random sampling is very important for generating data for experiments and running controlled trials.</li>
			</ul>
			<h3 id="_idParaDest-83"><a id="_idTextAnchor114"/>Exercise 14: Generating and Plotting Uniform Samples in R</h3>
			<p>In this exercise, we will generate uniform samples and plot them. To do this, perform the following steps:</p>
			<ol>
				<li>Use the built-in <strong class="inline">runif</strong> R function to generate uniform samples. Firstly, enter the number of samples you want to generate. Here we're generating 1,000 samples. Then, enter the <strong class="inline">min</strong> value and <strong class="inline">max</strong> value: <p class="snippet">uni&lt;-runif(1000, min=0, max=100)</p></li>
				<li>After storing the generated random numbers in the <strong class="inline">uni</strong> variable, we'll plot their values versus their index:<p class="snippet">plot(uni)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer085"><img alt="Figure 3.3: Uniform distribution" src="image/C12628_03_03.jpg"/></div><h6>Figure 3.3: Uniform distribution</h6><p>As you can see, the points are scattered everywhere almost equally. We can also plot a histogram of this to get a clearer picture of the distribution.</p></li>
				<li>We'll use the <strong class="inline">hist()</strong> function of R to plot a histogram of the generated sample:<p class="snippet">hist(uni)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer086">
					<img alt="Figure 3.4: Histogram of the distribution" src="image/C12628_03_04.jpg"/>
				</div>
			</div>
			<h6>Figure 3.4: Histogram of the distribution</h6>
			<p>As you can see, it's not exactly flat, as we envisioned it previously. It's more or less uniform, but not exactly uniform, because it was randomly generated. Each time we generate a new sample, it will resemble this histogram, and most probably won't be exactly flat, because of the noise that comes with all random sampling methods.</p>
			<h3 id="_idParaDest-84">Norm<a id="_idTextAnchor115"/>al Distribution</h3>
			<p>The normal distribution is a type of parametric distribution that is governed by two parameters: mean and standard deviation from the mean. It's symmetric about the mean, and most values are near the mean. Its curve is also known as a bell curve:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer087">
					<img alt="Figure 3.5: Approximate representation of a bell curve, typical with normally distributed data" src="image/C12628_03_05.jpg"/>
				</div>
			</div>
			<h6>Figure 3.5: Approximate representation of a bell curve, typical with normally distributed data</h6>
			<p>The normal distribution is defined by the following equation:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer088">
					<img alt="" src="image/C12628_03_06.jpg"/>
				</div>
			</div>
			<h6>Figure 3.6: Equation for the normal distribution</h6>
			<p>Here, <img alt="" src="image/C12628_03_Formula_01.png"/> is the mean and <img alt="" src="image/C12628_03_Formula_02.png"/> is the standard deviation.</p>
			<p>The normal distribution is a very common type of distribution in the real world. The following are some examples of the normal distribution:</p>
			<ul>
				<li>The height of NBA players is approximately normally distributed.</li>
				<li>The scores of students in a class could have a distribution that is very close to the normal distribution.</li>
				<li>The Nile's yearly flow volume is normally distributed.</li>
			</ul>
			<p>Now we're going to generate and plot a normal distribution in R.</p>
			<h3 id="_idParaDest-85">Exercise<a id="_idTextAnchor116"/> 15: Generating and Plotting a Normal Distribution in R</h3>
			<p>In this exercise, we will generate a normal distribution to model the test scores (out of 100) of 1,000 school pupils and plot them. To do this, perform the following steps:</p>
			<ol>
				<li value="1">Generate a normal distribution by entering the number of samples, the mean, and the standard deviation in the <strong class="inline">rnorm</strong> function in R:<p class="snippet">nor&lt;-rnorm(1000,mean=50, sd= 15)</p></li>
				<li>Plot the generated numbers against their index:<p class="snippet">plot(nor)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer091"><img alt="Figure 3.7: Normal distribution" src="image/C12628_03_07.jpg"/></div><h6>Figure 3.7: Normal distribution</h6><p>As you can see here, most values are around the mean of 50, and as we move away from 50, the number of points starts decreasing. This distribution will be clarified by with a histogram in the next step.</p></li>
				<li>Plot a histogram of the normal distribution with the <strong class="inline">hist()</strong> function:<p class="snippet">hist(nor)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="Figure 3.8: Normal distribution histogram" src="image/C12628_03_08.jpg"/>
				</div>
			</div>
			<h6>Figure 3.8: Normal distribution histogram</h6>
			<p>You can see that this shape very much resembles the bell curve of the normal distribution.</p>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor117"/>Skew and Kurtosis</h3>
			<p>As we have seen, many of the distributions you'll see in practice are assumed to be normal distributions. But not every distribution is a normal distribution. To measure the degree to which a distribution deviates from a standard normal distribution, we use two parameters:</p>
			<ul>
				<li>Skew</li>
				<li>Kurtosis</li>
			</ul>
			<p>The <strong class="bold">Skew</strong> of a distribution is the measure its asymmetry compared to the standard normal distribution. In a dataset with high skew, the mean and mode will differ from each other. The skew can be of two types: positive and negative:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer093">
					<img alt="Figure 3.9: Negative skew and positive skew" src="image/C12628_03_09.jpg"/>
				</div>
			</div>
			<h6>Figure 3.9: Negative skew and positive skew</h6>
			<p>Negative skew is when there is a long tail of values on the left-hand side of the mean, and positive skew is when there is a long tail of values on the right-hand side of the mean. Skewness can also be expressed with the following formula:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer094">
					<img alt="Figure 3.10: Mathematical formula for skewness" src="image/C12628_03_10.jpg"/>
				</div>
			</div>
			<h6>Figure 3.10: Mathematical formula for skewness</h6>
			<p>Here, <img alt="" src="image/C12628_03_Formula_03.png"/> is the expected value or the mean of <img alt="" src="image/C12628_03_Formula_04.png"/> , where <img alt="" src="image/C12628_03_Formula_05.png"/> and <img alt="" src="image/C12628_03_Formula_06.png"/> are the mean and standard deviation of the distribution respectively.</p>
			<p><strong class="bold">Kurtosis</strong> is a measure of the fatness of tails of a distribution compared to the normal distribution. Kurtosis doesn't introduce asymmetry in a distribution, unlike skewness. An illustration of this is provided here:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer099">
					<img alt="Figure 3.11: Kurtosis demonstration" src="image/C12628_03_11.jpg"/>
				</div>
			</div>
			<h6>Figure 3.11: Kurtosis demonstration</h6>
			<p>Kurtosis can also be expressed with the following formula:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer100">
					<img alt="Figure 3.12: Mathematical formula for Kurtosis" src="image/C12628_03_12.jpg"/>
				</div>
			</div>
			<h6>Figure 3.12: Mathematical formula for Kurtosis</h6>
			<p>Here, <img alt="" src="image/C12628_03_Formula_07.png"/> is the expected or average value of <img alt="" src="image/C12628_03_Formula_08.png"/>, where <img alt="" src="image/C12628_03_Formula_05.png"/> and <img alt="" src="image/C12628_03_Formula_06.png"/> are the mean and standard deviation of the distribution respectively. A standard normal distribution has a skew of 0 and a kurtosis measure equal to 3. Because normal distributions are very common, we sometimes just measure excess kurtosis, which is this:</p>
			<p class="snippet">Kexcess = K - 3</p>
			<p>So, a normal distribution has excess kurtosis equal to 0.</p>
			<h3 id="_idParaDest-87">Log-Normal Distributio<a id="_idTextAnchor118"/>ns</h3>
			<p>Log-normal distributions are distributions of values whose logarithm is distributed normally. If we show a log-normal distribution on a log scale, it is perfectly identical to a normal distribution, but if we show it on a standard distribution scale, it acquires very high positive skewness:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer105">
					<img alt="Figure 3.13: Log-normal distribution" src="image/C12628_03_13.jpg"/>
				</div>
			</div>
			<h6>Figure 3.13: Log-normal distribution</h6>
			<p>To show that the log-normal distribution is a normal distribution on log scale, we're going to do an exercise.</p>
			<p>Log-normal distributions are used in the field of financial risk management to model stock prices. As the growth factor is assumed to be normally distributed, the prices of stock can be modeled log-normally. This distribution is also used in calculations related to option pricing, including value at risk (VaR).</p>
			<h3 id="_idParaDest-88"><a id="_idTextAnchor119"/>Exercise 16: Generating a Log-Normal Distribution from a Normal Distribution</h3>
			<p>In this exercise, we will generate a log-normal distribution from a normal distribution. To do this, perform the following steps:</p>
			<ol>
				<li value="1">Generate a normal distribution and store the values in a variable:<p class="snippet">nor&lt;-rnorm(1000,mean=5, sd= 1)</p></li>
				<li>Plot a histogram of the normal distribution with 100 different bins:<p class="snippet">hist(nor,breaks = 100)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer106"><img alt="Figure 3.14: Normal distribution with a mean of 5 and a standard deviation 1" src="image/C12628_03_14.jpg"/></div><h6>Figure 3.14: Normal distribution with a mean of 5 and a standard deviation of 1</h6></li>
				<li>Create a vector that will store 1,000 values for a log-normal distribution:<p class="snippet">lnor &lt;- vector("list", 1000)</p></li>
				<li>Enter exponential values into the <strong class="inline">lnor</strong> vector. The exponent function is the inverse function of the log function:<p class="snippet">for (x in 1:1000){</p><p class="snippet">  lnor[x]=exp(nor[x])</p><p class="snippet">}</p></li>
				<li>Plot a histogram of <strong class="inline">lnor</strong>:<p class="snippet">hist(as.integer(lnor), breaks = 200)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="Figure 3.15: Log-normal distribution" src="image/C12628_03_15.jpg"/>
				</div>
			</div>
			<h6>Figure 3.15: Log-normal distribution</h6>
			<p>Notice how the preceding figure looks like a log-normal distribution plot and that this plot is generated from a normal distribution. If we plot a new graph after taking the log of values in the preceding graph, then it'll be a normal distribution again.</p>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor120"/>The Binomial Distribution</h3>
			<p>The binomial distribution is a discrete distribution, as opposed to normal or uniform distribution, which are continuous in nature. The binomial distribution is used to model the probability of multiple events occurring together where there are two possibilities for each event. One example where the binomial distribution can be applied is in finding out the probability of heads coming up for all three coins when we toss three coins together.</p>
			<p>The mean and variance of a binomial distribution are n*p and n*p(1-p) respectively, where p is the probability of success and n is the number of trials. The binomial distribution is symmetric when p= 0.5. When p is less than 0.5, it's skewed more toward the right, and is skewed more toward the left when p is greater than 0.5. </p>
			<p>The formula for the binomial distribution is as follows:</p>
			<p class="snippet">P(x) = n!/((n-x)!x!)*(p^x)*((1-p)^x)</p>
			<p>Here, n is the total number of trials, x is the focal number of trials, and p is the probability of success.</p>
			<h3 id="_idParaDest-90">Exercise 17: Generating a<a id="_idTextAnchor121"/> Binomial Distribution</h3>
			<p>In this exercise, we will generate a binomial distribution to model the number of times that heads will come up when tossing a coin 50 times. To do this, perform the following steps:</p>
			<ol>
				<li value="1">To generate a binomial distribution, we'll first need a sequence of 50 digits as an index, which will act as the number of successes we are interested in modeling. This would be x in the formula in the previous section:<p class="snippet">s &lt;- seq(0,50,by = 1)</p></li>
				<li>Now we will pass <strong class="inline">s</strong> as a parameter to the <strong class="inline">dbinom()</strong> function in R, which will calculate probabilities for every value in the <strong class="inline">s</strong> variable and store them in a new <strong class="inline">probs</strong> variable. Firstly, in the function, we enter the sequence that will encode the range of the number of successes. Then, we enter the length of the sequence, and then we enter the probability of success:<p class="snippet">probs &lt;- dbinom(s,50,0.5)</p></li>
				<li>In the final step, we plot <strong class="inline">s</strong> and <strong class="inline">probs</strong> together:<p class="snippet">plot(s,probs)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 3.16: Binomial distribution" src="image/C12628_03_16.jpg"/>
				</div>
			</div>
			<h6>Figure 3.16: Binomial distribution</h6>
			<p>Here, the x axis shows a number of heads we are interested in, and the y axis shows the probability of getting exactly that number of heads in 50 trials. When we toss a coin 50 times, the most probable outcome is that we will get 25 heads and 25 tails, but the probability of getting all 50 heads or tails is very low. This is explained very well by the preceding graph.</p>
			<h3 id="_idParaDest-91"><a id="_idTextAnchor122"/>The Poisson Distribution</h3>
			<p>The Poisson distribution is another type of discrete distribution that is used to model occurrences of an event in a given time period given the mean number of occurrences of that event in a particular time period.</p>
			<p>It's formulated by the following equation:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer109">
					<img alt="Figure 3.17: Formula for Poisson distribution" src="image/C12628_03_17.jpg"/>
				</div>
			</div>
			<h6>Figure 3.17: Formula for poisson distribution</h6>
			<p>Here, lambda is the mean number of occurrences in a given time period, <strong class="bold">e</strong> is Euler's constant, and <strong class="bold">x</strong> is the number of events for which you want to find the probability. Given the number of new people that have been observed coming to an event every minute so far, the, Poisson distribution can be used to calculate the probability of any number of people coming to that event in the next minute.</p>
			<p>A Poisson distribution plot looks like this:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 3.18: Plot for poisson distribution" src="image/C12628_03_18.jpg"/>
				</div>
			</div>
			<h6>Figure 3.18: Plot for poisson distribution</h6>
			<p>Here, we can see probabilities of the different values of <strong class="bold">x</strong> vary with respect to lambda.</p>
			<h3 id="_idParaDest-92"><a id="_idTextAnchor123"/>The Pareto Distribution</h3>
			<p>The pareto distribution is an exponent-based probability distribution. This distribution was invented to model the 80:20 rule that is observed in many real-world situations. Some fascinating situations that follow the 80:20 rule are listed here:</p>
			<ul>
				<li>Approximately 80% of the world's wealth is owned by 20% of people.</li>
				<li>In business management, it was found that for most companies, 80% of their revenue was generated by 20% of their customers.</li>
				<li>It is said that 20% of all drivers cause 80% of all accidents.</li>
			</ul>
			<p>There are many other real-world observations that can be modeled by the Pareto distribution. The mathematical formula of the Pareto distribution is as follows: </p>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 3.19: Mathematical formula of the Pareto distribution" src="image/C12628_03_19.jpg"/>
				</div>
			</div>
			<h6>Figure 3.19: Mathematical formula of the Pareto distribution</h6>
			<h2 id="_idParaDest-93">Introduction to Kernel Densit<a id="_idTextAnchor124"/>y Estimation</h2>
			<p>So far, we've studied parametric distributions in this chapter, but in real life, all distributions are either approximations of parametric distributions or don't resemble any parametric distributions at all. In such cases, we use a technique called <strong class="bold">Kernel Density Estimation</strong>, or <strong class="bold">KDE</strong>, to estimate their probability distributions.</p>
			<p>KDE is used to estimate the probability density function of distributions or random variables with given finite points of that distribution using something called a kernel. This will be more clear to you after you continue further in the chapter. </p>
			<h3 id="_idParaDest-94"><a id="_idTextAnchor125"/>KDE Algorithm</h3>
			<p>Contrary to wha<a id="_idTextAnchor126"/>t it might seem like given the heavy name, KDE is a very simple two-step process:</p>
			<ol>
				<li value="1">Choosing a kernel</li>
				<li>Placing the kernel on data points and taking the sum of kernels</li>
			</ol>
			<p>A kernel is a non-negative symmetric function that is used to model distributions. For example, in KDE, a normal distribution function is the most commonly used kernel function. Kernel functions can be of different types. They are very much related to the distributions we studied earlier in the chapter. Some of the types are summarized here:</p>
			<ul>
				<li>In a uniform kernel, all values in a range are given equal weightage. This is represented as follows:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 3.20: Representation of uniform kernel function" src="image/C12628_03_20.jpg"/>
				</div>
			</div>
			<h6>Figure 3.20: Representation of a uniform kernel function</h6>
			<ul>
				<li>In a triangular kernel, weightage increases linearly as values move toward the middle of the range. This is represented as follows:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 3.21: Representation of triangular kernel function" src="image/C12628_03_21.jpg"/>
				</div>
			</div>
			<h6>Figure 3.21: Representation of a triangular kernel function</h6>
			<ul>
				<li>In a Gaussian kernel, weightage is distributed normally. This is represented as follows:</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 3.22: Representation of a Gaussian kernel function" src="image/C12628_03_22.jpg"/>
				</div>
			</div>
			<h6>Figure 3.22: Representation of a Gaussian kernel function</h6>
			<p>Along with the kernel, in the first step, we have to choose another parameter called the bandwidth of the kernel. Bandwidth is the parameter that affects the smoothness of the kernel. Choosing the right bandwidth is very important, even more important than choosing the right kernel. We'll look at an example here.</p>
			<h3 id="_idParaDest-95"><a id="_idTextAnchor127"/>Exercise 18: Visualizing and Understanding KDE</h3>
			<p>Let's suppose we have a distribution of five different points (1, 2, 3, 4, and 5). Let's visualize and understand KDE using this example:</p>
			<ol>
				<li value="1">Store the vector of the five points in a variable:<p class="snippet">x&lt;- c(1,2,3,4,5)</p></li>
				<li>Plot the points:<p class="snippet">y&lt;-c(0,0,0,0,0)</p><p class="snippet">plot(x,y)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer115"><img alt="Figure 3.23: Plot of the five points" src="image/C12628_03_23.jpg"/></div><h6>Figure 3.23: Plot of the five points</h6></li>
				<li>Install the <strong class="inline">kdensity</strong> package, if you don't have it already, and import it:<p class="snippet">install.packages("kdensity")</p><p class="snippet">library('kdensity')</p></li>
				<li>Compute the kernel density with the <strong class="inline">kdensity()</strong> function. Enter the distribution, <strong class="inline">x</strong>, and the bandwidth parameter as <strong class="inline">.35</strong>. The kernel is <strong class="inline">gaussian</strong> by default:<p class="snippet">dist &lt;- kdensity(x, bw=.35)</p></li>
				<li>Plot the KDE as follows:<p class="snippet">plot(dist)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer116"><img alt="Figure3.24: Plot of the Gaussian kernel" src="image/C12628_03_24.jpg"/></div><h6>Figure 3.24: Plot of the Gaussian kernel</h6><p>This is the final output of KDE. In this next step, it assumed that there was a Gaussian kernel centered on every point (1, 2, 3, 4, and 5) and summed them together to get this plot. The following figure will make it clearer:</p><h4>Note</h4><p class="callout">This graph is for illustration purposes rather than for generation in R.</p><div class="IMG---Figure" id="_idContainer117"><img alt="Figure 3.25: Gaussian kernel plotted on each point" src="image/C12628_03_25.jpg"/></div><h6>Figure 3.25: Gaussian kernel plotted on each point</h6><p>As you can see in the preceding figure, a Gaussian kernel was plotted on each one of the points and then all the kernels were summed to get the final curve.</p><p>Now, what if we were to change the bandwidth to 0.5 instead of 0.35?</p></li>
				<li>Change the bandwidth to 0.5 in the <strong class="inline">kdensity()</strong> function and plot the <strong class="inline">kdensity</strong> plot again:<p class="snippet">dist &lt;- kdensity(x, bw=.5)</p><p class="snippet">plot(dist)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer118">
					<img alt="Figure 3.26: Plot of the Gaussian kernel with a bandwidth of .5" src="image/C12628_03_26.jpg"/>
				</div>
			</div>
			<h6>Figure 3.26: Plot of the Gaussian kernel with a bandwidth of .5</h6>
			<p>You can see that the kernel is much smoother now. The following kernels were used:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer119">
					<img alt="Figure 3.27: Gaussian kernel plotted on each point" src="image/C12628_03_27.jpg"/>
				</div>
			</div>
			<h6>Figure 3.27: Gaussian kernel plotted on each point</h6>
			<h4>Note</h4>
			<p class="callout">The graph is for illustration purposes rather than for generation in R.</p>
			<p>This time, the kernels are much wider.</p>
			<p>If we were given a sufficient amount of points for estimation, the choice of kernel wouldn't change the shape of the final KDE as much as the choice of bandwidth parameter. So, choosing the ideal bandwidth parameter is an important step. There are many techniques that are used to select the ideal bandwidth parameter. Studying them is beyond the scope of this book, but the R libraries can take care of selecting the ideal parameter on their own. We'll study this in the next exercise.</p>
			<h3 id="_idParaDest-96"><a id="_idTextAnchor128"/>Exercise 19: Studying the Effect of Changing Kernels on a Distribution</h3>
			<p>In this exercise, we'll generate two normal distributions with different standard deviations and means, and combine them both to generate their combined KDE:</p>
			<ol>
				<li value="1">Generate two different normal distributions and store them in two variables:<p class="snippet">y1 &lt;- rnorm(100,mean = 0, sd = 1)</p><p class="snippet">y2&lt;-rnorm(100, mean = 3, sd=.2)</p></li>
				<li>Combine the generated distributions and plot them:<p class="snippet">y3&lt;-c(y1,y2)</p><p class="snippet">plot(y3)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer120"><img alt="Figure 3.28: Plot of combined distributions" src="image/C12628_03_28.jpg"/></div><h6>Figure 3.28: Plot of combined distributions</h6><p>You can see there are two different distributions with different means and spreads (standard deviations).</p></li>
				<li>Plot a histogram of <strong class="inline">y3</strong> for reference:<p class="snippet">hist(y3)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer121"><img alt="Figure 3.29: Histogram of the resultant distribution" src="image/C12628_03_29.jpg"/></div><h6>Figure 3.29: Histogram of the resultant distribution</h6></li>
				<li>Generate and plot the KDE of <strong class="inline">y3</strong> with a <strong class="inline">gaussian</strong> kernel:<p class="snippet">dist&lt;-kdensity(y3,kernel = "gaussian")</p><p class="snippet">plot(dist)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer122"><img alt="Figure 3.30: Plot of Gaussian kernel density" src="image/C12628_03_30.jpg"/></div><h6>Figure 3.30: Plot of Gaussian kernel density</h6><p>In the preceding plot, we used a Gaussian kernel and the bandwidth was selected by the function automatically. In this distribution, we have 200 points, which should be enough for generating a robust KDE plot such that changing the kernel type won't produce a significant difference in the final KDE plot. In the next step, let's try and change the kernel and look at the final plot.</p></li>
				<li>Generate and plot the KDE with the <strong class="inline">triangular</strong> kernel:<p class="snippet">dist&lt;-kdensity(y3,kernel = "triangular")</p><p class="snippet">plot(dist)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer123">
					<img alt="Figure 3.31: KDE with triangular kernel" src="image/C12628_03_31.jpg"/>
				</div>
			</div>
			<h6>Figure 3.31: KDE with triangular kernel</h6>
			<p>Both plots with different kernels look almost identical. So, the choice of bandwidth is much more important than the choice of kernel. In this exercise, the bandwidth was chosen automatically by the <strong class="inline">kdensity</strong> library of R.</p>
			<h3 id="_idParaDest-97">Activity 8: Finding the Standard Distribu<a id="_idTextAnchor129"/>tion Closest to the Distribution of Variables of the Iris Dataset</h3>
			<p>In this activity, we will find the standard distribution closest to the distribution of variables of the Iris dataset for the setosa species. These steps will help you complete the activity:</p>
			<ol>
				<li value="1">Load the Iris dataset.</li>
				<li>Select rows corresponding to the setosa species only.</li>
				<li>Plot the distribution generated by the <strong class="inline">kdensity</strong> function for sepal length and sepal width.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 218.</p></li>
			</ol>
			<p>The final outcome of this activity will be a plot of KDE for sepal width, as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer124">
					<img alt="Figure 3.32: Expected plot of the KDE for sepal width " src="image/C12628_03_32.jpg"/>
				</div>
			</div>
			<h6>Figure 3.32: Expected plot of the KDE for sepal width </h6>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor130"/>Introduction to the Kolmogorov-Smirnov Test</h2>
			<p>Now that we’ve learned how to generate the probability density functions of datasets that don't closely resemble standard distributions, we’ll learn how to perform some tests to distinguish these nonstandard distributions from each other.</p>
			<p>Sometimes, we're given multiple observed samples of data and we want to find out whether those samples belong to the same distribution or not. In the case of standard distributions, we have multiple tests, such as Student's t-test and z-test, to determine this. For non-standard distributions, or where we don't know the type of distribution, we use the Kolmogorov-Smirnov test. To understand the Kolmogorov-Smirnov test, you first need to understand a few terms:</p>
			<ul>
				<li><strong class="bold">Cumulative Distribution Function (CDF)</strong>: This is a function whose value gives the probability of a random variable being less than or equal to the argument of the function.</li>
				<li><strong class="bold">Null Hypothesis</strong>: In hypothesis testing, a null hypothesis means there is no significant difference between the observed samples. In hypothesis testing, our aim is to falsify the null hypothesis.</li>
			</ul>
			<h3 id="_idParaDest-99"><a id="_idTextAnchor131"/>The Kolmogorov-Smirnov Test Algorithm</h3>
			<p>In a Kolmogorov-Smirnov test, we perform the following steps: </p>
			<ol>
				<li value="1">Generate a CDF for both functions.</li>
				<li>Specify one of the distributions as the parent distribution.</li>
				<li>Plot the CDF of the two functions together in the same plot.</li>
				<li>Find the greatest vertical difference between the points in both CDFs.</li>
				<li>Calculate the test statistic from the distance measured in the previous step.</li>
				<li>Find the critical values in the Kolmogorov-Smirnov table.</li>
			</ol>
			<p>In R, these steps are automated, so we don't have to do each one of them individually. </p>
			<h3 id="_idParaDest-100"><a id="_idTextAnchor132"/>Exercise 20: Performing the Kolmogorov-Smirnov Test on Two Samples</h3>
			<p>To perform the Kolmogorov-Smirnov test on two samples, execute the following steps: </p>
			<ol>
				<li value="1">Generate two independent distributions for comparison:<p class="snippet">x_norm&lt;-rnorm(100, mean = 100, sd=5)</p><p class="snippet">y_unif&lt;-runif(100,min=75,max=125)</p></li>
				<li>Plot the <strong class="inline">CDF</strong> of <strong class="inline">x_norm</strong> as follows:<p class="snippet">plot(ecdf(x_norm))</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer125"><img alt="Figure 3.33: Plot of ecdf(x_norm)" src="image/C12628_03_33.jpg"/></div><h6>Figure 3.33: Plot of ecdf(x_norm)</h6><p>To plot <strong class="inline">ecdf(y_unif)</strong>, execute the following:</p><p class="snippet">plot(ecdf(y_unif),add=TRUE)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer126"><img alt="" src="image/C12628_03_34.jpg"/></div><h6>Figure 3.34: Plot of ecdf(y_unif)</h6><p>As you can see, the CDF of the functions look completely different, so the Kolmogorov-Smirnov test will return p values that are very small.</p></li>
				<li>Run the Kolmogorov-Smirnov test with the <strong class="inline">ks.test()</strong> function in R:<p class="snippet">ks.test(x_norm,y_unif)</p><p>The output is as follows:</p><p class="snippet">    Two-sample Kolmogorov-Smirnov test</p><p class="snippet">data:  x_norm and y_unif</p><p class="snippet">D = 0.29, p-value = 0.0004453</p><p class="snippet">alternative hypothesis: two-sided</p><h4>Note</h4><p class="callout">This exercise depends on randomly generated data. So, when you run this code, some of the numbers might be different. In hypothesis testing, there are two hypotheses: the null hypothesis, and the test hypothesis. The goal of hypothesis testing is to determine whether we have strong enough evidence to reject the null hypothesis. In this case, the null hypothesis is that the two samples were generated by the same distribution, and the test hypothesis is that the two samples were not generated by the same distribution. The p-value represents the probability, assuming the null hypothesis is true, of observing differences as extreme or more extreme than what is observed. When the p-value is very close to zero, we take that as evidence that the null hypothesis is false and vice versa.</p><p>As you can see, <strong class="inline">ks.test()</strong> returns two values, <strong class="inline">D</strong> and p-value. The <strong class="inline">D</strong> value is the absolute maximum distance between two points in the CDF of both distributions. The closer it is to zero, the greater the chance that both samples belong to the same distribution. The p-value has the same interpretation as in any other case. </p><p>In our case, <strong class="inline">D</strong> is <strong class="inline">0.29</strong> and the p-value is very low, near zero. So, we reject the null hypothesis that both samples belong to the same distribution. Now, in the next step, let's generate a new normal distribution and see its effect on the p-value and <strong class="inline">D</strong>.</p></li>
				<li>Generate a new normal distribution with the same <strong class="inline">mean</strong> and <strong class="inline">sd</strong> as <strong class="inline">xnorm</strong>:<p class="snippet">x_norm2&lt;-rnorm(100,mean=100,sd=5)</p></li>
				<li>Plot the combined CDF of <strong class="inline">x_norm</strong> and <strong class="inline">x_norm2</strong>:<p class="snippet">plot(ecdf(x_norm))</p><p class="snippet">plot(ecdf(x_norm2),add=TRUE)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer127"><img alt="Figure 3.35 Plot of combined cdf" src="image/C12628_03_35.jpg"/></div><h6>Figure 3.35 Plot of combined cdf</h6></li>
				<li>Run <strong class="inline">ks.tes<a id="_idTextAnchor133"/>t()</strong> on <strong class="inline">x_norm</strong> and <strong class="inline">x_norm2</strong>:<p class="snippet">ks.test(x_norm,x_norm2)</p><p>The output is as follows:</p><p class="snippet">    Two-sample Kolmogorov-Smirnov test</p><p class="snippet">data:  x_norm and x_norm2</p><p class="snippet">D = 0.15, p-value = 0.2106</p><p class="snippet">alternative hypothesis: two-sided</p><p>As you can see, the p-value is much higher this time and <strong class="inline">D</strong> is much lower. So, according to the p-value, we are less justified in rejecting the null hypothesis that both samples belong to the same distribution.</p></li>
			</ol>
			<h3 id="_idParaDest-101"><a id="_idTextAnchor134"/>Activity 9: Calculating the CDF and Performing the Kolmogorov-Smirnov Test with the Normal Distribution</h3>
			<p>With the help of randomly generated distributions, calculate what standard distribution the sample of sepal length and width is closest to:</p>
			<ol>
				<li value="1">Load the Iris dataset into a variable.</li>
				<li>Keep rows with the setosa species only.</li>
				<li>Calculate the mean and standard deviation of sepal length.</li>
				<li>Generate a new normal distribution with the mean and standard deviation of the sepal length column.</li>
				<li>Plot the CDF of both functions.</li>
				<li>Generate the results of the Kolmogorov-Smirnov test and check whether the distribution is a normal distribution.</li>
				<li>Repeat steps 3, 4, 5, and 6 for the sepal width column.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 219.</p></li>
			</ol>
			<p>The final outcome of this activity will be as follows:</p>
			<p class="snippet">    Two-sample Kolmogorov-Smirnov test</p>
			<p class="snippet">data: xnorm and df$Sepal.Width</p>
			<p class="snippet">D = 0.12, p-value = 0.7232</p>
			<p class="snippet">alternative hypothesis: two-sided </p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor135"/>Summary</h2>
			<p>Congratulations on completing the third chapter of the book! In this chapter, we learned the types of standard probability distribution, as well as when and how to generate them in R. We also learned how to find PDFs and CDFs of unknown distributions with KDE. In the final section, we learned how to compare two samples and determine whether they belong to the same distribution in R. In further chapters, we will learn about other types of unsupervised learning techniques that will help not only in exploratory data analysis but also give us other useful insights into data as well.</p>
		</div>
	</body></html>