- en: Deep Learning with OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning is a state-of-the-art form of machine learning that is reaching
    its best accuracy in image classification and speech recognition. Deep learning
    is also used in other fields, such as robotics and artificial intelligence with
    reinforcement learning. This is the main reason OpenCV is making significant efforts
    to include deep learning at its core. We are going to learn the basic use of OpenCV
    deep learning interfaces and look at using them in two use cases: object detection
    and face detection.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn the basics of deep learning and see how
    to use it in OpenCV. To reach our objective, we are going to learn object detection
    and classification using the **you only look once **(**YOLO**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How OpenCV works with deep learning and implementing deep learning **neural
    networks**(**NN**s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO – a very fast deep learning object detection algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection using Single Shot Detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow the chapter with ease, it is required that you install OpenCV with
    the deep learning module compiled. If you do not have this module, you will not
    be able to compile and run the sample codes.
  prefs: []
  type: TYPE_NORMAL
- en: It's very useful to have an NVIDIA GPU with CUDA support. You can enable CUDA
    on OpenCV to improve the speed of training and detection.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can download the code used in this chapter from [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2SmbWf7](http://bit.ly/2SmbWf7)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is most commonly written about in scientific papers nowadays with
    regards to image classification and speech recognition. This is a subfield of
    machine learning, based on traditional neural networks and inspired by the structure
    of the brain. To understand this technology, it is very important to understand
    what a neural network is and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: What is a neural network and how can we learn from data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The neural network is inspired by the structure of the brain, in which multiple
    neurons are interconnected, creating a network. Each neuron has multiple inputs
    and multiple outputs, like a biological neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'This network is distributed in layers, and each layer contains a number of
    neurons that are connected to all the previous layer''s neurons. This always has
    an input layer, which normally consists of the features that describe the input
    image or data, and an output layer, which normally consists of the result of our
    classification. The other middle layers are called **hidden layers**. The following
    diagram shows a basic three-layer neural network in which the input layer contains
    three neurons, the output layer contains two neurons, and a hidden layer contains
    four neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5ed96eb-8a0a-4957-adf4-b31b8e65075d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The neuron is the basic element of a neural network and it uses a simple mathematical
    formula that we can see in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16287cff-6b21-429b-b6af-74a22e2719db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, for each neuron, **i**, we mathematically add all the previous
    neuron''s output, which is the input of neuron **i** (**x1**, **x2**...), by a
    weight (**wi1**, **wi2**...) plus a bias value, and the result is the argument
    of an activation function, **f**. The final result is the output of **i** neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d237c49-7c71-4b51-8d9f-e550d882ccca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The most common activation functions (**f**) on classical neural networks are
    the sigmoid function or linear functions. The sigmoid function is used most often,
    and it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad7a4ce4-bf8b-410f-beb6-d196b485d28b.png)'
  prefs: []
  type: TYPE_IMG
- en: But how can we learn a neural network with this formula and these connections?
    How do we classify input data? The learn algorithm of neural networks can be called
    **supervised** if we know the desired output; while learning, the input pattern
    is given to the net's input layer. Initially, we set up all weights as random
    numbers and send the input features into the network, checking the output result.
    If this is wrong, we have to adjust all the weights of the network to get the
    correct output. This algorithm is called **backpropagation**. If you want to read
    more about how a neural network learns, check out [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)
    and [https://youtu.be/IHZwWFHWa-w](https://youtu.be/IHZwWFHWa-w).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a brief introduction to what a neural network is and the internal
    architecture of NN, we are going to explore the differences between NN and deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning neural networks have the same background as the classical neural
    network. However, in the case of image analysis, the main difference is the input
    layer. In a classical machine learning algorithm, the researcher has to identify
    the best features that define the image target to classify. For example, if we
    want to classify numbers, we could extract the borders and lines of numbers in
    each image, measure the area of an object in an image, and all of these features
    are the input of the neural network, or any other machine learning algorithm.
    However, in deep learning, you don't have to explore what the features are; instead,
    you use whole image as an input of the neural network directly. Deep learning
    can learn what the most important features are and **deep neural networks** (**DNN**)
    are able to detect an image or input and recognize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn what these features are, we use one of the most important layers in
    deep learning and neural networks: the **convolutional layer**. A convolutional
    layer works like a convolutional operator, where a kernel filter is applied to
    the whole previous layer, giving us a new filtered image, like a sobel operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98fbb3b3-3557-40ed-9dd4-0805262af590.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in a convolutional layer we can define different parameters, and one
    of them is the number of filters and the sizes we want to apply to the previous
    layer or image. These filters are calculated in the learning step, just like the
    weights on a classical neural network. This is the magic of deep learning: it
    can extract the most significant features from labeled images.'
  prefs: []
  type: TYPE_NORMAL
- en: However, these convolutional layers are the main reason behind the name **deep**,
    and we are going to see why in the following basic example. Imagine we have a
    100 x 100 image. In a classical neural network, we will extract the most relevant
    features we can imagine from the input image. This will normally approximately
    1,000 features, and with each hidden layer we can increase or decrease this number,
    but the number of neurons to calculate its weights is reasonable to compute in
    a normal computer. However, in deep learning, we normally start applying a convolutional
    layer – with a 64 filter kernels of 3 x 3 size. This will generate a new layer
    of 100 x 100 x 64 neurons with 3 x 3 x 64 weights to calculate. If we continue
    adding more and more layers, these numbers quickly increase and require huge computing
    power to learn the good weights and parameters of our deep learning architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional layers are one of the most important aspects of the deep learning
    architecture, but there are also other important layers, such as **Pooling**,
    **Dropout**, **Flatten**, and **Softmax**. In the following diagram, we can see
    a basic deep learning architecture in which some convolutional and pooling layers
    are stacked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4aaa0318-f894-4bfb-8b1f-f52c66642517.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, there is one more very important thing that makes deep learning get
    the best results: the amount of labeled data. If you have a small dataset, a deep
    learning algorithm will not help you in your classification because there is not
    enough data to learn the features (the weights and parameters of your deep learning
    architecture). However, if you have tons of data, you will get very good results.
    But take care, you will need a lot of time to compute and learn the weights and
    parameters of your architecture. This is why deep learning was not used early
    in the process, because computing requires a lot of time. However, thanks to new
    parallel architectures, such as NVIDIA GPUs, we can optimize the learning backpropagation
    and speed up the learning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep learning module was introduced to OpenCV in version 3.1 as a contribute
    module. This was moved to part of OpenCV in 3.3, but it was not widely adopted
    by developers until versions 3.4.3 and 4.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV implements deep learning only for inference, which means that you cannot
    create your own deep learning architecture and train in OpenCV; you can only import
    a pre-trained model, execute it under OpenCV library, and use it as **feedforward**
    (inference) to obtain the results.
  prefs: []
  type: TYPE_NORMAL
- en: The most important reason to implement the feedforward method is to optimize
    OpenCV to speed up computing time and performance in inference. Another reason
    to not implement backward methods is to avoid wasting time developing something
    that other libraries, such as TensorFlow or Caffe, are specialized in. OpenCV
    then created importers for the most important deep learning libraries and frameworks
    to make it possible to import pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then if you wish to create a new deep learning model to use in OpenCV, you
    first have to create and train it using the TensorFlow, Caffe, Torch, or DarkNet
    frameworks or a framework that you can use to export your model in an **Open Neural
    Network Exchange** (**ONNX**) format. Creating a model with this framework can
    be easy or complex depending on the framework you use, but essentially you have
    to stack multiple layers like we did in the previous diagram, setting the parameters
    and the function required by the DNN. Nowadays there are other tools to help you
    to create your models without coding, such as [https://www.tensoreditor.com](https://www.tensoreditor.com)
    or [lobe.ai](https://lobe.ai/). TensorEditor allows you to download the TensorFlow
    code generated from a visual design architecture to train in your computer or
    in the cloud. In the following screenshot, we can see TensorEditor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df035e2c-2f4a-45dc-8a76-e0d649a005e8.png)'
  prefs: []
  type: TYPE_IMG
- en: When you have your model trained and you are comfortable with the results, you
    can import it to OpenCV directly to predict new input images. In the next section,
    you will see how to import and use deep learning models in OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO – real-time object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn how to use deep learning in OpenCV, we are going to present an example
    of object detection and classification based on the YOLO algorithm. This is one
    of the fastest object detection and recognition algorithms, which can run at around
    30 fps in an NVIDIA Titan X.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO v3 deep learning model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common object detection in classical computer vision uses a sliding window to
    detect objects, scanning a whole image with different window sizes and scales.
    The main problem here is the huge time consumption in scanning the image several
    times to find objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLO uses a different approach by dividing the diagram into an S x S grid.
    For each grid, YOLO checks for B bounding boxes, and then the deep learning model
    extracts the bounding boxes for each patch, the confidence to contain a possible
    object, and the confidence of each category in the training dataset per each box.
    The following screenshot shows the S x S grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d372b7ab-1d10-4ca4-b591-82d73bdc8d8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'YOLO is trained with a grid of 19 and 5 bounding boxes per grid using 80 categories.
    Then, the output result is 19 x 19 x 425, where 425 comes from the data of bounding
    box (x, y, width, height), the object confidence, and the 80 classes, confidence
    multiplied by the number of boxes per grid; *5_bounding boxes**(*x*,*y*,*w*,*h*,*object*_*confidence*,
    *classify*_*confidence*[*80*])=*5**(*4* + *1* + *80*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b1e7274-d759-46a3-a7ec-b09c9cffdc9f.png)'
  prefs: []
  type: TYPE_IMG
- en: The YOLO v3 architecture is based on DarkNet, which contains 53 layer networks,
    and YOLO adds 53 more layers for a total of 106 network layers. If you want a
    faster architecture, you can check version 2 or TinyYOLO versions, which use fewer
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO dataset, vocabulary, and model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start to import the model into our OpenCV code, we have to obtain
    it through the YOLO website: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
    This provides pre-trained model files based on the **COCO** dataset, which contains
    80 object categories, such as person, umbrella, bike, motorcycle, car, apple,
    banana, computer, and chair.'
  prefs: []
  type: TYPE_NORMAL
- en: To get all the names of categories and uses for visualization, check out [https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true](https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true).
  prefs: []
  type: TYPE_NORMAL
- en: The names are in the same order as the results of deep learning model confidences.
    If you want to see some images of the COCO dataset by category, you can explore
    the dataset at [http://cocodataset.org/#explore](http://cocodataset.org/#explore),
    and download some of them to test our sample application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the model configuration and pre-trained weights, you have to download
    the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are ready to start to import the models into OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Importing YOLO into OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deep learning OpenCV module is found under the `opencv2/dnn.hpp` header,
    which we have to include in our source header and in `cv::dnn namespace`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then our header for OpenCV must look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we have to do is import the COCO name''s vocabulary, which
    is in the `coco.names` file. This file is a plaintext file that contains one class
    category per line, and is ordered in the same way as the confidence results. Then
    we are going to read each line of this file and store it in a vector of strings,
    called classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are going to import the deep learning model into OpenCV. OpenCV implements
    the most common readers/importers for deep learning frameworks, such as TensorFlow
    and DarkNet, and all of them have a similar syntax. In our case, we are going
    to import a DarkNet model using the weights, and the model using the `readNetFromDarknet`
    OpenCV function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are in a position to read an image and send the deep neural network
    to inference. First we have to read an image with the `imread` function and convert
    it into a tensor/blob data that can read the **DotNetNuke** (**DNN**). To create
    the blob from an image, we are going to use the `blobFromImage` function by passing
    the image. This function accepts the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**image**: Input image (with 1, 3, or 4 channels).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**blob**: Output `mat`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scalefactor**: Multiplier for image values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size**: Spatial size for output blob required as input of DNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mean**: Scalar with mean values that are subtracted from channels. Values
    are intended to be in (mean-R, mean-G, and mean-B) order if the image has BGR
    ordering and `swapRB` is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**swapRB**: A flag that indicates to swap the first and last channels in a
    3-channel image is necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**crop**: A flag that indicates whether the image will be cropped after resize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read the full code on how to read and convert an image into a blob
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have to feed the blob into Deep Net and call the inference with
    the `forward` function, which requires two parameters: the out `mat` results,
    and the names of the layers that the output needs to retrieve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the `mat` output vector, we have all bounding boxes detected by the neural
    network and we have to post-process the output to get only the results that have
    a confidence greater than a threshold, normally 0.5, and finally apply non-maximum
    suppression to eliminate redundant overlapping boxes. You can get the full post-process
    code on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final result of our example is multiple-object detection and classification
    in deep learning that shows a window similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8f14417-bfe1-45c4-8f1b-2b5d5502b3c3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we are going to learn another commonly-used object detection function customized
    for face detection.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection with SSD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Single Shot Detection** (**SSD**) is another fast and accurate deep learning
    object-detection method with a similar concept to YOLO, in which the object and
    bounding box are predicted in the same architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: SSD model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SSD algorithm is called single shot because it predicts the bounding box
    and the class simultaneously as it processes the image in the same deep learning
    model. Basically, the architecture is summarized in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A 300 x 300 image is input into the architecture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input image is passed through multiple convolutional layers, obtaining different
    features at different scales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each feature map obtained in 2, we use a 3 x 3 convolutional filter to evaluate
    small set of default bounding boxes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each default box evaluated, the bounding box offsets and class probabilities
    are predicted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The model architecture looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35723e83-ec4e-46a3-99d6-bd32b4b67a98.png)'
  prefs: []
  type: TYPE_IMG
- en: SSD is used for predicting multiple classes similar to that in YOLO, but it
    can be modified to detect a single object, changing the last layer and training
    for only one class – this is what we used in our example, a re-trained model for
    face detection, where only one class is predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Importing SSD face detection into OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To work with deep learning in our code, we have to import the corresponding
    headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we will import the required namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are going to define the input image size and constant that we are going
    to use in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we need a few parameters as input, such as the model configuration
    and pre-trained model, if we are going to process camera or video input. We also
    need the minimum confidence to accept a prediction as correct or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are going to start with the `main` function, where we are going to
    parse the arguments with the `CommandLineParser` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also going to load the model architecture and pre-trained model files,
    and load the model in a deep learning network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s very important to check that we have imported the network correctly.
    We must also check whether the model is imported, using the `empty` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading our network, we are going to initialize our input source, a camera
    or video file, and load into `VideoCapture`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we are prepared to start capturing frames and processing each one into the
    deep neural network to find faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we have to capture each frame in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will put the input frame into a `Mat` blob structure that can manage
    the deep neural network. We have to send the image with the proper size of SSD,
    which is 300 x 300 (we will have initialized the `inWidth` and `inHeight` constant
    variables already) and we subtract from the input image a mean value, which is
    required in the SSD using the defined `meanVal` constant variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to set the data into the network and get the predictions/detections
    using the `net.setInput` and `net.forward` functions, respectively. This converts
    the detection results into a detection `mat` that we can read, where `detection.size[2]`
    is the number of detected objects and `detection.size[3]` is the number of results
    per detection (bounding box data and confidence):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Mat` detection contains the following per each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Column 0**: Confidence of object being present'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column 1**: Confidence of bounding box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column 2**: Confidence of face detected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column 3**: X bottom-left bounding box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column 4**: Y bottom-left bounding box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column 5**: X top-right bounding box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Column 6**: Y top-right bounding box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bounding box is relative (zero to one) to the image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have to apply the threshold to get only the desired detections based
    on the defined input threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are going to extract the bounding box, draw a rectangle over each detected
    face, and show it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The final result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6abd2d7b-110b-44a2-9507-9d7c07d1e330.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, you learned a new deep learning architecture, SSD, and how
    to use it for face detection.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what deep learning is and how to use it on OpenCV
    with object detection and classification. This chapter is a foundation for working
    with other models and deep neural networks for any purpose.
  prefs: []
  type: TYPE_NORMAL
- en: This book taught you how to obtain and compile OpenCV, how to use the basic
    image and `mat` operations, and how to create your own graphical user interfaces.
    You used basic filters and applied all of them in an industrial inspection example.
    We looked at how to use OpenCV for face detection and how to manipulate it to
    add masks. Finally, we introduced you to very complex use cases of object tracking,
    text segmentation, and recognition. Now you are ready to create your own applications
    in OpenCV, thanks to these use cases, which show you how to apply each technique
    or algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about deep learning in OpenCV, check out *Object Detection and
    Recognition Using Deep Learning in OpenCV* by *Packt Publishing*.
  prefs: []
  type: TYPE_NORMAL
