- en: Deep Learning with OpenCV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV进行深度学习
- en: 'Deep learning is a state-of-the-art form of machine learning that is reaching
    its best accuracy in image classification and speech recognition. Deep learning
    is also used in other fields, such as robotics and artificial intelligence with
    reinforcement learning. This is the main reason OpenCV is making significant efforts
    to include deep learning at its core. We are going to learn the basic use of OpenCV
    deep learning interfaces and look at using them in two use cases: object detection
    and face detection.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一种最先进的形式，在图像分类和语音识别方面正达到其最佳精度。深度学习还应用于其他领域，如机器人和增强学习的人工智能。这正是OpenCV致力于在其核心中包含深度学习的主要原因。我们将学习OpenCV深度学习接口的基本用法，并探讨在以下两个用例中使用它们：目标检测和面部检测。
- en: In this chapter, we are going to learn the basics of deep learning and see how
    to use it in OpenCV. To reach our objective, we are going to learn object detection
    and classification using the **you only look once **(**YOLO**) algorithm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习深度学习的基础知识，并了解如何在OpenCV中使用它。为了达到我们的目标，我们将学习使用**你只看一次**（**YOLO**）算法进行目标检测和分类。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is deep learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: How OpenCV works with deep learning and implementing deep learning **neural
    networks**(**NN**s)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV如何与深度学习协同工作以及实现深度学习**神经网络**（**NN**s）
- en: YOLO – a very fast deep learning object detection algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO - 一种非常快的深度学习目标检测算法
- en: Face detection using Single Shot Detector
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单次检测器进行面部检测
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow the chapter with ease, it is required that you install OpenCV with
    the deep learning module compiled. If you do not have this module, you will not
    be able to compile and run the sample codes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松跟随本章内容，您需要安装已编译深度学习模块的OpenCV。如果您没有这个模块，您将无法编译和运行示例代码。
- en: It's very useful to have an NVIDIA GPU with CUDA support. You can enable CUDA
    on OpenCV to improve the speed of training and detection.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个具有CUDA支持的NVIDIA GPU非常有用。您可以在OpenCV上启用CUDA以提高训练和检测的速度。
- en: Finally, you can download the code used in this chapter from [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以从此处下载本章使用的代码：[https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_12)。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码的实际应用：
- en: '[http://bit.ly/2SmbWf7](http://bit.ly/2SmbWf7)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2SmbWf7](http://bit.ly/2SmbWf7)'
- en: Introduction to deep learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Deep learning is most commonly written about in scientific papers nowadays with
    regards to image classification and speech recognition. This is a subfield of
    machine learning, based on traditional neural networks and inspired by the structure
    of the brain. To understand this technology, it is very important to understand
    what a neural network is and how it works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于图像分类和语音识别，深度学习通常在科学论文中讨论。这是一个基于传统神经网络并受到大脑结构启发的机器学习子领域。为了理解这项技术，了解神经网络是什么以及它是如何工作的非常重要。
- en: What is a neural network and how can we learn from data?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络是什么？我们如何从数据中学习？
- en: The neural network is inspired by the structure of the brain, in which multiple
    neurons are interconnected, creating a network. Each neuron has multiple inputs
    and multiple outputs, like a biological neuron.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络受到大脑结构的启发，其中多个神经元相互连接，形成一个网络。每个神经元有多个输入和多个输出，就像生物神经元一样。
- en: 'This network is distributed in layers, and each layer contains a number of
    neurons that are connected to all the previous layer''s neurons. This always has
    an input layer, which normally consists of the features that describe the input
    image or data, and an output layer, which normally consists of the result of our
    classification. The other middle layers are called **hidden layers**. The following
    diagram shows a basic three-layer neural network in which the input layer contains
    three neurons, the output layer contains two neurons, and a hidden layer contains
    four neurons:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络是分层分布的，每一层都包含一些与前一层的所有神经元相连的神经元。它总是有一个输入层，通常由描述输入图像或数据的特征组成，以及一个输出层，通常由我们的分类结果组成。其他中间层被称为**隐藏层**。以下图展示了一个基本的具有三层神经网络的示例，其中输入层包含三个神经元，输出层包含两个神经元，隐藏层包含四个神经元：
- en: '![](img/f5ed96eb-8a0a-4957-adf4-b31b8e65075d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f5ed96eb-8a0a-4957-adf4-b31b8e65075d.png)'
- en: 'The neuron is the basic element of a neural network and it uses a simple mathematical
    formula that we can see in the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是神经网络的基本元素，它使用一个简单的数学公式，我们可以在以下图中看到：
- en: '![](img/16287cff-6b21-429b-b6af-74a22e2719db.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/16287cff-6b21-429b-b6af-74a22e2719db.png)'
- en: 'As we can see, for each neuron, **i**, we mathematically add all the previous
    neuron''s output, which is the input of neuron **i** (**x1**, **x2**...), by a
    weight (**wi1**, **wi2**...) plus a bias value, and the result is the argument
    of an activation function, **f**. The final result is the output of **i** neuron:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对于每个神经元，**i**，我们通过权重（**wi1**，**wi2**...）将所有前一个神经元的输出（即神经元**i**的输入**x1**，**x2**...）相加，并加上一个偏置值，然后将结果作为激活函数**f**的参数。最终结果是**i**神经元的输出：
- en: '![](img/2d237c49-7c71-4b51-8d9f-e550d882ccca.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2d237c49-7c71-4b51-8d9f-e550d882ccca.png)'
- en: 'The most common activation functions (**f**) on classical neural networks are
    the sigmoid function or linear functions. The sigmoid function is used most often,
    and it looks as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典神经网络中最常见的激活函数（**f**）是Sigmoid函数或线性函数。Sigmoid函数使用得最为频繁，其形式如下：
- en: '![](img/ad7a4ce4-bf8b-410f-beb6-d196b485d28b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad7a4ce4-bf8b-410f-beb6-d196b485d28b.png)'
- en: But how can we learn a neural network with this formula and these connections?
    How do we classify input data? The learn algorithm of neural networks can be called
    **supervised** if we know the desired output; while learning, the input pattern
    is given to the net's input layer. Initially, we set up all weights as random
    numbers and send the input features into the network, checking the output result.
    If this is wrong, we have to adjust all the weights of the network to get the
    correct output. This algorithm is called **backpropagation**. If you want to read
    more about how a neural network learns, check out [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)
    and [https://youtu.be/IHZwWFHWa-w](https://youtu.be/IHZwWFHWa-w).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何通过这个公式和这些连接来学习神经网络？我们如何对输入数据进行分类？如果我们知道期望的输出，神经网络的学习算法可以称为**监督学习**；在学习过程中，输入模式被提供给网络的输入层。最初，我们将所有权重设置为随机数，并将输入特征送入网络，检查输出结果。如果这是错误的，我们必须调整网络的所有权重以获得正确的输出。这个算法被称为**反向传播**。如果您想了解更多关于神经网络如何学习的信息，请参阅[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)和[https://youtu.be/IHZwWFHWa-w](https://youtu.be/IHZwWFHWa-w)。
- en: Now that we have a brief introduction to what a neural network is and the internal
    architecture of NN, we are going to explore the differences between NN and deep
    learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对神经网络及其内部架构有了简要的介绍，我们将探讨神经网络与深度学习之间的区别。
- en: Convolutional neural networks
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Deep learning neural networks have the same background as the classical neural
    network. However, in the case of image analysis, the main difference is the input
    layer. In a classical machine learning algorithm, the researcher has to identify
    the best features that define the image target to classify. For example, if we
    want to classify numbers, we could extract the borders and lines of numbers in
    each image, measure the area of an object in an image, and all of these features
    are the input of the neural network, or any other machine learning algorithm.
    However, in deep learning, you don't have to explore what the features are; instead,
    you use whole image as an input of the neural network directly. Deep learning
    can learn what the most important features are and **deep neural networks** (**DNN**)
    are able to detect an image or input and recognize it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习神经网络与经典神经网络有相同的背景。然而，在图像分析的情况下，主要区别在于输入层。在经典机器学习算法中，研究人员必须识别出定义图像目标的最佳特征来进行分类。例如，如果我们想要对数字进行分类，我们可以从每个图像中提取数字的边缘和线条，测量图像中对象的面积，所有这些特征都是神经网络的输入，或者任何其他机器学习算法的输入。然而，在深度学习中，你不必探索特征是什么；相反，你直接将整个图像作为神经网络的输入。深度学习可以学习最重要的特征，**深度神经网络**（**DNN**）能够检测图像或输入并识别它。
- en: 'To learn what these features are, we use one of the most important layers in
    deep learning and neural networks: the **convolutional layer**. A convolutional
    layer works like a convolutional operator, where a kernel filter is applied to
    the whole previous layer, giving us a new filtered image, like a sobel operator:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这些特征是什么，我们使用深度学习和神经网络中最重要的一层：**卷积层**。卷积层的工作原理类似于卷积算子，其中核滤波器应用于整个前一层，为我们提供一个新的滤波图像，就像sobel算子：
- en: '![](img/98fbb3b3-3557-40ed-9dd4-0805262af590.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/98fbb3b3-3557-40ed-9dd4-0805262af590.png)'
- en: 'However, in a convolutional layer we can define different parameters, and one
    of them is the number of filters and the sizes we want to apply to the previous
    layer or image. These filters are calculated in the learning step, just like the
    weights on a classical neural network. This is the magic of deep learning: it
    can extract the most significant features from labeled images.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在卷积层中，我们可以定义不同的参数，其中之一是应用于前一层或图像的滤波器数量和大小。这些滤波器在学习步骤中计算，就像经典神经网络中的权重一样。这是深度学习的魔力：它可以从标记图像中提取最显著的特征。
- en: However, these convolutional layers are the main reason behind the name **deep**,
    and we are going to see why in the following basic example. Imagine we have a
    100 x 100 image. In a classical neural network, we will extract the most relevant
    features we can imagine from the input image. This will normally approximately
    1,000 features, and with each hidden layer we can increase or decrease this number,
    but the number of neurons to calculate its weights is reasonable to compute in
    a normal computer. However, in deep learning, we normally start applying a convolutional
    layer – with a 64 filter kernels of 3 x 3 size. This will generate a new layer
    of 100 x 100 x 64 neurons with 3 x 3 x 64 weights to calculate. If we continue
    adding more and more layers, these numbers quickly increase and require huge computing
    power to learn the good weights and parameters of our deep learning architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些卷积层是“深度”名称背后的主要原因，我们将在以下基本示例中看到原因。想象一下，我们有一个100 x 100的图像。在经典神经网络中，我们将从输入图像中提取我们可以想象的最相关特征。这通常大约是1,000个特征，并且随着每个隐藏层的增加或减少，这个数字可以增加或减少，但计算其权重的神经元数量在普通计算机中是合理的。然而，在深度学习中，我们通常开始应用一个卷积层——带有64个3
    x 3大小的滤波器核。这将生成一个包含100 x 100 x 64个神经元的新层，需要计算3 x 3 x 64个权重。如果我们继续添加更多层，这些数字会迅速增加，需要巨大的计算能力来学习我们深度学习架构的良好权重和参数。
- en: 'Convolutional layers are one of the most important aspects of the deep learning
    architecture, but there are also other important layers, such as **Pooling**,
    **Dropout**, **Flatten**, and **Softmax**. In the following diagram, we can see
    a basic deep learning architecture in which some convolutional and pooling layers
    are stacked:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是深度学习架构中最重要方面之一，但还有其他重要的层，例如**池化**、**dropout**、**展平**和**softmax**。在下面的图中，我们可以看到一个基本的深度学习架构，其中堆叠了一些卷积和池化层：
- en: '![](img/4aaa0318-f894-4bfb-8b1f-f52c66642517.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4aaa0318-f894-4bfb-8b1f-f52c66642517.png)'
- en: 'However, there is one more very important thing that makes deep learning get
    the best results: the amount of labeled data. If you have a small dataset, a deep
    learning algorithm will not help you in your classification because there is not
    enough data to learn the features (the weights and parameters of your deep learning
    architecture). However, if you have tons of data, you will get very good results.
    But take care, you will need a lot of time to compute and learn the weights and
    parameters of your architecture. This is why deep learning was not used early
    in the process, because computing requires a lot of time. However, thanks to new
    parallel architectures, such as NVIDIA GPUs, we can optimize the learning backpropagation
    and speed up the learning tasks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一件非常重要的事情使得深度学习能够取得最佳结果：标注数据的数量。如果你有一个小数据集，深度学习算法将无法帮助你进行分类，因为学习特征（你的深度学习架构的权重和参数）的数据不足。然而，如果你有大量的数据，你会得到非常好的结果。但请注意，你需要大量的时间来计算和学习你架构的权重和参数。这就是为什么深度学习在早期过程中没有被使用，因为计算需要大量的时间。然而，多亏了新的并行架构，例如NVIDIA
    GPU，我们可以优化学习反向传播并加快学习任务。
- en: Deep learning in OpenCV
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenCV中的深度学习
- en: The deep learning module was introduced to OpenCV in version 3.1 as a contribute
    module. This was moved to part of OpenCV in 3.3, but it was not widely adopted
    by developers until versions 3.4.3 and 4.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模块在OpenCV 3.1版本中被引入作为一个贡献模块。这被移动到OpenCV的一部分在3.3版本中，但它直到版本3.4.3和4才被开发者广泛采用。
- en: OpenCV implements deep learning only for inference, which means that you cannot
    create your own deep learning architecture and train in OpenCV; you can only import
    a pre-trained model, execute it under OpenCV library, and use it as **feedforward**
    (inference) to obtain the results.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV仅实现深度学习用于推理，这意味着你不能在OpenCV中创建自己的深度学习架构进行训练；你只能导入一个预训练模型，在OpenCV库下执行它，并使用它作为**前馈**（推理）来获得结果。
- en: The most important reason to implement the feedforward method is to optimize
    OpenCV to speed up computing time and performance in inference. Another reason
    to not implement backward methods is to avoid wasting time developing something
    that other libraries, such as TensorFlow or Caffe, are specialized in. OpenCV
    then created importers for the most important deep learning libraries and frameworks
    to make it possible to import pre-trained models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实现前馈方法最重要的原因是优化OpenCV以加快计算时间和推理性能。不实现反向方法的其他原因是为了避免浪费时间开发其他库，如TensorFlow或Caffe，已经专业化的东西。因此，OpenCV创建了导入最重要的深度学习库和框架的导入器，以便能够导入预训练模型。
- en: 'Then if you wish to create a new deep learning model to use in OpenCV, you
    first have to create and train it using the TensorFlow, Caffe, Torch, or DarkNet
    frameworks or a framework that you can use to export your model in an **Open Neural
    Network Exchange** (**ONNX**) format. Creating a model with this framework can
    be easy or complex depending on the framework you use, but essentially you have
    to stack multiple layers like we did in the previous diagram, setting the parameters
    and the function required by the DNN. Nowadays there are other tools to help you
    to create your models without coding, such as [https://www.tensoreditor.com](https://www.tensoreditor.com)
    or [lobe.ai](https://lobe.ai/). TensorEditor allows you to download the TensorFlow
    code generated from a visual design architecture to train in your computer or
    in the cloud. In the following screenshot, we can see TensorEditor:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果你想在OpenCV中使用新的深度学习模型，你首先必须使用TensorFlow、Caffe、Torch、DarkNet框架或可以用来导出你模型为**开放神经网络交换**（**ONNX**）格式的框架来创建和训练它。使用这个框架创建模型可能很简单或很复杂，这取决于你使用的框架，但本质上你必须堆叠多个层，就像我们在之前的图中做的那样，设置DNN所需的参数和函数。如今，有其他工具可以帮助你创建模型而无需编码，例如[https://www.tensoreditor.com](https://www.tensoreditor.com)或[lobe.ai](https://lobe.ai/)。TensorEditor允许你下载从可视化设计架构生成的TensorFlow代码，以便在计算机或云中训练。在下面的屏幕截图中，我们可以看到TensorEditor：
- en: '![](img/df035e2c-2f4a-45dc-8a76-e0d649a005e8.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/df035e2c-2f4a-45dc-8a76-e0d649a005e8.png)'
- en: When you have your model trained and you are comfortable with the results, you
    can import it to OpenCV directly to predict new input images. In the next section,
    you will see how to import and use deep learning models in OpenCV.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型训练完成并且你对结果感到满意时，你可以直接将其导入OpenCV以预测新的输入图像。在下一节中，你将看到如何在OpenCV中导入和使用深度学习模型。
- en: YOLO – real-time object detection
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO – 实时目标检测
- en: To learn how to use deep learning in OpenCV, we are going to present an example
    of object detection and classification based on the YOLO algorithm. This is one
    of the fastest object detection and recognition algorithms, which can run at around
    30 fps in an NVIDIA Titan X.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何在OpenCV中使用深度学习，我们将展示一个基于YOLO算法的对象检测和分类的示例。这是最快的目标检测和识别算法之一，在NVIDIA Titan
    X上大约可以以30 fps的速度运行。
- en: YOLO v3 deep learning model architecture
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO v3深度学习模型架构
- en: Common object detection in classical computer vision uses a sliding window to
    detect objects, scanning a whole image with different window sizes and scales.
    The main problem here is the huge time consumption in scanning the image several
    times to find objects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典计算机视觉中，常见的对象检测使用滑动窗口来检测对象，通过不同窗口大小和比例扫描整个图像。这里的主要问题是扫描图像多次以找到对象所消耗的大量时间。
- en: 'YOLO uses a different approach by dividing the diagram into an S x S grid.
    For each grid, YOLO checks for B bounding boxes, and then the deep learning model
    extracts the bounding boxes for each patch, the confidence to contain a possible
    object, and the confidence of each category in the training dataset per each box.
    The following screenshot shows the S x S grid:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO通过将图划分为S x S网格来采用不同的方法。对于每个网格，YOLO检查B个边界框，然后深度学习模型提取每个补丁的边界框、包含可能对象的置信度以及每个边界框在训练数据集中每个类别的置信度。以下截图显示了S
    x S网格：
- en: '![](img/d372b7ab-1d10-4ca4-b591-82d73bdc8d8a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d372b7ab-1d10-4ca4-b591-82d73bdc8d8a.png)'
- en: 'YOLO is trained with a grid of 19 and 5 bounding boxes per grid using 80 categories.
    Then, the output result is 19 x 19 x 425, where 425 comes from the data of bounding
    box (x, y, width, height), the object confidence, and the 80 classes, confidence
    multiplied by the number of boxes per grid; *5_bounding boxes**(*x*,*y*,*w*,*h*,*object*_*confidence*,
    *classify*_*confidence*[*80*])=*5**(*4* + *1* + *80*):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'YOLO使用每个网格19个和5个边界框进行训练，共80个类别。然后，输出结果为19 x 19 x 425，其中425来自边界框（x, y, 宽度，高度）、对象置信度和80个类别的置信度乘以每个网格的边界框数量；*5_bounding
    boxes**(*x*,*y*,*w*,*h*,*object*_*confidence*, *classify*_*confidence*[*80*])=*5**(*4*
    + *1* + *80*):'
- en: '![](img/5b1e7274-d759-46a3-a7ec-b09c9cffdc9f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b1e7274-d759-46a3-a7ec-b09c9cffdc9f.png)'
- en: The YOLO v3 architecture is based on DarkNet, which contains 53 layer networks,
    and YOLO adds 53 more layers for a total of 106 network layers. If you want a
    faster architecture, you can check version 2 or TinyYOLO versions, which use fewer
    layers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO v3架构基于DarkNet，包含53层网络，YOLO再增加53层，总共106层网络层。如果您需要一个更快的架构，可以查看版本2或TinyYOLO版本，它们使用更少的层。
- en: The YOLO dataset, vocabulary, and model
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOLO数据集、词汇表和模型
- en: 'Before we start to import the model into our OpenCV code, we have to obtain
    it through the YOLO website: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
    This provides pre-trained model files based on the **COCO** dataset, which contains
    80 object categories, such as person, umbrella, bike, motorcycle, car, apple,
    banana, computer, and chair.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将模型导入OpenCV代码之前，我们必须通过YOLO网站获取它：[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)。该网站提供了基于**COCO**数据集的预训练模型文件，该数据集包含80个对象类别，例如人、雨伞、自行车、摩托车、汽车、苹果、香蕉、计算机和椅子。
- en: To get all the names of categories and uses for visualization, check out [https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true](https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取所有类别的名称和用于可视化的用途，请查看[https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true](https://github.com/pjreddie/darknet/blob/master/data/coco.names?raw=true)。
- en: The names are in the same order as the results of deep learning model confidences.
    If you want to see some images of the COCO dataset by category, you can explore
    the dataset at [http://cocodataset.org/#explore](http://cocodataset.org/#explore),
    and download some of them to test our sample application.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 名称的顺序与深度学习模型置信度的结果相同。如果您想通过类别查看COCO数据集的一些图像，可以探索数据集[http://cocodataset.org/#explore](http://cocodataset.org/#explore)，并下载其中一些来测试我们的示例应用程序。
- en: 'To get the model configuration and pre-trained weights, you have to download
    the following files:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取模型配置和预训练权重，您必须下载以下文件：
- en: '[https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)'
- en: '[https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg?raw=true)'
- en: Now we are ready to start to import the models into OpenCV.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始将模型导入到OpenCV中。
- en: Importing YOLO into OpenCV
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将YOLO导入到OpenCV
- en: The deep learning OpenCV module is found under the `opencv2/dnn.hpp` header,
    which we have to include in our source header and in `cv::dnn namespace`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习OpenCV模块位于`opencv2/dnn.hpp`头文件下，我们必须将其包含在我们的源头文件和`cv::dnn命名空间`中。
- en: 'Then our header for OpenCV must look like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的OpenCV头文件必须看起来像这样：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first thing we have to do is import the COCO name''s vocabulary, which
    is in the `coco.names` file. This file is a plaintext file that contains one class
    category per line, and is ordered in the same way as the confidence results. Then
    we are going to read each line of this file and store it in a vector of strings,
    called classes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是导入COCO名称的词汇表，该词汇表位于`coco.names`文件中。此文件是一个纯文本文件，每行包含一个类别，并且按照置信度结果的相同方式排序。然后我们将读取此文件的每一行，并将其存储在一个名为`classes`的字符串向量中：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we are going to import the deep learning model into OpenCV. OpenCV implements
    the most common readers/importers for deep learning frameworks, such as TensorFlow
    and DarkNet, and all of them have a similar syntax. In our case, we are going
    to import a DarkNet model using the weights, and the model using the `readNetFromDarknet`
    OpenCV function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将导入深度学习模型到OpenCV。OpenCV实现了深度学习框架（如TensorFlow和DarkNet）最常用的读取器/导入器，并且它们都有类似的语法。在我们的案例中，我们将使用权重导入DarkNet模型，并使用`readNetFromDarknet`
    OpenCV函数：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we are in a position to read an image and send the deep neural network
    to inference. First we have to read an image with the `imread` function and convert
    it into a tensor/blob data that can read the **DotNetNuke** (**DNN**). To create
    the blob from an image, we are going to use the `blobFromImage` function by passing
    the image. This function accepts the following parameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处于读取图像并将深度神经网络发送到推理的位置。首先，我们必须使用`imread`函数读取一个图像，并将其转换为可以读取**DotNetNuke**（**DNN**）的张量/数据块。为了从图像创建数据块，我们将使用`blobFromImage`函数，通过传递图像。此函数接受以下参数：
- en: '**image**: Input image (with 1, 3, or 4 channels).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**image**：输入图像（具有1、3或4个通道）。'
- en: '**blob**: Output `mat`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**blob**：输出`mat`。'
- en: '**scalefactor**: Multiplier for image values.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scalefactor**：图像值的乘数。'
- en: '**size**: Spatial size for output blob required as input of DNN.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**size**：输出数据块所需的空间大小，作为DNN的输入。'
- en: '**mean**: Scalar with mean values that are subtracted from channels. Values
    are intended to be in (mean-R, mean-G, and mean-B) order if the image has BGR
    ordering and `swapRB` is true.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mean**：从通道中减去的标量，其值旨在（mean-R、mean-G和mean-B）的顺序，如果图像具有BGR排序且`swapRB`为真。'
- en: '**swapRB**: A flag that indicates to swap the first and last channels in a
    3-channel image is necessary.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**swapRB**：一个标志，表示在3通道图像中交换第一个和最后一个通道是必要的。'
- en: '**crop**: A flag that indicates whether the image will be cropped after resize.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**crop**：一个标志，表示图像在调整大小后是否将被裁剪。'
- en: 'You can read the full code on how to read and convert an image into a blob
    in the following snippet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下代码片段中阅读如何读取和将图像转换为数据块的完整代码。
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we have to feed the blob into Deep Net and call the inference with
    the `forward` function, which requires two parameters: the out `mat` results,
    and the names of the layers that the output needs to retrieve:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须将数据块喂入深度网络，并使用`forward`函数进行推理，该函数需要两个参数：输出的`mat`结果和需要检索的层的名称：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the `mat` output vector, we have all bounding boxes detected by the neural
    network and we have to post-process the output to get only the results that have
    a confidence greater than a threshold, normally 0.5, and finally apply non-maximum
    suppression to eliminate redundant overlapping boxes. You can get the full post-process
    code on GitHub.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mat`输出向量中，我们包含了神经网络检测到的所有边界框，并且我们必须后处理输出，以获取置信度大于阈值的仅结果，通常为0.5，最后应用非极大值抑制来消除冗余重叠的框。你可以在GitHub上找到完整的后处理代码。
- en: 'The final result of our example is multiple-object detection and classification
    in deep learning that shows a window similar to the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例的最终结果是深度学习中的多目标检测和分类，显示的窗口类似于以下内容：
- en: '![](img/e8f14417-bfe1-45c4-8f1b-2b5d5502b3c3.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8f14417-bfe1-45c4-8f1b-2b5d5502b3c3.jpg)'
- en: Now we are going to learn another commonly-used object detection function customized
    for face detection.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习另一个针对人脸检测定制的常用目标检测函数。
- en: Face detection with SSD
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SSD进行人脸检测
- en: '**Single Shot Detection** (**SSD**) is another fast and accurate deep learning
    object-detection method with a similar concept to YOLO, in which the object and
    bounding box are predicted in the same architecture.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**单次检测**（**SSD**）是另一种快速且准确的深度学习目标检测方法，其概念与YOLO类似，在该方法中，对象和边界框在相同的架构中被预测。'
- en: SSD model architecture
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SSD模型架构
- en: 'The SSD algorithm is called single shot because it predicts the bounding box
    and the class simultaneously as it processes the image in the same deep learning
    model. Basically, the architecture is summarized in the following steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SSD算法被称为单次检测，因为它在处理图像时，在相同的深度学习模型中同时预测边界框和类别。基本上，架构可以总结为以下步骤：
- en: A 300 x 300 image is input into the architecture.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个300 x 300的图像被输入到架构中。
- en: The input image is passed through multiple convolutional layers, obtaining different
    features at different scales.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入图像通过多个卷积层，在不同的尺度上获得不同的特征。
- en: For each feature map obtained in 2, we use a 3 x 3 convolutional filter to evaluate
    small set of default bounding boxes.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于在步骤2中获得的每个特征图，我们使用一个3 x 3的卷积滤波器来评估一组默认边界框。
- en: For each default box evaluated, the bounding box offsets and class probabilities
    are predicted.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个评估的默认框，将预测边界框偏移量和类别概率。
- en: 'The model architecture looks like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构看起来如下：
- en: '![](img/35723e83-ec4e-46a3-99d6-bd32b4b67a98.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35723e83-ec4e-46a3-99d6-bd32b4b67a98.png)'
- en: SSD is used for predicting multiple classes similar to that in YOLO, but it
    can be modified to detect a single object, changing the last layer and training
    for only one class – this is what we used in our example, a re-trained model for
    face detection, where only one class is predicted.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SSD用于预测与YOLO类似的多类，但它可以被修改为检测单个对象，这需要更改最后一层并仅针对一个类别进行训练——这就是我们在示例中使用的内容，一个用于人脸检测的重新训练模型，其中只预测一个类别。
- en: Importing SSD face detection into OpenCV
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将SSD人脸检测导入OpenCV
- en: 'To work with deep learning in our code, we have to import the corresponding
    headers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的代码中使用深度学习，我们必须导入相应的头文件：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After that, we will import the required namespaces:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将导入所需的命名空间：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we are going to define the input image size and constant that we are going
    to use in our code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义我们将要在代码中使用的目标图像大小和常量：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In this example, we need a few parameters as input, such as the model configuration
    and pre-trained model, if we are going to process camera or video input. We also
    need the minimum confidence to accept a prediction as correct or not:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们需要一些参数作为输入，例如模型配置和预训练模型，如果我们将要处理相机或视频输入。我们还需要确定接受预测为正确或错误的最低置信度：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we are going to start with the `main` function, where we are going to
    parse the arguments with the `CommandLineParser` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从`main`函数开始，我们将使用`CommandLineParser`函数解析参数：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are also going to load the model architecture and pre-trained model files,
    and load the model in a deep learning network:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将加载模型架构和预训练模型文件，并在深度学习网络中加载模型：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It''s very important to check that we have imported the network correctly.
    We must also check whether the model is imported, using the `empty` function,
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 检查我们是否正确导入了网络非常重要。我们还必须检查是否导入了模型，使用`empty`函数，如下所示：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After loading our network, we are going to initialize our input source, a camera
    or video file, and load into `VideoCapture`, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载我们的网络后，我们将初始化我们的输入源，一个摄像头或视频文件，并将其加载到`VideoCapture`中，如下所示：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now we are prepared to start capturing frames and processing each one into the
    deep neural network to find faces.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始捕获帧并将每个帧处理成深度神经网络以找到人脸。
- en: 'First of all, we have to capture each frame in a loop:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须在循环中捕获每个帧：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will put the input frame into a `Mat` blob structure that can manage
    the deep neural network. We have to send the image with the proper size of SSD,
    which is 300 x 300 (we will have initialized the `inWidth` and `inHeight` constant
    variables already) and we subtract from the input image a mean value, which is
    required in the SSD using the defined `meanVal` constant variable:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输入帧放入一个可以管理深度神经网络的`Mat` blob结构中。我们必须发送具有SSD正确尺寸的图像，即300 x 300（我们已初始化`inWidth`和`inHeight`常量变量），并从输入图像中减去所需的均值值，这是在SSD中使用定义的`meanVal`常量变量：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we are ready to set the data into the network and get the predictions/detections
    using the `net.setInput` and `net.forward` functions, respectively. This converts
    the detection results into a detection `mat` that we can read, where `detection.size[2]`
    is the number of detected objects and `detection.size[3]` is the number of results
    per detection (bounding box data and confidence):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好将数据放入网络，并使用 `net.setInput` 和 `net.forward` 函数分别获取预测/检测结果。这会将检测结果转换为可读取的检测
    `mat`，其中 `detection.size[2]` 是检测到的对象数量，`detection.size[3]` 是每个检测的结果数量（边界框数据和置信度）：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `Mat` detection contains the following per each row:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每行 `Mat` 检测包含以下内容：
- en: '**Column 0**: Confidence of object being present'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 0**：存在对象的置信度'
- en: '**Column 1**: Confidence of bounding box'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 1**：边界框的置信度'
- en: '**Column 2**: Confidence of face detected'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 2**：检测到的面部置信度'
- en: '**Column 3**: X bottom-left bounding box'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 3**：X 底部左边界框'
- en: '**Column 4**: Y bottom-left bounding box'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 4**：Y 底部左边界框'
- en: '**Column 5**: X top-right bounding box'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 5**：X 顶部右边界框'
- en: '**Column 6**: Y top-right bounding box'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列 6**：Y 顶部右边界框'
- en: The bounding box is relative (zero to one) to the image size.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框相对于图像大小是相对的（从零到一）。
- en: 'Now we have to apply the threshold to get only the desired detections based
    on the defined input threshold:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须应用阈值，仅基于定义的输入阈值获取所需的检测：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we are going to extract the bounding box, draw a rectangle over each detected
    face, and show it as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将提取边界框，在每个检测到的面部上画一个矩形，并如下所示显示：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The final result looks like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果看起来像这样：
- en: '![](img/6abd2d7b-110b-44a2-9507-9d7c07d1e330.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6abd2d7b-110b-44a2-9507-9d7c07d1e330.png)'
- en: In this section, you learned a new deep learning architecture, SSD, and how
    to use it for face detection.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了一种新的深度学习架构 SSD，以及如何使用它进行面部检测。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned what deep learning is and how to use it on OpenCV
    with object detection and classification. This chapter is a foundation for working
    with other models and deep neural networks for any purpose.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了什么是深度学习以及如何在 OpenCV 中使用它进行目标检测和分类。本章是使用其他模型和深度神经网络进行任何目的工作的基础。
- en: This book taught you how to obtain and compile OpenCV, how to use the basic
    image and `mat` operations, and how to create your own graphical user interfaces.
    You used basic filters and applied all of them in an industrial inspection example.
    We looked at how to use OpenCV for face detection and how to manipulate it to
    add masks. Finally, we introduced you to very complex use cases of object tracking,
    text segmentation, and recognition. Now you are ready to create your own applications
    in OpenCV, thanks to these use cases, which show you how to apply each technique
    or algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本书教你如何获取和编译 OpenCV，如何使用基本的图像和 `mat` 操作，以及如何创建你自己的图形用户界面。你使用了基本的过滤器，并在工业检测示例中应用了所有这些。我们探讨了如何使用
    OpenCV 进行面部检测以及如何对其进行操作以添加面具。最后，我们介绍了对象跟踪、文本分割和识别的非常复杂的使用案例。现在，你已准备好利用这些用例创建自己的
    OpenCV 应用程序，这些用例展示了如何应用每个技术或算法。
- en: Further reading
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To learn more about deep learning in OpenCV, check out *Object Detection and
    Recognition Using Deep Learning in OpenCV* by *Packt Publishing*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 OpenCV 中深度学习的信息，请查看由 *Packt Publishing* 出版的 *《OpenCV 中的深度学习目标检测与识别》*。
