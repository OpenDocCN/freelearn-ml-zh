- en: '*Chapter 9*: Implementing a Batch Scoring Solution'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：实现批量评分解决方案'
- en: You have trained regression, classification, and forecasting models with AutoML
    in Azure, and now it's time you learn how to put them in production and use them.
    **Machine learning** (**ML**) models, after all, are ultimately used to make predictions
    on new data, either in real time or in batches. In order to score new data points
    in batches in Azure, you must first create an ML pipeline.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在Azure中用AutoML训练了回归、分类和预测模型，现在是时候学习如何将它们投入生产并使用了。毕竟，**机器学习**（**ML**）模型最终是用来对新数据进行预测的，无论是在实时还是在批量中。为了在Azure中批量评分新数据点，你必须首先创建一个机器学习管道。
- en: An ML pipeline lets you run repeatable Python code in the **Azure Machine Learning
    services** (**AMLS**) that you can run on a schedule. While you can run any Python
    code using an ML pipeline, here you will learn how to build pipelines for scoring
    new data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道允许你在**Azure机器学习服务**（**AMLS**）中运行可重复的Python代码，你可以在计划中运行这些代码。虽然你可以使用机器学习管道运行任何Python代码，但在这里，你将学习如何构建用于评分新数据的管道。
- en: You will begin this chapter by writing a simple ML pipeline to score data using
    the multiclass classification model you trained on the Iris dataset in [*Chapter
    5*](B16595_05_ePub.xhtml#_idTextAnchor068), *Building an AutoML Classification
    Solution*. Using the same data, you will then learn how to score new data points
    in parallel, enabling you to quickly score models with millions to billions of
    data points simultaneously.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从此章节开始，编写一个简单的机器学习管道，使用在[*第5章*](B16595_05_ePub.xhtml#_idTextAnchor068)“构建AutoML分类解决方案”中训练的Iris数据集的多分类模型来评分数据。使用相同的数据，你将学习如何并行评分新的数据点，从而让你能够快速同时评分包含数百万到数十亿数据点的模型。
- en: Once you have written these two pipelines, you will learn how to create an ML
    pipeline for retraining an AutoML model. Finally, you will learn how to retrigger
    ML pipelines both manually through the GUI and programmatically through the **Azure
    ML SDK**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦编写了这两个管道，你将学习如何创建用于重新训练AutoML模型的机器学习管道。最后，你将学习如何通过GUI手动和通过**Azure ML SDK**编程方式重新触发机器学习管道。
- en: By the end of this chapter, you will be able to not only train AutoML models
    but also use them to score new data in a reproducible, automatable fashion. Furthermore,
    the code and techniques you learn here apply to all ML models, not just those
    that are AutoML-generated. Coding batch scoring solutions is a key skill for any
    ML engineer, and by working through the exercises in this chapter, you will be
    well on your way to mastering that skill.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你不仅能够训练AutoML模型，而且能够以可重复、可自动化的方式使用它们来评分新数据。此外，你在这里学到的代码和技术适用于所有机器学习模型，而不仅仅是AutoML生成的模型。编写批量评分解决方案是任何机器学习工程师的关键技能，通过完成本章的练习，你将朝着掌握这一技能迈出重要一步。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Creating an ML pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建机器学习管道
- en: Creating a parallel scoring pipeline
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并行评分管道
- en: Creating an AutoML training pipeline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建AutoML训练管道
- en: Triggering and scheduling your ML pipelines
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 触发和安排你的机器学习管道
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will feature a lot of coding using Jupyter notebooks within AMLS.
    Thus, you will need a working internet connection, an **AMLS workspace**, and
    a **compute instance**. ML pipelines also require a **compute cluster**. You will
    also need to have trained and registered the Iris multiclass classification model
    in [*Chapter 5*](B16595_05_ePub.xhtml#_idTextAnchor068), *Building an AutoML Classification
    Solution*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用AMLS中的Jupyter笔记本进行大量编码。因此，你需要一个有效的互联网连接、一个**AMLS工作空间**和一个**计算实例**。机器学习管道还需要一个**计算集群**。你还需要在[*第5章*](B16595_05_ePub.xhtml#_idTextAnchor068)“构建AutoML分类解决方案”中训练并注册Iris多分类模型。
- en: 'The following are the prerequisites for the chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的先决条件如下：
- en: Access to the internet.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问互联网。
- en: A web browser, preferably Google Chrome or Microsoft Edge Chromium.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器，最好是Google Chrome或Microsoft Edge Chromium。
- en: A Microsoft Azure account.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Microsoft Azure账户。
- en: Have created an AMLS workspace.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已创建一个AMLS工作空间。
- en: Have created the `compute-cluster` compute cluster in [*Chapter 2*](B16595_02_ePub.xhtml#_idTextAnchor023),
    *Getting Started with Azure Machine Learning Service*.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[*第2章*](B16595_02_ePub.xhtml#_idTextAnchor023)，“Azure机器学习服务入门”中创建了`compute-cluster`计算集群。
- en: Understand how to navigate to the Jupyter environment from an Azure compute
    instance as demonstrated in [*Chapter 4*](B16595_04_ePub.xhtml#_idTextAnchor056),
    *Building an AutoML Regression Solution*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何从 Azure 计算实例导航到 Jupyter 环境的方法，如 [*第 4 章*](B16595_04_ePub.xhtml#_idTextAnchor056)，*构建
    AutoML 回归解决方案* 中所示。
- en: Have trained and registered the `Iris-Multi-Classification-AutoML` ML model
    in [*Chapter 5*](B16595_05_ePub.xhtml#_idTextAnchor068), *Building an AutoML Classification
    Solution*.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [*第 5 章*](B16595_05_ePub.xhtml#_idTextAnchor068)，*构建 AutoML 分类解决方案* 中训练并注册了
    `Iris-Multi-Classification-AutoML` 机器学习模型。
- en: 'The code for this chapter is available here: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter09](https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter09).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码在此处可用：[https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter09](https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/tree/master/Chapter09)。
- en: Creating an ML pipeline
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个机器学习流程
- en: ML pipelines are Azure's solution for batch scoring ML models. You can use ML
    pipelines to score any model you train, including your own custom models as well
    as AutoML-generated models. They can only be created via code using the Azure
    ML Python SDK. In this section, you will code a simple pipeline to score diabetes
    data using the `Diabetes-AllData-Regression-AutoML` model you built in [*Chapter
    4*](B16595_04_ePub.xhtml#_idTextAnchor056), *Building an AutoML Regression Solution*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习流程是 Azure 的批量评分机器学习模型的解决方案。你可以使用机器学习流程来评分你训练的任何模型，包括你自己的自定义模型以及 AutoML 生成的模型。它们只能通过使用
    Azure ML Python SDK 的代码来创建。在本节中，你将编写一个简单的流程来使用你在 [*第 4 章*](B16595_04_ePub.xhtml#_idTextAnchor056)，*构建
    AutoML 回归解决方案* 中构建的 `Diabetes-AllData-Regression-AutoML` 模型来评分糖尿病数据。
- en: As in other chapters, you will begin by opening your compute instance and navigating
    to your Jupyter notebook environment. You will then create and name a new notebook.
    Once your notebook is created, you will build, configure, and run an ML pipeline
    step by step. After confirming your pipeline has run successfully, you will then
    publish your ML pipeline to a pipeline endpoint. **Pipeline endpoints** are simply
    URLs, web addresses that call ML pipeline runs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如其他章节所述，你将首先打开你的计算实例并导航到你的 Jupyter 笔记本环境。然后创建并命名一个新的笔记本。一旦你的笔记本创建完成，你将逐步构建、配置和运行一个机器学习流程。确认你的流程成功运行后，你将然后发布你的机器学习流程到流程端点。**流程端点**只是
    URL，是调用机器学习流程运行的网页地址。
- en: The following steps deviate greatly from previous chapters. You will have to
    load in many more libraries and also write a custom Python file for scoring new
    data. You will also learn how to create **environments**, that is, artifacts that
    specify which Python packages, package versions, and software settings you are
    using.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤与之前的章节有很大不同。你将需要加载更多的库，并编写一个用于评分新数据的自定义 Python 文件。你还将学习如何创建 **环境**，即指定你使用的
    Python 包、包版本和软件设置的工件。
- en: 'In addition to creating an environment, you will need to containerize it. Finally,
    you will need to configure and run your ML pipeline, completing the process by
    publishing your pipeline to an endpoint. The entire process is shown in *Figure
    9.1*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建环境之外，你还需要对其进行容器化。最后，你需要配置并运行你的机器学习流程，通过将流程发布到端点来完成整个过程。整个过程在 *图 9.1* 中展示：
- en: '![Figure 9.1 – Steps involved in creating your ML scoring pipeline ](img/Figure_9.1_B16595.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 创建你的机器学习评分流程所涉及的步骤](img/Figure_9.1_B16595.jpg)'
- en: Figure 9.1 – Steps involved in creating your ML scoring pipeline
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 创建你的机器学习评分流程所涉及的步骤
- en: The `ML-Scoring-Pipeline.ipynb` file in the GitHub repository contains the code
    for all the steps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 仓库中的 `ML-Scoring-Pipeline.ipynb` 文件包含了所有步骤的代码。
- en: Coding the first three steps of your ML scoring pipeline
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写你的机器学习评分流程的前三个步骤
- en: 'First, load in your libraries, set up your AMLS resources, and create a dataset
    to score with the following steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载你的库，设置你的 AMLS 资源，并按照以下步骤创建一个用于评分的数据集：
- en: Open **Azure Machine Learning studio** (**AML studio**) by navigating to [https://ml.azure.com/](https://ml.azure.com/).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过导航到 [https://ml.azure.com/](https://ml.azure.com/) 打开 **Azure 机器学习工作室**（**AML
    工作室**）。
- en: Click **Compute** and start up a compute instance. Any compute instance will
    work, as they all link to the same Jupyter notebook environment.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **计算** 并启动一个计算实例。任何计算实例都可以，因为它们都链接到相同的 Jupyter 笔记本环境。
- en: Create a new Jupyter notebook and name it `machine-learning-pipeline`. If you
    need a refresher on how to do this, please review [*Chapter 4*](B16595_04_ePub.xhtml#_idTextAnchor056),
    *Building an AutoML Regression Solution*.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 Jupyter 笔记本，并将其命名为 `machine-learning-pipeline`。如果你需要复习如何做这件事，请查阅 [*第4章*](B16595_04_ePub.xhtml#_idTextAnchor056)，*构建AutoML回归解决方案*。
- en: 'Open your newly created notebook and begin by importing all of the standard
    Azure libraries you will need using the following code:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你刚刚创建的笔记本，并开始使用以下代码导入你将需要的所有标准 Azure 库：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[*Chapter 4*](B16595_04_ePub.xhtml#_idTextAnchor056), *Building an AutoML Regression
    Solution*, explains `Workspace`, `Dataset`, `Datastore`, `Experiment`, and `ComputeTarget`.
    `Environment` lets you create an object that contains information on which Python
    packages your ML pipeline will need to install to run successfully. `Model` lets
    you retrieve your previously trained ML models.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[*第4章*](B16595_04_ePub.xhtml#_idTextAnchor056)，*构建AutoML回归解决方案*，解释了 `Workspace`、`Dataset`、`Datastore`、`Experiment`
    和 `ComputeTarget`。`Environment` 允许你创建一个对象，其中包含有关你的 ML 管道需要安装哪些 Python 包以成功运行的信息。`Model`
    允许你检索之前训练的 ML 模型。'
- en: 'Continuing on, import all of the Azure ML pipeline libraries with the following
    code:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续导入所有 Azure ML 管道库，使用以下代码：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`RunConfiguration` stores information that your ML pipeline will need to run,
    including the environment and base image. `CondaDependencies` lets you add Python
    packages to your environment. `Default_CPU_Image` is needed to specify which base
    image you will use in your run configuration. `PythonScriptStep` is a type of
    `Pipeline` is the core package to build ML pipelines. `PublishedPipeline` lets
    you publish ML pipelines to endpoints. `StepSequence` lets you set the order of
    your ML pipeline steps and `RunDetails` simply shows you the output of your ML
    pipeline as it runs.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`RunConfiguration` 存储了你的 ML 管道运行所需的信息，包括环境和基础镜像。`CondaDependencies` 允许你向环境中添加
    Python 包。`Default_CPU_Image` 用于指定你在运行配置中将要使用的基础镜像。`PythonScriptStep` 是 `Pipeline`
    类型，是构建 ML 管道的核心包。`PublishedPipeline` 允许你将 ML 管道发布到端点。`StepSequence` 允许你设置 ML 管道步骤的顺序，而
    `RunDetails` 则简单地显示你的 ML 管道在运行时的输出。'
- en: Important note
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If you are having trouble loading any Azure libraries, update the Azure ML
    SDK by running the `Update AzureML SDK.ipynb` notebook found here: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb).'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你加载任何 Azure 库时遇到困难，请通过运行此处找到的 `Update AzureML SDK.ipynb` 笔记本来更新 Azure ML SDK：[https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb)。
- en: 'Import `pandas`, `numpy`, and `os`. `os` will let you create and manipulate
    files and folders from your Jupyter notebook. `random` will let you generate random
    numbers, which is useful for simulating new Iris data:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`numpy` 和 `os`。`os` 允许你从 Jupyter 笔记本中创建和操作文件和文件夹。`random` 允许你生成随机数，这对于模拟新的鸢尾花数据很有用：
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Connect your Jupyter notebook to your AMLS workspace:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的 Jupyter 笔记本连接到你的 AMLS 工作区：
- en: '[PRE3]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you are prompted to log in, follow the instructions.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你被提示登录，请按照说明操作。
- en: 'Set your compute cluster to the one you created in [*Chapter 2*](B16595_02_ePub.xhtml#_idTextAnchor023),
    *Getting Started with Azure Machine Learning Service*:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的计算集群设置为你在 [*第2章*](B16595_02_ePub.xhtml#_idTextAnchor023)，*Azure 机器学习服务入门*
    中创建的那个：
- en: '[PRE4]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set your datastore:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置你的数据存储：
- en: '[PRE5]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, you will create simulated Iris data to score. Begin by creating four
    variables that contain a list of numbers based on the minimum and maximum values
    of the original Iris dataset with the following code. These variables contain
    all the possible values contained within the Iris dataset:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你将创建用于评分的模拟鸢尾花数据。首先，使用以下代码创建四个变量，这些变量包含基于原始鸢尾花数据集最小值和最大值的数字列表。这些变量包含鸢尾花数据集中所有可能的值：
- en: '[PRE6]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Continuing with the creation of simulated Iris data, create an empty `pandas`
    DataFrame called IrisDF, with the appropriate column names. Also, create an empty
    list called `IrisList`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建模拟的鸢尾花数据集的过程中继续，创建一个名为 IrisDF 的空 `pandas` DataFrame，并包含适当的列名。同时，创建一个名为 `IrisList`
    的空列表：
- en: '[PRE7]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Continuing with the creation of simulated Iris data, use the `choice` function
    from the `random` package within a `for` loop to create 100 new data points, rounding
    each value to `1` decimal place.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建模拟的鸢尾花数据集的过程中继续，使用 `random` 包中的 `choice` 函数在 `for` 循环中创建 100 个新的数据点，并将每个值四舍五入到
    `1` 位小数。
- en: 'Combine each set of four data points with the column names in a Python dictionary
    within the `for` loop and append that dictionary to `IrisList` row by row:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 `for` 循环中，将每组四个数据点与 Python 字典中的列名组合，并将该字典逐行追加到 `IrisList`：
- en: '[PRE8]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code will leave you with a list of randomly generated values from the original
    Iris dataset that can be turned into a `pandas` DataFrame.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此代码将为您留下从原始 Iris 数据集中随机生成的值列表，这些值可以转换为 `pandas` DataFrame。
- en: 'Completing the creation of simulated Iris data, append `IrisList` to `IrisDF`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成模拟 Iris 数据的创建，将 `IrisList` 追加到 `IrisDF`：
- en: '[PRE9]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Register your simulated Iris data with the following code:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码注册您的模拟 Iris 数据：
- en: '[PRE10]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will save it to your datastore and create an Azure dataset named `Iris
    Scoring`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这将将其保存到您的数据存储中，并创建一个名为 `Iris Scoring` 的 Azure 数据集。
- en: Creating a Python script to score data in your ML pipeline
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个用于在您的机器学习管道中评分数据的 Python 脚本
- en: 'In this section, you will create a folder and write a Python script that your
    ML pipeline will execute to score data using the following steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将创建一个文件夹并编写一个 Python 脚本，您的机器学习管道将执行该脚本以评分数据，具体步骤如下：
- en: 'Make a folder to hold your scoring script using `os`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `os` 创建一个用于存放评分脚本的文件夹：
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For every ML pipeline step you make, you have to have an accompanying Python
    script.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于您做出的每个机器学习管道步骤，您都必须有一个相应的 Python 脚本。
- en: Write a Python script to score new data. This script is long and has to be one
    block of code. Begin by writing out a new Python script file called `Iris_Scoring.py`
    using the `%%writefile` magic command. `Run` lets your program access the AMLS
    workspace you used to create the ML pipeline while running remotely on a compute
    cluster.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个用于评分新数据的 Python 脚本。此脚本很长，必须是一段代码块。首先，使用 `%%writefile` 魔法命令编写一个新的 Python
    脚本文件，名为 `Iris_Scoring.py`。`Run` 允许您的程序在远程计算集群上运行时访问您用于创建机器学习管道的 AMLS 工作区。
- en: 'Continuing in the same cell as part of `Iris_Scoring.py`, import the other
    Python packages:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，导入其他 Python 包：
- en: '[PRE12]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The only new package here is `joblib`, which will let you load saved ML models.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里唯一的新包是 `joblib`，它将允许您加载保存的机器学习模型。
- en: 'Continuing in the same cell as part of `Iris_Scoring.py`, set a variable called
    `run` using the `Run` function. You can use this variable to set your AMLS workspace:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，使用 `Run` 函数设置一个名为 `run` 的变量。您可以使用此变量设置您的 AMLS
    工作区：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Continuing in the same cell as part of `Iris_Scoring.py`, we are going to create
    a function called `main`. This function will run the main part of your scoring
    code.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，我们将创建一个名为 `main` 的函数。此函数将运行您的评分代码的主要部分。
- en: 'First, connect to your AMLS workspace using the `run` variable you created.
    Next, set your datastore to the default option and your dataset to `Iris Scoring`.
    Convert the `Iris Scoring` dataset into a `pandas` DataFrame called `scoringDF`:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，使用您创建的 `run` 变量连接到您的 AMLS 工作区。接下来，将数据存储设置为默认选项，并将数据集设置为 `Iris Scoring`。将
    `Iris Scoring` 数据集转换为名为 `scoringDF` 的 `pandas` DataFrame：
- en: '[PRE14]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Continuing in the same cell as part of `Iris_Scoring.py`, load your `Iris-Multi-Classification-AutoML`
    model with the Azure `Model` and `joblib` packages:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，使用 Azure 的 `Model` 和 `joblib` 包加载您的 `Iris-Multi-Classification-AutoML`
    模型：
- en: '[PRE15]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Continuing in the same cell as part of `Iris_Scoring.py`, use your model to
    make predictions on `scoringDF`, save those predictions to a `pandas` `Series`,
    and add the predictions back to your `scoringDF` DataFrame in a new column called
    `Prediction`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，使用您的模型对 `scoringDF` 进行预测，将这些预测保存到 `pandas` 的 `Series`
    中，并将这些预测添加到您的 `scoringDF` DataFrame 的新列 `Prediction` 中：
- en: '[PRE16]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When you add the new column, the predictions will be in the correct order and
    match the corresponding row.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当您添加新列时，预测将按正确顺序排列，并与相应的行匹配。
- en: 'Continuing in the same cell as part of `Iris_Scoring.py`, make a folder called
    `Output_Folder` using `os`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，使用 `os` 创建一个名为 `Output_Folder` 的文件夹：
- en: '[PRE17]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will create a folder on your compute cluster to store your predictions
    temporarily so you can transfer them to your datastore.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在您的计算集群上创建一个文件夹，用于临时存储您的预测，以便您可以将其传输到您的数据存储。
- en: 'Continuing in the same cell as part of `Iris_Scoring.py`, specify a file called
    `Iris_Predictions.csv`. Then, use `os` to specify a path on the compute cluster
    where you will write that file out. Finally, use `to_csv` to write out `scoringDF`
    to your compute cluster:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，指定一个名为 `Iris_Predictions.csv` 的文件。然后，使用 `os` 在计算集群上指定一个路径，您将在此路径上写入该文件。最后，使用
    `to_csv` 将 `scoringDF` 写入您的计算集群：
- en: '[PRE18]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This piece of code will write your output to your compute cluster. This is necessary
    to move it into your datastore.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码将您的输出写入您的计算集群。这是必要的，以便将其移动到您的数据存储中。
- en: Continuing in the same cell as part of `Iris_Scoring.py`, upload `Iris_Predictions.csv`
    to your datastore. This code will write it to a folder called `Output_Folder`,
    matching the directory structure on your compute cluster.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Iris_Scoring.py` 的同一单元格中继续，将 `Iris_Predictions.csv` 上传到您的数据存储。此代码将将其写入名为
    `Output_Folder` 的文件夹中，与您的计算集群上的目录结构相匹配。
- en: '[PRE19]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Completing `Iris_Scoring.py`, use `os` to remove the file and folder from your
    compute cluster. Finish the cell with boilerplate code that will automatically
    run your `main` function when the Python script is called within the ML pipeline:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成 `Iris_Scoring.py` 后，使用 `os` 从您的计算集群中删除文件和文件夹。使用模板代码完成单元格，当Python脚本在ML管道中调用时，将自动运行您的
    `main` 函数：
- en: '[PRE20]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This completes your Python script. Write the next piece of code in a new cell.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了您的Python脚本。在新的单元格中编写下一段代码。
- en: Creating and containerizing an environment
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和容器化环境
- en: 'The difficult part is over and the rest is pure boilerplate, beginning with
    creating an environment. **Environments** are collections of Python packages that
    are required to run your code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 困难的部分已经结束，剩下的都是纯模板代码，从创建环境开始。**环境**是运行您的代码所需的Python包的集合：
- en: 'In a new cell, create an environment. Also, set a variable using `CondaDependencies`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新单元格中创建一个环境。同时，使用 `CondaDependencies` 设置一个变量：
- en: '[PRE21]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: All the packages and versions of packages that are required to run your Python
    script will be added to the `conda_dep` variable in the next step.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下一步中，将运行您的Python脚本所需的全部包及其版本添加到 `conda_dep` 变量中。
- en: 'Attach Python packages to your `conda_dep` variable, beginning with the packages
    found in the `conda` package manager:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Python包附加到您的 `conda_dep` 变量中，从 `conda` 包管理器中找到的包开始：
- en: '[PRE22]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There are two package managers, `conda` and `pip`. `conda` automatically resolves
    dependencies for you. So, if a package requires another package, you don't have
    to worry about it. `pip` requires you to resolve those dependencies yourself.
    As a result, if a package is available in both `conda` and `pip`, always install
    it via `conda`.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有两个包管理器，`conda` 和 `pip`。`conda` 会自动为您解决依赖问题。因此，如果一个包需要另一个包，您无需担心。`pip` 则要求您自己解决这些依赖。结果，如果一个包在
    `conda` 和 `pip` 中都可用，请始终通过 `conda` 安装它。
- en: When installing packages, always specify the version. You can discover which
    version you are using by running the `!pip freeze` command in an empty cell. In
    addition to `numpy`, `joblib`, and `pandas`, AutoML-generated models also require
    `packaging` and `xgboost` to run.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在安装包时，始终指定版本。您可以通过在空单元格中运行 `!pip freeze` 命令来发现您正在使用哪个版本。除了 `numpy`、`joblib`
    和 `pandas` 之外，AutoML生成的模型还需要 `packaging` 和 `xgboost` 来运行。
- en: 'Attach Python packages to your `conda_dep` variable that are not available
    in `conda` using `pip` instead:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pip` 而不是 `conda` 将不可在 `conda` 中找到的Python包附加到您的 `conda_dep` 变量中：
- en: '[PRE23]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: There are three packages; `azureml-defaults` enables you to use standard Azure
    ML SDK functions, while `azureml-automl-core` and `azureml-automl-runtime` are
    required to score any AutoML-generated models.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有三个包；`azureml-defaults` 允许您使用标准的Azure ML SDK函数，而 `azureml-automl-core` 和 `azureml-automl-runtime`
    是评分任何AutoML生成的模型所必需的。
- en: 'Add the `conda_dep` variable to your environment and register the environment
    to your AMLS workspace by using the following code:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `conda_dep` 变量添加到您的环境中，并使用以下代码将环境注册到您的AMLS工作区：
- en: '[PRE24]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now that your environment is registered, you can call it anywhere in AMLS.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在您的环境已注册，您可以在AMLS的任何地方调用它。
- en: 'Create a `RunConfiguration` object for containerizing your environment. Set
    your environment, enable Docker, and use `DEFAULT_CPU_IMAGE` as your base image:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为您的环境创建一个 `RunConfiguration` 对象以进行容器化。设置您的环境，启用Docker，并使用 `DEFAULT_CPU_IMAGE`
    作为基础镜像：
- en: '[PRE25]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This will create a **Docker container**, a portable package of code that can
    be run anywhere as it contains all your code's scripts and dependencies. Your
    environment can now be utilized by your ML pipeline, which you will configure
    in the last series of steps.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个 **Docker容器**，一个包含所有代码脚本和依赖项的可移植包，可以在任何地方运行。现在，您的环境可以通过您的ML管道使用，您将在最后一系列步骤中进行配置。
- en: Configuring and running your ML scoring pipeline
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置和运行您的ML评分管道
- en: 'With your environment built and containerized, your Python script written,
    and all your AMLS resources set, you''re ready to configure and run your ML pipeline
    by following these steps:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的环境构建并容器化，你的 Python 脚本编写完毕，以及所有你的 AMLS 资源设置完成后，你就可以按照以下步骤配置和运行你的机器学习流程：
- en: 'Configure your ML pipeline step with the following code. You need to give your
    step a name, `iris-scoring-step`, and specify your Python script name, Python
    script folder location, compute target, and run configuration. Always set `allow_reuse`
    to `False`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码配置你的机器学习流程步骤。你需要给你的步骤起一个名字，`iris-scoring-step`，并指定你的 Python 脚本名称、Python
    脚本文件夹位置、计算目标和运行配置。始终将 `allow_reuse` 设置为 `False`：
- en: '[PRE26]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Setting `allow_reuse` to `True` is for debugging multi-step pipelines where
    you want to skip rerunning a successfully completed step.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 `allow_reuse` 设置为 `True` 是为了调试多步骤流程，其中你希望跳过重新运行已成功完成的步骤。
- en: 'Set your **step sequence** and **pipeline object**:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置你的 **步骤序列** 和 **流程对象**：
- en: '[PRE27]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The step sequence is the order in which your ML pipeline steps will run.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤序列是你的机器学习流程步骤将运行的顺序。
- en: 'Give your pipeline experiment run a name, `Iris-Scoring-Pipeline-Run`, and
    submit it with the following code:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给你的流程实验运行起一个名字，`Iris-Scoring-Pipeline-Run`，并使用以下代码提交：
- en: '[PRE28]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, this is what kicks your pipeline off!
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，这是启动你的流程的关键步骤！
- en: 'Use `RunDetails` to watch your pipeline get built and execute in real time:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `RunDetails` 来实时观察你的流程构建和执行：
- en: '[PRE29]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As it runs, you will see a lot of logs. If your pipeline runs successfully,
    it will complete with the words **Finished**. You should see a graphic identical
    to the one in *Figure 9.2*:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在运行过程中，你会看到很多日志。如果你的流程运行成功，它将以 **完成** 的字样结束。你应该看到一个与 *图 9.2* 中相同的图形：
- en: '![Figure 9.2 – Successful pipeline run graphic ](img/Figure_9.2_B16595.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 9.2 – 成功的流程运行图形](img/Figure_9.2_B16595.jpg)'
- en: Figure 9.2 – Successful pipeline run graphic
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.2 – 成功的流程运行图形
- en: 'Publish your pipeline to an endpoint with the following code, specifying a
    name, description, and version number:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将你的流程发布到端点，指定一个名称、描述和版本号：
- en: '[PRE30]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'All published pipelines require version numbers set by the creator. Running
    this code will give you a published pipeline ID as well as a link to the endpoint,
    as seen in *Figure 9.3*:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有已发布的流程都需要由创建者设置的版本号。运行此代码将给你一个已发布的流程 ID 以及一个访问端点的链接，如图 *图 9.3* 所示：
- en: '![Figure 9.3 – Successfully published pipeline ](img/Figure_9.3_B16595.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 成功发布的流程](img/Figure_9.3_B16595.jpg)'
- en: Figure 9.3 – Successfully published pipeline
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 成功发布的流程
- en: Now that you have executed and published your scoring pipeline, you can examine
    and download your scoring file. It was saved to a folder called `Output_Folder`
    in a file called `Iris_Predictions.csv`. Access the file directly by navigating
    to your storage account. You can do this either from the Azure portal ([https://portal.azure.com](https://portal.azure.com))
    or via **AML** **studio**.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经执行并发布了你的评分流程，你可以检查并下载你的评分文件。它被保存在一个名为 `Output_Folder` 的文件夹中，文件名为 `Iris_Predictions.csv`。通过导航到你的存储账户来直接访问该文件。你可以从
    Azure 门户（[https://portal.azure.com](https://portal.azure.com)）或通过 **AML** **工作室**
    来这样做。
- en: Accessing your scored predictions via AML studio
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 AML 工作室访问你的评分预测
- en: To access `Iris_Predictions.csv` and download it to your desktop, you first
    have to locate your storage account. You can find your storage account through
    AML studio.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 `Iris_Predictions.csv` 并将其下载到你的桌面，你首先必须定位你的存储账户。你可以通过 AML 工作室找到你的存储账户。
- en: 'The following steps will have you locate your datastore through the AML studio,
    access your storage account, navigate to the correct file, and download it to
    your local machine. This way, you can use the AI-generated predictions for any
    purpose you wish:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导你通过 AML 工作室定位你的数据存储，访问你的存储账户，导航到正确的文件，并将其下载到你的本地机器。这样，你可以使用 AI 生成的预测来完成任何你希望的目的：
- en: Navigate to AML studio at [https://ml.azure.com](https://ml.azure.com).
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 AML 工作室 [https://ml.azure.com](https://ml.azure.com)。
- en: Click **Datastores** on the left-hand panel.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧面板上的 **Datastores**。
- en: Click the blue link to **workspaceblobstore** (default) in the center of the
    page.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击页面中央的蓝色链接到 **workspaceblobstore**（默认）。
- en: Click the blue link to your storage account under `automlexamplew` followed
    by a string of numbers. This will take you to your storage account resource in
    Azure.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击页面下方 `automlexamplew` 后跟一串数字的蓝色链接到你的存储账户。这将带你到 Azure 中的存储账户资源。
- en: Once in your storage account, click the blue link to **Containers** in the center-left
    of your screen. Clicking anywhere in the box will work.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦在你的存储账户中，点击屏幕中央左上角的蓝色链接到 **容器**。
- en: You will now see a list of folders. Click the folder that begins with `azureml-blobstore-`
    followed by a unique identifier ID.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在将看到一个文件夹列表。点击以 `azureml-blobstore-` 开头并跟有一个唯一标识符 ID 的文件夹。
- en: Click `azureml`, `managed-dataset`, and `UI`. These folders hold logs for your
    experiments, among other objects. *Figure 9.4* shows the folder structure you
    need to follow to reach your file:![Figure 9.4 – Path to your output files ](img/Figure_9.4_B16595.jpg)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击屏幕中央左边的 `azureml`、`managed-dataset` 和 `UI`。这些文件夹包含你的实验日志以及其他对象。*图 9.4* 展示了你需要遵循的文件夹结构以到达你的文件：![图
    9.4 – 输出文件的路径](img/Figure_9.4_B16595.jpg)
- en: Figure 9.4 – Path to your output files
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.4 – 输出文件的路径
- en: Click **Iris_Predictions.csv**.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **Iris_Predictions.csv**。
- en: Click the `Iris_Predictions.csv` to your local machine. It is a comma-separated
    file with headers.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `Iris_Predictions.csv` 添加到你的本地机器。它是一个带有标题的逗号分隔文件。
- en: Open up your file with Microsoft Excel or similar software to look at your data.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Microsoft Excel 或类似软件打开你的文件，查看你的数据。
- en: That was a lot of work, but now you have a working pipeline endpoint that can
    score new data points in batch. This is a huge accomplishment, as many organizations
    have trouble setting up such workflows. You can also easily reuse this code in
    multiple projects, as the vast majority is boilerplate. While you have to alter
    the Python script and add packages as appropriate, this template can also be used
    to score both AutoML and custom ML models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项繁重的工作，但现在你有一个可以批量评分新数据点的有效管道端点。这是一个巨大的成就，因为许多组织在设置此类工作流程时遇到困难。你还可以轻松地在多个项目中重用此代码，因为其中绝大多数是样板代码。虽然你必须修改
    Python 脚本并添加适当的包，但此模板也可以用于评分 AutoML 和自定义机器学习模型。
- en: Next, you will learn how to score 10,000,000 data points in no time at all using
    a parallel scoring pipeline.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将学习如何使用并行评分管道在极短的时间内评分 1000 万个数据点。
- en: Creating a parallel scoring pipeline
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建并行评分管道
- en: Standard ML pipelines work just fine for the majority of ML use cases, but when
    you need to score a large amount of data at once, you need a more powerful solution.
    That's where `ParallelRunStep` comes in. `ParallelRunStep` is Azure's answer to
    scoring big data in batch. When you use `ParallelRunStep`, you leverage all of
    the cores on your compute cluster simultaneously.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 标准机器学习管道对于大多数机器学习用例来说效果很好，但当你需要一次性评分大量数据时，你需要一个更强大的解决方案。这就是 `ParallelRunStep`
    的作用。`ParallelRunStep` 是 Azure 对批量评分大数据的回应。当你使用 `ParallelRunStep` 时，你可以同时利用计算集群上的所有核心。
- en: Say you have a compute cluster consisting of eight `Standard_DS3_v2` virtual
    machines. Each `Standard_DS3_v2` node has four cores, so you can perform 32 parallel
    scoring processes at once. This parallelization essentially lets you score data
    many times faster than if you used a single machine. Furthermore, it can easily
    scale vertically (increasing the size of each virtual machine in the cluster)
    and horizontally (increasing the node count).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个由八个 `Standard_DS3_v2` 虚拟机组成的计算集群。每个 `Standard_DS3_v2` 节点有四个核心，因此你可以一次性执行
    32 个并行评分过程。这种并行化实际上让你比使用单台机器快得多地评分数据。此外，它还可以轻松地进行垂直扩展（增加集群中每个虚拟机的大小）和水平扩展（增加节点数量）。
- en: This section will allow you to become a *big data* scientist who can score large
    batches of data. Here, you will again be using simulated Iris data, but instead
    of 100 rows, you will be scoring 10 million rows at once.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将帮助你成为一名能够评分大量数据的 *大数据* 科学家。在这里，你将再次使用模拟的 Iris 数据，但不是 100 行，而是一次性评分 1000 万行。
- en: Furthermore, this is an advanced solution that utilizes two pipeline steps,
    a step that scores data in parallel and a step that transfers data to an output
    folder. By working through this example, you'll understand how to create advanced
    multi-step pipeline runs to solve difficult problems.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这是一个利用两个管道步骤的高级解决方案，一个步骤是并行评分数据，另一个步骤是将数据传输到输出文件夹。通过完成这个示例，你会了解如何创建高级多步骤管道运行来解决复杂问题。
- en: Important note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Parallel runs score data in batches, dividing your dataset into many small parts.
    If you have any data preprocessing that relies on calculations made with previous
    rows, this preprocessing should be done in a separate `PythonScriptStep` before
    being passed on to `ParallelRunStep`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Much of this code is similar to the *Creating an ML pipeline* section. However,
    there are two pipeline steps to create instead of one. Furthermore, you will be
    introduced to new concepts such as `input` and `output` pipeline configuration
    options. At the end of this section, you will also publish your pipeline to an
    endpoint. *Figure 9.5* shows the entire process:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Steps involved in creating a parallel scoring pipeline  ](img/Figure_9.5_B16595.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Steps involved in creating a parallel scoring pipeline
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: If you need a refresher on any of the steps, please refer to the *Creating an
    ML pipeline* section. You can find the code for all the steps in the `ML-Parallel-Pipeline.ipynb`
    file in the GitHub repository.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Coding the first three steps of your ML parallel scoring pipeline
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create your parallel scoring pipeline, begin with the following steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to your Jupyter environment on your compute instance and create a new
    Jupyter notebook. Name it `ml-parallel-pipeline`.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open your newly created notebook and import your standard Azure libraries:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'From `azureml.core.compute`, import `ComputeTargetNext` and import all of the
    Azure ML pipeline libraries with the following code:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: There are three additional packages compared to the previous section. `ParallelRunStep`
    is an ML pipeline step that lets you run Python code in parallel. `ParallelRunConfig`
    lets you configure `ParallelRunStep`. `PipelineData` lets you pass intermediate
    data from one step to another.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you are having trouble loading any Azure libraries, update the Azure ML
    SDK by running the `Update AzureML SDK.ipynb` notebook found here: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-Microsoft-Azure/blob/master/Update-AzureML-SDK.ipynb).'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `pandas`, `numpy`, `os`, and `random`:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As always, connect your Jupyter notebook to your AMLS workspace:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If you are prompted to log in, follow the instructions.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set your compute cluster with the following code:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Set your datastore:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Use the following code to create simulated Iris data:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Continuing with the creation of simulated Iris data, create an empty `pandas`
    DataFrame and list:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Continuing with the creation of simulated Iris data, use the `choice` function
    from the `random` package within a `for` loop to create 10,000,000 new data points,
    rounding each value to `1` decimal place:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will take a while, so give it some time to run.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Completing the creation of simulated Iris data, append `IrisList` to `IrisDF`:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Register your simulated Iris data with the following code:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This will save it to your datastore and create an Azure dataset named `Iris
    Parallel Scoring`. You're now ready to write your Python script.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Creating Python scripts to score data in your ML parallel pipeline
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, create a folder and write two Python scripts that your ML pipeline will
    execute to score data. You will need one step to make predictions and another
    step to transfer your output to a final destination on your datastore:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a folder to hold your scoring script using `os`:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This folder should already exist from the previous section, but this code will
    not error out since `exist_ok` is set to `True`. This folder will hold both the
    scripts you will write for your parallel pipeline run.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Write a Python script to score new data in parallel:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This script is significantly different from the previous script, but also has
    to be in one cell. Begin by writing a new Python script file called `Iris_Parallel_Scoring.py`
    using the `%%writefile` magic command. Begin by loading in your Azure libraries.
    You should recognize all of them from the previous section.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing in the same cell as part of `Iris_Parallel_Scoring.py`, import other
    Python packages, including the new `argparse` package:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This package lets you pass arguments into your script. **Arguments** are flexible
    pieces of code that you can pass into your pipeline at runtime. For example, you
    can use arguments to pass in different datastores or datasets.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing in the same cell as part of `Iris_Parallel_Scoring.py`, set a variable
    called `run` using the `Run` function:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Continuing in the same cell as part of `Iris_Parallel_Scoring.py`, create a
    function called `init`. This function will pass in your arguments and load your
    ML model, setting it to a global variable.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`parser` to store your arguments and add the `model_name` argument to that
    variable. `ParallelRunStep` also passes hidden arguments behind the scenes, so
    you also need to set `unknown_args`. Once that is done, use `joblib` to load your
    model using the `model_name` argument, as shown in the following code:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Continuing in the same cell as part of `Iris_Parallel_Scoring.py`, create a
    function called `main`. This function will score your data and return the result,
    which will be automatically stored in a file called `parallel_run_step.txt.model`
    with the Azure `Model` and `joblib` packages.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, you will use the `predict` function on your `model` variable to make
    predictions. Notice that the data is automatically passed into this function as
    a `pandas` DataFrame called `input_data`.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You then convert these predictions into a series and add it back to `input_data`
    as a column called `Prediction`. Completing `Iris_Parallel_Scoring.py`, you return
    the finished `input_data` DataFrame to be automatically written to the text file:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Write a Python script to transfer your results to the output location of your
    choice. `ParallelRunStep` outputs a file called `parallel_run_step.txt`. This
    will be stored as pipeline data.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pipeline data is data that is saved in your datastore as an intermediate step
    to be passed on to another ML pipeline step. Furthermore, `parallel_run_step.txt`
    has no headers and you need to add them.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Begin by writing out a new Python script file called `Iris_Parallel_Output_Creation.py`
    using the `%%writefile` magic command. Start by loading in your Azure libraries
    as usual:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`,
    load in all of the standard Python packages you need:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: You will once again need `argparse` to pass in arguments, namely the folder
    that holds `parallel_run_step.txt`.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`,
    set a variable called `run` using the `Run` function:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`,
    pass in your arguments to access the folder that holds `parallel_run_step.txt`:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Call the `input_data_folder` argument and pass it in as an argument when you
    configure this pipeline step.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`, write
    a function called `main`. This function will transfer your predictions from an
    intermediate pipeline data location to its final destination.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Begin by using `os` and `input_data_folder` to find the path holding `parallel_run_step.txt`.
    Then, read it in as a space-delimited text file with no headers in a `pandas`
    DataFrame called `result` using the following code:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`,
    as part of your `main` function, add columns to your `result` DataFrame:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: It's important to remember that `parallel_run_step.txt` never has headers, so
    you need to enter the columns manually in the correct order as shown in the preceding
    code.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`,
    as part of your `main` function, connect to your AMLS workspace using the `run`
    variable and set a variable for your datastore:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This is the datastore that will hold your final output file.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing in the same cell as part of `Iris_Parallel_Output_Creation.py`,
    as part of your `main` function, use `os` to create a folder called `Output_Folder`
    on your compute cluster, and write a CSV file called `Iris_Parallel_Predictions.csv`:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Even though the original `parallel_run_step.txt` file was space-delimited, you
    can set the delimiter on your final output file to whatever you wish.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Completing `Iris_Parallel_Output_Creation.py`, upload `Iris_Parallel_Predictions.csv`
    to your datastore in a folder called `Output_Folder`.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, use `os` to remove both `Iris_Parallel_Predictions.csv` and `Output_Folder`
    from your compute cluster. Finally, trigger your `main` function as you did in
    the *Creating an ML pipeline* section:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: With both steps written, the rest of the code is pure boilerplate to containerize
    your environment and to configure and run your pipeline.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and running your ML parallel scoring pipeline
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since you''ve already built your environment in the *Creating an ML pipeline*
    section, all that''s really left is configuring your ML pipeline steps. Use the
    following steps:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new cell, retrieve the environment that you created in the *Creating an
    ML pipeline* section with the following code:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, define a variable that defines your pipeline data and assigns it to a
    datastore:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This is the location that will hold `parallel_run_step.txt` between your first
    and second steps. You must also give this pipeline data object a name.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Enable Docker on your environment and specify `DEFAULT_CPU_IMAGE` as your base
    image for your `ParallelRunStep`:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: You do not need to specify a `RunConfiguration` object for `ParallelRunStep`.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a `RunConfiguration` object for your output creation step:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Set your parallel run configurations with the following code:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You need to input the name of your Python script as well as the source directory
    that holds it. You need to specify how much data will be scored in parallel with
    `mini_batch_size`. For all `error_threshold` is how many times the step can fail
    in parallel before the entire pipeline fails. Next, setting `output_action` to
    `append_row` will automatically generate `parallel_run_step.txt` for you. Other
    options for `output_action` are labor-intensive.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Set your compute target to the appropriate compute cluster and specify your
    environment. Leave `run_invocation_timeout` at 60 seconds so your run will fail
    if it idles for too long and set `node_count` equal to the number of nodes in
    your compute cluster to ensure maximum parallelization. Lastly, set `logging_level`
    to `DEBUG` for informative logs.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the two variables that you will need to create your parallel scoring step,
    your dataset, and your model name:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Datasets need to be passed in with the `as_named_input` code and appear as the
    `input_data` variable in your Python script.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create your parallel scoring step using the following code:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: You need to give the step a name, `iris-parallel-scoring-step`. No spaces are
    allowed. Pass in your configurations using `parallel_run_config` and pass in your
    dataset using `inputs`. Set `output` to your pipeline data object and pass in
    your model name as an argument. As always, set `allow_reuse` equal to `False`.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create your output creation step using the following code:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Give this step the name `iris-output-step` and pass in your script name and
    source directory. For `arguments` and `input`, you need to pass in `parallel_run_output`.
    This lets your output creation step use `parallel_run_step.txt` generated by your
    parallel scoring step. Then, set your compute target, specify your run configuration,
    and set `allow_reuse` to `False`.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set your step sequence and create a pipeline object:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This time, you have two steps to set, `parallel_run_step` and `output_step`.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Give your pipeline experiment run a name, `Iris-Parallel-Scoring-Pipeline-Run`,
    and submit it to your compute cluster:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Use `RunDetails` to watch your pipeline execute in real time:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Notice how fast this scores your data. If your pipeline runs successfully,
    it will complete with the word **Finished**. You should see a graphic identical
    to the one in *Figure 9.6*:'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Successful parallel pipeline run graphic ](img/Figure_9.6_B16595.jpg)'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9.6 – Successful parallel pipeline run graphic
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Publish your pipeline to an endpoint as you did in the previous section, naming
    it `Iris-Parallel-Scoring-Pipeline` or whatever you wish:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Running this code will give you your published pipeline ID as well as a link
    to the endpoint, as seen in *Figure 9.7*:'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Successfully published parallel pipeline ](img/Figure_9.7_B16595.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Successfully published parallel pipeline
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: You can now examine and download your scoring file as you did in the previous
    section. Look inside `Output_Folder` in your datastore for a file called `Iris_Parallel_Predictions.csv`.
    It's quite a bit larger than your last file, at around 30 MB.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: With both a standard scoring pipeline and a parallel run pipeline built, you
    are now at the cutting edge of AMLS. Both of these pipelines can be used to score
    not only AutoML-generated ML models, but custom models as well.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Even experienced data scientists have a hard time building these batch scoring
    solutions. So, you have acquired a desirable, marketable skill that will enable
    you to work alongside seasoned experts.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to build a pipeline for training AutoML
    models instead of scoring.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AutoML training pipeline
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it's necessary to retrain a model that you trained in AutoML. ML
    models can degrade over time if the relationship between your data and your target
    variable changes. This is true for all ML models, not just ones generated by AutoML.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Imagine, for example, that you build an ML model to predict demand for frozen
    pizza at a supermarket, and then one day, a famous pizza chain sets up shop next
    door. It's very likely that consumer buying behavior will change, and you will
    need to retrain the model. This is true for all ML models.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, AMLS has specialized ML pipeline steps built specifically for retraining
    models. In this section, we are going to use one of those steps, the AutoML step.
    The **AutoML step** lets you retrain models easily whenever you want, either with
    a push of a button or on a schedule.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Here, you will build a two-step ML pipeline where you will first train a model
    with an AutoML step and register it with a typical Python script step. This will
    enable you to build complete end-to-end solutions with automated scoring and training,
    completing your skillset. Furthermore, this will familiarize you with the AutoML
    step and all of its caveats.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Not all ML models require retraining, particularly those that predict physical
    phenomena, as the relationship between your data and the target variable is unlikely
    to change over time. However, most ML models will improve with additional data
    points, so it does make sense to retrain models as you collect and label more
    data.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should know what to expect in terms of creating ML pipelines. Most
    of the steps will be familiar to you, but you will have to work with specialized
    forms of pipeline data that pass data from your training step to your model registration
    step.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python scripts involved in this pipeline are much simpler than those involved
    in scoring pipelines, and thus require less customization when you try it with
    your own data. At the end of this section, like others, you will also publish
    your AutoML training pipeline to an endpoint. *Figure 9.8* outlines the process:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Steps involved in setting up an AutoML retraining pipeline ](img/Figure_9.8_B16595.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Steps involved in setting up an AutoML retraining pipeline
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: This process is a little bit different than the other pipelines, as you need
    to configure your AutoML settings early on. You can find the code for all the
    steps in the `ML-Retraining-Pipeline.ipynb` file in the GitHub repository.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Coding the first two steps of your AutoML training pipeline
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create your AutoML training pipeline, begin with the following steps:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to your Jupyter environment on your compute instance and create a new
    Jupyter notebook. Name it `automl-training-pipeline`.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open your newly created notebook and import the usual set of Azure libraries
    along with `AutoMLConfig`:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '`AutoMLConfig` lets you configure AutoML training runs.'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing on, import the necessary Azure ML pipeline libraries with the following
    code:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: There are two new packages. `AutoMLStep` is an ML pipeline step that lets you
    run AutoML training runs. `TrainingOutput` lets you access the output from your
    AutoML step to pass on to your model registration step.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `os`:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This is the only non-Azure Python package you will need to make this pipeline,
    and it will be used to create a new folder.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Connect your Jupyter notebook to your AMLS workspace:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Log in if prompted by following the instructions.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set your compute cluster:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Set your datastore:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Retrieve your `Iris Training` dataset with the following code. You will pass
    this dataset into your AutoML configuration settings:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Retrieve the environment named `AutoML Environment` that you created in the
    *Creating an ML pipeline* section of this chapter:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: This will be used only for your model registration step; your AutoML training
    step will use a standard, autogenerated environment.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Configuring your AutoML model training settings and step
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, you''ll configure everything related to your AutoML training step with
    the following code:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Set variables to pass into your AutoML configuration settings:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Explanations for these settings can be found in [*Chapter 5*](B16595_05_ePub.xhtml#_idTextAnchor068),
    *Building an AutoML Classification Solution*. One new setting is `iterations`.
    This will be used to determine how many AutoML models should be trained concurrently;
    this value should equal the number of nodes on your compute cluster to ensure
    maximum parallelization.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Configure your AutoML training run:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: An explanation for these settings can be found in [*Chapter 4*](B16595_04_ePub.xhtml#_idTextAnchor056),
    *Building an AutoML Regression Solution*. If you would like higher accuracy, adjust
    `experiment_timeout_minutes` to give AutoML more time to train. Note that you
    are passing in your dataset here.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the output for AutoML metrics:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This code is standard boilerplate used in every AutoML step. It saves metrics
    from your AutoML run as intermediate pipeline data that you can use to pass on
    to other steps in your ML pipeline.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the output for your best model:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This code is standard boilerplate used in every AutoML step. It saves information
    about the best model from your AutoML run as intermediate pipeline data to pass
    on to your model registration step.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Configure your AutoML step:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: All you need to do is give it a name, pass in your AutoML configuration settings,
    and specify `metrics_data` and `model_data` as output. As always, set `allow_reuse`
    to `False`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Python script to register your model
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With your AutoML training step configured, you now need to write another script
    to extract and register your model using the following steps:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a folder to hold your training script using `os`:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Write a Python script to register your model. This script is very short compared
    to the others you wrote. Begin by writing out a new Python script file called
    `Iris_Model_Registration.py` using the `%%writefile` magic command and loading
    in your Azure libraries and `argparse`:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Continuing in the same cell as part of `Iris_Model_Registration_Scoring.py`,
    set a variable called `run` using the `Run` function:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Continuing in the same cell as part of `Iris_Model_Registration_Scoring.py`,
    pass in your arguments to access the best model as trained by AutoML as well as
    the dataset you used to train the model. For this, you will need three arguments,
    `model_name`, `model_path`, and `dataset_name`:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Continuing in the same cell as part of `Iris_Model_Registration_Scoring.py`,
    write a function called `main`. This function will register your best model. Begin
    by connecting to your AMLS workspace using the `run` variable:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Continuing in the same cell as part of `Iris_Model_Registration_Scoring.py`,
    as part of your `main` function, retrieve your dataset through the `dataset_name`
    argument:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Use the `Dataset.Scenario.Training` line of code to specify the scenario in
    which it was used. This information will be saved when you register your model.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing in the same cell as part of `Iris_Model_Registration_Scoring.py`,
    as part of your `main` function, register your model using the `model_path` and
    `model_name` arguments:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: While `model_name` is something you specify, `model_path` will be automatically
    generated from `model_data`. The dataset you used to train your model will also
    be saved using this code. Finally, complete `Iris_Model_Registration_Scoring.py`
    with the boilerplate code that triggers `main`.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and running your AutoML training pipeline
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All that''s left is to containerize your environment, configure your model
    registration step, and run and publish your pipeline by following these steps:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `RunConfiguration` object for the model registration step:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Note that this uses `AutoML Environment`, but you can get away with using a
    simpler environment without `pandas` or NumPy if you so desire.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set variables for your model name and dataset name:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Your model name will be used to register the best model that AutoML produces
    and is your choice. Your dataset name, on the other hand, should match the dataset
    you used to train your model.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Configure your model registration step:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Here, you must pass in three arguments, `model_name`, `model_data`, and `dataset_name`,
    and pass in `model_data` as input. This is because `model_data` is pipeline data
    generated by your AutoML training step, while `model_name` and `dataset_name`
    are simple string variables. As always, set `allow_reuse` to `False`.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set your step sequence and create a pipeline object:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: This time, you have two steps to set, `automl_training step` and `model_registration_step`.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Give your pipeline experiment run a name, `Iris-AutoML-Training-Pipeline-Run`,
    and submit it to your compute cluster:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This will take much longer than your other pipelines, perhaps around a half
    hour.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `RunDetails` to watch your pipeline execute in real time:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'If your pipeline runs successfully, it will complete with the word **Finished**.
    You should see a graphic identical to the one in *Figure 9.9*:'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Successful AutoML training pipeline](img/Figure_9.9_B16595.jpg)'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 9.9 – Successful AutoML training pipeline
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Publish your pipeline to an endpoint as you did in the previous section, naming
    it `Iris-AutoML-Training-Pipeline` or whatever you wish:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Running this code will give you your published pipeline ID as well as a link
    to your endpoint, as seen in *Figure 9.10*:'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Successfully published AutoML training pipeline](img/Figure_9.10_B16595.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Successfully published AutoML training pipeline
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes when you''re running an ML pipeline, your computer will crash. However,
    your pipeline run will continue to run and you will still be able to publish it
    upon completion. To retrieve a completed pipeline so you can publish it, use the
    following code:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: You have now built three ML pipelines, a standard scoring pipeline, a parallel
    run pipeline, and an AutoML training pipeline. This was not a trivial effort,
    but all that hard work has paid off. You have mastered one of the most complex
    parts of AMLS.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: While making these pipelines is quite an undertaking, once you have them, they
    are very easy to manually rerun or automatically schedule, as you will see in
    the next section.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Triggering and scheduling your ML pipelines
  id: totrans-388
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest problems data scientists face is creating easy, rerunnable,
    production-ready code and scheduling it in an automatic, reliable manner. You've
    already accomplished the first part by creating your three ML pipelines. Now,
    it's time to learn how to do the second part.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will first learn how to manually trigger the pipelines
    you've created through the GUI. Then, you will learn how to trigger the pipelines
    via code, both manually and on an automated schedule. This will enable you to
    put your ML pipelines into production, generating results on an hourly, daily,
    weekly, or monthly basis.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Triggering your published pipeline from the GUI
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Triggering your published pipeline from the AML studio GUI is easy. However,
    you cannot set up an automated schedule for your ML pipelines at this time. As
    such, it is most useful for triggering training pipelines when you notice that
    your results seem off. Use the following steps to manually trigger your ML pipeline
    through the AML studio:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [https://ml.azure.com](https://ml.azure.com) to access AML studio.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Pipelines** under **Assets** on the left-hand side.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Pipeline endpoints** near the top of the page.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the blue link to **Iris-AutoML-Training-Pipeline**.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Submit**. This will open up a dialog box.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an experiment name from the drop-down box under `Iris-AutoML-Training-Pipeline-Run`.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Submit**.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Pipelines** under **Assets** again and, on the top line, you should
    see a new run, as shown in *Figure 9.11*:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Pipeline run submitted through the GUI ](img/Figure_9.11_B16595.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Pipeline run submitted through the GUI
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Compared to making an ML pipeline, resubmitting it is very easy. Next, we will
    look at ways to trigger our pipeline through code and create an automatic schedule.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Triggering and scheduling a published pipeline through code
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Triggering a published ML pipeline through code first requires you to obtain
    your pipeline ID. These were generated at the end of the previous sections whenever
    you published a pipeline. You can also find your pipeline ID by clicking on individual
    pipelines found under **Pipeline endpoints** through the AML studio. You will
    also need your pipeline IDs to set up schedules through code.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'All the code for this section can be found in the `ML-Pipeline-Scheduling.ipynb`
    file in the GitHub repository. Begin by opening a Jupyter notebook and following
    these steps:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Jupyter notebook as you have before. Name it `pipeline-scheduling`.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open your notebook and import the required Azure libraries, three of which
    are new. `PublishedPipeline` lets you access any ML pipelines you have published.
    `Schedule` and `ScheduleRecurrence` let you schedule ML pipelines:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'To manually trigger an ML pipeline, use the following code by replacing `your-published-pipeline-id`
    with the ID of your published AutoML training pipeline. That''s it:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'To create a schedule for running your ML pipeline, first determine an interval
    with the following code. The interval options are `Minute`, `Hour`, `Day`, `Week`,
    or `Month`. You can also specify `start_time` and `time_zone` as optional arguments.
    Another optional argument is `status`, which you can set to `Disabled` to turn
    off your schedule:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Create your schedule by giving it a name and passing in your `recurrence` settings,
    experiment name, published pipeline ID, and a description:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: You have now created a schedule that will automatically trigger your AutoML
    training pipeline once a day. This schedule will automatically spin up your compute
    cluster, train a model, and spin down. Many companies spend years trying to figure
    out how best to schedule ML training and scoring runs in a timely, reliable manner,
    and you've accomplished this task in a mere chapter!
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have now implemented a fully automated ML batch scoring solution using an
    AutoML trained model. You've created pipelines that can score models, pipelines
    that can process big data in parallel, and pipelines that can retrain AutoML models.
    You can trigger them whenever you want and you can even set up an automated scoring
    schedule. This is no small feat, as many organizations have spent years trying
    to learn best practices for these tasks.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B16595_10_ePub.xhtml#_idTextAnchor151), *Creating End-to-End
    AutoML Solutions*, you will cement your knowledge as you learn how to ingest data
    into Azure, score it with ML pipelines, and write your results to whatever location
    you want.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
