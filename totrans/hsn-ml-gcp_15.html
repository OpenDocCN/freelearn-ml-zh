<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>Nowadays, most computers are based on a symbolic elaboration. The problem is first encoded in a set of variables and then processed using an explicit algorithm that, for each possible input of the problem, offers an adequate output. However, there are problems in which resolution by an explicit algorithm is inefficient or even unnatural, for example, a speech recognizer; tackling this kind of problem with the classic approach is inefficient. This and other similar problems, such as autonomous navigation of a robot or voice assistance in performing an operation, are part of a very diverse set of problems that can be addressed directly through solutions based on reinforcement learning.</p>
<p>Reinforcement learning is based on a psychology <span>theory</span>, elaborated after a series of experiments performed on animals. Defining a goal to be achieved, reinforcement learning tries to maximize the rewards received for the execution of the action or set of actions that allow us to reach the designated goal. Reinforcement learning is a very exciting sector of machine learning, used in everything from autonomous cars to video games. It aims to create algorithms that can learn and adapt to environmental changes.</p>
<p>The topics covered in this chapter are:</p>
<ul>
<li>Reinforcement learning</li>
<li><strong>Markov Decision Process</strong> (<strong>MDP</strong>)</li>
<li>Q-learning</li>
<li><strong>Temporal difference</strong> (<strong>TD</strong>) learning</li>
<li>Deep Q-learning networks</li>
</ul>
<p>At the end of the chapter, you will be fully introduced to the power of reinforcement learning and will learn the different approaches to this technique. Several reinforcement learning methods will be covered.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning introduction</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning aims to create algorithms that can learn and adapt to environmental changes. This programming technique is based on the concept of receiving external stimuli depending on the algorithm choices. A correct choice will involve a premium while an incorrect choice will lead to a penalty. The goal of the system is to achieve the best possible result, of course.</p>
<p>In supervised learning, there is a teacher that tells the system which is the correct output (learning with a teacher). This is not always possible. Often we have only qualitative information (sometimes binary, right/wrong, or success/failure). The information available is called <strong>reinforcement signals</strong>. But the system does not give any information on how to update the agent's behavior (that is, weights). You cannot define a cost function or a gradient. The goal of the system is to create the smart agents that are able to learn from their experience.</p>
<p>The following is a flowchart that displays reinforcement learning interaction with the environment:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2b492b5e-a4d9-4cf9-b7e5-719a5f97220b.png" style=""/></div>
<p>Scientific literature has taken an uncertain stance on the classification of learning by reinforcement as a paradigm. In fact, in an initial phase it was considered as a special case of supervised learning, and then it was fully promoted as the third paradigm of machine learning algorithms. It is applied in different contexts in which supervised learning is inefficient; the problems of interaction with the environment are clear examples.</p>
<p>The following flow shows the steps to follow to correctly apply a reinforcement learning algorithm:</p>
<ol>
<li>Preparation of the agent</li>
<li>Observation of the environment</li>
<li>Selection of the optimal strategy</li>
<li>Execution of actions</li>
<li>Calculation of the corresponding reward (or penalty)</li>
<li>Development of updating strategies (if necessary)</li>
<li>Repeating steps 2-5 iteratively until the agent learns the optimal strategies</li>
</ol>
<p>Reinforcement learning is based on some theory of psychology, elaborated after a series of experiments performed on animals. In particular, <span>American psychologist </span>Edward Thorndike noted that if a cat is given a reward immediately after the execution of a behavior considered correct, it increases the probability that this behavior will repeat itself. While in the face of unwanted behavior, the application of a punishment decreases the probability of a repetition of error.</p>
<p>On the basis of this theory, and with a goal to be achieved <span>defined</span>, reinforcement learning tries to maximize the rewards received for execution of the action or set of actions that allow us to reach the designated goal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Agent-Environment interface</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning can be seen as a special case of the interaction problem for achieving a goal. The entity that must reach the goal is called an <strong>agent</strong>. The entity with which the agent must interact is called the <strong>environment</strong>, which corresponds to everything that is external to the agent.</p>
<p>So far, we have focused more on the term agent, but what does it represent? The agent is a software entity that performs services on behalf of another program, usually automatically and invisibly. These software are also called <strong>smart agents</strong>.</p>
<p>The following are the most important features of an agent:</p>
<ul>
<li>It can choose an action on the environment between a continuous and a discrete set</li>
<li>Action depends on the situation. The situation is summarized in the system state</li>
<li>The agent continuously monitors the environment (input) and continuously changes the status</li>
<li>The choice of action is not trivial and requires a certain degree of <strong>intelligence</strong></li>
<li>The agent has a smart memory</li>
</ul>
<p>The agent has a goal-directed behavior but acts in an uncertain environment not known a priori or partially known. An agent learns by interacting with the environment. Planning can be developed while learning about the environment through the measurements made by the agent itself. The strategy is close to trial-and-error theory.</p>
<div class="packt_tip">
<p>Trial and error is a fundamental method of problem solving. It is characterized by repeated, varied attempts that are continued until success, or until the agent stops trying.</p>
</div>
<p>The Agent-Environment interaction is continuous; the agent chooses an action to be taken, and in response, the environment changes states, presenting a new situation to be faced.</p>
<p>In the particular case of reinforcement learning, the environment provides the agent with a reward; it is essential that the source of the reward is the environment to avoid the formation of a personal reinforcement mechanism <span>within the agent </span><span>that would compromise learning.</span></p>
<p>The value of the reward is proportional to the influence that the action has in reaching the objective; so it is positive or high in the case of a correct <span>action, </span>or negative or low action for an incorrect action.</p>
<p>The following are some examples from real life in which there is an interaction between the agent and environment to solve the problem:</p>
<ul>
<li>A chess player, for each move, has information on the configurations of pieces that can create and on the possible countermoves of the opponent</li>
<li>A little giraffe learns to get up and run at 50 km/h in a few hours</li>
<li>A truly autonomous robot learns to move in a room to get out of it</li>
<li>The parameters of a refinery (oil pressure, flow, and so on) are set in real time so as to obtain the maximum yield or maximum quality</li>
</ul>
<p>All the examples we have analyzed have the following characteristics in common:</p>
<ul>
<li>Interaction with the environment</li>
<li>Objective of the agent</li>
<li>Uncertainty or partial knowledge of the environment</li>
</ul>
<p>From the analysis of these examples, it is possible to make the following observations:</p>
<ul>
<li>The agent learns from its own experience.</li>
<li>When the actions change the status (the situation), the possibilities of choices in the future change (delayed reward).</li>
<li>The effect of an action cannot be completely predicted.</li>
<li>The agent has a global assessment of it behavior.</li>
<li>It must exploit this information to improve his choices. Choices improve with experience.</li>
<li>Problems can have a finite or infinite time horizon.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov Decision Process</h1>
                </header>
            
            <article>
                
<p>To avoid load problems and computational difficulties, the Agent-Environment interaction is considered as a MDP. MDP is a discrete time stochastic control process. At each time step, the process is in a state <em>s</em>, and the decision maker may choose any action <em>a</em> that is available in state <em>s</em>. The process responds at the next time step by randomly moving into a new state <em>s' </em>and giving the decision maker a corresponding reward, <em>r(s,s')</em>.</p>
<p>Under these hypotheses, the Agent-Environment interaction can be schematized as follows:</p>
<ul>
<li>The agent and the environment interact at discrete intervals over time, <em>t = 0, 1, 2, … n</em>.</li>
<li>At each interval, the agent receives a representation of the state <em>st</em> of the environment.</li>
<li>Each element <em>s<sub>t</sub></em> of <em>S</em>, where <em>S</em> is the set of possible states.</li>
<li>Once the state is recognized, the agent must take an action a<sub>t</sub> of <em>A(s<sub>t</sub>)</em>, where <em>A(s<sub>t</sub>)</em> is the set of possible actions in the state <em>s<sub>t</sub></em>.</li>
<li>The choice of the action to be taken depends on the objective to be achieved and is mapped through the policy indicated with the symbol <em>π</em> (discounted cumulative reward), which associates the action with a<sub>t</sub> of <em>A(s)</em> for each state <em>s</em>. The term <em>π<sub>t</sub>(s,a)</em> represents the probability that action <em>a</em> is carried out in the state <em>s</em>.</li>
<li>During the next time interval <em>t + 1</em>, as part of the consequence of the action at, the agent receives a numerical reward <em>r<sub>t</sub> + 1</em> <em>R</em> corresponding to the action previously taken a<sub>t</sub>.</li>
<li>The consequence of the action represents, instead, the new state <em>s<sub>t</sub></em>. At this point, the agent must again code the state and make the choice of the action.</li>
<li>This iteration repeats itself until the achievement of the objective by the agent.</li>
</ul>
<p>The definition of the status <em>s<sub>t</sub> + 1</em> depends from the previous state and the action taken MDP, that is:</p>
<div class="CDPAlignCenter CDPAlign"><em>s<sub>t</sub> + 1 = δ (s<sub>t</sub>,a<sub>t</sub>)</em></div>
<p>Here <em>δ</em> represents the status function.</p>
<p>In summary:</p>
<ul>
<li>In an MDP, the agent can perceive the status <em>s S</em> in which he is and has an <em>A</em> set of actions at his disposal</li>
<li>At each discrete interval <em>t</em> of time, the agent detects the current status <em>st</em> and decides to implement an action at <em>A</em></li>
<li>The environment responds by providing a reward (a reinforcement) <em>r<sub>t</sub> = r (s<sub>t</sub>, a<sub>t</sub>)</em> and moving into the state <em>s<sub>t</sub> + 1 = δ (s<sub>t</sub>, a<sub>t</sub>)</em></li>
<li>The <em>r</em> and <em>δ</em> functions are part of the environment; they depend only on the current state and action (not the previous ones) and are not necessarily known to the agent</li>
<li>The goal of reinforcement learning is to learn a policy that, for each state <em>s</em> in which the system is located, indicates to the agent an action to maximize the total reinforcement received during the entire action sequence</li>
</ul>
<p>Let's go deeper into some of the terms used:</p>
<ul>
<li>A <strong>reward function</strong> defines the goal in a reinforcement learning problem. It maps the detected states of the environment into a single number, thus defining a reward. As already mentioned, the only goal is to maximize the total reward it receives in the long term. The reward function then defines what the good and bad events are for the agent. The reward function has the need to be correct, and it can be used as a basis for changing the policy. For example, if an action selected by the policy is followed by a low reward, the policy can be changed to select other actions in that situation in the next step.</li>
<li>A <strong>policy</strong> defines the behavior of the learning agent at a given time. It maps both the detected states of the environment and the actions to take when they are in those states. Corresponds to what in psychology would be called a <strong>set of rules</strong> or associations of stimulus response. Policy is the fundamental part of a reinforcing learning agent, in the sense that it alone is enough to determine behavior.</li>
<li>A <strong>value function</strong> represents how good a state is for an agent. It is equal to the total reward expected for an agent from the status <em>s</em>. The value function depends on the policy with which the agent selects the actions to be performed.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discounted cumulative reward</h1>
                </header>
            
            <article>
                
<p>In the previous section, we said this: the goal of reinforcement learning is to learn a policy that, for each state <em>s</em> in which the system is located, indicates to the agent an action to maximize the total reinforcement received during the entire action sequence. But how can we maximize the total reinforcement received during the entire sequence of actions?</p>
<p>The total reinforcement derived from the policy is calculated as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/2bce7ad4-a3fe-4ee6-92e6-560dff764b36.png" style="width:23.83em;height:4.83em;"/></div>
<p class="NormalPACKT"><span>Here, <em>r<sub>T</sub></em> represents the reward of the action that drives the environment in the terminal state <em>s<sub>T</sub></em>.</span></p>
<p class="NormalPACKT"><span>A possible solution to the problem is to associate the action that provides the highest reward to each individual state; that is, we must determine an optimal policy such that the previous quantity is maximized.</span></p>
<p class="NormalPACKT"><span>For problems that do not reach the goal or terminal state in a finite number of steps (continuing tasks), <em>R<sub>t</sub></em> tends to infinity.</span></p>
<p class="NormalPACKT"><span>In these cases, the sum of the rewards that one wants to maximize diverges at infinity, so this approach is not applicable. Then, it is necessary to develop an alternative reinforcement technique.</span></p>
<p class="NormalPACKT"><span>The technique that best suits the reinforcement learning paradigm turns out to be discounted cumulative reward, which tries to maximize the following quantity:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/94c8312c-b46e-461c-9180-85f557269910.png" style="width:24.58em;height:3.75em;"/></div>
<p>Here, <em>γ</em> is called <strong>discount factor</strong> and it represents the importance for future rewards. This parameter can take the values <em>0 ≤ γ ≤ 1</em>, with the following meanings:</p>
<ul>
<li>If <em>γ &lt;1</em>, the sequence <em>r<sub>t</sub></em> will converge to a finite value</li>
<li>If <em>γ = 0</em>, the agent will have no interest in future rewards, but will try to maximize the reward only for the current state</li>
<li>If <em>γ = 1</em>, the agent will try to increase future rewards even at the expense of immediate ones</li>
</ul>
<p>The discount factor can be modified during the learning process to highlight or not particular actions or states. An optimal policy can cause the reinforcement obtained in performing a single action to be even low (or even negative), provided that overall this leads to greater reinforcement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploration versus exploitation</h1>
                </header>
            
            <article>
                
<p>Ideally, the agent must associate with each action at the respective reward <em>r</em> in order to then choose the most rewarded behavior for achieving the goal. This approach is therefore impracticable for complex problems, in which the number of states is particularly high and consequently the possible associations increase exponentially.</p>
<p>This problem is called the <strong>exploration-exploitation dilemma</strong>. Ideally, the agent must explore all possible actions for each state, finding the one that is actually most rewarded for exploiting it in achieving its goal.</p>
<p>Thus, decision-making involves a fundamental choice:</p>
<ul>
<li><strong>Exploitation</strong>: Make the best decision given current information</li>
<li><strong>Exploration</strong>: Collect more information</li>
</ul>
<p>In this process, the best long-term strategy can lead to considerable sacrifices in the short term. Therefore, it is necessary to gather enough information to make the best decisions.</p>
<p>Here are some examples of adopting this technique for real-life cases:</p>
<p><strong>Selection of a store</strong>:</p>
<ul>
<li><strong>Exploitation</strong>: Go to your favorite store</li>
<li><strong>Exploration</strong>: Try a new store</li>
</ul>
<p><strong>Choice of a route</strong>:</p>
<ul>
<li><strong>Exploitation</strong>: Choose the best route so far</li>
<li><strong>Exploration</strong>: Try a new route</li>
</ul>
<p>In practice, in very complex problems, convergence to a very good strategy would be too slow.</p>
<p>A good solution to the problem is to find a balance between exploration and exploitation:</p>
<ul>
<li>An agent who limits himself to exploring will always act in a casual way in every state, and it is evident that convergence to an optimal strategy is impossible</li>
<li>If an agent explores little, it will always use the usual actions, which may not be optimal ones</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning techniques</h1>
                </header>
            
            <article>
                
<p>As we have seen in the previous sections, reinforcement learning is a programming philosophy that aims to develop algorithms that can learn and adapt to changes in the environment. This programming technique is based on the assumption of being able to receive stimuli from the outside according to the choices of the algorithm. So, a correct choice will result in a prize while an incorrect choice will lead to a penalization of the system. The goal of the system is to achieve the highest possible prize and consequently the best possible result. The techniques related to learning by reinforcement are divided into two categories:</p>
<ul>
<li><strong>Continuous learning algorithms</strong>: These techniques start from the assumption of having a simple mechanism able to evaluate the choices of the algorithm and then reward or punish the algorithm depending on the result. These techniques can also adapt to substantial changes in the environment. An example is speech recognition programs or OCR programs that improve their performance with use.</li>
<li><strong>Preventive training algorithms</strong>: These algorithms start from the observation that constantly evaluating the actions of the algorithm can be a process that cannot be automated or very expensive. In this case, a first phase is applied, in which the algorithm is taught; when the system is considered reliable, it is crystallized and no more editable. Many electronic components use neural networks within them, and the synaptic weights of these networks are not changeable since they are fixed during the construction of the circuit.</li>
</ul>
<p>It should be noted that the categories mentioned <span>previously</span> are implementation choices rather than conceptual differences in the algorithm. Therefore, an algorithm can <span>often</span> be in the first or second category depending on how it is implemented by the designer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning</h1>
                </header>
            
            <article>
                
<p>Q-learning is one of the most-used reinforcement learning algorithms. This is due to its ability to compare the expected utility of the available actions without requiring an environment model.</p>
<p>Thanks to this technique, it is possible to find an optimal action for every given state in a finished MDP.</p>
<p>A general solution to the reinforcement learning problem is to estimate, thanks to the learning process, an evaluation function. This function must be able to evaluate, through the sum of the rewards, the convenience or otherwise of a particular policy. In fact, Q-learning tries to maximize the value of the <em>Q</em> function (action-value function), which represents the maximum discounted future reward when we perform actions <em>a</em> in state <em>s</em>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Q(S<sub>t</sub>,a<sub>t</sub>) = max(R<sub>t+1</sub>)</em></p>
<p class="NormalPACKT"><span>Knowing the <em>Q</em> function, the optimal action <em>a</em> in a state <em>s</em> is the one with the highest <em>Q</em> value. At this point, we can define a policy <em>π(s)</em> that provides us with the best action in any state. Recalling that the policy <em>π</em> associates the pair <em>(s; a)</em> with the probability <em>(s; a)</em> that action is carried out in the state <em>s</em>, we can write the following:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/0bf8dd3d-0c65-4b9a-809a-75214b472d33.png" style="width:17.08em;height:1.83em;"/></div>
<p class="NormalPACKT"><span>The problem is reduced to the evaluation of the <em>Q</em> function. We can then estimate the <em>Q</em> function for a transition point in terms of the <em>Q</em> function at the next point through a recursive process. The following is the equation used in a single step of the process. This equation is known as <strong>Bellman's equation</strong> and represents the transition rule of Q-learning:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/4e95528e-5650-490d-8dce-f82aeb359907.png" style="width:26.17em;height:2.75em;"/></div>
<p>The terms are defined as follows:</p>
<ul>
<li><em>Q(s<sub>t</sub>,a<sub>t</sub>)</em> is the current policy of action <em>a</em> from state <em>s.</em></li>
<li><em>r</em> is the reward for the action.</li>
<li><em>max<sub>t+1</sub>(Q(s<sub>t+1</sub>,a<sub>t+1</sub>))</em> defines the maximum future reward. We performed the <em>a<sub>t</sub></em> action to state <em>s<sub>t</sub></em> to reach the <em>s<sub>t+1</sub></em> state. From here, we may have multiple actions, each corresponding to some rewards. The maximum of that reward is computed.</li>
<li><em>γ</em> is the discount factor. The <em>γ</em> value varies from 0 to 1; if the value is near 0, an immediate reward is given preference. If it goes near 1, the importance of future rewards increases until 1, where it is considered equal to immediate rewards.</li>
</ul>
<p class="mce-root">On the basis of the previous equation, the evaluation function <em>Q</em> is given by the sum of the immediate reward and the maximum reward obtainable starting from the next state.</p>
<p class="mce-root">Applying the previous formula, we are trying to formulate the delayed rewards into immediate rewards. We have previously said that the evaluation of the <em>Q</em> function represents a recursive process. We can then enter the values obtained during this process in a table that we will, of course, call table <em>Q</em>. In this table, the rows are the states and the columns are the actions. As a starting table <em>Q</em>, we can use a matrix containing all zeros (we have initialized table <em>Q</em>), as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/31e9ee64-1d2c-48d9-a410-2db4756872d1.png" style=""/></div>
<p>The elements of this table <em>Q</em> (cells) are the rewards that are obtained if one is in the state given by the row and the action given by the column is executed. The best action to take in any state is the one with the highest reward. Our task now is to update this table with new values. To do this, we can adopt the following algorithm:</p>
<ol>
<li>The status <em>s<sub>t</sub></em> is decoded</li>
<li>An action <em>a<sub>t</sub></em> is selected</li>
<li>Action <em>a<sub>t</sub></em> is performed and the reward <em>r</em> is received</li>
<li>The element of table <em>Q(s<sub>t</sub>; a<sub>t</sub>)</em> is updated with the training rule provided by Bellman's equation</li>
<li>The execution of the action a moves the environment in the state <em>s<sub>t+1</sub></em></li>
<li>Set the next state as the current state (<em>s<sub>t</sub> = s<sub>t+1</sub></em>)</li>
<li>Start again from point 1 and repeat the process until a terminal state is reached</li>
</ol>
<p>In more complex and efficient formulations, it is possible to replace the table, whose iteration is still inefficient for complex problems, with a neural network where the learning process will change the weights of the synaptic connections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Temporal difference learning</h1>
                </header>
            
            <article>
                
<p>TD learning algorithms are based on reducing the differences between estimates made by the agent at different times. Q-learning, seen in the previous section, is a TD algorithm, but it is based on the difference between states in immediately adjacent instants. TD is more generic and may consider moments and states further away.</p>
<p>It is a combination of the ideas of the <strong>Monte Carlo</strong> (<strong>MC</strong>) method and the <strong>Dynamic Programming</strong> (<strong>DP</strong>).</p>
<div class="packt_tip">
<p>MC methods allow solving reinforcement learning problems based on the average of the results obtained.</p>
<p>DP represents a set of algorithms that can be used to calculate an optimal policy given a perfect model of the environment in the form of an MDP.</p>
</div>
<p>A TD algorithm can learn directly from raw data, without a model of the dynamics of the environment (such as MC). This algorithm updates the estimates based partly on previously learned estimates, without waiting for the final result (bootstrap, such as DP). It is suitable for learning without a model of dynamic environments. Converge using a fixed policy if the time step is sufficiently small, or if it reduces over time.</p>
<p>As we saw in the previous section, Q-learning calculates its values according to the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/23d2a832-5cd5-4ef2-b9d9-de942401f24b.png" style="width:26.17em;height:2.75em;"/></div>
<p>By adopting a one-step look-ahead.</p>
<div class="packt_tip">
<p>Look-ahead is the generic term for a procedure that attempts to foresee the effects of choosing a branching variable to evaluate one of its values. The two main aims of look-ahead are to choose a variable to evaluate next and the order of values to assign to it.</p>
</div>
<p>It is clear that a two-step formula can also be used, as shown in the following line:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/dcdee0c9-9322-4a98-89ad-1cadf49d66e6.png" style="width:30.75em;height:2.58em;"/></div>
<p class="NormalPACKT"><span>More generally with n-step look-ahead, we obtain the following formula:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/ad4e7e0d-46c9-493b-b774-b8b5d7b84fe1.png" style="width:40.25em;height:2.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dynamic Programming</h1>
                </header>
            
            <article>
                
<p>DP represents a set of algorithms that can be used to calculate an optimal policy given a perfect model of the environment in the form of a MDP. The fundamental idea of DP, as well as reinforcement learning in general, is the use of state values and actions, to look for good policies.</p>
<p>The DP methods approach the resolution of Markov decision-making processes through the iteration of two processes called <strong>policy evaluation</strong> and <strong>policy improvement</strong>.</p>
<ul>
<li>Policy evaluation algorithm consists in applying an iterative method to the resolution of the Bellman equation. Since convergence is guaranteed to us only for <em>k → ∞</em>, we must be content to have good approximations by imposing a stopping condition.</li>
<li>Policy improvement algorithm improves policy based on current values.</li>
</ul>
<p>A disadvantage of the policy iteration algorithm is that we have to evaluate a policy <span>at every step</span>. This involves an iterative process whose <span>time of convergence</span> <span>we do not know a priori. This will depend on, among other things, how the starting policy was chosen.</span></p>
<p class="mce-root">One way to overcome this drawback is to cut off the evaluation of the policy at a specific step. This operation does not change the guarantee of convergence to the optimal value. A special case in which the assessment of the policy is blocked by a step by state (also called <strong>sweep</strong>) defines the value iteration algorithm. In the value iteration algorithm, a single iteration of calculation of the values is performed between each step of the policy improvement.</p>
<p class="mce-root">The DP algorithms are therefore essentially based on two processes that take place in parallel: policy evaluation and policy improvement. The repeated execution of these two processes makes the general process converge towards the optimal solution. In the policy iteration algorithm the two phases alternate and each ends before the other begins.</p>
<p>DP methods operate through the entire set of states that can be assumed by the environment, performing a complete backup for each state at each iteration. Each update operation performed by the backup updates the value of a status based on the values ​​of all possible successor states, weighted for their probability of occurrence and induced by the policy of choice and dynamics of the environment. Full backups are closely related to the Bellman equation; they are nothing more than the transformation of the equation into assignment instructions.</p>
<p>When a complete backup iteration does not bring any change to the state values, convergence is obtained; therefore the final state values ​​fully satisfy the Bellman equation. The DP methods are applicable only if there is a perfect model of the alternator, which must be equivalent to a MDP.</p>
<p>Precisely for this reason, the DP algorithms are of little use in reinforcement learning, both for their assumption of a perfect model of the environment, and for the high and expensive computation, but it is still opportune to mention them because they represent the theoretical basis of reinforcement learning. In fact, all the methods of reinforcement learning try to achieve the same goal of the DP methods, only with lower computational cost and without the assumption of a perfect model of the environment.</p>
<p>The DP methods converge to the optimal solution with a number of polynomial operations with respect to the number of states <em>n</em> and actions <em>m</em>, against the number of exponential operations <em>m*n</em> required by methods based on direct search.</p>
<p>The DP methods update the estimates of the values of the states, based on the estimates of the values of the successor states; or they update the estimates on the basis of past estimates. This represents a special property, which is called <strong>bootstrapping</strong>. Several methods of reinforcement learning perform bootstrapping, even methods that do not require a perfect model of the environment, as required by the DP methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo methods</h1>
                </header>
            
            <article>
                
<p>MC methods for estimating the value function and discovering excellent policies do not require the presence of a model of the environment. They are able to learn through the use of the agent's experience alone or from samples of state sequences, actions, and rewards obtained from the interactions between agent and environment. The experience can be acquired by the agent in line with the learning process or emulated by a previously populated dataset. The possibility of gaining experience during learning (online learning) is interesting because it allows obtaining excellent behavior even in the absence of a priori knowledge of the dynamics of the environment. Even learning through an already populated experience dataset can be interesting, because if combined with online learning, it makes automatic policy improvement induced by others' experiences possible.</p>
<p>To solve the reinforcement learning problems, MC methods estimate the value function on the basis of the total sum of rewards, obtained on average in the past episodes. This assumes that the experience is divided into episodes, and that all episodes are composed of a finite number of transitions. This is because in MC methods, only once an episode is completed takes place the estimate of the new values ​​and the modification of the policy. MC methods iteratively estimate policy and value function. In this case, however, each iteration cycle is equivalent to completing an episode—the new estimates of policy and value function occur episode by episode.</p>
<p>Usually the term MC is used for estimation methods, which operations involve random components; in this case, MC refers to reinforcement learning methods based on total reward averages. Unlike the DP methods that calculate the values ​​for each state, the MC methods calculate the values ​​for each state-action pair, because in the absence of a model, only state values ​​are not sufficient to decide which action is best performed in a certain state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep Q-Network</h1>
                </header>
            
            <article>
                
<p><strong>Deep Q-Network</strong> (<strong>DQN</strong>) algorithms combine both the reinforcement learning approach and the deep learning approach. DQN learns by itself, learning in an empirical way and without a rigid programming aimed at a particular objective, such as winning a game of chess.</p>
<p>DQN represents an application of Q-learning with the use of deep learning for the approximation of the evaluation function. The DQN was proposed by Mnih et al. through an article published in <em>Nature</em> on February <span>26,</span> 2015. As a consequence, a lot of research institutes joined this field, because deep neural networks can empower reinforcement learning algorithms to directly deal with high-dimensional states.</p>
<p>The use of deep neural networks is due to the fact that researchers noted the following: using a neural network to approximate the <strong>Q-evaluation</strong> function in algorithms with reinforcement learning made the system unstable or divergent. In fact, it is possible to notice that small updates to <em>Q</em> can significantly change the policy, distribution of data, and correlations between <em>Q</em> and target values. These correlations, present in the sequence of observations, are the cause of the instability of the algorithms.</p>
<p>To transform a normal Q-network into a DQN, it is necessary to carry out the following precautions:</p>
<ul>
<li>Replace the single-level neural network with a multi-level convolutional network for approximation of the Q-function evaluation</li>
<li>Implement the experience replay</li>
<li>Use a second network to calculate the target Q-values during your updates</li>
</ul>
<p>What is meant by the term <strong>experience replay</strong>? This means that, instead of running Q-learning on state/action pairs as they occur during a simulation or actual experience, the system stores the data discovered, typically in a large table. In this way, our network can train itself using stored memories from its experience.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenAI Gym</h1>
                </header>
            
            <article>
                
<p><strong>OpenAI Gym</strong> is a library that helps us to implement algorithms based on reinforcement learning. It includes a growing collection of benchmark issues that expose a common interface, and a website where people can share their results and compare algorithm performance.</p>
<p>OpenAI Gym focuses on the episodic setting of reinforced learning. In other words, the agent's experience is divided into a series of episodes. The initial state of the agent is randomly sampled by a distribution, and the interaction proceeds until the environment reaches a terminal state. This procedure is repeated for each episode, with the aim of maximizing the total reward expectation per episode and achieving a high level of performance in the fewest possible episodes.</p>
<div class="packt_tip">
<p>Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports the ability to teach agents everything from walking to playing games such as Pong or Pinball. The library is available at the following URL:</p>
<p><a href="https://gym.openai.com/" target="_blank">https://gym.openai.com/</a></p>
</div>
<p>The following figure shows the home page of the OpenAI Gym project site:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f87cdac-c992-470e-8e3e-1301f466cbe7.png" style=""/></div>
<div>
<p>OpenAI Gym is part of a much more ambitious project: the OpenAI project. OpenAI is an <strong>artificial intelligence</strong> (<strong>AI</strong>) research company founded by Elon Musk and Sam Altman. It is a non-profit project that aims to promote and develop friendly AI in such a way as to benefit humanity as a whole. The organization aims to collaborate freely with other institutions and researchers by making their patents and research open to the public. The founders decided to undertake this project as they were concerned by the existential risk deriving from the indiscriminate use of AI.</p>
<p>OpenAI Gym is a library of programs that allow you to develop AIs, measure their intellectual abilities ,and enhance their learning abilities. In short, a Gym in the form of algorithms that trains the present digital brains to OpenAI Gym project them into the future.</p>
<p>But there is also another <span>goal</span>. OpenAI wants to stimulate research in the AI ​​sector by funding projects that make humanity progress even in those fields where there is no economic return. With Gym, on the other hand, it intends to standardize the measurement of AI so that researchers can compete on equal terms and know where their colleagues have come but, above all, focus on results that are really useful for everyone.</p>
<p>The tools available are many. From the ability to play old video games like Pong to that of fighting in the GO to control a robot, we just enter our algorithm in this digital place to see how it works. The second step is to compare the benchmarks obtained with the other ones to see where we stand compared to others, and maybe we can collaborate with them to get mutual benefits.</p>
<p>OpenAI Gym makes no assumptions about the structure of our agent and is compatible with any numerical computation library, such as TensorFlow or Theano. The Gym library is a collection of test problems—environments—that we can use to work out our reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.</p>
<p>To install OpenAI Gym, make sure you have previously installed a Python 3.5+ version; then simply type the following command:</p>
<pre><strong>pip install gym</strong></pre>
<p>Once this is done, we will be able to insert the tools made available by the library in a simple and immediate way.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cart-Pole system</h1>
                </header>
            
            <article>
                
<p>The Cart-Pole system is a classic problem of reinforced learning. The system consists of a pole (which acts like an inverted pendulum) attached to a cart via a joint, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7f3ae859-2e08-4728-813d-8d0e2b489e6c.png" style=""/></div>
<p>The system is controlled by applying a force of +1 or -1 to the cart. The force applied to the cart can be controlled, and the objective is to swing the pole upwards and stabilize it. This must be done without the cart falling to the ground. At every step, the agent can choose to move the cart left or right, and it receives a reward of 1 for every time step that the pole is balanced. If the pole ever deviates by more than 15 degrees from upright, then the procedure ends.</p>
<p>To run the Cart-Pole example using the OpenAI Gym library, simply type the following code:</p>
<pre>import gym<br/>env = gym.make('CartPole-v0')<br/>env.reset()<br/>for i in range(1000):<br/>    env.render()<br/>    env.step(env.action_space.sample())</pre>
<p>As always, we will explain the meaning of each line of code in detail. The first line is used to import the <kbd>gym</kbd> library:</p>
<pre>import gym</pre>
<p>Then we move on to create the environment by calling the <kbd>make</kbd> method:</p>
<pre>env = gym.make('CartPole-v0')</pre>
<p>This method creates the environment that our agent will run in. An environment is a problem with a minimal interface that an agent can interact with. The environments in OpenAI Gym are designed in order to allow objective testing and benchmarking of an agent's abilities. The Gym library comes with a diverse suite of environments that range from easy to difficult and involve many different kinds of data.</p>
<div class="packt_tip">
<p>For a list of the available environments, refer to the following link:</p>
<p><a href="https://gym.openai.com/envs" target="_blank">https://gym.openai.com/envs</a></p>
</div>
<p>The most used <span>environments </span>are listed here:</p>
<ul>
<li><strong>Classic control and toy text</strong>: Complete small-scale tasks, mostly from the reinforcement learning literature. They're here to get you started.</li>
<li><strong>Algorithmic</strong>: Perform computations such as adding multi-digit numbers and reversing sequences.</li>
<li><strong>Atari</strong>: Play classic Atari games.</li>
<li><strong>2D and 3D robots</strong>: Control a robot in simulation.</li>
</ul>
<p>In our case we have called the CartPole-v0 environment. The <kbd>make</kbd> method returns an <kbd>env</kbd> object that we will use to interact with the game. But let's go back to analyzing the code. Now we have to initialize the system using the <kbd>reset()</kbd> method:</p>
<pre>env.reset()</pre>
<p>This method puts the environment into its initial state, returning an array that describes it. At this point, we will use a <kbd>for</kbd> loop to run an instance of the CartPole-v0 environment for <kbd>1000</kbd> time steps, rendering the environment at each step:</p>
<pre>for i in range(1000):<br/>    env.render()<br/>    env.step(env.action_space.sample())</pre>
<p>Calling the <kbd>render()</kbd> method will visually display the current state, while subsequent calls to <kbd>env.step()</kbd> will allow us to interact with the environment, returning the new states in response to the actions with which we call it.</p>
<p>In this way, we have adopted random actions at each step. At this point, it is certainly useful to know what actions we are doing on the environment to decide future actions. The <kbd>step()</kbd> method returns exactly this. In effect, this method returns the following four values:</p>
<ul>
<li><kbd>observation (object)</kbd>: An environment-specific object representing your observation of the environment.</li>
<li><kbd>reward (float)</kbd>: Amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.</li>
<li><kbd>done (boolean)</kbd>: Whether it's time to reset the environment again. Most (but not all) tasks are divided into well-defined episodes, and <kbd>done</kbd> being <kbd>True</kbd> indicates the episode has terminated.</li>
<li><kbd>info (dict)</kbd>: Diagnostic information useful for debugging. It can sometimes be useful for learning.</li>
</ul>
<p>To run this simple example, save the code in a file named <kbd>cart.py</kbd> and type the following command at the bash window:</p>
<pre><strong>python cart.py</strong></pre>
<p>In this way, a window will be displayed containing our system that is not stable and will soon go out of the screen. This is because the push to the cart is given randomly, without taking into account the position of the pole.</p>
<p>To solve the problem, that is, to balance the pole, it is therefore necessary to set the push in the opposite direction to the inclination of the pole. So, we have to set only two actions, -1 or +1, pushing the cart to the left or the right. But in order to do so, we need to know at all times the data deriving from the observation of the environment. As we have already said, these pieces of data are returned by the <kbd>step()</kbd> method, in particular they are contained in the observation object.</p>
<p>This object contains the following parameters:</p>
<ul>
<li>Cart position</li>
<li>Cart velocity</li>
<li>Pole angle</li>
<li>Pole velocity at tip</li>
</ul>
<p>These four values become the input of our problem. As we have also anticipated, the system is balanced by applying a push to the cart. There are two possible options:</p>
<ul>
<li>Push the cart to the left (0)</li>
<li>Push it to the right (1)</li>
</ul>
<p>It is clear that this is a binary classification problem: four inputs and a single binary output.</p>
<p>Let us first consider how to extract the values to be used as input. To extract these parameters, we just have to change the preceding proposed code:</p>
<pre>import gym<br/>env = gym.make('CartPole-v0')<br/>observation = env.reset()<br/>for i in range(1000):<br/>    env.render()<br/>    print(observation)<br/>    observation, reward, done, info = env.step(env.action_space.sample())</pre>
<p>By running the code, we can see that the values contained in the observation object are now printed on the screen. All this will be useful soon.</p>
<p>Using values returned from the environment observations, the agent has to decide on one of two possible actions: to move the cart left or right.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning phase</h1>
                </header>
            
            <article>
                
<p>Now we have to face the most demanding phase, namely the training of our system. In the previous section, we said that the Gym library is focused on the episodic setting of reinforced learning. The agent's experience is divided into a series of episodes. The initial state of the agent is randomly sampled by a distribution and the interaction proceeds until the environment reaches a terminal state. This procedure is repeated for each episode with the aim of maximizing the total reward expectation per episode and achieving a high level of performance in the fewest possible episodes.</p>
<p>In the learning phase, we must estimate an evaluation function. This function must be able to evaluate, through the sum of the rewards, the convenience or otherwise of a particular policy. In other words, we must approximate the evaluation function. How can we do? One solution is to use an artificial neural network as a function approximator.</p>
<p>Recall that the training of a neural network aims to identify the weights of the connections between neurons. In this case, we will choose random values with weights for each episode. At the end, we will choose the combination of weights that will have collected the maximum reward.</p>
<p>The state of the system at a given moment is returned to us by the observation object. To choose an action from the actual state, we can use a linear combination of the weights and the observation. This is one of the most important special cases of function approximation, in which the approximate function is a linear function of the weight vector <em>w</em>. For every state <em>s</em>, there is a real-valued vector <em>x(s)</em> with the same number of components as <em>w</em>. Linear methods approximate the state-value function by the inner product between <em>w</em> and <em>x(s)</em>.</p>
<p>In this way, we have specified the methodology that we intend to adopt for the solution of the problem. Now, to make the whole training phase easily understandable, we report the whole code block and then comment on it in detail on a line-by-line basis:</p>
<pre>import gym<br/>import numpy as np<br/>env = gym.make('CartPole-v0')<br/>HighReward = 0<br/>BestWeights = None<br/>for i in range(200):<br/>  observation = env.reset()<br/>  Weights = np.random.uniform(-1,1,4)<br/>  SumReward = 0<br/>  for j in range(1000):<br/>    env.render()<br/>    action = 0 if np.matmul(Weights,observation) &lt; 0 else 1<br/>    observation, reward, done, info = env.step(action)<br/>    SumReward += reward<br/>    print( i, j, Weights, observation, action, SumReward, BestWeights)<br/>   <br/>  if SumReward &gt; HighReward:<br/>    HighReward = SumReward<br/>    BestWeights = Weights</pre>
<p>The first part of the code deals with importing the libraries:</p>
<pre>import gym<br/>import numpy as np</pre>
<p>Then we move on to create the environment by calling the <kbd>make</kbd> method:</p>
<pre>env = gym.make('CartPole-v0')</pre>
<p>This method creates the environment that our agent will run in. Now let's initialize the parameters we will use:</p>
<pre>HighReward = 0<br/>BestWeights = None</pre>
<p><kbd>HighReward</kbd> will contain the maximum reward obtained up to the current episode; this value will be used as a comparison value. <kbd>BestWeights</kbd> will contain the sequence of weights that will have registered the maximum reward. We can now implement the best weight sequence search through an iterative procedure for episodes:</p>
<pre>for i in range(200):</pre>
<p>We decide to execute the procedure <kbd>200</kbd> times, so we initialize the system using the <kbd>reset()</kbd> method:</p>
<pre>  observation = env.reset() </pre>
<p>In each episode, we use a sequence of weights <span>equal</span> <span>in number to the observations of the environment, which as previously said is four (cart position, cart velocity, pole angle, and pole velocity at tip):</span></p>
<pre>Weights = np.random.uniform(-1,1,4)</pre>
<p class="mce-root">To fix the weights, we have used the <kbd>np.random.uniform()</kbd> function. This function draws samples from a uniform distribution. Samples are uniformly distributed over the half-open interval (low and high). It includes low but excludes high.</p>
<p>In other words, any value within the given interval is equally likely to be drawn by <span>a uniform distribution</span>. Three parameters have been passed: the lower boundary of the output interval, its upper boundary, and the output shape. In our case we requested four random values in the interval <kbd>(-1,1)</kbd>. After doing this we initialize the sum of the rewards:</p>
<pre>SumReward = 0</pre>
<p>At this point, we implement another iterative cycle to determine the maximum reward we can get with these weights:</p>
<pre>for j in range(1000):</pre>
<p>Calling the <kbd>render()</kbd> method will visually display the current state:</p>
<pre>env.render()</pre>
<p>Now, we have to decide the <kbd>action</kbd>:</p>
<pre>action = 0 if np.matmul(Weights,observation) &lt; 0 else 1</pre>
<p>As we said, to decide the action we have used a linear combination of two vectors: <kbd>weights</kbd> and <kbd>observation</kbd>. To perform a linear combination, we have used the <kbd>np.matmul()</kbd> function; it implements matrix product of two arrays. So, if this product is <kbd>&lt;0</kbd>, then <kbd>action</kbd> is 0 (move left); otherwise, <kbd>action</kbd> is 1 (move right).</p>
<div>
<p>It should be noted that a negative product means that the pole is tilted to the left, so in order to balance this trend, it is necessary to push the cart towards the left. A <span>positive product means the pole is tilted to the right, so in order to balance this trend, it is necessary to push the cart towards the right.</span></p>
</div>
<p>Now we use the <kbd>step()</kbd> method to return the new states in response to the actions with which we call it. Obviously, the action we pass to the method is the one we have just decided:</p>
<pre>observation, reward, done, info = env.step(action)</pre>
<p>As we said, this method returns the following four values:</p>
<ul>
<li><kbd>observation</kbd> (<kbd>object</kbd>): An environment-specific object representing your observation of the environment.</li>
<li><kbd>reward</kbd> (<kbd>float</kbd>): The amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.</li>
<li><kbd>done</kbd> (<kbd>boolean</kbd>): Whether it's time to reset the environment again. Most (but not all) tasks are divided into well-defined episodes, and <kbd>done</kbd> being <kbd>True</kbd> indicates that the episode has terminated.</li>
<li><kbd>info</kbd> (<kbd>dict</kbd>): Diagnostic information useful for debugging. It can sometimes be useful for learning.</li>
</ul>
<p>We can then update the sum of the rewards with the one just obtained. Remember that, for every time step where we keep the pole straight, we get +1 <kbd>reward</kbd>:</p>
<pre>SumReward += reward</pre>
<p>We just have to print the values obtained in this step:</p>
<pre>print( i, j, Weights, observation, action, SumReward, BestWeights)</pre>
<p>At the end of the current iteration, we can make a comparison to check whether the total reward obtained is the highest one obtained so far:</p>
<pre>if SumReward &gt; HighReward:</pre>
<p>If it is the highest reward obtained so far, update the <kbd>HighReward</kbd> parameter with this value:</p>
<pre>HighReward = SumReward</pre>
<p>Once this is done, fix the sequence of <kbd>Weights</kbd> of the current step as the best one:</p>
<pre>BestWeights = Weights</pre>
<p>With this instruction, the training phase ends, which will give us the sequence of weights that best approximate the evaluation function. We can now test the system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing phase</h1>
                </header>
            
            <article>
                
<p>When the training phase is achieved, in practice it means that we have found the sequence of weights that best approximates this function, that is, the one that has returned the best reward achievable. Now we have to test the system with these values to check whether the pole is able to stand for at least <kbd>100</kbd> time steps.</p>
<p>Now, as we are already done in the training phase, to make the whole testing phase easily understandable, we report the whole code block and then comment on it in detail on a line-by-line basis:</p>
<pre>observation = env.reset()<br/>for j in range(100):<br/>  env.render()<br/>  action = 0 if np.matmul(BestWeights,observation) &lt; 0 else 1<br/>  observation, reward, done, info = env.step(action)<br/>  print( j, action)</pre>
<p>First, we have to initialize the system once again, using the <kbd>reset()</kbd> method:</p>
<pre>observation = env.reset() </pre>
<p>Then, we have to run an iterative cycle to apply the results obtained in the training phase:</p>
<pre>for j in range(100):</pre>
<p>For each step, we will call the <kbd>render()</kbd> method to visually display the current state:</p>
<pre>env.render()</pre>
<p>Now, we have to decide the action to perform on the system based on the best weights obtained in the training phase and on the observations of the current state:</p>
<pre>action = 0 if np.matmul(BestWeights,observation) &lt; 0 else 1</pre>
<p>Now we use the <kbd>step()</kbd> method that returning the new states in response to the actions with which we call it. The action passed to the method is the one we have just decided:</p>
<pre>observation, reward, done, info = env.step(action)</pre>
<p>Finally, we print the step number and the action decided for visual control of the flow.</p>
<p>By running the proposed code, we can verify that after the training phase, the system is able to keep the pole in equilibrium for 100 time steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning aims to create algorithms that can learn and adapt to environmental changes. This programming technique is based on the concept of receiving external stimuli depending on the algorithm choices. A correct choice will involve a premium, while an incorrect choice will lead to a penalty. The goal of system is to achieve the best possible result, of course. In this chapter, we dealt with the basics of reinforcement learning.</p>
<p>To begin with, we saw that the goal of learning with reinforcement is to create intelligent agents that are able to learn from their experience. So we analyzed the steps to follow to correctly apply a reinforcement learning algorithm. Later we explored the Agent-Environment interface. The entity that must achieve the goal is called an <strong>agent</strong>. The entity with which the agent must interact is called the <strong>environment</strong>, which corresponds to everything outside the agent.</p>
<p>To avoid load problems and computational difficulties, the Agent-Environment interaction is considered an MDP. An MDP is a stochastic control process. Then the discount factor concept was introduced. The discount factor can be modified during the learning process to highlight or not <span>highlight</span> particular actions or states. An optimal policy can cause the reinforcement obtained in performing a single action to be even low (or negative), provided that overall this leads to greater reinforcement.</p>
<p>In the central part of the chapter, were dedicated to the analysis of the most common reinforcement learning techniques. Q-learning, TD learning, and deep Q-learning networks were covered. Finally, we explored the OpenAI Gym libraries and tackled the analysis of a practical example of reinforcement learning.</p>


            </article>

            
        </section>
    </body></html>