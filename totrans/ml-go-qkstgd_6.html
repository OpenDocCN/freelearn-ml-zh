<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Machine Learning Applications</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we learned how to create an application that can prepare data (<a href="532d8304-b31d-41ef-81c1-b13f4c692824.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up the Development Environment</em>) for either a supervised (<a href="48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml" target="_blank">Chapter 3</a>, <em>Supervised Learning</em>) or unsupervised (<a href="26788e93-3614-413f-bcde-5580516f9c5f.xhtml" target="_blank">Chapter 4</a>, <em>Unsupervised Learning</em>) ML algorithm. We also learned how to evaluate and test the output of these algorithms with the added complication that we have incomplete knowledge about the algorithm's inner state and workings, and must therefore treat it as a black box. In <a href="815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml" target="_blank">Chapter 5</a>, <em>Using Pre-Trained Models,</em> we looked at model persistence and how Go applications can leverage models written in other languages. Together, the skills you have learned so far constitute the fundamentals required to successfully prototype ML applications. In this chapter, we will look at how to prepare your prototype for commercial readiness, focusing on aspects specific to ML applications. </p>
<p class="mce-root">In this chapter, you will cover the following topics:</p>
<ul>
<li>The continuous delivery feedback loop, including how to test, deploy, and monitor ML applications</li>
<li>Deployment models for ML applications</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The continuous delivery feedback loop</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Continuous delivery</strong> (<strong>CD</strong>) is the practice of using short feedback loops in the software development life cycle to ensure that the resulting application can be released at any moment in time<sup>[1]</sup>. While there are alternative approaches to release management, we will only consider this one because creating a meaningful, short—and therefore automated—feedback loop with ML applications presents unique challenges that are not created by alternative methodologies that may not require this degree of automation. </p>
<p class="mce-root">The CD feedback loop consists of the following process:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-206 image-border" src="assets/c4985c73-3a46-4ab8-99de-d63683cbf9f6.png" style="width:15.08em;height:12.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Fig. 1: The continuous delivery feedback loop</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing</h1>
                </header>
            
            <article>
                
<p>The development portion of the feedback loop is what we have covered so far in this book. As we argued in <a href="815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml" target="_blank">Chapter 5</a>, <em>Using Pre-Trained Models</em>,<em> </em>developing ML models in Go has both advantages and disadvantages, and sometimes combining Go with other languages, such as Python, to benefit from libraries, such as Keras, can significantly shorten the development portion of the cycle. The downside is reduced maintainability and more work to test the resulting solution, as it will necessarily contain a Go–Python interface (for example).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>Because humans are prone to making errors, testing the source code we create is a critical element of the development life cycle to guarantee an accurate and reliable product. Entire books have been dedicated to the subject, and it seems there are as many different approaches to software testing as there are software engineers (as an internet search for software-testing methodologies will confirm). ML applications, on the surface, are particularly difficult to test because they seem like a black box, whose output depends on the training set we provide: we feed them data, and they feed us answers, but a slight change of the train–test split or the hyperparameters could produce a different output for a given input vector. How can we determine whether the answers they provide are erroneous because the model's hyperparameters are incorrect, because the input data is corrupt, or because the model's algorithms are flawed? Or is this particular response an outlier buried in a population of otherwise acceptable responses?</p>
<p>In the previous chapters, we performed statistical testing of models using the validation set to measure the responses of the model to a meaningful sample of inputs, comparing them to expected output values when these were available (supervised learning). Arguably, this is the only way to test ML models for accuracy or precision because retraining them on a different sample of the dataset (or with altered hyperparameters) could produce a different output for the same input, but should not produce statistically inferior results on a large validation set with regards to the same accuracy/precision metrics. In other words, with small changes to the model, we could see large changes to the way it responds to one input vector, but its response should not be too different when tested against a large enough sample of input vectors, such as the validation set. </p>
<p>This has two consequences. First, the way that unit tests are usually constructed, where the developer chooses input values and asserts on the output, could break down with the slightest change to the model. Therefore, it is best not to rely on assertions based on a single response. Rather, it is better to assert using an accuracy or precision metric across a larger set, using the techniques we introduced in <a href="48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml" target="_blank">Chapters 3</a>, <em>Supervised Learning</em>, and <a href="26788e93-3614-413f-bcde-5580516f9c5f.xhtml" target="_blank">Chapter 4</a>, <em>Unsupervised Learning</em>.</p>
<p>Second, there may be edge cases, where we wish to guarantee the behavior of a model, or certain responses that we wish to guarantee will never occur (not even as outlying behavior). If we cannot be sure that a black box model can achieve this, combining an ML algorithm with traditional logic is the only way to ensure that the constraints are met. For example, consider Google's recent ban of "gorilla" as a search term on Google Images in an effort to prevent some accidentally racist results from appearing<sup>[2]</sup>. Performing statistical testing of the image classifier with gorilla images would have been difficult and would only have covered this one edge case; however, knowing what an unacceptable response was and adding constraining logic to prevent this edge case was a trivial, if embarrassing, affair. As with this example, traditional unit tests can be combined with statistical testing, with the traditional unit tests asserting on the output of the constraints while the statistical tests assert on the model output directly. An holistic strategy for ML testing thus emerges:</p>
<ol>
<li><strong>Define accuracy/precision goals for the model</strong>: This may not be as simple as coming up with a single accuracy score, as reducing false positives or false negatives may take precedence. For example, a classifier that aims to determine whether a mortgage applicant should get a loan may be required to err on the side of caution, with more false negatives tolerated than false positives, depending on the risk profile of the lender. </li>
<li><strong>Define edge case behavior and codify this into unit tests</strong>: This may require traditional logic to restrict the output of the ML model to ensure that these constraints are met and traditional unit tests to assert on the constrained output of the ML model. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p class="mce-root">Once the ML application has been developed and you have tested it to satisfy yourself that it works as intended, the next step in the CD life cycle is to deploy the software—that is, take steps to ensure that users are able to access it. There are different deployment models, depending on factors such as whether you are intending to run the application on your own hardware or whether you intend to use an <strong>infrastructure-as-a-service</strong> (<strong>IaaS</strong>) or <strong>platform-as-a-service</strong> (<strong>PaaS</strong>) cloud, and we will touch upon these differences in the next section. Here, we will assume that you are either running the application on your own servers or using a virtual infrastructure supplied by an IaaS provider. </p>
<p>ML applications can present unique challenges in deployment that are absent from simpler software, such as an HTTP server that connects to a database:</p>
<ul>
<li>Dependency on scientific libraries that require LAPACK or BLAS entails complex installation processes with many steps, and chances for mistakes.</li>
<li>Dependency on deep-learning libraries, such as TensorFlow, entails dynamic linking to C libraries, again leading to a complex installation process, with many OS and architecture-specific steps, and chances for mistakes.</li>
<li>Deep learning models may need to run on specialized hardware (for example, servers with GPUs), even for testing</li>
<li>Where should ML models be persisted? Should they be committed as though they were source code? If so, how can we be sure we are deploying the correct version?</li>
</ul>
<p>Next, we will present solutions to these challenges and a sample application that embodies these solutions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dependencies</h1>
                </header>
            
            <article>
                
<p>Anyone who has tried to build TensorFlow or NumPy from source will sympathize with the saying that <em>anything that can go wrong, will go wrong</em>. A search on Google, Stack Overflow, or their respective GitHub issue pages will reveal many obscure potential issues with the build process<sup>[3][4][5]</sup>. These are not isolated finding in the sense that the scientific computing libraries that ML applications rely on tend to be highly complex and depend on a convoluted set of other libraries that are also highly complex. An academic ML researcher might have need to build dependencies from source to benefit from a certain optimization, or perhaps because they need to modify them. On the contrary, an ML application developer must try to avoid this process and instead use prebuilt images available as Python wheels<sup>[6]</sup>, prebuilt packages for their chosen package manager (such as apt on Ubuntu Linux or Chocolatey<sup>[7]</sup> on Windows), or Docker images. </p>
<p>We will focus on Docker as a solution for developing and packaging Go ML applications for several reasons:</p>
<ul>
<li>Portability across a wide range of operating systems</li>
<li>Excellent support from major cloud vendors, such as Microsoft Azure<sup>[8]</sup> and Amazon Web Services<sup>[9]</sup></li>
<li>Support for Docker integration in popular provisioning and infrastructure configuration using tools such as Terraform<sup>[10]</sup>, Chef<sup><span>[11]</span></sup>, and Ansible<sup><span>[12]</span></sup>. </li>
<li>Availability of ML libraries through prebuilt Docker images</li>
<li>Go's particular suitability for Docker, as it can always be configured to produce static binaries, allowing us to greatly reduce the production Docker image size</li>
</ul>
<div class="packt_tip">If you have reduced the size of the Docker image as much as possible (maybe by using the <kbd>scratch</kbd> image), but the size of the Go binary makes the overall image still too large for you, consider using the <kbd>strip</kbd> command or a packer like <kbd>upx</kbd>.</div>
<p>In all the examples we have looked at so far, we have created a single Docker image that contains all the dependencies for our application, as well as the application files, usually added to the container using the <kbd>ADD</kbd> or <kbd>COPY</kbd> command in the Dockerfile. While this has the advantage of simplicity (there is only one Dockerfile for development and production), it also means that we will need to push or pull an oversized Docker image with all the dependencies for developing an application.</p>
<p>However, the dependencies are probably not required to run it because Go can always be configured to produce static binaries that run on stripped-down Docker images. This means slower deployment times and slower testing times, as intermediate Docker images may not be cached in the CI environment, not to mention that a smaller container tends to use less disk and memory on its host server. Smaller images also have the benefit of added security from reducing the attack surface, as they will contain far fewer dependencies that an attacker could exploit. The <kbd>scratch</kbd> image, for example, does not even contain a shell, making it very hard for an attacker to compromise, even if the application running in the container is itself compromised.</p>
<p>The process we advocate is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-207 image-border" src="assets/4c230b82-c158-4b8c-bad6-51a6406f9879.png" style="width:64.75em;height:8.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Fig 2: Deployment using two separate Docker images (one for development and one for testing/production)</div>
<p>In the following example, we assume that you already have a development environment, where all your dependencies live (which could be Docker based, or not—it does not matter). You have developed your ML application, which consists of a <kbd>main</kbd> package and some saved model weights, <kbd>model.data</kbd>, and would like to create a production-ready container. To create this container, we need to do two things.</p>
<p>First, we need to compile the Go application to a static binary. If you are not using CGO and linking to some C libraries (such as the TensorFlow C library), then using <kbd>go build</kbd> without any additional flags will suffice. However, if your application depends on, say, the TensorFlow C library, then you need to add some additional command-line arguments to ensure that the resulting binary is static—that is, that it includes all the dependent code. At the time of writing, there is a proposal for Go 1.13 to have a <kbd>-static</kbd> flag for the <kbd>build</kbd> command that will achieve this with no further work. Until then, there is an excellent blog post by Diogok that explains the different flags in the following command, and how to tweak them if it does not work in your particular case:</p>
<pre>CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -tags netgo -ldflags '-w -extldflags "-static"' -o mlapp *.go</pre>
<p>This will produce a single output binary <kbd>mlapp</kbd> with all the required dependencies. The purpose of using all these flags is to produce a static binary that contains all our dependencies so that we only have the simple task of adding them to a "vanilla" Docker image, giving us the Dockerfile:</p>
<pre class="graf graf--pre graf-after--p">FROM scratch<br/>ADD . /usr/share/app<br/>ENTRYPOINT ["/usr/share/app/mlapp"]</pre>
<p>That's it! There is nothing else to add, unlike the long Dockerfiles we previously used because we needed all the dependencies. In this case, we already have these dependencies inside our Go binary. This is another advantage of Go; unlike some other programming languages, Go makes this type of deployment possible. </p>
<div class="packt_tip">You can also expose a port using your Dockerfile (for example, if you intend to serve your app from an HTTP server) by using the <kbd>EXPOSE</kbd> command. To expose an HTTP server listening on port 80, use the, <kbd>EXPOSE 80/tcp</kbd> command.</div>
<p>In the preceding example, we assumed that our model file containing the trained model weights/hyperparameters was persisted to disk and saved alongside our binary, ready to be added to the Docker container; however, there are cases where this may be impractical or undesirable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model persistence</h1>
                </header>
            
            <article>
                
<p>Most of the time, you can follow the aforementioned pattern of committing your model file alongside the source code and adding it to a Docker image during deployment together with your binary; however, there are times when you may want to reconsider this:</p>
<ul>
<li>The model file is very large, so it leads to a very large Docker image and slows down deployments.</li>
<li>The number of model files you have is dynamic and each model is associated with an object of your application—that is, you train one model per user.</li>
<li>The model is retrained much more frequently than the code is likely to change, leading to very frequent deployments.</li>
</ul>
<p>In these cases, you may want to make the model available from a different source and not commit it to source control. At a basic level, model files are just a sequence of bytes, so there is no real limit to where they can be stored: on a file server elsewhere, cloud file storage, or a database. </p>
<p>The exception is the second case: where you have a dynamic number of model files that are associated with application objects, such as users. For example, if you are building a system that aims to forecast how much electricity a household will consume the following day, you might end up having one model for all households or one model per household. In the latter case, you would be better served using a database to hold these model files:</p>
<ul>
<li>The model files could be seen to contain sensitive data that is probably best secured and governed in a database.</li>
<li>A large number of model files could benefit from advanced compression techniques that are leveraged by database software, such as using page-level compression instead of row-level compression. This can reduce their overall size on the disk.</li>
<li>It may be easier to keep data associated with application objects all in the same place to limit the number of queries required with authorize an operation related to a model, for example.</li>
</ul>
<p>For these reasons, among others, we recommend saving the model to a database in the event that your application requires many models, each associated to an application object, such as a user.</p>
<p>This poses a small challenge, because some Go ML libraries, such as GoML, expose persistence functions, such as <kbd>PersistToFile</kbd> of the <kbd>linear</kbd> package models, and these functions persist the model to a file; however, they do not directly offer access to serialized model should we want to persist it elsewhere. </p>
<p>There are two techniques we can apply:</p>
<ul>
<li>Look through the Godocs to see if the model struct has any unexported fields. If not, we can simply use <kbd>encoding/json</kbd> to serialize the model.</li>
<li>If there are unexported fields, we can save the model to a temporary file, read the temporary file into memory, and delete it again.</li>
</ul>
<div class="mce-root packt_infobox">In Go, an <strong>unexported field</strong> is a struct field with a lowercase name, which is not accessible outside the package in which it is defined. Such fields are absent from serialization using <kbd>encoding/json</kbd>. </div>
<p>In the case of GoML's <kbd>LeastSquares</kbd> model, there are no unexported fields, and a cursory examination of the <kbd>PersistToFile</kbd> method would reveals that it is using encoding/JSON to marshal the model to a byte slice. Therefore, we can just use <kbd>serializedModel, err := json.Marshal(leastSquaresModel)</kbd> to serialize it. The resulting <kbd>serializedModel</kbd> can then be saved anywhere we wish.</p>
<p>But what if, for argument's sake, we could not do this because the model struct had unexported fields? For example, the golearn library's <kbd>linear_models</kbd> package has an <kbd>Export</kbd> method that persists models to the file, but this relies on a call to a C function, and the model has unexported fields. In this case, we have no choice but to first persist the model to a temporary file and then recover the file contents:</p>
<pre>import (<br/>  "io/ioutil"<br/>  linear "github.com/sjwhitworth/golearn/linear_models"<br/>)<br/><br/>func Marshal(model *linear.Model) ([]byte, error) {<br/>  tmpfile, err := ioutil.TempFile("", "models")<br/>  if err != nil {<br/>    return nil, err<br/>  }<br/>  defer os.Remove(tmpfile.Name())<br/>  if err := linear.Export(model, tmpfile.Name()); err != nil {<br/>    return nil, err<br/>  }<br/>  return ioutil.ReadAll(tmpfile)<br/>}</pre>
<p>All we are doing in the preceding code is providing a temporary location to store the model file on disk and then moving it back to memory. While this is not the most performant way to store a model, it is necessary because of the limitations on some of the interfaces for some Go ML libraries, and there is already an open issue on GoLearn's GitHub page to improve this.</p>
<p>Now that the application is deployed, we want some certainty that it is functioning correctly, using up an appropriate amount of resources, and that there is no underlying issue that could prevent it from being available. In the next subsection, we will look at monitoring techniques specific to ML applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring</h1>
                </header>
            
            <article>
                
<p>In his book, <em>Architecting for Scale</em>, Lee Atchison, Principal Cloud Architect at New Relic, argues for the use of a risk matrix, also known as a <strong>risk register</strong>, to keep track of what is likely to go wrong with an application and how it should be mitigated<sup>[16]</sup>. While this may seem like overkill for a simple application, it is a great tool for managing risk in a complex environment, especially where ML models are involved. This is because the entire team can be aware of the main risks, their likelihoods, and mitigation, even if they did not have a hand in creating every part of the application in the first place. ML models can sometimes be created by a data scientist and then later handed over to a software development team via one of the polyglot integration approaches we outlined in <a href="48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml"/><a href="48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml">Chapter 5</a>, <em>Supervised Learning</em>, so this makes knowing any risk associated with their use in production all the more important. </p>
<p>While this may seem like a rather opinionated approach, remember that the goal is simply to make developers think about what can cause their application to become unavailable. There is no obligation to write down a risk register or run your team using one (although both could be beneficial), and the practice of thinking about risk always helps by shining light on dark recesses, where no one had thought to look for that elusive Friday night bug that took the whole application offline until Monday morning. </p>
<div class="packt_infobox">A <strong>risk</strong> associated with a production application is different from a failure of a test, which you would hopefully have caught before deploying it to production in the first place. It is the risk that something you assumed constant in testing (such as available memory or a training algorithm converging) has changed to a critical state. </div>
<p>Risks associated with ML applications could include, but are not limited to, the following:</p>
<ul>
<li>Running out of memory to run more instances of the model</li>
<li>A model file becoming corrupt, leading to the model being unavailable to be run, even though the rest of the application might still be available</li>
<li>A nonconvergent training procedure, if model retraining is done in production, leading to a useless model</li>
<li>Malicious users crafting input to try to trick the model into producing a desired output</li>
<li>Malicious users crafting badly formatted input (fuzzing) to crash the model</li>
<li>Upstream services, such as databases used to store ML models, being unavailable</li>
<li>The cloud datacentre, where the model runs runs low on GPU availability, meaning that an autoscale feature fails and availability of your deep learning model is reduced as a result</li>
</ul>
<p>The list is obviously not exhaustive, but hopefully it gives you an idea of the kind of issues that could arise so you can look for them in your own applications. Because it is very difficult to come up with an exhaustive list, general monitoring principles apply:</p>
<ul>
<li>Use structured logging in the application wherever possible and centralize these logs</li>
<li>If retraining in production, make sure that you set up alerts for any error in the training procedure, since this will necessarily lead to a useless model (or falling back to a deprecated one)</li>
<li>Capture metrics whose significant change could be used to detect any risks in your register materializing (for example, availability of memory space)</li>
</ul>
<p>Go was designed partly to serve web applications<sup>[17]</sup>, so there are many third-party packages that can help you perform these tasks, and we will now explore some of them. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structured logging</h1>
                </header>
            
            <article>
                
<p>There are many logging libraries for Go, such as the standard library's <kbd>log</kbd> package<sup>[18]<span>[19]</span><span>[20]</span></sup>. A significant advantage to using a structured logging library—which logs to a standardized format, such as JSON—over unstructured logging that simply uses free text is that it is far easier to work with the log data once it has been created. Not only is searching by a particular field easier (using, say, <kbd>jq</kbd><span><sup>[21]</sup> </span>to work with JSON data), but structured logs allow far richer integration with existing monitoring and analytics tooling, such as Splunk<sup><span>[22]</span></sup> or Datadog<sup><span>[23]</span></sup>. </p>
<p>In the following example, we will use the Logrus package to log an error message returned by a training procedure. Note that the use of this particular logging package is a personal choice, and any other structured logging package would also work.</p>
<p>First, we configure the logger:</p>
<pre>import "github.com/sirupsen/logrus"<br/><br/>logrus.SetFormatter(&amp;logrus.JSONFormatter{})<br/>logrus.SetReportCaller(true) // Add a field that reports the func name</pre>
<p>The output format can be configured by using the properties of the <kbd>JSONFormatter</kbd> struct<sup>[24]</sup>:</p>
<ul>
<li><kbd>TimestampFormat</kbd>: The format of the timestamps using a time-compatible format string (for example, <kbd>Mon Jan 2 15:04:05 -0700 MST 2006</kbd>). </li>
<li><kbd>DisableTimestamp</kbd>: Removes the timestamp from the output</li>
<li><kbd>DataKey</kbd>: Instead of a flat JSON output, this puts all the log entry parameters into a map at the given key</li>
<li><kbd>FieldMap</kbd>: Use this to rename the default output properties, such as the timestamp</li>
<li><kbd>CallerPrettyfier</kbd>: When <kbd>ReportCaller</kbd> is activated (as shown in the preceding code snippet), this function can be called to customize the output—for example, stripping the package name from the caller's method</li>
<li><kbd>PrettyPrint</kbd>: This determines whether to indent JSON output</li>
</ul>
<p>Here is an example, where we use it in practice:</p>
<pre>import "github.com/sajari/regression"<br/>model := new(regression.Regression)<br/> logrus.WithFields(logrus.Fields{ "model": "linear regression", }).Info("Starting training")<br/>for i := range trainingX {<br/> model.Train(regression.DataPoint(trainingY[i], trainingX[i]))<br/>}<br/>if err := model.Run(); err != nil {<br/> <br/>logrus.WithFields(log.Fields{<br/> "model": "linear regression",<br/> "error": err.Error(), }).Error("Training error")<br/><br/>}<br/> logrus.WithFields(logrus.Fields{ "model": "linear regression", }).Info("Finished training")</pre>
<p>While this may produce more output than necessary, because of the addition of the two info-level messages, we can filter out this level of output if it is not required by using <kbd>logrus.SetLevel</kbd>; however, in the case of retraining in production, the training time is important (as is making sure that the training process completes), so it is never a bad idea to have records of the process in the log, even if it becomes more verbose as a result. </p>
<div class="packt_tip">When logging ML-related information, it is a good idea to have a field with the model name (which may be something meaningful to a data scientist, if they created it). When you have multiple models running concurrently in production, it is sometimes hard to tell which one has produced the error!</div>
<p>The time taken to train an algorithm is one metric that we would recommend computing regularly and sending to a dedicated metrics system. We will discuss capturing metrics in the next subsection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capturing metrics</h1>
                </header>
            
            <article>
                
<p>In the preceding example, we inserted info-level messages in the logs to signify the start and end of the training process. While we could look at the timestamp fields of both messages and compare them to determine how long the training process took (Splunk, for example, is able to do this with the right query), a more direct and less cumbersome way to achieve the same result is to monitor this specific datapoint, or metric, explicitly. We could then raise alerts if the training process becomes too long or have a chart that logs and displays the time taken by the regular model training processes.</p>
<p>There are two approaches that we can use:</p>
<ul>
<li>Store the metric as an additional field on the log entry with a <kbd>float64</kbd> value</li>
<li>Store the metric in a separate analytics system</li>
</ul>
<p>Ultimately, the approach you take depends on your current analytics systems, team preferences, and application size. As far as ML applications go, either approach works equally well, so we will assume the first one, as it reduces the amount of third-party application code required.</p>
<p>Reusing the same example as earlier, let's set this up:</p>
<pre>import "github.com/sajari/regression"<br/>model := new(regression.Regression)<br/> log.WithFields(log.Fields{ "model": "linear regression", }).Info("Starting training")<br/>start := time.Now()<br/><br/>for i := range trainingX {<br/> model.Train(regression.DataPoint(trainingY[i], trainingX[i]))<br/>}<br/>if err := model.Run(); err != nil {<br/>log.WithFields(log.Fields{ "model": "linear regression",<br/> "error": err.Error(), }).Error("Training error")<br/><br/>}<br/>elapsed := time.Since(start)<br/> log.WithFields(log.Fields{ "model": "linear regression",<br/> "time_taken": elapsed.Seconds(), }).Info("Finished training")</pre>
<p>Note that we did not include any of the logging calls in the timed block. This is because we want to measure the time taken by the training process rather than any logging around it.</p>
<div class="packt_tip">If your company uses an analytics system, such as Grafana or InfluxDB, you can still use the same approach as previously described—just make sure that you create a sensible name for your metric, including the name of the ML model.</div>
<p>In the final subsection the CD feedback loop, we will consider how accuracy/precision metrics can help create a feedback loop in an ML application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feedback</h1>
                </header>
            
            <article>
                
<p>The process of acquiring feedback in any system is intended to improve the system. In the case of an ML application, feedback can help make the application more robust with regards to risks on its register (or the addition of new risks that were previously unmitigated), but this is not specific to ML applications; all production applications benefit from a feedback cycle. There is, however, one special feedback cycle that is particular to ML applications.</p>
<p>An ML model is used on the basis that it satisfies some accuracy/precision criteria that make it better or more generic at extracting meaning from data than a naive heuristic. In <a href="48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml">Chapter 3</a>, <em>Supervised Learning</em>, and <a href="26788e93-3614-413f-bcde-5580516f9c5f.xhtml">Chapter 4</a>, <em>Unsupervised Learning</em>, we outlined some of these metrics, such as the mean square error of a regression of house prices or the test/validation accuracy of binary classifiers on images of clothes. In our CD cycle so far, we have assumed that once a model is created, its accuracy will never change with regards to new input; however, this is rarely a realistic assumption.</p>
<p>Consider our MNIST fashion classifier from <a href="48817ff3-5622-4f43-88e7-d3dfccacb25d.xhtml" target="_blank">Chapter 3</a>, <em>Supervised Learning</em>, which aims to determine whether an image represents a pair of trousers or not. At the moment, this database does not contain any images of flared trousers. What if these come back into fashion and all the images our model begins to receive are of flared trousers? We may notice users complaining that images are not being correctly classified. Such considerations have led to numerous websites that rely on ML models adding "Rate my prediction" models to their websites in a bid to ensure that models are still outputting relevant predictions.</p>
<p>This is, of course, a valid approach, albeit one that relies on the customer to tell you when your product is and is not working. Because customers are more likely to use these feedback features during an unsatisfactory experience<sup>[26]</sup>, any data you gather from this exercise, while still useful, is likely biased toward the negative and therefore cannot automatically be used as a proxy accuracy metric.</p>
<p>In cases where the customer supplies images and your model classifies them, this may still be your best option, unless you can write a scraper for new trouser images that continuously feeds them to a model and measures its response. That would be labor-intensive, but would clearly produce better results, assuming, of course, that the types of trousers found by your scraper were representative of the types of trouser images supplied by your customers. In other cases, some automated feedback loops may be possible, where you are able to directly monitor the accuracy of a model, either in testing or production, and use this to make a decision on when the model should be retrained. </p>
<p>Consider a different scenario, one where you are asked to forecast the next day's individual electricity consumption of a large number of households, given data points such as the number of occupants and a forecast temperature curve. You decide that you will use, say, one regression per household and store the regression parameters in a database once the model is trained. Then, every day, you will run every model in your database to generate predictions. </p>
<p>A very easy feedback cycle exists in this case because, every day, you can also measure the actual electricity consumption of the household and compare this to your model's prediction. A scheduled script could then compare the relative difference between the two over a certain period, perhaps using a moving average to smooth out any anomalies, and should this difference be greater than a certain predefined threshold, it would then be entitled to assume that some of the model's input data had changed and the model required retraining on a new dataset. An alternative would be to retrain that the model if any of its input parameters changed, although that could lead to a lot of unnecessary retraining and thus additional cost, as forecast temperature curves likely change daily, so every model would likely need to be re-trained every day. </p>
<p>The feedback loop for ML applications with continuous validation and retraining is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-208 image-border" src="assets/3292a8f0-4375-4ca5-83d3-ea81d2f255ec.png" style="width:20.08em;height:22.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Fig. 3: The feedback loop for ML applications with continuous validation</div>
<div class="packt_tip">The feedback loop cannot be applied to every ML application, but with a little creativity, you can usually find a way in which to find input samples that were not in either the training or testing dataset, but are of updated relevance. If you can automate the process of generating predictions from these samples and storing their difference to a ground truth, then you can still generate the same feedback loop.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment models for ML applications</h1>
                </header>
            
            <article>
                
<p>In the preceding example, we explained how to deploy an ML application using Docker to encompass it and its dependencies. We deliberately stayed away from any discussion pertaining to the infrastructure that was going to run these containers or any Platform-as-a-Service offerings that could facilitate the development or deployment itself. In the current section, we consider different deployment models for ML applications under the assumption that the application will be deployed to a cloud platform that supports both IAAS and platform-as-a-service models, such as Microsoft Azure and Amazon Web Services. </p>
<div class="packt_infobox">This section is specifically written to help you decide what virtual infrastructure to use if you are deploying an ML application to the cloud. </div>
<p>There are two main deployment models for any cloud application:</p>
<ul>
<li><strong>Infrastructure-as-a-service</strong>: This is the cloud service that offers a high-level interaction with virtualized hardware, such as virtual machines, without the customer needing to maintain the hardware or the virtualization layer. </li>
<li><strong>Platform-as-a-service</strong>: This is a cloud service that offers Software-as-a-Service components that you can then build your application from, such as a serverless execution environment (for example, AWS Lambda).</li>
</ul>
<p>We will consider both options and how to make best use of them for ML applications. We will compare and contrast the three main vendors by market share, as of Q4 2018: Amazon Web Services, Microsoft Azure, and Google Cloud<sup>[30]</sup>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Infrastructure-as-a-service</h1>
                </header>
            
            <article>
                
<p>Earlier in this chapter, we explained how to package an ML application using Docker. In this subsection, we will look at simple ways to deploy an ML application using Docker to AWS, Azure, or Google Cloud. </p>
<p>In each case, we will start by explaining how to push one of your local Docker images to a <strong>registry</strong> (that is, a machine that will store images and serve them to the rest of your infrastructure). There are several advantages to using a Docker registry to store your images:</p>
<ul>
<li><strong>Faster deployments and build times</strong>: Virtual infrastructure components requiring images can just pull them from the registry instead of building them from scratch every time</li>
<li><strong>Ease of implementing autoscale in your application</strong>: If you have to wait for a long Docker build—say, 20 minutes, for TensorFlow—every time you need to scale your service up, you may experience degradation or unavailability</li>
<li><strong>Security</strong>: Pulling images from a single trusted source reduces the attack surface</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon Web Services</h1>
                </header>
            
            <article>
                
<p class="mce-root">The core of AWS's virtualized IaaS offering is <strong>Elastic Compute</strong> (<strong>EC2</strong>). AWS also offers <strong>Elastic Container Registry</strong> (<strong>ECR</strong>) as a registry service to serve images from. To set this up, go through the following steps:</p>
<div class="packt_tip">Before you can push or pull an image to an ECR registry, you need <span><kbd>ecr:GetAuthorizationToken</kbd> permissions. </span></div>
<ol>
<li class="mce-root">Tag your image, assuming its ID is <kbd>f8</kbd><span><kbd>ab2d331c34</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px">docker tag <span>f8</span><span>ab2d331c34</span> your_aws_account_id.dkr.ecr.region.amazonaws.com/my-ml-app</pre>
<ol start="2">
<li>Push the image to the ECR:</li>
</ol>
<pre style="padding-left: 60px">docker push your_aws_account_id.dkr.ecr.region.amazonaws.com/my-ml-app</pre>
<p>The image is now available to use from an EC2 instance. First, SSH into your instance where you have installed Docker, following the instructions in <a href="815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml" target="_blank">Chapter 5</a>, <em>Using Pre-Trained Models,</em> and then run the following commands to install Docker and start a container from the image (amend the <kbd>docker run</kbd> command to add exposed ports or volumes):</p>
<pre>docker pull your_aws_account_id.dkr.ecr.region.amazonaws.com/my-ml-app &amp;&amp; \<br/>docker run -d your_aws_account_id.dkr.ecr.region.amazonaws.com/my-ml-app</pre>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microsoft Azure</h1>
                </header>
            
            <article>
                
<p>Similar to Amazon's ECR, which we discussed in the previous subsection, Microsoft Azure offers a registry, Azure Container Registry. We can use this by following the same steps as AWS ECR, but there is a difference, namely the requirement to log in via the Docker command-line interface. Once this is done, you can follow the same instructions as the previous subsection, but with your registry and image details:</p>
<pre><span>docker login myregistry.azurecr.io</span></pre>
<p>Microsoft also allows Docker as a deployment method for App Service Apps, a managed web app service based on Microsoft's <strong>Internet Information Services</strong> (<strong>IIS</strong>). If you have followed the preceding steps to deploy your Docker image to a registry, you can use the <kbd>az</kbd> command-line tool to create a web app from your image:</p>
<pre><span class="hljs-keyword">az</span><span> </span><span class="hljs-keyword">webapp</span><span> </span><span class="hljs-keyword">create</span><span class="hljs-parameter"> --resource-group</span><span> myResourceGroup</span><span class="hljs-parameter"> --plan</span><span> myAppServicePlan</span><span class="hljs-parameter"> --name</span><span> </span><span class="hljs-string">&lt;app name&gt;</span><span class="hljs-parameter"> --deployment-container-image-name</span><span> myregistry.azurecr.io/my-ml-app</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud</h1>
                </header>
            
            <article>
                
<p>Like Amazon and Microsoft, Google also offers a registry, called Container Registry, which can be used as a Docker registry. The steps to use it are the same as for Amazon ECR, except for the addition of a preliminary authentication step using the <kbd>gcloud</kbd> command-line tool:</p>
<pre>gcloud auth configure-docker</pre>
<p>Now you can push the image:</p>
<pre>docker tag quickstart-image gcr.io/[PROJECT-ID]/quickstart-image:tag1</pre>
<p>The steps to run a Docker container on a Google Cloud VM are the same as for an EC2 VM, with the addition of the authentication step. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Platform-as-a-Service</h1>
                </header>
            
            <article>
                
<p>With the rising popularity of ML components in applications, cloud vendors have scrambled to provide platform-as-a-service offerings that make it easier to deploy ML applications in an effort to win over customers. It is worth a brief review of each of the three main cloud vendors by market share as of 2018<sup>[30]</sup>. This is not an attempt to recommend one vendor over another, but rather an attempt to explore solutions while remaining agnostic to any decisions regarding cloud vendors that you may have already made. In other words, the deployment models we will discuss will work in all three clouds—and probably others—but some platforms offer specific services that may better suit certain applications or reduce their development effort.</p>
<div class="packt_infobox">Cloud vendors make such frequent changes to their offerings that it is possible that by the time you are reading this, there will be newer, better services than the ones described here. Look in the <em>Further reading</em> section for some links to Google Cloud, AWS, and Azure ML services<sup>[27]<span>[28]</span><span>[29]</span></sup>. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon Web Services</h1>
                </header>
            
            <article>
                
<p><strong>Amazon Web Services</strong> (<strong>AWS</strong>) has two main types of service offerings regarding ML space:</p>
<ul>
<li><strong>AWS Sagemaker</strong>: A hosted environment to run ML notebooks and SDK to efficiently perform various ML-related tasks, including data labeling</li>
<li><strong>AWS AI Services</strong>: A set of pretrained models for specific tasks, such as image recognition </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon Sagemaker</h1>
                </header>
            
            <article>
                
<p class="mce-root">Amazon Sagemaker uses Jupyter as a development environment for ML models, as we have done throughout the book. The environment in which these Jupyter notebooks run comes with some Python ML libraries. For Python developers, this service can be thought of as another environment to run ML code with some features to accelerate large-scale learning through AWS resources. An example using Sagemaker to perform hyperparameter tuning on a natural language processing task can be found on the AWS GitHub<sup>[31]</sup>, and for a longer introduction there are some exploratory videos available on YouTube<sup>[33]</sup>. Unfortunately, at this time, there is no way to use Sagemaker with a Go kernel for Jupyter (such as gophernotes), so it is not a pure-Go solution for interactively developing ML applications in a remote environment.</p>
<p>For Go developers who need to interact with an existing Sagemaker solution, there is an SDK that has much of the same features as the Python SDK<sup>[32]</sup>, so it is possible to use gophernotes locally to create Sagemaker tasks. In fact, the SDK is so powerful that it allows Go developers to access a useful data preprocessing service: the Sagemaker Labeling Job service. This service integrates with Mechanical Turk to provide ground truth labels for training data where they are either missing entirely or from part of the dataset. This saves a lot of time compared to manually setting up Mechanical Turk jobs. The function that exposes this functionality is <kbd>CreateLabelingJob</kbd>. </p>
<div class="packt_tip">If you need to use a supervised learning algorithm, but have only an unlabeled dataset, consider using Sagemaker's interface to Mechanical Turk to label your dataset cheaply. Alternatively, you can create a labeling task through the Mechanical Turk UI at <a href="https://www.mturk.com/">https://www.mturk.com/</a>. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon AI Services</h1>
                </header>
            
            <article>
                
<p>If there is already a model exposed that solves your ML problem, then there is no need for you to reinvent the wheel and train a new model, especially considering the large resources that AWS will have invested in ensuring the accuracy and efficiency of its models. At the time of writing, the following types of algorithms are available on a pay-for-usage basis:</p>
<ul>
<li><strong>Amazon Personalize</strong>: Built on the same recommendation techniques used by Amazon in their online retail store, these allow you to solve problems, such as showing customers items similar to those they have already bought</li>
<li><strong>Amazon Forecast</strong>: Timeseries forecasting models</li>
<li><strong>Amazon Rekognition</strong>: Image and video analysis</li>
<li><strong>Amazon Comprehend</strong>: Natural language processing tasks and text analysis</li>
<li><strong>Amazon Textract</strong>: Large-scale document analysis</li>
<li><strong>Amazon Polly</strong>: Text-to-speech</li>
<li><strong>Amazon Lex</strong>: Build chatbots in a UI environment</li>
<li><strong>Amazon Translate</strong>: Automated translation to and from a multitude of languages</li>
<li><strong>Amazon Transcribe</strong>: Speech-to-text service</li>
</ul>
<p>While none of these services are Go specific, they all offer Go SDKs that you can use to interact with them. This is very similar to the example we saw in <a href="815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml" target="_blank">Chapter 5</a>, <em>Using Pre-Trained Models</em>, where a model was exposed over HTTP and we used this protocol to send it data and receive predictions.</p>
<p>Generally, the methods are synchronous—that is, you will get the result in the output argument, and do not need to make a further request later. They also have the same type of signature, where the name of the prediction method may vary, and the structure of the input/output will also vary:</p>
<pre>func (c *NameOfService) NameOfPredictionMethod(input<br/>     *PredictionMethodInput) (*PredictionMethodOutput, error)</pre>
<p>By way of example, consider Rekognition, which, like the other services, has a Go SDK<sup>[34]</sup>. Suppose that we wish to detect faces in an image. For this, we use the <kbd>DetectFaces</kbd> func; this has the following signature:</p>
<pre>func (c *Rekognition) DetectFaces(input *DetectFacesInput<br/>     (*DetectFacesOutput, error)</pre>
<p>The input, in this case, contains, among other things, an array of facial attributes that we wish to be returned, as well as an image, either as base-64 encoded bytes or an S3 object. The output will contain a <kbd>FaceDetail</kbd> struct, which, among other things, will describe an age range for each face, whether it is bearded, a confidence in its bounding box, any detected emotions, whether they are wearing glasses, and so on. This depends on which facial attributes we requested in the input, and necessarily, the more attributes we requested, the more expensive the request will be (as Amazon will need to run more models to give us the answer).</p>
<p>Generally, if it is possible to build your ML application by composing prebuilt models exposed over SDKs, such as AWS, then you will save a lot of time, and it will allow you to focus on adding value specific to your business; however, there are risks associated with vendor lock-in, and at the time of writing, no other cloud platform offers a feature-for-feature alternative to Amazon AI services.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microsoft Azure</h1>
                </header>
            
            <article>
                
<p>Azure's main offerings geared at ML applications are as follows:</p>
<ul>
<li><strong>Azure ML Studio</strong>: A UI environment to build ML pipelines and train models</li>
<li><strong>Azure Cognitive Services</strong>: Pretrained models exposed over HTTP</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Azure ML Studio</h1>
                </header>
            
            <article>
                
<p>Azure ML Studio is a cloud-based IDE for ML. It allows users to import data from other Azure services (such as Blob Storage), transform the data, and use it to train one of the included ML algorithms. The resulting model can then be exposed via HTTP or composed with other Azure services, such as Azure Stream Analytics for a real-time ML application<sup>[35]</sup>. </p>
<p>While it is possible to run custom Python code within the Azure ML Studio UI, at the time of writing, this does not extend to Go; however, because it is possible to expose models via HTTP, you can integrate with an existing Azure ML Studio model by following the same pattern that we discussed in <a href="815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml">Chapter 5</a>, <em>Using Pretrained Models</em>, where the <kbd>net/http</kbd> client is used to make requests. It is worth using the Azure SDK just to generate authentication tokens rather than trying to implement this yourself, as the procedure can be error prone<sup>[36]</sup>. The JSON structure of the request and response are very simple compared to AWS, so the resulting code can be clean and easy to maintain.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Azure Cognitive Services</h1>
                </header>
            
            <article>
                
<p>Azure Cognitive Services exposes several pretrained ML models over HTTP:</p>
<ul>
<li><strong>Computer Vision</strong>: Image recognition </li>
<li><strong>Speech</strong>: Speech recognition and transcription</li>
<li><strong>LUIS</strong>: Textual intent analysis</li>
<li><strong>Bing Image Search</strong>: Retrieves images matching a text string</li>
<li><strong>Bing Web Search</strong>: Retrieves URLs matching a text string</li>
<li><strong>Text Analytics</strong>: Sentiment analysis</li>
</ul>
<p>At the time of writing, there is no Go SDK to interact with Cognitive Services, but it is possible to invoke the models by using the REST API, and Microsoft provides an example of this in a Quickstart article<sup>[37]</sup>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud</h1>
                </header>
            
            <article>
                
<p>Google Cloud currently has two main services to offer ML application developers, in addition to the free Google Colaboratory<sup>[29]</sup>:</p>
<ul>
<li><strong>AI Platform</strong>: Hosted development environment using Notebooks, VM images, or Kubernetes images</li>
<li><strong>AI Hub</strong>: Hosted repository of plug-and-play AI components</li>
<li><strong>AI Building Blocks</strong>: Pretrained models, exposed via SDK or HTTP</li>
</ul>
<p>Because AI Hub is targeted only at Python developers and its deployment model is the same as AI Platform, we will not discuss it any further. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AI Platform</h1>
                </header>
            
            <article>
                
<p>Google's AI Hub is a code-based environment aimed at facilitating all aspects of the ML application development life cycle, from data ingestion to deployment, via AI Platform Prediction (applicable to TensorFlow models exported as a <kbd>SavedModel</kbd>, as in our <a href="815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml" target="_blank">Chapter 5</a>, <em>Using Pretrained Models</em>, example) or Kubernetes. It has loose integrations with other Google Cloud Services, but remains, at its core, a hosted notebook environment. </p>
<p>Because there is no high-level API to create TensorFlow graphs in Go, analogous to Keras in Python, it is unlikely that a Go developer will find the end-to-end platform useful. However, if you are interacting with a TensorFlow model, using AI Platform Prediction to manage the resources for the model and calling it via HTTP<sup>[40]</sup> is an excellent strategy, particularly as the model can be made to run on VMs with a Tensor Processing Unit, which can be a significantly cheaper way to run TensorFlow workflows<sup>[39]</sup>.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AI Building Blocks</h1>
                </header>
            
            <article>
                
<p>Google's AI Building Blocks are a suite of pretrained models, exposed via HTTP or through one of Google Cloud's SDKs:</p>
<ul>
<li><strong>Sight</strong>: Includes Vision, for image recognition, and Video, for content discovery</li>
<li><strong>Language</strong>: Comprises translation and natural language processing functionality</li>
<li><strong>Conversation</strong>: Consists of a speech-to-text model, a text-to-speech model, and a chatbox builder</li>
<li><strong>Structured data</strong>:</li>
</ul>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Recommendations AI</strong>: Recommendation engine</li>
<li><strong>AutoML Tables</strong>: UI to generate predictive models</li>
<li><strong>Cloud Inference AI</strong>: Time series inference and correlations tool</li>
</ul>
</li>
</ul>
<p>The Go SDK is very easy to use, as the following example shows. The example uses the text-to-speech API to download a recording of the phrase <kbd>hello, world</kbd>, as spoken by the ML model:</p>
<pre>package main<br/>import (<br/>     "context"<br/>     "fmt"<br/>     "io/ioutil"<br/>     "log"<br/>    texttospeech "cloud.google.com/go/texttospeech/apiv1"<br/>     texttospeechpb "google.golang.org/genproto/googleapis/cloud/texttospeech/v1"<br/>)<br/>func main() {<br/>     ctx := context.Background()<br/>    c, err := texttospeech.NewClient(ctx)<br/>     if err != nil {<br/>         log.Fatal(err)<br/>     }<br/><br/>    req := texttospeechpb.SynthesizeSpeechRequest{<br/>         Input: &amp;texttospeechpb.SynthesisInput{<br/>     InputSource: &amp;texttospeechpb.SynthesisInput_Text{Text: "Hello, World!"},<br/>     },<br/>     Voice: &amp;texttospeechpb.VoiceSelectionParams{<br/>         LanguageCode: "en-US",<br/>         SsmlGender: texttospeechpb.SsmlVoiceGender_NEUTRAL,<br/>     },<br/>     AudioConfig: &amp;texttospeechpb.AudioConfig{<br/>         AudioEncoding: texttospeechpb.AudioEncoding_WAV,<br/>     },<br/> }<br/>    resp, err := c.SynthesizeSpeech(ctx, &amp;req)<br/>         if err != nil {<br/>         log.Fatal(err)<br/>     }<br/>     filename := "prediction.wav"<br/>     err = ioutil.WriteFile(filename, resp.AudioContent, 0666)<br/>     if err != nil {<br/>         log.Fatal(err)<br/>     }<br/>}</pre>
<p>As with other models-over-HTTP type services, if you can build your application by composing these premade models, then you can dedicate your time to work on value-adding business logic; however, always consider the downsides of vendor lock-in.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how to take a prototype ML application to production. Along the way, we explored concerns that a software developer or DevOps engineer would typically think of, but from an ML application developers point of view. Specifically, we learned<span> h</span><span>ow to apply a continuous development life cycle to an ML application and the d</span><span>ifferent ways to deploy ML applications in the cloud.</span></p>
<p>In the next and final chapter, we will take a step back and look at ML development from a project management point of view. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further readings</h1>
                </header>
            
            <article>
                
<ol>
<li><em>Continuous Software Engineering and Beyond: Trends and Challenges Brian Fitzgerald</em>, 1st International Workshop on Rapid Continuous Software Engineering. <span>New York, NY: Association for Computing Machinery, pp. 1–9. </span></li>
<li><em>Google's solution to accidental algorithmic racism</em>: ban gorillas: <a href="https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people">https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people</a>. Retrieved May 3, 2019.</li>
<li><em>Building Numpy</em> from source: <a href="http://robpatro.com/blog/?p=47">http://robpatro.com/blog/?p=47</a>. Retrieved May 5, 2019. </li>
<li><em>Python—Compiling Numpy with OpenBLAS integration</em>: <a href="https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration">https://stackoverflow.com/questions/11443302/compiling-numpy-with-openblas-integration</a>. Retrieved May 5, 2019. </li>
<li><em>Issues—TensorFlow</em>: <a href="https://github.com/tensorflow/tensorflow/issues">https://github.com/tensorflow/tensorflow/issues</a>. Retrieved May 5, 2019.</li>
<li><em>Python Wheels</em>: <a href="https://pythonwheels.com/">https://pythonwheels.com/</a>. Retrieved May 5, 2019. </li>
<li><em>Chocolateay—The Package Manager for Windows</em>: <a href="https://chocolatey.org/">https://chocolatey.org/</a>. Retrieved May 5, 2019. </li>
<li><em>Docker Deployment on Azure</em>: <a href="https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/">https://azure.microsoft.com/en-gb/services/kubernetes-service/docker/</a>. Retrieved May 5, 2019. </li>
<li><em>What is Docker? | AWS</em>: <a href="https://aws.amazon.com/docker/">https://aws.amazon.com/docker/</a>. Retrieved May 5, 2019. </li>
<li><em>Docker Provider for Terraform</em>: <a href="https://www.terraform.io/docs/providers/docker/r/container.html">https://www.terraform.io/docs/providers/docker/r/container.html</a>. Retrieved May 5, 2019. </li>
<li><em>Chef Cookbook for Docker</em>: <a href="https://github.com/chef-cookbooks/docker">https://github.com/chef-cookbooks/docker</a>. Retrieved May 5, 2019. </li>
</ol>
<ol start="12">
<li><em>Docker<span>—</span>manage Docker containers</em><a href="https://docs.ansible.com/ansible/2.6/modules/docker_module.html">: https://docs.ansible.com/ansible/2.6/modules/docker_module.html</a>. Retrieved May 5, 2019. </li>
<li>cmd/go: build: add static flag: <a href="https://github.com/golang/go/issues/26492">https://github.com/golang/go/issues/26492</a>. Retrieved May 5, 2019. </li>
<li><span><em>On Golang static binaries, cross-compiling, and plugins</em>: </span><a href="https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671">https://medium.com/@diogok/on-golang-static-binaries-cross-compiling-and-plugins-1aed33499671</a><span>. Retrieved May 5, 2019. </span></li>
<li><em>Saving model outside filesystem</em>: <a href="https://github.com/sjwhitworth/golearn/issues/220">https://github.com/sjwhitworth/golearn/issues/220</a>. Retrieved May 6, 2019. </li>
<li><em>Architecting for Scale</em>, Lee Atchison, 2016, O'Reilly Press. </li>
<li><em>Server-side I/O: Node.js vs PHP vs Java vs Go</em>: <a href="https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go">https://www.toptal.com/back-end/server-side-io-performance-node-php-java-go</a>. Retrieved May 6, 2019. </li>
<li><em>Zap</em>: <a href="https://github.com/uber-go/zap">https://github.com/uber-go/zap</a>. Retrieved May 6, 2019.</li>
<li><em>Logrus</em>: <a href="https://github.com/sirupsen/logrus">https://github.com/sirupsen/logrus</a>. Retrieved May 6, 2019.</li>
<li><em>Log</em>: <a href="https://github.com/apex/log">https://github.com/apex/log</a>. Retrieved May 6, 2019.</li>
<li><em>jq</em>: <a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>. Retrieved May 6, 2019. </li>
<li><em>Splunk</em>: <a href="https://www.splunk.com/">https://www.splunk.com/</a>. Retrieved May 6, 2019. </li>
<li><em>Datadog</em>: <a href="https://www.datadoghq.com/">https://www.datadoghq.com/</a>. Retrieved May 6, 2019. </li>
<li><em>logrus<span>—</span>GoDoc</em>: <a href="https://godoc.org/github.com/sirupsen/logrus#JSONFormatter">https://godoc.org/github.com/sirupsen/logrus#JSONFormatter</a>. Retrieved May 6, 2019.</li>
<li><em>Grafana</em>: <a href="https://grafana.com/">https://grafana.com/</a>. Retrieved May 6, 2019. </li>
<li><em>Bias of bad customer service interactions</em>: <a href="https://www.marketingcharts.com/digital-28628">https://www.marketingcharts.com/digital-28628</a>. Retrieved May 6, 2019. </li>
<li><em>Machine Learning on AWS</em>: <a href="https://aws.amazon.com/machine-learning/">https://aws.amazon.com/machine-learning/</a>. Retrieved May 6, 2019. </li>
<li><em>Azure Machine Learning Service</em>: <a href="https://azure.microsoft.com/en-gb/services/machine-learning-service/">https://azure.microsoft.com/en-gb/services/machine-learning-service/</a>. Retrieved May 6, 2019.</li>
<li><em>Cloud AI</em>: <a href="https://cloud.google.com/products/ai/">https://cloud.google.com/products/ai/</a>. Retrieved May 6, 2019.</li>
<li><em>Cloud Market Share Q4 2018 and Full Year 2018</em>: <a href="https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018">https://www.canalys.com/newsroom/cloud-market-share-q4-2018-and-full-year-2018</a>. Retrieved May 11, 2019.</li>
<li><em>Amazon Sagemaker Example</em>: <a href="https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb">https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb</a>. Retrieved May 11, 2019. </li>
<li><em>Sagemaker SDK for Go</em>: <a href="https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker">https://docs.aws.amazon.com/sdk-for-go/api/service/sagemaker</a>/. Retrieved May 11, 2019. </li>
</ol>
<ol start="33">
<li><em>An overview of Sagemaker</em>: <a href="https://www.youtube.com/watch?v=ym7NEYEx9x4">https://www.youtube.com/watch?v=ym7NEYEx9x4</a>. Retrieved May 11, 2019. </li>
<li><em>Rekognition Go SDK</em>: <a href="https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/">https://docs.aws.amazon.com/sdk-for-go/api/service/rekognition/</a>. Retrieved May 11, 2019. </li>
<li><em>Azure Stream Analytics integration with Azure Machine Learning</em>: <a href="https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial">https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-machine-learning-integration-tutorial</a>. Retrieved May 11, 2019. </li>
<li><em>Azure Go SDK</em>: <a href="https://github.com/Azure/azure-sdk-for-go">https://github.com/Azure/azure-sdk-for-go</a>. Retrieved May 11, 2019. </li>
<li><em>Consume web service</em>: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services">https://docs.microsoft.com/en-us/azure/machine-learning/studio/consume-web-services</a>. Retrieved May 11, 2019. </li>
<li><em>Quickstart: Using Go to call the Text Analytics API</em>. <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go">https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/go</a>. Retrieved May 11, 2019. </li>
<li><em>Cost comparison of deep learning hardware</em>: <a href="https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f">https://medium.com/bigdatarepublic/cost-comparison-of-deep-learning-hardware-google-tpuv2-vs-nvidia-tesla-v100-3c63fe56c20f</a>. Retrieved May 11, 2019. </li>
<li><em>Prediction Overview</em>: <a href="https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview">https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview</a>. Retrieved May 11, 2019.</li>
<li><em>Google AI Hub</em>: <a href="https://cloud.google.com/ai-hub/">https://cloud.google.com/ai-hub/</a>. Retrieved May 11, 2019. </li>
<li><em>Amazon ECR Managed Policies</em>: <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ecr_managed_policies.html</a>. Retrieved May 11, 2019. </li>
<li><em>App Service - Web App for Containers</em>: <a href="https://azure.microsoft.com/en-gb/services/app-service/containers/">https://azure.microsoft.com/en-gb/services/app-service/containers/</a>. Retrieved May 11, 2019. </li>
<li><em>Push Docker Image to Private Registry</em>: <a href="https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli">https://docs.microsoft.com/en-gb/azure/container-registry/container-registry-get-started-docker-cli</a>. Retrieved May 11, 2019. </li>
<li><em>Create Docker/Go app on Linux</em>: <a href="https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go">https://docs.microsoft.com/en-gb/azure/app-service/containers/quickstart-docker-go</a>. Retrieved May 11, 2019. </li>
<li><em>Container Registry</em>: <a href="https://cloud.google.com/container-registry/">https://cloud.google.com/container-registry/</a>. Retrieved May 11, 2019. </li>
<li><em>Quickstart for Docker</em>: <a href="https://cloud.google.com/cloud-build/docs/quickstart-docker">https://cloud.google.com/cloud-build/docs/quickstart-docker</a>. Retrieved May 11, 2019. </li>
<li><em>Mechanical Turk</em>: <a href="https://www.mturk.com/">https://www.mturk.com/</a>. Retrieved May 15, 2019. </li>
<li><em>Shrink your Go binaries with this one weird trick</em>: <a href="https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/">https://blog.filippo.io/shrink-your-go-binaries-with-this-one-weird-trick/</a>. Retrieved May 16th, 2019. </li>
</ol>


            </article>

            
        </section>
    </body></html>