<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;2.&#xA0;Let's Help Machines Learn" id="KVCC1-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Let's Help Machines Learn</h1></div></div></div><p class="calibre8">Machine learning, when you first hear it, sounds more like a fancy word from a sci-fi movie than the latest trend in the tech industry. Talk about it to people in general and their responses are either related to being generally curious about the concept or being cautious and fearful about intelligent machines taking over our world in some sort of Terminator-Skynet way.</p><p class="calibre8">We live in a digital age and are constantly presented with all sorts of information all the time. As we will see in this and the coming chapters, machine learning is something that loves data. In fact, the recent hype and interest in this field has been fueled by not just the improvements in computing technology but also due to exponential growth in the amount of data being generated every second. The latest numbers stand at around 2.5 quintillion bytes of data every day (that's 2.5 followed by 18 zeroes)!</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note04" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Fun Fact</strong></span>: More than 300 hours of video data is uploaded to YouTube every minute</p><p class="calibre8">Source: <a class="calibre1" href="https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html">https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html</a></p></div><p class="calibre8">Just take a deep breath and look around. Everything around you is generating data all the time, of all sorts; your phone, your car, the traffic signals, GPS, thermostats, weather systems, social networks, and on and on and on! There is data everywhere and we can do all sorts of interesting things with it and help the systems learn. Well, as fascinating as it sounds, let us start our journey on machine learning. Through this chapter we will cover:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Understanding machine learning</li><li class="listitem">Algorithms in machine learning and their application</li><li class="listitem">Families of algorithms: supervised and unsupervised</li></ul></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Let's Help Machines Learn" id="KVCC1-973e731d75c2419489ee73e3a0cf4be8">
<div class="book" title="Understanding machine learning"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec16" class="calibre1"/>Understanding machine learning</h1></div></div></div><p class="calibre8">Aren't we <a id="id85" class="calibre1"/>taught that computer systems have to be programmed to do certain tasks? They may be a million times faster at doing things but they have to be programmed. We have to code each and every step and only then do these systems work and complete a task. Isn't then the very notion of machine learning a very contradictory concept?</p><p class="calibre8">In the simplest ways, machine learning refers to a method of teaching the systems to learn to do certain tasks, such as learning a function. As simple as it sounds, it is a bit confusing and difficult to digest. Confusing because our view of the way the systems (computer systems specifically) work and the way we learn are two concepts that hardly intersect. It is even more difficult to digest because learning, though an inherent capability of the human race, is difficult to put in to words, let alone teach to the systems.</p><p class="calibre8">Then what is machine learning? Before we even try to answer this question, we need to understand that at a philosophical level it is something more than just a way to program. Machine learning is a lot of things.</p><p class="calibre8">There are many ways in which machine learning can be described. Continuing from the high level definition we presented in the previous chapter, let us go through the definition given by Tom Mitchell in 1997:</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre17"><span class="strong"><em class="calibre10">"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."</em></span></p></blockquote></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note05" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Quick Note about Prof Tom Mitchell</strong></span></p><p class="calibre8">Born in 1951, he is an American computer scientist and professor at Carnegie Mellon University (CMU). He is also the chair of the machine learning department at CMU. He is well known for his contributions in the fields of machine learning, artificial intelligence, and cognitive neuroscience. He is part of various institutions such as the Association for the Advancement of Artificial Intelligence.</p></div><p class="calibre8">Now let us try to make sense out of this concise yet powerful definition with the help of an example. Let us say we want to build a system that predicts the weather. For the current example, the task (T) of the system would be to predict the weather for a certain place. To perform such a task, it needs to rely upon weather information from the past. We shall term it as experience E. Its performance (P) is measured on how well it predicts the weather at any given day. Thus, we can generalize that a system has successfully learned how to predict the weather (or task T) if it gets better at predicting it (or improves its performance P) utilizing the past information (or experience E).</p><p class="calibre8">As seen in the preceding example, this definition not only helps us understand machine learning from an engineering point of view, it also gives us tools to quantify the terms. The definition helps us with the fact that learning a particular task involves understanding and processing of the data in the form of experience. It also mentions that if a computer program learns, its <a id="id86" class="calibre1"/>performance improves with experience, pretty similar to the way we learn.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Algorithms in machine learning"><div class="book" id="LTSU2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec17" class="calibre1"/>Algorithms in machine learning</h1></div></div></div><p class="calibre8">So far we have <a id="id87" class="calibre1"/>developed an abstract understanding of machine learning. We understand the definition of machine learning which states that a task T can be learned by a computer program utilizing data in the form of experience E when its performance P improves with it. We have also seen how machine learning is different from conventional programming paradigms because of the fact that we do not code each and every step, rather we let the program form an understanding of the problem space and help us solve it. It is rather surprising to see such a program work right in front of us.</p><p class="calibre8">All along while we learned about the concept of machine learning, we treated this magical computer program as a mysterious black box which learns and solves the problems for us. Now is the time we unravel its enigma and look under the hood and see these magical algorithms in full glory.</p><p class="calibre8">We will begin with some of the most commonly and widely used algorithms in machine learning, looking at their intricacies, usage, and a bit of mathematics wherever necessary. Through this chapter, you will be introduced to different families of algorithms. The list is by no means exhaustive and, even though the algorithms will be explained in fair detail, a deep theoretical understanding of each of them is beyond the scope of this book. There is tons of material easily available in the form of books, online courses, blogs, and more.</p></div>

<div class="book" title="Algorithms in machine learning">
<div class="book" title="Perceptron"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec29" class="calibre1"/>Perceptron</h2></div></div></div><p class="calibre8">This is like the <code class="email">Hello World</code> <a id="id88" class="calibre1"/>algorithm of the machine learning universe. It may be one of the easiest of the lot to understand and use but it is by no means any less powerful.</p><p class="calibre8">Published in 1958 by <a id="id89" class="calibre1"/>Frank Rosenblatt, the perceptron algorithm gained much attention because of its guarantee to find a separator in a separable data set.</p><p class="calibre8">A perceptron is a function (or a simplified neuron to be precise) which takes a vector of real numbers as input and generates a real number as output.</p><p class="calibre8">Mathematically, a perceptron can be represented as:</p><div class="mediaobject"><img src="../images/00050.jpeg" alt="Perceptron" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Where, <code class="email">w1,…,wn</code> are weights, <code class="email">b</code> is a constant termed as bias, <code class="email">x1,…,xn</code> are inputs, and <code class="email">y</code> is the output of the function <code class="email">f</code>, which is called the activation function.</p><p class="calibre8">The algorithm is as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Initialize weight vector <code class="email">w</code> and bias <code class="email">b</code> to small random numbers.</li><li class="listitem" value="2">Calculate the output vector <code class="email">y</code> based on the function <code class="email">f</code> and vector <code class="email">x</code>.</li><li class="listitem" value="3">Update the weight vector <code class="email">w</code> and bias <code class="email">b</code> to counter the error.</li><li class="listitem" value="4">Repeat steps 2 and 3 until there is no error or the error drops below a certain threshold.</li></ol><div class="calibre14"/></div><p class="calibre8">The algorithm tries to find a separator which divides the input into two classes by using a labeled data set called the training data set (the training data set corresponds to the experience E as stated in the definition for machine learning in the previous section). The algorithm starts by assigning random weights to the weight vector <code class="email">w</code> and the bias <code class="email">b</code>. It then processes the input based on the function <code class="email">f</code> and gives a vector <code class="email">y</code>. This generated output is then compared with the correct output value from the training data set and the updates are made to <code class="email">w</code> and <code class="email">b</code> respectively. To understand the weight update process, let us consider a point, say <code class="email">p1</code>, with a correct output value of <code class="email">+1</code>. Now, suppose if the perceptron misclassifies <code class="email">p1</code> as <code class="email">-1</code>, it updates the weight <code class="email">w</code> and bias <code class="email">b</code> to move the perceptron by a small amount (movement is restricted by learning rate to prevent sudden jumps) in the direction of <code class="email">p1</code> in order to correctly classify it. The algorithm stops when it finds the correct separator <a id="id90" class="calibre1"/>or when the error in classifying the inputs drops below a certain user defined threshold.</p><p class="calibre8">Now, let us see the algorithm in action with the help of small example.</p><p class="calibre8">For the algorithm to work, we need a linearly separable data set. Let us assume the data is generated by the following function:</p><div class="mediaobject"><img src="../images/00051.jpeg" alt="Perceptron" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Based on the preceding equation, the correct separator will be given as:</p><div class="mediaobject"><img src="../images/00052.jpeg" alt="Perceptron" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Generating an input vector <code class="email">x</code> using uniformly distributed data in R is done as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#30 random numbers between -1 and 1 which are uniformly distributed</strong></span>
<span class="strong"><strong class="calibre9">x1 &lt;- runif(30,-1,1) </strong></span>
<span class="strong"><strong class="calibre9">x2 &lt;- runif(30,-1,1)</strong></span>
<span class="strong"><strong class="calibre9">#form the input vector x</strong></span>
<span class="strong"><strong class="calibre9">x &lt;- cbind(x1,x2)</strong></span>
</pre></div><p class="calibre8">Now that we have the data, we need a function to classify it into one of the two categories.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#helper function to calculate distance from hyperplane</strong></span>
<span class="strong"><strong class="calibre9">calculate_distance = function(x,w,b) {</strong></span>
<span class="strong"><strong class="calibre9"> sum(x*w) + b</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>

<span class="strong"><strong class="calibre9">#linear classifier</strong></span>
<span class="strong"><strong class="calibre9">linear_classifier = function(x,w,b) {</strong></span>
<span class="strong"><strong class="calibre9">distances =apply(x, 1, calculate_distance, w, b)</strong></span>
<span class="strong"><strong class="calibre9">return(ifelse(distances &lt; 0, -1, +1))</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">The helper function, <code class="email">calculate_distance</code>, calculates the distance of each point from the separator, while <code class="email">linear_classifier</code> classifies each point either as belonging to class <code class="email">-1</code> or class <code class="email">+1</code>.</p><p class="calibre8">The perceptron <a id="id91" class="calibre1"/>algorithm then uses the preceding classifier function to <a id="id92" class="calibre1"/>find the correct separator using the training data set.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#function to calculate 2nd norm</strong></span>
<span class="strong"><strong class="calibre9">second_norm = function(x) {sqrt(sum(x * x))}</strong></span>

<span class="strong"><strong class="calibre9">#perceptron training algorithm</strong></span>
<span class="strong"><strong class="calibre9">perceptron = function(x, y, learning_rate=1) {</strong></span>

<span class="strong"><strong class="calibre9">w = vector(length = ncol(x)) # initialize w</strong></span>
<span class="strong"><strong class="calibre9">b = 0 # Initialize b</strong></span>
<span class="strong"><strong class="calibre9">k = 0 # count iterations</strong></span>

<span class="strong"><strong class="calibre9">#constant with value greater than distance of furthest point</strong></span>
<span class="strong"><strong class="calibre9">R = max(apply(x, 1, second_norm)) </strong></span>

<span class="strong"><strong class="calibre9">incorrect = TRUE # flag to identify classifier</strong></span>

<span class="strong"><strong class="calibre9">#initialize plot</strong></span>
<span class="strong"><strong class="calibre9">plot(x,cex=0.2)</strong></span>

<span class="strong"><strong class="calibre9">#loop till correct classifier is not found</strong></span>
<span class="strong"><strong class="calibre9">while (incorrect ) {</strong></span>

<span class="strong"><strong class="calibre9">      incorrect =FALSE </strong></span>

<span class="strong"><strong class="calibre9">   #classify with current weights</strong></span>
<span class="strong"><strong class="calibre9">      yc &lt;- linear_classifier(x,w,b)</strong></span>
<span class="strong"><strong class="calibre9">      #Loop over each point in the input x</strong></span>
<span class="strong"><strong class="calibre9">      for (i in 1:nrow(x)) {</strong></span>
<span class="strong"><strong class="calibre9">        #update weights if point not classified correctly</strong></span>
<span class="strong"><strong class="calibre9">        if (y[i] != yc[i]) {</strong></span>
<span class="strong"><strong class="calibre9">          w &lt;- w + learning_rate * y[i]*x[i,]</strong></span>
<span class="strong"><strong class="calibre9">        b &lt;- b + learning_rate * y[i]*R^2</strong></span>
<span class="strong"><strong class="calibre9">        k &lt;- k+1</strong></span>

<span class="strong"><strong class="calibre9">          #currect classifier's components</strong></span>
<span class="strong"><strong class="calibre9">          # update plot after ever 5 iterations</strong></span>
<span class="strong"><strong class="calibre9">       if(k%%5 == 0){</strong></span>
<span class="strong"><strong class="calibre9">           intercept &lt;- - b / w[[2]]</strong></span>
<span class="strong"><strong class="calibre9">          slope &lt;- - w[[1]] / w[[2]]</strong></span>
<span class="strong"><strong class="calibre9">          #plot the classifier hyper plane</strong></span>
<span class="strong"><strong class="calibre9">          abline(intercept,slope,col="red")</strong></span>
<span class="strong"><strong class="calibre9">          #wait for user input</strong></span>
<span class="strong"><strong class="calibre9">          cat ("Iteration # ",k,"\n")</strong></span>
<span class="strong"><strong class="calibre9">          cat ("Press [enter] to continue")</strong></span>
<span class="strong"><strong class="calibre9">          line &lt;- readline()</strong></span>
<span class="strong"><strong class="calibre9">        }</strong></span>
<span class="strong"><strong class="calibre9">       incorrect =TRUE</strong></span>
<span class="strong"><strong class="calibre9">       }</strong></span>
<span class="strong"><strong class="calibre9">     }</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>

<span class="strong"><strong class="calibre9">s = second_norm(w)</strong></span>
<span class="strong"><strong class="calibre9">#scale the classifier with unit vector</strong></span>
<span class="strong"><strong class="calibre9">return(list(w=w/s,b=b/s,updates=k))</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">It's now time to train the perceptron!</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#train the perceptron</strong></span>
<span class="strong"><strong class="calibre9">p &lt;- perceptron(x,Y)</strong></span>
</pre></div><p class="calibre8">The plot will look as follows:</p><div class="mediaobject"><img src="../images/00053.jpeg" alt="Perceptron" class="calibre11"/><div class="caption"><p class="calibre18">The perceptron working its way to find the correct classifier. The correct classifier is shown in green</p></div></div><p class="calibre12"> </p><p class="calibre8">The preceding plots show the perceptron's training state. Each incorrect classifier is shown with a red line. As <a id="id93" class="calibre1"/>shown, the perceptron ends after finding the correct classifier marked in green.</p><p class="calibre8">A zoomed-in view of the final separator can be seen as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#classify based on calculated </strong></span>
<span class="strong"><strong class="calibre9">y &lt;- linear_classifier(x,p$w,p$b)</strong></span>


<span class="strong"><strong class="calibre9">plot(x,cex=0.2)</strong></span>

<span class="strong"><strong class="calibre9">#zoom into points near the separator and color code them</strong></span>
<span class="strong"><strong class="calibre9">#marking data points as + which have y=1 and – for others</strong></span>
<span class="strong"><strong class="calibre9">points(subset(x,Y==1),col="black",pch="+",cex=2)</strong></span>
<span class="strong"><strong class="calibre9">points(subset(x,Y==-1),col="red",pch="-",cex=2)</strong></span>

<span class="strong"><strong class="calibre9"># compute intercept on y axis of separator</strong></span>
<span class="strong"><strong class="calibre9"># from w and b</strong></span>
<span class="strong"><strong class="calibre9">intercept &lt;- - p$b / p$w[[2]]</strong></span>

<span class="strong"><strong class="calibre9"># compute slope of separator from w</strong></span>
<span class="strong"><strong class="calibre9">slope &lt;- - p$w[[1]] /p$ w[[2]]</strong></span>

<span class="strong"><strong class="calibre9"># draw separating boundary</strong></span>
<span class="strong"><strong class="calibre9">abline(intercept,slope,col="green")</strong></span>
</pre></div><p class="calibre8">The plot <a id="id94" class="calibre1"/>looks as <a id="id95" class="calibre1"/>shown in the following figure:</p><div class="mediaobject"><img src="../images/00054.jpeg" alt="Perceptron" class="calibre11"/><div class="caption"><p class="calibre18">The correct classifier function as found by perceptron</p></div></div><p class="calibre12"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Families of algorithms"><div class="book" id="MSDG2-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec18" class="calibre1"/>Families of algorithms</h1></div></div></div><p class="calibre8">There are tons of <a id="id96" class="calibre1"/>algorithms in the machine learning universe and more are devised each year. There is tremendous research happening in this space and hence the ever increasing list of algorithms. It is also a fact that the more these algorithms are being used, the more improvements in them are being discovered. Machine learning is one space where industry and academia are running hand in hand.</p><p class="calibre8">But, as Spider-Man was <a id="id97" class="calibre1"/>told that <span class="strong"><em class="calibre10">with great power comes great responsibility</em></span>, the reader should also understand the responsibility at hand. With so many algorithms available, it is necessary to understand what they are and where they fit. It can feel overwhelming and confusing at first but that is when categorizing them into families helps.</p><p class="calibre8">Machine learning algorithms can be categorized in many ways. The most common way is to group them into supervised learning algorithms and unsupervised learning algorithms.</p></div>

<div class="book" title="Families of algorithms">
<div class="book" title="Supervised learning algorithms"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec30" class="calibre1"/>Supervised learning algorithms</h2></div></div></div><p class="calibre8">Supervised learning <a id="id98" class="calibre1"/>refers to algorithms which are trained on a predefined data set called the training data set. The training data set is usually a two element tuple consisting of an input element and a desired output element or signal. In general, the input element is a vector. The supervised learning algorithm uses the training data set to produce the desired function. The function so produced (or rather inferred) is then utilized to correctly map new data, better termed as the test data.</p><p class="calibre8">An algorithm which <a id="id99" class="calibre1"/>has learnt well will be able to correctly determine the outputs for unseen data in a reasonable way. This brings in the concepts of generalization and overfitting.</p><p class="calibre8">Briefly, generalization refers to the concept wherein an algorithm generalizes the desired function based upon the (limited) training data to handle unseen data in a correct manner. Overfitting is exactly the opposite concept of generalization, wherein an algorithm infers a function such that it maps exactly to the training data set (including the noise). This may result in huge errors when the function learnt by the algorithm is checked against new/unseen data.</p><p class="calibre8">Both generalization and overfitting revolve around the random errors or noise in the input data. While generalization tries to minimize the effect of noise, overfitting does the opposite by fitting in noise as well.</p><p class="calibre8">Problems to be solved using supervised methods can be divided into the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre9">Prepare training data</strong></span>: Data preparation is the most important step for all machine <a id="id100" class="calibre1"/>learning algorithms. Since supervised learning utilizes labeled input data sets (data sets which consist of corresponding outputs for given inputs), this step becomes even more important. This data is usually labeled by human experts or from measurements.</li><li class="listitem" value="2"><span class="strong"><strong class="calibre9">Prepare the model</strong></span>: The model is the representation of the input data set and the learnt pattern. The model representation is affected by factors such as input features and the learning algorithm itself. The accuracy of the inferred function also depends on how this representation is formed.</li><li class="listitem" value="3"><span class="strong"><strong class="calibre9">Choose an algorithm</strong></span>: Based on the problem being solved and the input information, an algorithm is then chosen to learn and solve the problem.</li><li class="listitem" value="4"><span class="strong"><strong class="calibre9">Examine and fine tune</strong></span>: This is an iterative step where the algorithm is run on the input data <a id="id101" class="calibre1"/>set and the parameters are fine tuned to achieve the desired level of output. The algorithm is then tested on the test data set to evaluate its performance and measure the error.</li></ol><div class="calibre14"/></div><p class="calibre8">Under supervised <a id="id102" class="calibre1"/>learning, two major sub categories are:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre9">Regression based machine learning</strong></span>: Learning algorithms which help us answer <a id="id103" class="calibre1"/>quantitative questions such as how many? or how much? The outputs are generally continuous values. More formally, these algorithms predict the output values for unseen/new data based on the training data and the model formed. The output values are continuous in this case. Linear regression, multivariate regression, regression trees, and so on are a few supervised regression algorithms.</li><li class="listitem" value="2"><span class="strong"><strong class="calibre9">Classification based machine learning</strong></span>: Learning algorithms which help us answer objective questions or yes-or-no predictions. For example, questions such as is this component faulty? or can this tumor cause cancer? More formally, these algorithms predict the class labels for unseen or new data based upon <a id="id104" class="calibre1"/>the training data and model <a id="id105" class="calibre1"/>formed. <span class="strong"><strong class="calibre9">Support Vector Machines</strong></span> (<span class="strong"><strong class="calibre9">SVM</strong></span>), decision trees, random forests, and so on are a few commonly used supervised classification algorithms.</li></ol><div class="calibre14"/></div><p class="calibre8">Let us look at some supervised learning algorithms in detail.</p><div class="book" title="Linear regression"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec13" class="calibre1"/>Linear regression</h3></div></div></div><p class="calibre8">Regression, as <a id="id106" class="calibre1"/>mentioned previously, helps us answer quantitative <a id="id107" class="calibre1"/>questions. Regression has its roots in the statistics domain. Researchers use a linear relationship to predict the output value <code class="email">Y</code> for a given input value <code class="email">X</code>. This linear relationship is called a linear regression or regression line.</p><p class="calibre8">Mathematically, linear regression is represented as:</p><div class="mediaobject"><img src="../images/00055.jpeg" alt="Linear regression" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Where, <code class="email">b<sub class="calibre19">0</sub></code> is the intercept or the point where the line crosses the <code class="email">y</code> axis.</p><p class="calibre8"><code class="email">b<sub class="calibre19">1</sub></code> is the slope of the line, that is, the change in <code class="email">y</code> over change in <code class="email">x</code>.</p><p class="calibre8">The preceding equation is pretty similar to how a straight line is represented and hence the name linear regression.</p><p class="calibre8">Now, how do we decide which line we fit for our input so that it predicts well for unknown data? Well, for this we need an error measure. There can be various error measures; the most commonly used is the <span class="strong"><strong class="calibre9">least squares method</strong></span>.</p><p class="calibre8">Before we define the least squares method, we first need to understand the term residual. Residual is simply the <a id="id108" class="calibre1"/>deviation of Y from a fitted value. Mathematically:</p><div class="mediaobject"><img src="../images/00056.jpeg" alt="Linear regression" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Where, <code class="email">ŷ<sub class="calibre19">i</sub></code> is the deviated value of <code class="email">y</code>.</p><p class="calibre8">The least squares method states that the most optimal fit of a model to data occurs when the sum of the squares of residuals is minimum.</p><p class="calibre8">Mathematically:</p><div class="mediaobject"><img src="../images/00057.jpeg" alt="Linear regression" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">We use calculus to minimize the sum of squares for residuals and find the corresponding coefficients.</p><p class="calibre8">Now that we understand linear regression, let us take a real world example to see it in action.</p><p class="calibre8">Suppose we have data related to the height and weight of school children. The data scientist in you suddenly starts thinking about whether there is any relation between the weights and heights of these children. Formally, could the weight of a child be predicted based upon his/her given height?</p><p class="calibre8">To fit in linear regression, the first step is to understand the data and see whether there is a correlation between the two variables (<code class="email">weight</code> and <code class="email">height</code>). Since in this case we are dealing with just two dimensions, visualizing the data using a scatter plot will help us understand it quickly. This will also enable us to determine if the variables have some linear relationship or not.</p><p class="calibre8">Let us prepare our <a id="id109" class="calibre1"/>data first and visualize it on a scatter plot along with <a id="id110" class="calibre1"/>the correlation coefficient.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#Height and weight vectors for 19 children</strong></span>

<span class="strong"><strong class="calibre9">height &lt;- c(69.1,56.4,65.3,62.8,63,57.3,59.8,62.5,62.5,59.0,51.3,64,56.4,66.5,72.2,65.0,67.0,57.6,66.6)</strong></span>

<span class="strong"><strong class="calibre9">weight &lt;- c(113,84,99,103,102,83,85,113,84,99,51,90,77,112,150,128,133,85,112)</strong></span>

<span class="strong"><strong class="calibre9">plot(height,weight)</strong></span>
<span class="strong"><strong class="calibre9">cor(height,weight)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">[1] 0.8848454</strong></span>
</pre></div><p class="calibre8">The scatter plot looks like this:</p><div class="mediaobject"><img src="../images/00058.jpeg" alt="Linear regression" class="calibre11"/><div class="caption"><p class="calibre18">Figure displaying data points across weight and height dimensions</p></div></div><p class="calibre12"> </p><p class="calibre8">The preceding scatter plot proves that our intuition about weight and height having a linear relationship was correct. This can further be confirmed using the correlation function, which gives us a value of <code class="email">0.88</code>.</p><p class="calibre8">Time to prepare the <a id="id111" class="calibre1"/>model for our data set! We use the inbuilt utility <code class="email">lm</code> or the <a id="id112" class="calibre1"/>linear model utility to find the coefficients <code class="email">b<sub class="calibre19">0</sub></code> and <code class="email">b<sub class="calibre19">1</sub></code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#Fitting the linear model</strong></span>

<span class="strong"><strong class="calibre9">model &lt;- lm(weight ~ height) # weight = slope*weight + intercept</strong></span>

<span class="strong"><strong class="calibre9">#get the intercept(b0) and the slope(b1) values</strong></span>
<span class="strong"><strong class="calibre9">model</strong></span>
</pre></div><p class="calibre8">The output looks like this:</p><div class="mediaobject"><img src="../images/00059.jpeg" alt="Linear regression" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">You can experiment a bit more to find out more details calculated by the <code class="email">lm</code> utility using the following commands. We <a id="id113" class="calibre1"/>encourage you to go ahead and try these out.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#check all attributes calculated by lm</strong></span>
<span class="strong"><strong class="calibre9">attributes(model)</strong></span>

<span class="strong"><strong class="calibre9">#getting only the intercept</strong></span>
<span class="strong"><strong class="calibre9">model$coefficients[1] #or model$coefficients[[1]]</strong></span>

<span class="strong"><strong class="calibre9">#getting only the slope</strong></span>
<span class="strong"><strong class="calibre9">model$coefficients[2] #or model$coefficients[[2]]</strong></span>

<span class="strong"><strong class="calibre9">#checking the residuals</strong></span>
<span class="strong"><strong class="calibre9">residuals(model)</strong></span>

<span class="strong"><strong class="calibre9">#predicting the weight for a given height, say 60 inches</strong></span>
<span class="strong"><strong class="calibre9">model$coefficients[[2]]*50 + model$coefficients[[1]]</strong></span>

<span class="strong"><strong class="calibre9">#detailed information about the model</strong></span>
<span class="strong"><strong class="calibre9">summary(model)</strong></span>
</pre></div><p class="calibre8">As the final piece, let us visualize the regression line on our scatter plot itself.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#plot data points</strong></span>
<span class="strong"><strong class="calibre9"> plot(height,weight)</strong></span>

<span class="strong"><strong class="calibre9">#draw the regression line</strong></span>
<span class="strong"><strong class="calibre9">abline(model)</strong></span>
</pre></div><p class="calibre8">The scatter plot looks like the following figure:</p><div class="mediaobject"><img src="../images/00060.jpeg" alt="Linear regression" class="calibre11"/><div class="caption"><p class="calibre18">Scatter plot with regression calculated regression line</p></div></div><p class="calibre12"> </p><p class="calibre8">Thus, we saw how the relationship between two variables can be identified and predictions can be made using a few lines of code. But we are not done yet. There are a couple of caveats which the reader must <a id="id114" class="calibre1"/>understand before deciding whether to use linear regression or not.</p><p class="calibre8">Linear regression can be used to predict the output values for given inputs, if and only if:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The scatter plot forms a linear pattern</li><li class="listitem">The correlation between them is moderate to strong (beyond <code class="email">0.5</code> or <code class="email">-0.5</code>)</li></ul></div><p class="calibre8">Cases where only one of the previous two conditions are met can lead to incorrect predictions or altogether invalid models. For example, if we only check the correlation and find it to be strong and skip the step where we look at the scatter plot, then that may lead to invalid predictions as you might have tried to fit a straight line when the data itself is following a curved shape (note that curved data sets can also have high correlation values and hence the mistake).</p><p class="calibre8">It is important to remember that <span class="strong"><strong class="calibre9">correlation does not imply causation</strong></span>. In simple words, correlation between two <a id="id115" class="calibre1"/>variables does not necessarily imply that one causes the other. There might be a case that cause and effect are indirectly related due to a third variable termed as the cofounding variable. The most common example used to describe this issue is the relationship between shoe size and reading ability. From the survey data (if one existed!) it could be inferred that larger shoe sizes relate to higher reading ability, but this clearly does not mean that large feet cause good reading skills to be developed. It might be rather interesting to note that young children have small feet and have not yet been taught to read. In such a case, the two variables are more accurately correlated with age.</p><p class="calibre8">You should now be coming to similar conclusion about the weight to height example we used earlier. Well yes, the earlier example also suffers from similar fallacy, yet it served as an easy to use scenario. Feel free to look around and model cases which do not suffer from this.</p><p class="calibre8">Linear regression finds application in the field of finance where it is used for things such as quantification of risks on investments. It is also used widely in the field of economics for trendline analysis and so on.</p><p class="calibre8">Apart from linear regression, <span class="strong"><strong class="calibre9">logistic regression</strong></span>, <span class="strong"><strong class="calibre9">stepwise regression</strong></span>, <span class="strong"><strong class="calibre9">Multivariate Adaptive </strong></span><a id="id116" class="calibre1"/>
<span class="strong"><strong class="calibre9">Regression Splines</strong></span> (<span class="strong"><strong class="calibre9">MARS</strong></span>), and <a id="id117" class="calibre1"/>others are a few more supervised regression learning algorithms.</p></div><div class="book" title="K-Nearest Neighbors (KNN)"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec14" class="calibre1"/>K-Nearest Neighbors (KNN)</h3></div></div></div><p class="calibre8">K-Nearest <a id="id118" class="calibre1"/>Neighbors or KNN algorithms are <a id="id119" class="calibre1"/>amongst the simplest algorithms from an implementation and understanding point of view. They are yet another type of supervised learning algorithm which helps us classify data.</p><p class="calibre8">KNN can be easily described using the quote <span class="strong"><em class="calibre10">like and like strike together—Plato</em></span>, that is, similar things are likely have similar properties. KNN utilizes this very concept to label a data point based upon its similarity with respect to its neighbors.</p><p class="calibre8">Formally, KNN can be described as the process to classify unlabeled (or unseen) data points by assigning them the class of the most similar labeled data points (or training samples).</p><p class="calibre8">KNN is a supervised learning algorithm. Hence, it begins with a training data set of examples which are classified into different classes. The algorithm then picks each data point in the test data set and, based upon a chosen similarity measure, identifies its <span class="strong"><em class="calibre10">k</em></span> nearest neighbors (where k is specified in advance). The data point is then assigned the class of the majority of the k nearest neighbors.</p><p class="calibre8">The trick up the KNN's sleeve is the similarity measure. There are various similarity measures available to us. The decision to choose one is based on the complexity of the problem, the type of data, and so on. Euclidean distance is one such measure which is widely used. Euclidean distance is the shortest direct route between two points. Mathematically it is given as:</p><div class="mediaobject"><img src="../images/00061.jpeg" alt="K-Nearest Neighbors (KNN)" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Manhattan distance, Cosine distance, and Minkowski distance are some other types of distance measures which can be used for finding the nearest neighbors.</p><p class="calibre8">The next parameter for KNN algorithm is the <code class="email">k</code> in the K-Nearest Neighbors. The value of k determines how well the KNN model generalizes to the test data. The balance between overfitting and underfitting the training data depends upon the value of <code class="email">k</code>. With slight deliberation, it is easy to understand that a large <code class="email">k</code> will minimize the impact of variance caused by noisy data, but at the same time, it will also undermine small but important patterns in the data. This problem is called the <span class="strong"><strong class="calibre9">bias-variance tradeoff</strong></span>.</p><p class="calibre8">The optimal value of k, even though hard to determine, lies between the extremes of <code class="email">k=1</code> to <code class="email">k=total number of training samples</code>. A common practice is to set the value of <code class="email">k</code> equal to the square root of training instances, usually between 3 and 10. Though a common practice, the value of <code class="email">k</code> is dependent on the complexity of the concept to be learned and the number of training examples.</p><p class="calibre8">The next step in pursuit of the KNN algorithm is preparation of data. Features used to prepare the input vectors should be on similar scales. The rationale for this step is that the distance formula is dependent on how the features are measured. For example, if certain features have large range of values as compared to others, the distance measurements will then be dominated by such measures. The method of scaling features to a similar scale is called <span class="strong"><strong class="calibre9">normalization</strong></span>. Very much like the distance measure, there are various normalization methods available. One such method is min-max normalization, given mathematically as:</p><div class="mediaobject"><img src="../images/00062.jpeg" alt="K-Nearest Neighbors (KNN)" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Before we begin with our example to understand KNN, let us outline the steps to be performed to execute KNN:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre9">Collect data and explore data</strong></span>: We need to collect data relevant to the concept to be learnt. We also need to explore the data to understand various features, know the range of their values, and determine the class labels.</li><li class="listitem" value="2"><span class="strong"><strong class="calibre9">Normalize data</strong></span>: As discussed previously, KNN's dependence on distance measure makes it very important that we normalize the data to remove any inconsistency or bias in calculations.</li><li class="listitem" value="3"><span class="strong"><strong class="calibre9">Create training and test data sets</strong></span>: Since it is important to learn a concept and prepare a model which generalizes to acceptable levels for unseen data, we need to prepare training and test data sets. The test data set, even though labeled, is used to determine the accuracy and the ability of the model to generalize the concept learnt. A usual practice is to divide the input sample into two-third and one-third portions for training and test data sets respectively. It is equally important that the two data sets are a good mix of all the class labels and data points, that is both the data sets should be representative subsets of the full data.</li><li class="listitem" value="4"><span class="strong"><strong class="calibre9">Train the model</strong></span>: Now that we have all the things in place, we can use the training data set, test data set, the labels, and the value of <code class="email">k</code> to train our model and label the data points in the test data set.</li><li class="listitem" value="5"><span class="strong"><strong class="calibre9">Evaluate the model</strong></span>: The final step is to evaluate the learnt pattern. In this step, we <a id="id120" class="calibre1"/>determine how well the algorithm has predicted the class labels of the test data set as compared to their known labels. Usually a confusion matrix is prepared for the same.</li></ol><div class="calibre14"/></div><p class="calibre8">Now, let us see KNN in action. The problem at hand is to classify different species of flowers based upon certain features. For this particular example, we will be using the Iris data set. This data set comes built in with the default installation of R.</p><div class="book" title="Collecting and exploring data"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch02lvl4sec01" class="calibre1"/>Collecting and exploring data</h4></div></div></div><p class="calibre8">Step one is to <a id="id121" class="calibre1"/>collect and explore the data. Let us first gather the data.</p><p class="calibre8">To check if your <a id="id122" class="calibre1"/>system has the required dataset, type in just the name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">iris</strong></span>
<span class="strong"><strong class="calibre9">#this should print the contents of data set onto the console.</strong></span>
</pre></div><p class="calibre8">If you do not have the data set available, no worries! You can download it as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#skip these steps if you already have iris on your system</strong></span>
<span class="strong"><strong class="calibre9">iris &lt;- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)</strong></span>

<span class="strong"><strong class="calibre9">#assign proper headers</strong></span>
<span class="strong"><strong class="calibre9">names(iris) &lt;- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")</strong></span>
</pre></div><p class="calibre8">Now that we have the data, it is time to explore and understand it. For exploring the data set and its attributes, we use the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#to view top few rows of data</strong></span>
<span class="strong"><strong class="calibre9">head(iris)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00063.jpeg" alt="Collecting and exploring data" class="calibre16"/></span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#to view data types, sample values, categorical values, etc</strong></span>
<span class="strong"><strong class="calibre9">str(iris)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00064.jpeg" alt="Collecting and exploring data" class="calibre16"/></span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#detailed view of the data set</strong></span>
<span class="strong"><strong class="calibre9">summary(iris)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00065.jpeg" alt="Collecting and exploring data" class="calibre16"/></span></p><p class="calibre8">The summary command helps us understand the data in a better way. It clearly shows different attributes along with <code class="email">min</code>, <code class="email">max</code>, <code class="email">median</code>, and other such statistics. These help us in the coming steps where we might have to scale or normalize the data or features.</p><p class="calibre8">During step one is where we usually label our input data. Since our current data set is already labeled, we can skip this step for this example problem. Let us visually see how the species are spread. We take help of the famous scatter plot again, but this time we use a package called <code class="email">ggvis</code>.</p><p class="calibre8">You can install <code class="email">ggvis</code> as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">install.packages("ggvis")</strong></span>
</pre></div><p class="calibre8">For visualizing petal widths and lengths for all 3 species, we use the following code snippet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#load the package</strong></span>
<span class="strong"><strong class="calibre9">library(ggvis)</strong></span>

<span class="strong"><strong class="calibre9">#plot the species</strong></span>
<span class="strong"><strong class="calibre9">iris %&gt;% ggvis(~Petal.Length, ~Petal.Width, fill = ~factor(Species)) %&gt;% layer_points()</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note06" class="calibre1"/>Note</h3><p class="calibre8">The <code class="email">ggvis</code> package is an interactive graphics package in R. It follows a unique way of expressing inputs to generate visualizations. The preceding snippet of code uses the pipe operator, <code class="email">%&gt;%</code>, to pass input data to <code class="email">ggvis</code> and again uses the pipe operator to pass on the output to <code class="email">layer_points</code> for final plotting. The <code class="email">~</code> operator signifies to <code class="email">ggvis</code> that <code class="email">Petal.Length</code> is a variable in the input dataset (iris). Read more <a id="id123" class="calibre1"/>about <code class="email">ggvis</code> at <a class="calibre1" href="http://ggvis.rstudio.com/ggvis-basics.html">http://ggvis.rstudio.com/ggvis-basics.html</a>.</p></div><div class="mediaobject"><img src="../images/00066.jpeg" alt="Collecting and exploring data" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The preceding plot <a id="id124" class="calibre1"/>clearly shows that there is a high correlation between petal widths and lengths for <span class="strong"><strong class="calibre9">Iris-setosa</strong></span> flowers, while it is a little less for the other two species.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note07" class="calibre1"/>Note</h3><p class="calibre8">Try visualizing <a id="id125" class="calibre1"/>sepal width versus sepal length as well and see if you can spot any correlation.</p></div></div><div class="book" title="Normalizing data"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch02lvl4sec02" class="calibre1"/>Normalizing data</h4></div></div></div><p class="calibre8">The next step is to <a id="id126" class="calibre1"/>normalize the data so that all the features are on the same scale. As seen from the data exploration step, the values of all the attributes are more or less in a comparable range. But, for the sake of this example, let us write a min-max normalization function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#normalization function</strong></span>

<span class="strong"><strong class="calibre9">min_max_normalizer &lt;- function(x)</strong></span>
<span class="strong"><strong class="calibre9">{</strong></span>
<span class="strong"><strong class="calibre9">num &lt;- x - min(x) </strong></span>
<span class="strong"><strong class="calibre9">denom &lt;- max(x) - min(x)</strong></span>
<span class="strong"><strong class="calibre9">return (num/denom)</strong></span>
<span class="strong"><strong class="calibre9">}</strong></span>
</pre></div><p class="calibre8">Remember, normalization does not alter the data, it simply scales it. Therefore, even though our data does not require normalization, doing so will not cause any harm.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note08" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre9">Note</strong></span></p><p class="calibre8">In the following <a id="id127" class="calibre1"/>steps, we will be using un-normalized data for clarity of output.</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#normalizing iris data set</strong></span>
<span class="strong"><strong class="calibre9">normalized_iris &lt;- as.data.frame(lapply(iris[1:4], min_max_normalizer))</strong></span>

<span class="strong"><strong class="calibre9">#viewing normalized data</strong></span>
<span class="strong"><strong class="calibre9">summary(normalized_iris)</strong></span>
</pre></div><p class="calibre8">The following is a summary of the normalized data frame:</p><p class="calibre8"><span class="strong"><img src="../images/00067.jpeg" alt="Normalizing data" class="calibre16"/></span></p></div><div class="book" title="Creating training and test data sets"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch02lvl4sec03" class="calibre1"/>Creating training and test data sets</h4></div></div></div><p class="calibre8">Now that <a id="id128" class="calibre1"/>we have our data normalized, we can divide it into training and test data sets. We will follow the usual two-third one-third rule of splitting the data into two. As mentioned earlier, both data sets should be representative of the whole data and hence we need to pick proper samples. We will utilize R's <code class="email">sample()</code> function to prepare our samples.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#checking the data constituency</strong></span>
<span class="strong"><strong class="calibre9">table(iris$Species)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00068.jpeg" alt="Creating training and test data sets" class="calibre16"/></span></p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#set seed for randomization</strong></span>
<span class="strong"><strong class="calibre9">set.seed(1234)</strong></span>

<span class="strong"><strong class="calibre9"># setting the training-test split to 67% and 33% respectively</strong></span>
<span class="strong"><strong class="calibre9">random_samples &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))</strong></span>

<span class="strong"><strong class="calibre9"># training data set</strong></span>
<span class="strong"><strong class="calibre9">iris.training &lt;- iris[</strong></span>
<span class="strong"><strong class="calibre9">random_samples ==1, 1:4] </strong></span>

<span class="strong"><strong class="calibre9">#training labels</strong></span>
<span class="strong"><strong class="calibre9">iris.trainLabels &lt;- iris[</strong></span>
<span class="strong"><strong class="calibre9">random_samples ==1, 5]</strong></span>


<span class="strong"><strong class="calibre9"># test data set</strong></span>
<span class="strong"><strong class="calibre9">iris.test &lt;- iris[</strong></span>
<span class="strong"><strong class="calibre9">random_samples ==2, 1:4]</strong></span>

<span class="strong"><strong class="calibre9">#testing labels</strong></span>
<span class="strong"><strong class="calibre9">iris.testLabels &lt;- iris[</strong></span>
<span class="strong"><strong class="calibre9">random_samples ==2, 5]</strong></span>
</pre></div></div><div class="book" title="Learning from data/training the model"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch02lvl4sec04" class="calibre1"/>Learning from data/training the model</h4></div></div></div><p class="calibre8">Once we have the <a id="id129" class="calibre1"/>data ready in our training and test data sets, we can proceed to the next step and learn from the data using KNN. The KNN implementation in R is present in the class library. The KNN function takes the following inputs:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">train</code>: The data frame containing the training data.</li><li class="listitem"><code class="email">test</code>: The data frame containing the test data.</li><li class="listitem"><code class="email">class</code>: A vector containing the class labels. Also called the factor vector.</li><li class="listitem"><code class="email">k</code>: The value of k-nearest neighbors.</li></ul></div><p class="calibre8">For the current case, let us assume the value of <code class="email">k</code> to be <code class="email">3</code>. Odd numbers are usually good at breaking ties. KNN is executed as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#setting library</strong></span>
<span class="strong"><strong class="calibre9">library(class)</strong></span>

<span class="strong"><strong class="calibre9">#executing knn for k=3</strong></span>
<span class="strong"><strong class="calibre9">iris_model &lt;- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)</strong></span>

<span class="strong"><strong class="calibre9">#summary of the model learnt</strong></span>
<span class="strong"><strong class="calibre9">iris_model</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00069.jpeg" alt="Learning from data/training the model" class="calibre16"/></span></p><p class="calibre8">A quick scan of the output shows everything correct except one versicolor label amongst virginica's. Though <a id="id130" class="calibre1"/>this one was easy to spot, there are better ways to evaluate the model.</p></div><div class="book" title="Evaluating the model"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch02lvl4sec05" class="calibre1"/>Evaluating the model</h4></div></div></div><p class="calibre8">This brings us to the <a id="id131" class="calibre1"/>last step where we evaluate the model. We do this by preparing a confusion matrix or a cross table which helps us understand how the predicted labels are with respect to the known labels of the test data. R provides us with yet another utility function called <code class="email">CrossTable()</code> which is present in the <code class="email">gmodels</code> library. Let us see the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9">#setting library</strong></span>
<span class="strong"><strong class="calibre9">library(gmodels)</strong></span>

<span class="strong"><strong class="calibre9">#Preparing cross table</strong></span>
<span class="strong"><strong class="calibre9">CrossTable(x = iris.testLabels, y = iris_model, prop.chisq=FALSE)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00070.jpeg" alt="Evaluating the model" class="calibre16"/></span></p><p class="calibre8">From the preceding output we can conclude that the model has labeled one instance of virginica as versicolor while all the other test data points have been correctly labeled. This also helps us infer that the choice of <code class="email">k=3</code> was indeed good enough. We urge the reader to try the same example with different values of <code class="email">k</code> and see the change in results.</p><p class="calibre8">KNN is a simple yet powerful algorithm which makes no assumptions about the underlying data distribution and hence can be used in cases where relationships between features and classes are complex or difficult to understand.</p><p class="calibre8">On the downside, KNN is a resource intensive algorithm as it requires a large amount of memory to process the data. Dependency on distance measures and missing data requires additional processing which is another overhead with this algorithm.</p><p class="calibre8">Despite its limitations, KNN is used in a number of real life applications such as for text mining, predicting heart attacks, predicting cancer and so on. KNN also finds application in the field of finance and agriculture as well.</p><p class="calibre8">Decision trees, random <a id="id132" class="calibre1"/>forests, and support vector machines are some of the most popular and widely used supervised classification algorithms.</p></div></div></div></div>

<div class="book" title="Families of algorithms">
<div class="book" title="Unsupervised learning algorithms"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec31" class="calibre1"/>Unsupervised learning algorithms</h2></div></div></div><p class="calibre8">Unsupervised <a id="id133" class="calibre1"/>learning refers to algorithms which learn a concept(s) on their own. Now that we are familiar with the concept of supervised learning, let us utilize our knowledge to understand unsupervised learning.</p><p class="calibre8">Unlike supervised learning algorithms, which require a labeled input training data set, unsupervised learning algorithms are tasked with finding relationships and patterns in the data without any labeled training data set. These algorithms process the input data to mine for rules, detect patterns, summarize and group the data points which helps in deriving meaningful insights, and describing the data to the users. In the case of unsupervised learning algorithms, there is no concept of training and test data sets. Rather, as mentioned before, input data is analyzed and used to derive patterns and relationships.</p><p class="calibre8">Similar to supervised learning, unsupervised learning algorithms can also be divided into two main categories:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre9">Association rule based machine learning</strong></span>: These algorithms mine the input data to <a id="id134" class="calibre1"/>identify patterns and rules. The rules explain interesting relationships between the variables in the data set to depict frequent itemsets and patterns which occur in the data. These rules in turn help discover useful insights for any business or organization from their huge data repositories. Popular algorithms include Apriori and FP-Growth.</li><li class="listitem"><span class="strong"><strong class="calibre9">Clustering based machine learning</strong></span>: Similar to supervised learning based classification <a id="id135" class="calibre1"/>algorithms, the main objective of these algorithms is to cluster or group the input data points into different classes or categories using just features derived from the input data alone and no other external information. Unlike classification, the output labels are not known beforehand in clustering. Some popular clustering algorithms include k-means, k-medoids, and hierarchical clustering.</li></ul></div><p class="calibre8">Let us look at some unsupervised learning algorithms.</p><div class="book" title="Apriori algorithm"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec15" class="calibre1"/>Apriori algorithm</h3></div></div></div><p class="calibre8">This algorithm <a id="id136" class="calibre1"/>which took the world by storm was proposed by Agarwal and Srikant in 1993. The algorithm is designed to handle transactional data, where each transaction is a set of items, or itemset. The <a id="id137" class="calibre1"/>algorithm in short identifies the item sets which are subsets of at least C transactions in the data set.</p><p class="calibre8">Formally, let <code class="email">┬</code> be a set of items and <code class="email">D</code> be a set of transactions, where each transaction <code class="email">T</code> is a subset of <code class="email">┬</code>. Mathematically:</p><div class="mediaobject"><img src="../images/00071.jpeg" alt="Apriori algorithm" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Then an association rule is an implication of the form <code class="email">X → Y</code>, where a transaction <code class="email">T</code> contains <code class="email">X</code> as a subset of <code class="email">┬</code> and:</p><div class="mediaobject"><img src="../images/00072.jpeg" alt="Apriori algorithm" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The implication <code class="email">X → Y</code> holds true in the transaction set <code class="email">D</code> with a confidence factor <code class="email">c</code> if <code class="email">c%</code> of the transactions in <code class="email">D</code> that contain <code class="email">X</code> also contain <code class="email">Y</code>. The association rule <code class="email">X → Y</code> is said to have a support factor of <code class="email">s</code> if <code class="email">s%</code> of transactions in <code class="email">D</code> contain <code class="email">X U Y</code>. Hence, given a set of transactions <code class="email">D</code>, the task of identifying association rules implies generating all such rules that have confidence and support greater than a user defined thresholds called <code class="email">minsup</code> (for minimum support threshold) and <code class="email">minconf</code> (for minimum confidence threshold).</p><p class="calibre8">Broadly, the algorithm works in two steps. The first one being identification of the itemsets whose occurrence exceeds a predefined threshold. Such itemsets are called <span class="strong"><strong class="calibre9">Frequent Itemsets</strong></span>. The <a id="id138" class="calibre1"/>second step is to generate association rules from the identified frequent itemsets which satisfy the constraints of minimum confidence and support.</p><p class="calibre8">The same two steps can be explained better using the following pseudo-code:</p><div class="mediaobject"><img src="../images/00073.jpeg" alt="Apriori algorithm" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Now let us see the algorithm in action. The data set in consideration is the UCI machine learning repository's <code class="email">Adult</code> data set. The data set contains census data with attributes such as gender, age, marital status, native country, and occupation, along with economic attributes such as work class, income, and so on. We will use this data set to identify if there are association rules between census information and the income of the person.</p><p class="calibre8">The Apriori algorithm is present in the <code class="email">arules</code> library and the data set in consideration is named <code class="email">Adult.</code> It is also available with default R installation.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># setting the apriori library</strong></span>
<span class="strong"><strong class="calibre9">library(arules)</strong></span>

<span class="strong"><strong class="calibre9"># loading data</strong></span>
<span class="strong"><strong class="calibre9">data("Adult");</strong></span>
</pre></div><p class="calibre8">Time to explore our data set and see a few sample records:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># summary of data set</strong></span>
<span class="strong"><strong class="calibre9">summary(Adult);</strong></span>

<span class="strong"><strong class="calibre9"># Sample 5 records</strong></span>
<span class="strong"><strong class="calibre9">inspect(Adult[0:5]);</strong></span>
</pre></div><p class="calibre8">We get to know <a id="id139" class="calibre1"/>that the data set contains some <code class="email">48k</code> transactions with 115 columns. We also get information regarding the distribution of itemsets based on their sizes. The <code class="email">inspect</code> function gives us a peek into sample transactions and the values each of the columns hold.</p><p class="calibre8">Now, let us build some relationships:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># executing apriori with support=50% confidence =80%</strong></span>
<span class="strong"><strong class="calibre9">rules &lt;- apriori(Adult, parameter=list(support=0.5, confidence=0.8,target="rules"));</strong></span>

<span class="strong"><strong class="calibre9"># view a summary</strong></span>
<span class="strong"><strong class="calibre9">summary(rules);</strong></span>

<span class="strong"><strong class="calibre9">#view top 3 rules</strong></span>
<span class="strong"><strong class="calibre9">as(head(sort(rules, by = c("confidence", "support")), n=3), "data.frame")</strong></span>
</pre></div><p class="calibre8">The Apriori algorithm uses the <code class="email">Adult</code> data set as input to identify rules and patterns in the transactional data. On viewing the summary, we can see that the algorithm successfully identified <code class="email">84</code> rules meeting the support and confidence constraints of <code class="email">50%</code> and <code class="email">80%</code> respectively. Now that we have identified the rules, let us see what they are:</p><div class="mediaobject"><img src="../images/00074.jpeg" alt="Apriori algorithm" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The rules are of the form <code class="email">X→ Y</code> where <code class="email">X</code> is the <code class="email">lhs</code> or left-hand side and <code class="email">Y</code> is the <code class="email">rhs</code> or right-hand side. The preceding image displays corresponding confidence and support values as well. From the output we can infer that if people are working full time then their chances of facing capital loss is almost none (confidence factor 95.8%). Another rule helps us infer that people who work for a private employer also have close to no chance of facing a capital loss. Such rules can be used in preparing policies or schemes for social welfare, economic reforms, and so on.</p><p class="calibre8">Apart from Apriori, there <a id="id140" class="calibre1"/>are other association rule mining algorithms such as FP Growth, ECLAT, and many others which have been used for various applications over the years.</p></div><div class="book" title="K-Means"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec16" class="calibre1"/>K-Means</h3></div></div></div><p class="calibre8">In the world of <a id="id141" class="calibre1"/>unsupervised clustering algorithms, the most simple and widely used algorithm is K-Means. As we have seen recently, unsupervised learning algorithms process the input data without any prior labels or training <a id="id142" class="calibre1"/>to come up with patterns and relationships. Clustering algorithms in particular help us cluster or partition the data points.</p><p class="calibre8">By definition, clustering refers to the task of grouping objects into groups such that elements of a group are more similar to each other than those in other groups. K-Means does the same in an unsupervised manner.</p><p class="calibre8">Mathematically, given a set of <code class="email">n</code> observations <code class="email">{x1,x2,…,xn}</code>, where each observation is a <span class="strong"><em class="calibre10">d</em></span> dimensional vector, the algorithm tries to partition these <span class="strong"><em class="calibre10">n</em></span> observations into <code class="email">k (≤ n)</code> sets by minimizing an objective function.</p><p class="calibre8">As with the other algorithms, there can be different objective functions. For the sake of simplicity, we will use the most widely used function called the <span class="strong"><strong class="calibre9">with-in cluster sum of squares</strong></span> or <span class="strong"><strong class="calibre9">WCSS function</strong></span>.</p><div class="mediaobject"><img src="../images/00075.jpeg" alt="K-Means" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Here <code class="email">μ<sub class="calibre19">i</sub></code> is the mean of points in the partition <code class="email">S<sub class="calibre19">i</sub></code>.</p><p class="calibre8">The algorithm follows a simple two step iterative process, where the first step is called the assignment step, followed by the update step.</p><div class="book"><ul class="itemizedlist"><li class="listitem">Initialize by setting the means for <code class="email">k</code> partitions: <code class="email">m1,m2…mk</code></li><li class="listitem">Until the mean does not change or the change is lower than a certain threshold:<div class="book"><ol class="orderedlist1"><li class="listitem" value="1"><span class="strong"><strong class="calibre9">Assignment step</strong></span>: Assign each observation to a partition for which the with-in cluster sum of squares value is minimum, that is, assign the observation to a partition whose mean is closest to the observation.</li><li class="listitem" value="2"><span class="strong"><strong class="calibre9">Update step</strong></span>: For <code class="email">i</code> in <code class="email">1 to k</code>, update each mean <code class="email">mi</code> based on all the observations in that partition.</li></ol><div class="calibre14"/></div></li></ul></div><p class="calibre8">The algorithm can use different initialization methods. The most common ones are Forgy and Random Partition methods. I encourage you to read more on these. Also, apart from the input data set, the algorithm requires the value of <code class="email">k</code>, that is the number of clusters to be formed. The optimal value may depend on various factors and is generally decided based on the use case.</p><p class="calibre8">Let us see the algorithm in action.</p><p class="calibre8">We will again use the Iris flower data set which we already used for the KNN algorithm. For KNN we already had the species labeled and then tried to learn and classify the data points in the test data set into correct classes.</p><p class="calibre8">With K-Means, we also aim to achieve the same partitioning of the data but without any labeled training data set (or supervision).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># prepare a copy of iris data set</strong></span>
<span class="strong"><strong class="calibre9">kmean_iris &lt;- iris</strong></span>

<span class="strong"><strong class="calibre9">#Erase/ Nullify species labels</strong></span>
<span class="strong"><strong class="calibre9">kmean_iris$Species &lt;- NULL</strong></span>

<span class="strong"><strong class="calibre9">#apply k-means with k=3</strong></span>
<span class="strong"><strong class="calibre9">(clusters &lt;- kmeans(kmean_iris, 3))</strong></span>
</pre></div><p class="calibre8">Now that we have the output from <code class="email">k-means</code>, let us see how well it has partitioned the various species. Remember, <code class="email">k-means</code> does not have partition labels and simply groups the data points.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># comparing cluster labels with actual iris  species labels.</strong></span>
<span class="strong"><strong class="calibre9">table(iris$Species, clusters$cluster)</strong></span>
</pre></div><p class="calibre8"><span class="strong"><strong class="calibre9">Output:</strong></span></p><p class="calibre8"><span class="strong"><img src="../images/00076.jpeg" alt="K-Means" class="calibre16"/></span></p><p class="calibre8">The output shows that <a id="id143" class="calibre1"/>the species setosa matches cluster label 2, versicolor matches label 3, and so on. Visually, it is easy to make out how the data <a id="id144" class="calibre1"/>points are clustered:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre9"># plot the clustered points along sepal length and width</strong></span>
<span class="strong"><strong class="calibre9">plot(kmean_iris[c("Sepal.Length", "Sepal.Width")], col=clusters$cluster,pch = c(15,16,17)[as.numeric(clusters$cluster)])</strong></span>

<span class="strong"><strong class="calibre9">points(clusters$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=8, cex=2)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00077.jpeg" alt="K-Means" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">K-Means finds wide usage in areas like computer graphics for color quantization; it is combined with other algorithms and used for natural language processing, computer vision, and so on.</p><p class="calibre8">There are different variations of <code class="email">k-means</code> (R itself provides three different variations). Apart from <a id="id145" class="calibre1"/>k-means, other unsupervised clustering <a id="id146" class="calibre1"/>algorithms are k-medoids, hierarchical clustering, and others.</p></div></div></div>
<div class="book" title="Summary" id="NQU21-973e731d75c2419489ee73e3a0cf4be8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec19" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Through this chapter, we formally defined the concept of machine learning. We talked about how a machine learning algorithm actually learns a concept. We touched upon various other concepts such as generalization, overfitting, training, testing, frequent itemsets, and so on. We also learnt about the families of machine learning algorithms. We went through different machine learning algorithms to understand the magic under the hood, along with their areas of application.</p><p class="calibre8">With this knowledge we are ready to solve some real world problems and save the world.</p><p class="calibre8">The coming few chapters build on the concepts in this chapter to solve specific problems and use cases. Get ready for some action!</p></div></body></html>