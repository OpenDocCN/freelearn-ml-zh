- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Black-Box Methods – Neural Networks and Support Vector Machines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 黑盒方法 – 神经网络和支持向量机
- en: The late science fiction author Arthur C. Clarke wrote, “Any sufficiently advanced
    technology is indistinguishable from magic.” This chapter covers a pair of machine
    learning methods that may appear at first glance to be magic. Though they are
    extremely powerful, their inner workings can be difficult to understand.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 已故科幻小说作家亚瑟·C·克拉克写道：“任何足够先进的技术都与魔法无法区分。”本章涵盖了两种机器学习方法，它们乍一看可能像是魔法。虽然它们非常强大，但它们的内部工作原理可能难以理解。
- en: In engineering, these are referred to as black-box processes because the mechanism
    that transforms the input into the output is obfuscated by an imaginary box. For
    instance, the black box of closed-source software intentionally conceals proprietary
    algorithms, the black box of political lawmaking is rooted in bureaucratic processes,
    and the black box of sausage-making involves a bit of purposeful (but tasty) ignorance.
    In the case of machine learning, the black box is due to the complex mathematics
    allowing them to function.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在工程学中，这些被称为黑盒过程，因为将输入转换为输出的机制被一个想象中的盒子所掩盖。例如，封闭源代码软件的黑盒有意隐藏了专有算法，政治立法的黑盒根植于官僚程序，而香肠制作的黑盒则涉及一点有意的（但美味的）无知。在机器学习的情况下，黑盒是由于复杂的数学使得它们能够运作。
- en: 'Although they may not be easy to understand, it is dangerous to apply black-box
    models blindly. Thus, in this chapter, we’ll peek inside the box and investigate
    the statistical sausage-making involved in fitting such models. You’ll discover
    how:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们可能不容易理解，但盲目地应用黑盒模型是危险的。因此，在本章中，我们将窥视盒子内部，调查在拟合此类模型中涉及的统计香肠制作过程。你会发现如何：
- en: Neural networks mimic living brains to model mathematic functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络模仿生物大脑来模拟数学函数
- en: Support vector machines use multidimensional surfaces to define the relationship
    between features and outcomes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机使用多维表面来定义特征与结果之间的关系
- en: Despite their complexity, these can be applied easily to real-world problems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管它们很复杂，但它们可以轻松应用于现实世界的问题
- en: With any luck, you’ll realize that you don’t need a black belt in statistics
    to tackle black-box machine learning methods—there’s no need to be intimidated!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能意识到，你不需要在统计学上拥有黑带级别的能力来应对黑盒机器学习方法——无需感到害怕！
- en: Understanding neural networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络
- en: An **artificial neural network** (**ANN**) models the relationship between a
    set of input signals and an output signal using a model derived from our understanding
    of how a biological brain responds to stimuli from sensory inputs. Just like a
    brain uses a network of interconnected cells called **neurons** to provide vast
    learning capability, an ANN uses a network of artificial neurons or **nodes**
    to solve challenging learning problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）通过从我们对生物大脑如何对感官输入的刺激做出反应的理解中得出的模型，模拟了一组输入信号与输出信号之间的关系。就像大脑使用称为**神经元**的相互连接的细胞网络来提供强大的学习能力一样，ANN使用人工神经元或**节点**来解决具有挑战性的学习问题。'
- en: The human brain is made up of about 85 billion neurons, resulting in a network
    capable of representing a tremendous amount of knowledge. As you might expect,
    this dwarfs the brains of other living creatures. For instance, a cat has roughly
    a billion neurons, a mouse has about 75 million neurons, and a cockroach has only
    about 1 million neurons. In contrast, many ANNs contain far fewer neurons, typically
    only hundreds or thousands, so we’re in no danger of creating an artificial brain
    in the near future—even a fruit fly with 100,000 neurons far exceeds a state-of-the-art
    ANN on standard computing hardware. Some of the largest ANNs ever designed run
    on clusters of tens of thousands of CPU cores and have millions of nodes, but
    these are still dwarfed by the brains of small animals, let alone human beings—and
    the biological brains fit inside a much smaller package!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑由大约850亿个神经元组成，形成一个能够表示大量知识网络的网络。正如你所预期的那样，这使其他生物的大脑相形见绌。例如，一只猫大约有10亿个神经元，一只老鼠大约有7500万个神经元，而一只蟑螂只有大约100万个神经元。相比之下，许多ANN包含的神经元要少得多，通常只有数百或数千个，所以我们不会在近期内创造出人工大脑——即使是一只拥有10万个神经元的果蝇也远远超过在标准计算硬件上运行的最新ANN——而且这些ANN仍然被小型动物的大脑所
    dwarf，更不用说人类的大脑了——而生物大脑可以装入一个更小的包装中！
- en: Though it may be infeasible to completely model a cockroach’s brain, a neural
    network may still provide an adequate heuristic model of certain behaviors. Suppose
    that we develop an algorithm that can mimic how a roach flees when discovered.
    If the behavior of the robot roach is convincing, does it matter whether its brain
    is as sophisticated as the living creature? Similarly, if the written text produced
    by neural network-based tools like ChatGPT ([https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/))
    can pass for human-produced text most of the time, does it matter if the neural
    network isn’t a perfect model of a human brain? This question is at the heart
    of the controversial **Turing test**, proposed in 1950 by the pioneering computer
    scientist Alan Turing, which grades a machine as intelligent if a human being
    cannot distinguish its behavior from a living creature’s.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可能无法完全模拟蟑螂的大脑，但神经网络仍然可以提供某些行为的足够启发式模型。假设我们开发了一个算法，可以模仿蟑螂被发现时逃跑的方式。如果机器人蟑螂的行为令人信服，那么它的脑部是否像活体生物一样复杂就无关紧要了。同样，如果基于神经网络工具（如ChatGPT
    [https://openai.com/blog/ChatGPT/](https://openai.com/blog/ChatGPT/)）产生的文本大多数时候可以冒充人类文本，那么神经网络是否是完美的人脑模型就无关紧要了。这个问题是1950年由计算机科学先驱艾伦·图灵提出的有争议的**图灵测试**的核心，该测试将机器视为智能，如果人类无法区分其行为与生物体的行为。
- en: 'For more about the intrigue and controversy that surrounds the Turing test,
    refer to the *Stanford Encyclopedia of Philosophy*: [https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于围绕图灵测试的神秘和争议的信息，请参阅*斯坦福哲学百科全书*：[https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/)。
- en: 'Rudimentary ANNs have been used for over 60 years to simulate the brain’s approach
    to problem-solving. At first, this involved learning simple functions like the
    logical AND function or the logical OR function. These early exercises were used
    primarily to help scientists understand how biological brains might operate. However,
    as computers have become increasingly powerful in recent years, the complexity
    of ANNs has likewise increased so much that they are now frequently applied to
    more practical problems, including:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的ANNs已经超过60年用于模拟大脑的解决问题的方法。最初，这涉及到学习简单的函数，如逻辑与函数或逻辑或函数。这些早期的练习主要用于帮助科学家了解生物大脑可能如何运作。然而，随着近年来计算机变得越来越强大，ANNs的复杂性也相应增加，以至于它们现在经常应用于更实际的问题，包括：
- en: Speech, handwriting, and image recognition programs like those used by smartphone
    applications, mail-sorting machines, and search engines
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音、手写和图像识别程序，如智能手机应用、邮件分拣机和搜索引擎所使用的程序
- en: The automation of smart devices, such as an office building’s environmental
    controls, or the control of self-driving cars and self-piloting drones
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能设备的自动化，例如办公大楼的环境控制，或自动驾驶汽车和自主飞行的无人机控制
- en: Sophisticated models of weather and climate patterns, tensile strength, fluid
    dynamics, and many other scientific, social, or economic phenomena
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂的天气和气候模式、抗拉强度、流体动力学以及许多其他科学、社会或经济现象的模型
- en: 'Broadly speaking, ANNs are versatile learners that can be applied to nearly
    any learning task: classification, numeric prediction, and even unsupervised pattern
    recognition.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，ANNs是多才多艺的学习者，可以应用于几乎任何学习任务：分类、数值预测，甚至无监督模式识别。
- en: Whether deserving or not, ANN learners are often reported in the media with
    great fanfare. For instance, an “artificial brain” developed by Google was touted
    for its ability to identify cat videos on YouTube. Such hype may have less to
    do with anything unique to ANNs and more to do with the fact that ANNs are captivating
    because of their similarities to living minds.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否值得，ANN学习者经常在媒体上被大肆报道。例如，谷歌开发的一个“人工大脑”因其能够识别YouTube上的猫视频而备受赞誉。这种炒作可能更多与ANNs的独特性无关，而更多与ANNs因其与生物大脑的相似性而具有吸引力的事实有关。
- en: ANNs are often applied to problems where the input data and output data are
    well defined, yet the process that relates the input to the output is extremely
    complex and hard to define. As a black-box method, ANNs work well for these types
    of black-box problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs通常应用于输入数据和输出数据定义良好，但将输入与输出联系起来的过程极其复杂且难以定义的问题。作为一种黑盒方法，ANNs对于这些类型的黑盒问题非常有效。
- en: From biological to artificial neurons
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从生物神经元到人工神经元
- en: Because ANNs were intentionally designed as conceptual models of human brain
    activity, it is helpful to first understand how biological neurons function. As
    illustrated in the following figure, incoming signals are received by the cell’s
    **dendrites** through a biochemical process. The process allows the impulse to
    be weighted according to its relative importance or frequency. As the **cell body**
    begins to accumulate the incoming signals, a threshold is reached at which the
    cell fires, and the output signal is transmitted via an electrochemical process
    down the axon. At the axon’s terminals, the electric signal is again processed
    as a chemical signal to be passed to the neighboring neurons across a tiny gap
    known as a **synapse**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人工神经网络（ANNs）被有意设计成人类大脑活动的概念模型，因此首先了解生物神经元是如何工作的是有帮助的。如图所示，细胞通过生化过程接收传入的信号，这些信号通过细胞**树突**。这个过程允许根据其相对重要性或频率对冲动进行加权。当**细胞体**开始积累传入的信号时，会达到一个阈值，此时细胞会放电，输出信号通过电化学过程沿着轴突传递。在轴突的末端，电信号再次被处理成化学信号，通过一个称为**突触**的微小间隙传递给相邻的神经元。
- en: '![Diagram  Description automatically generated](img/B17290_07_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_01.png)'
- en: 'Figure 7.1: An artistic depiction of a biological neuron'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：生物神经元的艺术描绘
- en: The model of a single artificial neuron can be understood in terms very similar
    to the biological model. As depicted in the following figure, a directed network
    diagram defines a relationship between the input signals received by the dendrites
    (*x* variables) and the output signal (*y* variable). Just as with the biological
    neuron, each dendrite’s signal is weighted (*w* values) according to its importance—ignore,
    for now, how these weights are determined. The input signals are summed by the
    cell body and the signal is passed on according to an **activation function**
    denoted by *f*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 单个人工神经元的模型可以用与生物模型非常相似的方式来理解。如图所示，一个有向网络图定义了通过树突接收的输入信号（*x*变量）和输出信号（*y*变量）之间的关系。就像生物神经元一样，每个树突的信号都会根据其重要性进行加权（现在忽略这些权重是如何确定的）。输入信号由细胞体相加，然后根据一个表示为*f*的**激活函数**传递信号。
- en: '![Diagram  Description automatically generated](img/B17290_07_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_02.png)'
- en: 'Figure 7.2: An artificial neuron is designed to mimic the structure and function
    of a biological neuron'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：人工神经元的设计旨在模仿生物神经元的结构和功能
- en: 'A typical artificial neuron with *n* input dendrites can be represented by
    the formula that follows. The *w* weights allow each of the *n* inputs (denoted
    by *x*[i]) to contribute a greater or lesser amount to the sum of input signals.
    The net total is used by the activation function *f(x)* and the resulting signal,
    *y(x)*, is the output axon:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的人工神经元可以用以下公式表示，其中*n*表示输入树突的数量。*w*权重允许每个*n*个输入（用*x*[i]表示）对输入信号的总和贡献更多或更少的量。净总信号由激活函数*f(x)*使用，产生的信号*y(x)*是输出轴突：
- en: '![](img/B17290_07_001.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_001.png)'
- en: 'Neural networks use neurons defined in this way as building blocks to construct
    complex models of data. Although there are numerous variants of neural networks,
    each can be defined in terms of the following characteristics:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用这种方式定义的神经元作为构建复杂数据模型的基石。尽管神经网络有无数种变体，但每种都可以用以下特征来定义：
- en: An **activation function**, which transforms a neuron’s net input signal into
    a single output signal to be broadcasted further in the network
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**，它将神经元的净输入信号转换成单个输出信号，以便在网络中进一步广播'
- en: A **network topology** (or architecture), which describes the number of neurons
    in the model, as well as the number of layers and the manner in which they are
    connected
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络拓扑**（或架构），它描述了模型中的神经元数量，以及层数和它们之间的连接方式'
- en: The **training algorithm**, which specifies how connection weights are set to
    inhibit or excite neurons in proportion to the input signal
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练算法**，它指定了如何根据输入信号的比例来设置连接权重以抑制或兴奋神经元'
- en: Let’s take a look at some of the variations within each of these categories
    to see how they can be used to construct typical neural network models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看每个类别中的一些变体，看看它们如何被用来构建典型的神经网络模型。
- en: Activation functions
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: The **activation function** is the mechanism by which the artificial neuron
    processes incoming information and determines whether to pass the signal to other
    neurons in the network. Just as an artificial neuron is modeled after the biological
    version, so too is the activation function modeled after nature’s design.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**是人工神经元处理传入信息并确定是否将信号传递给网络中其他神经元的机制。正如人工神经元是模仿生物版本一样，激活函数也是模仿自然的设计。'
- en: In the biological case, the activation function could be imagined as a process
    that involves summing the total input signal and determining whether it meets
    the firing threshold. If so, the neuron passes on the signal; otherwise, it does
    nothing. In ANN terms, this is known as a **threshold activation function**, as
    it results in an output signal only once a specified input threshold has been
    attained.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物案例中，激活函数可以想象为一个涉及求和总输入信号并确定它是否达到触发阈值的进程。如果是这样，神经元就会传递信号；否则，它就什么都不做。在ANN术语中，这被称为**阈值激活函数**，因为它只在达到指定的输入阈值时产生输出信号。
- en: The following figure depicts a typical threshold function; in this case, the
    neuron fires when the sum of input signals is at least zero. Because its shape
    resembles a stair, it is sometimes called a **unit step activation function**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图描绘了一个典型的阈值函数；在这种情况下，当输入信号的求和至少为零时，神经元就会触发。由于其形状类似于楼梯，有时也被称为**单位步激活函数**。
- en: '![Chart, histogram  Description automatically generated](img/B17290_07_03.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图  自动生成的描述](img/B17290_07_03.png)'
- en: 'Figure 7.3: The threshold activation function is “on” only after the input
    signals meet a threshold'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：阈值激活函数仅在输入信号达到阈值后才“开启”
- en: Although the threshold activation function is interesting due to its parallels
    with biology, it is rarely used in ANNs. Freed from the limitations of biochemistry,
    ANN activation functions can be chosen based on their ability to demonstrate desirable
    mathematical characteristics and their ability to accurately model relationships
    among data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管阈值激活函数由于其与生物学的相似性而有趣，但在ANN中很少使用。摆脱生物化学的限制，ANN激活函数可以根据其展示的期望数学特征和准确模拟数据之间关系的能力来选择。
- en: Perhaps the most common alternative is the **sigmoid activation function** (more
    specifically the *logistic* sigmoid) shown in the following figure. Note that
    in the formula shown, *e* is the base of the natural logarithm (a value of approximately
    2.72). Although it shares a similar step or “S” shape with the threshold activation
    function, the output signal is no longer binary; output values can fall anywhere
    in the range from zero to one. Additionally, the sigmoid is **differentiable**,
    which means that it is possible to calculate the derivative (the slope of the
    tangent line for a point on the curve) across the entire range of inputs. As you
    will learn later, this feature is crucial for creating efficient ANN optimization
    algorithms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最常用的替代方案是以下图中所示的**S型激活函数**（更具体地说，是*逻辑* S型）。请注意，在所示公式中，*e*是自然对数的底（大约为2.72）。尽管它与阈值激活函数具有相似的步骤或“S”形状，但输出信号不再是二进制的；输出值可以落在零到一之间的任何地方。此外，S型函数是**可微分的**，这意味着可以计算整个输入范围内的导数（曲线上的某一点的切线斜率）。正如你稍后将会学到的那样，这一特性对于创建高效的ANN优化算法至关重要。
- en: '![Chart  Description automatically generated](img/B17290_07_04.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成的描述](img/B17290_07_04.png)'
- en: 'Figure 7.4: The sigmoid activation function uses a smooth curve to mimic the
    unit step activation function found in nature'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：S型激活函数使用平滑曲线来模拟自然界中发现的单位步激活函数
- en: 'Although the sigmoid is perhaps the most commonly used activation function
    and is often used by default, some neural network algorithms allow a choice of
    alternatives. A selection of such activation functions is shown in *Figure 7.5*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管S型函数可能是最常用的激活函数，并且通常默认使用，但一些神经网络算法允许选择替代方案。以下图7.5中展示了这类激活函数的选择：
- en: '![Chart, line chart  Description automatically generated](img/B17290_07_05.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B17290_07_05.png)'
- en: 'Figure 7.5: Several common neural network activation functions'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：几种常见的神经网络激活函数
- en: The primary detail that differentiates these activation functions is the output
    signal range. Typically, this is one of (0, 1), (-1, +1), or (-inf, +inf). The
    choice of activation function biases the neural network such that it may fit certain
    types of data more appropriately, allowing the construction of specialized neural
    networks. For instance, a linear activation function results in a neural network
    very similar to a linear regression model, while a Gaussian activation function
    is the basis of a **radial basis function** (**RBF**) network. Each of these has
    strengths better suited for certain learning tasks and not others.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 区分这些激活函数的主要细节是输出信号的范围。通常，这可能是以下之一：(0, 1)，(-1, +1)，或 (-∞, +∞)。激活函数的选择会影响神经网络的偏差，使其可能更适合某些类型的数据，从而允许构建专门的神经网络。例如，线性激活函数会导致神经网络非常类似于线性回归模型，而高斯激活函数是**径向基函数**（RBF）网络的基础。这些中的每一个都有适合某些学习任务的优点，而不适合其他任务。
- en: Neural networks use nonlinear activation functions almost exclusively since
    this is what allows the network to become more intelligent as more nodes are added.
    Limited to linear activation functions only, a network is limited to linear solutions
    and will perform no better than the much simpler regression methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络几乎只使用非线性激活函数，因为这是网络随着节点数量的增加而变得更智能的原因。仅限于线性激活函数的网络将限于线性解决方案，并且其表现不会优于许多更简单的回归方法。
- en: It’s important to recognize that for many of the activation functions, the range
    of input values that affect the output signal is relatively narrow. For example,
    in the case of the sigmoid, the output signal is very near 0 for an input signal
    below -5 and very near 1 for an input signal above 5\. The compression of the
    signal in this way results in a saturated signal at the high and low ends of very
    dynamic inputs, just as turning a guitar amplifier up too high results in a distorted
    sound due to clipping of the peaks of sound waves. Because this essentially squeezes
    the input values into a smaller range of outputs, activation functions like the
    sigmoid are sometimes called **squashing functions**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到，对于许多激活函数，影响输出信号的输入值范围相对较窄。例如，在sigmoid函数的情况下，当输入信号低于-5时，输出信号非常接近0，而当输入信号高于5时，输出信号非常接近1。以这种方式压缩信号会导致动态输入的高低端饱和，就像将吉他放大器音量调得过高，由于声音波峰的截断而导致失真声音一样。因为这种做法本质上是将输入值挤压到一个更小的输出值范围内，所以像sigmoid这样的激活函数有时被称为**挤压函数**。
- en: One solution to the squashing problem is to transform all neural network inputs
    such that the feature values fall within a small range around zero. This may involve
    standardizing or normalizing the features. By restricting the range of input values,
    the activation function will be able to work across the entire range. A side benefit
    is that the model may also be faster to train since the algorithm can iterate
    more quickly through the actionable range of input values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 解决挤压问题的方法之一是将所有神经网络输入转换，使得特征值落在围绕零的小范围内。这可能涉及标准化或归一化特征。通过限制输入值的范围，激活函数将能够在整个范围内工作。一个附带的好处是，模型也可能训练得更快，因为算法可以更快地遍历可操作的输入值范围。
- en: Although theoretically, a neural network can adapt to a very dynamic feature
    by adjusting its weight over many iterations, in extreme cases, many algorithms
    will stop iterating long before this occurs. If your model is failing to converge,
    double-check that you’ve correctly standardized the input data. Choosing a different
    activation function may also be appropriate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从理论上讲，神经网络可以通过多次迭代调整其权重来适应非常动态的特征，但在极端情况下，许多算法会在这种情况发生之前就停止迭代。如果你的模型未能收敛，请务必检查你是否已正确标准化了输入数据。选择不同的激活函数也可能是合适的。
- en: Network topology
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络拓扑
- en: 'The capacity of a neural network to learn is rooted in its topology, or the
    patterns and structures of interconnected neurons. Although there are countless
    forms of network architecture, they can be differentiated by three key characteristics:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络学习的能力根植于其拓扑结构，即相互连接的神经元的模式和结构。尽管有无数种网络架构形式，但它们可以通过三个关键特征来区分：
- en: The number of layers
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数的数量
- en: Whether information in the network is allowed to travel backward
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中信息是否允许向后传播
- en: The number of nodes within each layer of the network
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络每一层中的节点数量
- en: The topology determines the complexity of tasks that can be learned by the network.
    Generally, larger and more complex networks can identify more subtle patterns
    and more complex decision boundaries. However, the power of a network is not only
    a function of the network size but also the way units are arranged.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑决定了网络可以学习的任务复杂性。一般来说，更大、更复杂的网络可以识别更微妙的模式和更复杂的决策边界。然而，网络的力量不仅取决于网络的大小，还取决于单元的排列方式。
- en: The number of layers
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层数的数量
- en: To define topology, we need terminology that distinguishes artificial neurons
    based on their position in the network. *Figure 7.6* illustrates the topology
    of a very simple network. A set of neurons called **input nodes** receives unprocessed
    signals directly from the input data. Each input node is responsible for processing
    a single feature in the dataset; the feature’s value will be transformed by the
    corresponding node’s activation function. The signals sent by the input nodes
    are received by the **output node**, which uses its own activation function to
    generate a final prediction (denoted here as *p*).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义拓扑，我们需要术语来区分网络中位置不同的人工神经元。*图7.6*展示了一个非常简单的网络的拓扑结构。一组称为**输入节点**的神经元直接从输入数据接收未经处理的信号。每个输入节点负责处理数据集中单个特征；该特征值将通过相应节点的激活函数进行转换。输入节点发送的信号被**输出节点**接收，该节点使用自己的激活函数生成最终的预测（在此处表示为*p*）。
- en: The input and output nodes are arranged in groups known as **layers**. Because
    the input nodes process the incoming data exactly as received, the network has
    only one set of connection weights (labeled here as *w*[1], *w*[2], and *w*[3]).
    It is therefore termed a **single-layer network**. Single-layer networks can be
    used for basic pattern classification, particularly for patterns that are linearly
    separable, but more sophisticated networks are required for most learning tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出节点被安排在称为**层**的组中。因为输入节点以接收到的数据完全相同的方式处理传入的数据，所以网络只有一个连接权重集（在此处标记为*w*[1]，*w*[2]，和*w*[3]）。因此，它被称为**单层网络**。单层网络可用于基本的模式分类，尤其是对于线性可分模式的分类，但对于大多数学习任务，需要更复杂的网络。
- en: '![Diagram  Description automatically generated](img/B17290_07_06.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的描述](img/B17290_07_06.png)'
- en: 'Figure 7.6: A simple single-layer ANN with three input nodes'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：具有三个输入节点的简单单层ANN
- en: As you might expect, an obvious way to create more complex networks is by adding
    additional layers. As depicted in *Figure 7.7*, a **multilayer network** adds
    one or more **hidden layers** that process the signals from the input nodes prior
    to reaching the output node. Hidden nodes get their name from the fact that they
    are obscured in the heart of the network and their relationship to the data and
    output is much more difficult to understand. The hidden layers are what make ANNs
    a black-box model; knowing what is happening inside these layers is practically
    impossible, particularly as the topology becomes more complicated.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所预期的那样，创建更复杂网络的一个明显方法是通过添加额外的层。如图*图7.7*所示，一个**多层网络**添加一个或多个**隐藏层**，这些隐藏层在信号到达输出节点之前处理来自输入节点的信号。隐藏节点之所以得名，是因为它们在网络中心被遮挡，并且它们与数据和输出的关系更难以理解。隐藏层使得人工神经网络成为一个黑盒模型；了解这些层内部发生的事情实际上是不可能的，尤其是当拓扑变得更加复杂时。
- en: '![Diagram  Description automatically generated](img/B17290_07_07.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的描述](img/B17290_07_07.png)'
- en: 'Figure 7.7: A multilayer network with a single two-node hidden layer'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：具有单个两个节点隐藏层的多层网络
- en: Examples of more complex topologies are depicted in *Figure 7.8*. Multiple output
    nodes can be used to represent outcomes with multiple categories. Multiple hidden
    layers can be used to allow even more complexity inside the black box, and thus
    model more challenging problems.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂拓扑的示例在*图7.8*中展示。可以使用多个输出节点来表示具有多个类别的结果。可以使用多个隐藏层来允许黑盒内部有更多的复杂性，从而模拟更复杂的问题。
- en: '![A picture containing text, scissors, tool  Description automatically generated](img/B17290_07_08.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、剪刀、工具的图片，自动生成的描述](img/B17290_07_08.png)'
- en: 'Figure 7.8: Complex ANNs can have multiple output nodes or multiple hidden
    layers'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：复杂的ANN可以具有多个输出节点或多个隐藏层
- en: A neural network with multiple hidden layers is called a **deep neural network**
    (**DNN**), and the practice of training such networks is referred to as **deep
    learning**. DNNs trained on large datasets are capable of human-like performance
    on complex tasks like image recognition and text processing. Deep learning has
    thus been hyped as the next big leap in machine learning, but deep learning is
    better suited to some tasks than others.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多个隐藏层的神经网络被称为**深度神经网络**（**DNN**），训练此类网络的实践被称为**深度学习**。在大数据集上训练的DNN在复杂任务如图像识别和文本处理上能够达到类似人类的性能。因此，深度学习被炒作成为机器学习中的下一个重大飞跃，但深度学习更适合某些任务而不是其他任务。
- en: Though deep learning performs quite well on complex learning tasks that conventional
    models tend to struggle with, it requires much larger datasets with much richer
    feature sets than found in most projects. Typical learning tasks include modeling
    unstructured data like images, audio, or text, as well as outcomes measured repeatedly
    over time, such as stock market prices or energy consumption. Building DNNs on
    these types of data requires specialized computing software (and sometimes also
    hardware) that is more challenging to use than a simple R package. *Chapter 15*,
    *Making Use of Big Data*, provides details on how to use these tools to perform
    deep learning and image recognition in R.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在传统模型难以应对的复杂学习任务上表现相当出色，但它需要比大多数项目中找到的更大的数据集和更丰富的特征集。典型的学习任务包括对图像、音频或文本等非结构化数据进行建模，以及随时间重复测量的结果，如股票市场价格或能源消耗。在这些类型的数据上构建深度神经网络（DNN）需要专门的计算软件（有时还需要硬件），其使用难度比简单的R包要大。第15章“利用大数据”提供了如何使用这些工具在R中执行深度学习和图像识别的详细信息。
- en: The direction of information travel
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息传递的方向
- en: Simple multilayer networks are usually **fully connected**, which means that
    every node in one layer is connected to every node in the next layer, but this
    is not required. Much larger deep learning models, such as the **convolutional
    neural network** (**CNN**) for image recognition that will be introduced in *Chapter
    15*, *Making Use of Big Data*, are only partially connected. Removing some connections
    helps limit the amount of overfitting that can occur inside the numerous hidden
    layers. However, this is not the only way we can manipulate the topology. In addition
    to whether nodes are connected at all, we can also dictate the direction of information
    flow throughout the connections and produce neural networks suited to different
    types of learning tasks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的多层网络通常是**全连接**的，这意味着一个层中的每个节点都与下一层的每个节点相连，但这并非必需。更大的深度学习模型，例如将在第15章“利用大数据”中介绍的用于图像识别的**卷积神经网络**（**CNN**），只是部分连接。移除一些连接有助于限制在众多隐藏层中可能发生的过度拟合的数量。然而，这并非我们操纵拓扑的唯一方式。除了节点是否连接之外，我们还可以指定信息流在整个连接中的方向，并产生适合不同类型学习任务的神经网络。
- en: You may have noticed that in the prior examples, arrowheads were used to indicate
    signals traveling in only one direction. Networks in which the input signal is
    fed continuously in one direction from the input layer to the output layer are
    called **feedforward networks**. Despite the restriction on information flow,
    feedforward networks offer a surprising amount of flexibility.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在前面的例子中，箭头被用来指示只在一个方向上传递的信号。从输入层到输出层连续单向输入输入信号的神经网络被称为**前馈网络**。尽管对信息流有限制，但前馈网络提供了令人惊讶的灵活性。
- en: For instance, the number of levels and nodes at each level can be varied, multiple
    outcomes can be modeled simultaneously, or multiple hidden layers can be applied.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以改变每个级别的级别和节点数量，可以同时建模多个结果，或者应用多个隐藏层。
- en: 'In contrast to feedforward networks, a **recurrent neural network** (**RNN**)
    (or **feedback network**) allows signals to travel backward using loops. This
    property, which more closely mirrors how a biological neural network works, allows
    extremely complex patterns to be learned. The addition of a short-term memory,
    or **delay**, increases the power of recurrent networks immensely. Notably, this
    includes the capability to understand sequences of events over time. This could
    be used for stock market prediction, speech comprehension, or weather forecasting.
    A simple recurrent network is depicted as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络不同，**循环神经网络**（**RNN**）（或**反馈网络**）允许信号通过循环向后传递。这种特性更接近于生物神经网络的工作方式，使得学习极其复杂的模式成为可能。加入短期记忆，或**延迟**，极大地增强了循环网络的能力。值得注意的是，这包括理解随时间推移的事件序列的能力。这可以用于股市预测、语音理解或天气预报。一个简单的循环网络如下所示：
- en: '![Diagram  Description automatically generated](img/B17290_07_09.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图示描述自动生成](img/B17290_07_09.png)'
- en: 'Figure 7.9: Allowing information to travel backward in the network can model
    a time delay'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：允许信息在网络中向后传递可以模拟时间延迟
- en: As the short-term memory of an RNN is, unsurprisingly, short by definition,
    one form of RNN known as **long short-term memory** (**LSTM**) adapts the model
    to have substantially longer recall—much like living creatures that have both
    short- and long-term memory. While this may seem to be an obvious enhancement,
    computers have perfect memory and thus need to be explicitly told when to forget
    and when to remember. The challenge was striking a balance between forgetting
    too soon and remembering for too long, which is easier said than done because
    the mathematical functions used to train the model are naturally pulled toward
    either of these extremes for reasons that will become clearer as you continue
    in this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RNN的短期记忆在定义上显然是短的，因此一种名为**长短期记忆**（**LSTM**）的RNN形式调整了模型，使其具有显著更长的回忆能力——就像既有短期又有长期记忆的活物一样。虽然这似乎是一种明显的改进，但计算机具有完美的记忆，因此需要明确告知何时忘记和何时记住。挑战在于在过早忘记和过长记住之间找到平衡，这比说起来要难得多，因为用于训练模型的数学函数自然地被拉向这两个极端，原因将在本章继续阅读时变得更加清晰。
- en: For more information about LSTM neural networks, see *Understanding LSTM—a tutorial
    into Long Short-Term Memory Recurrent Neural Networks, Staudemeyer RC and Morris
    ER, 2019*. [https://arxiv.org/abs/1909.09586](https://arxiv.org/abs/1909.09586).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于LSTM神经网络的信息，请参阅*理解LSTM——关于长短期记忆循环神经网络教程，Staudemeyer RC和Morris ER，2019*。[https://arxiv.org/abs/1909.09586](https://arxiv.org/abs/1909.09586)。
- en: The development of LSTM neural networks has led to advances in artificial intelligence
    such as the ability for robots to mimic sequences of human behaviors necessary
    for controlling machinery, driving, and playing video games. The LSTM model also
    shows aptitude for speech and text recognition, understanding language semantics
    and translating between languages, and learning complex strategies. DNNs and recurrent
    networks are increasingly being used for a variety of high-profile applications
    and consequently have become much more popular since the first edition of this
    book was published. However, building such networks uses techniques and software
    outside the scope of this book, and often requires access to specialized computing
    hardware or cloud servers.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM神经网络的发展导致了人工智能的进步，例如使机器人能够模仿控制机械、驾驶和玩电子游戏所需的人类行为序列。LSTM模型也显示出在语音和文本识别、理解语言语义以及在不同语言之间翻译和学习复杂策略方面的能力。DNN和循环网络越来越多地被用于各种高调应用，因此自本书第一版出版以来，它们变得更加流行。然而，构建这样的网络需要超出本书范围的技术和软件，并且通常需要访问专门的计算硬件或云服务器。
- en: On the other hand, simpler feedforward networks are also very capable of modeling
    many real-world tasks, though the tasks may not be quite as exciting as autonomous
    vehicles and computers playing video games. While deep learning is quickly becoming
    the status quo, the multilayer feedforward network, also known as the **multilayer
    perceptron** (**MLP**), may still be the de facto standard ANN topology for conventional
    learning tasks. Furthermore, understanding the MLP topology provides a strong
    theoretical basis for building more complex deep learning models later.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，简单的前馈网络也非常擅长模拟许多现实世界的任务，尽管这些任务可能不如自动驾驶汽车和玩电子游戏的计算机那样令人兴奋。虽然深度学习正在迅速成为主流，但多层前馈网络，也称为**多层感知器**（**MLP**），可能仍然是传统学习任务的默认标准人工神经网络拓扑。此外，理解MLP拓扑为以后构建更复杂的深度学习模型提供了强大的理论基础。
- en: The number of nodes in each layer
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每层的节点数量
- en: In addition to variations in the number of layers and the direction of information
    travel, neural networks can also vary in complexity by the number of nodes in
    each layer. The number of input nodes is predetermined by the number of features
    in the input data. Similarly, the number of output nodes is predetermined by the
    number of outcomes to be modeled or the number of class levels in the outcome.
    However, the number of hidden nodes is left to the user to decide prior to training
    the model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了层数和信息传递方向的变化之外，神经网络还可以通过每层的节点数量来变化其复杂性。输入节点的数量由输入数据中的特征数量预先确定。同样，输出节点的数量由要模拟的结果数量或结果中的类别级别数量预先确定。然而，隐藏节点的数量留给用户在训练模型之前决定。
- en: Unfortunately, there is no reliable rule for determining the number of neurons
    in the hidden layer. The appropriate number depends on the number of input nodes,
    the amount of training data, the amount of noisy data, and the complexity of the
    learning task, among many other factors.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有可靠的规则来确定隐藏层中神经元的数量。合适的数量取决于输入节点的数量、训练数据量、噪声数据量以及学习任务的复杂性等因素。
- en: In general, more complex network topologies with a greater number of network
    connections allow the learning of more complex problems. A greater number of neurons
    will result in a model that more closely mirrors the training data, but this runs
    a risk of overfitting; it may generalize poorly to future data. Large neural networks
    can also be computationally expensive and slow to train. The best practice is
    to use the fewest nodes that result in adequate performance on a validation dataset.
    In most cases, even with only a small number of hidden nodes—often as few as a
    handful—the neural network can demonstrate a tremendous ability to learn.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，具有更多网络连接的更复杂网络拓扑允许学习更复杂的问题。更多的神经元将导致一个模型更接近训练数据，但这也存在过拟合的风险；它可能对未来数据的泛化能力较差。大型神经网络也可能计算成本高昂且训练缓慢。最佳实践是使用最少节点，这些节点在验证数据集上能够实现足够的性能。在大多数情况下，即使只有少量隐藏节点——通常只有几个——神经网络也能展现出惊人的学习能力。
- en: It has been proven that a neural network with at least one hidden layer of sufficiently
    many neurons with nonlinear activation functions is a **universal function approximator**.
    This means that neural networks can be used to approximate any continuous function
    to an arbitrary level of precision over a finite interval. This is where a neural
    network gains the ability to perform the type of “magic” described in the chapter
    introduction; put a set of inputs into the black box of a neural network and it
    can learn to produce any set of outputs, no matter how complex the relationships
    are between the inputs and outputs. Of course, this assumes “sufficiently many
    neurons” as well as enough data to reasonably train the network—all while avoiding
    overfitting to noise so that the approximation will generalize beyond the training
    examples. We’ll peek further into the black box that allows this magic to happen
    in the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 已被证明，具有至少一个具有足够多神经元和非线性激活函数的隐藏层的神经网络是一个**通用函数逼近器**。这意味着神经网络可以用来逼近任何连续函数，在有限区间内达到任意精度的近似。这就是神经网络获得在章节引言中描述的那种“魔法”能力的地方；将一组输入放入神经网络的黑盒中，它可以学会产生任何一组输出，无论输入和输出之间的关系有多么复杂。当然，这假设“足够多的神经元”以及足够的数据来合理训练网络——同时避免对噪声过拟合，以便近似可以泛化到训练示例之外。我们将在下一节进一步探讨允许这种魔法发生的黑盒。
- en: To see a real-time visualization of how changes to network topology allow neural
    networks to become a universal function approximator, visit the Deep Learning
    Playground at [http://playground.tensorflow.org/](http://playground.tensorflow.org/).
    The playground allows you to experiment with predictive models where the relationship
    between the features and target is complex and nonlinear. While methods like regression
    and decision trees would struggle to find any solution to these problems, you
    will find that adding more hidden nodes and layers allows the network to come
    up with a reasonable approximation for each of the examples given enough training
    time. Note, especially, that choosing the linear activation function rather than
    a sigmoid or hyperbolic tangent (tanh) prevents the network from learning a reasonable
    solution regardless of the complexity of the network topology.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看网络拓扑变化如何使神经网络成为通用函数逼近器的实时可视化，请访问[http://playground.tensorflow.org/](http://playground.tensorflow.org/)的深度学习游乐场。游乐场允许你实验预测模型，其中特征与目标之间的关系复杂且非线性。虽然回归和决策树等方法在解决这些问题上会遇到困难，但你将发现，添加更多的隐藏节点和层可以使网络在足够的训练时间内为每个示例提供一个合理的近似。请注意，特别是选择线性激活函数而不是Sigmoid或双曲正切（tanh）函数，可以防止网络无论网络拓扑的复杂性如何都能学习到一个合理的解决方案。
- en: Training neural networks with backpropagation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过调整连接权重来训练神经网络
- en: The network topology is a blank slate that by itself has not learned anything.
    Like a newborn child, it must be trained with experience. As the neural network
    processes the input data, connections between the neurons are strengthened or
    weakened, similar to how a baby’s brain develops as they experience the environment.
    The network’s connection weights are adjusted to reflect the patterns observed
    over time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑是一个白板，它本身并没有学习到任何东西。就像一个新生儿一样，它必须通过经验来训练。随着神经网络处理输入数据，神经元之间的连接会加强或减弱，这类似于婴儿的大脑在经历环境时的发展。网络的连接权重会调整以反映随时间观察到的模式。
- en: Training a neural network by adjusting connection weights is very computationally
    intensive. Consequently, though they had been studied for decades prior, ANNs
    were rarely applied to real-world learning tasks until the mid-to-late 1980s,
    when an efficient method of training an ANN was discovered. The algorithm, which
    used a strategy of back-propagating errors, is now known simply as **backpropagation**.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整连接权重来训练神经网络是非常计算密集的。因此，尽管它们在几十年前就已经被研究，但直到20世纪80年代中后期，当发现了一种有效的训练ANN的方法之前，ANNs很少应用于实际的学习任务。这个使用反向传播错误策略的算法现在简单地被称为**反向传播**。
- en: Coincidentally, several research teams independently discovered and published
    the backpropagation algorithm around the same time. Among them, perhaps the most
    often cited work is *Learning representations by back-propagating errors, Rumelhart,
    DE, Hinton, GE, Williams, RJ, Nature, 1986, Vol. 323, pp. 533-566*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 意外的是，几个研究团队几乎在同一时间独立发现了并发表了反向传播算法。其中，最常被引用的工作可能是*Rumelhart, DE, Hinton, GE,
    Williams, RJ, Nature, 1986, Vol. 323, pp. 533-566，通过反向传播错误学习表示*。
- en: 'Although still somewhat computationally expensive relative to many other machine
    learning algorithms, the backpropagation method led to a resurgence of interest
    in ANNs. As a result, multilayer feedforward networks that use the backpropagation
    algorithm are now common in the field of data mining. Such models offer the following
    strengths and weaknesses:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与许多其他机器学习算法相比，反向传播方法在计算上仍然有些昂贵，但它导致了人工神经网络（ANNs）兴趣的复苏。因此，使用反向传播算法的多层前馈网络现在在数据挖掘领域很常见。这些模型具有以下优点和缺点：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can be adapted to classification or numeric prediction problems
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以适应分类或数值预测问题
- en: A “universal approximator” capable of modeling more complex patterns than nearly
    any algorithm
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种“通用逼近器”，能够模拟比几乎所有算法更复杂的模式
- en: Makes few assumptions about the data’s underlying relationships
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据的潜在关系假设很少
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Extremely computationally intensive and slow to train, particularly if the network
    topology is complex
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极其计算密集且训练缓慢，尤其是如果网络拓扑复杂
- en: Very prone to overfitting training data, leading to poor generalization
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极易过拟合训练数据，导致泛化能力差
- en: Results in a complex black-box model that is difficult, if not impossible, to
    interpret
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导致了一个复杂的黑盒模型，难以解释，如果不是不可能的话
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'In its most general form, the backpropagation algorithm iterates through many
    cycles of two processes. Each cycle is known as an **epoch**. Because the network
    contains no *a priori* (existing) knowledge, the starting weights are typically
    set at random. Then, the algorithm iterates through the processes until a stopping
    criterion is reached. Each epoch in the backpropagation algorithm includes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最一般的形式中，反向传播算法通过许多两个过程的循环迭代。每个循环被称为一个**时代**。因为网络不包含**先验**（现有）知识，所以起始权重通常设置为随机。然后，算法通过这些过程迭代，直到达到停止标准。反向传播算法中的每个时代包括：
- en: A **forward phase**, in which the neurons are activated in sequence from the
    input layer to the output layer, applying each neuron’s weights and activation
    function along the way. Upon reaching the final layer, an output signal is produced.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**前向阶段**，在这个阶段，神经元按顺序从输入层到输出层被激活，沿途应用每个神经元的权重和激活函数。当达到最后一层时，产生一个输出信号。
- en: A **backward phase**, in which the network’s output signal resulting from the
    forward phase is compared to the true target value in the training data. The difference
    between the network’s output signal and the true value results in an error that
    is propagated backward in the network to modify the connection weights between
    neurons and reduce future errors.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**反向阶段**，在这个阶段，网络的前向阶段产生的输出信号与训练数据中的真实目标值进行比较。网络输出信号与真实值之间的差异导致一个错误，该错误在网络中向后传播以修改神经元之间的连接权重并减少未来的错误。
- en: 'Over time, the algorithm uses the information sent backward to reduce the total
    errors made by the network. Yet one question remains: because the relationship
    between each neuron’s inputs and outputs is complex, how does the algorithm determine
    how much a weight should be changed?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，算法使用向后发送的信息来减少网络犯下的总错误。然而，一个问题仍然存在：由于每个神经元的输入和输出之间的关系复杂，算法如何确定权重应该改变多少？
- en: The answer to this question involves a technique called **gradient descent**.
    Conceptually, this works similarly to how an explorer trapped in the jungle might
    find a path to water. By examining the terrain and continually walking in the
    direction with the greatest downward slope, the explorer will eventually reach
    the lowest valley, which is likely to be a riverbed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案涉及一种称为**梯度下降**的技术。从概念上讲，这类似于一个被困在丛林中的探险者如何找到通往水源的道路。通过检查地形并持续向最大下坡方向行走，探险者最终会到达最低的谷地，这很可能是河床。
- en: In a similar process, the backpropagation algorithm uses the derivative of each
    neuron’s activation function to identify the gradient in the direction of each
    of the incoming weights—hence the importance of having a differentiable activation
    function. The gradient suggests how steeply the error will be reduced or increased
    for a change in the weight. The algorithm will attempt to change the weights that
    result in the greatest reduction in error by an amount known as the **learning
    rate**. The greater the learning rate, the faster the algorithm will attempt to
    descend down the gradients, which could reduce the training time at the risk of
    overshooting the valley.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个类似的过程中，反向传播算法使用每个神经元的激活函数的导数来识别每个输入权重的方向梯度——因此具有可微分的激活函数的重要性。梯度表明误差将如何随着权重的变化而减少或增加。算法将尝试通过称为**学习率**的量来改变权重，以实现误差的最大减少。学习率越大，算法尝试沿着梯度下降的速度就越快，这可能会减少训练时间，但也可能超过谷地。
- en: '![Diagram  Description automatically generated](img/B17290_07_10.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图解 描述自动生成](img/B17290_07_10.png)'
- en: 'Figure 7.10: The gradient descent algorithm seeks the minimum error but may
    also find a local minimum'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：梯度下降算法寻求最小误差，但也可能找到一个局部最小值
- en: Although the math needed to find the minimum error rate using gradient descent
    is complex and therefore outside the scope of this book, it is easy to apply in
    practice via its implementation in neural network algorithms in R. Let’s apply
    our understanding of multilayer feedforward networks to a real-world problem.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用梯度下降法找到最小误差率所需的数学知识复杂，因此超出了本书的范围，但在实践中通过其在R中神经网络算法的实现应用起来却很容易。让我们将我们对多层前馈网络的理解应用于一个实际问题。
- en: Example – modeling the strength of concrete with ANNs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - 使用人工神经网络建模混凝土强度
- en: In the field of engineering, it is crucial to have accurate estimates of the
    performance of building materials. These estimates are required in order to develop
    safety guidelines governing the materials used in the construction of buildings,
    bridges, and roadways.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在工程领域，拥有建筑材料的准确性能估计至关重要。这些估计是制定用于建筑、桥梁和道路建设中所用材料的安全生产指南所必需的。
- en: Estimating the strength of concrete is a challenge of particular interest. Although
    it is used in nearly every construction project, concrete performance varies greatly
    due to a wide variety of ingredients that interact in complex ways. As a result,
    it is difficult to accurately predict the strength of the final product. A model
    that could reliably predict concrete strength given a listing of the composition
    of the input materials could result in safer construction practices.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 估计混凝土的强度是一个特别有趣的问题。尽管它在几乎每个建设项目中都被使用，但由于各种成分以复杂的方式相互作用，混凝土的性能差异很大。因此，准确预测最终产品的强度是困难的。一个能够根据输入材料的成分列表可靠地预测混凝土强度的模型，可能导致更安全的施工实践。
- en: Step 1 – collecting data
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 收集数据
- en: For this analysis, we will utilize data on the compressive strength of concrete
    donated to the UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))
    by I-Cheng Yeh. As he found success using neural networks to model this data,
    we will attempt to replicate Yeh’s work using a simple neural network model in
    R.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次分析，我们将利用I-Cheng Yeh捐赠给UCI机器学习仓库（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）的混凝土抗压强度数据。由于他发现使用神经网络来模拟这些数据是成功的，我们将尝试使用R中的简单神经网络模型来复制Yeh的工作。
- en: For more information on Yeh’s approach to this learning task, refer to *Modeling
    of Strength of High-Performance Concrete Using Artificial Neural Networks, Yeh,
    IC, Cement and Concrete Research, 1998, Vol. 28, pp. 1797-1808*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Yeh对这一学习任务的方法的更多信息，请参阅*使用人工神经网络建模高性能混凝土强度，Yeh, IC，水泥与混凝土研究，1998，第28卷，第1797-1808页*。
- en: According to the website, the dataset contains 1,030 examples of concrete, with
    8 features describing the components used in the mixture. These features are thought
    to be related to the final compressive strength and include the amount (in kilograms
    per cubic meter) of cement, slag, ash, water, superplasticizer, coarse aggregate,
    and fine aggregate used in the product, in addition to the aging time (measured
    in days).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据网站信息，该数据集包含1,030个混凝土示例，其中包含8个特征，描述了混合物中使用的成分。这些特征被认为与最终的抗压强度相关，包括产品中使用的水泥、矿渣、灰、水、超塑化剂、粗骨料和细骨料的量（以每立方米千克计），以及老化时间（以天为单位）。
- en: To follow along with this example, download the `concrete.csv` file from the
    Packt Publishing website and save it to your R working directory.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随这个示例，请从Packt Publishing网站下载`concrete.csv`文件，并将其保存到您的R工作目录中。
- en: Step 2 – exploring and preparing the data
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 探索和准备数据
- en: 'As usual, we’ll begin our analysis by loading the data into an R object using
    the `read.csv()` function and confirming that it matches the expected structure:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们将通过使用`read.csv()`函数将数据加载到R对象中，并确认它符合预期的结构来开始我们的分析：
- en: '[PRE0]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The nine variables in the data frame correspond to the eight features and one
    outcome we expected, although a problem has become apparent. Neural networks work
    best when the input data is scaled to a narrow range around zero, and here we
    see values ranging anywhere from zero to over a thousand.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框中的九个变量对应于八个特征和一个预期的结果，尽管已经出现了一个问题。神经网络在输入数据缩放到零附近的狭窄范围内表现最佳，而在这里我们看到值从零到超过一千。
- en: Typically, the solution to this problem is to rescale the data with a normalizing
    or standardization function. If the data follows a bell-shaped curve (a normal
    distribution, as described in *Chapter 2*, *Managing and Understanding Data*),
    then it may make sense to use standardization via R’s built-in `scale()` function.
    On the other hand, if the data follows a uniform distribution or is severely non-normal,
    then normalization to a zero-to-one range may be more appropriate. In this case,
    we’ll use the latter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解决这个问题的方法是使用归一化或标准化函数对数据进行缩放。如果数据遵循钟形曲线（如*第2章*中描述的*管理和理解数据*的正常分布），那么使用R内置的`scale()`函数进行标准化可能是有意义的。另一方面，如果数据遵循均匀分布或严重非正态分布，那么将数据归一化到零到一的范围可能更合适。在这种情况下，我们将使用后者。
- en: 'In *Chapter 3*, *Lazy Learning – Classification Using Nearest Neighbors*, we
    defined our own `normalize()` function as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第3章*，*懒惰学习 – 使用最近邻进行分类* 中，我们定义了自己的 `normalize()` 函数如下：
- en: '[PRE2]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After executing this code, our `normalize()` function can be applied to every
    column in the concrete data frame using the `lapply()` function as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们可以使用 `lapply()` 函数将 `normalize()` 函数应用于混凝土数据框中的每一列，如下所示：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To confirm that the normalization worked, we can see that the minimum and maximum
    strength are now zero and one, respectively:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认归一化工作正常，我们可以看到现在最小和最大的强度分别为零和一：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In comparison, the original minimum and maximum values were 2.33 and 82.60:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相比，原始的最小值和最大值分别为 2.33 和 82.60：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Any transformation applied to the data prior to training the model must be applied
    in reverse to convert it back into the original units of measurement. To facilitate
    the rescaling, it is wise to save the original data, or at least the summary statistics
    of the original data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前应用于数据的任何转换都必须反向应用，以便将其转换回原始的单位。为了便于缩放，保存原始数据或至少原始数据的摘要统计信息是明智的。
- en: 'Following Yeh’s precedent in the original publication, we will partition the
    data into a training set with 75 percent of the examples and a testing set with
    25 percent. The CSV file we used was already sorted in random order, so we simply
    need to divide it into two portions:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 沿用原始出版物中 Yeh 的先例，我们将数据分为包含 75% 示例的训练集和包含 25% 的测试集。我们使用的 CSV 文件已经随机排序，所以我们只需将其分为两部分：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We’ll use the training dataset to build the neural network and the testing dataset
    to evaluate how well the model generalizes to future results. As it is easy to
    overfit a neural network, this step is very important.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用训练数据集来构建神经网络，并使用测试数据集来评估模型对未来结果的泛化能力。由于神经网络很容易过拟合，这一步非常重要。
- en: Step 3 – training a model on the data
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: To model the relationship between the ingredients used in concrete and the strength
    of the finished product, we will use a multilayer feedforward neural network.
    The `neuralnet` package by Stefan Fritsch and Frauke Guenther provides a standard
    and easy-to-use implementation of such networks. It also offers a function to
    plot the network topology. For these reasons, the `neuralnet` implementation is
    a strong choice for learning more about neural networks, though that’s not to
    say that it cannot be used to accomplish real work as well—it’s quite a powerful
    tool, as you will soon see.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟混凝土中使用的成分与最终产品强度之间的关系，我们将使用多层前馈神经网络。Stefan Fritsch 和 Frauke Guenther 的 `neuralnet`
    包提供了一个标准且易于使用的此类网络实现。它还提供了一个用于绘制网络拓扑的功能。因此，`neuralnet` 实现是学习更多关于神经网络的一个很好的选择，尽管这并不意味着它不能用于完成实际工作——它是一个非常强大的工具，你很快就会看到。
- en: There are several other commonly used packages to train simple ANN models in
    R, each with unique strengths and weaknesses. Because it ships as part of the
    standard R installation, the `nnet` package is perhaps the most frequently cited
    ANN implementation. It uses a slightly more sophisticated algorithm than standard
    backpropagation. Another option is the `RSNNS` package, which offers a complete
    suite of neural network functionality, with the downside being that it is more
    difficult to learn. The specialized software for building or using deep learning
    neural networks is covered in *Chapter 15*, *Making Use of Big Data*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中训练简单的 ANN 模型有几个其他常用的包，每个包都有其独特的优点和缺点。由于它作为标准 R 安装的一部分提供，`nnet` 包可能是引用最多的
    ANN 实现。它使用比标准反向传播稍微复杂一点的算法。另一个选择是 `RSNNS` 包，它提供了一个完整的神经网络功能套件，缺点是它更难学习。构建或使用深度学习神经网络的专用软件在第
    15 章 *利用大数据* 中介绍。
- en: 'As `neuralnet` is not included in base R, you will need to install it by typing
    `install.packages``("neuralnet")` and load it with the `library(neuralnet)` command.
    The included `neuralnet()` function can be used for training neural networks for
    numeric prediction using the following syntax:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `neuralnet` 不包含在基础 R 中，您需要通过输入 `install.packages("neuralnet")` 来安装它，并使用 `library(neuralnet)`
    命令加载它。包含的 `neuralnet()` 函数可以用于使用以下语法训练用于数值预测的神经网络：
- en: '![Text, letter  Description automatically generated](img/B17290_07_11.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![文本，字母  描述自动生成](img/B17290_07_11.png)'
- en: 'Figure 7.11: Neural network syntax'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：神经网络语法
- en: 'We’ll begin by training the simplest multilayer feedforward network with the
    default settings using only a single hidden node. Because the process of training
    an ANN involves randomization, the `set.seed()` function used here will ensure
    the same result is produced when the `neuralnet()` function is run:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始使用默认设置训练最简单的多层前馈网络，仅使用单个隐藏节点。由于训练人工神经网络的过程涉及随机化，这里使用的 `set.seed()` 函数将确保在运行
    `neuralnet()` 函数时产生相同的结果：
- en: '[PRE9]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can then visualize the network topology using the `plot()` function on the
    resulting model object:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用结果模型对象的 `plot()` 函数来可视化网络拓扑：
- en: '[PRE10]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Diagram  Description automatically generated](img/B17290_07_12.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_12.png)'
- en: 'Figure 7.12: Topology visualization of a simple multilayer feedforward network'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：简单多层前馈网络的拓扑可视化
- en: In this simple model, there is one input node for each of the eight features,
    followed by a single hidden node and a single output node that predicts the concrete
    strength. The weights for each of the connections are also depicted, as are the
    **bias terms** indicated by the nodes labeled with the number `1`. The bias terms
    are numeric constants that allow the value at the indicated nodes to be shifted
    upward or downward, much like the intercept in a linear equation. In a linear
    equation of the form *y = ax + b*, the intercept *b* allows *y* to have a value
    other than 0 when *x = 0*. Similarly, the bias terms in a neural network allow
    the node to pass a value other than zero when the inputs are zero. This gives
    more flexibility for learning the true patterns found in the data, which in turn
    helps the model fit better. In the specific case of our concrete model, even though
    it isn’t feasible in the real world to have a case where all inputs to the concrete
    such as cement, age, and water are all zero, we wouldn’t necessarily expect strength
    to cross the origin exactly at zero even as the values of these factors approach
    zero. We might expect a model of body weight versus age to have a bias term above
    zero because the weight at birth (*age = 0*) is greater than zero.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单模型中，每个特征都有一个输入节点，接着是一个单一的隐藏节点和一个单一的输出节点，该输出节点预测混凝土强度。每个连接的权重也被描绘出来，以及由标记为数字
    `1` 的节点表示的**偏差项**。偏差项是数值常数，允许指示节点的值向上或向下移动，就像线性方程中的截距一样。在形式为 *y = ax + b* 的线性方程中，截距
    *b* 允许当 *x = 0* 时，*y* 有一个非零的值。同样，神经网络中的偏差项允许当输入为零时，节点传递一个非零的值。这为学习数据中找到的真实模式提供了更多的灵活性，从而有助于模型更好地拟合。在具体到我们的混凝土模型的情况下，尽管在现实世界中不可能所有输入到混凝土中的因素（如水泥、年龄和水）都为零，但我们不一定会期望强度在接近零的这些因素值时，强度会精确地穿过原点。我们可能期望体重与年龄的关系模型具有一个大于零的偏差项，因为出生时的体重（*年龄
    = 0*）是大于零的。
- en: A neural network with a single hidden node can be thought of as a cousin of
    the linear regression models we studied in *Chapter 6*, *Forecasting Numeric Data
    – Regression Methods*. The weight between each input node and the hidden node
    is similar to the beta coefficients, and the weight for the bias term is similar
    to the intercept. If a linear activation function is used, the neural network
    is almost exactly linear regression. A key difference, however, is that the ANN
    is trained using gradient descent while linear regression typically uses the least
    squares approach.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有单个隐藏节点的神经网络可以被视为我们在 *第6章* 中研究的线性回归模型的远亲，即 *预测数值数据 – 回归方法*。每个输入节点与隐藏节点之间的权重类似于beta系数，而偏差项的权重类似于截距。如果使用线性激活函数，神经网络几乎就是线性回归。然而，一个关键的区别是，人工神经网络使用梯度下降进行训练，而线性回归通常使用最小二乘法。
- en: At the bottom of the figure, R reports the number of training steps and an error
    measure called the **sum of squared errors** (**SSE**), which, as you might expect,
    is the sum of the squared differences between the predicted and actual values.
    The lower the SSE, the more closely the model conforms to the training data, which
    tells us about performance on the training data but little about how it will perform
    on unseen data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的下部，R报告了训练步骤的数量和一个称为**平方误差和**（**SSE**）的错误度量，正如你可能预期的，它是预测值和实际值之间平方差的和。SSE越低，模型与训练数据的符合度越高，这告诉我们关于训练数据的表现，但很少告诉我们它在未见数据上的表现。
- en: Step 4 – evaluating model performance
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: 'The network topology diagram gives us a peek into the black box of the ANN,
    but it doesn’t provide much information about how well the model fits future data.
    To generate predictions on the test dataset, we can use the `compute()` function
    as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑图让我们窥视了人工神经网络（ANN）的黑箱，但它并没有提供太多关于模型如何适应未来数据的信息。为了在测试数据集上生成预测，我们可以使用`compute()`函数如下：
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `compute()` function works a bit differently from the `predict()` functions
    we’ve used so far. It returns a list with two components: `$neurons`, which stores
    the neurons for each layer in the network, and `$net.result`, which stores the
    predicted values. We’ll want the latter:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute()` 函数与我们迄今为止使用的 `predict()` 函数工作方式略有不同。它返回一个包含两个组件的列表：`$neurons`，它存储网络中每一层的神经元，以及`$net.result`，它存储预测值。我们想要后者：'
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Because this is a numeric prediction problem rather than a classification problem,
    we cannot use a confusion matrix to examine model accuracy. Instead, we’ll measure
    the correlation between our predicted concrete strength and the true value. If
    the predicted and actual values are highly correlated, the model is likely to
    be a useful gauge of concrete strength.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个数值预测问题而不是分类问题，我们不能使用混淆矩阵来检查模型精度。相反，我们将测量我们预测的混凝土强度与真实值之间的相关性。如果预测值和实际值高度相关，则该模型很可能是混凝土强度的有用衡量标准。
- en: 'Recall that the `cor()` function is used to obtain a correlation between two
    numeric vectors:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`cor()` 函数用于获取两个数值向量之间的相关性：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Correlations close to one indicate strong linear relationships between two variables.
    Therefore, the correlation here of about 0.806 indicates a fairly strong relationship.
    This implies that our model is doing a fairly good job, even with only a single
    hidden node. Yet, given that we only used one hidden node, it is likely that we
    can improve the performance of our model. Let’s try to do a bit better.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接近1的相关性表明两个变量之间存在强烈的线性关系。因此，这里大约0.806的相关性表明存在相当强的关系。这表明我们的模型做得相当不错，即使只有一个隐藏节点。然而，鉴于我们只使用了一个隐藏节点，我们很可能会提高我们模型的表现。让我们尝试做得更好。
- en: Step 5 – improving model performance
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 - 提高模型性能
- en: 'As networks with more complex topologies are capable of learning more difficult
    concepts, let’s see what happens when we increase the number of hidden nodes to
    five. We use the `neuralnet()` function as before, but add the parameter `hidden
    = 5`. Note that due to the increased complexity of this neural network, depending
    on the capabilities of your computer, the new model may take 30 to 60 seconds
    to train:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于具有更复杂拓扑结构的网络能够学习更复杂的概念，让我们看看当我们把隐藏节点的数量增加到五个时会发生什么。我们像以前一样使用`neuralnet()`函数，但添加参数`hidden
    = 5`。请注意，由于这个神经网络复杂性的增加，根据您计算机的能力，新的模型可能需要30到60秒来训练：
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Plotting the network again, we see a drastic increase in the number of connections.
    How did this impact performance?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 再次绘制网络图，我们发现连接数量急剧增加。这对性能有何影响？
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Diagram  Description automatically generated](img/B17290_07_13.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_13.png)'
- en: 'Figure 7.13: Topology visualization of a neural network with additional hidden
    nodes'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13：具有额外隐藏节点的神经网络的拓扑可视化
- en: Notice that the reported error (measured again by the SSE) has been reduced
    from 5.08 in the previous model to 1.63 here. Additionally, the number of training
    steps rose from 4,882 to 86,849, which should come as no surprise given how much
    more complex the model has become. More complex networks take many more iterations
    to find the optimal weights.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，报告的误差（再次通过SSE测量）已从先前模型的5.08降低到这里的1.63。此外，训练步骤的数量从4,882增加到86,849，考虑到模型变得更加复杂，这一点并不令人惊讶。更复杂的网络需要更多的迭代来找到最优权重。
- en: 'Applying the same steps to compare the predicted values to the true values,
    we now obtain a correlation of around 0.92, which is a considerable improvement
    over the previous result of 0.80 with a single hidden node:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同的步骤应用于将预测值与真实值进行比较，我们现在获得大约0.92的相关性，这比之前单个隐藏节点时的0.80结果有相当大的改进：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Despite these substantial improvements, there is still more we can do to attempt
    to improve the model’s performance. In particular, we have the ability to add
    additional hidden layers and change the network’s activation function. In making
    these changes, we create the foundations of a very simple DNN.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些显著的改进，我们仍然可以做一些更多的事情来尝试提高模型的表现。特别是，我们有能力添加额外的隐藏层并更改网络的激活函数。在做出这些更改时，我们为一个非常简单的深度神经网络（DNN）奠定了基础。
- en: The choice of activation function is usually very important for deep learning.
    The best function for a particular learning task is typically identified through
    experimentation, then shared more widely within the machine learning research
    community. As deep learning has become more studied, an activation function known
    as a **rectifier** has become extremely popular due to its success in complex
    tasks such as image recognition. A node in a neural network that uses the rectifier
    activation function is known as a **rectified linear unit** (**ReLU**). As depicted
    in the following figure, the ReLU activation function is defined such that it
    returns *x* if *x* is at least zero, and zero otherwise. The popularity of this
    function for deep learning is due to the fact that it is nonlinear yet has simple
    mathematical properties that make it both computationally inexpensive and highly
    efficient for gradient descent. Unfortunately, its derivative is undefined at
    *x = 0* and therefore cannot be used with the `neuralnet()` function.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择通常对深度学习非常重要。对于特定的学习任务，最佳函数通常是通过实验确定的，然后在机器学习研究社区中更广泛地共享。随着深度学习研究的深入，一种称为**rectifier**的激活函数因其成功应用于图像识别等复杂任务而变得极为流行。使用rectifier激活函数的神经网络中的节点被称为**rectified
    linear unit**（**ReLU**）。如图所示，ReLU激活函数的定义是，当*x*至少为零时返回*x*，否则返回零。这种函数在深度学习中的流行是因为它非线性，但具有简单的数学性质，这使得它在计算上既便宜又高效，非常适合梯度下降。不幸的是，其导数在*x
    = 0*时未定义，因此不能与`neuralnet()`函数一起使用。
- en: 'Instead, we can use a smooth approximation of the ReLU known as **softplus**
    or **SmoothReLU**, an activation function defined as *log*(*1 + e*^x). As shown
    in the following figure, the softplus function is nearly zero for *x* less than
    zero and approximately *x* when *x* is greater than zero:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以使用ReLU的平滑近似，称为**softplus**或**SmoothReLU**，这是一个定义为*log*(*1 + e*^x)的激活函数。如图所示，当*x*小于零时，softplus函数几乎为零，而当*x*大于零时，大约等于*x*：
- en: '![Chart, line chart  Description automatically generated](img/B17290_07_14.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B17290_07_14.png)'
- en: 'Figure 7.14: The softplus activation function provides a smooth, differentiable
    approximation of ReLU'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：softplus激活函数提供了ReLU的平滑、可微近似的描述
- en: 'To define a `softplus()` function in R, use the following code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要在R中定义`softplus()`函数，请使用以下代码：
- en: '[PRE19]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This activation function can be provided to `neuralnet()` using the `act.fct`
    parameter. Additionally, we will add a second hidden layer of five nodes by supplying
    the `hidden` parameter the integer vector `c(5, 5)`. This creates a two-layer
    network, each having five nodes, all using the softplus activation function. As
    before, this may take a minute or so to run:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`act.fct`参数将此激活函数提供给`neuralnet()`。此外，我们还将通过将`hidden`参数设置为整数向量`c(5, 5)`来添加一个包含五个节点的第二隐藏层。这创建了一个两层网络，每层都有五个节点，都使用softplus激活函数。和之前一样，这可能需要一分钟或更长时间来运行：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The network visualization now shows a topology of two hidden layers of five
    nodes each:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 网络可视化现在显示了一个由每个包含五个节点的两层隐藏层组成的拓扑结构：
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Diagram  Description automatically generated](img/B17290_07_15.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成的描述](img/B17290_07_15.png)'
- en: 'Figure 7.15: Visualizing our network with two layers of hidden nodes using
    the softplus activation function'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：使用软plus激活函数，用两层隐藏节点可视化我们的网络
- en: 'Again, let’s compute the correlation between the predicted and actual concrete
    strength:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们计算预测值和实际混凝土强度之间的相关性：
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The correlation between the predicted and actual strength was 0.935, which is
    our best performance yet. Interestingly, in the original publication, Yeh reported
    a correlation of 0.885\. This means that with relatively little effort, we were
    able to match and even exceed the performance of a subject matter expert. Of course,
    Yeh’s results were published in 1998, giving us the benefit of 25 years of additional
    neural network research!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值和实际强度之间的相关性为0.935，这是我们迄今为止的最佳性能。有趣的是，在原始出版物中，Yeh报告的相关性为0.885。这意味着我们只需付出相对较小的努力，就能匹配甚至超过领域专家的性能。当然，Yeh的结果是在1998年发表的，这使我们能够受益于25年额外的神经网络研究！
- en: 'One important thing to be aware of is that, because we had normalized the data
    prior to training the model, the predictions are also on a normalized scale from
    zero to one. For example, the following code shows a data frame comparing the
    original dataset’s concrete strength values to their corresponding predictions
    side by side:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个重要的事情需要注意，因为我们训练模型之前已经对数据进行归一化处理，所以预测也是在从零到一的归一化尺度上。例如，以下代码显示了一个数据框，它将原始数据集的混凝土强度值与其对应的预测值并排比较：
- en: '[PRE24]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using correlation as a performance metric, the choice of normalized or unnormalized
    data does not affect the result. For example, the correlation of 0.935 is the
    same whether the predicted strengths are compared to the original, unnormalized
    concrete strength values (`strengths$actual`) or to the normalized values (`concrete_test$strength`):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相关性作为性能指标，归一化或未归一化数据的选择不会影响结果。例如，无论预测强度与原始、未归一化的混凝土强度值（`strengths$actual`）还是与归一化值（`concrete_test$strength`）进行比较，相关性为0.935都是相同的：
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: However, if we were to compute a different performance metric, such as the percentage
    difference between the predicted and actual values, the choice of scale would
    matter quite a bit.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们计算不同的性能指标，例如预测值和实际值之间的百分比差异，那么选择的比例就会相当重要。
- en: 'With this in mind, we can create an `unnormalize()` function that reverses
    the min-max normalization procedure and allow us to convert the normalized predictions
    into the original scale:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个前提下，我们可以创建一个`unnormalize()`函数，它反转了min-max归一化过程，并允许我们将归一化的预测值转换为原始尺度：
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After applying the custom `unnormalize()` function to the predictions, we can
    see that the new predictions (`pred_new`) are on a similar scale to the original
    concrete strength values. This allows us to compute a meaningful percentage error
    value. The resulting `error_pct` is the percentage difference between the true
    and predicted values:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在将自定义的`unnormalize()`函数应用于预测之后，我们可以看到新的预测值（`pred_new`）与原始混凝土强度值处于相似的尺度上。这使我们能够计算一个有意义的百分比误差值。得到的`error_pct`是真实值和预测值之间的百分比差异：
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Unsurprisingly, the correlation remains the same despite reversing the normalization:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，尽管反转了归一化，相关性仍然保持不变：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: When applying neural networks to your own projects, you will need to perform
    a similar series of steps to return the data to its original scale.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当将神经网络应用于自己的项目时，你需要执行一系列类似的步骤，以将数据返回到其原始尺度。
- en: You may also find that neural networks quickly become much more complicated
    as they are applied to more challenging learning tasks. For example, you may find
    that you run into the so-called **vanishing gradient problem** and closely related
    **exploding gradient problem**, where the backpropagation algorithm fails to find
    a useful solution due to an inability to converge in a reasonable amount of time.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现，随着神经网络应用于更具挑战性的学习任务，它们会迅速变得更加复杂。例如，你可能会遇到所谓的**消失梯度问题**和与之密切相关的**爆炸梯度问题**，在这些问题中，由于无法在合理的时间内收敛，反向传播算法无法找到有用的解决方案。
- en: 'As a remedy to these problems, one may perhaps try varying the number of hidden
    nodes, applying different activation functions such as the ReLU, adjusting the
    learning rate, and so on. The `?neuralnet` help page provides more information
    on the various parameters that can be adjusted. This leads to another problem,
    however, in which testing many parameters becomes a bottleneck to building a strong-performing
    model. This is the tradeoff of ANNs and, even more so, DNNs: harnessing their
    great potential requires a great investment of time and computing power.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这些问题的补救措施，人们可能会尝试改变隐藏节点的数量，应用不同的激活函数，如ReLU，调整学习率等等。`?neuralnet`帮助页面提供了更多关于可以调整的各种参数的信息。然而，这又导致另一个问题，即测试许多参数成为构建高性能模型的一个瓶颈。这是人工神经网络（ANNs）和，尤其是深度神经网络（DNNs）的权衡：利用它们的巨大潜力需要巨大的时间和计算资源投入。
- en: Just as is often the case in life more generally, it is possible to trade time
    and money in machine learning. Using paid cloud computing resources such as **Amazon
    Web Services** (**AWS**) and Microsoft Azure allows one to build more complex
    models or test many models more quickly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 正如生活中更普遍的情况一样，在机器学习中，我们可以权衡时间和金钱。使用付费的云计算资源，如**亚马逊网络服务**（**AWS**）和微软Azure，可以更快地构建更复杂的模型或测试许多模型。
- en: Understanding support vector machines
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解支持向量机
- en: A **support vector machine** (**SVM**) can be imagined as a surface that creates
    a boundary between points of data plotted in a multidimensional space representing
    examples and their feature values. The goal of an SVM is to create a flat boundary
    called a **hyperplane**, which divides the space to create fairly homogeneous
    partitions on either side. In this way, SVM learning combines aspects of both
    the instance-based nearest neighbor learning presented in *Chapter 3*, *Lazy Learning
    – Classification Using Nearest Neighbors*, and the linear regression modeling
    described in *Chapter 6*, *Forecasting Numeric Data – Regression Methods*. The
    combination is extremely powerful, allowing SVMs to model highly complex relationships.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将**支持向量机**（**SVM**）想象为在多维空间中绘制数据点所形成的表面，该空间代表示例及其特征值。SVM的目标是创建一个称为**超平面**的平坦边界，将空间分割成两侧相对均匀的分区。通过这种方式，SVM学习结合了第3章中介绍的基于实例的最近邻学习（*Lazy
    Learning – Classification Using Nearest Neighbors*）和第6章中描述的线性回归建模（*Forecasting
    Numeric Data – Regression Methods*）的方面。这种组合非常强大，使得SVM能够模拟高度复杂的关系。
- en: Although the basic mathematics that drive SVMs has been around for decades,
    interest in them grew greatly after they were adopted by the machine learning
    community. Their popularity exploded after high-profile success stories about
    difficult learning problems, as well as the development of award-winning SVM algorithms
    that were implemented in well-supported libraries across many programming languages,
    including R. SVMs have thus been adopted by a wide audience, which might have
    otherwise been unable to apply the somewhat complex mathematics needed to implement
    an SVM. The good news is that although the mathematics may be difficult, the basic
    concepts are understandable.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管驱动SVM的基本数学原理已经存在了几十年，但自从机器学习社区采用它们之后，对它们的兴趣大大增加。在关于困难学习问题的知名成功故事以及获奖的SVM算法在许多编程语言（包括R）中得到实现之后，它们的受欢迎程度爆炸式增长。因此，SVM被广泛采用，这可能会让那些否则无法应用实现SVM所需的相对复杂数学的受众。好消息是，尽管数学可能很难，但基本概念是可理解的。
- en: 'SVMs can be adapted for use with nearly any type of learning task, including
    both classification and numeric prediction. Many of the algorithm’s key successes
    have been in pattern recognition. Notable applications include:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVMs）可以适应用于几乎任何类型的机器学习任务，包括分类和数值预测。该算法的关键成功之一在于模式识别。值得注意的应用包括：
- en: Classification of microarray gene expression data in the field of bioinformatics
    to identify cancer or other genetic diseases
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生物信息学领域对微阵列基因表达数据进行分类，以识别癌症或其他遗传疾病
- en: Text categorization, such as identification of the language used in a document
    or classification of documents by subject matter
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，例如识别文档中使用的语言或根据主题内容对文档进行分类
- en: The detection of rare yet important events like engine failure, security breaches,
    or earthquakes
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测罕见但重要的事件，如发动机故障、安全漏洞或地震
- en: SVMs are most easily understood when used for binary classification, which is
    how the method has been traditionally applied. Therefore, in the remaining sections,
    we will focus only on SVM classifiers. Principles like those presented here also
    apply when adapting SVMs for numeric prediction.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当用于二元分类时，SVM最容易理解，这也是该方法传统上应用的方式。因此，在接下来的部分中，我们将仅关注SVM分类器。这里提出的原则也适用于将SVM适应于数值预测。
- en: Classification with hyperplanes
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用超平面进行分类
- en: As noted previously, SVMs use a boundary called a **hyperplane** to partition
    data into groups of similar class values. For example, the following figure depicts
    hyperplanes that separate groups of circles and squares in two and three dimensions.
    Because the circles and squares can be separated perfectly by a straight line
    or flat surface, they are said to be **linearly separable**. At first, we’ll consider
    only a simple case where this is true, but SVMs can also be extended to problems
    where the points are not linearly separable.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，SVM使用称为**超平面**的边界将数据分组为具有相似类值的组。例如，以下图展示了在二维和三维空间中分离圆形和正方形组的超平面。因为圆形和正方形可以通过一条直线或平坦的表面完美地分开，所以它们被称为**线性可分**。最初，我们将考虑这种情况是简单的情况，但SVM也可以扩展到点不是线性可分的问题。
- en: '![](img/B17290_07_16.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_07_16.png)'
- en: 'Figure 7.16: The squares and circles are linearly separable in both two and
    three dimensions'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：在二维和三维空间中，正方形和圆形都是线性可分的
- en: For convenience, the hyperplane is traditionally depicted as a line in 2D space,
    but this is simply because it is difficult to illustrate space in greater than
    2 dimensions. In reality, the hyperplane is a flat surface in a high-dimensional
    space—a concept that can be difficult to get your mind around.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，超平面在二维空间中被传统地描绘为一条线，但这仅仅是因为在超过2维的空间中很难描绘空间。实际上，超平面是高维空间中的一个平面——这是一个可能难以理解的概念。
- en: In two dimensions, the task of the SVM algorithm is to identify a line that
    separates the two classes. As shown in the following figure, there is more than
    one choice of dividing line between the groups of circles and squares. Three such
    possibilities are labeled *a*, *b*, and *c*. How does the algorithm choose?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，SVM算法的任务是识别一条将两个类别分开的线。如图所示，在圆和正方形的组之间有不止一条分割线。三种这样的可能性被标记为*a*，*b*和*c*。算法是如何选择的？
- en: '![Diagram  Description automatically generated with low confidence](img/B17290_07_17.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成，置信度低](img/B17290_07_17.png)'
- en: 'Figure 7.17: Three of many potential lines dividing the squares and circles'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：许多可能的分割正方形和圆的线中的三条
- en: The answer to that question involves a search for the **maximum margin hyperplane**
    (**MMH**) that creates the greatest separation between the two classes. Although
    any of the three lines separating the circles and squares would correctly classify
    all the data points, the line that leads to the greatest separation is expected
    to generalize the best to future data. The maximum margin will improve the chance
    that even if random noise is added, each class will remain on its own side of
    the boundary.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 那个问题的答案涉及到寻找**最大间隔超平面**（**MMH**）以在两个类别之间创建最大的分离。尽管任何三条分割圆和正方形的线都可以正确分类所有数据点，但预期导致最大分离的线将最好地推广到未来的数据。最大间隔将提高即使添加随机噪声，每个类别仍然保持在边界一侧的机会。
- en: The **support vectors** (indicated by arrows in the figure that follows) are
    the points from each class that are the closest to the MMH. Each class must have
    at least one support vector, but it is possible to have more than one. The support
    vectors alone define the MMH. This is a key feature of SVMs; the support vectors
    provide a very compact way to store a classification model, even if the number
    of features is extremely large.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量**（如图中箭头所示）是每个类别中最接近MMH的点。每个类别至少必须有一个支持向量，但可能有多个。支持向量本身定义了MMH。这是SVM的一个关键特性；支持向量提供了一种非常紧凑的方式来存储分类模型，即使特征数量非常大。'
- en: '![Diagram  Description automatically generated](img/B17290_07_18.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_18.png)'
- en: 'Figure 7.18: The MMH is defined by the support vectors'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18：MMH由支持向量定义
- en: The algorithm to identify the support vectors relies on vector geometry and
    involves some tricky mathematics outside the scope of this book. However, the
    basic principles of the process are straightforward.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 识别支持向量的算法依赖于向量几何，并涉及到一些超出本书范围的复杂数学。然而，这个过程的基本原理是直接的。
- en: 'More information on the mathematics of SVMs can be found in the classic paper
    *Support-Vector Networks, Cortes, C and Vapnik, V, Machine Learning, 1995, Vol.
    20, pp. 273-297*. A beginner-level discussion can be found in *Support Vector
    Machines: Hype or Hallelujah?, Bennett, KP and Campbell, C, SIGKDD Explorations,
    2000, Vol. 2, pp. 1-13*. A more in-depth look can be found in *Support Vector
    Machines, Steinwart, I and Christmann, A, New York: Springer, 2008*.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于SVM数学的信息可以在经典论文《支持向量机，Cortes, C 和 Vapnik, V，机器学习，1995，第20卷，第273-297页》中找到。入门级别的讨论可以在《支持向量机：炒作还是赞美？，Bennett,
    KP 和 Campbell, C，SIGKDD Explorations，2000，第2卷，第1-13页》中找到。更深入的探讨可以在《支持向量机，Steinwart,
    I 和 Christmann, A，纽约：Springer，2008》中找到。
- en: The case of linearly separable data
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性可分数据的案例
- en: Finding the maximum margin is easiest under the assumption that the classes
    are linearly separable. In this case, the MMH is as far away as possible from
    the outer boundaries of the two groups of data points. These outer boundaries
    are known as the **convex hull**. The MMH is then the perpendicular bisector of
    the shortest line between the two convex hulls. Sophisticated computer algorithms
    that use a technique known as **quadratic optimization** can find the maximum
    margin in this way.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设类别是线性可分的情况下，找到最大间隔是最容易的。在这种情况下，最大间隔距离两组数据点的外部边界尽可能远。这些外部边界被称为**凸包**。最大间隔是两个凸包之间最短路径的垂直平分线。使用称为**二次优化**的技术的高级计算机算法可以通过这种方式找到最大间隔。
- en: '![Diagram  Description automatically generated](img/B17290_07_19.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_19.png)'
- en: 'Figure 7.19: The MMH is the perpendicular bisector of the shortest path between
    convex hulls'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19：最大间隔超平面是凸包之间最短路径的垂直平分线
- en: An alternative (but equivalent) approach involves a search through the space
    of every possible hyperplane in order to find a set of two parallel planes that
    divide the points into homogeneous groups yet themselves are as far apart as possible.
    To use a metaphor, one can imagine this process as like trying to find the thickest
    mattress that can fit up a stairwell to your bedroom.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种（但等效）的方法是通过搜索每个可能超平面的空间来找到一组平行平面，这些平面将点分成同质组，同时它们彼此尽可能远。用比喻来说，可以想象这个过程就像尝试找到可以塞进你卧室楼梯井的最厚的床垫。
- en: 'To understand this search process, we’ll need to define exactly what we mean
    by a hyperplane. In *n*-dimensional space, the following equation is used:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个搜索过程，我们需要精确地定义我们所说的超平面。在 *n*-维空间中，以下方程被使用：
- en: '![](img/B17290_07_002.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_002.png)'
- en: If you aren’t familiar with this notation, the arrows above the letters indicate
    that they are vectors rather than single numbers. In particular, *w* is a vector
    of *n* weights, that is, {*w*[1]*, w*[2]*, ..., w*[n]}, and *b* is a single number
    known as the bias. The bias is conceptually equivalent to the intercept term in
    the slope-intercept form discussed in *Chapter 6*, *Forecasting Numeric Data –
    Regression Methods*.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个符号不熟悉，字母上方的箭头表示它们是向量而不是单个数字。特别是，*w* 是一个包含 *n* 个权重的向量，即 {*w*[1]*, w*[2]*,
    ..., w*[n]}，而 *b* 是一个称为偏置的单个数字。偏置在概念上等同于在第 6 章“预测数值数据 - 回归方法”中讨论的斜截式中的截距项。
- en: If you’re having trouble imagining the plane in multidimensional space, don’t
    worry about the details. Simply think of the equation as a way to specify a surface,
    much like the slope-intercept form (*y = mx + b*) is used to specify lines in
    2D space.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你难以想象多维空间中的平面，不必担心细节。只需将方程视为指定一个表面的方式，就像斜截式（*y = mx + b*）在二维空间中用于指定直线一样。
- en: 'Using this formula, the goal of the process is to find a set of weights that
    specify two hyperplanes, as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个公式，该过程的目的是找到一组权重，以指定两个超平面，如下所示：
- en: '![](img/B17290_07_003.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_003.png)'
- en: '![](img/B17290_07_004.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_004.png)'
- en: 'We will also require that these hyperplanes are specified such that all the
    points of one class fall above the first hyperplane and all the points of the
    other class fall beneath the second hyperplane. The two planes should create a
    gap such that there are no points from either class in the space between them.
    This is possible so long as the data is linearly separable. Vector geometry defines
    the distance between these two planes—the distance we want as large as possible—as:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还要求这些超平面被指定得使得一个类别的所有点都位于第一个超平面上方，而另一个类别的所有点都位于第二个超平面下方。这两个平面应该产生一个间隙，使得两个平面之间的空间中没有来自任一类的点。只要数据是线性可分的，这是可能的。向量几何定义了这两个平面之间的距离——我们希望尽可能大的距离——如下：
- en: '![](img/B17290_07_005.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_005.png)'
- en: 'Here, ||*w*|| indicates the **Euclidean norm** (the distance from the origin
    to vector *w*). Because ||*w*|| is the denominator, to maximize distance, we need
    to minimize ||*w*||. The task is typically re-expressed as a set of constraints,
    as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，||*w*|| 表示 **欧几里得范数**（从原点到向量 *w* 的距离）。因为 ||*w*|| 是分母，为了最大化距离，我们需要最小化 ||*w*||。这个任务通常被重新表述为以下一组约束条件：
- en: '![](img/B17290_07_006.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_006.png)'
- en: '![](img/B17290_07_007.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_007.png)'
- en: Although this looks messy, it’s really not too complicated to understand conceptually.
    Basically, the first line implies that we need to minimize the Euclidean norm
    (squared and divided by two to make the calculation easier). The second line notes
    that this is subject to (*s.t.*) the condition that each of the *y*[i] data points
    is correctly classified. Note that *y* indicates the class value (transformed
    into either +1 or -1) and the upside-down “A” is shorthand for “for all.”
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来很混乱，但从概念上理解其实并不复杂。基本上，第一行意味着我们需要最小化欧几里得范数（平方并除以二以简化计算）。第二行指出，这受制于（*s.t.*）每个
    *y*[i] 数据点被正确分类的条件。请注意，*y* 表示类别值（转换为+1或-1），倒置的“A”是“对于所有”的简称。
- en: As with the other method for finding the maximum margin, finding a solution
    to this problem is a task best left for quadratic optimization software. Although
    it can be processor-intensive, specialized algorithms can solve these problems
    quickly even on large datasets.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 与寻找最大边缘的其他方法一样，找到这个问题的解决方案最好是留给二次优化软件的任务。尽管它可能需要大量的处理器资源，但专门的算法甚至可以在大型数据集上快速解决这个问题。
- en: The case of nonlinearly separable data
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性可分数据的情况
- en: 'As we’ve worked through the theory behind SVMs, you may be wondering about
    the elephant in the room: what happens in a case where that the data is not linearly
    separable? The solution to this problem is the use of a **slack variable**, which
    creates a soft margin that allows some points to fall on the incorrect side of
    the margin. The figure that follows illustrates two points falling on the wrong
    side of the line with the corresponding slack terms (denoted by the Greek letter
    Xi):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们研究SVM背后的理论，你可能想知道房间里的大象：当数据不是线性可分时会发生什么？这个问题的解决方案是使用**松弛变量**，它创建了一个软边缘，允许一些点落在边缘的错误一侧。下面的图示说明了两个点落在错误一侧的线，以及相应的松弛项（用希腊字母Xi表示）：
- en: '![A picture containing text, clock  Description automatically generated](img/B17290_07_20.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片，时钟  描述自动生成](img/B17290_07_20.png)'
- en: 'Figure 7.20: Points falling on the wrong side of the boundary come with a cost
    penalty'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20：落在边界错误一侧的点会带来成本惩罚
- en: 'A cost value (denoted as *C*) is applied to all points that violate the constraints
    and rather than finding the maximum margin, the algorithm attempts to minimize
    the total cost. We can therefore revise the optimization problem to:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有违反约束的点应用一个成本值（表示为 *C*），而不是寻找最大边缘，算法试图最小化总成本。因此，我们可以将优化问题修改为：
- en: '![](img/B17290_07_008.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_008.png)'
- en: '![](img/B17290_07_009.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_07_009.png)'
- en: If you’re confused by now, don’t worry, you’re not alone. Luckily, SVM packages
    will happily optimize this for you without you having to understand the technical
    details. The important piece to understand is the addition of the cost parameter,
    *C*. Modifying this value will adjust the penalty for points that fall on the
    wrong side of the hyperplane. The greater the cost parameter, the harder the optimization
    will try to achieve 100 percent separation. On the other hand, a lower cost parameter
    will place the emphasis on a wider overall margin. It is important to strike a
    balance between these two in order to create a model that generalizes well to
    future data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在感到困惑，不要担心，你不是一个人。幸运的是，SVM软件包会乐意为你优化，而无需你理解技术细节。重要的是要理解成本参数 *C* 的添加。修改这个值将调整落在超平面错误一侧的点的惩罚。成本参数越大，优化尝试达到100%分离的努力就越大。另一方面，较低的成本参数将强调更宽的整体边缘。为了创建一个对未来数据泛化良好的模型，重要的是在这两者之间取得平衡。
- en: Using kernels for nonlinear spaces
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于非线性空间的核函数
- en: In many real-world datasets, the relationships between variables are nonlinear.
    As we just discovered, an SVM can still be trained on such data through the addition
    of a slack variable, which allows some examples to be misclassified. However,
    this is not the only way to approach the problem of nonlinearity. A key feature
    of SVMs is their ability to map the problem into a higher dimension space using
    a process known as the **kernel trick**. In doing so, a nonlinear relationship
    may suddenly appear to be quite linear.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的数据集中，变量之间的关系是非线性的。正如我们刚刚发现的，通过添加松弛变量，SVM仍然可以在这种数据上训练，这允许一些示例被错误分类。然而，这并不是解决非线性问题的唯一方法。SVM的一个关键特性是它们能够使用称为**核技巧**的过程将问题映射到更高维的空间。在这个过程中，非线性关系可能突然显得非常线性。
- en: 'Though this seems like nonsense, it is actually quite easy to illustrate with
    an example. In the following figure, the scatterplot on the left depicts a nonlinear
    relationship between a weather class (Sunny or Snowy) and two features: Latitude
    and Longitude. The points at the center of the plot are members of the Snowy class,
    while the points at the margins are all Sunny. Such data could have been generated
    from a set of weather reports, some of which were obtained from stations near
    the top of a mountain, while others were obtained from stations around the base
    of the mountain.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这听起来可能有些荒谬，但实际上用例子来说明非常简单。在下面的图中，左边的散点图描绘了天气类别（晴朗或雪）与两个特征：纬度和经度之间的非线性关系。图中中心的点是雪类别成员，而边缘的点都是晴朗的。这样的数据可能来自一组天气预报，其中一些是从山顶附近的站点获得的，而另一些是从山脉底部附近的站点获得的。
- en: '![Diagram  Description automatically generated](img/B17290_07_21.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_07_21.png)'
- en: 'Figure 7.21: The kernel trick can help transform a nonlinear problem into a
    linear one'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21：核技巧可以帮助将非线性问题转化为线性问题
- en: 'On the right side of the figure, after the kernel trick has been applied, we
    look at the data through the lens of a new dimension: Altitude. With the addition
    of this feature, the classes are now perfectly linearly separable. This is possible
    because we have obtained a new perspective on the data. In the left figure, we
    are viewing the mountain from a bird’s-eye view, while on the right, we are viewing
    the mountain from a distance at ground level. Here, the trend is obvious: snowy
    weather is found at higher altitudes.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在图右侧，在应用核技巧之后，我们通过新维度来观察数据：高度。通过添加这个特征，类别现在可以完美地线性分离。这是可能的，因为我们获得了对数据的新视角。在左侧的图中，我们是从鸟瞰的角度观察山脉，而在右侧，我们是从地面水平距离观察山脉。在这里，趋势很明显：在较高的海拔处发现了雪天气候。
- en: In this way, SVMs with nonlinear kernels add additional dimensions to the data
    in order to create separation. Essentially, the kernel trick involves a process
    of constructing new features that express mathematical relationships between measured
    characteristics. For instance, the Altitude feature can be expressed mathematically
    as an interaction between Latitude and Longitude—the closer the point is to the
    center of each of these scales, the greater the Altitude. This allows the SVM
    to learn concepts that were not explicitly measured in the original data.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，具有非线性核的SVM通过向数据添加额外维度来创建分离。本质上，核技巧涉及构建新特征的过程，这些特征表达了测量特征之间的数学关系。例如，高度特征可以数学上表示为纬度和经度之间的相互作用——点越接近每个尺度中心，高度就越大。这使得SVM能够学习原始数据中未明确测量的概念。
- en: 'SVMs with nonlinear kernels are extremely powerful classifiers, although they
    do have some downsides, as shown in the following table:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 具有非线性核的SVM是极其强大的分类器，尽管它们也有一些缺点，如下表所示：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can be used for classification or numeric prediction problems
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于分类或数值预测问题
- en: Not overly influenced by noisy data and not very prone to overfitting
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不太受噪声数据的影响，并且不太容易过拟合
- en: May be easier to use than neural networks, particularly due to the existence
    of several well-supported SVM algorithms
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能比神经网络更容易使用，尤其是由于存在几个得到良好支持的SVM算法
- en: Gained popularity due to their high accuracy and high-profile wins in data mining
    competitions
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其在数据挖掘竞赛中的高准确率和显眼的胜利而受到欢迎
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Finding the best model requires the testing of various combinations of kernels
    and model parameters
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找最佳模型需要测试各种核和模型参数的组合
- en: Can be slow to train, particularly if the input dataset has a large number of
    features or examples
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能较慢，尤其是如果输入数据集具有大量特征或示例
- en: Results in a complex black-box model that is difficult, if not impossible, to
    interpret
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导致产生复杂的黑盒模型，难以解释，甚至可能无法解释
- en: '|'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Kernel functions, in general, are of the following form. The function denoted
    by the Greek letter phi, that is, ![](img/B17290_07_010.png), is a mapping of
    the data into another space. Therefore, the general kernel function applies some
    transformation to the feature vectors *x*[i] and *x*[j], and combines them using
    the **dot product**, which takes two vectors and returns a single number:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数通常具有以下形式。由希腊字母phi表示的函数，即![图片](img/B17290_07_010.png)，是将数据映射到另一个空间的映射。因此，一般的核函数会对特征向量
    *x*[i] 和 *x*[j] 进行一些变换，并使用**点积**将它们结合起来，点积将两个向量作为输入并返回一个单一数值：
- en: '![](img/B17290_07_011.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_07_011.png)'
- en: Using this form, kernel functions have been developed for many different domains.
    A few of the most commonly used kernel functions are listed as follows. Nearly
    all SVM software packages will include these kernels, among many others.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种形式，已经为许多不同的领域开发了核函数。以下列出了最常用的几个核函数。几乎所有的SVM软件包都将包括这些核函数以及其他许多核函数。
- en: 'The **linear kernel** does not transform the data at all. Therefore, it can
    be expressed simply as the dot product of the features:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性核**根本不转换数据。因此，它可以简单地表示为特征的点积：'
- en: '![](img/B17290_07_012.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_07_012.png)'
- en: 'The **polynomial kernel** of degree *d* adds a simple nonlinear transformation
    of the data:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**多项式核**将数据添加一个简单的非线性变换：'
- en: '![](img/B17290_07_013.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_07_013.png)'
- en: 'The **sigmoid kernel** results in an SVM model somewhat analogous to a neural
    network using a sigmoid activation function. The Greek letters kappa and delta
    are used as kernel parameters:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid核**导致SVM模型类似于使用sigmoid激活函数的神经网络。希腊字母kappa和delta用作核参数：'
- en: '![](img/B17290_07_014.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_07_014.png)'
- en: 'The **Gaussian RBF kernel** is similar to an RBF neural network. The RBF kernel
    performs well on many types of data and is thought to be a reasonable starting
    point for many learning tasks:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯径向基函数核**类似于径向基神经网络。径向基函数核在许多类型的数据上表现良好，并被认为是为许多学习任务提供一个合理的起点：'
- en: '![](img/B17290_07_015.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_07_015.png)'
- en: There is no reliable rule for matching a kernel to a particular learning task.
    The fit depends heavily on the concept to be learned, as well as the amount of
    training data and the relationships between the features. Often, a bit of trial
    and error is required by training and evaluating several SVMs on a validation
    dataset. That said, in many cases, the choice of the kernel is arbitrary, as the
    performance may vary only slightly. To see how this works in practice, let’s apply
    our understanding of SVM classification to a real-world problem.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 没有可靠的规则来匹配核函数与特定的学习任务。拟合程度很大程度上取决于要学习的概念、训练数据量以及特征之间的关系。通常，需要在验证数据集上训练和评估几个SVM来尝试和错误地找到合适的核函数。尽管如此，在许多情况下，核函数的选择是任意的，因为性能可能只有细微的差异。为了了解这在实践中是如何工作的，让我们将我们对SVM分类的理解应用到现实世界的问题中。
- en: Example – performing OCR with SVMs
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - 使用SVM进行OCR
- en: Image processing is a difficult task for many types of machine learning algorithms.
    The relationships linking patterns of pixels to higher concepts are extremely
    complex and hard to define. For instance, it’s easy for a human being to recognize
    a face, a cat, or the letter “A,” but defining these patterns in strict rules
    is difficult. Furthermore, image data is often noisy. There can be many slight
    variations in how the image was captured depending on the lighting, orientation,
    and positioning of the subject.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多机器学习算法来说，图像处理是一个困难的任务。将像素模式与更高层次概念联系起来的关系极其复杂且难以定义。例如，人类很容易识别人脸、猫或字母“A”，但在严格的规则中定义这些模式是困难的。此外，图像数据通常很嘈杂。根据光线、方向和主题的位置，图像的捕获可能会有许多细微的变化。
- en: SVMs are well suited to tackle the challenges of image data. Capable of learning
    complex patterns without being overly sensitive to noise, they can recognize visual
    patterns with a high degree of accuracy. Moreover, the key weakness of SVMs—the
    black-box model representation—is less critical for image processing. If an SVM
    can differentiate a cat from a dog, it does not matter much how it is doing so.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: SVM非常适合解决图像数据的挑战。它们能够学习复杂的模式，同时对噪声不太敏感，可以以高精度识别视觉模式。此外，SVM的关键弱点——黑盒模型表示——对于图像处理来说不那么关键。如果一个SVM能够区分猫和狗，那么它如何做到这一点并不重要。
- en: In this section, we will develop a model like those used at the core of the
    **optical character recognition** (**OCR**) software often bundled with desktop
    document scanners or smartphone applications. The purpose of such software is
    to process paper-based documents by converting printed or handwritten text into
    an electronic form to be saved in a database.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一个模型，类似于通常与桌面文档扫描仪或智能手机应用程序捆绑的光学字符识别（**OCR**）软件的核心。此类软件的目的是通过将打印或手写文本转换为电子形式以保存到数据库中来处理基于纸张的文档。
- en: Of course, this is a difficult problem due to the many variants in handwriting
    style and printed fonts. Even so, software users expect perfection, as errors
    or typos can result in embarrassing or costly mistakes in a business environment.
    Let’s see whether our SVM is up to the task.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个困难的问题，因为手写风格和印刷字体有很多变体。尽管如此，软件用户期望完美无缺，因为错误或打字错误可能导致在商业环境中尴尬或昂贵的错误。让我们看看我们的SVM是否能够胜任这项任务。
- en: Step 1 – collecting data
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 – 收集数据
- en: When OCR software first processes a document, it divides the paper into a matrix
    such that each cell in the grid contains a single **glyph**, which is a term referring
    to a letter, symbol, or number. Next, for each cell, the software will attempt
    to match the glyph to a set of all characters it recognizes. Finally, the individual
    characters can be combined into words, which optionally could be spell-checked
    against a dictionary in the document’s language.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当OCR软件首次处理文档时，它会将纸张划分为一个矩阵，使得网格中的每个单元格都包含一个单个**符号**，这是一个指代字母、符号或数字的术语。接下来，对于每个单元格，软件将尝试将符号与它所识别的所有字符集合进行匹配。最后，单个字符可以组合成单词，这些单词可以选择性地与文档语言的词典进行拼写检查。
- en: In this exercise, we’ll assume that we have already developed the algorithm
    to partition the document into rectangular regions, each consisting of a single
    glyph. We will also assume the document contains only alphabetic characters in
    English. Therefore, we’ll simulate a process that involves matching glyphs to
    1 of the 26 letters, A to Z.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们假设我们已经开发出了将文档划分为矩形区域的算法，每个区域只包含一个符号。我们还将假设文档只包含英语字母。因此，我们将模拟一个将符号与26个字母之一（从A到Z）匹配的过程。
- en: To this end, we’ll use a dataset donated to the UCI Machine Learning Repository
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)) by W. Frey and
    D. J. Slate. The dataset contains 20,000 examples of 26 English alphabet capital
    letters as printed using 20 different randomly reshaped and distorted black-and-white
    fonts.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，我们将使用W. Frey和D. J. Slate捐赠给UCI机器学习仓库([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))的数据集。该数据集包含20,000个示例，展示了使用20种不同随机变形和扭曲的黑白字体打印的26个英文字母大写字母。
- en: For more information about this data, refer to *Letter Recognition Using Holland-Style
    Adaptive Classifiers, Slate, DJ and Frey, PW, Machine Learning, 1991, Vol. 6,
    pp. 161-182*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些数据的更多信息，请参阅*使用荷兰风格自适应分类器的字母识别，Slate, DJ和Frey, PW，机器学习，1991，第6卷，第161-182页*。
- en: 'The following figure, published by Frey and Slate, provides an example of some
    of the printed glyphs. Distorted in this way, the letters are challenging for
    a computer to identify, yet are easily recognized by a human being:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 下图由Frey和Slate发布，提供了一些打印符号的示例。以这种方式扭曲后，字母对计算机来说具有挑战性，但对人类来说很容易识别：
- en: '![Text  Description automatically generated](img/B17290_07_22.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成](img/B17290_07_22.png)'
- en: 'Figure 7.22: Examples of glyphs the SVM algorithm will attempt to identify'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22：SVM算法将尝试识别的符号示例
- en: Step 2 – exploring and preparing the data
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 – 探索和准备数据
- en: According to the documentation provided by Frey and Slate, when the glyphs are
    scanned into the computer, they are converted into pixels and 16 statistical attributes
    are recorded.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Frey和Slate提供的文档，当符号被扫描到计算机中时，它们被转换为像素，并记录了16个统计属性。
- en: The attributes measure characteristics such as the horizontal and vertical dimensions
    of the glyph; the proportion of black (versus white) pixels; and the average horizontal
    and vertical position of the pixels. Presumably, differences in the concentration
    of black pixels across various areas of the box should provide a way to differentiate
    between the 26 letters of the alphabet.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 属性衡量诸如符号的水平尺寸和垂直尺寸、黑色（相对于白色）像素的比例以及像素的平均水平和垂直位置等特征。据推测，盒子不同区域黑色像素浓度的差异应该提供一种区分26个字母的方法。
- en: To follow along with this example, download the `letterdata.csv` file from the
    Packt Publishing website and save it to your R working directory.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随这个示例，请从Packt Publishing网站下载`letterdata.csv`文件，并将其保存到您的R工作目录中。
- en: 'Reading the data into R, we confirm that we have received the data with the
    16 features that define each example of the letter class. As expected, it has
    26 levels:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据读入R中，我们确认我们已经收到了定义字母类每个示例的16个特征的完整数据。正如预期的那样，它有26个级别：
- en: '[PRE35]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: SVM learners require all features to be numeric, and moreover, that each feature
    is scaled to a fairly small interval. In this case, every feature is an integer,
    so we do not need to convert any factors into numbers. On the other hand, some
    of the ranges for these integer variables appear wide. This indicates that we
    need to normalize or standardize the data. However, we can skip this step for
    now because the R package that we will use for fitting the SVM model will perform
    rescaling automatically.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: SVM学习器需要所有特征都是数值的，并且每个特征都缩放到一个相当小的区间。在这种情况下，每个特征都是整数，因此我们不需要将任何因素转换为数字。另一方面，这些整数变量的范围似乎很宽。这表明我们需要对数据进行归一化或标准化。然而，我们现在可以跳过这一步，因为我们将要使用的R包将自动执行缩放。
- en: 'Given that there is no data preparation left to perform, we can move directly
    to the training and testing phases of the machine learning process. In previous
    analyses, we randomly divided the data between the training and testing sets.
    Although we could do so here, Frey and Slate have already randomized the data
    and therefore suggest using the first 16,000 records (80 percent) for building
    the model and the next 4,000 records (20 percent) for testing. Following their
    advice, we can create training and testing data frames as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有剩余的数据准备要执行，我们可以直接进入机器学习过程的训练和测试阶段。在先前的分析中，我们随机地将数据分配到训练集和测试集中。虽然我们也可以这样做，但Frey和Slate已经随机化了数据，因此建议使用前16,000条记录（80%）来构建模型，以及接下来的4,000条记录（20%）用于测试。遵循他们的建议，我们可以创建以下训练和测试数据框：
- en: '[PRE37]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With our data ready to go, let’s start building our classifier.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准备工作已经就绪，现在让我们开始构建我们的分类器。
- en: Step 3 – training a model on the data
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: When it comes to fitting an SVM model in R, there are several outstanding packages
    to choose from. The `e1071` package from the Department of Statistics at the **Vienna
    University of Technology** (**TU Wien**) provides an R interface to the award-winning
    LIBSVM library, a widely used open-source SVM program written in C++. If you are
    already familiar with LIBSVM, you may want to start here.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到在R中拟合SVM模型时，有多个出色的包可供选择。维也纳科技大学（**TU Wien**）统计学系的`e1071`包提供了一个R接口到获奖的LIBSVM库，这是一个广泛使用的开源SVM程序，用C++编写。如果您已经熟悉LIBSVM，您可能想从这里开始。
- en: For more information on LIBSVM, refer to the authors’ website at [http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LIBSVM的更多信息，请参考作者网站[http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)。
- en: Similarly, if you’re already invested in the SVMlight algorithm, the `klaR`
    package from the Department of Statistics at the **Dortmund University of Technology**
    (**TU Dortmund**) provides functions for working with this SVM implementation
    directly from R.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果您已经投资于SVMlight算法，杜伊斯堡-埃森科技大学（**TU Dortmund**）统计学系的`klaR`包提供了直接从R使用此SVM实现的功能。
- en: For information on SVMlight, see [https://www.cs.cornell.edu/people/tj/svm_light/](https://www.cs.cornell.edu/people/tj/svm_light/).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SVMlight的信息，请参阅[https://www.cs.cornell.edu/people/tj/svm_light/](https://www.cs.cornell.edu/people/tj/svm_light/)。
- en: Finally, if you are starting from scratch, it is perhaps best to begin with
    the SVM functions in the `kernlab` package. An interesting advantage of this package
    is that it was developed natively in R rather than C or C++, which allows it to
    be easily customized; none of the internals are hidden behind the scenes. Perhaps
    even more importantly, unlike the other options, `kernlab` can be used with the
    `caret` package, which allows SVM models to be trained and evaluated using a variety
    of automated methods (covered in *Chapter 14*, *Building Better Learners*).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您是从零开始，那么最好从`kernlab`包中的SVM函数开始。这个包的一个有趣的优势是它是在R中本地开发的，而不是在C或C++中，这使得它可以很容易地进行定制；内部没有任何东西隐藏在幕后。也许更重要的是，与其他选项不同，`kernlab`可以与`caret`包一起使用，这使得可以使用各种自动化方法（在第14章*构建更好的学习者*中介绍）来训练和评估SVM模型。
- en: For a more thorough introduction to `kernlab`, please refer to the authors’
    paper at [http://www.jstatsoft.org/v11/i09/](http://www.jstatsoft.org/v11/i09/).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 要更全面地了解`kernlab`，请参阅作者在[http://www.jstatsoft.org/v11/i09/](http://www.jstatsoft.org/v11/i09/)上的论文。
- en: 'The syntax for training SVM classifiers with `kernlab` is as follows. If you
    do happen to be using one of the other packages, the commands are largely similar.
    By default, the `ksvm()` function uses the Gaussian RBF kernel, but a number of
    other options are provided:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kernlab`训练SVM分类器的语法如下。如果您确实在使用其他包，命令在很大程度上是相似的。默认情况下，`ksvm()`函数使用高斯RBF核，但还提供了许多其他选项：
- en: '![Text, letter  Description automatically generated](img/B17290_07_23.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![文本，字母，描述自动生成](img/B17290_07_23.png)'
- en: 'Figure 7.23: SVM syntax'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23：SVM语法
- en: 'To provide a baseline measure of SVM performance, let’s begin by training a
    simple linear SVM classifier. If you haven’t already, install the `kernlab` package
    to your library using the `install.packages("kernlab")` command. Then, we can
    call the `ksvm()` function on the training data and specify the linear (that is,
    “vanilla”) kernel using the `vanilladot` option, as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个SVM性能的基线度量，让我们首先训练一个简单的线性SVM分类器。如果您还没有安装，请使用`install.packages("kernlab")`命令将`kernlab`包安装到您的库中。然后，我们可以在训练数据上调用`ksvm()`函数，并使用`vanilladot`选项指定线性（即“纯”）核，如下所示：
- en: '[PRE38]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Depending on the performance of your computer, this operation may take some
    time to complete. When it finishes, type the name of the stored model to see some
    basic information about the training parameters and the fit of the model:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您计算机的性能，此操作可能需要一些时间才能完成。完成后，输入存储模型的名称，以查看有关训练参数和模型拟合的一些基本信息：
- en: '[PRE39]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This information tells us very little about how well the model will perform
    in the real world. We’ll need to examine its performance on the testing dataset
    to know whether it generalizes well to unseen data.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息对我们了解模型在现实世界中的表现帮助甚微。我们需要检查它在测试数据集中的表现，才能知道它是否很好地泛化到未见过的数据。
- en: Step 4 – evaluating model performance
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: 'The `predict()` function allows us to use the letter classification model to
    make predictions on the testing dataset:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`函数允许我们使用字母分类模型对测试数据集进行预测：'
- en: '[PRE41]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Because we didn’t specify the `type` parameter, the default `type = "response"`
    was used. This returns a vector containing a predicted letter for each row of
    values in the testing data. Using the `head()` function, we can see that the first
    six predicted letters were `U`, `N`, `V`, `X`, `N`, and `H`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有指定`type`参数，因此使用了默认的`type = "response"`。这返回一个包含测试数据中每行值预测字母的向量。使用`head()`函数，我们可以看到前六个预测字母分别是`U`、`N`、`V`、`X`、`N`和`H`：
- en: '[PRE42]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To examine how well our classifier performed, we need to compare the predicted
    letter with the true letter in the testing dataset. We’ll use the `table()` function
    for this purpose (only a portion of the full table is shown here):'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的分类器表现如何，我们需要将预测字母与测试数据集中的真实字母进行比较。我们将使用`table()`函数来完成此操作（此处仅显示部分完整表格）：
- en: '[PRE44]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The diagonal values of `144`, `121`, `120`, `156`, and `127` indicate the total
    number of records where the predicted letter matches the true value. Similarly,
    the number of mistakes is also listed. For example, the value of `5` in row `B`
    and column `D` indicates that there were 5 cases where the letter `D` was misidentified
    as a `B`.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线上的`144`、`121`、`120`、`156`和`127`值表示预测字母与真实值匹配的记录总数。同样，错误数量也被列出。例如，行`B`和列`D`中的`5`表示有5个案例，其中字母`D`被错误地识别为`B`。
- en: Looking at each type of mistake individually may reveal some interesting patterns
    about the specific types of letters the model has trouble with, but this is time-consuming.
    We can simplify our evaluation by instead calculating the overall accuracy. This
    considers only whether the prediction was correct or incorrect and ignores the
    type of error.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 单独查看每种类型的错误可能会揭示模型在特定字母类型上遇到困难的一些有趣模式，但这很耗时。我们可以通过计算整体准确率来简化评估。这仅考虑预测是否正确或错误，而忽略错误类型。
- en: 'The following command returns a vector of `TRUE` or `FALSE` values indicating
    whether the model’s predicted letter agrees with (that is, matches) the actual
    letter in the test dataset:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令返回一个包含`TRUE`或`FALSE`值的向量，指示模型预测的字母是否与测试数据集中的实际字母相符（即匹配）：
- en: '[PRE46]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Using the `table()` function, we see that the classifier correctly identified
    the letter in 3,357 out of the 4,000 test records:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`table()`函数，我们看到分类器在4000个测试记录中的3357个中正确识别了字母：
- en: '[PRE47]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In percentage terms, the accuracy is about 84 percent:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 从百分比的角度来看，准确率大约是84%：
- en: '[PRE49]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note that when Frey and Slate published the dataset in 1991, they reported a
    recognition accuracy of about 80 percent. Using just a few lines of R code, we
    were able to surpass their result, although we also have the benefit of decades
    of additional machine learning research. With that in mind, it is likely that
    we can do even better.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当Frey和Slate在1991年发布数据集时，他们报告的识别准确率约为80%。仅用几行R代码，我们就能够超越他们的结果，尽管我们也受益于数十年的额外机器学习研究。考虑到这一点，我们很可能会做得更好。
- en: Step 5 – improving model performance
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 - 提高模型性能
- en: Let’s take a moment to contextualize the performance of the SVM model we trained
    to identify letters of the alphabet from image data. With 1 line of R code, the
    model was able to achieve an accuracy of nearly 84 percent, which slightly surpassed
    the benchmark percent published by academic researchers in 1991\. Although an
    accuracy of 84 percent is not nearly high enough to be useful for OCR software,
    the fact that a relatively simple model can reach this level is a remarkable accomplishment
    in itself. Keep in mind that the probability the model’s prediction would match
    the actual value by dumb luck alone is quite small at under four percent. This
    implies that our model performs over 20 times better than random chance. As remarkable
    as this is, perhaps by adjusting the SVM function parameters to train a slightly
    more complex model, we can also find that the model is useful in the real world.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间来了解一下我们训练的SVM模型在从图像数据中识别字母时的性能。通过一行R代码，该模型能够达到近84%的准确率，略高于1991年学术研究人员发布的基准百分比。尽管84%的准确率远远不足以用于OCR软件，但相对简单的模型能够达到这个水平本身就是一项了不起的成就。请记住，模型预测与实际值仅靠运气匹配的概率相当小，不到四%。这表明我们的模型的表现比随机机会高出20倍以上。尽管如此出色，也许通过调整SVM函数参数来训练一个稍微复杂一些的模型，我们还可以发现该模型在现实世界中是有用的。
- en: To calculate the probability of the SVM model’s predictions matching the actual
    values by chance alone, apply the joint probability rule for independent events
    covered in *Chapter 4*, *Probabilistic Learning – Classification Using Naive Bayes*.
    Because there are 26 letters, each appearing at approximately the same rate in
    the test set, the chance that any one letter is predicted correctly is *(1 / 26)
    * (1 / 26)*. Since there are 26 different letters, the total probability of agreement
    is *26 * (1 / 26) * (1 / 26) = 0.0384*, or 3.84 percent.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算SVM模型预测与实际值偶然匹配的概率，应用*第4章*中涵盖的独立事件的联合概率规则，即*朴素贝叶斯分类的概率学习*。因为测试集中有26个字母，每个字母出现的频率大约相同，所以任何单个字母被正确预测的概率是*(1
    / 26) * (1 / 26)*。由于有26个不同的字母，总的一致性概率是*26 * (1 / 26) * (1 / 26) = 0.0384*，或3.84%。
- en: Changing the SVM kernel function
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改变SVM核函数
- en: Our previous SVM model used the simple linear kernel function. By using a more
    complex kernel function, we can map the data into a higher dimensional space,
    and potentially obtain a better model fit.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的SVM模型使用了简单的线性核函数。通过使用更复杂的核函数，我们可以将数据映射到更高维的空间，并可能获得更好的模型拟合。
- en: It can be challenging, however, to choose from the many different kernel functions.
    A popular convention is to begin with the Gaussian RBF kernel, which has been
    shown to perform well for many types of data.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从众多不同的核函数中选择可能具有挑战性。一个流行的惯例是从高斯RBF核开始，它已被证明对许多类型的数据表现良好。
- en: 'We can train an RBF-based SVM using the `ksvm()` function as shown here. Note
    that, much like other methods used previously, we need to set the random seed
    to ensure the results are reproducible:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`ksvm()`函数训练基于RBF的SVM，如下所示。请注意，与之前使用的方法类似，我们需要设置随机种子以确保结果可重复：
- en: '[PRE51]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we make predictions as before:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们像之前一样进行预测：
- en: '[PRE52]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we’ll compare the accuracy to our linear SVM:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将比较其准确性与我们的线性SVM：
- en: '[PRE53]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Simply by changing the kernel function, we were able to increase the accuracy
    of our character recognition model from 84 percent to 93 percent.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过改变核函数，我们就能够将我们的字符识别模型的准确率从84%提高到93%。
- en: Identifying the best SVM cost parameter
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定最佳的SVM成本参数
- en: If this level of performance is still unsatisfactory for the OCR program, it
    is certainly possible to test additional kernels. However, another fruitful approach
    is to vary the cost parameter, which modifies the width of the SVM decision boundary.
    This governs the model’s balance between overfitting and underfitting the training
    data—the larger the cost value, the harder the learner will try to perfectly classify
    every training instance, as there is a higher penalty for each mistake. On the
    one hand, a high cost can lead the learner to overfit the training data. On the
    other hand, a cost parameter set too small can cause the learner to miss important,
    subtle patterns in the training data and underfit the true pattern.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no rule of thumb for knowing the ideal value beforehand, so instead,
    we will examine how the model performs for various values of `C`, the cost parameter.
    Rather than repeating the training and evaluation process repeatedly, we can use
    the `sapply()` function to apply a custom function to a vector of potential cost
    values. We begin by using the `seq()` function to generate this vector as a sequence
    counting from 5 to 40 by 5\. Then, as shown in the following code, the custom
    function trains the model as before, each time using the cost value and making
    predictions on the test dataset. Each model’s accuracy is computed as the number
    of predictions that match the actual values divided by the total number of predictions.
    The result is visualized using the `plot()` function. Note that depending on the
    capabilities of your computer, this may take a few minutes to complete:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '![Shape  Description automatically generated](img/B17290_07_24.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.24: Mapping accuracy against the SVM cost for the RBF kernel'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in the visualization, with an accuracy of 93 percent, the default
    SVM cost parameter of `C = 1` resulted in by far the least accurate model among
    the 9 models evaluated. Instead, setting `C` to a value of 10 or higher results
    in an accuracy of around 97 percent, which is quite an improvement in performance!
    Perhaps this is close enough to perfect for the model to be deployed in a real-world
    environment, though it may still be worth experimenting further with various kernels
    to see if it is possible to get even closer to 100 percent accuracy. Each additional
    improvement in accuracy will result in fewer mistakes for the OCR software and
    a better overall experience for the end user.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examined two machine learning methods that offer a great
    deal of potential but are often overlooked due to their complexity. Hopefully,
    you now see that this reputation is at least somewhat undeserved. The basic concepts
    that drive ANNs and SVMs are not too difficult to understand.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, because ANNs and SVMs have been around for many decades,
    each of them has numerous variations. This chapter just scratches the surface
    of what is possible with these methods. By utilizing the terminology that you
    learned here, you should be capable of picking up the nuances that distinguish
    the many advancements that are being developed every day, including the ever-growing
    field of deep learning. We will revisit deep learning in *Chapter 15*, *Making
    Use of Big Data*, to see how it can solve some of machine learning’s most challenging
    problems.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于人工神经网络（ANNs）和支持向量机（SVMs）已经存在了几十年，它们各自都有许多变体。本章只是对这些方法可能实现的内容进行了初步探讨。通过利用在这里学到的术语，你应该能够捕捉到每天正在开发的许多进步的细微差别，包括不断发展的深度学习领域。我们将在第
    15 章“利用大数据”中重新探讨深度学习，看看它如何解决机器学习中最具挑战性的问题。
- en: In the past several chapters, we have learned about many different types of
    predictive models, from those based on simple heuristics like nearest neighbors
    to sophisticated black-box models and many others. In the next chapter, we will
    begin to consider methods for another type of learning task. These unsupervised
    learning techniques will bring to light fascinating patterns within the data as
    they assist us with finding needles in haystacks.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们学习了多种不同类型的预测模型，从基于简单启发式算法如最近邻算法的模型到复杂的黑盒模型以及其他许多模型。在下一章中，我们将开始考虑另一种学习任务的方法。这些无监督学习技术将在帮助我们找到“大海捞针”的过程中，揭示数据中的迷人模式。
- en: Join our book’s Discord space
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并在以下地点与超过 4000 人一起学习：
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/r](https://packt.link/r)'
- en: '![](img/r.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/r.jpg)'
