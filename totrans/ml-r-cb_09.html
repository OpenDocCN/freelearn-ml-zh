<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;9.&#xA0;Clustering"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09" class="calibre1"/>Chapter 9. Clustering</h1></div></div></div><p class="calibre7">In this chapter, we will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Clustering data with hierarchical clustering</li><li class="listitem">Cutting a tree into clusters</li><li class="listitem">Clustering data with the k-means method</li><li class="listitem">Drawing a bivariate cluster plot</li><li class="listitem">Comparing clustering methods </li><li class="listitem">Extracting silhouette information from clustering</li><li class="listitem">Obtaining optimum clusters for k-means</li><li class="listitem">Clustering data with the density-based method </li><li class="listitem">Clustering data with the model-based method</li><li class="listitem">Visualizing a dissimilarity matrix</li><li class="listitem">Validating clusters externally</li></ul></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Clustering">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch09lvl1sec100" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">Clustering is <a id="id720" class="calibre1"/>a technique used to group similar objects (close in terms of distance) together in the same group (cluster). Unlike supervised learning methods (for example, classification and regression) covered in the previous chapters, a clustering analysis does not use any label information, but simply uses the similarity between data features to group them into clusters.</p><p class="calibre7">Clustering can be widely adapted in the analysis of businesses. For example, a marketing department can use clustering to segment customers by personal attributes. As a result of this, different marketing campaigns targeting various types of customers can be designed.</p><p class="calibre7">The four most<a id="id721" class="calibre1"/> common types of clustering methods <a id="id722" class="calibre1"/>are<a id="id723" class="calibre1"/> hierarchical <a id="id724" class="calibre1"/>clustering, k-means clustering, model-based clustering, and density-based clustering:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Hierarchical clustering</strong></span>: It creates a hierarchy of clusters, and presents the hierarchy in a <a id="id725" class="calibre1"/>dendrogram. This method does not require the number of clusters to be specified at the beginning.</li><li class="listitem"><span class="strong"><strong class="calibre2">k-means clustering</strong></span>: It is also referred to as flat clustering. Unlike hierarchical <a id="id726" class="calibre1"/>clustering, it does not create a hierarchy of clusters, and it requires the number of clusters as an input. However, its performance is faster than hierarchical clustering.</li><li class="listitem"><span class="strong"><strong class="calibre2">Model-based clustering</strong></span>: Both hierarchical clustering and k-means clustering <a id="id727" class="calibre1"/>use a heuristic approach to construct clusters, and do not rely on a formal model. Model-based clustering assumes a data model and applies an EM algorithm to find the most likely model components and the number of clusters.</li><li class="listitem"><span class="strong"><strong class="calibre2">Density-based clustering</strong></span>: It constructs clusters in regard to the density measurement. Clusters in this method have a higher density than the remainder <a id="id728" class="calibre1"/>of the dataset.</li></ul></div><p class="calibre7">In the following recipes, we will discuss how to use these four clustering techniques to cluster data. We discuss how to validate clusters internally, using within clusters the sum of squares, average silhouette width, and externally, with ground truth.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Clustering data with hierarchical clustering"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec101" class="calibre1"/>Clustering data with hierarchical clustering</h1></div></div></div><p class="calibre7">Hierarchical clustering adopts either an agglomerative or divisive method to build a hierarchy<a id="id729" class="calibre1"/> of clusters. Regardless of which approach is adopted, both first use a distance similarity measure to combine or split clusters. The recursive process continues until there is only one cluster left or you cannot split more clusters. Eventually, we can use a dendrogram to represent the hierarchy of clusters. In this recipe, we will demonstrate how to cluster customers with hierarchical clustering.</p></div>

<div class="book" title="Clustering data with hierarchical clustering">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec345" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will perform hierarchical clustering on customer data, which involves segmenting customers into different groups. You can download the data from this Github page: <a class="calibre1" href="https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9">https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9</a>.</p></div></div>

<div class="book" title="Clustering data with hierarchical clustering">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec346" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to cluster customer data into a hierarchy of clusters:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you need to load data from <code class="email">customer.csv</code> and save it into <code class="email">customer</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; customer= read.csv('customer.csv', header=TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; head(customer)</strong></span>
<span class="strong"><strong class="calibre2">  ID Visit.Time Average.Expense Sex Age</strong></span>
<span class="strong"><strong class="calibre2">1  1          3             5.7   0  10</strong></span>
<span class="strong"><strong class="calibre2">2  2          5            14.5   0  27</strong></span>
<span class="strong"><strong class="calibre2">3  3         16            33.5   0  32</strong></span>
<span class="strong"><strong class="calibre2">4  4          5            15.9   0  30</strong></span>
<span class="strong"><strong class="calibre2">5  5         16            24.9   0  23</strong></span>
<span class="strong"><strong class="calibre2">6  6          3            12.0   0  15</strong></span>
</pre></div></li><li class="listitem" value="2">You<a id="id730" class="calibre1"/> can then examine the dataset structure:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; str(customer)</strong></span>
<span class="strong"><strong class="calibre2">'data.frame':  60 obs. of  5 variables:</strong></span>
<span class="strong"><strong class="calibre2"> $ ID             : int  1 2 3 4 5 6 7 8 9 10 ...</strong></span>
<span class="strong"><strong class="calibre2"> $ Visit.Time     : int  3 5 16 5 16 3 12 14 6 3 ...</strong></span>
<span class="strong"><strong class="calibre2"> $ Average.Expense: num  5.7 14.5 33.5 15.9 24.9 12 28.5 18.8 23.8 5.3 ...</strong></span>
<span class="strong"><strong class="calibre2"> $ Sex            : int  0 0 0 0 0 0 0 0 0 0 ...</strong></span>
<span class="strong"><strong class="calibre2"> $ Age            : int  10 27 32 30 23 15 33 27 16 11 ...</strong></span>
</pre></div></li><li class="listitem" value="3">Next, you should normalize the customer data into the same scale:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; customer = scale(customer[,-1])</strong></span>
</pre></div></li><li class="listitem" value="4">You can then use agglomerative hierarchical clustering to cluster the customer data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; hc = hclust(dist(customer, method="euclidean"), method="ward.D2")</strong></span>
<span class="strong"><strong class="calibre2">&gt; hc</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">hclust(d = dist(customer, method = "euclidean"), method = "ward.D2")</strong></span>

<span class="strong"><strong class="calibre2">Cluster method   : ward.D2 </strong></span>
<span class="strong"><strong class="calibre2">Distance         : euclidean </strong></span>
<span class="strong"><strong class="calibre2">Number of objects: 60</strong></span>
</pre></div></li><li class="listitem" value="5">Lastly, you <a id="id731" class="calibre1"/>can use the <code class="email">plot</code> function to plot the dendrogram:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(hc, hang = -0.01, cex = 0.7)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00150.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The dendrogram of hierarchical clustering using "ward.D2"</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">Additionally, you can use the single method to perform hierarchical clustering and see how the generated dendrogram differs from the previous:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; hc2 = hclust(dist(customer), method="single")</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(hc2, hang = -0.01, cex = 0.7)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00151.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The dendrogram of hierarchical clustering using "single"</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Clustering data with hierarchical clustering">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec347" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Hierarchical<a id="id732" class="calibre1"/> clustering is a clustering technique that tries to build a hierarchy of clusters iteratively. Generally, there are two approaches to build hierarchical clusters:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Agglomerative hierarchical clustering</strong></span>: This is a bottom-up approach. Each observation<a id="id733" class="calibre1"/> starts <a id="id734" class="calibre1"/>in its own cluster. We can then compute the similarity (or the distance) between each cluster and then merge the two most similar ones at each iteration until there is only one cluster left.</li><li class="listitem"><span class="strong"><strong class="calibre2">Divisive hierarchical clustering</strong></span>: This is a top-down approach. All observations <a id="id735" class="calibre1"/>start in <a id="id736" class="calibre1"/>one cluster, and then we split the cluster into the two least dissimilar clusters recursively until there is one cluster for each observation:<div class="mediaobject"><img src="../images/00152.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">An illustration of hierarchical clustering</p></div></div><p class="calibre13"> </p></li></ul></div><p class="calibre7">Before <a id="id737" class="calibre1"/>performing hierarchical clustering, we need to determine how similar the two clusters are. Here, we list some common distance functions used for the measurement of similarity:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Single linkage</strong></span>: This <a id="id738" class="calibre1"/>refers to the shortest distance between two points in each cluster:<div class="mediaobject"><img src="../images/00153.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre13"> </p></li><li class="listitem"><span class="strong"><strong class="calibre2">Complete linkage</strong></span>: This<a id="id739" class="calibre1"/> refers to the longest distance between two points in each cluster:<div class="mediaobject"><img src="../images/00154.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre13"> </p></li><li class="listitem"><span class="strong"><strong class="calibre2">Average linkage</strong></span>: This<a id="id740" class="calibre1"/> refer to the average distance between two points in each cluster (where <span class="strong"><img src="../images/00155.jpeg" alt="How it works..." class="calibre24"/></span> is the size of cluster <span class="strong"><img src="../images/00156.jpeg" alt="How it works..." class="calibre24"/></span> and <span class="strong"><img src="../images/00157.jpeg" alt="How it works..." class="calibre24"/></span> is the size of cluster <span class="strong"><img src="../images/00158.jpeg" alt="How it works..." class="calibre24"/></span>):
<div class="mediaobject"><img src="../images/00159.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre13"> </p></li><li class="listitem"><span class="strong"><strong class="calibre2">Ward method</strong></span>: This<a id="id741" class="calibre1"/> refers to the sum of the squared distance from each point to the mean of the merged clusters (where <span class="strong"><img src="../images/00160.jpeg" alt="How it works..." class="calibre24"/></span> is the mean vector of <span class="strong"><img src="../images/00161.jpeg" alt="How it works..." class="calibre24"/></span>):
<div class="mediaobject"><img src="../images/00162.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre13"> </p></li></ul></div><p class="calibre7">In this recipe, we perform hierarchical clustering on customer data. First, we load the data from <code class="email">customer.csv</code>, and then load it into the customer data frame. Within the data, we find five<a id="id742" class="calibre1"/> variables of customer account information, which are ID, number of visits, average expense, sex, and age. As the scale of each variable varies, we use the scale function to normalize the scale.</p><p class="calibre7">After the scales of all the attributes are normalized, we perform hierarchical clustering using the <code class="email">hclust</code> function. We use the Euclidean distance as distance metrics, and use Ward's minimum variance method to perform agglomerative clustering.</p><p class="calibre7">Finally, we use the <code class="email">plot</code> function to plot the dendrogram of hierarchical clusters. We specify <code class="email">hang</code> to display labels at the bottom of the dendrogram, and use <code class="email">cex</code> to shrink the label to 70 percent of the normal size. In order to compare the differences using the <code class="email">ward.D2</code> and <code class="email">single</code> methods to generate a hierarchy of clusters, we draw another dendrogram using <code class="email">single</code> in the preceding figure (step 6).</p></div></div>

<div class="book" title="Clustering data with hierarchical clustering">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec348" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">You can choose a different distance measure and method while performing hierarchical clustering. For more details, you can refer to the documents for <code class="email">dist</code> and <code class="email">hclust</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ? dist</strong></span>
<span class="strong"><strong class="calibre2">&gt; ? hclust</strong></span>
</pre></div><p class="calibre7">In this recipe, we use <code class="email">hclust</code> to perform agglomerative hierarchical clustering; if you would like to perform divisive hierarchical clustering, you can use the <code class="email">diana</code> function:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can use <code class="email">diana</code> to perform divisive hierarchical clustering:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dv = diana(customer, metric = "euclidean")</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you can use <code class="email">summary</code> to obtain the summary information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(dv)</strong></span>
</pre></div></li><li class="listitem" value="3">Lastly, you can plot a dendrogram and banner with the <code class="email">plot</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(dv)</strong></span>
</pre></div></li></ol><div class="calibre14"/></div><p class="calibre7">If you <a id="id743" class="calibre1"/>are interested in drawing a horizontal dendrogram, you can use the <code class="email">dendextend</code> package. Use the following procedure to generate a horizontal dendrogram:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">dendextend</code> and <code class="email">magrittr</code> packages (if your R version is 3.1 and above, you do not have to install and load the <code class="email">magrittr</code> package):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("dendextend")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(dendextend)</strong></span>
<span class="strong"><strong class="calibre2">&gt; install.packages("margrittr")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(magrittr)</strong></span>
</pre></div></li><li class="listitem" value="2">Set up the dendrogram:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dend = customer %&gt;% dist %&gt;% hclust %&gt;% as.dendrogram</strong></span>
</pre></div></li><li class="listitem" value="3">Finally, plot the horizontal dendrogram:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">dend %&gt;% plot(horiz=TRUE, main = "Horizontal Dendrogram")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00163.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">The horizontal dendrogram</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Cutting trees into clusters"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec102" class="calibre1"/>Cutting trees into clusters</h1></div></div></div><p class="calibre7">In a dendrogram, we can see the hierarchy of clusters, but we have not grouped data into different clusters yet. However, we can determine how many clusters are within the dendrogram and cut the dendrogram at a certain tree height to separate the data into different <a id="id744" class="calibre1"/>groups. In this recipe, we demonstrate how to use the <code class="email">cutree</code> function to separate the data into a given number of clusters.</p></div>

<div class="book" title="Cutting trees into clusters">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec349" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In order to perform the <code class="email">cutree</code> function, you need to have the previous recipe completed by generating the hclust object, <code class="email">hc</code>.</p></div></div>

<div class="book" title="Cutting trees into clusters">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec350" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to cut the hierarchy of clusters into a given number of clusters:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, categorize the data into four groups:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; fit = cutree(hc, k = 4)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then examine the cluster labels for the data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; fit</strong></span>
<span class="strong"><strong class="calibre2"> [1] 1 1 2 1 2 1 2 2 1 1 1 2 2 1 1 1 2 1 2 3 4 3 4 3 3 4 4 3 4</strong></span>
<span class="strong"><strong class="calibre2">[30] 4 4 3 3 3 4 4 3 4 4 4 4 4 4 4 3 3 4 4 4 3 4 3 3 4 4 4 3 4</strong></span>
<span class="strong"><strong class="calibre2">[59] 4 3</strong></span>
</pre></div></li><li class="listitem" value="3">Count the number of data within each cluster:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table(fit)</strong></span>
<span class="strong"><strong class="calibre2">fit</strong></span>
<span class="strong"><strong class="calibre2"> 1  2  3  4 </strong></span>
<span class="strong"><strong class="calibre2">11  8 16 25 </strong></span>
</pre></div></li><li class="listitem" value="4">Finally, you can visualize how data is clustered with the red rectangle border:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(hc)</strong></span>
<span class="strong"><strong class="calibre2">&gt; rect.hclust(hc, k = 4 , border="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00164.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Using the red rectangle border to distinguish different clusters within the dendrogram</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Cutting trees into clusters">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec351" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">We <a id="id745" class="calibre1"/>can determine the number of clusters from the dendrogram in the preceding figure. In this recipe, we determine there should be four clusters within the tree. Therefore, we specify the number of clusters as <code class="email">4</code> in the <code class="email">cutree</code> function. Besides using the number of clusters to cut the tree, you can specify the <code class="email">height</code> as the cut tree parameter.</p><p class="calibre7">Next, we can output the cluster labels of the data and use the <code class="email">table</code> function to count the number of data within each cluster. From the counting table, we find that most of the data is in cluster 4. Lastly, we can draw red rectangles around the clusters to show how data is categorized into the four clusters with the <code class="email">rect.hclust</code> function.</p></div></div>

<div class="book" title="Cutting trees into clusters">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec352" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Besides drawing rectangles around all hierarchical clusters, you can place a red rectangle around a certain cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; rect.hclust(hc, k = 4 , which =2, border="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00165.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">Drawing a red rectangle around a certain cluster.</p></div></div><p class="calibre10"> </p><p class="calibre7">Also, you <a id="id746" class="calibre1"/>can color clusters in different colors with a red rectangle around the clusters by using the <code class="email">dendextend</code> package. You have to complete the instructions outlined in the <span class="strong"><em class="calibre8">There's more</em></span> section of the previous recipe and perform the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Color the branch according to the cluster it belongs to:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dend %&gt;% color_branches(k=4) %&gt;% plot(horiz=TRUE, main = "Horizontal Dendrogram")</strong></span>
</pre></div></li><li class="listitem" value="2">You can then add a red rectangle around the clusters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dend %&gt;% rect.dendrogram(k=4,horiz=TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00166.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">Drawing red rectangles around clusters within a horizontal dendrogram</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Finally, you<a id="id747" class="calibre1"/> can add a line to show the tree cutting location:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; abline(v = heights_per_k.dendrogram(dend)["4"] + .1, lwd = 2, lty = 2, col = "blue")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00167.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">Drawing a cutting line within a horizontal dendrogram</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Clustering data with the k-means method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec103" class="calibre1"/>Clustering data with the k-means method</h1></div></div></div><p class="calibre7">k-means clustering is a flat clustering technique, which produces only one partition with <span class="strong"><em class="calibre8">k</em></span> clusters. Unlike hierarchical clustering, which does not require a user to determine the number<a id="id748" class="calibre1"/> of clusters at the beginning, the k-means method requires this to be determined first. However, k-means clustering is much faster than hierarchical clustering as the construction of a hierarchical tree is very time consuming. In this recipe, we will demonstrate how to perform k-means clustering on the customer dataset.</p></div>

<div class="book" title="Clustering data with the k-means method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec353" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the customer dataset as the input data source to perform k-means clustering.</p></div></div>

<div class="book" title="Clustering data with the k-means method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec354" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the<a id="id749" class="calibre1"/> following steps to cluster the <code class="email">customer</code> dataset with the k-means method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can use <code class="email">kmeans</code> to cluster the customer data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(22)</strong></span>
<span class="strong"><strong class="calibre2">&gt; fit = kmeans(customer, 4)</strong></span>
<span class="strong"><strong class="calibre2">&gt; fit</strong></span>
<span class="strong"><strong class="calibre2">K-means clustering with 4 clusters of sizes 8, 11, 16, 25</strong></span>

<span class="strong"><strong class="calibre2">Cluster means:</strong></span>
<span class="strong"><strong class="calibre2">  Visit.Time Average.Expense        Sex        Age</strong></span>
<span class="strong"><strong class="calibre2">1  1.3302016       1.0155226 -1.4566845  0.5591307</strong></span>
<span class="strong"><strong class="calibre2">2 -0.7771737      -0.5178412 -1.4566845 -0.4774599</strong></span>
<span class="strong"><strong class="calibre2">3  0.8571173       0.9887331  0.6750489  1.0505015</strong></span>
<span class="strong"><strong class="calibre2">4 -0.6322632      -0.7299063  0.6750489 -0.6411604</strong></span>

<span class="strong"><strong class="calibre2">Clustering vector:</strong></span>
<span class="strong"><strong class="calibre2"> [1] 2 2 1 2 1 2 1 1 2 2 2 1 1 2 2 2 1 2 1 3 4 3 4 3 3 4 4 3</strong></span>
<span class="strong"><strong class="calibre2">[29] 4 4 4 3 3 3 4 4 3 4 4 4 4 4 4 4 3 3 4 4 4 3 4 3 3 4 4 4</strong></span>
<span class="strong"><strong class="calibre2">[57] 3 4 4 3</strong></span>

<span class="strong"><strong class="calibre2">Within cluster sum of squares by cluster:</strong></span>
<span class="strong"><strong class="calibre2">[1]  5.90040 11.97454 22.58236 20.89159</strong></span>
<span class="strong"><strong class="calibre2"> (between_SS / total_SS =  74.0 %)</strong></span>

<span class="strong"><strong class="calibre2">Available components:</strong></span>

<span class="strong"><strong class="calibre2">[1] "cluster"      "centers"      "totss"       </strong></span>
<span class="strong"><strong class="calibre2">[4] "withinss"     "tot.withinss" "betweenss"   </strong></span>
<span class="strong"><strong class="calibre2">[7] "size"         "iter"         "ifault</strong></span>
</pre></div></li><li class="listitem" value="2">You can then inspect the center of each cluster using <code class="email">barplot</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; barplot(t(fit$centers), beside = TRUE,xlab="cluster", ylab="value")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00168.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The barplot of centers of different attributes in four clusters</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Lastly, you<a id="id750" class="calibre1"/> can draw a scatter plot of the data and color the points according to the clusters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(customer, col = fit$cluster)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00169.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The scatter plot showing data colored with regard to its cluster label</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Clustering data with the k-means method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec355" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">k-means clustering is a method of partitioning clustering. The goal of the algorithm is to partition n <a id="id751" class="calibre1"/>objects into <span class="strong"><em class="calibre8">k</em></span> clusters, where each object belongs to the cluster with the nearest mean. The objective of the algorithm is to minimize the <span class="strong"><strong class="calibre2">within-cluster sum of squares</strong></span> (<span class="strong"><strong class="calibre2">WCSS</strong></span>). Assuming <span class="strong"><em class="calibre8">x</em></span> is the given set of <a id="id752" class="calibre1"/>observations, S = <span class="strong"><img src="../images/00170.jpeg" alt="How it works..." class="calibre24"/></span> denotes <span class="strong"><em class="calibre8">k</em></span> partitions, and <span class="strong"><img src="../images/00171.jpeg" alt="How it works..." class="calibre24"/></span> is the mean of <span class="strong"><img src="../images/00172.jpeg" alt="How it works..." class="calibre24"/></span>, then we can formulate the WCSS function as follows:</p><div class="mediaobject"><img src="../images/00173.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">The process of k-means clustering can be illustrated by the following five steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Specify the number of <span class="strong"><em class="calibre8">k</em></span> clusters.</li><li class="listitem" value="2">Randomly create k partitions.</li><li class="listitem" value="3">Calculate the center of the partitions.</li><li class="listitem" value="4">Associate objects closest to the cluster center.</li><li class="listitem" value="5">Repeat steps 2, 3, and 4 until the WCSS changes very little (or is minimized).</li></ol><div class="calibre14"/></div><p class="calibre7">In this recipe, we demonstrate how to use k-means clustering to cluster customer data. In contrast to hierarchical clustering, k-means clustering requires the user to input the number of <span class="strong"><em class="calibre8">K</em></span>. In this example, we use <span class="strong"><em class="calibre8">K=4</em></span>. Then, the output of a fitted model shows the size of each cluster, the cluster means of four generated clusters, the cluster vectors with regard to each data point, the within cluster sum of squares by the clusters, and other available components.</p><p class="calibre7">Further, you can draw the centers of each cluster in a bar plot, which will provide more details on how each attribute affects the clustering. Lastly, we plot the data point in a scatter plot and use the fitted cluster labels to assign colors with regard to the cluster label.</p></div></div>

<div class="book" title="Clustering data with the k-means method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec356" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">In k-means clustering, you can specify the algorithm used to perform clustering analysis. You can specify either Hartigan-Wong, Lloyd, Forgy, or MacQueen as the clustering algorithm. For more details, please use the <code class="email">help</code> function<a id="id753" class="calibre1"/> to refer to the document for the <code class="email">kmeans</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt;help(kmeans)</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Drawing a bivariate cluster plot"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec104" class="calibre1"/>Drawing a bivariate cluster plot</h1></div></div></div><p class="calibre7">In the previous recipe, we employed the k-means method to fit data into clusters. However, if there are more than two variables, it is impossible to display how data is clustered in two dimensions. Therefore, you can use a bivariate cluster plot to first reduce variables into two components, and then use components, such as axis and circle, as clusters to show<a id="id754" class="calibre1"/> how data is clustered. In this recipe, we will illustrate how to create a bivariate cluster plot.</p></div>

<div class="book" title="Drawing a bivariate cluster plot">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec357" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the <code class="email">customer</code> dataset as the input data source to draw a bivariate cluster plot.</p></div></div>

<div class="book" title="Drawing a bivariate cluster plot">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec358" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to draw a bivariate cluster plot:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Install and load the cluster package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("cluster")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(cluster)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then draw a bivariate cluster plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; clusplot(customer, fit$cluster, color=TRUE, shade=TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00174.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The bivariate clustering plot of the customer dataset</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">You can<a id="id755" class="calibre1"/> also zoom into the bivariate cluster plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; par(mfrow= c(1,2))</strong></span>
<span class="strong"><strong class="calibre2">&gt; clusplot(customer, fit$cluster, color=TRUE, shade=TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; rect(-0.7,-1.7, 2.2,-1.2, border = "orange", lwd=2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; clusplot(customer, fit$cluster, color = TRUE, xlim = c(-0.7,2.2), ylim = c(-1.7,-1.2))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00175.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The zoom-in of the bivariate clustering plot</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Drawing a bivariate cluster plot">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec359" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we draw a bivariate cluster plot to show how data is clustered. To draw a bivariate cluster plot, we first need to install the <code class="email">cluster</code> package and load it into R. We then use the <code class="email">clusplot</code> function to draw a bivariate cluster plot from a customer dataset. In the <code class="email">clustplot</code> function, we can set <code class="email">shade</code> to <code class="email">TRUE</code> and <code class="email">color</code> to <code class="email">TRUE</code> to display a cluster with colors and shades. As per the preceding figure (step 2) we found that the bivariate uses two components, which explains 85.01 percent of point variability, as the x-axis and y-axis. The<a id="id756" class="calibre1"/> data points are then scattered on the plot in accordance with component 1 and component 2. Data within the same cluster is circled in the same color and shade.</p><p class="calibre7">Besides drawing the four clusters in a single plot, you can use <code class="email">rect</code> to add a rectangle around a specific area within a given x-axis and y-axis range. You can then zoom into the plot to examine the data within each cluster by using <code class="email">xlim</code> and <code class="email">ylim</code> in the <code class="email">clusplot</code> function.</p></div></div>

<div class="book" title="Drawing a bivariate cluster plot">
<div class="book" title="There's more"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec360" class="calibre1"/>There's more</h2></div></div></div><p class="calibre7">The <code class="email">clusplot</code> function uses <code class="email">princomp</code> and <code class="email">cmdscale</code> to reduce the original feature dimension to the principal component. Therefore, one can see how data is clustered in a single plot with these two components as the x-axis and y-axis. To learn more about <code class="email">princomp</code> and <code class="email">cmdscale</code>, one can use the <code class="email">help</code> function to view related documents:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(cmdscale)</strong></span>
<span class="strong"><strong class="calibre2">&gt; help(princomp)</strong></span>
</pre></div><p class="calibre7">For those interested in how to use <code class="email">cmdscale</code> to reduce the dimensions, please perform the following steps:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; mds = cmdscale(dist(customer), k = 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(mds, col = fit$cluster)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00176.jpeg" alt="There's more" class="calibre9"/><div class="caption"><p class="calibre12">The scatter plot of data with regard to scaled dimensions </p></div></div><p class="calibre10"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Comparing clustering methods"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec105" class="calibre1"/>Comparing clustering methods</h1></div></div></div><p class="calibre7">After fitting data into clusters using different clustering methods, you may wish to measure <a id="id757" class="calibre1"/>the accuracy of the clustering. In most cases, you can use either intracluster or intercluster metrics as measurements. We now introduce how to compare different clustering methods using <code class="email">cluster.stat</code> from the <code class="email">fpc</code> package.</p></div>

<div class="book" title="Comparing clustering methods">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec361" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In order to perform a clustering method comparison, one needs to have the previous recipe completed by generating the <code class="email">customer</code> dataset.</p></div></div>

<div class="book" title="Comparing clustering methods">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec362" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to compare clustering methods:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">fpc</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("fpc")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(fpc)</strong></span>
</pre></div></li><li class="listitem" value="2">You then need to use hierarchical clustering with the <code class="email">single</code> method to cluster customer data and generate the object <code class="email">hc_single</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; single_c =  hclust(dist(customer), method="single")</strong></span>
<span class="strong"><strong class="calibre2">&gt; hc_single = cutree(single_c, k = 4)</strong></span>
</pre></div></li><li class="listitem" value="3">Use hierarchical clustering with the <code class="email">complete</code> method to cluster customer data and generate the object <code class="email">hc_complete</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; complete_c =  hclust(dist(customer), method="complete")</strong></span>
<span class="strong"><strong class="calibre2">&gt; hc_complete =  cutree(complete_c, k = 4)</strong></span>
</pre></div></li><li class="listitem" value="4">You <a id="id758" class="calibre1"/>can then use k-means clustering to cluster customer data and generate the object <code class="email">km</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(22)</strong></span>
<span class="strong"><strong class="calibre2">&gt; km = kmeans(customer, 4)</strong></span>
</pre></div></li><li class="listitem" value="5">Next, retrieve the cluster validation statistics of either clustering method:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; cs = cluster.stats(dist(customer), km$cluster)</strong></span>
</pre></div></li><li class="listitem" value="6">Most often, we focus on using <code class="email">within.cluster.ss</code> and <code class="email">avg.silwidth</code> to validate the clustering method:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; cs[c("within.cluster.ss","avg.silwidth")]</strong></span>
<span class="strong"><strong class="calibre2">$within.cluster.ss</strong></span>
<span class="strong"><strong class="calibre2">[1] 61.3489</strong></span>

<span class="strong"><strong class="calibre2">$avg.silwidth</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.4640587</strong></span>
</pre></div></li><li class="listitem" value="7">Finally, we can generate the cluster statistics of each clustering method and list them in a table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; sapply(list(kmeans = km$cluster, hc_single = hc_single, hc_complete = hc_complete), function(c) cluster.stats(dist(customer), c)[c("within.cluster.ss","avg.silwidth")])</strong></span>
<span class="strong"><strong class="calibre2">                  kmeans    hc_single hc_complete</strong></span>
<span class="strong"><strong class="calibre2">within.cluster.ss 61.3489   136.0092  65.94076</strong></span>
<span class="strong"><strong class="calibre2">avg.silwidth      0.4640587 0.2481926 0.4255961</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Comparing clustering methods">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec363" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we demonstrate how to validate clusters. To validate a clustering method, we often employ two techniques: intercluster distance and intracluster distance. In these techniques, the higher the intercluster distance, the better it is, and the lower the intracluster distance, the better it is. In order to calculate related statistics, we can apply <code class="email">cluster.stat</code> from the fpc package on the fitted clustering object.</p><p class="calibre7">From the <a id="id759" class="calibre1"/>output, the <code class="email">within.cluster.ss</code> measurement stands for the within clusters sum of squares, and avg.silwidth represents the average silhouette width. The <code class="email">within.cluster.ss</code> measurement shows how closely related objects are in clusters; the smaller the value, the more closely related objects are within the cluster. On the other hand, a silhouette is a measurement that considers how closely related objects are within the cluster and how clusters are separated from each other. Mathematically, we can define the silhouette width for each point <span class="strong"><em class="calibre8">x</em></span> as follows:</p><div class="mediaobject"><img src="../images/00177.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">In the preceding equation, <span class="strong"><em class="calibre8">a(x)</em></span> is the average distance between <span class="strong"><em class="calibre8">x</em></span> and all other points within the cluster, and <span class="strong"><em class="calibre8">b(x)</em></span> is the minimum of the average distances between x and the points in the other clusters. The silhouette value usually ranges from <span class="strong"><em class="calibre8">0</em></span> to <span class="strong"><em class="calibre8">1</em></span>; a value closer to <span class="strong"><em class="calibre8">1</em></span> suggests the data is better clustered.</p><p class="calibre7">The summary table generated in the last step shows that the complete hierarchical clustering method outperforms a single hierarchical clustering method and k-means clustering in <code class="email">within.cluster.ss</code> and <code class="email">avg.silwidth</code>.</p></div></div>

<div class="book" title="Comparing clustering methods">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec364" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <code class="email">kmeans</code> function also outputs statistics (for example, <code class="email">withinss</code> and <code class="email">betweenss</code>) for users to validate a clustering method:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(22)</strong></span>
<span class="strong"><strong class="calibre2">&gt; km = kmeans(customer, 4)</strong></span>
<span class="strong"><strong class="calibre2">&gt; km$withinss</strong></span>
<span class="strong"><strong class="calibre2">[1]  5.90040 11.97454 22.58236 20.89159</strong></span>
<span class="strong"><strong class="calibre2">&gt; km$betweenss</strong></span>
<span class="strong"><strong class="calibre2">[1] 174.6511</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Extracting silhouette information from clustering"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec106" class="calibre1"/>Extracting silhouette information from clustering</h1></div></div></div><p class="calibre7">Silhouette<a id="id760" class="calibre1"/> information is a measurement to validate a<a id="id761" class="calibre1"/> cluster of data. In the previous <a id="id762" class="calibre1"/>recipe, we mentioned that the measurement of a cluster involves the calculation of how closely the data is clustered within each cluster, and measures how far different clusters are apart from each other. The silhouette coefficient combines the measurement of the intracluster and intercluster distance. The output value typically ranges from <span class="strong"><em class="calibre8">0</em></span> to <span class="strong"><em class="calibre8">1</em></span>; the closer to <span class="strong"><em class="calibre8">1</em></span>, the better the cluster is. In this recipe, we will introduce how to compute silhouette information.</p></div>

<div class="book" title="Extracting silhouette information from clustering">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec365" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In <a id="id763" class="calibre1"/>order to extract the silhouette<a id="id764" class="calibre1"/> information from a cluster, you need to have the previous recipe completed by generating the <code class="email">customer</code> dataset.</p></div></div>

<div class="book" title="Extracting silhouette information from clustering">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec366" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to compute the silhouette information:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Use <code class="email">kmeans</code> to generate a k-means object, <code class="email">km</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(22)</strong></span>
<span class="strong"><strong class="calibre2">&gt; km = kmeans(customer, 4)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then compute the silhouette information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; kms = silhouette(km$cluster,dist(customer))</strong></span>
<span class="strong"><strong class="calibre2">&gt; summary(kms)</strong></span>
<span class="strong"><strong class="calibre2">Silhouette of 60 units in 4 clusters from silhouette.default(x = km$cluster, dist = dist(customer)) :</strong></span>
<span class="strong"><strong class="calibre2"> Cluster sizes and average silhouette widths:</strong></span>
<span class="strong"><strong class="calibre2">        8        11        16        25 </strong></span>
<span class="strong"><strong class="calibre2">0.5464597 0.4080823 0.3794910 0.5164434 </strong></span>
<span class="strong"><strong class="calibre2">Individual silhouette widths:</strong></span>
<span class="strong"><strong class="calibre2">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </strong></span>
<span class="strong"><strong class="calibre2"> 0.1931  0.4030  0.4890  0.4641  0.5422  0.6333 </strong></span>
</pre></div></li><li class="listitem" value="3">Next, you can plot the silhouette information:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(kms)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00178.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The silhouette plot of the k-means clustering result</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Extracting silhouette information from clustering">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec367" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id765" class="calibre1"/> recipe, we demonstrate how to<a id="id766" class="calibre1"/> use the silhouette plot to validate clusters. You can first retrieve the silhouette information, which shows cluster sizes, the average silhouette widths, and individual silhouette widths. The silhouette coefficient is a value ranging from <span class="strong"><em class="calibre8">0</em></span> to <span class="strong"><em class="calibre8">1</em></span>; the closer to <span class="strong"><em class="calibre8">1</em></span>, the better the quality of the cluster.</p><p class="calibre7">Lastly, we use the <code class="email">plot</code> function to draw a silhouette plot. The left-hand side of the plot shows the number of horizontal lines, which represent the number of clusters. The right-hand column shows the mean similarity of the plot of its own cluster minus the mean similarity of the next similar cluster. The average silhouette width is presented at the bottom of the plot.</p></div></div>

<div class="book" title="Extracting silhouette information from clustering">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec368" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For those interested in how silhouettes are computed, please refer to the Wikipedia<a id="id767" class="calibre1"/> entry for <span class="strong"><strong class="calibre2">Silhouette Value</strong></span>: <a class="calibre1" href="http://en.wikipedia.org/wiki/Silhouette_%28clustering%29">http://en.wikipedia.org/wiki/Silhouette_%28clustering%29</a></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Obtaining the optimum number of clusters for k-means"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec107" class="calibre1"/>Obtaining the optimum number of clusters for k-means</h1></div></div></div><p class="calibre7">While k-means clustering is fast and easy to use, it requires <span class="strong"><em class="calibre8">k</em></span> to be the input at the beginning. Therefore, we can use the sum of squares to determine which <span class="strong"><em class="calibre8">k</em></span> value is best for finding the optimum number of clusters for k-means. In the following recipe, we will discuss<a id="id768" class="calibre1"/> how to find the optimum number of clusters for the k-means clustering method.</p></div>

<div class="book" title="Obtaining the optimum number of clusters for k-means">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec369" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In order to find the optimum number of clusters, you need to have the previous recipe completed by generating the <code class="email">customer</code> dataset.</p></div></div>

<div class="book" title="Obtaining the optimum number of clusters for k-means">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec370" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to find the optimum number of clusters for the k-means clustering:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, calculate the within sum of squares (<code class="email">withinss</code>) of different numbers of clusters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; nk = 2:10</strong></span>
<span class="strong"><strong class="calibre2">&gt; set.seed(22)</strong></span>
<span class="strong"><strong class="calibre2">&gt; WSS = sapply(nk, function(k) {</strong></span>
<span class="strong"><strong class="calibre2">+     kmeans(customer, centers=k)$tot.withinss</strong></span>
<span class="strong"><strong class="calibre2">+ })</strong></span>
<span class="strong"><strong class="calibre2">&gt; WSS</strong></span>
<span class="strong"><strong class="calibre2">[1] 123.49224  88.07028  61.34890  48.76431  47.20813</strong></span>
<span class="strong"><strong class="calibre2">[6]  45.48114  29.58014  28.87519  23.21331</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use a line plot to plot the within sum of squares with a different number of <code class="email">k</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(nk, WSS, type="l", xlab= "number of k", ylab="within sum of squares")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00179.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The line plot of the within sum of squares with regard to the different number of k</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Next, you<a id="id769" class="calibre1"/> can calculate the average silhouette width (avg.silwidth) of different numbers of clusters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; SW = sapply(nk, function(k) {</strong></span>
<span class="strong"><strong class="calibre2">+   cluster.stats(dist(customer), kmeans(customer, centers=k)$cluster)$avg.silwidth</strong></span>
<span class="strong"><strong class="calibre2">+ })</strong></span>
<span class="strong"><strong class="calibre2">&gt; SW</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.4203896 0.4278904 0.4640587 0.4308448 0.3481157</strong></span>
<span class="strong"><strong class="calibre2">[6] 0.3320245 0.4396910 0.3417403 0.4070539</strong></span>
</pre></div></li><li class="listitem" value="4">You can then use a line plot to plot the average silhouette width with a different number of <code class="email">k</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(nk, SW, type="l", xlab="number of clusers", ylab="average silhouette width")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00180.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The line plot of average silhouette width with regard to the different number of k</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">Retrieve<a id="id770" class="calibre1"/> the maximum number of clusters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; nk[which.max(SW)]</strong></span>
<span class="strong"><strong class="calibre2">[1] 4</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Obtaining the optimum number of clusters for k-means">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec371" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we demonstrate how to find the optimum number of clusters by iteratively getting within the sum of squares and the average silhouette value. For the within sum of squares, lower values represent clusters with better quality. By plotting the within sum of squares in regard to different number of <code class="email">k</code>, we find that the elbow of the plot is at <code class="email">k=4</code>.</p><p class="calibre7">On the other hand, we also compute the average silhouette width based on the different numbers of clusters using <code class="email">cluster.stats</code>. Also, we can use a line plot to plot the average silhouette width with regard to the different numbers of clusters. The preceding figure (step 4) shows the maximum average silhouette width appears at <code class="email">k=4</code>. Lastly, we use <code class="email">which.max</code> to obtain the value of k to determine the location of the maximum average silhouette width.</p></div></div>

<div class="book" title="Obtaining the optimum number of clusters for k-means">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec372" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For<a id="id771" class="calibre1"/> those interested in how the within sum of squares is computed, please refer to the Wikipedia<a id="id772" class="calibre1"/> entry of <span class="strong"><strong class="calibre2">K-means clustering</strong></span>: <a class="calibre1" href="http://en.wikipedia.org/wiki/K-means_clustering">http://en.wikipedia.org/wiki/K-means_clustering</a></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Clustering data with the density-based method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec108" class="calibre1"/>Clustering data with the density-based method</h1></div></div></div><p class="calibre7">As an<a id="id773" class="calibre1"/> alternative to distance measurement, you can use a density-based measurement to cluster data. This method finds an area with a higher density than the remaining area. One of the most famous methods is <a id="id774" class="calibre1"/>DBSCAN. In the following recipe, we will demonstrate<a id="id775" class="calibre1"/> how to use DBSCAN to <a id="id776" class="calibre1"/>perform density-based clustering.</p></div>

<div class="book" title="Clustering data with the density-based method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec373" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use simulated data generated from the <code class="email">mlbench</code> package.</p></div></div>

<div class="book" title="Clustering data with the density-based method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec374" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform density-based clustering:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">fpc</code> and <code class="email">mlbench</code> packages:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("mlbench")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(mlbench)</strong></span>
<span class="strong"><strong class="calibre2">&gt; install.packages("fpc")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(fpc)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use the <code class="email">mlbench</code> library to draw a Cassini problem graph:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; p = mlbench.cassini(500)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(p$x)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00181.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The Cassini problem graph</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Next, you<a id="id777" class="calibre1"/> can cluster <a id="id778" class="calibre1"/>data with regard to its <a id="id779" class="calibre1"/>density measurement:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ds = dbscan(dist(p$x),0.2, 2, countmode=NULL, method="dist")</strong></span>
<span class="strong"><strong class="calibre2">&gt; ds</strong></span>
<span class="strong"><strong class="calibre2">dbscan Pts=500 MinPts=2 eps=0.2</strong></span>
<span class="strong"><strong class="calibre2">        1   2   3</strong></span>
<span class="strong"><strong class="calibre2">seed  200 200 100</strong></span>
<span class="strong"><strong class="calibre2">total 200 200 100</strong></span>
</pre></div></li><li class="listitem" value="4">Plot the data in a scatter plot with different cluster labels as the color:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(ds, p$x)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00182.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The data scatter plot colored with regard to the cluster label</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">You <a id="id780" class="calibre1"/>can also use <code class="email">dbscan</code> to<a id="id781" class="calibre1"/> predict <a id="id782" class="calibre1"/>which cluster the data point belongs to. In this example, first make three inputs in the matrix <code class="email">p</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; y = matrix(0,nrow=3,ncol=2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; y[1,] = c(0,0)</strong></span>
<span class="strong"><strong class="calibre2">&gt; y[2,] = c(0,-1.5)</strong></span>
<span class="strong"><strong class="calibre2">&gt; y[3,] = c(1,1)</strong></span>
<span class="strong"><strong class="calibre2">&gt; y</strong></span>
<span class="strong"><strong class="calibre2">     [,1] [,2]</strong></span>
<span class="strong"><strong class="calibre2">[1,]    0  0.0</strong></span>
<span class="strong"><strong class="calibre2">[2,]    0 -1.5</strong></span>
<span class="strong"><strong class="calibre2">[3,]    1  1.0</strong></span>
</pre></div></li><li class="listitem" value="6">You can then predict which cluster the data belongs to:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predict(ds, p$x, y)</strong></span>
<span class="strong"><strong class="calibre2">[1] 3 1 2</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Clustering data with the density-based method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec375" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Density-based clustering uses the idea of density reachability and density connectivity, which makes it very useful in discovering a cluster in nonlinear shapes. Before discussing the process of density-based clustering, some important background concepts must be explained. Density-based clustering takes two parameters into account: <code class="email">eps</code> and <code class="email">MinPts</code>. <code class="email">eps</code> stands for the maximum radius of the neighborhood; <code class="email">MinPts</code> denotes the minimum number of points within the <code class="email">eps</code> neighborhood. With these two parameters, we can define the core point as having points more than <code class="email">MinPts</code> within <code class="email">eps</code>. Also, we can define the board point as having points less than <code class="email">MinPts</code>, but is in the neighborhood of the core points. Then, we can define the core object as if the number of points in the <code class="email">eps</code>-neighborhood of <code class="email">p</code> is more than <code class="email">MinPts</code>.</p><p class="calibre7">Furthermore, we have to define the reachability between two points. We can say that a point, <code class="email">p</code>, is directly density reachable from another point, <code class="email">q</code>, if q is within the <code class="email">eps</code>-neighborhood of <code class="email">p</code> and <code class="email">p</code> is a core object. Then, we can define that a point, <code class="email">p</code>, is generic and density reachable from the point <code class="email">q</code>, if there exists a chain of points, p<sub class="calibre25">1</sub>,p<sub class="calibre25">2</sub>...,p<sub class="calibre25">n</sub>, where p<sub class="calibre25">1</sub> = q, p<sub class="calibre25">n</sub> = p, and p<sub class="calibre25">i</sub>+1 is directly density reachable from pi with regard to Eps and <code class="email">MinPts</code> for 1 &lt;= i &lt;= n:</p><div class="mediaobject"><img src="../images/00183.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">Point p and q is density reachable</p></div></div><p class="calibre10"> </p><p class="calibre7">With a<a id="id783" class="calibre1"/> preliminary concept of density-based <a id="id784" class="calibre1"/>clustering, we can then<a id="id785" class="calibre1"/> illustrate the process of DBSCAN, the most popular density-based clustering, as shown in these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Randomly select a point, <code class="email">p</code>.</li><li class="listitem" value="2">Retrieve all the points that are density-reachable from <code class="email">p</code> with regard to <code class="email">Eps</code> and <code class="email">MinPts</code>.</li><li class="listitem" value="3">If <code class="email">p</code> is a core point, then a cluster is formed. Otherwise, if it is a board point and no points are density reachable from <code class="email">p</code>, the process will mark the point as noise and continue visiting the next point.</li><li class="listitem" value="4">Repeat the process until all points have been visited.</li></ol><div class="calibre14"/></div><p class="calibre7">In this recipe, we demonstrate how to use the DBSCAN density-based method to cluster customer data. First, we have to install and load the <code class="email">mlbench</code> and <code class="email">fpc</code> libraries. The <code class="email">mlbench</code> package provides many methods to generate simulated data with different shapes and sizes. In this example, we generate a Cassini problem graph.</p><p class="calibre7">Next, we perform <code class="email">dbscan</code> on a Cassini dataset to cluster the data. We specify the reachability distance as 0.2, the minimum reachability number of points to <code class="email">2</code>, the progress reporting as null, and use distance as a measurement. The clustering method successfully clusters data into three clusters with sizes of 200, 200, and 100. By plotting the points and cluster labels on the plot, we see that three sections of the Cassini graph are separated in different colors.</p><p class="calibre7">The <code class="email">fpc</code> package also provides a <code class="email">predict</code> function, and you can use this to predict the cluster <a id="id786" class="calibre1"/>labels of the input matrix. Point c(0,0) is classified into cluster 3, point c(0, -1.5) is classified into cluster 1, and point c(1,1) is classified into cluster 2.</p></div></div>

<div class="book" title="Clustering data with the density-based method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec376" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <code class="email">fpc</code> package contains flexible procedures of clustering, and has useful clustering <a id="id787" class="calibre1"/>analysis functions. For <a id="id788" class="calibre1"/>example, you can generate a discriminant projection plot using the <code class="email">plotcluster</code> function. For more information, please refer to the following document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(plotcluster)</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Clustering data with the model-based method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec109" class="calibre1"/>Clustering data with the model-based method</h1></div></div></div><p class="calibre7">In contrast to hierarchical clustering and k-means clustering, which use a heuristic approach<a id="id789" class="calibre1"/> and do not depend on a formal model. Model-based clustering techniques assume varieties of data models and apply an EM algorithm to obtain the most likely model, and further use the model to infer the most likely number of clusters. In this recipe, we will demonstrate how to use the model-based method to determine the most likely number of clusters.</p></div>

<div class="book" title="Clustering data with the model-based method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec377" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In order to perform a model-based method to cluster customer data, you need to have the previous recipe completed by generating the customer dataset.</p></div></div>

<div class="book" title="Clustering data with the model-based method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec378" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform model-based clustering:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, please install and load the library <code class="email">mclust</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("mclust")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(mclust)</strong></span>
</pre></div></li><li class="listitem" value="2">You<a id="id790" class="calibre1"/> can then perform model-based clustering on the <code class="email">customer</code> dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; mb = Mclust(customer)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(mb)</strong></span>
</pre></div></li><li class="listitem" value="3">Then, you can press 1 to obtain the BIC against a number of components:<div class="mediaobject"><img src="../images/00184.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Plot of BIC against number of components</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="4">Then, you can press 2 to show the classification with regard to different combinations of features:<div class="mediaobject"><img src="../images/00185.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Plot showing classification with regard to different combinations of features</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">Press 3 <a id="id791" class="calibre1"/>to show the classification uncertainty with regard to different combinations of features:<div class="mediaobject"><img src="../images/00186.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Plot showing classification uncertainty with regard to different combinations of features</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">Next, press 4 to plot the density estimation:<div class="mediaobject"><img src="../images/00187.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A plot of density estimation</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="7">Then, you <a id="id792" class="calibre1"/>can press 0 to plot density to exit the plotting menu.</li><li class="listitem" value="8">Lastly, use the <code class="email">summary</code> function to obtain the most likely model and number of clusters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(mb)</strong></span>
<span class="strong"><strong class="calibre2">----------------------------------------------------</strong></span>
<span class="strong"><strong class="calibre2">Gaussian finite mixture model fitted by EM algorithm </strong></span>
<span class="strong"><strong class="calibre2">----------------------------------------------------</strong></span>

<span class="strong"><strong class="calibre2">Mclust VII (spherical, varying volume) model with 5 components:</strong></span>

<span class="strong"><strong class="calibre2"> log.likelihood  n df       BIC       ICL</strong></span>
<span class="strong"><strong class="calibre2">      -218.6891 60 29 -556.1142 -557.2812</strong></span>

<span class="strong"><strong class="calibre2">Clustering table:</strong></span>
<span class="strong"><strong class="calibre2"> 1  2  3  4  5</strong></span>
<span class="strong"><strong class="calibre2">11  8 17 14 10</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Clustering data with the model-based method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec379" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Instead of taking a heuristic approach to build a cluster, model-based clustering uses a probability-based approach. Model-based clustering assumes that the data is generated by an <a id="id793" class="calibre1"/>underlying probability distribution and tries to recover the distribution from the data. One common model-based approach is using finite mixture models, which provide a flexible modeling framework for the analysis of the probability distribution. Finite mixture models are a linearly weighted sum of component probability distribution. Assume the data <span class="strong"><em class="calibre8">y=(y<sub class="calibre25">1</sub>,y<sub class="calibre25">2</sub>…y<sub class="calibre25">n</sub>)</em></span> contains n independent and multivariable observations; G is the number of components; the likelihood of finite mixture models can be formulated as:</p><div class="mediaobject"><img src="../images/00188.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">Where <span class="strong"><img src="../images/00189.jpeg" alt="How it works..." class="calibre24"/></span> and <span class="strong"><img src="../images/00190.jpeg" alt="How it works..." class="calibre24"/></span> are the density and parameters of the <span class="strong"><em class="calibre8">k</em></span>th component in the mixture, and <span class="strong"><img src="../images/00191.jpeg" alt="How it works..." class="calibre24"/></span> (<span class="strong"><img src="../images/00192.jpeg" alt="How it works..." class="calibre24"/></span> and <span class="strong"><img src="../images/00193.jpeg" alt="How it works..." class="calibre24"/></span>) is the probability that an observation belongs to the <span class="strong"><em class="calibre8">k</em></span>th component.</p><p class="calibre7">The process of model-based clustering has several steps: First, the process selects the number and types of component probability distribution. Then, it fits a finite mixture model and calculates the posterior probabilities of a component membership. Lastly, it assigns the membership of each observation to the component with the maximum probability.</p><p class="calibre7">In this recipe, we demonstrate how to use model-based clustering to cluster data. We first install and load the <code class="email">Mclust</code> library into R. We then fit the customer data into the model-based method by using the <code class="email">Mclust</code> function.</p><p class="calibre7">After the data is fit into the model, we plot the model based on clustering results. There are four different plots: BIC, classification, uncertainty, and density plots. The BIC plot shows the BIC value, and one can use this value to choose the number of clusters. The classification plot shows how data is clustered in regard to different dimension combinations. The uncertainty plot shows the uncertainty of classifications in regard to different dimension combinations. The density plot shows the density estimation in contour.</p><p class="calibre7">You <a id="id794" class="calibre1"/>can also use the <code class="email">summary</code> function to obtain the most likely model and the most possible number of clusters. For this example, the most possible number of clusters is five, with a BIC value equal to -556.1142.</p></div></div>

<div class="book" title="Clustering data with the model-based method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec380" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For those interested in detail on how <code class="email">Mclust</code> works, please refer to the following source: C. Fraley, A. E. Raftery, T. B. Murphy and L. Scrucca (2012). <span class="strong"><em class="calibre8">mclust Version 4 for R: Normal Mixture Modeling for Model-Based Clustering, Classification, and Density Estimation</em></span>. <span class="strong"><em class="calibre8">Technical Report No. 597</em></span>, Department of Statistics, University of Washington.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Visualizing a dissimilarity matrix"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec110" class="calibre1"/>Visualizing a dissimilarity matrix</h1></div></div></div><p class="calibre7">A dissimilarity matrix <a id="id795" class="calibre1"/>can be used as a measurement for the quality <a id="id796" class="calibre1"/>of a cluster. To visualize the matrix, we can use a heat map on a distance matrix. Within the plot, entries with low dissimilarity (or high similarity) are plotted darker, which is helpful to identify hidden structures in the data. In this recipe, we will discuss some techniques that are useful to visualize a dissimilarity matrix.</p></div>

<div class="book" title="Visualizing a dissimilarity matrix">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec381" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In order to visualize the dissimilarity matrix, you need to have the previous recipe completed by generating the customer dataset. In addition to this, a k-means object needs to be generated and stored in the variable <code class="email">km</code>.</p></div></div>

<div class="book" title="Visualizing a dissimilarity matrix">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec382" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to visualize the dissimilarity matrix:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">seriation</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("seriation")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(seriation)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use <code class="email">dissplot</code> to visualize the dissimilarity matrix in a heat map:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dissplot(dist(customer), labels=km$cluster, options=list(main="Kmeans Clustering With k=4"))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00194.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A dissimilarity plot of k-means clustering</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Next, apply <code class="email">dissplot</code> on<a id="id797" class="calibre1"/> hierarchical clustering in the heat map:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; complete_c =  hclust(dist(customer), method="complete")</strong></span>
<span class="strong"><strong class="calibre2">&gt; hc_complete =  cutree(complete_c, k = 4)</strong></span>
<span class="strong"><strong class="calibre2">&gt; dissplot(dist(customer), labels=hc_complete, options=list(main="Hierarchical Clustering"))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00195.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A dissimilarity plot of hierarchical clustering</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Visualizing a dissimilarity matrix">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec383" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we use a dissimilarity plot to visualize the dissimilarity matrix. We first install <a id="id798" class="calibre1"/>and load the package <code class="email">seriation</code>, and then apply the <code class="email">dissplot</code> function on the k-means clustering output, generating the preceding figure (step 2).</p><p class="calibre7">It shows that clusters similar to each other are plotted darker, and dissimilar combinations are plotted lighter. Therefore, we can see clusters against their corresponding clusters (such as cluster 4 to cluster 4) are plotted diagonally and darker. On the other hand, clusters dissimilar to each other are plotted lighter and away from the diagonal.</p><p class="calibre7">Likewise, we can apply the <code class="email">dissplot</code> function on the output of hierarchical clustering. The generated plot in the figure (step 3) shows the similarity of each cluster in a single heat map.</p></div></div>

<div class="book" title="Visualizing a dissimilarity matrix">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec384" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Besides using <code class="email">dissplot</code> to visualize the dissimilarity matrix, one can also visualize a distance matrix by using the <code class="email">dist</code> and <code class="email">image</code> functions. In the resulting graph, closely related entries are plotted in red. Less related entries are plotted closer to white:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; image(as.matrix(dist(customer)))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00196.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">A distance matrix plot of customer dataset</p></div></div><p class="calibre10"> </p><p class="calibre7">In order to plot both a dendrogram and heat map to show how data is clustered, you can use<a id="id799" class="calibre1"/> the <code class="email">heatmap</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; cd=dist(customer)</strong></span>
<span class="strong"><strong class="calibre2">&gt; hc=hclust(cd)</strong></span>
<span class="strong"><strong class="calibre2">&gt; cdt=dist(t(customer))</strong></span>
<span class="strong"><strong class="calibre2">&gt; hcc=hclust(cdt)</strong></span>
<span class="strong"><strong class="calibre2">&gt; heatmap(customer, Rowv=as.dendrogram(hc), Colv=as.dendrogram(hcc))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00197.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">A heat map with dendrogram on the column and row side</p></div></div><p class="calibre10"> </p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Validating clusters externally"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec111" class="calibre1"/>Validating clusters externally</h1></div></div></div><p class="calibre7">Besides generating statistics to validate the quality of the generated clusters, you can use known data clusters as the ground truth to compare different clustering methods. In this recipe, we<a id="id800" class="calibre1"/> will demonstrate how clustering methods differ with regard to data with known clusters.</p></div>

<div class="book" title="Validating clusters externally">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec385" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use handwriting digits as clustering inputs; you can find the figure on the author's Github page: <a class="calibre1" href="https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9">https://github.com/ywchiu/ml_R_cookbook/tree/master/CH9</a>.</p></div></div>

<div class="book" title="Validating clusters externally">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec386" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to cluster digits with different clustering techniques:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you need to install and load the package <code class="email">png</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("png")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(png)</strong></span>
</pre></div></li><li class="listitem" value="2">Then, please <a id="id801" class="calibre1"/>read images from <code class="email">handwriting.png</code> and transform the read data into a scatter plot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; img2 = readPNG("handwriting.png", TRUE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; img3 = img2[,nrow(img2):1]</strong></span>
<span class="strong"><strong class="calibre2">&gt; b = cbind(as.integer(which(img3 &lt; -1) %% 28), which(img3 &lt; -1) / 28)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(b, xlim=c(1,28), ylim=c(1,28))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00198.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">A scatter plot of handwriting digits</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">Perform a k-means clustering method on the handwriting digits:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(18)</strong></span>
<span class="strong"><strong class="calibre2">&gt; fit = kmeans(b, 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(b, col=fit$cluster)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(b, col=fit$cluster,  xlim=c(1,28), ylim=c(1,28))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00199.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">k-means clustering result on handwriting digits</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="4">Next, perform the <code class="email">dbscan</code> clustering <a id="id802" class="calibre1"/>method on the handwriting digits:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ds = dbscan(b, 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ds</strong></span>
<span class="strong"><strong class="calibre2">dbscan Pts=212 MinPts=5 eps=2</strong></span>
<span class="strong"><strong class="calibre2">       1   2</strong></span>
<span class="strong"><strong class="calibre2">seed  75 137</strong></span>
<span class="strong"><strong class="calibre2">total 75 137</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(ds, b,  xlim=c(1,28), ylim=c(1,28))</strong></span>
</pre></div></li></ol><div class="calibre14"/></div><div class="mediaobject"><img src="../images/00200.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">DBSCAN clustering result on handwriting digits</p></div></div><p class="calibre10"> </p></div></div>

<div class="book" title="Validating clusters externally">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec387" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we demonstrate how different clustering methods work in regard to a handwriting dataset. The aim of the clustering is to separate 1 and 7 into different clusters. We <a id="id803" class="calibre1"/>perform different techniques to see how data is clustered in regard to the k-means and DBSCAN methods.</p><p class="calibre7">To generate the data, we use the Windows application <code class="email">paint.exe</code> to create a PNG file with dimensions of 28 x 28 pixels. We then read the PNG data using the <code class="email">readPNG</code> function and transform the read PNG data points into a scatter plot, which shows the handwriting digits in 17.</p><p class="calibre7">After the data is read, we perform clustering techniques on the handwriting digits. First, we perform k-means clustering, where <code class="email">k=2</code> on the dataset. Since k-means clustering employs distance measures, the constructed clusters cover the area of both the 1 and 7 digits. We then perform DBSCAN on the dataset. As DBSCAN is a density-based clustering technique, it successfully separates digit 1 and digit 7 into different clusters.</p></div></div>

<div class="book" title="Validating clusters externally">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec388" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">If you are interested in how to read various graphic formats in R, you may refer to the following document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(package="png")</strong></span>
</pre></div></li></ul></div></div></div></body></html>