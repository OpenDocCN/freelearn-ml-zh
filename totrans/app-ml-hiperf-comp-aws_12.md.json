["```py\n…\nfrom sagemaker.huggingface import HuggingFaceModel\nimport sagemaker\nrole = sagemaker.get_execution_role()\n# Step 1: Hub Model configuration. https://huggingface.co/models\nhub = {\n     'HF_MODEL_ID':'AidenH20/DNABERT-500down',\n     'HF_TASK':'text-classification'\n}\n# Step 2: create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n     transformers_version='4.17.0',\n     pytorch_version='1.10.2',\n     py_version='py38',\n     env=hub,\n     role=role,\n)\n# Step 3: deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n     initial_instance_count=1, # number of instances\n     instance_type='ml.m5.xlarge' # ec2 instance type\n)\n…\n```", "```py\n…\ndna_sequence = 'CTAATC TAATCT AATCTA ATCTAG TCTAGT CTAGTA TAGTAA AGTAAT GTAATG TAATGC AATGCC ATGCCG TGCCGC GCCGCG CCGCGT CGCGTT GCGTTG CGTTGG GTTGGT TTGGTG TGGTGG GGTGGA GTGGAA TGGAAA GGAAAG GAAAGA AAAGAC AAGACA AGACAT GACATG ACATGA CATGAC ATGACA TGACAT GACATA ACATAC CATACC ATACCT TACCTC ACCTCA CCTCAA CTCAAA TCAAAC CAAACA AAACAG AACAGC ACAGCA CAGCAG AGCAGG GCAGGG CAGGGG AGGGGG GGGGGC GGGGCG GGGCGC GGCGCC GCGCCA CGCCAT GCCATG CCATGC CATGCG ATGCGC TGCGCC GCGCCA CGCCAA GCCAAG CCAAGC CAAGCC AAGCCC AGCCCG GCCCGC CCCGCA CCGCAG CGCAGA GCAGAG CAGAGG AGAGGG GAGGGT AGGGTT GGGTTG GGTTGT GTTGTC TTGTCC TGTCCA GTCCAA TCCAAC CCAACT CAACTC AACTCC ACTCCT CTCCTA TCCTAT CCTATT CTATTC TATTCC ATTCCT'\npredictor.predict({\n     'inputs': dna_sequence\n})\n…\n```", "```py\n...\nhuggingface_estimator = HuggingFace(entry_point='train.py',\n                           source_dir='./code',\n                           instance_type='ml.g5.12xlarge',\n                           instance_count=1,\n                           role=role,\n                           transformers_version='4.12',\n                           pytorch_version='1.9',\n                           py_version='py38',\n                           hyperparameters=hyperparameters)\n...\n```", "```py\n    …\n    ```", "```py\n    # starting the train job with our uploaded datasets as input\n    ```", "```py\n    huggingface_estimator.fit({'data': data_input_path})\n    ```", "```py\n    …\n    ```", "```py\n…\n# configuration for running training on smdistributed Model Parallel\nmpi_options = {\n    \"enabled\": True,\n    \"processes_per_host\": 8,\n}\nsmp_options = {\n    \"enabled\":True,\n    \"parameters\": {\n        \"microbatches\": 1,\n        \"placement_strategy\": \"cluster\",\n        \"pipeline\": \"interleaved\",\n        \"optimize\": \"memory\",\n        \"partitions\": 4,\n        \"ddp\": True,\n        # \"tensor_parallel_degree\": 2,\n        \"shard_optimizer_state\": True,\n        \"activation_checkpointing\": True,\n        \"activation_strategy\": \"each\",\n        \"activation_offloading\": True,\n    }\n}\ndistribution = {\n    \"smdistributed\": {\"modelparallel\": smp_options},\n    \"mpi\": mpi_options\n}\n…\n```", "```py\n…\nhuggingface_estimator = HuggingFace(entry_point='train.py',\n                           source_dir='./code',\n                           instance_type='ml.g5.48xlarge',\n                           instance_count=1,\n                           role=role,\n                           transformers_version='4.12',\n                           pytorch_version='1.9',\n                           py_version='py38',\n                           distribution= distribution,\n                           hyperparameters=hyperparameters)\nhuggingface_estimator.fit({'data': data_input_path})\n…\n```", "```py\n…\nfrom transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\nfrom transformers.sagemaker import SageMakerTrainer as Trainer\n…\n```", "```py\n…\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForTokenClassification, BertTokenizerFast, EvalPrediction, T5Tokenizer\n…\n```", "```py\n…\ntraining_args = TrainingArguments(\n    output_dir='./results',         # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=1,   # batch size per device during training\n    per_device_eval_batch_size=1,   # batch size for evaluation\n    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n    learning_rate=3e-05,             # learning rate\n    weight_decay=0.0,                # strength of weight decay\n    logging_dir='./logs',            # directory for storing logs\n    logging_steps=200,               # How often to print logs\n    do_train=True,                   # Perform training\n    do_eval=True,                    # Perform evaluation\n    evaluation_strategy=\"epoch\",     # evalute after each epoch\n    gradient_accumulation_steps=32,  # total number of steps before back propagation\n    fp16=True,                       # Use mixed precision\n    fp16_opt_level=\"02\",             # mixed precision mode\n    run_name=\"ProBert-T5-XL\",      # experiment name\n    seed=3,                         # Seed for experiment reproducibility\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_accuracy\",\n    greater_is_better=True,\n    save_strategy=\"epoch\",\n    max_grad_norm=0,\n    dataloader_drop_last=True,\n    )\n…\n```", "```py\n…\ntrainer = Trainer(\n    model_init=model_init,                # the instantiated Transformers model to be trained\n    args=training_args,                   # training arguments, defined above\n    train_dataset=train_dataset,          # training dataset\n    eval_dataset=val_dataset,             # evaluation dataset\n    compute_metrics = compute_metrics,    # evaluation metrics\n    )\n…\n```", "```py\n…\ntrainer.train()\ntrainer.save_model(args.model_dir)\n…\n```", "```py\n…\npredictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")\n…\n```", "```py\n…\npredictor.predict(input_sequence)\n…\n```"]