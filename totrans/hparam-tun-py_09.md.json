["```py\n    git clone https://github.com/louisowen6/scikit-hyperband\n    ```", "```py\n    cd scikit-hyperband\n    ```", "```py\n    mv hyperband \"path/to/your/working/directory\"\n    ```", "```py\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    df = pd.read_csv(\"train.csv\",sep=\";\")\n    df_train, df_test = train_test_split(df, test_size=0.1, random_state=0)\n    ```", "```py\n    import numpy as np\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import f1_score\n    ```", "```py\nX_train_numerical = df_train.select_dtypes(include=np.number).drop(columns=['y'])\ny_train = df_train['y']\n```", "```py\nX_test_numerical = df_test.select_dtypes(include=np.number).drop(columns=['y'])\ny_test = df_test['y']\n```", "```py\nmodel = RandomForestClassifier(random_state=0)\nmodel.fit(X_train_numerical,y_train)\n```", "```py\ny_pred = model.predict(X_test_numerical)\nprint(f1_score(y_test, y_pred))\n```", "```py\n    hyperparameter_space = {\n    \"n_estimators\": [25,50,100,150,200],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"max_depth\": [3, 5, 10, 15, 20, None],\n    \"class_weight\": [\"balanced\",\"balanced_subsample\"],\n    \"min_samples_split\": [0.01,0.1,0.25,0.5,0.75,1.0],\n    }\n    ```", "```py\n    from sklearn.model_selection import GridSearchCV\n    ```", "```py\nmodel = RandomForestClassifier(random_state=0)\n```", "```py\nclf = GridSearchCV(model, hyperparameter_space, \n                   scoring='f1', cv=5, \n                   n_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_numerical, y_train)\n```", "```py\nprint(clf.best_params_,clf.best_score_)\n```", "```py\nprint(clf.score(X_test_numerical,y_test))\n```", "```py\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n    ```", "```py\nnumerical_feats = list(df_train.drop(columns='y').select_dtypes(include=np.number).columns)\ncategorical_feats = list(df_train.drop(columns='y').select_dtypes(exclude=np.number).columns)\n```", "```py\nnumeric_preprocessor = StandardScaler()\ncategorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n```", "```py\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_preprocessor, numerical_feats),\n        (\"cat\", categorical_preprocessor, categorical_feats),\n    ])\n```", "```py\npipe = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), \n           (\"model\", RandomForestClassifier(random_state=0))])\n```", "```py\n    pipe.fit(X_train_full,y_train)\n    y_pred = pipe.predict(X_test_full)\n    print(f1_score(y_test, y_pred))\n    ```", "```py\n    hyperparameter_space = { \n    \"model__n_estimators\": [25,50,100,150,200], \n    \"model__criterion\": [\"gini\", \"entropy\"], \n    \"model__class_weight\": [\"balanced\", \"balanced_subsample\"],\n    \"model__min_samples_split\": [0.01,0.1,0.25,0.5,0.75,1.0], \n    }\n    ```", "```py\nclf = GridSearchCV(pipe, hyperparameter_space, \n                   scoring = 'f1', cv=5, \n                   n_jobs=-1, refit = True\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```", "```py\nfrom sklearn.impute import SimpleImputer\n```", "```py\nnumeric_preprocessor = Pipeline(\n```", "```py\nsteps=[(\"missing_value_imputation\", SimpleImputer(strategy=\"mean\")),     (\"normalization\", StandardScaler())]\n```", "```py\n)\n```", "```py\nfrom scipy.stats import randint, truncnorm\n```", "```py\nhyperparameter_space = { \n```", "```py\n\"model__n_estimators\": randint(5, 200), \n```", "```py\n\"model__criterion\": [\"gini\", \"entropy\"],\n```", "```py\n\"model__class_weight\": [\"balanced\",\"balanced_subsample\"],\n```", "```py\n\"model__min_samples_split\": truncnorm(a=0,b=0.5,loc=0.005, scale=0.01),\n```", "```py\n}\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\n```", "```py\nclf = RandomizedSearchCV(pipe, hyperparameter_space, \n```", "```py\n                         n_iter = 200, random_state = 0,\n```", "```py\n                         scoring = 'f1', cv=5, \n```", "```py\n                         n_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```", "```py\nclf = CoarseToFineSearchCV(pipe, hyperparameter_space,\n```", "```py\nrandom_iters=25, top_n_percentile=50, n_iter=10, \n```", "```py\ncontinuous_hyperparams=['model__min_samples_split'],\n```", "```py\nrandom_state=0, scoring='f1', cv=5, \n```", "```py\nn_jobs=-1, refit=True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\ny_pred = clf.predict(X_test_full)\n```", "```py\nprint(f1_score(y_test, y_pred))\n```", "```py\nfrom sklearn.experimental import enable_halving_search_cv\n```", "```py\nfrom sklearn.model_selection import HalvingRandomSearchCV\n```", "```py\nclf = HalvingRandomSearchCV(pipe, hyperparameter_space, \n```", "```py\n                            factor=3,\n```", "```py\n aggressive_elimination=False,\n```", "```py\n                            random_state = 0,\n```", "```py\n                            scoring = 'f1', cv=5, \n```", "```py\n                            n_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```", "```py\n    import matplotlib.pyplot as plt\n    ```", "```py\nresults = pd.DataFrame(clf.cv_results_)\nresults[\"params_str\"] = results.params.apply(str)\nresults.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\nmean_scores = results.pivot(\nindex=\"iter\", columns=\"params_str\", values=\"mean_test_score\")\n```", "```py\nfig, ax = plt.subplots(figsize=(16,16))\nax = mean_scores.plot(legend=False, alpha=0.6, ax=ax)\nlabels = [\n    f\"Iteration {i+1}\\nn_samples={clf.n_resources_[i]}\\nn_candidates={clf.n_candidates_[i]}\"\n    for i in range(clf.n_iterations_)]\nax.set_xticks(range(clf.n_iterations_))\nax.set_xticklabels(labels, rotation=0, multialignment=\"left\",size=16)\nax.set_title(\"F1-Score of Candidates over Iterations\",size=20)\nax.set_ylabel(\"5-Folds Cross Validation F1-Score\", fontsize=18)\nax.set_xlabel(\"\")\nplt.tight_layout()\nplt.show()\n```", "```py\nfrom hyperband import HyperbandSearchCV\n```", "```py\nclf = HyperbandSearchCV(pipe, hyperparameter_space,\n```", "```py\n                        resource_param='model__n_estimators',\n```", "```py\n                        eta=3, min_iter=1, max_iter=100,\n```", "```py\n                        random_state = 0,\n```", "```py\n                        scoring = 'f1', cv=5, \n```", "```py\n                        n_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```", "```py\nfrom skopt.space import *\n```", "```py\nhyperparameter_space = {\n```", "```py\n\"model__n_estimators\": Integer(low=5, high=200),\n```", "```py\n\"model__criterion\": Categorical([\"gini\", \"entropy\"]),\n```", "```py\n\"model__class_weight\": Categorical([\"balanced\",\"balanced_subsample\"]),\n```", "```py\n\"model__min_samples_split\": Real(low=0,high=0.5,prior=\"truncnorm\",\n```", "```py\n                                 **{\"loc\":0.005,\"scale\":0.01})\n```", "```py\n}\n```", "```py\nfrom skopt import BayesSearchCV\n```", "```py\nclf = BayesSearchCV(pipe, hyperparameter_space, n_iter=50,\n```", "```py\noptimizer_kwargs={\"base_estimator\":\"GP\",\n```", "```py\n                  \"n_initial_points\":10,\n```", "```py\n                  \"initial_point_generator\":\"random\",\n```", "```py\n                  \"acq_func\":\"EI\",\n```", "```py\n                  \"acq_optimizer\":\"auto\",\n```", "```py\n                  \"n_jobs\":-1,\n```", "```py\n                  \"random_state\":0,\n```", "```py\n                  \"acq_func_kwargs\": {\"xi\":0.01}\n```", "```py\n                  },\n```", "```py\nrandom_state = 0,\n```", "```py\nscoring = 'f1', cv=5, \n```", "```py\nn_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```", "```py\nfrom skopt import BayesSearchCV\n```", "```py\nclf = BayesSearchCV(pipe, hyperparameter_space, n_iter=50,\n```", "```py\noptimizer_kwargs={\"base_estimator\":\"RF\",\n```", "```py\n                  \"n_initial_points\":10,\n```", "```py\n                  \"initial_point_generator\":\"random\",\n```", "```py\n                  \"acq_func\":\"LCB\",\n```", "```py\n                  \"acq_optimizer\":\"auto\",\n```", "```py\n                  \"n_jobs\":-1,\n```", "```py\n                  \"random_state\":0,\n```", "```py\n                  \"acq_func_kwargs\": {\"kappa\":1.96}\n```", "```py\n                  },\n```", "```py\nrandom_state = 0,\n```", "```py\nscoring = 'f1', cv=5, \n```", "```py\nn_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```", "```py\nfrom skopt import BayesSearchCV\n```", "```py\nclf = BayesSearchCV(pipe, hyperparameter_space, n_iter=50,\n```", "```py\noptimizer_kwargs={\"base_estimator\":\"GBRT\",\n```", "```py\n                  \"n_initial_points\":10,\n```", "```py\n                  \"initial_point_generator\":\"random\",\n```", "```py\n                  \"acq_func\":\"LCB\",\n```", "```py\n                  \"acq_optimizer\":\"auto\",\n```", "```py\n                  \"n_jobs\":-1,\n```", "```py\n                  \"random_state\":0,\n```", "```py\n                  \"acq_func_kwargs\": {\"kappa\":1.96}\n```", "```py\n                  },\n```", "```py\nrandom_state = 0,\n```", "```py\nscoring = 'f1', cv=5, \n```", "```py\nn_jobs=-1, refit = True)\n```", "```py\nclf.fit(X_train_full, y_train)\n```", "```py\nprint(clf.best_params_, clf.best_score_)\n```", "```py\nprint(clf.score(X_test_full, y_test))\n```"]