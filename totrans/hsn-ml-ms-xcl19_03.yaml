- en: Hands-On Examples of Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised learning is the simplest way of teaching a model about how the world
    looks. Showing how a given combination of input variables leads to a certain output,
    that is, using labeled data, makes it possible for a computer to predict the output
    for another similar dataset that it has never seen. Unsupervised learning deals
    with finding patterns and useful insights into non-labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: We will study different types of machine learning models, trying to understand
    the details and actually performing the necessary calculations so that the inner
    workings of these models are clear and reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding supervised learning with multiple linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding supervised learning with decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding unsupervised learning with clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no technical requirements for this chapter. We just need to input
    the values shown in the tables within each section in an Excel sheet in order
    to follow the explanation closely.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding supervised learning with multiple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we followed an example of linear regression using two
    variables. It is interesting to see how we can apply regression to more than two
    variables (called **multiple linear regression**) and extract useful information
    from the results.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you are asked to test whether there exists a hidden policy of gender
    discrimination in a company. You could be working for a law firm that is leading
    a trial against this company, and they need data-based evidence to back up their
    claim.
  prefs: []
  type: TYPE_NORMAL
- en: 'You would start by taking a sample of the company''s payroll, including several
    variables that describe each employee and the last salary increase amount. The
    following screenshot shows a set of values after they''ve been entered in an Excel
    worksheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c0631ca-5c1a-4395-8e26-09a4274b0473.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are four numerical features in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ID`: The employee identification, which is not relevant to our analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Score`: The result of the last employee''s performance evaluation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Years in company`: Years that the employee has worked in the company'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Salary increase`: Amount in dollars of the last salary increase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The remaining two are categorical:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Gender`: Male (`M`) or Female (`F`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Division`: In which part of the company the employee works'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Categorical values need to be encoded before being used in a model. The final
    data table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a60e2c4-eb38-4adf-9357-2283b2bcc6a2.png)'
  prefs: []
  type: TYPE_IMG
- en: The one-hot encoding is easily obtained by applying standard Excel functions.
    Assuming *B2* is the first cell containing the gender classification, we can enter *=IF(B2="F";1;0) *in
    cell *B21* and copy this value to all cells down to *B37*.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on which character is defined in the Windows list separator option,
    you should either use a comma (*,*) or a semi-colon (*;*) in formulas.
  prefs: []
  type: TYPE_NORMAL
- en: To encode the employee's division, we use one-hot encoding (refer to [Chapter
    1](b0dde0bb-32ef-4535-9e19-7999e8e9a631.xhtml), *Implementing Machine Learning
    Algorithms*, for a detailed explanation) and create three new variables: `IsProduction?`,
    `IsResearch?`*,* and `IsSales?`*.* We can use Excel functions to calculate the
    encoding if *E2* is the first row containing the `Division` data, then we can
    use the *=IF(E2="Production";1;0)*, *=IF(E2="Research";1;0),* and *=IF(E2="Sales";1;0)* functions in
    cells *E21*, *F21,* and *G21*, respectively, and then copy them column-wise down
    to cells *E37*, *F37,* and *G37*.
  prefs: []
  type: TYPE_NORMAL
- en: Before trying to use regression on the full dataset, we can try some feature
    engineering. Let's see how well we can predict the salary increase based on which `Division` each
    employee works. This will give us an idea of how much the `Salary Increase`target
    variable correlates with `Division` (there will be more details about correlations
    between variables in [Chapter 5](0da64bd8-0bc9-491b-875c-7ec7c35c6165.xhtml),
    *Correlations and the Importance of Variables*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s follow some simple steps to use the built-in regression tool:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on Data Analysis, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/40602c2b-0444-4e3e-b685-20b98aa69cb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select Regression, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/423d05a3-7725-42f1-816e-ff01211cc34e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the Input Y Range, select the `Salary` data and as the Input X Range, select
    the three `Division` columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/11bab1bf-9e2d-4ccb-8054-15419abf959b.png)'
  prefs: []
  type: TYPE_IMG
- en: The results show *R**²** = 0.1*, meaning that only 10% of the salary increase
    is related or can be explained by the fact that the employee belongs to a given
    division. We can therefore discard these columns as input and concentrate on the
    rest.
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the regression, now choosing the X values as the columns `Gender`,
    `Score`, and `Years in company`.
  prefs: []
  type: TYPE_NORMAL
- en: The results are quite different now, with R² close to 0.85, meaning that 85%
    of the salary increase values are explained by the chosen variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'How important is `Gender`? Taking a look at the P-value coefficients that Excel
    gives us, in the following table, we can see that, according to the P-value associated
    with the input variables, the most important one is gender, followed by the score
    and the number of years in the company. It is then clear that gender plays an
    important role when deciding a salary increase, and we have evidence to prove
    that the company policy is not gender neutral:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Coefficients** | **P-value** |'
  prefs: []
  type: TYPE_TB
- en: '| **Intercept** | 141.72775 | 0.083481944 |'
  prefs: []
  type: TYPE_TB
- en: '| **Gender** | -221.9209346 | 6.47796E-05 |'
  prefs: []
  type: TYPE_TB
- en: '| **Score** | 2.697512241 | 0.004201513 |'
  prefs: []
  type: TYPE_TB
- en: '| **Years in company** | 8.118352407 | 0.332588988 |'
  prefs: []
  type: TYPE_TB
- en: 'The output results of the regression tell us how well we can explain the data
    sample, but cannot give us an accurate measure of how the model will predict a
    salary increase. To explore this, we should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain a different sample of the payroll (in our case, we could generate new
    data by hand)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the coefficients in the previous table to build an expression and calculate
    the predicted salary increase given the input variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare the predicted and real values using root mean square error, as explained
    in [Chapter 1](b0dde0bb-32ef-4535-9e19-7999e8e9a631.xhtml), *Implementing Machine
    Learning Algorithms*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see if you can finish this exercise; I am hoping that the basic information
    that's been provided to you carry this out has been understood.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have shown how to perform a multiple linear regression in data to extract
    interesting insights from them. Let''s continue with another important machine
    learning model: decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding supervised learning with decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision tree algorithm uses a tree-like model of decisions. Its name is
    derived from the graphical representation of the cascading process that partitions
    the records. The algorithm chooses the input variables that better split the dataset
    into subsets that are more pure in terms of the target variable, ideally a subset
    that contains only one value of this variable. Decision trees are some of the
    most widely used and easy to understand classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of the tree algorithm calculation is a set of simple rules that
    explain which values or intervals of the input values split the original data
    better. The fact that the results and the path followed to get to them can be
    clearly shown gives decision trees an advantage over other algorithms. **Explainability**
    is a serious problem for some machine learning and artificial intelligence systems
    – which are mostly used as black boxes – and is a study subject in itself.
  prefs: []
  type: TYPE_NORMAL
- en: In complex problems, we need to decide when to stop the tree development. A
    large number of features can lead to a very large and complex tree, so the number
    of branches and the length of the tree are usually limited by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is a very important concept in decision trees and the way of quantifying
    the purity of each subsample. It measures the amount of information contained
    in each leaf of the tree. The lower the entropy, the larger the amount of information.
    Zero entropy means that a subset contains only one value of the target variable,
    while a value of one represents a subset that contains the same amount of both
    values. This concept will be explained later with examples.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is an indicator of how messy your data is.
  prefs: []
  type: TYPE_NORMAL
- en: Using the entropy that's calculated in every step, the algorithm chooses the
    best variable to split the data and recursively repeats the same procedure. The
    user can decide how to stop the calculation, either when all subsets have an entropy
    of zero, when there are no more features to split by, or a minimum entropy level.
  prefs: []
  type: TYPE_NORMAL
- en: The input features that are best suited for use in a decision tree are the categorical
    ones. In case of a continuous, numerical variable, it should be first converted
    into categories by dividing it into ranges; for example, A > 0.5 would be A1 and
    A ≤ 0.5 would be A2.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example that explains the concept of the decision tree algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding whether to train outdoors depending on the weather
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose we have historical data on the decisions made by an experienced
    football trainer about training outdoors (outside the gym) or not with her team,
    including the weather conditions on the days when the decisions were made.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical dataset could look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/978298f2-c1f8-4e2d-8e85-1de0fc8e7e74.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset was specifically created for this example and, of course, might
    not represent any real decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the target variable is `Train outside` and the rest of the
    variables are the model features.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the data table, a possible decision tree would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e2f36c5-6565-43d4-9a5a-d1b35c0e7724.png)'
  prefs: []
  type: TYPE_IMG
- en: We choose to start splitting the data by the value of the **Outlook **feature.We
    can see that if the value is **Overcast**,then the decision to train outside is
    always **Yes**and does not depend on the values of the other features. **Sunny**
    and **Rainy** can be further split to get an answer.
  prefs: []
  type: TYPE_NORMAL
- en: How can we decide which feature to use first and how to continue? We will use
    the value of the entropy*,* measuring how much its value changes when considering
    different input features.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy of the target variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The definition of entropy when looking at a single attribute is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f157814c-737c-458d-9303-4c13687692c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *c* is the total number of possible values of the feature *f*, *p[i]*
    is the probability of each value, and *log*[*2*]*(p[i])*is the base two logarithm
    of the same probability. The calculation details are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to count the number of Yes and No decisions in the dataset. In our
    simple example, they can be counted by hand, but if the dataset is larger, we
    can use Excel functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*COUNTIF(F2:F15;"Yes")* and *COUNTIF(F2:F15;"No")*'
  prefs: []
  type: TYPE_NORMAL
- en: We then get the calculation that *Yes = 9* and *No = 5*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When applying the entropy formula to the target variable, we get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4efa2635-4bc5-4980-89eb-b009696db614.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the probabilities are calculated as the number of *Yes* (*9*) or *No* (*5*)
    over the total number (*14*).
  prefs: []
  type: TYPE_NORMAL
- en: This calculation can also be easily performed in the Excel sheet using *I3/(I3+J3)*LOG(I3/(I3+J3);2)-J3/(I3+J3)*LOG(J3/(I3+J3);2)*
    with *I3=9* and *J3=5*.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy of each feature with respect to the target variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The entropy of two variables *f*[*1* ]and *f[2]* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b6434d3-d2fb-4a52-820d-e3b14870dc30.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *v* represents each possible value of *f[2]*, *P(v)* is the probability
    of each value, and *S(v)* was defined in the previous equation.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s build a frequency table, which is the usual way of counting the total
    number of combinations between variables. In our case, we use it to decide which
    variable choice leads to a larger reduction of the entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: Count the different combinations of feature values, taking each feature compared
    to the `Train outside` target variable. You can count them manually in this particular
    example, but it is useful to have a general method to do this in case we are working
    with a larger dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To count the number of feature combinations, we start by concatenating the values
    in the data table in pairs. For example, *CONCATENATE(B2;"_";F2)* gives us `Hot_No`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we copy the formula down to complete the total number of rows, we get all
    possible combinations of the `Temperature` and `Train outside` variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we repeat the same calculation with the rest of the features, the results
    will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6c8884f8-2fee-4191-aa34-c46ccf88f197.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create pivot tables to count the number of unique values in each column, that
    is, the number of unique combinations. This can be done by selecting the full
    range in the column, right-clicking anywhere in the selection, and left-clicking
    on Quick Analysis. The following dialogue will pop up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/05e68fd4-4daf-4008-b468-7fe4b4ea5b14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select Tables | PivotTableto create a table like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/df9d5cee-2ee6-4af7-b19a-3e7fb08a59d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Repeat the same procedure with all columns and build all frequency tables and
    the two-variable entropy. The resulting tables and the entropy calculations are
    shown in the following subsection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Entropy calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The frequency table for the combination Outlook-Train outside is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Outlook** | **Train outside** |'
  prefs: []
  type: TYPE_TB
- en: '|  | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Sunny | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Overcast | 4 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Rainy | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Using these values, we get the entropy of two variables, as shown here in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/376672c9-2d09-4c28-8fab-67e9601dc316.png)'
  prefs: []
  type: TYPE_IMG
- en: '*p(Sunny).S(Sunny)+p(Overcast).S(Overcast)+p(Rainy)*S(Rainy)=*'
  prefs: []
  type: TYPE_NORMAL
- en: '*5/14*(-3/5*log2(3/5)-2/5*log2(2/5)) +*'
  prefs: []
  type: TYPE_NORMAL
- en: '*4/14*(-4/4*log2(4/4)-0/4*log2(0/4))+*'
  prefs: []
  type: TYPE_NORMAL
- en: '*5/14*(-2/5*log2(2/5)-3/5*log2(3/5))=*'
  prefs: []
  type: TYPE_NORMAL
- en: '*0.693*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *p(Sunny) = (#Yes+#No)/Total entries = (2+3)/14, p(Overcast) = (#Yes+#No)/Total
    entries = (4+0)/14,* and *p(Rainy) = (#Yes+#No)/Total entries = (2+3)/14*. The
    entropy values *S(v)* are calculated using the corresponding probabilities, that
    is, *#Yes* or *#No* over the total *#Yes+#No*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The frequency table for the combination Temperature-Train outside is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Temperature** |  | **Train Outside** |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Yes** | **No** |'
  prefs: []
  type: TYPE_TB
- en: '| **Hot** | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mild** | 4 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cool** | 3 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Using these values and an analogous calculation, the entropy is shown in detail
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eff7388b-ac6d-4b8c-95fa-5d057e97fd1d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*p(Hot).S(Hot)+p(Mild).S(Mild)+p(Cool)*S(Cool)=*'
  prefs: []
  type: TYPE_NORMAL
- en: '*4/14*(-2/4*log[2](2/4)-2/4*log[2](2/4)) +*'
  prefs: []
  type: TYPE_NORMAL
- en: '*6/14*(-4/6*log[2](4/6)-2/6*log[2](2/6))+*'
  prefs: []
  type: TYPE_NORMAL
- en: '*4/14*(-3/4*log[2](3/4)-1/4*log[2](1/4)) =*'
  prefs: []
  type: TYPE_NORMAL
- en: '*0,911*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The frequency table for the combination Humidity-Train outside is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Humidity** |  | **Train Outside** |'
  prefs: []
  type: TYPE_TB
- en: '| **Yes** | **No** |'
  prefs: []
  type: TYPE_TB
- en: '| **High** | 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Normal** | 6 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Using these values, we get the entropy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91f2ee7e-5fee-497d-8b8a-3a42b9d21d98.png)'
  prefs: []
  type: TYPE_IMG
- en: '*p(High).S(High)+p(Normal).S(Normal)=*'
  prefs: []
  type: TYPE_NORMAL
- en: '*7/14*(-3/7*log[2](3/7)-4/7*log[2](4/7)) +*'
  prefs: []
  type: TYPE_NORMAL
- en: '*7/14*(-6/7*log[2](6/7)-1/7*log[2](1/7))=*'
  prefs: []
  type: TYPE_NORMAL
- en: '*0,788*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The frequency table for the combination Windy-Train outside is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Windy** |  | **Train Outside** |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Yes** | **No** |'
  prefs: []
  type: TYPE_TB
- en: '| **TRUE** | 6 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **FALSE** | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: 'Using these values, we get the entropy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae116dfd-9968-47ba-ad17-fbd5ec08073e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*p(True).S(True)+p(False).S(False)=*'
  prefs: []
  type: TYPE_NORMAL
- en: '*8/14*(-6/8*log[2](6/8)-2/8*log[2](2/8)) +*'
  prefs: []
  type: TYPE_NORMAL
- en: '*6/14*(-3/6*log[2](3/6)-3/6*log[2](3/6))*'
  prefs: []
  type: TYPE_NORMAL
- en: '*=0,892*'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the entropy differences (information gain)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To know which variable to choose for the first split, we calculate the information
    gain *G* when going from the original data to the corresponding subset as the
    difference between the entropy values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35fbfbaa-4a17-4a9b-9029-0e8732cf6118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *S(f[1])* is the entropy of the target variable and *S(f[1],f2)* is the
    entropy of each feature with respect to the target variable. The entropy values
    were calculated in the previous subsections, so we use them here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we choose *Outlook* as the first variable to split the tree, the information
    gain is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G(Train outside,Outlook) = S(Train outside) - S(Train outside,Outlook)* * 
                                                   = 0.94-0.693=0.247*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we choose *Temperature*, the information gain is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G(Train outside,Temperature) = S(Train outside) - S(Train outside,Temperature)*
    *                                                           = 0.94-0.911=0.029*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we choose *Humidity*, the information gain is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G(Train outside,Humidity) = S(Train outside) - S(Train outside,Humidity)*
    *                                                     = 0.94-0.788=0.152*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, choosing *Windy* gives the following information gain:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G(Train outside,Windy) = S(Train outside) - S(Train outside,Windy)'
  prefs: []
  type: TYPE_NORMAL
- en: = 0.94-0.892=0.048*
  prefs: []
  type: TYPE_NORMAL
- en: All these calculations are easily performed in a worksheet using Excel formulas.
  prefs: []
  type: TYPE_NORMAL
- en: The variable to choose for the first splitting of the tree is the one showing
    the largest information gain, that is, *Outlook*. If we do this, we will notice
    that one of the resulting subsets after the splitting has zero entropy, so we
    don't need to split it further.
  prefs: []
  type: TYPE_NORMAL
- en: 'To continue building the tree following a similar procedure, the steps to take
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate *S(Sunny)*, *S(Sunny,Temperature)*, *S(Sunny,Humidity),* and *S(Sunny,Windy).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate *G**(Sunny,Temperature)*, *G(Sunny,Humidity),* and *G(Sunny,Windy).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The larger value will tell us what feature to use to split *Sunny.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate other gains, using *S(Rainy)*, *S(Rainy,Temperature)*, *S(Rainy,Humidity),*
    and *S(Rainy,Windy).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The larger value will tell us what feature to use to split *Rainy.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue iterating until there are no features left to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we will see later in this book, trees are never built by hand. It is important
    to understand how they work and which calculations are involved. Using Excel,
    it is easy to follow the full process and each step. Following the same principle,
    we will work through an unsupervised learning example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding unsupervised learning with clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a statistical method that attempts to group the points in a dataset
    according to a distance measure, usually the Euclidean distance, which calculates
    the root of the squared differences between coordinates of a pair of points. To
    put this simply, those points that are classified within the same cluster are
    closer (in terms of the distance defined) to each other than they are to the points
    belonging to other clusters. At the same time, the larger the distance between
    two clusters, the better we can distinguish them. This is similar to saying that
    we try to build groups in which members are more alike and are more different
    to members of other groups.
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the most important part of a clustering algorithm is to define
    and calculate the distance between two given points and to iteratively assign
    the points to the defined clusters, until there is no change in the cluster composition.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few points to consider before trying a clustering analysis. Not
    every type of data is adequate for clustering. For example, we cannot use binary
    data since it is not possible to define distances. The values are either `1` or
    `0`, and there is no value in-between. This excludes the type of data generated
    by one-hot encoding. Only data that shows some ordering or scale is useful for
    clustering. Even if the data values are real (such as, for example, a client's
    expenditure amounts or annual income), it is better to group them in a scale of
    ranges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of clustering use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic grouping of IT alerts to assign priorities and solve them accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of customer communication through different channels (segmentation
    in time periods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Criminal profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban mobility analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud detection (looking for outliers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of athletes' performances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crime analysis by geography
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delivery logistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification of documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's go through some examples that explains the concept of clustering
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping customers by monthly purchase amount
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now follow the full calculation and analysis necessary to generate clusters
    from customer data. This is a simplified version of what would be a typical clustering
    algorithm, showing all the steps but reducing the number of iterations to make
    it understandable. Clustering is usually done automatically, but it is important
    to understand the logic behind the calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset to be used contains the total monthly amount spent by 20 different
    customers in an online store, corresponding to `May`, `June`, and `July` in a
    given year. Once typed in an Excel sheet, the data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1b1a2e3-cefe-4bb3-8fbd-bdc9e5a4e9e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For each month, we can calculate the main parameters that describe the data:
    minimum, maximum, median, and average:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **May** | **June** | **July** |'
  prefs: []
  type: TYPE_TB
- en: '| **Minimum** | 316.89 | 500.66 | 185.63 |'
  prefs: []
  type: TYPE_TB
- en: '| **Maximum** | 11889.66 | 12214.41 | 11982.64 |'
  prefs: []
  type: TYPE_TB
- en: '| **Median** | 8388.63 | 8156.16 | 7708.27 |'
  prefs: []
  type: TYPE_TB
- en: '| **Average** | 6182.20 | 6229.24 | 6227.81 |'
  prefs: []
  type: TYPE_TB
- en: We simply use the Excel built-in functions *MIN()*, *MAX()*, *MEDIAN()*, and
    *AVERAGE()*, including the full range of each column.
  prefs: []
  type: TYPE_NORMAL
- en: In cluster analysis, it is useful to *normalize* the dataset, that is, to convert
    all values so that they fall in to the interval [0,1]. This helps us deal with
    the **outlier** data points, whose value is very different from the majority of
    points, which can affect the cluster definition. After normalization, those points
    are not so far away from the rest and can be easily grouped. Clearly, if the goal
    of the clustering analysis is to find those outliers, it is a better idea to leave
    the dataset as it is and highlight the difference between the outliers and the
    rest of the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to normalize the data is to divide each value by the maximum
    in the corresponding column. To do this, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In cell *G2,* type `=B2/$B$24`. We are assuming that *B2* is the first value
    in the `May` column and that the maximum value is in *B24*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy this formula into the whole column. Recall that adding *$* to the cell
    ID in Excel fixes that value when copying the contents into another cell. The
    normalized table is then as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/eea879ef-2c9f-4b03-a40d-fc677dd1804d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a moment to visualize the data and understand it a little more.
    If we take the columns in pairs, then it is possible to generate scatter plots
    and try to find clusters visually by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select `May` and `June` data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Insert | Scatter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting diagram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11c7e4a0-ca9a-4662-9d35-a7249965949a.png)'
  prefs: []
  type: TYPE_IMG
- en: Three clusters can be identified, and are circled in the preceding screenshot.
    They correspond to groups of customers who spend similar amounts of money monthly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Doing the same with `May` and `July`, we get the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c7005221-b01b-49e0-864e-ca94affdd0da.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we could either say that there are two big clusters or that one
    of them can be further split in two. The separation is not so clear and the choice
    will depend on other variables (remember that the best model is always the one
    that best suits the business' needs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we plot `June` and `July`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b4ef4a9d-7137-44d7-a37e-37f4a22b7078.png)'
  prefs: []
  type: TYPE_IMG
- en: The division of clusters seems even more clear here, and we can circle three
    sets of points.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to consider all three months at the same time? There is an
    iterative process to accomplish this, which is the base of the clustering algorithm
    known as **K-means**. Let''s follow the steps of this algorithm in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Decide how many clusters you want to split the data into. This is not an easy
    decision in general. It will strongly depend on the dataset and, in some cases,
    will be a matter of testing different values until you get a number of clusters
    that gives useful insights on the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Taking into account the previous visual analysis, we decide to choose three
    as the number of clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take any three points as the center of the clusters. The choice of the starting
    points is not relevant, as we will repeat the whole process until there is no
    change in the resulting cluster members. We then choose the first three points
    in the list, as shown in the following table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|  | **May** | **June** | **July** |'
  prefs: []
  type: TYPE_TB
- en: '| **Random1** | 0.055568104 | 0.043735522 | 0.15581034 |'
  prefs: []
  type: TYPE_TB
- en: '| **Random2** | 0.07079235 | 0.067065974 | 0.079319396 |'
  prefs: []
  type: TYPE_TB
- en: '| **Random3** | 0.026652635 | 0.040988882 | 0.171590079 |'
  prefs: []
  type: TYPE_TB
- en: 'Find the points that are closer to them, computing the distance from all other
    points to these cluster centers. The Euclidean distance between two points, *P[1]
    =(x[1],y[1]**,z[1])* and *P[2] = (x[2],y[2]**,z[2]),* is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/93758e1f-101e-412e-bf3e-023268aa23e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Use Excel's built-in *SUMXMY2([array1];[array2})* function to calculate *(DE)²*
    for each point with respect to the cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each data point, you will get three distance values. Pick the smallest
    one to decide which cluster the point belongs to. For example, for customer ID
    = 4, we get the following information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **D1** | **D2** | **D3** | **Cluster** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.019689391 | 0.004847815 | 0.025218271 | 2 |'
  prefs: []
  type: TYPE_TB
- en: Here, *D1*, *D1,* and *D3* are the distances from the point to the respective
    cluster centers. The smallest distance tells us that this point belongs to cluster
    two. As an example, *D1* for customer ID = 4 is calculated as *=SUMXMY2(B5:D5;$B$23:$D$23)*,
    assuming that Random1 `May` and Random1 `June` are in cells *$B$23* and *$D$23*,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete resulting data table is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9934f1dc-2e42-47c0-aadb-b636e379059c.png)'
  prefs: []
  type: TYPE_IMG
- en: The last column can be created by typing the following formula into the first
    row and then copying it down: *=IF(E2=MIN(E2:G2);1;IF(F2=MIN(E2:G2);2;3))*.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the table, our first result is really unbalanced. Most of the
    points fall in cluster one, a few in cluster two, and only one in cluster three.
    We need to continue the calculations and see how the result evolves. Follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of choosing random points, we will now use the mean values of the clusters
    we obtained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Order the table by cluster number. The resulting table is a little different
    now, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e0519ae9-bd12-4c5b-947f-4d17857df7ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use the *MEAN()* function to calculate the average value per cluster for each
    month. You should get the same results that are shown in the following table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|  | **May** | **June** | **July** |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean1** | 0.618762809 | 0.605805489 | 0.618056642 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean2** | 0.157477363 | 0.155314048 | 0.111411008 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean3** | 0.026652635 | 0.040988882 | 0.171590079 |'
  prefs: []
  type: TYPE_TB
- en: As an example, **Mean1** corresponding to `May` is calculated as *AVERAGE(B2:B17)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same formulas as before and calculating the distances from all other
    points to the mean values, you get a table similar to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4f669d09-d40e-4b33-8f05-dab4a8ce8214.png)'
  prefs: []
  type: TYPE_IMG
- en: After the second iteration, a few more points, which, when moved away from cluster
    one, now belong to cluster two and three.
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeat the calculation one more time. The new mean values, according to the
    preceding table, are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|  | **May** | **June** | **July** |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean1** | 0.843481911 | 0.832289469 | 0.810799822 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean2** | 0.292624962 | 0.280240044 | 0.310753303 |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean3** | 0.052180197 | 0.048870976 | 0.105552835 |'
  prefs: []
  type: TYPE_TB
- en: 'The table containing the distances and cluster numbers can be given as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8722ed23-eb6b-4609-b9d8-b8fe2d37ff99.png)'
  prefs: []
  type: TYPE_IMG
- en: After the third iteration, only one point changed cluster, from two to three;
    so, we are getting close to the final result. You should be able to perform one
    more iteration, following the same steps, proving that it does not change the
    clustering labels and meaning that the calculations converged to a stable number
    of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Real-life datasets might not converge so fast. What we have shown is a simplified
    example, good enough to show every step of the iteration, understand them, and
    get to a reasonable result. Clustering is not usually calculated manually, but
    performed by pre-built algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, you will learn how to import data from different sources
    to Excel, so you don't need to type in the values manually. This will give you
    a starting point to analyze real data, usually containing many more variables
    and values than the examples shown in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have described real life examples of supervised and unsupervised
    machine learning models that have been applied to solving problems. We covered
    multiple regression, decision trees, and clustering. We have also shown how to
    choose and transform the input variables or features to be ingested by the models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter only shows the basic principles of each algorithm. In real data
    analysis and prediction using machine learning, models are already programmed
    and can be used as black boxes. It is, therefore, extremely important to understand
    the basics of each model and know whether we are using it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will focus on how to extract the data from different
    sources, transform it according to our needs, and use previously built models
    for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why is it important to encode categorical features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different ways to stop a decision tree calculation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Temperature_hot` has an entropy value of one in the example. Why?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following the diagram of the decision tree at the beginning of the *Understanding
    supervised learning with decision trees* section, what would be the path to decide
    whether or not to train outside? Consider using `IF` statements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Would the cluster distribution change if we choose different starting points?
    You can read about this in the recommended articles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the clustering that's obtained with iterative analysis the same as the one
    that's determined visually? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*How to Interpret Regression Analysis Results: P-values and Coefficients*: [http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients](http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Teaching Decision Tree Classification Using Microsoft Excel INFORMS Transactions
    on Education* 11(3), pp. 123–131, by Kaan Ataman, George Kulick, Thaddeus Sim:
    [https://pubsonline.informs.org/doi/10.1287/ited.1100.0060](https://pubsonline.informs.org/doi/10.1287/ited.1100.0060)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Review of K-mean Algorithm*, International Journal of Engineering Trends
    and Technology (IJETT) – Volume 4 Issue 7- July 2013: [http://www.ijettjournal.org/volume-4/issue-7/IJETT-V4I7P139.pdf](http://www.ijettjournal.org/volume-4/issue-7/IJETT-V4I7P139.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
