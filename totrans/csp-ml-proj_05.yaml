- en: Fair Value of House and Property
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to expand our knowledge and skills in building
    regression **machine learning** (**ML**) models in C#. In the last chapter, we
    built a linear regression and linear support vector machine model on a foreign
    exchange rate dataset, where all the features were continuous variables. However,
    we are going to be dealing with a more complex dataset, where some features are
    categorical variables and some others are continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using a house prices dataset that contains numerous
    attributes of houses with mixed variable types. Using this data, we will start
    looking at the two common types of categorical variables (ordinal versus non-ordinal)
    and the distributions of some of the categorical variables in the housing dataset.
    We will also look at the distributions of some of the continuous variables in
    the dataset and the benefits of using log transformations for variables that show
    skewed distributions. Then, we are going to learn how to encode and engineer such
    categorical features so that we can fit machine learning models. Unlike the last
    chapter, where we explored the basics of **Support Vector Machine** (**SVM**),
    we are going to apply different Kernel methods for our SVM models and see how
    it affects the model performances.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the last chapter, we will be using **root mean squared error** (**RMSE**),
    R², and a plot of actual versus predicted values to evaluate the performances
    of our ML models. By the end of this chapter, you will have a better understanding
    of how to handle categorical variables, how to encode and engineer such features
    for regression models, how to apply various kernel methods for building SVM models,
    and how to build models that predict the fair values of houses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition for the fair value of house/property project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for categorical versus continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering and encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression versus Support Vector Machine with kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model validations using RMSE, R², and actual versus predicted plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start this chapter by understanding exactly what ML models we are going
    to build. When you are looking for a house or a property to purchase, you consider
    numerous attributes of those houses or properties that you look at. You might
    be looking at the number of bedrooms and bathrooms, how many cars you can park
    in your garage, the neighborhoods, the materials or finishes of the house, and
    so forth. All of these attributes of a house or property go into how you decide
    the price you want to pay for the given property or how you negotiate the price
    with the seller. However, it is very difficult to understand and estimate what
    the fair value of a property is. By having a model that predicts the fair value
    or the final price of each property, you can make better informed decisions when
    you are negotiating with the seller.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build such models for fair value of a house predictions, we are
    going to use a dataset that contains 79 explanatory variables that cover almost
    all attributes of residential homes in Ames, Iowa, U.S.A. and their final sale
    prices from 2006 to 2010\. This dataset was compiled by Dean De Cock ([https://ww2.amstat.org/publications/jse/v19n3/decock.pdf](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf))
    at the Truman State University and can be downloaded from this link: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).
    With this data, we are going to build features that contain information about
    square footage or sizes of different parts of the houses, the styles and materials
    used for the houses, the conditions and finishes of different parts of the houses,
    and various other attributes that further describe the information of each house.
    Using these features, we are going to explore different regression machine learning
    models, such as linear regression, Linear Support Vector Machine, and **Support
    Vector Machines** (**SVMs**) with polynomial and Gaussian kernels. Then, we will
    evaluate these models by looking at RMSE, R², and a plot of actual versus predicted
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our problem definition for the fair value of house and property
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem? We need a regression model that predicts the fair values
    of residential homes in Ames, Iowa, U.S.A., so that we can understand and make
    better informed decisions when purchasing houses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it a problem? Due to the complex nature and numerous moving parts in
    deciding the fair value of a house or a property, it is advantageous to have a
    machine learning model that can predict and inform home buyers what the expected
    values of houses that they are looking at are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some of the approaches to solving this problem? We are going to use
    a pre-compiled dataset that contains 79 explanatory variables that contain information
    of residential homes in Ames, Iowa, U.S.A., and build and encode features of mixed
    types (both categorical and continuous). Then, we will explore linear regression
    and support vector machines with different Kernels for making predictions of fair
    values of houses. We will evaluate the model candidates by looking at RMSE, R²,
    and an actual versus predicted values plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the success criteria? As we want our predictions of house prices to
    be as close to the actual house sale prices as possible, we want to gain as low
    an RMSE as possible, without hurting our goodness of fit measure, R², and the
    plot of actual versus predicted values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical versus continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's start looking at the actual dataset. You can follow this link: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    and download the `train.csv` and `data_description.txt` files. We are going to
    build models using the `train.csv` file, and the `data_description.txt` file will
    help us better understand the structure of the dataset, especially concerning
    the categorical variables we have.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the train data file and the description file, you can easily
    find that there are some variables with certain names or codes that represent
    specific types of each house's attributes. For example, the `Foundation` variable
    can take one of the values among `BrkTil`, `CBlock`, `PConc`, `Slab`, `Stone`,
    and `Wood`, where each of those values or codes represents the type of foundation
    that a house is built with—Brick and Tile, Cinder Block, Poured Contrete, Slab, Stone,
    and Wood respectively. On the other hand, if you look at the `TotalBsmtSF` variable
    in the data, you can see that it can take any numerical values and the values
    are continuous. As mentioned previously, this dataset contains mixed types of
    variables and we need to approach carefully when we are dealing with a dataset
    with both categorical and continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: Non-ordinal categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first look at some categorical variables and their distributions. The
    first house attribute that we are going to look at is the building type. The code
    to build a bar chart that shows the distributions of the building type is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, it will display a bar chart like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can tell from this bar chart, the majority of the building types in our
    dataset is 1Fam, which represents the *Single-family Detached* building type.
    The second most common building type is TwnhsE, which represents the *Townhouse
    End Unit* building type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at one more categorical variable, Lot Configuration (`LotConfig`
    field in the dataset). The code to build a bar chart for lot configuration distributions
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, it will display the following bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this bar chart, inside lot is the most common lot configuration
    in our dataset, and corner lot is the second most common log configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal categorical variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two categorical variables that we just looked at have no natural ordering.
    One type does not come before another or one type does not have more weight than
    another. However, there are some categorical variables that have natural ordering,
    and we call such categorical variables ordinal categorical variables. For example,
    when you rank a quality of a material from 1 to 10, where 10 represents the best
    and 1 represents the worst, there is a natural ordering. Let's look at some of
    the ordinal categorical variables in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first ordinal categorical variable that we are going to look at is the
    `OverallQual` attribute, which represents the overall material and finish of the
    house. The code to look at the distributions of this variable is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, it will display the following bar chart in order from
    10 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, there is a smaller number of houses in the *Very Excellent, *encoded
    as 10, or *Excellent*,encoded as 9, categories than there are in the *Above Average*,
    encoded as 6, or *Average* categories, encoded as 5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another ordinal categorical variable that we will be looking at is the `ExterQual`
    variable, which represents the exterior quality. The code to look at the distributions
    of this variable is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, it will display the following bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike the `OverallQual` variable, the `ExterQual` variable does not have numerical
    values for the ordering. In our dataset, it has one of the following values: `Ex`,
    `Gd`, `TA`, and `FA`, and these represent excellent, good, average/typical, and
    fair respectively. Although this variable does not have numerical values, it clearly
    has a natural ordering, where the excellent category (Ex) represents the best
    quality of material on the exterior and the good category (Gd) represents the
    second best quality of material on the exterior. In the feature engineering step,
    we will discuss how we can encode this type of variable for our future model building
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have so far looked at two types of categorical variables in our dataset.
    However, there is another type of variable in the dataset; the continuous variable.
    Unlike categorical variables, continuous variables have no limited number of values
    they can take. For example, square footage for basement area of a house can be
    any positive number. A house can have a 0 square foot basement area (or no basement)
    or a house can have a 1,000 square feet basement area. The first continuous variable
    that we are going to look at is `1stFlrSF`, which represents the first floor square
    feet. The following code shows how we can build a histogram for `1stFlrSF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, the following histogram will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'One thing that is obvious from this chart is that it has a long tail in the
    positive direction, or in other words, the distribution is right skewed. The skewness
    in the data can adversely affect us when we build ML models. One way to handle
    this skewness in the dataset is to apply some transformations. One frequently
    used transformation is the log transformation, where you take log values of a
    given variable. In this example, the following code shows how we can apply log
    transformation to the `1stFlrSF` variable and show a histogram for the transformed
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from this chart, the distribution looks more symmetric and closer
    to the bell shape that we are familiar with, compared to the previous histogram
    that we looked at for the same variable. Log transformation is frequently used
    to handle skewness in the dataset and make the distribution closer to the normal
    distribution. Let''s look at another continuous variable in our dataset. The following
    code is used to show the distribution of the `GarageArea` variable, which represents
    the size of the garage in square feet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to the previous case of `1stFlrSF`, it is also right skewed, although
    it seems the degree of skewness is less than `1stFlrSF`. We used the following
    code to apply log transformation for the `GarageArea` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following histogram chart will be displayed when you run this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the distribution looks closer to the normal distribution when the
    log transformation is applied to the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Target variable – sale price
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is one last variable we need to take a look at before we move onto the
    feature engineering step; the target variable. In this fair value of a house project,
    our target variable for predictions is `SalePrice`, which represents the final
    sale price in U.S. dollar amounts for each residential home sold in Ames, Iowa,
    U.S.A. from 2006 to 2010\. Since the sale price can take any positive numerical
    value, it is a continuous variable. Let''s first look at how we built a histogram
    for the sale price variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, the following histogram chart will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to the previous cases of continuous variables, the distribution of
    *SalePrice* has a long right tail and it''s heavily skewed to the right. This
    skewness often adversely affects the regression models, as some of those models,
    such as the linear regression model, assume that variables are normally distributed. 
    As discussed previously, we can fix this issue by applying log transformation.
    The following code shows how we log transformed the sale price variable and built
    a histogram chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following histogram for the log-transformed
    sale price variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.gif)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the distribution of the `SalePrice` variable looks much closer
    to the normal distribution. We are going to use this log-transformed `SalePrice`
    variable as the target variable for our future model building steps.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this data analysis step can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/DataAnalyzer.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have looked at our dataset and the distributions of the categorical,
    continuous, and target variables, let's start building features for our ML models.
    As we discussed previously, categorical variables in our dataset have certain
    string values to represent each type of variable. However, as it might already
    be clear to you, we cannot use string types to train our ML models. All the values
    of variables need to be numerical to be able to used for fitting the models. One
    way to handle categorical variables with multiple types or categories is to create
    dummy variables.
  prefs: []
  type: TYPE_NORMAL
- en: Dummy variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A dummy variable is a variable that takes a value of 0 or 1 to indicate whether
    a given category or type exists or not. For example, in the case of `BldgType`
    variable, where it has the five different categories `1Fam`, `2FmCon`, `Duplx`, `TwnhsE`,
    and `Twnhs`, we will create five dummy variables, where each dummy variable represents
    the existence or absence of each of those five categories in a given record. The
    following shows an example of how dummy variable encoding works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this example, the absence and existence of each category
    of the building types is encoded into a separate dummy variable as `0` or `1`.
    For example, for the record with the ID `1`, the building type is `1Fam` and this
    is encoded with the value 1 for the new variable, `BldgType_1Fam`, and 0 for the
    other four new variables, `BldgType_2fmCon`, `BldgType_Duplex`, `BldgType_TwnhsE`,
    and `BldgType_Twnhs`. On the other hand, for the record with the ID `10`, the
    building type is `2fmCon` and this is encoded with the value 1 for the variable `BldgType_2fmCon` and
    0 for the other four new variables, `BldgType_1Fam`, `BldgType_Duplex`, `BldgType_TwnhsE`,
    and `BldgType_Twnhs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, we created dummy variables for the following list of categorical
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows a method we wrote to create and encode dummy variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from line 8 of this method, we prefix the newly created dummy
    variables with the original categorical variable's names and append them with
    each category. For example, `BldgType` variables in the `1Fam` category will be
    encoded as `BldgType_1Fam`. Then, in line 15 of the `CreateCategories` method,
    we are encoding all the other values with 0s to indicate the absence of such categories
    in the given categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: Feature encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know which categorical variables to encode and have created a method
    for dummy variable encoding for those categorical variables, it is time to build
    a data frame with features and their values. Let''s first take a look at how we
    went about creating a features data frame in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have created and encoded all the features for our model training, we
    then export this `featuresDF` data frame into a `.csv` file. The following code
    shows how we export the data frame into a `.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We now have all the necessary features that we can use to start building machine
    learning models to predict fair values of houses. The full code for feature encoding
    and engineering can be found in this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/FeatureEngineering.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression versus SVM with kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need to do before we start training our machine learning
    models is to split our dataset into train and test sets. In this section, we will
    split the sample set into train and test sets by randomly sub-selecting and dividing
    the indexes at a pre-defined proportion. The code we used to split the dataset
    into train and test sets is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have these train and test data frames ready, we need to filter out
    unnecessary columns from the data frames, since the train and test data frames
    currently have values for columns, such as `SalePrice` and `Id`. Then, we will
    have to cast the two data frames into arrays of double arrays, which will be input
    to our learning algorithms. The code to filter out unwanted columns from the train
    and test data frames and to cast the two data frames into arrays of arrays is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first ML model we are going to explore for this chapter''s housing price
    prediction project is the linear regression model. You should already be familiar
    with building linear regression models in C# using the Accord.NET framework. We
    use the following code to build a linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between this chapter's linear regression model code and
    the previous chapter's code is the `IsRobust` parameter to the `OrdinaryLeastSquares`
    learning algorithm. As the name suggests, it makes the learning algorithm fit
    a more robust linear regression model, meaning it is less sensitive to outliers.
    When we have variables that are not normally distributed, as is the case for this
    project, it often causes problems when fitting a linear regression model as traditional
    linear regression models are sensitive to outliers from non-normal distributions.
    Setting this parameter to `true` helps resolve this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Linear SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second learning algorithm we are going to experiment with in this chapter
    is the linear SVM. The following code shows how we build a linear SVM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you might have noticed, and similar to the previous chapter, we used `LinearRegressionNewtonMethod`
    as a learning algorithm to fit a linear SVM.
  prefs: []
  type: TYPE_NORMAL
- en: SVM with a polynomial kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next model we are going to experiment with is an SVM with a polynomial kernel.
    We will not go into too much detail about the kernel methods, but simply put,
    kernels are functions of input feature variables that can transform and project
    the original variables into a new feature space that is more linearly separable.
    The polynomial kernel looks at the combinations of input features, on top of the
    original input features. These combinations of input feature variables are often
    called **interaction variables** in regression analysis. Using different kernel
    methods will make SVM models learn and behave differently with the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how you can build a SVM model with a polynomial kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `FanChenLinSupportVectorRegression` learning algorithm for
    a support vector machine with a polynomial kernel. In this example, we used a
    degree 3 polynomial, but you can experiment with different degrees. However, the
    higher the degrees are, the more likely it is to overfit to the training data.
    So, you will have to take cautious steps when you are using high degree polynomial
    kernels.
  prefs: []
  type: TYPE_NORMAL
- en: SVM with a Gaussian kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another commonly used kernel method is the Gaussian kernel. Simply put, the
    Gaussian kernel looks at the distance between the input feature variables and
    results in higher values for close or similar features and lower values for more
    distanced features. The Gaussian kernel can help transform and project a linearly
    inseparable dataset into a more linearly separable feature space and can improve
    the model performances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how you can build a SVM model with a Gaussian kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the case of the polynomial kernel, we used the `FanChenLinSupportVectorRegression`
    learning algorithm, but replaced the kernel with the `Gaussian` method.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed how we can use different kernel methods for SVMs so far. We
    will now compare the performances of these models on the housing price dataset.
    You can find the full code we used for building and evaluating models at this
    link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/Modeling.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Model validations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start looking into the performances of the linear regression and
    SVM models that we built in the previous section, let''s refresh our memory on
    the metrics and the diagnostics plot we discussed in the previous chapter. We
    are going to look at RMSE, R², and a plot of actual versus predicted values to
    evaluate the performances of our models. The code we are going to use throughout
    this section for model evaluation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The way we use this method for our models is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00077.gif)'
  prefs: []
  type: TYPE_IMG
- en: When looking at the values of the goodness of fit, R², and the RMSE values,
    the linear SVM model seems to have the best fit to the dataset, and the SVM model
    with the Gaussian kernel seems to have the second best fit to the dataset. Looking
    at this output, the SVM model with the polynomial kernel does not seem to work
    well for predicting the fair values of house prices. Now, let's look at the diagnostic
    plots to evaluate how well our models predict the house prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the diagnostic plot for the linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This diagnostic plot for the linear regression model looks good. Most of the
    points seem to be aligned on a diagonal line, which suggests that the linear regression
    model's predictions are well aligned with the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the diagnostic plot for the linear SVM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected from the previous R² metrics value, the goodness of fit for the
    linear SVM model looks good, even though there seems to be one prediction that
    is far off from the actual value. Most of the points seem to be aligned on a diagonal
    line, which suggests that the linear SVM model's predictions are well aligned
    with the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the diagnostic plot for the SVM model with the polynomial
    kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This diagnostic plot for the SVM model with the polynomial kernel suggests that
    the goodness of fit for this model is not so good. Most of the predictions lie
    on a straight line at around 12\. This is well aligned with the other metrics,
    where we have seen that RMSE and R² measures were the worst among the four models
    we tried.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the diagnostic plot for the SVM model with the Gaussian
    kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This diagnostic plot result for the SVM model with the Gaussian kernel is rather
    surprising. From the RMSE and R² measures, we expected the model fit using SVM
    with Gaussian kernel will be good. However, most of the predictions by this model
    are on a straight line, without showing any patterns of a diagonal line. Looking
    at this diagnostic plot, we cannot conclude that the model fit for the SVM model
    with the Gaussian kernel is good, even though the R² metrics showed a strong positive
    sign of the goodness of model fit.
  prefs: []
  type: TYPE_NORMAL
- en: By looking at both the metrics numbers and the diagnostic plots, we can conclude
    that the linear regression model and the linear SVM model seem to work the best
    for predicting the fair values of house prices. This project shows us a good example
    of the importance of looking at the diagnostic plots. Looking at and optimizing
    for single metrics might be tempting, but it is always better to evaluate models
    with more than one validation metric, and looking at diagnostic plots, such as
    the plot of actual values against predicted values, is especially helpful for
    regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we expanded our knowledge and skills regarding building regression
    models. We built prediction models using the sale price data of residential homes
    in Ames, Iowa, U.S.A. Unlike other chapters, we had a more complex dataset, where
    the variables had mixed types, categorical and continuous. We looked at the categorical
    variables, where there were no natural orderings (non-ordinal) and where there
    were natural orderings (ordinal) in the categories. We then looked at continuous
    variables, whose distributions had long right tails. We also discussed how we
    can use log transformations on such variables with high skewness in the data to
    mediate the skewness and make those variables' distributions closer to normal
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed how to handle categorical variables in our dataset. We learned
    how to create and encode dummy variables for each type of categorical variable.
    Using these features, we experimented with four different machine learning models—linear
    regression, linear support vector machine, SVM with a polynomial kernel, and SVM
    with a Gaussian kernel. We briefly discussed the purpose and usage of kernel methods
    and how they can be used for linearly inseparable datasets. Using RMSE, R², and
    the plot of the actual values against the predicted values, we evaluated the performances
    of those four models we built for predicting the fair values of house prices in
    Ames, Iowa, U.S.A. During our model validation step, we saw a case where the validation
    metrics results contradict with the diagnostic plots results and we have learned
    the importance of looking at more than one metric and diagnostic plots to be sure
    of our model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to switch gear again. So far, we have been
    learning how to use and build supervised learning algorithms. However, in the
    next chapter, we are going to learn unsupervised learning and more specifically
    clustering algorithms. We will discuss how to use clustering algorithms to gain
    insights on the customer segments using an online retail dataset.
  prefs: []
  type: TYPE_NORMAL
