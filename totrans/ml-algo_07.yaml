- en: Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to introduce another approach to classification
    using a family of algorithms called support vector machines. They can work with
    both linear and non-linear scenarios, allowing high performance in many different
    contexts. Together with neural networks, SVMs probably represent the best choice
    for many tasks where it's not easy to find out a good separating hyperplane. For
    example, for a long time, SVMs were the best choice for MNIST dataset classification,
    thanks to the fact that they can capture very high non-linear dynamics using a
    mathematical trick, without complex modifications in the algorithm. In the first
    part, we're going to discuss the basics of linear SVM, which then will be used
    for their non-linear extensions. We'll also discuss some techniques to control
    the number of parameters and, at the end, the application of support vector algorithms
    to regression problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Linear support vector machines
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a dataset of feature vectors we want to classify:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8ebf2e9-c219-44a4-bbfd-190e1364aaef.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, we assume it as a binary classification (in all the other cases,
    it''s possible to use automatically the one-versus-all strategy) and we set our
    class labels as -1 and 1:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56b6c321-1019-4144-84fc-e5bd104e136a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to find the best separating hyperplane, for which the equation
    is:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d48b7499-e5f1-4593-8180-bf8e3793c2fe.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure, there''s a bidimensional representation of such a
    hyperplane:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2582ae4-446f-426d-9a37-3edd703ee10b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'In this way, our classifier can be written as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01bbc949-57f8-4dd0-928e-de89da5ea280.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: 'In a realistic scenario, the two classes are normally separated by a margin
    with two boundaries where a few elements lie. Those elements are called **support
    vectors**. For a more generic mathematical expression, it''s preferable to renormalize
    our dataset so that the support vectors will lie on two hyperplanes with equations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed90dc11-fc80-474f-aace-973758eb6ecb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure, there''s an example with two support vectors. The
    dashed line is the original separating hyperplane:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93c4e356-cf65-4925-9c48-6591dcfaf927.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Our goal is to maximize the distance between these two boundary hyperplanes
    so as to reduce the probability of misclassification (which is higher when the
    distance is short, and there aren't two well-defined blobs as in the previous
    figure).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that the boundaries are parallel, the distance between them is
    defined by the length of the segment perpendicular to both and connecting two
    points:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37e1322e-e6d5-4fbd-83d5-54ba7c0982d1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'Considering the points as vectors, therefore, we have:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bde1f510-5fab-4167-8e97-cd3164a5b07f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'Now, considering the boundary hyperplane equations, we get:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1a13501-f5c0-41cc-a4c6-ebe3cedbefcd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'The first term of the last part is equal to -1, so we solve for *t*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d0a8ff9-e358-429c-a4a2-fb6e69eac926.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'The distance between *x[1]* and *x[2]* is the length of the segment *t*; hence
    we get:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e5930f6-b8c6-40a4-a67a-89d7ecf6d919.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'Now, considering all points of our dataset, we can impose the following constraint:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0637c2a1-8d11-4d8b-ac8d-0b2bff9bec9f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: This is guaranteed by using -1, 1 as class labels and boundary margins. The
    equality is true only for the support vectors, while for all the other points
    it will greater than 1\. It's important to consider that the model doesn't take
    into account vectors beyond this margin. In many cases, this can yield a very
    robust model, but in many datasets this can also be a strong limitation. In the
    next paragraph, we're going to use a trick to avoid this rigidness while keeping
    the same optimization technique.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can define the function to minimize in order to train a support
    vector machine:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1ea27ad-76a9-476f-8710-3cea0273df5d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: 'This can be further simplified (by removing the square root from the norm)
    in the following quadratic programming problem:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a21ed94-6589-4cd0-996f-0ef0dd4b6e76.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: scikit-learn implementation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to allow the model to have a more flexible separating hyperplane,
    all scikit-learn implementations are based on a simple variant that includes so-called
    **slack variables** in the function to minimize:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c0cd89f-8dc3-4a0e-8649-ef36e6c0df0a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the constraints become:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4198582-8d33-4738-9d96-8133c14f06e7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: The introduction of the slack variables allows us to create a flexible margin
    so that some vectors belonging to a class can also be found in the opposite part
    of the hyperspace and can be included in the model training. The strength of this
    flexibility can be set using the parameter *C*. Small values (close to zero) bring
    about very hard margins, while values greater than or equal to 1 allow more and
    more flexibility (also increasing the misclassification rate). The right choice
    of *C* is not immediate, but the best value can be found automatically by using
    a grid search as seen in the previous chapters. In our examples, we keep the default
    value of 1.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our first example is based on a linear SVM, as described in the previous section.
    We start by creating a dummy dataset with 500 vectors subdivided into two classes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the following figure, there's a plot of our dataset. Notice that some points
    overlap the two main blobs. For this reason, a positive *C* value is needed to
    allow the model to capture a more complex dynamic.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1707af43-728e-43d6-acd8-924b1cb35601.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'scikit-learn provides the `SVC` class, which is a very efficient implementation
    that can be used in most cases. We''re going to use it together with cross-validation
    to validate performance:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `kernel` parameter must be set to `''linear''` in this example. In the
    next section, we''re going to discuss how it works and how it can improve the
    SVM''s performance dramatically in non-linear scenarios. As expected, the accuracy
    is comparable to a logistic regression, as this model tries to find an optimal
    linear separator. After training a model, it''s possible to get an array of support
    vectors, through the instance variable called `support_vectors_`. A plot of them,
    for our example, is shown in the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31b9af15-46ea-4aa2-aab7-89bc88beea07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: As it's possible to see, they are placed in a strip along the separating line.
    The effect of `C` and the slack variables determined a movable margin that partially
    captured the existing overlap. Of course, it's impossible to separate the sets
    in a perfect way with a linear classifier and, on the other hand, most real-life
    problems are non-linear; for this reason, it's a necessary further step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Kernel-based classification
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with non-linear problems, it''s useful to transform the original
    vectors by projecting them into a higher dimensional space where they can be linearly
    separated. We saw a similar approach when we discussed polynomial regression.
    SVMs also adopt the same approach, even if there''s now a complexity problem that
    we need to overcome. Our mathematical formulation becomes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13acd1b1-0e4a-44b7-b546-76a7174f2e5d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Every feature vector is now filtered by a non-linear function that can completely
    reshape the scenario. However, the introduction of such a function increased the
    computational complexity in a way that could apparently discourage this approach.
    To understand what has happened, it''s necessary to express the quadratic problem
    using Lagrange multipliers. The entire procedure is beyond the scope of this book
    (in Nocedal J., Wright S. J., *Numerical Optimization*, Springer, you can find
    a complete and formal description of quadratic programming problems); however,
    the final formulation is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd4e49ed-3a01-497c-b755-2fdf06753109.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Therefore it''s necessary to compute the following for every couple of vectors:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82139cea-c5ca-4297-a4eb-3b519e4733cc.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'And this procedure can be a bottleneck, unacceptable for large problems. However,
    it''s now that the so-called **kernel trick** takes place. There are particular
    functions (called kernels) that have the following property:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec32e87d-dc9b-44b5-bec1-d4bc04037db4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: In other words, the value of the kernel for two feature vectors is the product
    of the two projected vectors. With this trick, the computational complexity remains
    almost the same, but we can benefit from the power of non-linear projections even
    in a very large number of dimensions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Excluding the linear kernel, which is a simple product, scikit-learn supports
    three different kernels that can solve many real-life problems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Radial Basis Function
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The RBF kernel is the default value for SVC and is based on the function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2268fe32-9d0f-43c7-882c-9c3c51c37479.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: The gamma parameter determines the amplitude of the function, which is not influenced
    by the direction but only by the distance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial kernel
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The polynomial kernel is based on the function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54a30a9a-c292-4d46-9561-9f3d54313be7.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: The exponent *c* is specified through the parameter degree, while the constant
    term *r* is called `coef0`. This function can easily expand the dimensionality
    with a large number of support variables and overcome very non-linear problems;
    however, the requirements in terms of resources are normally higher. Considering
    that a non-linear function can often be approximated quite well for a bounded
    area (by adopting polynomials), it's not surprising that many complex problems
    become easily solvable using this kernel.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid kernel
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sigmoid kernel is based on this function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48cff76f-30b8-402d-b170-81fa7035e466.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: The constant term *r* is specified through the parameter `coef0`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Custom kernels
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Normally, built-in kernels can efficiently solve most real-life problems; however
    scikit-learn allows us to create custom kernels as normal Python functions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The function can be passed to SVC through the `kernel` parameter, which can
    assume fixed string values (`'linear'`, `'rbf'`, `'poly'` and `'sigmoid'`) or
    a callable (such as `kernel=custom_kernel`).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear examples
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To show the power of kernel SVMs, we''re going to solve two problems. The first
    one is simpler but purely non-linear and the dataset is generated through the
    `make_circles()` built-in function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A plot of this dataset is shown in the following figure:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cdb11e2-c2f8-4311-9b95-31f15d3014b1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'As it''s possible to see, a linear classifier can never separate the two sets
    and every approximation will contain on average 50% misclassifications. A logistic
    regression example is shown here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As expected, the accuracy is below 50% and no other optimizations can increase
    it dramatically. Let''s consider, instead, a grid search with an SVM and different
    kernels (keeping the default values of each one):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected from the geometry of our dataset, the best kernel is a radial basis
    function, which yields 87% accuracy. Further refinements on `gamma` could slightly
    increase this value, but as there is a partial overlap between the two subsets,
    it's very difficult to achieve an accuracy close to 100%. However, our goal is
    not to overfit our model; it is to guarantee an appropriate level of generalization.
    So, considering the shape, a limited number of misclassifications is acceptable
    to ensure that the model captures sub-oscillations in the boundary surface.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting example is provided by the MNIST handwritten digit dataset.
    We have already seen it and classified it using linear models. Now we can try
    to find the best kernel with an SVM:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hence the best classifier (with almost 98% accuracy) is based on a polynomial
    kernel and a very low `C` value. This means that a non-linear transformation with
    very hard margins can easily capture the dynamics of all digits. Indeed, SVMs
    (with various internal alternatives) have always shown excellent performance with
    this dataset and their usage can easily be extended to similar problems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting example is based on the Olivetti face dataset, which is
    not part of scikit-learn but can be automatically downloaded and set up using
    a built-in function called `fetch_olivetti_faces()`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Through the `data_home` parameter, it is possible to specify in which local
    folder the dataset must be placed. A subset of samples is shown in the following
    figure:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29c1cfe5-9c08-44c8-b5f2-19bac1d60f67.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'There are 40 different people and each of them is represented with 10 pictures
    of 64 x 64 pixels. The number of classes (40) is not high, but considering the
    similarity of many photos, a good classifier should be able to capture some specific
    anatomical details. Performing a grid search with non-linear kernels, we get:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So the best estimator is polynomial-based with `degree=2`, and the corresponding
    accuracy is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This confirms the ability of SVM to capture non-linear dynamics even with simple
    kernels that can be computed in a very limited amount of time. It would be interesting
    for the reader to try different parameter combinations or preprocess the data
    and apply principal component analysis to reduce its dimensionality.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Controlled support vector machines
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With real datasets, SVM can extract a very large number of support vectors to
    increase accuracy, and that can slow down the whole process. To allow finding
    out a trade-off between precision and number of support vectors, scikit-learn
    provides an implementation called `NuSVC`, where the parameter `nu` (bounded between
    0—not included—and 1) can be used to control at the same time the number of support
    vectors (greater values will increase their number) and training errors (lower
    values reduce the fraction of errors). Let''s consider an example with a linear
    kernel and a simple dataset. In the following figure, there''s a scatter plot
    of our set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96bb39ac-c5b9-4100-bc07-46b56e06bff0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start checking the number of support vectors for a standard SVM:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So the model has found 242 support vectors. Let''s now try to optimize this
    number using cross-validation. The default value is 0.5, which is an acceptable
    trade-off:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As expected, the behavior is similar to a standard SVC. Let''s now reduce the
    value of `nu`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this case, the number of support vectors is less than before and also the
    accuracy has been affected by this choice. Instead of trying different values,
    we can look for the best choice with a grid search:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Therefore, in this case as well, the default value of 0.5 yielded the most accurate
    results. Normally, this approach works quite well, but when it's necessary to
    reduce the number of support vectors, it can be a good starting point for progressively
    reducing the value of `nu` until the result is acceptable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，默认值 0.5 产生了最准确的结果。通常，这种方法工作得相当好，但当需要减少支持向量的数量时，它可以是一个逐步减少 `nu` 值直到结果可接受的好起点。
- en: Support vector regression
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量回归
- en: scikit-learn provides a support vector regressor based on a very simple variant
    of the algorithm already described (see the original documentation for further
    information). The real power of this approach resides in the usage of non-linear
    kernels (in particular, polynomials); however, the user is advised to evaluate
    the degree progressively because the complexity can grow rapidly, together with
    the training time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一种基于已描述算法（请参阅原始文档以获取更多信息）的非常简单的变体支持向量回归器。这种方法真正的力量在于使用非线性核（特别是多项式核）；然而，用户应建议逐步评估度数，因为复杂性可以迅速增长，同时训练时间也会增加。
- en: 'For our example, I''ve created a dummy dataset based on a second-order noisy
    function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我创建了一个基于二次噪声函数的虚拟数据集：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The dataset in plotted in the following figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在以下图中展示：
- en: '![](img/27ae755b-e335-45e8-a74d-bc694fac9ef6.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27ae755b-e335-45e8-a74d-bc694fac9ef6.png)'
- en: 'In order to avoid a very long training process, the model is evaluated with
    `degree` set to `2`. The epsilon parameter allows us to specify a soft margin
    for predictions; if a predicted value is contained in the ball centered on the
    target value and the radius is equal to epsilon, no penalty is applied to the
    function to be minimized. The default value is 0.1:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免非常长的训练过程，模型使用`degree`设置为`2`进行评估。epsilon 参数允许我们指定预测的软边缘；如果预测值包含在以目标值为中心、半径等于
    epsilon 的球内，则不对要最小化的函数应用惩罚。默认值是 0.1：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Nocedal J., Wright S. J., *Numerical Optimization*, Springer
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Nocedal J.，Wright S. J.，*数值优化*，Springer
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how a support vector machine works in both linear
    and non-linear scenarios, starting from the basic mathematical formulation. The
    main concept is to find the hyperplane that maximizes the distance between the
    classes by using a limited number of samples (called support vectors) that are
    closest to the separation margin.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了支持向量机在线性和非线性场景下的工作原理，从基本的数学公式开始。主要概念是找到通过使用有限数量的样本（称为支持向量）来最大化类别之间的距离的超平面，这些样本是最接近分离边缘的。
- en: We saw how to transform a non-linear problem using kernel functions, which allow
    remapping of the original space to a another high-dimensional one where the problem
    becomes linearly separable. We also saw how to control the number of support vectors
    and how to use SVMs for regression problems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用核函数将非线性问题转换为线性问题，这些核函数允许将原始空间重新映射到另一个高维空间，在那里问题变得线性可分。我们还看到了如何控制支持向量的数量以及如何使用
    SVM 进行回归问题。
- en: In the next chapter, we're going to introduce another classification method
    called decision trees, which is the last one explained in this book.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种称为决策树的分类方法，这是本书中最后解释的方法。
