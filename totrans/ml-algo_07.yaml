- en: Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to introduce another approach to classification
    using a family of algorithms called support vector machines. They can work with
    both linear and non-linear scenarios, allowing high performance in many different
    contexts. Together with neural networks, SVMs probably represent the best choice
    for many tasks where it's not easy to find out a good separating hyperplane. For
    example, for a long time, SVMs were the best choice for MNIST dataset classification,
    thanks to the fact that they can capture very high non-linear dynamics using a
    mathematical trick, without complex modifications in the algorithm. In the first
    part, we're going to discuss the basics of linear SVM, which then will be used
    for their non-linear extensions. We'll also discuss some techniques to control
    the number of parameters and, at the end, the application of support vector algorithms
    to regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Linear support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a dataset of feature vectors we want to classify:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8ebf2e9-c219-44a4-bbfd-190e1364aaef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, we assume it as a binary classification (in all the other cases,
    it''s possible to use automatically the one-versus-all strategy) and we set our
    class labels as -1 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56b6c321-1019-4144-84fc-e5bd104e136a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to find the best separating hyperplane, for which the equation
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d48b7499-e5f1-4593-8180-bf8e3793c2fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure, there''s a bidimensional representation of such a
    hyperplane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2582ae4-446f-426d-9a37-3edd703ee10b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this way, our classifier can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01bbc949-57f8-4dd0-928e-de89da5ea280.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a realistic scenario, the two classes are normally separated by a margin
    with two boundaries where a few elements lie. Those elements are called **support
    vectors**. For a more generic mathematical expression, it''s preferable to renormalize
    our dataset so that the support vectors will lie on two hyperplanes with equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed90dc11-fc80-474f-aace-973758eb6ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following figure, there''s an example with two support vectors. The
    dashed line is the original separating hyperplane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93c4e356-cf65-4925-9c48-6591dcfaf927.png)'
  prefs: []
  type: TYPE_IMG
- en: Our goal is to maximize the distance between these two boundary hyperplanes
    so as to reduce the probability of misclassification (which is higher when the
    distance is short, and there aren't two well-defined blobs as in the previous
    figure).
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that the boundaries are parallel, the distance between them is
    defined by the length of the segment perpendicular to both and connecting two
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37e1322e-e6d5-4fbd-83d5-54ba7c0982d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the points as vectors, therefore, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bde1f510-5fab-4167-8e97-cd3164a5b07f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, considering the boundary hyperplane equations, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1a13501-f5c0-41cc-a4c6-ebe3cedbefcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term of the last part is equal to -1, so we solve for *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d0a8ff9-e358-429c-a4a2-fb6e69eac926.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The distance between *x[1]* and *x[2]* is the length of the segment *t*; hence
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e5930f6-b8c6-40a4-a67a-89d7ecf6d919.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, considering all points of our dataset, we can impose the following constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0637c2a1-8d11-4d8b-ac8d-0b2bff9bec9f.png)'
  prefs: []
  type: TYPE_IMG
- en: This is guaranteed by using -1, 1 as class labels and boundary margins. The
    equality is true only for the support vectors, while for all the other points
    it will greater than 1\. It's important to consider that the model doesn't take
    into account vectors beyond this margin. In many cases, this can yield a very
    robust model, but in many datasets this can also be a strong limitation. In the
    next paragraph, we're going to use a trick to avoid this rigidness while keeping
    the same optimization technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can define the function to minimize in order to train a support
    vector machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1ea27ad-76a9-476f-8710-3cea0273df5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be further simplified (by removing the square root from the norm)
    in the following quadratic programming problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a21ed94-6589-4cd0-996f-0ef0dd4b6e76.png)'
  prefs: []
  type: TYPE_IMG
- en: scikit-learn implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to allow the model to have a more flexible separating hyperplane,
    all scikit-learn implementations are based on a simple variant that includes so-called
    **slack variables** in the function to minimize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c0cd89f-8dc3-4a0e-8649-ef36e6c0df0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the constraints become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4198582-8d33-4738-9d96-8133c14f06e7.png)'
  prefs: []
  type: TYPE_IMG
- en: The introduction of the slack variables allows us to create a flexible margin
    so that some vectors belonging to a class can also be found in the opposite part
    of the hyperspace and can be included in the model training. The strength of this
    flexibility can be set using the parameter *C*. Small values (close to zero) bring
    about very hard margins, while values greater than or equal to 1 allow more and
    more flexibility (also increasing the misclassification rate). The right choice
    of *C* is not immediate, but the best value can be found automatically by using
    a grid search as seen in the previous chapters. In our examples, we keep the default
    value of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our first example is based on a linear SVM, as described in the previous section.
    We start by creating a dummy dataset with 500 vectors subdivided into two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the following figure, there's a plot of our dataset. Notice that some points
    overlap the two main blobs. For this reason, a positive *C* value is needed to
    allow the model to capture a more complex dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1707af43-728e-43d6-acd8-924b1cb35601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'scikit-learn provides the `SVC` class, which is a very efficient implementation
    that can be used in most cases. We''re going to use it together with cross-validation
    to validate performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kernel` parameter must be set to `''linear''` in this example. In the
    next section, we''re going to discuss how it works and how it can improve the
    SVM''s performance dramatically in non-linear scenarios. As expected, the accuracy
    is comparable to a logistic regression, as this model tries to find an optimal
    linear separator. After training a model, it''s possible to get an array of support
    vectors, through the instance variable called `support_vectors_`. A plot of them,
    for our example, is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31b9af15-46ea-4aa2-aab7-89bc88beea07.png)'
  prefs: []
  type: TYPE_IMG
- en: As it's possible to see, they are placed in a strip along the separating line.
    The effect of `C` and the slack variables determined a movable margin that partially
    captured the existing overlap. Of course, it's impossible to separate the sets
    in a perfect way with a linear classifier and, on the other hand, most real-life
    problems are non-linear; for this reason, it's a necessary further step.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel-based classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with non-linear problems, it''s useful to transform the original
    vectors by projecting them into a higher dimensional space where they can be linearly
    separated. We saw a similar approach when we discussed polynomial regression.
    SVMs also adopt the same approach, even if there''s now a complexity problem that
    we need to overcome. Our mathematical formulation becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13acd1b1-0e4a-44b7-b546-76a7174f2e5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every feature vector is now filtered by a non-linear function that can completely
    reshape the scenario. However, the introduction of such a function increased the
    computational complexity in a way that could apparently discourage this approach.
    To understand what has happened, it''s necessary to express the quadratic problem
    using Lagrange multipliers. The entire procedure is beyond the scope of this book
    (in Nocedal J., Wright S. J., *Numerical Optimization*, Springer, you can find
    a complete and formal description of quadratic programming problems); however,
    the final formulation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd4e49ed-3a01-497c-b755-2fdf06753109.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore it''s necessary to compute the following for every couple of vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82139cea-c5ca-4297-a4eb-3b519e4733cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And this procedure can be a bottleneck, unacceptable for large problems. However,
    it''s now that the so-called **kernel trick** takes place. There are particular
    functions (called kernels) that have the following property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec32e87d-dc9b-44b5-bec1-d4bc04037db4.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the value of the kernel for two feature vectors is the product
    of the two projected vectors. With this trick, the computational complexity remains
    almost the same, but we can benefit from the power of non-linear projections even
    in a very large number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Excluding the linear kernel, which is a simple product, scikit-learn supports
    three different kernels that can solve many real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: Radial Basis Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The RBF kernel is the default value for SVC and is based on the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2268fe32-9d0f-43c7-882c-9c3c51c37479.png)'
  prefs: []
  type: TYPE_IMG
- en: The gamma parameter determines the amplitude of the function, which is not influenced
    by the direction but only by the distance.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The polynomial kernel is based on the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54a30a9a-c292-4d46-9561-9f3d54313be7.png)'
  prefs: []
  type: TYPE_IMG
- en: The exponent *c* is specified through the parameter degree, while the constant
    term *r* is called `coef0`. This function can easily expand the dimensionality
    with a large number of support variables and overcome very non-linear problems;
    however, the requirements in terms of resources are normally higher. Considering
    that a non-linear function can often be approximated quite well for a bounded
    area (by adopting polynomials), it's not surprising that many complex problems
    become easily solvable using this kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sigmoid kernel is based on this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48cff76f-30b8-402d-b170-81fa7035e466.png)'
  prefs: []
  type: TYPE_IMG
- en: The constant term *r* is specified through the parameter `coef0`.
  prefs: []
  type: TYPE_NORMAL
- en: Custom kernels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Normally, built-in kernels can efficiently solve most real-life problems; however
    scikit-learn allows us to create custom kernels as normal Python functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The function can be passed to SVC through the `kernel` parameter, which can
    assume fixed string values (`'linear'`, `'rbf'`, `'poly'` and `'sigmoid'`) or
    a callable (such as `kernel=custom_kernel`).
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To show the power of kernel SVMs, we''re going to solve two problems. The first
    one is simpler but purely non-linear and the dataset is generated through the
    `make_circles()` built-in function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of this dataset is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cdb11e2-c2f8-4311-9b95-31f15d3014b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As it''s possible to see, a linear classifier can never separate the two sets
    and every approximation will contain on average 50% misclassifications. A logistic
    regression example is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the accuracy is below 50% and no other optimizations can increase
    it dramatically. Let''s consider, instead, a grid search with an SVM and different
    kernels (keeping the default values of each one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As expected from the geometry of our dataset, the best kernel is a radial basis
    function, which yields 87% accuracy. Further refinements on `gamma` could slightly
    increase this value, but as there is a partial overlap between the two subsets,
    it's very difficult to achieve an accuracy close to 100%. However, our goal is
    not to overfit our model; it is to guarantee an appropriate level of generalization.
    So, considering the shape, a limited number of misclassifications is acceptable
    to ensure that the model captures sub-oscillations in the boundary surface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting example is provided by the MNIST handwritten digit dataset.
    We have already seen it and classified it using linear models. Now we can try
    to find the best kernel with an SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Hence the best classifier (with almost 98% accuracy) is based on a polynomial
    kernel and a very low `C` value. This means that a non-linear transformation with
    very hard margins can easily capture the dynamics of all digits. Indeed, SVMs
    (with various internal alternatives) have always shown excellent performance with
    this dataset and their usage can easily be extended to similar problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another interesting example is based on the Olivetti face dataset, which is
    not part of scikit-learn but can be automatically downloaded and set up using
    a built-in function called `fetch_olivetti_faces()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Through the `data_home` parameter, it is possible to specify in which local
    folder the dataset must be placed. A subset of samples is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29c1cfe5-9c08-44c8-b5f2-19bac1d60f67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are 40 different people and each of them is represented with 10 pictures
    of 64 x 64 pixels. The number of classes (40) is not high, but considering the
    similarity of many photos, a good classifier should be able to capture some specific
    anatomical details. Performing a grid search with non-linear kernels, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So the best estimator is polynomial-based with `degree=2`, and the corresponding
    accuracy is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This confirms the ability of SVM to capture non-linear dynamics even with simple
    kernels that can be computed in a very limited amount of time. It would be interesting
    for the reader to try different parameter combinations or preprocess the data
    and apply principal component analysis to reduce its dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Controlled support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With real datasets, SVM can extract a very large number of support vectors to
    increase accuracy, and that can slow down the whole process. To allow finding
    out a trade-off between precision and number of support vectors, scikit-learn
    provides an implementation called `NuSVC`, where the parameter `nu` (bounded between
    0—not included—and 1) can be used to control at the same time the number of support
    vectors (greater values will increase their number) and training errors (lower
    values reduce the fraction of errors). Let''s consider an example with a linear
    kernel and a simple dataset. In the following figure, there''s a scatter plot
    of our set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96bb39ac-c5b9-4100-bc07-46b56e06bff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start checking the number of support vectors for a standard SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So the model has found 242 support vectors. Let''s now try to optimize this
    number using cross-validation. The default value is 0.5, which is an acceptable
    trade-off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the behavior is similar to a standard SVC. Let''s now reduce the
    value of `nu`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the number of support vectors is less than before and also the
    accuracy has been affected by this choice. Instead of trying different values,
    we can look for the best choice with a grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, in this case as well, the default value of 0.5 yielded the most accurate
    results. Normally, this approach works quite well, but when it's necessary to
    reduce the number of support vectors, it can be a good starting point for progressively
    reducing the value of `nu` until the result is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn provides a support vector regressor based on a very simple variant
    of the algorithm already described (see the original documentation for further
    information). The real power of this approach resides in the usage of non-linear
    kernels (in particular, polynomials); however, the user is advised to evaluate
    the degree progressively because the complexity can grow rapidly, together with
    the training time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, I''ve created a dummy dataset based on a second-order noisy
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset in plotted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27ae755b-e335-45e8-a74d-bc694fac9ef6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to avoid a very long training process, the model is evaluated with
    `degree` set to `2`. The epsilon parameter allows us to specify a soft margin
    for predictions; if a predicted value is contained in the ball centered on the
    target value and the radius is equal to epsilon, no penalty is applied to the
    function to be minimized. The default value is 0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nocedal J., Wright S. J., *Numerical Optimization*, Springer
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how a support vector machine works in both linear
    and non-linear scenarios, starting from the basic mathematical formulation. The
    main concept is to find the hyperplane that maximizes the distance between the
    classes by using a limited number of samples (called support vectors) that are
    closest to the separation margin.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to transform a non-linear problem using kernel functions, which allow
    remapping of the original space to a another high-dimensional one where the problem
    becomes linearly separable. We also saw how to control the number of support vectors
    and how to use SVMs for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to introduce another classification method
    called decision trees, which is the last one explained in this book.
  prefs: []
  type: TYPE_NORMAL
