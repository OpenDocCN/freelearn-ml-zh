<html><head></head><body>
		<div id="_idContainer953">
			<h1 class="chapter-number" id="_idParaDest-262"><a id="_idTextAnchor689"/>13</h1>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor690"/>Tracking and Visualizing ML Experiments</h1>
			<p>In the world of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), <strong class="bold">visualization</strong> and <strong class="bold">experiment tracking systems</strong> play a crucial role. These tools provide a way to understand complex data, track experiments, and make informed decisions about <span class="No-Break">model development.</span></p>
			<p>In ML, visualizing data is essential for understanding patterns, relationships, and trends. Data visualization tools allow engineers to create charts, graphs, and plots that help them explore and analyze their data. With the right visualization tool, engineers can quickly identify patterns and anomalies, which can be used to improve <span class="No-Break">model performance.</span></p>
			<p>Experiment tracking systems are designed to keep track of the progress of multiple experiments. They allow engineers to compare results, identify best practices, and avoid repeating mistakes. Experiment tracking tools also help with reproducibility, ensuring that experiments can be repeated accurately <span class="No-Break">and efficiently.</span></p>
			<p>Choosing the right tools for visualization and experiment tracking is critical. There are many open source and commercial options available, each with its strengths and weaknesses. It’s important to consider factors such as ease of use, integration with other tools, and the specific needs of your project when selecting <span class="No-Break">a tool.</span></p>
			<p>In this chapter, we’ll briefly discuss <strong class="bold">TensorBoard</strong>, one of the most widespread experiment visualization systems available. We’ll also learn what type of visualizations it can provide and the challenges of using it with C++. As for the tracking system, we’ll discuss the <strong class="bold">MLflow framework</strong> and provide a hands-on example of how to use it with C++. This example covers setting up a project, defining experiments, logging metrics, and visualizing the training process and showcases the power of experiment tracking tools in enhancing the ML <span class="No-Break">development process.</span></p>
			<p>By the end of this chapter, you should have a clear understanding of why these tools are essential for ML engineers and how they can help you achieve <span class="No-Break">better results.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding visualization and experiment <span class="No-Break">tracking systems</span></li>
				<li>Experiment tracking with MLflow’s <span class="No-Break">REST API</span></li>
			</ul>
			<h1 id="_idParaDest-264"><a id="_idTextAnchor691"/>Technical requirements</h1>
			<p>The following are the technical requirements for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Flashlight <span class="No-Break">library 0.4.0</span></li>
				<li><span class="No-Break">MLflow 2.5.0</span></li>
				<li><span class="No-Break"><strong class="source-inline">cpp-httplib</strong></span><span class="No-Break"> v0.16.0</span></li>
				<li><strong class="source-inline">nlohmann </strong><span class="No-Break"><strong class="source-inline">json</strong></span><span class="No-Break"> v3.11.2</span></li>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.22</span></li>
			</ul>
			<p>The code files for this chapter can be found in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter13/flashlight"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter13/flashlight</span></a></p>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor692"/>Understanding visualization and tracking systems for experiments</h1>
			<p>Visualization and tracking systems for ML experiments are essential components of the ML development process. Together, these systems enable engineers to build more robust and effective ML models. They also help ensure reproducibility and transparency in the development process, which is crucial for scientific rigor <span class="No-Break">and collaboration.</span></p>
			<p>Visualization tools provide <a id="_idIndexMarker1501"/>a graphical representation of data, allowing engineers to see patterns, trends, and relationships that might be difficult to detect in raw data. This can help engineers gain insights into the behavior of their models, identify areas for improvement, and make informed decisions about model design and <span class="No-Break">hyperparameter tuning.</span></p>
			<p>Experiment tracking systems allow engineers to log and organize experiments, including model architectures, hyperparameters, and training data. These systems provide an overview of the entire experimentation process, making it easier to compare different models and determine which ones <span class="No-Break">perform best.</span></p>
			<p>Next, we’ll look at some of the key features of TensorBoard, a powerful visualization tool, and understand the essential components of MLflow, an effective experiment <span class="No-Break">tracking system.</span></p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor693"/>TensorBoard</h2>
			<p>TensorBoard is a visualization tool for ML models that provides insights into model performance and training<a id="_idIndexMarker1502"/> progress. It also provides an interactive dashboard where users can explore graphs, histograms, scatter plots, and other visualizations related to <span class="No-Break">their experiments.</span></p>
			<p>Here are some key features <span class="No-Break">of TensorBoard:</span></p>
			<ul>
				<li><strong class="bold">Visualization of metrics and scalars</strong>: TensorBoard <a id="_idIndexMarker1503"/>allows users to visualize various metrics related to their ML experiments. These metrics include <strong class="source-inline">loss</strong>, <strong class="source-inline">accuracy</strong>, <strong class="source-inline">precision</strong>, <strong class="source-inline">recall</strong>, and <span class="No-Break"><strong class="source-inline">F1 score</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Histogram plots</strong>: TensorBoard also provides histogram plots for a better understanding of model performance. These plots can help users understand the distribution of layer weight and <span class="No-Break">gradient values.</span></li>
				<li><strong class="bold">Graphs</strong>: Graphs in TensorBoard provide a visual representation of model architecture. Users can create graphs to analyze the correlation between inputs and outputs or to compare <span class="No-Break">different models.</span></li>
				<li><strong class="bold">Images</strong>: TensorBoard allows you to display image data and connect such a visualization to <a id="_idIndexMarker1504"/>a training timeline. This can help users analyze input data, intermediate<a id="_idIndexMarker1505"/> outputs, or convolutional filter <span class="No-Break">result visualizations.</span></li>
				<li><strong class="bold">Embedding projector</strong>: The embedding projector in TensorBoard allows users to explore high-dimensional data in lower<a id="_idIndexMarker1506"/> dimensions using techniques such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>). This feature helps in visualizing <span class="No-Break">complex datasets.</span></li>
				<li><strong class="bold">Comparison</strong>: In TensorBoard, comparison enables users to compare the performance of multiple models side by side, making it easy to identify the <span class="No-Break">best-performing model.</span></li>
			</ul>
			<p>Unfortunately, TensorBoard doesn’t integrate easily with C++ ML frameworks. The native C++ support only exists in the TensorFlow framework. Also, there’s only one third-party open source library that allows us to use TensorBoard, and it isn’t <span class="No-Break">actively maintained.</span></p>
			<p>TensorBoard can be integrated with various Python-based deep learning frameworks, including TensorFlow, PyTorch, and others. So, if you train your models in Python, it makes sense to consider it as an instrument that can help you understand how models are performing, identify potential issues, and make informed decisions about hyperparameters, data preprocessing, and <span class="No-Break">model design.</span></p>
			<p>Otherwise, to visualize training data, you can use <strong class="source-inline">gnuplot</strong>-based libraries such as <strong class="source-inline">CppPlot</strong>, as we did in the previous chapters. See <a href="B19849_03.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> for the 2D scatter and line plot <span class="No-Break">visualization examples.</span></p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor694"/>MLflow</h2>
			<p>MLflow is an open source framework that’s designed for <strong class="bold">machine learning operations</strong> (<strong class="bold">MLOps</strong>) and helps<a id="_idIndexMarker1507"/> teams manage, track, and scale their ML projects. It provides a set of tools and features for building, training, and deploying models, as well as for monitoring their performance <a id="_idIndexMarker1508"/><span class="No-Break">and experimentation.</span></p>
			<p>The main components of MLflow are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Experiment tracking</strong>: MLflow allows users to track their experiments, including hyperparameters, code versions, and metrics. This helps in understanding the impact of different configurations on <span class="No-Break">model performance.</span></li>
				<li><strong class="bold">Code reproducibility</strong>: With MLflow, users can easily reproduce their experiments by tracking code <a id="_idIndexMarker1509"/>versions and dependencies. This ensures consistency across experiments and makes it easier to <span class="No-Break">identify issues.</span></li>
				<li><strong class="bold">Model Registry</strong>: MLflow provides a Model Registry component where users can store, version, and manage their models. This allows for easy collaboration and model sharing <span class="No-Break">within teams.</span></li>
				<li><strong class="bold">Integration with other tools</strong>: MLflow integrates with popular data science and ML tools, such as Jupyter Notebook, TensorFlow, PyTorch, and more. This enables seamless integration with existing workflows. For non-Python environments, MLflow provides the <span class="No-Break">REST API.</span></li>
				<li><strong class="bold">Deployment options</strong>: MLflow offers various options for deploying models, including Docker containers, Kubernetes, and cloud platforms. This flexibility allows users to choose the best deployment strategy based on <span class="No-Break">their needs.</span></li>
			</ul>
			<p>Internally, MLflow uses a database to store metadata about experiments, models, and parameters. By default, it uses SQLite, but other databases such as PostgreSQL and MySQL are also supported. This allows for scalability and flexibility in terms of storage requirements. MLflow uses unique identifiers to track objects and operations within the platform. These identifiers are used to link different components of an experiment together, such as a run and its associated parameters. This makes it easy to reproduce experiments and understand the relationships between different parts of a workflow. It also provides a REST API for programmatic access to features such as model registration, tracking, and model life cycle management. It uses YAML configuration files for customizing and configuring MLflow behavior, and Python APIs for easy integration with MLflow components <span class="No-Break">and workflows.</span></p>
			<p>So, we can summarize that<a id="_idIndexMarker1510"/> visualization and experiment tracking systems are essential tools for data scientists and engineers to understand, analyze, and optimize their ML models. These systems <a id="_idIndexMarker1511"/>allow users to track the performance of different models, compare results, identify patterns, and make informed decisions about model development <span class="No-Break">and deployment.</span></p>
			<p>To illustrate how experiment tracking tools can be integrated into ML workflows, we’ll provide a concrete example in the <span class="No-Break">following section.</span></p>
			<h1 id="_idParaDest-268"><a id="_idTextAnchor695"/>Experiment tracking with MLflow’s REST API</h1>
			<p>Let’s consider an example of an experiment involving a regression model. We’ll use MLflow to log performance <a id="_idIndexMarker1512"/>metrics and the parameters of a model for several experiments. While training the model, we’ll visualize the results using a plot to show the accuracy and loss curves over time. Finally, we’ll compare the<a id="_idIndexMarker1513"/> results of different experiments using the tracking system so that we can select the best-performing model and optimize <span class="No-Break">it further.</span></p>
			<p>This example will demonstrate how experiment tracking can be seamlessly integrated into a C++ ML workflow, providing valuable insights and improving the overall quality <span class="No-Break">of research.</span></p>
			<p>Before you can use MLflow, you need to install it. You can install MLflow <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
pip install mlflow</pre>			<p>Then, you’ll need to start a server, <span class="No-Break">like so:</span></p>
			<pre class="console">
mlflow server --backend-store-uri file:////samples/Chapter13/mlruns</pre>			<p>This command starts the local tracking server at <strong class="source-inline">http://localhost:5000</strong>, which saves tracking data to the <strong class="source-inline">/samples/Chapter13/mlruns</strong> directory. If you need to access the MLflow server from remote machines, you can start the command with the <strong class="source-inline">--host</strong> and <strong class="source-inline">--</strong><span class="No-Break"><strong class="source-inline">port</strong></span><span class="No-Break"> arguments.</span></p>
			<p>Having started tracking the server, we can communicate with it using the REST API. The access point to this API is hosted at <strong class="source-inline">http://localhost:5000/api/2.0/mlflow/</strong>. MLflow uses JSON as its data representation for the <span class="No-Break">REST API.</span></p>
			<p>To implement a <strong class="source-inline">REST</strong> client <a id="_idIndexMarker1514"/>for communicating with the tracking server, we’ll use two <span class="No-Break">additional libraries:</span></p>
			<ul>
				<li><strong class="source-inline">cpp-httplib</strong>: For implementing <span class="No-Break">HTTP communication</span></li>
				<li><strong class="source-inline">nlohmann json</strong>: For implementing REST requests <span class="No-Break">and responses</span></li>
			</ul>
			<p>Note that the basic linear regression<a id="_idIndexMarker1515"/> model will be implemented using the <span class="No-Break"><strong class="source-inline">Flashlight</strong></span><span class="No-Break"> library.</span></p>
			<p>Next, we’ll learn how to connect all these pieces. The first part we’ll cover is implementing the <span class="No-Break">REST client.</span></p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor696"/>Implementing MLflow’s REST C++ client</h2>
			<p>There are two main concepts in MLflow: <strong class="bold">experiments</strong> and <strong class="bold">runs</strong>. Together, they provide a structured <a id="_idIndexMarker1516"/>approach to managing and tracking ML workflows. They help us organize our projects, ensure reproducibility, and facilitate collaboration among <span class="No-Break">team members.</span></p>
			<p>In MLflow, we can organize and track our ML experiments. An experiment can be thought of as a container for all the runs related to a specific project or goal. It allows you to keep track of different versions of your models, compare their performance, and identify the <span class="No-Break">best one.</span></p>
			<p>The following are the key features of <span class="No-Break">an experiment:</span></p>
			<ul>
				<li><strong class="bold">Name</strong>: Each experiment has a <a id="_idIndexMarker1517"/>unique name that <span class="No-Break">identifies it.</span></li>
				<li><strong class="bold">Tags</strong>: You can add tags to <a id="_idIndexMarker1518"/>an experiment to categorize it based on <span class="No-Break">different criteria.</span></li>
				<li><strong class="bold">Artifacts location</strong>: Artifacts are files that are generated throughout the experiment, such as images, logs, and <a id="_idIndexMarker1519"/>more. MLflow allows you to store and version <span class="No-Break">these artifacts.</span></li>
			</ul>
			<p>A run represents a single execution of an experiment or a specific task within an experiment. Runs are used to<a id="_idIndexMarker1520"/> record the details of each execution, such as its start time, end time, parameters, <span class="No-Break">and metrics.</span></p>
			<p>The key features of a run are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Start time</strong>: The time when the <span class="No-Break">run started</span></li>
				<li><strong class="bold">End time</strong>: The time when the <span class="No-Break">run finished</span></li>
				<li><strong class="bold">Parameters</strong>: Model parameters such as batch size, learning rate, <span class="No-Break">and more</span></li>
				<li><strong class="bold">Model</strong>: The model code that was executed during <span class="No-Break">the run</span></li>
				<li><strong class="bold">Output</strong>: The results that were produced by the run, including metrics, artifacts, <span class="No-Break">and more</span></li>
			</ul>
			<p>Within a run, users can log parameters, metrics, and <span class="No-Break">model representation.</span></p>
			<p>Now that we understand the main concepts of MLflow’s tracking structure, let’s implement the MLflow <span class="No-Break">REST client:</span></p>
			<ol>
				<li>First, we’re going to put all the implementation details for the <strong class="source-inline">REST</strong> client in a single <strong class="source-inline">MLFlow</strong> class. The header file should look <span class="No-Break">as follows:</span><pre class="source-code">
class MLFlow {
 public:
  MLFlow(const std::string&amp; host, size_t port);
  void set_experiment(const std::string&amp; name);
  void start_run();
  void end_run();
  void log_metric(const std::string&amp; name, float value,
                  size_t epoch);
  void log_param(const std::string&amp; name, 
                 const std::string&amp; value);
  template &lt;typename T&gt;
  void log_param(const std::string&amp; name, T value) {
    log_param(name, std::to_string(value));
  }
 private:
  httplib::Client http_client_;
  std::string experiment_id_;
  std::string run_id_;
}</pre><p class="list-inset">We made the constructor take a host and the port of the tracking server to communicate with. Then, we defined methods to start a named experiment and run inside it, as well as methods to log named metrics and parameters. After, we declared an instance of the <strong class="source-inline">httplib::Client</strong> class, which will be used for HTTP communication<a id="_idIndexMarker1521"/> with the tracking server. Finally, we provided member variables, which are the IDs of the current experiment <span class="No-Break">and run.</span></p></li>				<li>Now, let’s learn how to implement these methods. The constructor implementation is <span class="No-Break">as follows:</span><pre class="source-code">
MLFlow::MLFlow(const std::string&amp; host, size_t port)
     : http_client_(host, port) {
}</pre><p class="list-inset">Here, we initialized the <strong class="source-inline">httplib::Client</strong> instance with the host and port values to initialize a connection with a <span class="No-Break">tracking server.</span></p></li>				<li>The following code<a id="_idIndexMarker1522"/> shows the <strong class="source-inline">set_experiment</strong> <span class="No-Break">method’s implementation:</span><pre class="source-code">
void MLFlow::set_experiment(const std::string&amp; name) {
  auto res = http_client_.Get(
      "/api/2.0/mlflow/experiments/"
      "get-by-name?experiment_name=" +
      name);
  if (check_result(res, 404)) {
    // Create a new experiment
    nlohmann::json request;
    request["name"] = name;
    res = http_client_.Post(
        "/api/2.0/mlflow/experiments/create",
        request.dump(), "application/json");
    handle_result(res);
    // Remember experiment ID
    auto json = nlohmann::json::parse(res-&gt;body);
    experiment_id_ =
        json["experiment_id"].get&lt;std::string&gt;();
  } else if (check_result(res, 200)) {
    // Remember experiment ID
    auto json = nlohmann::json::parse(res-&gt;body);
    experiment_id_ = json["experiment"]["experiment_id"]
                         .get&lt;std::string&gt;();
  } else {
    handle_result(res);
  }
}</pre><p class="list-inset">This method initializes an experiment for the following runs. There are two parts to this method—one for a new experiment and another for the <span class="No-Break">existing one:</span></p><ol><li class="upper-roman">First, we checked<a id="_idIndexMarker1523"/> whether the experiment with the given name existed on the server using the <span class="No-Break">following code:</span></li></ol><pre class="source-code">auto res = http_client_.Get(
    "/api/2.0/mlflow/experiments/get-byname?experiment_name=" + 
    name)</pre><p class="list-inset">By comparing the results with the <strong class="source-inline">404</strong> and <strong class="source-inline">202</strong> codes, we identified that there’s no such experiment or that it <span class="No-Break">already exists.</span></p><ol><li class="upper-roman" value="2">Because there’s no existing experiment, we created a JSON-based request to create a new experiment, <span class="No-Break">as follows:</span></li></ol><pre class="source-code">  nlohmann::json request;
  request["name"] = name;</pre><ol><li class="upper-roman" value="3">Then, we passed it as the body of the HTTP request to the server, <span class="No-Break">as follows:</span></li></ol><pre class="source-code">res = http_client_.Post("/api/2.0/mlflow/experiments/create",
                        request.dump(), "application/json");
handle_result(res);</pre><ol><li class="upper-roman" value="4">The <strong class="source-inline">dump</strong> method of the <strong class="source-inline">nlohmann::json</strong> object was used to convert the JSON into a string representation. After we got the result, we used the <strong class="source-inline">handle_result</strong> function to check for errors (this function will be discussed in more detail later). With the answer in the <strong class="source-inline">res</strong> variable, we took the <strong class="source-inline">experiment_id</strong> value, <span class="No-Break">as follows:</span></li></ol><pre class="source-code">  auto json = nlohmann::json::parse(res-&gt;body);
  experiment_id_ = json["experiment_id"].get&lt;std::string&gt;();</pre><p class="list-inset">Here, we parsed the string that was returned by the server with the <strong class="source-inline">nlohmann::json::parse</strong> function and read the <strong class="source-inline">experiment_id</strong> value from the JSON object into our class <span class="No-Break">member variable.</span></p><ol><li class="upper-roman" value="5">In the second part of the method, which works when an experiment exists on the server, we parsed the response in a JSON object and took the <span class="No-Break"><strong class="source-inline">experiment_id</strong></span><span class="No-Break"> value.</span></li></ol><p class="list-inset">There are two functions named <strong class="source-inline">handle_result</strong> that are used to check response codes and<a id="_idIndexMarker1524"/> report errors if needed. The first one is used to check whether a response has some particular code and is implemented <span class="No-Break">as follows:</span></p><pre class="source-code">bool check_result(const httplib::Result&amp; res, int code) {
  if (!res) {
    throw std::runtime_error(
      "REST error: " + httplib::to_ string(res.error()));
    }
    return res-&gt;status == code;
}</pre><p class="list-inset">Here, we checked whether the <strong class="source-inline">httplib::Result</strong> object has a valid response by using its Boolean cast operator. If there was a communication error, we threw the runtime exception. Otherwise, we returned the response code’s <span class="No-Break">comparison result.</span></p><ol><li class="upper-roman" value="6">The second <strong class="source-inline">handle_result</strong> function is used to check that we got the successful answer from the server. The following code snippet shows how <span class="No-Break">it’s implemented:</span></li></ol><pre class="source-code">void handle_result(const httplib::Result&amp; res) {
  if (check_result(res, 200))
    return;
  std::ostringstream oss;
  oss &lt;&lt; "Request error status: " &lt;&lt; res-&gt;status &lt;&lt; " "
      &lt;&lt; httplib::detail::status_message(res-&gt;status);
  oss &lt;&lt; ", message: " &lt;&lt; std::endl
      &lt;&lt; res-&gt;body;
  throw std::runtime_error(oss.str());
}</pre><p class="list-inset">We used the previous <strong class="source-inline">handle_result</strong> function to check whether the response was valid and we got a <strong class="source-inline">200</strong> response code. If it’s true, we’re OK. However, in the case <a id="_idIndexMarker1525"/>of a failure, we must make a detailed report and throw a <span class="No-Break">runtime exception.</span></p><p class="list-inset">These functions help to simplify response error handling code and make it easier to <span class="No-Break">debug communications.</span></p></li>				<li>The next two methods we’re going to discuss are <strong class="source-inline">start_run</strong> and <strong class="source-inline">end_run</strong>. These methods mark a single run’s bounds, within which we can log metrics, parameters, and artifacts. In production code, it makes sense to wrap such functionality into some RAII abstraction, but we made two methods <span class="No-Break">for simplicity.</span><p class="list-inset">The <strong class="source-inline">start_run</strong> method can be implemented <span class="No-Break">as follows:</span></p><pre class="source-code">
void MLFlow::start_run() {
  nlohmann::json request;
  request["experiment_id"] = experiment_id_;
  request["start_time"] =
      std::chrono::duration_ cast&lt;std::chrono::milliseconds&gt;(
          std::chrono::system_ clock::now().time_since_epoch())
          .count();
  auto res =
      http_client_.Post("/api/2.0/mlflow/runs/create",
                        request.dump(), "application/json");
  handle_result(res);
  auto json = nlohmann::json::parse(res-&gt;body);
  run_id_ = json["run"]["info"]["run_id"];
}</pre><p class="list-inset">Here, we made a JSON-based request to create a run. This request was filled with the current <strong class="source-inline">experiment_id</strong> value and the run’s start time. Then, we sent a request to the server<a id="_idIndexMarker1526"/> and got a response that we checked with the <strong class="source-inline">handle_result</strong> function. If we receive an answer, we parse it in the <strong class="source-inline">nlohmann::json</strong> object and take the <strong class="source-inline">run_id</strong> value. The <strong class="source-inline">run_id</strong> value is stored in the object member and will be used in the following requests. After we call this method, the tracking server will write all metrics and parameters into this <span class="No-Break">new run.</span></p></li>				<li>To complete the run, we have to tell the server about it. The <strong class="source-inline">end_run</strong> method does <span class="No-Break">just this:</span><pre class="source-code">
void MLFlow::end_run() {
  nlohmann::json request;
  request["run_id"] = run_id_;
  request["status"] = "FINISHED";
  request["end_time"] =
      std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(
          std::chrono::system_clock::now()
              .time_since_epoch())
          .count();
  auto res = http_client_.Post(
      "/api/2.0/mlflow/runs/update", request.dump(),
      "application/json");
  handle_result(res);
}</pre></li>			</ol>
			<p>Here, we made a JSON-based request that includes the <strong class="source-inline">run_id</strong> value, the finished status, and the end time. Then, we sent this request to the tracking server and checked the response. Notice that<a id="_idIndexMarker1527"/> we sent the start and end times for a run, at which point the server used them to calculate the run duration. Due to this, you’ll be able to see how the run duration time depends on <span class="No-Break">its parameters.</span></p>
			<p>Now that we have methods for setting an experiment and defining a run, we need methods so that we can log metrics values and <span class="No-Break">run parameters.</span></p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor697"/>Logging metric values and running parameters</h2>
			<p>The difference between metrics and parameters is that metrics are sequences of values within a run. You can log as many values of a single metric as you need. Usually, this number equals epochs or <a id="_idIndexMarker1528"/>batches and MLflow will show live plots for <a id="_idIndexMarker1529"/>these metrics. However, a single parameter can only be logged once per run, and it’s typically a training characteristic such as the <span class="No-Break">learning rate.</span></p>
			<p>A metric is usually a numeric value, so we’ve made our <strong class="source-inline">log_metric</strong> method take a float and an argument for a value. Note that this method takes the metric name and the epoch index to make several distinct values for the same metric. The method’s implementation is shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
void MLFlow::log_metric(const std::string&amp; name,
                        float value, size_t epoch) {
  nlohmann::json request;
  request["run_id"] = run_id_;
  request["key"] = name;
  request["value"] = value;
  request["step"] = epoch;
  request["timestamp"] =
      std::chrono::duration_
      cast&lt;std::chrono::milliseconds&gt;(
          std::chrono::system_clock::now()
              .time_ since_epoch())
          .count();
  auto res = http_client_.Post(
      "/api/2.0/mlflow/runs/log-metric", request.dump(),
      "application/json");
  handle_result(res);
}</pre>			<p>Here, we made a JSON-based request that includes the <strong class="source-inline">run_id</strong> value, the metric’s name as the <strong class="source-inline">key</strong> field, the metric’s value, the<a id="_idIndexMarker1530"/> epoch index as the <strong class="source-inline">step</strong> field, and the timestamp value. Then, we <a id="_idIndexMarker1531"/>sent the request to the tracking server and checked <span class="No-Break">the response.</span></p>
			<p>A parameter value can have an arbitrary value type, so we used C++ templates to write a single method to process different value types. There are two <strong class="source-inline">log_param</strong> functions here—the first is a template function that converts any suitable parameter value into a string, whereas the second only takes a parameter name and a string value as arguments. The template can be implemented <span class="No-Break">like so:</span></p>
			<pre class="source-code">
template &lt;typename T&gt;
void log_param(const std::string&amp; name, T value) {
  log_param(name, std::to_string(value));
}</pre>			<p>This template simply redirects a call to the second function after the value is converted into a string with the <strong class="source-inline">std::to_string</strong> function. So, if the value’s type can’t be converted into a string, a<a id="_idIndexMarker1532"/> compilation error <span class="No-Break">will occur.</span></p>
			<p>The second <strong class="source-inline">log_param</strong> function’s<a id="_idIndexMarker1533"/> implementation can be seen in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
void MLFlow::log_param(const std::string&amp; name,
                       const std::string&amp; value) {
  nlohmann::json request;
  request["run_id"] = run_id_;
  request["key"] = name;
  request["value"] = value;
  auto res = http_client_.Post("/api/2.0/mlflow/runs/log-parameter", 
                               request.dump(), "application/json");
  handle_result(res);
}</pre>			<p>Here, we made a JSON-based request that includes the current <strong class="source-inline">run_id</strong> value, the parameter name as the <strong class="source-inline">key</strong> field, and the value. Then, we just sent the request and checked <span class="No-Break">the response.</span></p>
			<p>The REST API in MLflow is much richer than this; we only covered the basic functions here. For example, it’s also capable of accepting model architectures in JSON format, logging input datasets, managing experiments and models, and <span class="No-Break">much more.</span></p>
			<p>Now that we understand the basic functionality for communicating with the MLflow server, let’s learn how to implement an experiments tracking session for a <span class="No-Break">regression task.</span></p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor698"/>Integrating experiment tracking into linear regression training</h2>
			<p>In this section, we’ll be using the <strong class="source-inline">Flashlight</strong> library to implement a linear regression model and train it. Our <a id="_idIndexMarker1534"/>code starts with initializing<a id="_idIndexMarker1535"/> Flashlight and connecting to an MLflow server, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
fl::init();
MLFlow mlflow("127.0.0.1","5000");
mlflow.set_experiment("Linear regression");</pre>			<p>Here, we assumed that the tracking server has already been started on localhost. After, we set the experiment’s name to <strong class="source-inline">Linear regression</strong>. Now, we can define the necessary parameters and start <span class="No-Break">the run:</span></p>
			<pre class="source-code">
int batch_size = 64;
float learning_rate = 0.0001;
float momentum = 0.5;
int epochs = 100;
mlflow.start_run();</pre>			<p>Having configured the run, we can load datasets for training and testing, define a model, and create an optimizer and loss function according to the parameters we <span class="No-Break">defined previously:</span></p>
			<pre class="source-code">
// load datasets
auto train_dataset = make_dataset(/*n=*/10000, batch_size);
auto test_dataset = make_dataset(/*n=*/1000, batch_size);
// Define a model
fl::Sequential model;
model.add(fl::View({1, 1, 1, -1}));
model.add(fl::Linear(1, 1));
// define MSE loss
auto loss = fl::MeanSquaredError();
// Define optimizer
auto sgd = fl::SGDOptimizer(model.params(), learning_rate, momentum);
// Metrics meter
fl::AverageValueMeter meter;</pre>			<p>Notice that we used all <a id="_idIndexMarker1536"/>the previously defined parameters except for the epoch number. Now, we’re ready to define the<a id="_idIndexMarker1537"/> training cycle, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
for (int epoch_i = 0; epoch_i &lt; epochs; ++epoch_i) {
  meter.reset();
  model.train();
  for (auto&amp; batch : *train_dataset) {
    sgd.zeroGrad();
    // Forward propagation
    auto predicted = model(fl::input(batch[0]));
    // Calculate loss
    auto local_batch_size = batch[0].shape().dim(0);
    auto target =
        fl::reshape(batch[1], {1, 1, 1, local_batch_ size});
    auto loss_value = loss(predicted, fl::noGrad(target));
    // Backward propagation
    loss_value.backward();
    // Update parameters
    sgd.step();
    meter.add(loss_value.scalar&lt;float&gt;());
  }
  // Train metrics logging
  // ...
  // Calculate and log test metrics
  // ...
}</pre>			<p>The main part of the training cycle looks normal as we implemented this in the previous chapters. Note that we have two nested cycles—one for epochs and another for batches. At the beginning of the training epoch, we cleared the meter that’s used for averaging the training loss metric and put the model into the training mode. Then, we cleared the gradients, made a<a id="_idIndexMarker1538"/> forward pass, calculated the loss value, made a backward pass, updated the model <a id="_idIndexMarker1539"/>weights with the optimizer step, and added the loss value to the averaging meter object. After the internal cycle, training was completed. At this point, we can log the average training loss metric value to the tracking server, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
// train metrics logging
auto avr_loss_value = meter.value()[0];
mlflow.log_metric("train loss", avr_loss_value, epoch_i);</pre>			<p>Here, we logged the train loss value for the epoch with the <strong class="source-inline">epoch_i</strong> index and used <strong class="source-inline">train loss</strong> as its name. For every epoch, this logging will add a new value for the metric and we’ll be able to see<a id="_idIndexMarker1540"/> the live plot of how the training loss changes over epochs in the MLflow UI. This plot<a id="_idIndexMarker1541"/> will be shown in the <span class="No-Break">following subsection.</span></p>
			<p>After the training cycle, for every 10th epoch, we calculate the test loss metric, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
// Every 10th epoch calculate test metric
if (epoch_i % 10 == 0) {
  fl::AverageValueMeter test_meter;
  model.eval();
  for (auto&amp; batch : *test_dataset) {
    // Forward propagation
    auto predicted = model(fl::input(batch[0]));
    // Calculate loss
    auto local_batch_size = batch[0].shape().dim(0);
    auto target =
        fl::reshape(batch[1], {1, 1, 1, local_batch_ size});
    auto loss_value = loss(predicted, fl::noGrad(target));
    // Add loss value to test meter
    test_meter.add(loss_value.scalar&lt;float&gt;());
  }
  // Logging the test metric
  // ...
}</pre>			<p>Once we’ve checked that the current epoch is the 10th one, we defined an additional averaging meter object for <a id="_idIndexMarker1542"/>the test loss metric and implemented evaluation mode. Then, we calculated the<a id="_idIndexMarker1543"/> loss value for every batch and added these values to the averaging meter. At this point, we can implement a loss calculation for the test dataset and log the test metric to the <span class="No-Break">tracking server:</span></p>
			<pre class="source-code">
// logging the test metric
auto avr_loss_value = test_meter.value()[0];
mlflow.log_metric("test loss", avr_loss_value, epoch_i);</pre>			<p>Here, we logged the test loss value for the epoch with the <strong class="source-inline">epoch_i</strong> index and used <strong class="source-inline">test loss</strong> as its name. MLflow will provide a plot for this metric too. We’ll be able to overlap this plot with the train metric plot to check whether there are issues such <span class="No-Break">as overfitting.</span></p>
			<p>Now that we’ve finished using the training cycle, we can end the run and log its parameters, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
mlflow.end_run();
mlflow.log_param("epochs", epochs);
mlflow.log_param("batch_size", batch_size);
mlflow.log_param("learning_rate", learning_rate);
mlflow.log_param("momentum", momentum);</pre>			<p>Here, we logged the run parameters with the <strong class="source-inline">end_run</strong> call. This is a requirement when using the MLflow API. Note that parameter values can have different types and that they were only <span class="No-Break">logged once.</span></p>
			<p>Now, let’s see how MLflow will display the program runs with different <span class="No-Break">training parameters.</span></p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor699"/>Experiments tracking process</h2>
			<p>The following<a id="_idIndexMarker1544"/> figure shows the MLflow UI after the tracking server has <span class="No-Break">been started:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer943">
					<img alt="Figure 13.1 – Overview of the MLflow UI without experiments and runs" src="image/B19849_13_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Overview of the MLflow UI without experiments and runs</p>
			<p>As we can see there are no experiments and there’s run information. After executing a program run with a set of parameters, the UI will look <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer944">
					<img alt="Figure 13.2 – Overview of the MLflow UI with a single experiment and one run" src="image/B19849_13_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Overview of the MLflow UI with a single experiment and one run</p>
			<p>As you can see, <strong class="bold">Linear regression</strong> appeared in the left panel. Also, there’s a new record for the run in the right-hand side table. Notice that the run name of <strong class="bold">peaceful-ray-50</strong> was automatically generated. Here, we can see the start time and how much time the run takes. Clicking on the run’s name will open the run details page, which looks <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer945">
					<img alt="Figure 13.3 – Overview of the run details in the MLflow UI" src="image/B19849_13_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Overview of the run details in the MLflow UI</p>
			<p>Here, we can see the start time and date, the experiment ID that this run is associated with, the run ID, and its <a id="_idIndexMarker1545"/>duration. Note that additional information might be provided here, such as a username, what datasets were used, tags, and the model source. These additional attributes can be also configured with the <span class="No-Break">REST API.</span></p>
			<p>At the bottom, we can see the <strong class="bold">Parameters</strong> table, where we can find the parameters we logged from our code. There’s also the <strong class="bold">Metrics</strong> table, which shows the final values for our train and test loss <span class="No-Break">value metrics.</span></p>
			<p>If we click on the <strong class="bold">Model metrics</strong> tab, the following page will <span class="No-Break">be displayed:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer946">
					<img alt="Figure 13.4 – The Model metrics page in the MLflow UI" src="image/B19849_13_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – The Model metrics page in the MLflow UI</p>
			<p>Here, we can see the train and test loss metrics plots. These plots show how the loss values changed over epochs. Usually, it’s <a id="_idIndexMarker1546"/>useful to overlap the train and test loss plots to see some dependencies. We can do this by clicking on the metric’s name on the page displayed in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.3</em>. The following page will be displayed upon clicking <span class="No-Break"><strong class="bold">train loss</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer947">
					<img alt="Figure 13.5 – The train loss metric plot" src="image/B19849_13_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – The train loss metric plot</p>
			<p>Here, we can see the plot for the single metric. On this page, we can configure some visualization parameters for the plot, such as the smoothness and the step. However, in this case, we’re interested<a id="_idIndexMarker1547"/> in the <strong class="bold">Y-axis</strong> field, which allows us to add additional metrics to the same plot. If we add the <strong class="bold">test loss</strong> metric, we’ll see the <span class="No-Break">following page:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer948">
					<img alt="Figure 13.6 – Overlapping the metric plots" src="image/B19849_13_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Overlapping the metric plots</p>
			<p>Now, we have two overlapped plots for the train and test metrics. In this visualization, we can see that for the first few epochs, the test loss was greater than the train loss, but after the 15<span class="superscript">th</span> epoch, the loss values were pretty similar. This means that there’s no <span class="No-Break">model overfitting.</span></p>
			<p>In this instance, we looked at the main regimes of the MLflow UI for the single train run. For more advanced cases, there will be pages that consist of artifacts and model sources, but we’ve skipped <span class="No-Break">them here.</span></p>
			<p>Next, let’s learn how to work with several runs for an experiment. We ran our application again but with a different value for momentum. MLflow shows us that we have two runs for the same experiment, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer949">
					<img alt="Figure 13.7 – The experiment with two runs" src="image/B19849_13_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – The experiment with two runs</p>
			<p>As we can see, there are two runs for the experiment. Also, there are only two minor differences between them—the names and their duration. To compare the runs, we must click on both <a id="_idIndexMarker1548"/>checkboxes that precede the run names, as <span class="No-Break">shown here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer950">
					<img alt="Figure 13.8 – Selecting both runs" src="image/B19849_13_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Selecting both runs</p>
			<p>After selecting both runs, the <strong class="bold">Compare</strong> button appears at the top of the <strong class="bold">Runs</strong> table. Clicking this button opens the <span class="No-Break">following page:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer951">
					<img alt="" role="presentation" src="image/B19849_13_09.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer952">
					<img alt="" role="presentation" src="image/B19849_13_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – Overview of the runs comparison page</p>
			<p>This page shows two runs side by side and shows different visualizations of the difference between various metrics and parameters. Note that the run parameter differences will also be<a id="_idIndexMarker1549"/> highlighted. From the left top panel, you can select the parameters and metrics you wish to compare. By doing this, we can see that the new run with a lower momentum value performs worse. This is indicated by the top plot, where lines connect parameters with metrics and there are scales with values. This can also be seen at the bottom in the metrics rows, where you can compare final <span class="No-Break">metric values.</span></p>
			<p>In this section, we learned how to use the MLflow UI to explore experiment run behavior, as well as <a id="_idIndexMarker1550"/>how to view metrics visualizations and how to compare different runs. All tracked information is saved by the tracking server and can be used later, after a server restart, so it’s quite a useful tool for <span class="No-Break">ML practitioners.</span></p>
			<h1 id="_idParaDest-273"><a id="_idTextAnchor700"/>Summary</h1>
			<p>Visualization and experiment tracking systems are essential tools for ML engineers. They allow us to understand the performance of models, analyze results, and improve the <span class="No-Break">overall process.</span></p>
			<p>TensorBoard is a popular visualization system that provides detailed information about model training, including metrics, loss curves, histograms, and more. It supports multiple frameworks, including TensorFlow, and allows us to easily compare <span class="No-Break">different runs.</span></p>
			<p>MLflow is an open source framework that offers end-to-end solutions for model life cycle management. It includes features such as experiment tracking, Model Registry, artifact management, and deployment. MLflow helps teams collaborate, reproduce experiments, and <span class="No-Break">ensure reproducibility.</span></p>
			<p>Both TensorBoard and MLflow are powerful tools that can be used together or separately, depending on <span class="No-Break">your needs.</span></p>
			<p>After understanding both TensorBoard and MLflow, we implemented a linear regression training example with experiment tracking. By doing so, we learned how to implement the REST API client for the MLflow server and how to use it to log metrics and parameters for an experiment. Then, we explored the MLflow UI, where we learned how to view an experiment and its run details, as well as metrics plots, and learned how to compare <span class="No-Break">different runs.</span></p>
			<p>In the next chapter, we’ll learn how to use ML models for computer vision on the Android mobile platform <span class="No-Break">using C++.</span></p>
			<h1 id="_idParaDest-274"><a id="_idTextAnchor701"/>Further reading</h1>
			<ul>
				<li><strong class="bold">MLflow REST </strong><span class="No-Break"><strong class="bold">API</strong></span><span class="No-Break">: </span><a href="https://mlflow.org/docs/latest/rest-api.html"><span class="No-Break">https://mlflow.org/docs/latest/rest-api.html</span></a></li>
				<li><strong class="bold">MLflow </strong><span class="No-Break"><strong class="bold">documentation</strong></span><span class="No-Break">: </span><a href="https://mlflow.org/docs/latest/index.html"><span class="No-Break">https://mlflow.org/docs/latest/index.html</span></a></li>
				<li><strong class="bold">TensorBoard </strong><span class="No-Break"><strong class="bold">documentation</strong></span><span class="No-Break">: </span><a href="https://www.tensorflow.org/tensorboard/get_started"><span class="No-Break">https://www.tensorflow.org/tensorboard/get_started</span></a></li>
				<li><em class="italic">How to use TensorBoard with </em><span class="No-Break"><em class="italic">PyTorch</em></span><span class="No-Break">: </span><a href="https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html"><span class="No-Break">https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html</span></a></li>
			</ul>
		</div>
	</body></html>