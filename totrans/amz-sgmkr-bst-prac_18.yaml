- en: 'Chapter 14: Managing SageMaker Features across Accounts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS publishes best practices around the management and governance of workloads.
    These practices touch on many areas, such as cost optimization, security, compliance,
    and ensuring the operational efficiency of workloads scaled on AWS. Multi-account
    patterns are one common architectural consideration when building, deploying,
    and operating workloads that utilize the features of Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we won't cover the well-established recommendations and considerations
    around the governance of AWS workloads across AWS accounts. Rather, we will specifically
    focus on some of the considerations around the usage of AWS features across AWS
    accounts. For more information about general recommendations for choosing the
    right account strategy, please refer to **AWS Management and Governance services**
    ([https://aws.amazon.com/products/management-and-governance/](https://aws.amazon.com/products/management-and-governance/))
    and the **AWS Multi-Account Landing Zone strategy** – **AWS Control Tower** ([https://docs.aws.amazon.com/controltower/latest/userguide/aws-multi-account-landing-zone.html](https://docs.aws.amazon.com/controltower/latest/userguide/aws-multi-account-landing-zone.html)).
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a multi-account strategy is built on the **AWS Well-Architected
    Framework**, where having multiple AWS accounts allows you to better govern and
    manage machine learning activities on **Amazon SageMaker** across the **Machine
    Learning Development Lifecycle (ML Lifecycle)**. The benefits of using multiple
    AWS accounts are documented for general workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll discuss the following topics as they relate to managing
    SageMaker features across multiple AWS accounts:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining an overview of the AWS multi-account environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the benefits of using multiple AWS accounts with Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining multi-account considerations with Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining an overview of the AWS multi-account environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many variations of multi-account strategies that are valid. Multi-account
    implementations can vary based on the organizational and technical needs of a
    customer. For the purposes of this chapter, we will focus on a basic multi-account
    strategy, focusing on only the accounts that are most relevant to a machine learning
    workload using Amazon SageMaker. We don't explicitly call out accounts (such as
    security or logging) because they are already well defined in the context of AWS
    governance practices. *Figure 14.1* illustrates the general, high-level accounts
    we will use to discuss the concepts in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Example of AWS accounts and SageMaker features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_14_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – Example of AWS accounts and SageMaker features
  prefs: []
  type: TYPE_NORMAL
- en: 'Using *Figure 14.1* as an example, the following AWS accounts may be used as
    part of an end-to-end ML Lifecycle. Please keep in mind that account naming and
    resource placement may vary considerably across implementations. Each account
    is described at a high level, in order to focus more on the account purpose versus
    the naming standard itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared Services account**: This account can be named many things, and is
    also referred to as a **DevOps** or application management account. For the purposes
    of this chapter, we refer to this account as the one that can often include the
    services and tooling used for the management of end-to-end pipelines and the ongoing
    management of workloads.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data platform/data lake**: This account acts as the central repository for
    datasets, both raw and curated, used for model-building activities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data science account**: This account (or accounts) represents the environments
    where model development activities are performed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test account**: This account represents the environment where a model will
    be tested. This account typically includes integration and performance testing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Production account**: This account represents the environment hosting models
    supporting live applications and workloads. This account typically has the highest
    levels of controls and restrictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Service Catalog master account**: The purpose of this account is to maintain
    a central hub of products that can be offered through the **AWS Service Catalog**
    and used to consistently provision resources in spoke accounts, such as the **data
    science account**. A spoke account is an AWS account that has been given access
    to portfolios managed from the master account.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, these accounts are high-level representations of a potential account
    structure and are not inclusive of every variation that is valid given the requirements
    of your own environments. In the next section, we'll discuss the benefits of using
    multiple AWS accounts specifically as they relate to using Amazon SageMaker across
    the ML Lifecycle
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the benefits of using multiple AWS accounts with Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll cover the general, high-level benefits of using multiple
    AWS accounts. We''ll also discuss the considerations that are specific to using
    Amazon SageMaker across the ML Lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Benefit #1**: Implementing specific security controls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple AWS accounts allows customers to implement security controls
    that are specific to the workload, environment, or data. As an example, some workloads
    may have unique security requirements (such as PCI compliance) and require additional
    controls. Using multiple accounts allows you to maintain fine-grained controls
    that are isolated and auditable at the AWS account level.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the model-building activities included in the ML Lifecycle, using multiple
    AWS accounts allows you to create and manage data science environments that include
    the controls that are specific to machine learning, as well as to your security
    requirements. With machine learning, data scientists need access to live production
    data. Typically, that data should be scrubbed of any sensitive data before a data
    scientist gains access. However, there are use cases where a data scientist may
    need access to that sensitive data. By separating data science environments that
    have access to sensitive data and those that do not have access to sensitive data,
    you're able to implement controls at the account level, as well as to audit at
    the account level.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For model deployment activities included in the ML Lifecycle, you will want
    to ensure your models serving live traffic or providing critical inference data
    are managed and controlled. This would be the case with any other production application.
    You wish to ensure availability. Just as you would not implement a live web application
    in the same account where developers have broad access, the same is true for machine
    learning workloads serving live production workloads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As an example, a **SageMaker endpoint** serving a production application should
    be hosted in an AWS account that has all of the controls and restricted access
    in place (you would want this to be the case as with any other production workload).
    This ensures the endpoint isn't inadvertently deleted in a lower-level account
    that may have fewer controls and broader access permissions granted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Benefit #2**: Supporting the needs of multiple teams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large organizations and enterprises are often looking for scalable mechanisms
    to support the resource needs and responsibilities of different teams. Across
    lines of business, it's common to have separate AWS accounts. The same is true
    for machine learning workloads. An example here includes **data science environments**
    (as discussed in [*Chapter 2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Data Science Environments*), where each team may have different requirements for
    an environment in which to build machine learning models. In this case, it's common
    to have multiple data science environments supporting multiple teams, as well
    as supporting the requirements across and within teams.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Examining multi-account considerations with Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll cover multi-account considerations with Amazon SageMaker.
    We'll first look at a general reference architecture, then discuss some of the
    considerations for specific SageMaker features across the ML Lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.2* shows an example of a multi-account structure mapping key SageMaker
    features and other common AWS services to the accounts they are typically used
    in. This is not a one-size-fits-all view, as there may be other AWS services or
    third-party tools that are performing one or more of the functions performed by
    the AWS services shown. As an example, your model registry may be the **SageMaker
    model registry**, or it could alternatively be **Amazon DynamoDB** or a tool such
    as **MLflow**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Example of service use across AWS accounts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_14_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – Example of service use across AWS accounts
  prefs: []
  type: TYPE_NORMAL
- en: 'The placement of the AWS, or equivalent, supporting the ML Lifecycle map to
    the phase, model build, or model deploy. This is in combination with the benefits
    addressed earlier in being able to implement security controls by accounts, as
    well as to support the requirements of the different roles and personas that operate
    within each account. The naming and structure of accounts may vary across multi-account
    implementations. Therefore, in the following list, we describe the purpose of
    each account, knowing these may vary across implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: A Shared Services account, or DevOps account, is often used to centralize the
    tooling that is used to manage workloads across multiple accounts and environments.
    In this case, you see a few common services, such as the **Amazon Elastic Container
    Registry** for managing SageMaker compatible images for training and inference.
    You also often find developer tools that enable **continuous integration** (**CI)**/
    **continuous delivery or deployment** (**CD)** practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are the tools that are needed to automate and orchestrate the steps of
    the machine learning workflow across accounts. These can include native **AWS
    Developer Tools** or third-party tooling such as **GitHub** or **Jenkins**. The
    tools and services used in this account require cross-account **identity and access
    management** (**IAM**) permission policies. Finally, you need to create centralized
    dashboards for monitoring the health of your machine learning workloads. These
    shared dashboards are often placed in the Shared Services account, an **infrastructure
    account**, or one of the environment- or workload-specific accounts, such as production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **data platform**, or **data lake account**, contains a data lake using
    a native service, such as **AWS Lake Formation** or a custom data lake. This account
    is also a common option for placing the centralized feature store that is used
    to store features for use across teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data science account is primarily used for model building activities so
    this includes all of the activities required to perform data understanding, feature
    engineering, model training across experiments, and model evaluation. This account
    requires access to SageMaker features needed for those model-building activities
    including features such as **Amazon SageMaker Studio**, **SageMaker training jobs**,
    **SageMaker Pocessing jobs**, and **SageMaker Data Wrangler**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the common features needed for model building, there are additional
    AWS services that get provisioned in this account when you are using **SageMaker
    projects**. By default, SageMaker projects automatically provision and configure
    AWS Developer Tools and the AWS Service Catalog products for built-in MLOps project
    templates in the account you are using for your model-building activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workload or environment-specific accounts, such as test and production, are
    used to host live models. These accounts also commonly host the broader solution
    where your model is used. From a SageMaker perspective, the features used in these
    accounts typically focus on model deploy and operate activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you may also have an **AWS Service Catalog master** or infrastructure
    account that contains the portfolios of products that can be shared across multiple
    teams. This is known as the hub account. This can be used to create and manage
    a central catalog of products for data science environments or for custom MLOps
    project templates with SageMaker projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some AWS features are very specific to the persona and phase in the ML Lifecycle
    where they are needed. As an example, SageMaker training jobs are typically needed
    by data scientists for model-building activities or are needed as part of an automated
    model retraining workflow. However, there are several AWS services that span phases
    of the ML Lifecycle that require some unique considerations. These will be explored
    further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for SageMaker features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several SageMaker features that require additional considerations
    when attempting to implement them in a multi-account strategy, specifically because
    these features are used across the ML Lifecycle. Considerations for features,
    such as SageMaker Processing, SageMaker training jobs, and SageMaker hosting,
    are generally specific to a phase in the lifecycle. Therefore, their placement
    across accounts is covered in *Figure 14.3*. In this section, we'll cover a few
    of the SageMaker features that span the ML Lifecycle and require additional consideration
    as part of your multi-account strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker Pipelines allows you to code your machine learning pipelines using
    the Amazon SageMaker Python SDK. Pipelines includes SageMaker native steps focused
    on data preparation (via SageMaker Processing), model training (via SageMaker
    training jobs), and model deployment (via SageMaker batch transform). `CallbackStep`
    to integrate with other AWS services or third-party tasks. Finally, Pipelines
    has built-in steps for pipeline functionality, such as a conditional step. All
    of the current capabilities within SageMaker Pipelines focus on model building
    and model deployment for batch inference. As a result, we'll look at two common
    patterns that have cross-account considerations when using SageMaker Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the first pattern, we'll discuss an end-to-end pipeline scenario where you
    are deploying a model for real-time inference using SageMaker hosting. In this
    case, you can use SageMaker Pipelines in your data science account to create a
    pipeline that can be used to automate the model-building activities. These activities
    include data preparation, model training, model evaluation, and a conditional
    step for model registration. Once a model passes evaluation and is registered,
    it can be used as a trigger for downstream deployment to your accounts (such as
    testing or production) that will host and integrate deployed endpoints. This same
    pipeline can be used for your retraining workflows.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, model deployment to higher environments can be done using a cross-account
    resource policy, as shown in *Figure 14.3*. The cross-account resource policy
    is created for the **model group** in the **SageMaker model registry**. That model
    group contains the model versions, the Amazon ECR repository for the inference
    image, and the S3 location of the model artifacts. A cross-account resource policy
    can be created with all three of these resources that then allows you to deploy
    a model that was created in your data science environment into your application
    or workload environments (such as testing or production).
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Cross-account resource policy to deploy a model trained in
    a data science account'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_14_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – Cross-account resource policy to deploy a model trained in a data
    science account
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second pattern, we''ll discuss an end-to-end pipeline scenario where
    you are deploying a model for batch inference using SageMaker hosting. In this
    case, you can use SageMaker Pipelines in your data science account to create a
    pipeline that can be used to automate the model-building activities. These include
    data preparation, model training, model evaluation, a conditional step for model
    registration, and a batch transform step. In this case, there are two options
    depending on your use case and requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Run your end-to-end pipeline in your data science account**: This option
    is valid if you are using batch transform to validate your models or you''re running
    batch jobs that don''t have production-level availability requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run your end-to-end pipeline in workload accounts**: This option is valid
    if you are using batch transform to deploy models that have production-level availability
    requirements and/or require integration with systems in higher-level environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker projects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Amazon SageMaker projects build on SageMaker Pipelines by incorporating CI/CD
    practices (such as source and version control) combined with automated deployment
    pipelines into one or more target environments. When considering integrating SageMaker
    projects with multiple AWS accounts, the following are key points to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: When you enable project templates for your Studio domain or domain users, the
    account where projects are enabled is the one that will be used for the built-in
    MLOps project templates offered through AWS Service Catalog. If you build custom
    MLOps project templates, you can still use the hub-and-spoke model to manage your
    portfolio and products in a Service Catalog master account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All built-in MLOps project templates will provision and configure the following
    resources in the same account where projects are enabled: **AWS CodePipeline**,
    **AWS CodeBuild**, **AWS CodeCommit**, and **Amazon EventBridge**. This is important
    as some organizations assume or require these services to be centrally configured
    and managed through a shared services account (or equivalent).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The built-in MLOps project templates will deploy your SageMaker endpoints to
    the same account where projects are enabled. This behavior can be modified. However,
    the model registry still exists in the data science account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon SageMaker Feature Store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazon SageMaker Feature Store** allows creating and sharing features, both
    for model-building activities and model inference. Because a feature store can
    be used for both model-building activities as well as a dependency for model inference,
    it''s important to ensure features remain consistent across teams and are consistently
    available when needed.'
  prefs: []
  type: TYPE_NORMAL
- en: When you create a feature store, it gets instantiated in the account that you
    created it in. However, that may not be the optimal choice when centralizing features
    for sharing across teams, or when using the feature store for real-time inference.
    If you create the feature store in your data science account, that account may
    have fewer controls and more access permissions in place for a broader set of
    roles. This creates risk when supporting production applications.
  prefs: []
  type: TYPE_NORMAL
- en: There are two common cross-account patterns related to Feature Store that facilitate
    feature sharing and consistency across teams, as well as allowing the flexibility
    for team- or organization-specific feature stores when needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first pattern, shown in *Figure 14.4*, a central feature store is created
    in a separate AWS account that is accessible via an IAM cross-account role for
    both the population and consumption of features. For the population of features,
    this is typically done through a feature pipeline that is automated and collecting
    data at regular frequencies. However, it can also be done from the data science
    environment for more static features. Features can then be consumed for both inference
    as well as for model-building activities. Model-building activities often consume
    features from the offline feature store using cross-account permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Central Feature Store pattern'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_14_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – Central Feature Store pattern
  prefs: []
  type: TYPE_NORMAL
- en: In the second pattern, similar to *Figure 14.4*, there is a central feature
    store that is used for sharing features that may be common or useful across teams,
    but there is also the flexibility for individual teams to create their own feature
    stores in separate AWS accounts. This pattern is useful to facilitate the ability
    to share common features in a central store, while also allowing workload- or
    application-specific features to be secured in an account that only requires access
    by the specific teams or applications that need those features.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Data Wrangler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Amazon SageMaker Data Wrangler allows data scientists to explore and prepare
    data for machine learning during the model build phases of the ML Lifecycle. Because
    Data Wrangler is purpose-built for feature engineering and data preparation, the
    most common persona that will work with Data Wrangler are **ML builders**. Most
    model-building activities are going to happen inside one or more data science
    accounts; however, you typically need a way to securely access data from a data
    platform or data lake account for those model-building activities.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.5* illustrates a common pattern for enabling cross-account access
    from a data science account, where Data Wrangler is being used, to a data platform/data
    lake account, where the data typically resides. In this case, we are using AWS
    Lake Formation for our secure data lake. The same concepts apply when utilizing
    other technologies for your data lake; however, the implementation may differ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – Cross-account access for SageMaker Data Wrangler'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_14_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – Cross-account access for SageMaker Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: With Data Wrangler, you're able to enable cross-account permissions using AWS
    IAM. To do this, you need to set up cross-account permissions for Data Wrangler
    in the data science account that allows access to the data tables stored in your
    data platform/data lake account. This is accomplished through Lake Formation permissions.
    This setup allows you to still provide access to datasets for your data scientists,
    but also allows you to take advantage of the security controls that Lake Formation
    offers.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can choose to share only specific tables or even to share only
    specific columns of tables stored in your data lake. Tables are shared using AWS
    Resource Access Manager. This provides a way to share Lake Formation tables across
    AWS accounts. This allows users to access shared tables in secondary accounts.
    These shared tables are accessible directly in Lake Formation, but they are also
    available as a data source, via Amazon Athena, in your Data Wrangler UI.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the benefits of using multiple accounts to manage
    and operate machine learning workloads that use Amazon SageMaker across the ML
    Lifecycle. We also looked at common patterns for account isolation across the
    ML Lifecycle. Finally, we focused specifically on the SageMaker features that
    are most often used across accounts, and the considerations you should be aware
    of when architecting and building end-to-end machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter wraps up the book where we covered best practices for SageMaker
    across features spanning the machine learning lifecycle of data preparation, model
    training, and operations. In this book, we discussed best practices, as well as
    considerations, that you can draw on when creating your own projects. We used
    an example use case, using open weather data to demonstrate the concepts throughout
    the chapters of the book. This was done so you can get hands-on with the concepts
    and practices discussed. We hope you're able to apply these practices to your
    own projects while benefiting from the overall capabilities and features offered
    by Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please see the following references for general AWS best practices on governance
    and multi-account strategies, as well as information specific to SageMaker features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Establishing best practices in your AWS environment : [https://aws.amazon.com/organizations/getting-started/best-practices/](https://aws.amazon.com/organizations/getting-started/best-practices/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS Control Tower – AWS services to establish and manage multiple AWS accounts:
    [https://aws.amazon.com/controltower/](https://aws.amazon.com/controltower/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker – Deploying a model to a different AWS account: [https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-cross-account-model/](https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-cross-account-model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker Data Wrangler – Enable cross-account access: [https://aws.amazon.com/blogs/machine-learning/enable-cross-account-access-for-amazon-sagemaker-data-wrangler-using-aws-lake-formation/](https://aws.amazon.com/blogs/machine-learning/enable-cross-account-access-for-amazon-sagemaker-data-wrangler-using-aws-lake-formation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker Pipelines – Multi-account deployments: [https://aws.amazon.com/blogs/machine-learning/multi-account-model-deployment-with-amazon-sagemaker-pipelines/](https://aws.amazon.com/blogs/machine-learning/multi-account-model-deployment-with-amazon-sagemaker-pipelines/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker Feature Store: [https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/](https://aws.amazon.com/blogs/machine-learning/enable-feature-reuse-across-accounts-and-teams-using-amazon-sagemaker-feature-store/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
