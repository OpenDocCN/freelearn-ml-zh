<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Classification</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Discriminant function analysis - geological measurements on brines from wells</li><li class="listitem" style="list-style-type: disc">Multinomial logistic regression - understanding program choices made by students</li><li class="listitem" style="list-style-type: disc">Tobit regression - measuring students' academic aptitude</li><li class="listitem" style="list-style-type: disc">Poisson regression - understanding species present in Galapagos Islands</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec10"/>Introduction</h1></div></div></div><p>
<span class="strong"><strong>Discriminant analysis</strong></span> is used to distinguish distinct sets of observations and allocate new observations to previously defined groups. For example, if a study was to be carried out in order to investigate the variables that discriminate between fruits eaten by (1) primates, (2) birds, or (3) squirrels, the researcher could collect data on numerous fruit characteristics of those species eaten by each of the animal groups. Most fruits will naturally fall into one of the three categories. Discriminant analysis could then be used to determine which variables are the best predictors of whether a fruit will be eaten by birds, primates, or squirrels. Discriminant analysis is commonly used in biological species classification, in medical classification of tumors, in facial recognition technologies, and in the credit card and insurance industries for determining risk. The main goals of discriminant analysis are discrimination and classification. The assumptions regarding discriminant analysis are multivariate normality, equality of variance-covariance within group and low multicollinearity of the variables.</p><p>
<span class="strong"><strong>Multinomial logistic regression</strong></span> is used to predict categorical placement in or the probability of category membership on a dependent variable, based on multiple independent variables. It is used when the dependent variable has more than two nominal or unordered categories, in which dummy coding of independent variables is quite common. The independent variables can be either dichotomous (binary) or continuous (interval or ratio in scale). Multinomial logistic regression uses maximum likelihood estimation to evaluate the probability of categorical membership. It uses maximum likelihood estimation rather than the least squares estimation used in traditional multiple regression. The general form of the distribution is assumed. The starting values of the estimated parameters are used and the likelihood that the sample came from a population with those parameters is computed. The values of the estimated parameters are adjusted iteratively until the maximum likelihood value for the estimated parameters is obtained.</p><p>
<span class="strong"><strong>Tobit regression</strong></span> is used to describe the relationship between non-negative dependent variables and independent variables. It is also known as a censored regression model, designed to estimate linear relationships between variables when there is either left or right censoring in the dependent variable. Censoring takes place when cases with a value at or above some threshold, all take on the value of that threshold, so that the true value might be equal to the threshold, but it might also be higher. The Tobit model has been used in a large number of applications where the dependent variable is observed to be zero for some individuals in the sample (automobile expenditures, medical expenditures, hours worked, wages, and so on). This model is for metric dependent variables and then it is limited in the sense that we observe it only if it is above or below some cut off level. For example:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The wages may be limited from below by the minimum wage</li><li class="listitem" style="list-style-type: disc">The donation amount given to charity</li><li class="listitem" style="list-style-type: disc">Top coding income</li><li class="listitem" style="list-style-type: disc">Time used and leisure activity of individuals</li></ul></div><p>
<span class="strong"><strong>Poisson regression</strong></span> deals with situations in which the dependent variable is a count. Poisson regression is similar to regular multiple regression except that the dependent (Y) variable is an observed count that follows the Poisson distribution. Thus, the possible values of Y are the nonnegative integers: 0, 1, 2, 3, and so on. It is assumed that large counts are rare. Hence, Poisson regression is similar to logistic regression, which also has a discrete response variable. However, the response is not limited to specific values as it is in logistic regression.</p></div></div>
<div class="section" title="Discriminant function analysis - geological measurements on brines from wells"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec11"/>Discriminant function analysis - geological measurements on brines from wells</h1></div></div></div><p>Let us assume that a study of ancient artifacts that have been collected from mines needs to be carried out. Rock samples have been collected from the mines. On the collected rock samples geochemical measurements have been carried out. A similar study has been carried out on the collected artifacts. In order to separate the samples into the mine from which they were excavated, DFA can be used as a function. The function can then be applied to the artifacts to predict which mine was the source of each artifact.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec1"/>Getting ready</h2></div></div></div><p>In order to perform discriminant function analysis we shall be using a dataset collected from mines.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec0"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset on data analysis in geology titled <code class="literal">BRINE</code> shall be used. This can be obtained from <a class="ulink" href="http://www.kgs.ku.edu/Mathgeo/Books/Stat/ASCII/BRINE.TXT">
http://www.kgs.ku.edu/Mathgeo/Books/Stat/ASCII/BRINE.TXT
</a>. The dataset is in a standard form, with rows corresponding to samples and columns corresponding to variables. Each sample is assigned to a stratigraphic unit, listed in the last column. There are 19 cases and eight variables in the dataset. The eight numeric measurements include the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">No</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">HCO3</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">SO4</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">CL</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">CA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">MG</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">NA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Group</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec2"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec1"/>Step 2 - exploring data</h3></div></div></div><p>The first step is to load the following package:</p><pre class="programlisting">    &gt; <span class="strong"><strong>library(MASS)</strong></span>
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note1"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>Let's explore the data and understand the relationships among the variables. We'll begin by importing the txt data file named <code class="literal">brine.txt</code>. We will be saving the data to the <code class="literal">brine</code> data frame, as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; brine &lt;- read.table("d:/brine.txt", header=TRUE, sep=",", row.names=1)</strong></span>
</pre><p>Next we shall print the <code class="literal">brine</code> data frame. The <code class="literal">head()</code> function returns the <code class="literal">brine</code> data frame. The <code class="literal">brine</code> data frame is passed as an input parameter. Use the following code:</p><pre class="programlisting">    &gt; <span class="strong"><strong>head(brine)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>    HCO3    SO4      Cl      Ca      Mg       Na   GROUP</strong></span>
<span class="strong"><strong>1   10.4   30.0    967.1    95.9    53.7    857.7     1</strong></span>
<span class="strong"><strong>2    6.2   29.6   1174.9   111.7    43.9   1054.7     1</strong></span>
<span class="strong"><strong>3    2.1   11.4   2387.1   348.3   119.3   1932.4     1</strong></span>
<span class="strong"><strong>4    8.5   22.5   2186.1   339.6    73.6   1803.4     1</strong></span>
<span class="strong"><strong>5    6.7   32.8   2015.5   287.6    75.1   1691.8     1</strong></span>
<span class="strong"><strong>6    3.8   18.9   2175.8   340.4    63.8   1793.9     1</strong></span>
</pre><p>DFA assumes multivariate normality. The data must be checked to verify the normality before performing the analysis. 
</p><p>
In order to verify the appropriateness of the transformation, plotting of the data is carried out. The <code class="literal">pairs</code>
<code class="literal">()</code> function is used to plot the data. It produces a matrix of scatterplots. The cross-plots should only compare the measurement variables in columns 1-6, the last (7th column) is the name of the group. Consider the following:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pairs(brine[ ,1:6])</strong></span>
</pre><p>The plot is as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_001.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - transforming data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec2"/>Step 3 - transforming data</h3></div></div></div><p>It is visible that the data has a comet-shaped distribution pattern. This indicates that log transformation of the data is required, which is common for geochemical data. It is good practice to first make a copy of the entire dataset, and then apply the log transformation only to the geochemical measurements. Since the data includes zeros as well; <code class="literal">log+1</code> transformation should be carried out instead of <code class="literal">log</code> transformation on the dataset. The <code class="literal">brine</code> data frame is copied to the <code class="literal">brine.log</code> data frame. The log transformation on the data frame is carried out. As stated earlier, log transformation is carried out. Look at the following code:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; brine.log &lt;- brine</strong></span>
<span class="strong"><strong>    &gt; brine.log[ ,1:6] &lt;- log(brine[ ,1:6]+1)</strong></span>
<span class="strong"><strong>    &gt; pairs(brine.log[ ,1:6])</strong></span>
</pre><p>After the data transformation, in order to re-evaluate the normality condition using the <code class="literal">pairs()</code> function data frame, <code class="literal">brine.log</code> is replotted. The distribution appears to be more normal. The skewness has been reduced compared to the earlier plot:</p><pre class="programlisting">    &gt; <span class="strong"><strong>pairs(brine.log[ ,1:6])</strong></span>
</pre><p>The plot is as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_002.jpg" alt="Step 3 - transforming data"/></div><p>
</p></div><div class="section" title="Step 4 - training the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec3"/>Step 4 - training the model</h3></div></div></div><p>The next step is about training the model. This is carried out by discriminant function analysis. The <code class="literal">lda()</code> function is called to perform discriminant function analysis as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; brine.log.lda &lt;- lda(GROUP ~ HCO3 + SO4 + Cl + Ca + Mg + Na, data=brine.log)</strong></span>
</pre><p>The format of this call is much like a linear regression or ANOVA, in that we specify a formula. Here, the <code class="literal">GROUP</code> variable should be treated as the dependent variable, with the geochemical measurements as the independent variables. In this case, no interactions between the variables are being modeled, so the variables are added with <code class="literal">+</code> instead of <code class="literal">*</code>. Because <code class="literal">attach()</code> was not called, the name of the data frame must be supplied to the data parameter. After running the DFA, the first step is to view the results, as follows:</p><pre class="programlisting">    &gt; <span class="strong"><strong>brine.log.lda</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>lda(GROUP ~ HCO3 + SO4 + Cl + Ca + Mg + Na, data = brine.log)</strong></span>
<span class="strong"><strong>Prior probabilities of groups:</strong></span>
<span class="strong"><strong>        1             2             3 </strong></span>
<span class="strong"><strong>0.3684211     0.3157895     0.3157895 </strong></span>
<span class="strong"><strong>Group means:</strong></span>
<span class="strong"><strong>        HCO3        SO4         Cl         Ca         Mg         Na</strong></span>
<span class="strong"><strong>1   1.759502   3.129009   7.496891   5.500942   4.283490   7.320686</strong></span>
<span class="strong"><strong>2   2.736481   3.815399   6.829565   4.302573   4.007725   6.765017</strong></span>
<span class="strong"><strong>3   1.374438   2.378965   6.510211   4.641049   3.923851   6.289692</strong></span>
<span class="strong"><strong>Coefficients of linear discriminants:</strong></span>
<span class="strong"><strong>                  LD1             LD2</strong></span>
<span class="strong"><strong>HCO3      -1.67799521      0.64415802</strong></span>
<span class="strong"><strong>SO4        0.07983656      0.02903096</strong></span>
<span class="strong"><strong>Cl         22.27520614     -0.31427770</strong></span>
<span class="strong"><strong>Ca        -1.26859368      2.54458682</strong></span>
<span class="strong"><strong>Mg        -1.88732009     -2.89413332</strong></span>
<span class="strong"><strong>Na       -20.86566883      1.29368129</strong></span>
<span class="strong"><strong>Proportion of trace:</strong></span>
<span class="strong"><strong>      LD1        LD2 </strong></span>
<span class="strong"><strong>   0.7435     0.2565</strong></span>
</pre><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The first part of the output displays the formula that was fitted.</li><li class="listitem" style="list-style-type: disc">The second section is the prior probabilities of the groups, which reflects the proportion of each group within the dataset. In other words, if you had no measurements and the number of measured samples represented the actual relative abundances of the groups, the prior probabilities would describe the probability that any unknown sample would belong to each of the groups.</li><li class="listitem" style="list-style-type: disc">The third section shows the group means, which is a table of the average value of each of the variables for each of your groups. Scanning this table can help you to see if the groups are distinctive in terms of one or more of the variables.</li><li class="listitem" style="list-style-type: disc">The fourth section reports the coefficients of the discriminant function (a, b, and c). Because there are three groups, there are 3-1 linear discriminants (if you had only two groups, you would need only 1 [2-1] linear discriminants). For each linear discriminant (<code class="literal">LD1</code> and <code class="literal">LD2</code>), there is one coefficient corresponding, in order, to each of the variables.</li><li class="listitem" style="list-style-type: disc">Finally, the fifth section shows the proportion of the trace, which gives the variance explained by each discriminant function. Here, first the discriminant explains 75% of the variance, with the remainder explained by the second discriminant.</li></ul></div></div><div class="section" title="Step 5 - classifying the data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec4"/>Step 5 - classifying the data</h3></div></div></div><p>The <code class="literal">predict()</code> function, also part of the <code class="literal">MASS</code> package, uses the <code class="literal">lda()</code> results to assign the samples to the groups. In other words, since <code class="literal">lda()</code> derived a linear function that should classify the groups, <code class="literal">predict()</code> allows you to apply this function to the same data to see how successful the classification function is. Following the statistical convention that x-hat is the prediction of x, (hat is added to the object name to make it clear that these are the predictions). Consider the following:</p><pre class="programlisting">    &gt; <span class="strong"><strong>brine.log.hat &lt;- predict(brine.log.lda)</strong></span>
</pre><p>Let us print <code class="literal">brine.log.hat</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; brine.log.hat</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>$class</strong></span>
<span class="strong"><strong> [1] 2 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3</strong></span>
<span class="strong"><strong>Levels: 1 2 3</strong></span>
<span class="strong"><strong>$posterior</strong></span>
<span class="strong"><strong>              1                2                3</strong></span>
<span class="strong"><strong>1    2.312733e-01     7.627845e-01     5.942270e-03</strong></span>
<span class="strong"><strong>2    9.488842e-01     3.257237e-02     1.854347e-02</strong></span>
<span class="strong"><strong>3    8.453057e-01     9.482540e-04     1.537461e-01</strong></span>
<span class="strong"><strong>4    9.990242e-01     8.794725e-04     9.632578e-05</strong></span>
<span class="strong"><strong>5    9.965920e-01     2.849903e-03     5.581176e-04</strong></span>
<span class="strong"><strong>6    9.984987e-01     1.845534e-05     1.482872e-03</strong></span>
<span class="strong"><strong>7    8.676660e-01     7.666611e-06     1.323263e-01</strong></span>
<span class="strong"><strong>8    4.938019e-03     9.949035e-01     1.584755e-04</strong></span>
<span class="strong"><strong>9    4.356152e-03     9.956351e-01     8.770078e-06</strong></span>
<span class="strong"><strong>10   2.545287e-05     9.999439e-01     3.066264e-05</strong></span>
<span class="strong"><strong>11   2.081510e-02     9.791728e-01     1.210748e-05</strong></span>
<span class="strong"><strong>12   1.097540e-03     9.989023e-01     1.455693e-07</strong></span>
<span class="strong"><strong>13   1.440307e-02     9.854613e-01     1.356671e-04</strong></span>
<span class="strong"><strong>14   4.359641e-01     2.367602e-03     5.616683e-01</strong></span>
<span class="strong"><strong>15   6.169265e-02     1.540353e-04     9.381533e-01</strong></span>
<span class="strong"><strong>16   7.500357e-04     4.706701e-09     9.992500e-01</strong></span>
<span class="strong"><strong>17   1.430433e-03     1.095281e-06     9.985685e-01</strong></span>
<span class="strong"><strong>18   2.549733e-04     3.225658e-07     9.997447e-01</strong></span>
<span class="strong"><strong>19   6.433759e-02     8.576694e-03     9.270857e-01</strong></span>
<span class="strong"><strong>$x</strong></span>
<span class="strong"><strong>              LD1            LD2</strong></span>
<span class="strong"><strong>1      -1.1576284     -0.1998499</strong></span>
<span class="strong"><strong>2     -0.1846803      0.6655823</strong></span>
<span class="strong"><strong>3       1.0179998      0.6827867</strong></span>
<span class="strong"><strong>4      -0.3939366      2.6798084</strong></span>
<span class="strong"><strong>5      -0.3167164      2.0188002</strong></span>
<span class="strong"><strong>6       1.0061340      2.6434491</strong></span>
<span class="strong"><strong>7       2.0725443      1.5714400</strong></span>
<span class="strong"><strong>8      -2.0387449     -0.9731745</strong></span>
<span class="strong"><strong>9      -2.6054261     -0.2774844</strong></span>
<span class="strong"><strong>10     -2.5191350     -2.8304663</strong></span>
<span class="strong"><strong>11     -2.4915044      0.3194247</strong></span>
<span class="strong"><strong>12     -3.4448401      0.1869864</strong></span>
<span class="strong"><strong>13     -2.0343204     -0.4674925</strong></span>
<span class="strong"><strong>14      1.0441237     -0.0991014</strong></span>
<span class="strong"><strong>15      1.6987023     -0.6036252</strong></span>
<span class="strong"><strong>16      3.9138884     -0.7211078</strong></span>
<span class="strong"><strong>17      2.7083649     -1.3896956</strong></span>
<span class="strong"><strong>18      2.9310268     -1.9243611</strong></span>
<span class="strong"><strong>19      0.7941483     -1.2819190</strong></span>
</pre><p>The output starts with the assigned classifications of each of our samples. Next, it lists the posterior probabilities of each sample to each group, with the probabilities in each row (that is, for each sample) summing to 1.0. These posterior probabilities measure the strength of each classification. If one of these probabilities for a sample is much greater than all the others, that sample is assigned to one group with a high degree of certainty. If two or more of the probabilities are nearly equal, the assignment is much less certain. If there are many groups, the following command is a quick way to find the maximum probability for each sample:</p><pre class="programlisting">
<span class="strong"><strong>&gt; apply(brine.log.hat$posterior, MARGIN=1, FUN=max)</strong></span>
<span class="strong"><strong>        1           2             3  4             5         6             7         8 </strong></span>
<span class="strong"><strong>0.7627845 0.9488842 0.8453057 0.9990242 0.9965920 0.9984987 0.8676660 0.9949035 </strong></span>
<span class="strong"><strong>        9          10          11        12          13        14          15        16 </strong></span>
<span class="strong"><strong>0.9956351 0.9999439 0.9791728 0.9989023 0.9854613 0.5616683 0.9381533 0.9992500 </strong></span>
<span class="strong"><strong>       17          18          19 </strong></span>
<span class="strong"><strong>0.9985685 0.9997447 0.9270857</strong></span>
</pre><p>Since most of the probabilities in the dataset are large (&gt;0.9), this indicates that most of the samples in the set have been assigned to one group.</p><p>If most of these probabilities are large, the overall classification is successful. The last part of the <code class="literal">predict()</code> output lists the scores of each sample for each discriminant function axis. These scores can be plotted to show graphically how the groups are distributed in the discriminant function, just as scores from a principal components analysis could be plotted, as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(brine.log.lda)</strong></span>
</pre><p>The three groups occupy distinctly different and non-overlapping regions. There is just one case of group 1 being close to group 2, so one can clearly state that the discrimination has been successful.</p><p>The plot is as shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_003.jpg" alt="Step 5 - classifying the data"/></div><p>
</p><p>A second type of plot shows the data plot along a particular discriminant function axis as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(brine.log.lda, dimen=1, type="both")</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_02_004.jpg" alt="Step 5 - classifying the data"/></div><p>
</p><p>Again, note the good separation of the groups along discriminant function 1, and particularly so for group 2.</p></div><div class="section" title="Step 6 - evaluating the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec5"/>Step 6 - evaluating the model</h3></div></div></div><p>The effectiveness of DFA in classifying the groups must be evaluated, and this is done by comparing the assignments made by <code class="literal">predict()</code> to the actual group assignments. The <code class="literal">table()</code> function is most useful for this. By convention, it is called with the actual assignments as the first argument and the fitted assignments as the second argument, as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; tab &lt;- table(brine.log$GROUP, brine.log.hat$class)</strong></span>
</pre><p>Printing the value of tab.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; tab</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>      1   2   3</strong></span>
<span class="strong"><strong>  1   6   1   0</strong></span>
<span class="strong"><strong>  2   0   6   0</strong></span>
<span class="strong"><strong>  3   0   0   6</strong></span>
</pre><p>The rows in the output correspond to the groups specified in the original data and the columns correspond to the classification made by the DFA. In a perfect classification, large values would lie along the diagonal, with zeroes off the diagonal, which would indicate that all samples that belong to group 1 were discriminated by the DFA as belonging to group 1, and so on. The form of this table can give you considerable insight into which groups are reliably discriminated. It can also show which groups are likely to be confused and which types of misclassification are more common than others. The following command will calculate the overall predictive accuracy, that is, the proportion of cases that lie along the diagonal:</p><pre class="programlisting">
<span class="strong"><strong>&gt; sum(tab[row(tab) == col(tab)]) / sum(tab)</strong></span>
</pre><p>The result is as follows:</p><pre class="programlisting">
<span class="strong"><strong>[1] 0.9473684</strong></span>
</pre><p>Here the predictive accuracy is almost 95%, quite a success. This approach measures what is called the resubstitution error, how well the samples are classified when all the samples are used to develop the discriminant function.</p><p>A second approach for evaluating a DFA is leave-one-out cross-validation (also called jackknifed validation), which excludes one observation. This approach of evaluating DFA uses the data that has been left out, that is, excluding one observation. We are now left with n - 1 observation. This cross-validation technique is done automatically for each sample in the dataset. To do this, add <code class="literal">CV=TRUE</code> (think Cross-validation) to the <code class="literal">lda()</code> call as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; brine.log.lda &lt;- lda(GROUP ~ HCO3 + SO4 + Cl + Ca + Mg + Na, data=brine.log, CV=TRUE) </strong></span>
</pre><p>The success of the discrimination can be measured similarly, as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; tab &lt;- table(brine.log$GROUP, brine.log.lda$class)</strong></span>
</pre><p>Print the value of tab as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; tab</strong></span>
</pre><p>The result is as follows:</p><pre class="programlisting">
<span class="strong"><strong>      1   2   3</strong></span>
<span class="strong"><strong>  1  6   1   0</strong></span>
<span class="strong"><strong>  2   1   4   1</strong></span>
<span class="strong"><strong>  3   1   0   5</strong></span>
<span class="strong"><strong>&gt; sum(tab[row(tab) == col(tab)]) / sum(tab)</strong></span>
</pre><p>The result is as follows:</p><pre class="programlisting">
<span class="strong"><strong>[1] 0.7894737</strong></span>
</pre><p>In this dataset, the jackknifed validation is considerably less accurate (only 79% accurate), reflecting that the resubstitution error always overestimates the performance of a DFA. Such a discrepancy is particularly common with small datasets such as this, and discriminant function analysis is often much more successful with large datasets.</p></div></div></div>
<div class="section" title="Multinomial logistic regression - understanding program choices made by students"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec12"/>Multinomial logistic regression - understanding program choices made by students</h1></div></div></div><p>Let's assume that high school students are to be enrolled on a program. The students are given the opportunity to choose programs of their choice. The choices of the students are based on three options. These choices are general program, vocational program, and academic program. The choice of each student is based on each student's writing score and social economic status.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec3"/>Getting ready</h2></div></div></div><p>In order to complete this recipe we shall be using a student's dataset. The first step is collecting the data.</p><div class="section" title="Step 1 - collecting data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec6"/>Step 1 - collecting data</h3></div></div></div><p>The student's dataset titled <code class="literal">hsbdemo</code> is being utilized. The dataset is available at: <a class="ulink" href="http://voia.yolasite.com/resources/hsbdemo.csv">
http://voia.yolasite.com/resources/hsbdemo.csv
</a> in an MS Excel format. There are 201 data rows and 13 variables in the dataset. The eight numeric measurements are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">id</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">read</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">write</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">math</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">science</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">socst</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">awards</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">cid</code></li></ul></div><p>The non-numeric measurements are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">gender</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ses</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">schtyp</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">prog</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">honors</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec4"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec7"/>Step 2 - exploring data</h3></div></div></div><p>The first step is loading the packages. The <code class="literal">library ()</code> returns an error if the package does not exist. Use the following commands:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; library(foreign)</strong></span>
<span class="strong"><strong>    &gt; library (nnet)</strong></span>
<span class="strong"><strong>    &gt; library (ggplot2)</strong></span>
<span class="strong"><strong>    &gt; library (reshape2)</strong></span>
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note2"/>Note</h3><p> Version info: Code for this page was tested in R version 3.2.3 (2015-12-10).</p></div></div><p>Exploring the data throws some light on the relationships of the data. The CSV file titled <code class="literal">hsbdemo.csv</code> needs to be imported in the R environment. The imported data is saved in the data frame titled <code class="literal">ml</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ml &lt;- read.table("d:/hsbdemo.csv", header=TRUE, sep=",", row.names="id")</strong></span>
</pre><p>Exploring the descriptive statistics of the variables that are of interest is to be carried out using the <code class="literal">with()</code> function as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; with(ml, table(ses, prog))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>    prog</strong></span>
<span class="strong"><strong>  ses         academic         general    vocation</strong></span>
<span class="strong"><strong>    high           42                9           7</strong></span>
<span class="strong"><strong>    low            19               16          12</strong></span>
<span class="strong"><strong>    middle         44               20          31</strong></span>
</pre><p>Let us obtain the mean and the standard deviation as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; with(ml, do.call(rbind, tapply(write, prog, function(x) c(M = mean(x), SD = sd(x)))))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>                    M           SD</strong></span>
<span class="strong"><strong>academic     56.25714     7.943343</strong></span>
<span class="strong"><strong>general      51.33333     9.397775</strong></span>
<span class="strong"><strong>vocation     46.76000     9.318754</strong></span>
</pre><p>The mean is the highest for academic and the standard deviation is the highest for general.</p></div><div class="section" title="Step 3 - training the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec8"/>Step 3 - training the model</h3></div></div></div><p>In order to estimate multinomial logistic regression, the <code class="literal">multinom()</code> function is used. The <code class="literal">multinom()</code> function does not require the reshaping of the data.</p><p>It is important to choose a reference group for the outcome. We can choose the level of our outcome that we wish to use as our baseline. This is specified in the <code class="literal">relevel ()</code> function. Then, we run our model using the <code class="literal">multinom()</code> function. Since no p-value calculations are carried out for the regression coefficients, p-value tests are carried out using Wald tests (z-tests). The formula mentioned in the <code class="literal">multinom()</code> function is of the form response ~ predictors. The data frame <code class="literal">ml</code> is the data frame to interpret the variables occurring in the formula, as follows:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; ml$prog2 &lt;- relevel(ml$prog, ref = "academic") </strong></span>
<span class="strong"><strong>    &gt; test &lt;- multinom(prog2 ~ ses + write, data = ml)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong># weights:  15 (8 variable)</strong></span>
<span class="strong"><strong>initial  value          219.722458 </strong></span>
<span class="strong"><strong>iter     10 value     179.983731</strong></span>
<span class="strong"><strong>final    value         179.981726 </strong></span>
<span class="strong"><strong>converged</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>    &gt; summary(test)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>    Call:</strong></span>
<span class="strong"><strong>multinom(formula = prog2 ~ ses + write, data = ml)</strong></span>
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>            (Intercept)       seslow   sesmiddle         write</strong></span>
<span class="strong"><strong>general     1.689478       1.1628411   0.6295638   -0.05793086</strong></span>
<span class="strong"><strong>vocation    4.235574       0.9827182   1.2740985   -0.11360389</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>    Std. Errors:</strong></span>
<span class="strong"><strong>            (Intercept)       seslow   sesmiddle        write</strong></span>
<span class="strong"><strong>general     1.226939       0.5142211   0.4650289   0.02141101</strong></span>
<span class="strong"><strong>vocation    1.204690       0.5955688   0.5111119   0.02222000</strong></span>
<span class="strong"><strong>Residual Deviance: 359.9635 </strong></span>
<span class="strong"><strong>AIC: 375.9635 </strong></span>
</pre><p>Next, the test summary of coefficients is divided by the test summary of standard errors, as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; z &lt;- summary(test)$coefficients/summary(test)$standard.errors</strong></span>
</pre><p>Display the value of <code class="literal">z</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; z</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>           (Intercept)     seslow     sesmiddle       write</strong></span>
<span class="strong"><strong>general       1.376987   2.261364      1.353816   -2.705658</strong></span>
<span class="strong"><strong>vocation      3.515904   1.650050      2.492798   -5.112687</strong></span>
</pre></div><div class="section" title="Step 4 - testing the results of the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec9"/>Step 4 - testing the results of the model</h3></div></div></div><p>A two-tailed z test is carried out as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p &lt;- (1 - pnorm(abs(z), 0, 1))*2</strong></span>
</pre><p>Display the value of p as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>            (Intercept)       seslow     sesmiddle          write</strong></span>
<span class="strong"><strong>general    0.1685163893   0.02373673     0.1757949   6.816914e-03</strong></span>
<span class="strong"><strong>vocation   0.0004382601   0.09893276     0.0126741   3.176088e-07</strong></span>
</pre></div><div class="section" title="Step 5 - model improvement performance"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec10"/>Step 5 - model improvement performance</h3></div></div></div><p>Relative risk is defined as the ratio between choosing one outcome category and choosing the baseline category. The relative risk is the exponential of the right-hand side of the linear equation. The exponential regression coefficients are relative risk ratios for a unit change in the predictor variable.</p><p>Extract the coefficients from the model and perform an exponential on it as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; exp(coef(test))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>    (Intercept)   seslow         sesmiddle       write</strong></span>
<span class="strong"><strong>general         5.416653   3.199009    1.876792   0.9437152</strong></span>
<span class="strong"><strong>vocation       69.101326   2.671709    3.575477   0.8926115</strong></span>
</pre><p>The relative risk ratio for a one-unit increase in the variable write is <code class="literal">.9437</code> for being in a general program versus an academic program. The relative risk ratio switching from <code class="literal">ses = 1</code> to <code class="literal">3</code> is <code class="literal">.3126</code> for being in a general program versus an academic program. Use the probabilities that have been predicted to get an insight into the model. The <code class="literal">fitted()</code> function is used to calculate predicted probabilities for each of our outcome levels as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; head(pp &lt;- fitted(test))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>       academic     general    vocation</strong></span>
<span class="strong"><strong>45     .1482721   0.3382509   0.5134769</strong></span>
<span class="strong"><strong>108   0.1201988   0.1806335   0.6991678</strong></span>
<span class="strong"><strong>15    0.4186768   0.2368137   0.3445095</strong></span>
<span class="strong"><strong>67    0.1726839   0.3508433   0.4764728</strong></span>
<span class="strong"><strong>153   0.1001206   0.1689428   0.7309367</strong></span>
<span class="strong"><strong>51    0.3533583   0.2378047   0.4088370</strong></span>
</pre><p>Examine the changes in probability associated with one of the two variables, <code class="literal">ses</code> and <code class="literal">write</code>. Create small datasets varying one variable while holding the other one constant. First, hold the write variable at its mean and then examine the predicted probability for each level of the <code class="literal">ses</code> variable as follows:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; dses &lt;- data.frame(ses = c("low", "middle", "high"),write = mean(ml$write))</strong></span>
<span class="strong"><strong>    &gt; predict(test, newdata = dses, "probs")</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>     academic     general      vocation</strong></span>
<span class="strong"><strong>1   0.4396813   0.3581915   0.2021272</strong></span>
<span class="strong"><strong>2   0.4777451   0.2283359   0.2939190</strong></span>
<span class="strong"><strong>3   0.7009046   0.1784928   0.1206026</strong></span>
</pre><p>Looking at the average predicted probabilities for different values of the continuous predictor variable, using predicted probabilities as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; dwrite &lt;- data.frame(ses = rep(c("low", "middle", "high"), each = 41), write = rep(c(30:70), 3))</strong></span>
</pre><p>Store the predicted probabilities for each value of <code class="literal">ses</code> and write as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pp.write &lt;- cbind(dwrite, predict(test, newdata = dwrite, type = "probs", se = TRUE))</strong></span>
</pre><p>Calculate the mean probabilities within each level of <code class="literal">ses</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; by(pp.write[, 3:5], pp.write$ses, colMeans)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>pp.write$ses: high</strong></span>
<span class="strong"><strong>   academic     general      vocation </strong></span>
<span class="strong"><strong>  0.6164348   0.1808049   0.2027603 </strong></span>
<span class="strong"><strong>-------------------------------------------------------------------------- </strong></span>
<span class="strong"><strong>pp.write$ses: low</strong></span>
<span class="strong"><strong>   academic     general      vocation </strong></span>
<span class="strong"><strong>  0.3972955   0.3278180   0.2748864 </strong></span>
<span class="strong"><strong>-------------------------------------------------------------------------- </strong></span>
<span class="strong"><strong>pp.write$ses: middle          </strong></span>
<span class="strong"><strong>   academic     general      vocation </strong></span>
<span class="strong"><strong>  0.4256172   0.2010877   0.3732951 </strong></span>
</pre><p>Sometimes, a couple of plots can convey a good deal of information. Using the predictions we generated for the <code class="literal">pp.write</code> object previously, we can plot the predicted probabilities against the writing score by the level of <code class="literal">ses</code> for different levels of the outcome variable. The <code class="literal">melt()</code> function takes data in wide format and stacks a set of columns into a single column of data. The <code class="literal">lpp</code> data frame is used to specify the data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; lpp &lt;- melt(pp.write, id.vars = c("ses", "write"), value.name = "probability")</strong></span>
</pre><p>Print the values for <code class="literal">head</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; head(lpp)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>  ses   write   variable     probability</strong></span>
<span class="strong"><strong>1  low      30   academic    0.09843258</strong></span>
<span class="strong"><strong>2   low      31   academic    0.10716517</strong></span>
<span class="strong"><strong>3   low      32   academic    0.11650018</strong></span>
<span class="strong"><strong>4   low      33   academic    0.12645441</strong></span>
<span class="strong"><strong>5   low      34   academic    0.13704163</strong></span>
<span class="strong"><strong>6   low      35   academic    0.14827211</strong></span>
</pre><p>Next we plot predicted probabilities across write values for each level of <code class="literal">ses</code> facetted by program type as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ggplot(lpp, aes(x = write, y = probability, colour = ses)) +</strong></span>
<span class="strong"><strong>+     geom_line() +</strong></span>
<span class="strong"><strong>+     facet_grid(variable ~ ., scales="free")</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_02_005.jpg" alt="Step 5 - model improvement performance"/></div><p>
</p></div></div></div>
<div class="section" title="Tobit regression - measuring the students' academic aptitude"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Tobit regression - measuring the students' academic aptitude</h1></div></div></div><p>Let us measure the academic aptitude of a student on a scale of 200-800. This measurement is based on the model using reading and math scores. The nature of the program in which the student has been enrolled is also to be taken into consideration. There are three types of programs: academic, general, and vocational. The problem is that some students may answer all the questions on the academic aptitude test correctly and score 800 even though it is likely that these students are not truly equal in aptitude. This may be true for all the students who may answer all the questions incorrectly and score 200.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec5"/>Getting ready</h2></div></div></div><p>In order to complete this recipe we shall be using a student's dataset. The first step is collecting the data.</p><div class="section" title="Step 1 - collecting data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec11"/>Step 1 - collecting data</h3></div></div></div><p>To develop the Tobit regression model we shall use the student dataset titled tobit, which is available at <a class="ulink" href="http://www.ats.ucla.edu/stat/data/tobit.csv">
http://www.ats.ucla.edu/stat/data/tobit.csv
</a> in an MS Excel format. There are 201 data rows and five variables in the dataset. The four numeric measurements are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">id</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">read</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">math</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">apt</code></li></ul></div><p>The non-numeric measurement is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">prog</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec6"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec12"/>Step 2 - exploring data</h3></div></div></div><p>The first step is to load the following packages. The <code class="literal">require()</code> function is designed for use inside other functions; it returns <code class="literal">FALSE</code> and gives a warning (rather than an error as <code class="literal">library ()</code> does by default) if the package does not exist. Use the following commands:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; require(ggplot2)</strong></span>
<span class="strong"><strong>    &gt; require(GGally)</strong></span>
<span class="strong"><strong>    &gt; require(VGAM)</strong></span>
</pre><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note3"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>Explore the data and understand the relationships among the variables. Begin by importing the CSV data file named <code class="literal">gala.txt</code>. This will save the data to the <code class="literal">dat</code> data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; dat &lt;- read.table("d:/tobit.csv", header=TRUE, sep=",", row.names="id")</strong></span>
</pre><p>In this dataset, the lowest value of <code class="literal">apt</code> is 352. This indicates that no student has received the lowest score of 200. Even though censoring from below was possible, it is not required in this dataset. Use the following command:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(dat)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Id         read         math      prog           apt    </strong></span>
<span class="strong"><strong>Min.   :  1.0   Min.   :28.0   Min.   :33.0   academic  : 45 Min.   :352</strong></span>
<span class="strong"><strong>1st Qu.: 50.8   1st Qu.:44.0   1st Qu.:45.0   general   :105 1st Qu.:576</strong></span>
<span class="strong"><strong>Median :100.5   Median :50.0   Median :52.0   vocational: 50 Median :633</strong></span>
<span class="strong"><strong>Mean   :100.5   Mean   :52.2   Mean   :52.6      Mean   :640</strong></span>
<span class="strong"><strong>3rd Qu.:150.2   3rd Qu.:60.0   3rd Qu.:59.0      3rd Qu.:705</strong></span>
<span class="strong"><strong>Max.   :200.0   Max.   :76.0   Max.   :75.0       Max.   :800</strong></span>
</pre></div><div class="section" title="Step 3 - plotting data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec13"/>Step 3 - plotting data</h3></div></div></div><p>Write is a function that gives the density of a normal distribution for a given mean and standard deviation, which has been scaled on the count metric. In order to generate the histogram formulate count as <span class="emphasis"><em>density * sample size * bin</em></span> width use the following code:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; f &lt;- function(x, var, bw = 15) {</strong></span>
<span class="strong"><strong>    dnorm(x, mean = mean(var), sd(var)) * length(var) * bw</strong></span>
<span class="strong"><strong>    }</strong></span>
</pre><p>Now we shall set up the base plot as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p &lt;- ggplot(dat, aes(x = apt, fill=prog))</strong></span>
</pre><p>Now we shall prepare a histogram, colored by proportion in different programs with a normal distribution overlaid as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p + stat_bin(binwidth=15) +</strong></span>
<span class="strong"><strong>    stat_function(fun = f, size = 1,</strong></span>
<span class="strong"><strong>    args = list(var = dat$apt))</strong></span>
</pre><p>The histogram plotted is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_006.jpg" alt="Step 3 - plotting data"/></div><p>
</p><p>Looking at the preceding histogram, we can see the censoring in the values of <code class="literal">apt</code>, that is, there are far more cases with scores between 750 to 800 than one would expect compared to the rest of the distribution.</p><p>In the following alternative histogram, the excess of cases where <code class="literal">apt</code>=800 have been highlighted. In the following histogram, the breaks option produces a histogram where each unique value of <code class="literal">apt</code> has its own bar (by setting breaks equal to a vector containing values from the minimum of <code class="literal">apt</code> to the maximum of apt). Because <code class="literal">apt</code> is continuous, most values of <code class="literal">apt</code> are unique in the dataset, although close to the center of the distribution there are a few values of apt that have two or three cases.</p><p>The spike on the far right of the histogram is the bar for cases where <code class="literal">apt</code>=800, the height of this bar, relative to all the others, clearly shows the excess number of cases with this value. Use the following command:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p + stat_bin(binwidth = 1) + stat_function(fun = f, size = 1, args = list(var = dat$apt, </strong></span>
<span class="strong"><strong>    bw = 1))</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_02_007.jpg" alt="Step 3 - plotting data"/></div><p>
</p></div><div class="section" title="Step 4 - exploring relationships"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec14"/>Step 4 - exploring relationships</h3></div></div></div><p>The following command enables use to explore the bivariate relationships in the dataset:</p><pre class="programlisting">
<span class="strong"><strong>&gt; cor(dat[, c("read", "math", "apt")])</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>              read        math             apt</strong></span>
<span class="strong"><strong>read     1.0000000   0.6622801   0.6451215</strong></span>
<span class="strong"><strong>math     0.6622801   1.0000000   0.7332702</strong></span>
<span class="strong"><strong>apt      0.6451215   0.7332702   1.0000000</strong></span>
</pre><p>Now plot the matrix as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ggpairs(dat[, c("read", "math", "apt")])</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_02_008.jpg" alt="Step 4 - exploring relationships"/></div><p>
</p><p>In the first row of the scatterplot matrix, the scatterplots display a relationship between read and apt. The relationship between math and apt is also established.</p></div><div class="section" title="Step 5 - training the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec15"/>Step 5 - training the model</h3></div></div></div><p>Run the Tobit model, using the <code class="literal">vglm</code> function of the VGAM package using this command:</p><pre class="programlisting">    &gt; <span class="strong"><strong>summary(m &lt;- vglm(apt ~ read + math + prog, tobit(Upper = 800), data = dat))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>vglm(formula = apt ~ read + math + prog, family = tobit(Upper = 800), </strong></span>
<span class="strong"><strong>    data = dat)</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>Pearson residuals:</strong></span>
<span class="strong"><strong>                 Min        1Q           Median        3Q       Max</strong></span>
<span class="strong"><strong>mu           -2.5684    -0.7311        -0.03976    0.7531     2.802</strong></span>
<span class="strong"><strong>loge(sd)     -0.9689    -0.6359        -0.33365    0.2364     4.845</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>               Estimate Std.       Error     z value     Pr(&gt;|z|)   </strong></span>
<span class="strong"><strong>(Intercept):1     209.55956     32.54590     6.439     1.20e-10 ***</strong></span>
<span class="strong"><strong>(Intercept):2       4.18476      0.05235    79.944      &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>read                2.69796      0.61928     4.357     1.32e-05 ***</strong></span>
<span class="strong"><strong>math                5.91460      0.70539     8.385      &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>proggeneral       -12.71458     12.40857    -1.025     0.305523    </strong></span>
<span class="strong"><strong>progvocational   -46.14327     13.70667    -3.366     0.000761 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:    0 '***'   0.001 '**'   0.01 '*'   0.05 '.' 0.1 ' ' 1</strong></span>
<span class="strong"><strong>Number of linear predictors:  2 </strong></span>
<span class="strong"><strong>Names of linear predictors: mu, loge(sd)</strong></span>
<span class="strong"><strong>Dispersion Parameter for tobit family:   1</strong></span>
<span class="strong"><strong>Log-likelihood: -1041.063 on 394 degrees of freedom</strong></span>
<span class="strong"><strong>Number of iterations: 5 </strong></span>
</pre><p>The preceding output informs us about the options specified.</p><p>The table labeled coefficients gives the coefficients their standard errors and the z-statistic. No p-values are included in the summary table.</p><p>The interpretation of the Tobit regression coefficients is similar to that of OLS regression coefficients. The linear coefficients effect is on the uncensored latent variable:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For a one-unit increase in read, there is a <code class="literal">2.6981</code> point increase in the predicted value of <code class="literal">apt</code>.</li><li class="listitem" style="list-style-type: disc">A one-unit increase in <code class="literal">math</code> is associated with a <code class="literal">5.9146</code> unit increase in the predicted value of <code class="literal">apt</code>.</li><li class="listitem" style="list-style-type: disc">The terms for <code class="literal">prog</code> have a slightly different interpretation. The predicted value of apt is <code class="literal">-46.1419</code> points lower for students in a vocational program than for students in an academic program.</li><li class="listitem" style="list-style-type: disc">The coefficient labeled <code class="literal">(Intercept):1</code> is the intercept or constant for the model.</li><li class="listitem" style="list-style-type: disc">The coefficient labeled <code class="literal">(Intercept):2</code> is an ancillary statistic. Exponential of this value, is analogous to the square root of the residual variance in OLS regression. The value of <code class="literal">65.6773</code> can be compared to the standard deviation of academic aptitude, which was <code class="literal">99.21</code>, a substantial reduction.</li></ul></div><p>The final log likelihood, <code class="literal">-1041.0629</code>, is shown toward the bottom of the output; it can be used in comparisons of nested models.</p></div><div class="section" title="Step 6 - testing the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec16"/>Step 6 - testing the model</h3></div></div></div><p>Calculate the p - values for each of the coefficients in the model. Calculate the p - value for each of the coefficients using z - values and then display them in a tabular manner. The coefficients for <code class="literal">read</code>, <code class="literal">math</code>, and <code class="literal">prog</code> = 3 (vocational) are statistically significant as follows:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; ctable &lt;- coef(summary(m))</strong></span>
<span class="strong"><strong>    &gt; pvals &lt;- 2 * pt(abs(ctable[, "z value"]), df.residual(m), lower.tail = FALSE) </strong></span>
<span class="strong"><strong>    &gt; cbind(ctable, pvals)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>    Estimate    Std. Error      z value       Pr(&gt;|z|)       pvals</strong></span>
<span class="strong"><strong>(Intercept):1    209.559557   32.54589921    6.438893   1.203481e-10  3.505839e-10</strong></span>
<span class="strong"><strong>(Intercept):2      4.184759    0.05234618   79.943922   0.000000e+00 1.299833e-245</strong></span>
<span class="strong"><strong>read               2.697959    0.61927743    4.356625   1.320835e-05  1.686815e-05</strong></span>
<span class="strong"><strong>math               5.914596    0.70538721    8.384892   5.077232e-17  9.122434e-16</strong></span>
<span class="strong"><strong>proggeneral      -12.714581   12.40856959   -1.024661   3.055230e-01  3.061517e-01</strong></span>
<span class="strong"><strong>progvocational   -46.143271   13.70667208   -3.366482   7.613343e-04  8.361912e-04</strong></span>
</pre><p>We can test the significance of the program type overall by fitting a model without a program in it and using a likelihood ratio test as follows:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; m2 &lt;- vglm(apt ~ read + math, tobit(Upper = 800), data = dat) </strong></span>
<span class="strong"><strong>    &gt; (p &lt;- pchisq(2 * (logLik(m) - logLik(m2)), df = 2, lower.tail = FALSE))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong> [1] 0.003155176</strong></span>
</pre><p>The statistical significance of the prog variable is indicated by the p - value equal to <code class="literal">0.0032</code>. We calculate the upper and lower 95% confidence intervals for the coefficients as follows:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; b &lt;- coef(m)</strong></span>
<span class="strong"><strong>    &gt; se &lt;- sqrt(diag(vcov(m)))</strong></span>
<span class="strong"><strong>    &gt; cbind(LL = b - qnorm(0.975) * se, UL = b + qnorm(0.975) * se)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>      LL             UL</strong></span>
<span class="strong"><strong>(Intercept):1      145.770767   273.348348</strong></span>
<span class="strong"><strong>(Intercept):2        4.082163     4.287356</strong></span>
<span class="strong"><strong>read                 1.484198     3.911721</strong></span>
<span class="strong"><strong>math                 4.532062     7.297129</strong></span>
<span class="strong"><strong>proggeneral        -37.034931    11.605768</strong></span>
<span class="strong"><strong>progvocational     -73.007854   -19.278687</strong></span>
</pre><p>By plotting residuals to one, we can assess the absolute as well as relative (Pearson) values and assumptions such as normality and homogeneity of variance. This shall help in examining the model and the data fit.</p><p>We may also wish to examine how well our model fits the data. One way to start is with plots of the residuals to assess their absolute as well as relative (Pearson) values and assumptions such as normality and homogeneity of variance. Use the following commands:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; dat$yhat &lt;- fitted(m)[,1]</strong></span>
<span class="strong"><strong>    &gt; dat$rr &lt;- resid(m, type = "response")</strong></span>
<span class="strong"><strong>    &gt; dat$rp &lt;- resid(m, type = "pearson")[,1]</strong></span>
<span class="strong"><strong>    &gt; par(mfcol = c(2, 3))</strong></span>
<span class="strong"><strong>    &gt; with(dat, {</strong></span>
<span class="strong"><strong>      plot(yhat, rr, main = "Fitted vs Residuals")</strong></span>
<span class="strong"><strong>      qqnorm(rr)</strong></span>
<span class="strong"><strong>      plot(yhat, rp, main = "Fitted vs Pearson Residuals")</strong></span>
<span class="strong"><strong>      qqnorm(rp)</strong></span>
<span class="strong"><strong>      plot(apt, rp, main = "Actual vs Pearson Residuals")</strong></span>
<span class="strong"><strong>      plot(apt, yhat, main = "Actual vs Fitted")</strong></span>
<span class="strong"><strong>    })</strong></span>
</pre><p>The plot is as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_009.jpg" alt="Step 6 - testing the model"/></div><p>
</p><p>Establish the correlation as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; (r &lt;- with(dat, cor(yhat, apt)))</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>[1] 0.7825</strong></span>
</pre><p>Variance accounted for is calculated as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; r^2</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>[1] 0.6123</strong></span>
</pre><p>The correlation between the predicted and observed values of <code class="literal">apt</code> is <code class="literal">0.7825</code>. If we square this value, we get the multiple squared correlation, this indicates that the predicted values share 61.23% of their variance with <code class="literal">apt</code>.</p></div></div></div>
<div class="section" title="Poisson regression - understanding species present in Galapagos Islands"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Poisson regression - understanding species present in Galapagos Islands</h1></div></div></div><p>The Galapagos Islands are situated in the Pacific Ocean about 1000 km from the Ecuadorian coast. The archipelago consists of 13 islands, five of which are inhabited. The islands are rich in flora and fauna. Scientists are still perplexed by the fact that such a diverse set of species can flourish in such a small and remote group of islands.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec7"/>Getting ready</h2></div></div></div><p>In order to complete this recipe we shall be using species dataset. The first step is collecting the data.</p><div class="section" title="Step 1 - collecting and describing the data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec17"/>Step 1 - collecting and describing the data</h3></div></div></div><p>We will utilize the number of species dataset titled <code class="literal">gala</code> that is available at <a class="ulink" href="https://github.com/burakbayramli/kod/blob/master/books/Practical_Regression_Anove_Using_R_Faraway/gala.txt">
https://github.com/burakbayramli/kod/blob/master/books/Practical_Regression_Anove_Using_R_Faraway/gala.txt
</a>. </p><p>The dataset includes 30 cases and seven variables in the dataset. The seven numeric measurements include the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Species</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Endemics</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Area</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Elevation</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Nearest</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Scruz</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Adjcacent</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec8"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring the data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec18"/>Step 2 - exploring the data</h3></div></div></div><p>Exploring the data will throw some light on the relationships. Begin by importing the txt data file named <code class="literal">gala.txt</code>. We will be saving the data to the gala data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; gala &lt;- read.table("d:/gala.txt")</strong></span>
</pre><p>The <code class="literal">regpois()</code> gives the Poisson regression on the variables that are expected to be important from an ecological point of view as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; regpois &lt;- glm( Species ~ Area + Elevation + Nearest, family=poisson, data=gala)</strong></span>
</pre><p>Next provide the summary of the data as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(regpois)</strong></span>
</pre><p>The <code class="literal">summary</code> function will provide deviance residuals, coefficients, <code class="literal">signif</code> codes, null deviance, residual deviance, AIC, and number of Fisher scoring iterations.The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Deviance residuals:</strong></span>
<span class="strong"><strong>     Min          1Q      Median          3Q         Max</strong></span>
<span class="strong"><strong>-17.1900     -6.1715     -2.7125      0.7063     21.4237</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                Estimate      Std. Error     z value      Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)    3.548e+00       3.933e-02      90.211       &lt; 2e-16 *** </strong></span>
<span class="strong"><strong>Area          -5.529e-05        1.890e-05      -2.925       0.00344 ** </strong></span>
<span class="strong"><strong>Elevation      1.588e-03        5.040e-05      31.502        &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>Nearest        5.921e-03       1.466e-03      4.039          5.38e-05 ***</strong></span>
<span class="strong"><strong>---</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>Signif. codes:</strong></span>
<span class="strong"><strong>  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre><p>(Dispersion parameter for Poisson family taken to be 1)</p><pre class="programlisting">
<span class="strong"><strong>Null deviance:</strong></span>
<span class="strong"><strong>3510.7  on 29  degrees of freedom</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>Residual deviance:</strong></span>
<span class="strong"><strong>1797.8  on 26  degrees of freedom</strong></span>
</pre><pre class="programlisting">
<span class="strong"><strong>AIC:</strong></span>
<span class="strong"><strong>1966.7</strong></span>
</pre><p>Number of Fisher Scoring iterations:</p><pre class="programlisting">
<span class="strong"><strong>5</strong></span>
<span class="strong"><strong>&gt; plot(regpois$fit,gala$Species)</strong></span>
</pre><p>The plot is shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_010.jpg" alt="Step 2 - exploring the data"/></div><p>
</p></div><div class="section" title="Step 3 - plotting data and testing empirical data"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec19"/>Step 3 - plotting data and testing empirical data</h3></div></div></div><p>
<code class="literal">ppois()</code> is the distribution function of a Poisson where the parameter is <code class="literal">lambda=regpois$fit</code> and it is computed in <code class="literal">gala$Species</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p &lt;- ppois(gala$Species,regpois$fit)</strong></span>
</pre><p>The values should be close to uniform in nature. Check the uniformity by plotting the values as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; hist(p,breaks=10)</strong></span>
</pre><p>The plot result is shown in the screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_011.jpg" alt="Step 3 - plotting data and testing empirical data"/></div><p>
</p><p>The plot clearly shows that they are not in uniform.</p><p>Now carry out the Kolmogorov-Smirnov test on whether empirical data fits a given distribution.</p><p>The Kolmogorov-Smirnov test is a test for goodness of fit and it usually involves examining a random sample from some unknown distribution in order to test the null hypothesis that the unknown distribution function is in fact a known, specified function. We usually use the Kolmogorov-Smirnov test to check the normality assumption in the analysis of variance.</p><p>The Kolmogorov-Smirnov test is constructed as a statistical hypothesis test. We determine a null hypothesis, <span class="inlinemediaobject"><img src="graphics/image_02_012.jpg" alt="Step 3 - plotting data and testing empirical data"/></span>, that the two samples we are testing come from the same distribution. Then we search for evidence that this hypothesis should be rejected and express this in terms of a probability. If the likelihood of the samples being from different distributions exceeds a confidence level we demand that the original hypothesis is rejected in favor of the hypothesis, <span class="inlinemediaobject"><img src="graphics/image_02_013.jpg" alt="Step 3 - plotting data and testing empirical data"/></span>, that the two samples are from different distributions.</p><p>To do this we devise a single number calculated from the samples, that is, a statistic. The trick is to find a statistic that has a range of values that do not depend on things we do not know, such as the actual underlying distributions in this case.</p><p>The test statistic in the Kolmogorov-Smirnov test is very easy; it is just the maximum vertical distance between the empirical cumulative distribution functions of the two samples. The empirical cumulative distribution of a sample is the proportion of the sample values that are less than or equal to a given value.</p><p>One sample Kolmogorov-Smirnov test is as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ks.test(p,"punif")</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>One-sample Kolmogorov-Smirnov test</strong></span>
<span class="strong"><strong>data:  p</strong></span>
<span class="strong"><strong>D = 0.57731, p-value = 4.134e-09</strong></span>
<span class="strong"><strong>alternative hypothesis: two-sided  </strong></span>
</pre><p>Therefore, we can safely conclude that the model is not adequate.</p></div><div class="section" title="Step 4 - rectifying discretization of the Poisson model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec20"/>Step 4 - rectifying discretization of the Poisson model</h3></div></div></div><p>Now make a correction since Poisson  is discrete. The change is as follows:</p><pre class="programlisting">
<span class="strong"><strong>  p = 1/2*(F(Y)+F(Y-1)) </strong></span>
<span class="strong"><strong>  ; where Y are the data, </strong></span>
<span class="strong"><strong>  ; and F are the distribution functions coming from Poisson</strong></span>
</pre><p>A correction of the procedure is carried out, taking into account discrete distribution as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p &lt;- 0.5*(ppois(gala$Species,regpois$fit) + ppois(gala$Species-1,regpois$fit))</strong></span>
</pre><p>Let us check the uniformity by plotting the values as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; hist(p,breaks=10)</strong></span>
</pre><p>The plot result is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_014.jpg" alt="Step 4 - rectifying discretization of the Poisson model"/></div><p>
</p><p>The correction does not make much of a difference. The plot clearly shows that they are not in uniform.</p><p>Now let us carry out the Kolmogorov-Smirnov test again to verify whether empirical data fits a given distribution as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ks.test(p,"punif")</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>    One-sample Kolmogorov-Smirnov test</strong></span>
<span class="strong"><strong>data:  p</strong></span>
<span class="strong"><strong>D = 0.58571, p-value = 2.3e-09</strong></span>
<span class="strong"><strong>alternative hypothesis: two-sided</strong></span>
</pre></div><div class="section" title="Step 5 - training and evaluating the model using the link function"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec21"/>Step 5 - training and evaluating the model using the link function</h3></div></div></div><p>We shall see how generalized linear models fit using the <code class="literal">glm( )</code> function as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; regpois2 &lt;- glm( Species ~ Area + Elevation + Nearest, family=poisson(link=sqrt), data=gala)</strong></span>
</pre><p>Let us print the results of <code class="literal">regpois2</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(regpois2)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>glm(formula = Species ~ Area + Elevation + Nearest, family = poisson(link = sqrt), </strong></span>
<span class="strong"><strong>    data = gala)</strong></span>
<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min         1Q       Median         3Q          Max  </strong></span>
<span class="strong"><strong>-19.108     -5.129     -1.335      1.846       16.918  </strong></span>
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                Estimate   Std. Error   z value     Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)    4.1764222    0.1446592    28.871    &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>Area          -0.0004844    0.0001655    -2.926      0.00343 ** </strong></span>
<span class="strong"><strong>Elevation      0.0110143    0.0003372    32.664    &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>Nearest        0.0083908    0.0065858     1.274      0.20264    </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:    0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
<span class="strong"><strong>(Dispersion parameter for poisson family taken to be 1)</strong></span>
<span class="strong"><strong>Null deviance:   3510.7 on 29 degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance:   1377.5 on 26 degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 1546.3</strong></span>
<span class="strong"><strong>Number of Fisher Scoring iterations: 5</strong></span>
</pre></div><div class="section" title="Step 6 - revaluating using the Poisson model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec22"/>Step 6 - revaluating using the Poisson model</h3></div></div></div><p>A correction of the procedure is carried out, taking into account discrete distribution as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; p2 &lt;- 0.5*(ppois(gala$Species,regpois2$fit) + ppois(gala$Species-1,regpois2$fit)) </strong></span>
</pre><p>Check the uniformity by plotting the values as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; hist(p,breaks=10)</strong></span>
</pre><p>The plot result is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_015.jpg" alt="Step 6 - revaluating using the Poisson model"/></div><p>
</p><p>Carry out the Kolmogorov-Smirnov test again to verify whether empirical data fits a given distribution as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; ks.test(p2,"punif")</strong></span>
</pre><p>A one sample Kolmogorov-Smirnov test is carried out as follows:</p><pre class="programlisting">
<span class="strong"><strong>data:  p2</strong></span>
<span class="strong"><strong>D = 0.47262, p-value = 3.023e-06</strong></span>
<span class="strong"><strong>alternative hypothesis: two-sided</strong></span>
</pre><p>The result still does not pass the test.</p></div><div class="section" title="Step 7 - revaluating using the linear model"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec23"/>Step 7 - revaluating using the linear model</h3></div></div></div><p>Applying usual the linear model: <code class="literal">lm()</code> function is used to fit linear models. It can be used to carry out regression, single stratum analysis of variance, and analysis of covariance (although <code class="literal">
<a class="ulink" href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/aov.html">aov</a>
</code> may provide a more convenient interface for these). The <code class="literal">reg</code> data frame is used to store the results returned from the <code class="literal">lm()</code> function as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; reg &lt;- lm(Species ~ Area+Elevation+Nearest, data=gala)</strong></span>
</pre><p>Let us now view the results of the <code class="literal">reg</code> data frame using the following command:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(reg)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>lm(formula = Species ~ Area + Elevation + Nearest, data = gala)</strong></span>
<span class="strong"><strong>Residuals:</strong></span>
<span class="strong"><strong>       Min         1Q       Median         3Q          Max </strong></span>
<span class="strong"><strong>-191.856    -33.111    -18.626      5.673      262.209 </strong></span>
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>              Estimate   Std. Error   t value     Pr(&gt;|t|)   </strong></span>
<span class="strong"><strong>(Intercept)   16.46471     23.38884     0.704      0.48772   </strong></span>
<span class="strong"><strong>Area           0.01908      0.02676     0.713      0.48216   </strong></span>
<span class="strong"><strong>Elevation      0.17134      0.05452     3.143      0.00415 **</strong></span>
<span class="strong"><strong>Nearest        0.07123      1.06481     0.067      0.94718   </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:    0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
<span class="strong"><strong>Residual standard error:   80.84 on 26 degrees of freedom</strong></span>
<span class="strong"><strong>Multiple R-squared:      0.5541,  Adjusted R-squared:  0.5027 </strong></span>
<span class="strong"><strong>F-statistic:       10.77 on 3 and 26 DF,  p-value: 8.817e-05</strong></span>
</pre><p>Now let us plot the reg data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(reg)</strong></span>
</pre><p>The <span class="strong"><strong>Residuals vs Fitted</strong></span> plot is shown in the folllowing figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_016.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The Normal Q-Q linear model plot is shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_017.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The <span class="strong"><strong>Scale-Location</strong></span> linear model plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_018.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>Now let us apply a transformation by using the following square root function. The <code class="literal">reg2</code> data frame is used to store the results returned from the <code class="literal">lm</code> function:</p><pre class="programlisting">
<span class="strong"><strong>&gt; reg2 &lt;- lm(sqrt(Species) ~ Area+Elevation+Nearest, data=gala)</strong></span>
</pre><p>Let us now view the results of the <code class="literal">reg</code> data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(reg2)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>lm(formula = sqrt(Species) ~ Area + Elevation + Nearest, data = gala)</strong></span>
<span class="strong"><strong>Residuals:</strong></span>
<span class="strong"><strong>      Min          1Q      Median        3Q         Max </strong></span>
<span class="strong"><strong>-8.8057   -2.1775   -0.2086    1.3943    8.8730 </strong></span>
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                Estimate   Std. Error   t value     Pr(&gt;|t|)    </strong></span>
<span class="strong"><strong>(Intercept)    3.744e+00    1.072e+00     3.492     0.001729 ** </strong></span>
<span class="strong"><strong>Area          -2.253e-05    1.227e-03    -0.018     0.985485    </strong></span>
<span class="strong"><strong>Elevation      9.795e-03    2.499e-03     3.920 0.  000576 ***</strong></span>
<span class="strong"><strong>Nearest        2.002e-02    4.880e-02     0.410     0.685062    </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
<span class="strong"><strong>Residual standard error:   3.705 on 26 degrees of freedom</strong></span>
<span class="strong"><strong>Multiple R-squared:    0.5799,  Adjusted R-squared:  0.5315 </strong></span>
<span class="strong"><strong>F-statistic:     11.96 on 3 and 26 DF,  p-value: 4.144e-05</strong></span>
</pre><p>Now let us plot the <code class="literal">reg2</code> data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(reg2)</strong></span>
</pre><p>The <span class="strong"><strong>Residual vs Fitted</strong></span> plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_019.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The <span class="strong"><strong>Normal Q-Q</strong></span> linear model plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_020.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The Poisson regression  <span class="strong"><strong>Scale-Location</strong></span> linear model plot is shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_021.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The <span class="strong"><strong>Scale-Location</strong></span> linear model plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_022.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>Let us carry out the Shapiro  test. Given a sample X1, . . . , Xn of n real-valued observations, the Shapiro-Wilk test (Shapiro and Wilk, 1965) is a test of the composite hypothesis that the data is <span class="strong"><strong>i.i.d</strong></span>. (<span class="strong"><strong>independent and identically distributed</strong></span>) and normal, that is, N(µ, σ2) for some unknown real µ and some σ &gt; 0. Use the following command:</p><pre class="programlisting">
<span class="strong"><strong>&gt; shapiro.test(reg2$res)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Shapiro-Wilk normality test</strong></span>
<span class="strong"><strong>data:  reg2$res</strong></span>
<span class="strong"><strong>W = 0.9633, p-value = 0.375</strong></span>
</pre><p>Now let us apply a transformation by using the log function as follows.</p><p>The <code class="literal">reg3</code> data frame is used to store the results returned from the <code class="literal">lm()</code> function as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; reg3 &lt;- lm(log(Species) ~ Area+Elevation+Nearest, data=gala)</strong></span>
</pre><p>Let us now view the results of the <code class="literal">reg3</code> data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(reg3)</strong></span>
</pre><p>The results are as follows:</p><pre class="programlisting">
<span class="strong"><strong>Call:</strong></span>
<span class="strong"><strong>lm(formula = log(Species) ~ Area + Elevation + Nearest, data = gala)</strong></span>
<span class="strong"><strong>Residuals:  </strong></span>
<span class="strong"><strong>      Min        1Q      Median        3Q         Max </strong></span>
<span class="strong"><strong>-2.0739   -0.5161    0.3307    0.7472    1.6271 </strong></span>
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                Estimate   Std. Error   t value     Pr(&gt;|t|)    </strong></span>
<span class="strong"><strong>(Intercept)    2.3724325    0.3448586     6.879     2.65e-07 ***</strong></span>
<span class="strong"><strong>Area          -0.0002687    0.0003946    -0.681    0.50197    </strong></span>
<span class="strong"><strong>Elevation      0.0029096    0.0008039     3.620      0.00125 ** </strong></span>
<span class="strong"><strong>Nearest        0.0133869    0.0157001     0.853      0.40163    </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
<span class="strong"><strong>Residual standard error:   1.192 on 26 degrees of freedom</strong></span>
<span class="strong"><strong>Multiple R-squared:      0.4789,  Adjusted R-squared:  0.4187 </strong></span>
<span class="strong"><strong>F-statistic:       7.964 on 3 and 26 DF,  p-value: 0.0006281</strong></span>
</pre><p>Now let us plot the <code class="literal">reg3</code> data frame as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(reg3)</strong></span>
</pre><p>The <span class="strong"><strong>Residuals vs Fitted</strong></span> plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_023.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The Normal Q-Q linear model plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_024.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>The <span class="strong"><strong>Scale-Location</strong></span> linear model plot is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_025.jpg" alt="Step 7 - revaluating using the linear model"/></div><p>
</p><p>Let us carry out a Shapiro test as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; shapiro.test(reg3$res)</strong></span>
</pre><p>The result is:</p><pre class="programlisting">
<span class="strong"><strong>Shapiro-Wilk normality test</strong></span>
<span class="strong"><strong>data:  reg3$res</strong></span>
<span class="strong"><strong>W = 0.91925, p-value = 0.02565</strong></span>
</pre></div></div></div></body></html>