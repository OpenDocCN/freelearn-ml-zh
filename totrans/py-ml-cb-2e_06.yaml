- en: Building Recommendation Engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Building function compositions for data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the nearest neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing a k-nearest neighbors classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing a k-nearest neighbors regressor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the Euclidean distance score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the Pearson correlation score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding similar users in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating movie recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing ranking algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a filtering model using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you will need the following files (which
    are available on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`function_composition.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`knn.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nn_classification.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nn_regression.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`euclidean_score.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pearson_score.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find_similar_users.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movie_recommendations.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LambdaMARTModel.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vali.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorFilter.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the recommendation engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recommendation engine is a model that can predict what a user may be interested
    in. When we apply this to the context of movies, for example, this becomes a movie
    recommendation engine. We filter items in our database by predicting how the current
    user might rate them. This helps us in connecting the user to the right content
    in our dataset. Why is this relevant? If you have a massive catalog, then the
    user may or may not find all the content that is relevant to them. By recommending
    the right content, you increase consumption. Companies such as Netflix heavily
    rely on recommendations to keep the user engaged.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation engines usually produce a set of recommendations using either
    collaborative filtering or content-based filtering. The difference between the
    two approaches is in the way that the recommendations are mined. Collaborative
    filtering builds a model from the past behavior of the current user, as well as
    ratings given by other users. We then use this model to predict what this user
    might be interested in. Content-based filtering, on the other hand, uses the characteristics
    of the item itself in order to recommend more items to the user. The similarity
    between items is the main driving force here. In this chapter, we will focus on
    collaborative filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Building function compositions for data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the major parts of any machine learning system is the data processing
    pipeline. Before data is fed into the machine learning algorithm for training,
    we need to process it in different ways to make it suitable for that algorithm.
    Having a robust data processing pipeline goes a long way in building an accurate
    and scalable machine learning system. There are a lot of basic functionalities
    available, and data processing pipelines usually consist of a combination of these.
    Instead of calling these functions in a nested or loopy way, it's better to use
    the functional programming paradigm to build the combination.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at how to combine these basic functions to form a reusable
    function composition. In this recipe, we will create three basic functions and
    look at how to compose a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to build function compositions for data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and add the following line (the full code is in the `function_composition.py`
    file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function to add `3` to each element of the array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define a second function to multiply `2` with each element of the
    array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s now define a third function to subtract `5` from each element of
    the array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function composer that takes functions as input arguments and
    returns a composed function. This composed function is basically a function that
    applies all the input functions in a sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use the `reduce` function to combine all the input functions by successively
    applying the functions in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to play with this function composer. Let''s define some data
    and a sequence of operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use the regular method, we apply this successively, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use the function composer to achieve the same thing in a single
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do the same thing in a single line with the previous method as well,
    but the notation becomes very nested and unreadable. Also, it is not reusable;
    you will have to write the whole thing again if you want to reuse this sequence
    of operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we have created three basic functions and have learned how to
    compose a pipeline. To do this, the `reduce()` function was used. This function
    accepts a function and a sequence and returns a single value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `reduce ()` function calculates the return value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To start, the function calculates the result by using the first two elements
    of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, the function uses the result obtained in the previous step and the next
    value in the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process is repeated until the end of the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three basic functions used at the beginning of the recipe make use of the
    `map()` function. This function is used to apply a function on all the elements
    of a specific value. As a result, a map object is returned; this object is an
    iterator, so we can iterate over its elements. To print this object, we have converted
    the map object to sequence objects as a list.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to Python's official documentation of the `map()` function: [https://docs.python.org/3/library/functions.html#map](https://docs.python.org/3/library/functions.html#map)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to Python's official documentation of the `reduce()` function: [https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce](https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `scikit-learn` library is used to build machine learning pipelines. When
    we define the functions, the library will build a composed object that makes the
    data go through the entire pipeline. This pipeline can include functions, such
    as preprocessing, feature selection, supervised learning, and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be building a pipeline to take the input feature vector,
    select the top *k* features, and then classify them using a random forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to build machine learning pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `pipeline.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s generate some sample data to play with, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This line generated `20` dimensional feature vectors because this is the default
    value. You can change it using the `n_features` parameter in the previous line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step of the pipeline is to select the *k* best features before the
    datapoint is used further. In this case, let''s set `k` to `10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to use a random forest classifier method to classify the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to build the pipeline. The `Pipeline()` method allows us to
    use predefined objects to build the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can also assign names to the blocks in our pipeline. In the preceding line,
    we'll assign the `selector` name to our feature selector, and `rf` to our random
    forest classifier. You are free to use any other random names here!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also update these parameters as we go along. We can set the parameters
    using the names that we assigned in the previous step. For example, if we want
    to set `k` to `6` in the feature selector and set `n_estimators` to `25` in the
    random forest classifier, we can do so as demonstrated in the following code.
    Note that these are the variable names given in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and train the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now predict the output for the training data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s estimate the performance of this classifier, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see which features will get selected, so let''s go ahead and print
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advantage of selecting the *k* best features is that we will be able to
    work with low-dimensional data. This is helpful in reducing the computational
    complexity. The way in which we select the *k* best features is based on univariate
    feature selection. This performs univariate statistical tests and then extracts
    the top performing features from the feature vector. Univariate statistical tests
    refer to analysis techniques where a single variable is involved.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once these tests are performed, each feature in the feature vector is assigned
    a score. Based on these scores, we select the top *k*features. We do this as a
    preprocessing step in our classifier pipeline. Once we extract the top *k*features,
    a k-dimensional feature vector is formed, and we use it as the input training
    data for the random forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.ensemble.RandomForestClassifier()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.feature_selection.SelectKBest()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.pipeline.Pipeline()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.feature_selection.f_regression()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The nearest neighbors model refers to a general class of algorithms that aim
    to make a decision based on the number of nearest neighbors in the training dataset. The
    nearest neighbors method consists of finding a predefined number of training samples
    that are close to the distance from the new point and predicting the label. The
    number of samples can be user defined, consistent, or differ from each other –
    it depends on the local density of points. The distance can be calculated with
    any metric measure – the standard Euclidean distance is the most common choice.
    Neighbor-based methods simply remember all training data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will find the nearest neighbors using a series of points
    on a Cartesian plane.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to find the nearest neighbors, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `knn.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create some sample two-dimensional data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Our goal is to find the three closest neighbors to any given point, so let''s
    define this parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a random datapoint that''s not present in the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to see what this data looks like; let''s plot it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to find the nearest neighbors, we need to define the `NearestNeighbors`
    object with the right parameters and train it on the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now find the `distances` parameter of the input point to all the points
    in the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print `k nearest neighbors`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `indices` array is already sorted, so we just need to parse it and print
    the datapoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s plot the input datapoint and highlight the k-nearest neighbors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will get the following output on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot of the input datapoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f5c2f9d-5498-4c6d-8eb3-4e0d0c9f9346.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second output diagram depicts the location of the test datapoint and the
    three nearest neighbors, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87cc760b-f611-4cb4-90c1-862b9e9f6b2d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we looked for the nearest neighbors by using a series of points
    on a Cartesian plane. To do this, the space is partitioned into regions based
    on the positions and characteristics of the training objects. This can be considered
    as the training set for the algorithm even if it is not explicitly required by
    the initial conditions. To calculate the distance, the objects are represented
    through position vectors in a multidimensional space. Finally, a point is assigned
    to a class if it is the most frequent of the *k* examples closest to the object
    under examination. The proximity is measured by the distance between points. Neighbors
    are taken from a set of objects for which the correct classification is known.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build the nearest neighbors model, the `BallTree` algorithm was used. `BallTree`
    is a data structure that organizes points in a multidimensional space. The algorithm
    gets its name because it partitions datapoints into a nested set of hyperspheres,
    known as **balls**. It's useful for a number of applications, most notably, the
    nearest neighbor search.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.neighbors.NearestNeighbors()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.neighbors.BallTree()` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Nearest Neighbors* (from Texas A&M University College of Engineering):
    [https://www.nada.kth.se/~stefanc/DATORSEENDE_AK/l8.pdf](https://www.nada.kth.se/~stefanc/DATORSEENDE_AK/l8.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing a k-nearest neighbors classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-nearest neighbors algorithm is an algorithm that uses k-nearest neighbors
    in the training dataset to find the category of an unknown object. When we want
    to find the class that an unknown point belongs to, we find the k-nearest neighbors
    and take a majority vote.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create a k-nearest neighbors classifier starting from
    the input data that contains a series of points arranged on a Cartesian plane
    that shows a grouping within three areas.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to build a k-nearest neighbors classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `nn_classification.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `data_nn_classifier.txt` file for input data. Let''s load this
    input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first two columns contain input data, and the last column contains the labels.
    Hence, we separated them into `X` and `y`, as shown in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s visualize the input data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We iterate through all the datapoints and use the appropriate markers to separate
    the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build the classifier, we need to specify the number of nearest
    neighbors that we want to consider. Let''s define this parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to visualize the boundaries, we need to define a grid and evaluate
    the classifier on that grid. Let''s define the step size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to build the k-nearest neighbors classifier. Let''s define
    this and train it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create a mesh to plot the boundaries. Let''s define this, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s evaluate the `classifier` output for all the points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have plotted the color mesh, let''s overlay the training datapoints
    to see where they lie in relation to the boundaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can consider a test datapoint and see whether the classifier performs
    correctly. Let''s define it and plot it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to extract the k-nearest neighbors classifier using the following model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the k-nearest neighbors classifier and highlight it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s print the `classifier` output on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, a series of diagrams are shown. The first output diagram depicts
    the distribution of the input datapoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f7bfe09-977b-4b0f-bf5e-5427fa2ef23a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second output diagram depicts the boundaries obtained using the `k-nearest
    neighbors` classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e34cdba7-455f-4eeb-973d-81be00b44cbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The third output diagram depicts the location of the test datapoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b7d05a3-6f5b-4fa9-8e28-4024c34dd5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The fourth output diagram depicts the location of the 10 nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/169f394f-96e0-471d-a2c2-78c02098a65b.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-nearest neighbors classifier stores all the available datapoints and classifies
    new datapoints based on a similarity metric. This similarity metric usually appears
    in the form of a distance function. This algorithm is a nonparametric technique,
    which means that it doesn't need to find out any underlying parameters before
    formulation. All we need to do is select a value of `k` that works for us.
  prefs: []
  type: TYPE_NORMAL
- en: Once we find out the k-nearest neighbors classifier, we take a majority vote.
    A new datapoint is classified by this majority vote of the k-nearest neighbors classifier.
    This datapoint is assigned to the class that is most common among its k-nearest
    neighbors. If we set the value of `k` to `1`, then this simply becomes a case
    of a nearest neighbor classifier where we just assign the datapoint to the class
    of its nearest neighbor in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-nearest neighbor algorithm is based on the concept of classifying an unknown
    sample by considering the class of *k* samples closest to the training set. The
    new sample will be assigned to the class that most of the *k* nearest samples
    belong to. The choice of *k* is, therefore, very important for the sample to be
    assigned to the correct class. If k is too small, the classification may be sensitive
    to noise; if *k* is too large, the classification may be computationally expensive,
    and the neighborhood may include samples belonging to other classes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to *kNN classifiers* (from the Faculty of Humanities, University of Amsterdam): [http://www.fon.hum.uva.nl/praat/manual/kNN_classifiers_1__What_is_a_kNN_classifier_.html](http://www.fon.hum.uva.nl/praat/manual/kNN_classifiers_1__What_is_a_kNN_classifier_.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.neighbors()` module: [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Nearest neighbor methods* (from New York University): [http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture11.pdf](http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture11.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.neighbors.KNeighborsClassifier()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing a k-nearest neighbors regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how to use the k-nearest neighbors algorithm to build a classifier.
    The good thing is that we can also use this algorithm as a regressor. The object's
    output is represented by its property value, which is the average of the values
    of its k-nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use the k-nearest neighbors algorithm to
    build a regressor.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to build a k-nearest neighbors regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `nn_regression.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s generate some sample Gaussian-distributed data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to add some noise to the data to introduce some randomness into it.
    The goal of adding noise is to see whether our algorithm can get past it and still
    function in a robust way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s visualize it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We just generated some data and evaluated a continuous-valued function on all
    these points. Let''s define a denser grid of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We defined this denser grid because we want to evaluate our regressor on all
    of these points and look at how well it approximates our function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now define the number of nearest neighbors that we want to consider:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s initialize and train the k-nearest neighbors regressor using the parameters
    that we defined earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the regressor performs by overlapping the input and output data
    on top of each other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, the first diagram depicts the input datapoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b7a60e11-8e5a-4fb0-9f15-53ecb8bfe37b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second diagram depicts the values predicted by the regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/789d6996-9492-43c9-a5ac-a2ac90b7654c.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of a regressor is to predict continuous valued outputs. We don''t
    have a fixed number of output categories in this case. We just have a set of real-valued
    output values, and we want our regressor to predict the output values for unknown
    datapoints. In this case, we used a `sinc` function to demonstrate the k-nearest
    neighbors regressor. This is also referred to as the **cardinal sine function**.
    A `sinc` function is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5aa5578d-f46b-4180-9d59-43cc7205db19.png)'
  prefs: []
  type: TYPE_IMG
- en: When `x` is `0`, *sin(x)/x* takes the indeterminate form of *0/0*. Hence, we
    have to compute the limit of this function as `x` tends to be `0`. We used a set
    of values for training, and we defined a denser grid for testing. As you can see
    in the preceding diagram, the output curve is close to the training outputs.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main advantages of this method are that it does not require learning or
    the construction of a model; it can adapt its decision boundaries in an arbitrary
    way, producing a representation of the most flexible model; and it also guarantees
    the possibility of increasing the training set. However, this algorithm also has
    many drawbacks, including being susceptible to data noise, being sensitive to
    the presence of irrelevant features, and requiring a similarity measure to evaluate
    proximity.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official documentation of the `sklearn.neighbors.KNeighborsRegressor()`
    function: [https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *Regression Analysis with R*, Giuseppe Ciaburro, Packt Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Comparison of Linear Regression with K-Nearest Neighbors* (from Duke
    University): [http://www2.stat.duke.edu/~rcs46/lectures_2017/03-lr/03-knn.pdf](http://www2.stat.duke.edu/~rcs46/lectures_2017/03-lr/03-knn.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the Euclidean distance score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have sufficient background in machine learning pipelines and the
    nearest neighbors classifier, let's start the discussion on recommendation engines.
    In order to build a recommendation engine, we need to define a similarity metric
    so that we can find users in the database who are similar to a given user. The
    Euclidean distance score is one such metric that we can use to compute the distance
    between datapoints. We will shift the discussion toward movie recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to compute the Euclidean score between two users.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to compute the Euclidean distance score:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `euclidean_score.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define a function to compute the Euclidean score between two users. The
    first step is to check whether the users are present in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to compute the score, we need to extract the movies that both the
    users rated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'If there are no common movies, then there is no similarity between the users
    (or at least, we cannot compute it given the ratings in the database):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of the common ratings, we just compute the square root of the sum
    of squared differences and normalize it, so that the score is between 0 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: If the ratings are similar, then the sum of squared differences will be very
    low. Hence, the score will become high, which is what we want from this metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `movie_ratings.json` file as our data file. Let''s load it,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consider two random users and compute the Euclidean distance score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following Euclidean distance score
    printed on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, the distance used in the nearest neighbors algorithm is defined
    as the Euclidean distance between two
  prefs: []
  type: TYPE_NORMAL
- en: 'points, calculated according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de18fb90-9e22-492e-ac02-1ce2a95fe7e2.png)'
  prefs: []
  type: TYPE_IMG
- en: On a bidimensional plane, the Euclidean distance represents the minimum distance
    between two points, hence the straight line connecting two points. This distance
    is calculated as the square root of the sum of the squared difference between
    the elements of two vectors, as indicated in the previous formula.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are other types of metrics for calculating distances. All of these types
    generally try to avoid the square roots, since they are expensive in computational
    terms, and are the source of several errors. Metrics include **Minkowski**, **Manhattan**,
    and the **cosine** distance.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to *MATLAB for Machine Learning*, Giuseppe Ciaburro, Packt Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Similarities, Distances, and Manifold Learning* (from The University
    of York): [http://simbad-fp7.eu/images/tutorial/02-ECCV2012Tutorial.pdf](http://simbad-fp7.eu/images/tutorial/02-ECCV2012Tutorial.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *The Euclidean distance* (from Wikipedia): [https://en.wikipedia.org/wiki/Euclidean_distance](https://en.wikipedia.org/wiki/Euclidean_distance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the Pearson correlation score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Euclidean distance score is a good metric, but it has some shortcomings.
    Hence, the Pearson correlation score is frequently used in recommendation engines.
    The Pearson correlation score between two statistical variables is an index that
    expresses a possible linear relation between them. It measures the tendency of
    two numerical variables to vary simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to compute the Pearson correlation score.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to compute the Pearson correlation score:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `pearson_score.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a function to compute the Pearson correlation score between
    two users in the database. Our first step is to confirm that these users exist
    in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to get the movies that both of these users rated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'If there are no common movies, then there is no discernible similarity between
    these users; hence, we return `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to compute the sum of squared values of common movie ratings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compute the sum of squared ratings of all the common movie ratings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now compute the sum of the products:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to compute the various elements that we require to calculate
    the Pearson correlation score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to take care of the case where the denominator becomes `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is good, we `return` the Pearson correlation score, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now define the `main` function and compute the Pearson correlation score
    between two users:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following Pearson correlation score
    printed on the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *r* correlation coefficient of Pearson measures the correlation between
    variables at intervals or equivalent ratios. It is given by the sum of the products
    of the standardized scores of the two variables (z[x] * z[y]) divided by the number
    of subjects (or observations), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ea3ed8d-7463-47fc-9550-ebfc7eaf7d12.png)'
  prefs: []
  type: TYPE_IMG
- en: This coefficient can assume values ranging between -1.00 (between the two variables,
    there is a perfect negative correlation) and + 1.00 (between the two variables,
    there is a perfect positive correlation). A correlation of 0 indicates that there
    is no relationship between the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is necessary to remember that Pearson's formula is related to a linear relationship,
    and therefore, all the different forms of relationship can produce anomalous results.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to *Regression Analysis with R*, Giuseppe Ciaburro, Packt Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *The Pearson Correlation* (from Ken State University): [https://libguides.library.kent.edu/spss/pearsoncorr](https://libguides.library.kent.edu/spss/pearsoncorr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding similar users in the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important tasks in building a recommendation engine is finding
    users who are similar. This is useful in creating the recommendations that will
    be provided to these users.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to build a model to find users who are similar.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to find similar users in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `find_similar_users.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function to find users who are similar to the input user. It
    takes three input arguments: the database, the input user, and the number of similar
    users that we are looking for. Our first step is to check whether the user is
    present in the database. If the user exists, we need to compute the Pearson correlation
    score between this user and all the other users in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to sort these scores in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the *k* top scores and then return them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now define the main function and load the input database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to find three similar users to, `John Carson`, for example. We do this
    by using the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following printed on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are looking for similar users to the input user. Given the
    database, the input user, and the number of similar users that we are looking
    for, we first check whether the user is present in the database. If the user exists,
    the Pearson correlation score between this user and all the other users in the
    database is computed.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To calculate the Pearson correlation score, the `pearson_score()` function was
    used. This function was defined in the previous *Computing the Pearson correlation
    score* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Pearson''s Correlation Coefficient* (from the University of the West
    of England): [http://learntech.uwe.ac.uk/da/default.aspx?pageid=1442](http://learntech.uwe.ac.uk/da/default.aspx?pageid=1442)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Pearson correlation coefficient* (from Wikipedia): [https://en.wikipedia.org/wiki/Pearson_correlation_coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating movie recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will generate movie recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use all the functionality that we built in the previous
    recipes to build a movie recommendation engine. Let's take a look at how to build
    it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to generate movie recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `movie_recommendations.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a function to generate movie recommendations for a given user. The
    first step is to check whether the user exists in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now compute the Pearson score of this user with all the other users
    in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to find the movies that haven''t been rated by this user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'If the user has watched every single movie in the database, then we cannot
    recommend anything to this user. Let''s take care of this condition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a list of these scores. Let''s create a normalized list of movie
    ranks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to sort the list in descending order based on the score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We are finally ready to extract the movie recommendations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the `main` function and load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now generate recommendations for `Michael Henry`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The `John Carson` user has watched all the movies. Therefore, if we try to
    generate recommendations for him, it should display 0 recommendations. Let''s
    see whether this happens, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, you will see the following output on your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we have built a movie recommendation engine. To generate recommendations
    for a given user, the following steps are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we check whether the user is present in the database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we calculate the Person correlation score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then create the normalized list
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we sort this list in decreasing order based on the first column
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we extract the recommended movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build a movie recommendation engine, the `pearson_score()` function was used.
    This function was defined in the previous *Computing the Pearson correlation score* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Introduction to Correlation and Regression Analysis* (from Boston
    University School of Public Health): [http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/bs704_multivariable5.html](http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/bs704_multivariable5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Pearson Correlation Coefficient r* (From Penn State University):
    [https://newonlinecourses.science.psu.edu/stat501/node/256/](https://newonlinecourses.science.psu.edu/stat501/node/256/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Correlation and Causation* (from the Australian Bureau of Statistics):
    [http://www.abs.gov.au/websitedbs/a3121120.nsf/home/statistical+language+-+correlation+and+causation](http://www.abs.gov.au/websitedbs/a3121120.nsf/home/statistical+language+-+correlation+and+causation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing ranking algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Learning to rank** (**LTR**) is a method that is used in the construction
    of classification models for information retrieval systems. The training data
    consists of lists of articles with an induced partial order that gives a numerical
    or ordinal score, or a binary judgment for each article. The purpose of the model
    is to order the elements into new lists according to the scores that take into
    account the judgments obtained from the articles.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the `pyltr` package, which is a Python LTR toolkit
    with ranking models, evaluation metrics, and data-wrangling helpers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to implement ranking algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following package (the full code is
    in the `LambdaMARTModel.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the data contained in the Letor dataset that''s already provided
    for you (`train.txt`, `vali.txt`, and `test.txt`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now perform a validation of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can fit the model using the text data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can predict the data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the results, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LambdaMART is the enhanced tree version of LambdaRank, which is, in turn, based
    on RankNet. RankNet, LambdaRank, and LambdaMART are algorithms that are used to
    solve classification problems in many contexts. RankNet, LambdaRank, and LambdaMART
    have been developed by Chris Burges and his group at Microsoft Research. RankNet
    was the first one to be developed, followed by LambdaRank, and then LambdaMART.
  prefs: []
  type: TYPE_NORMAL
- en: RankNet is based on the use of neural networks, but the underlying model is
    not limited to neural networks alone. The cost function for RankNet aims to minimize
    the number of reversals in the ranking. RankNet optimizes the cost function using
    the stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The researchers found that during the RankNet training procedure, the costs
    are not required, only the gradients (λ) of the cost compared to the model score.
    You can think of these gradients as small arrows attached to each document in
    the classified list, indicating the direction in which we could move those documents.
    LambdaRank is based on this assumption.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, LambdaMART combines the methods contained in LambdaRank and those present
    in **multiple regression additive trees** (**MART**). While MART uses decision
    trees with enhanced gradient for forecasting, LambdaMART uses enhanced gradient
    decision trees using a cost function derived from LambdaRank to solve a ranking
    task. LambdaMART proved to be more efficient than LambdaRank and the original
    RankNet.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Learning to Rank using Gradient Descent*: [https://www.microsoft.com/en-us/research/wp-content/uploads/2005/08/icml_ranking.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2005/08/icml_ranking.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the Python LTR toolkit: [https://github.com/jma127/pyltr](https://github.com/jma127/pyltr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *LETOR: Learning to Rank for Information Retrieval*: [https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fbeijing%2Fprojects%2Fletor%2F](https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fbeijing%2Fprojects%2Fletor%2F)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a filtering model using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collaborative filtering refers to a class of tools and mechanisms that allow
    the retrieval of predictive information regarding the interests of a given set
    of users starting from a large and yet undifferentiated mass of knowledge. Collaborative
    filtering is widely used in the context of recommendation systems. A well-known
    category of collaborative algorithms is matrix factorization.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental assumption behind the concept of collaborative filtering is
    that every single user who has shown a certain set of preferences will continue
    to show them in the future. A popular example of collaborative filtering can be
    a system of suggested movies starting from a set of basic knowledge of the tastes
    and preferences of a given user. It should be noted that although this information
    is referring to a single user, they derive this from the knowledge that has been
    processed throughout the whole system of users.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see how to build a collaborative filtering model for
    personalized recommendations using TensorFlow. We will use the MovieLens 1M dataset,
    which contains 1 million ratings from approximately 6,000 users for approximately
    4,000 movies.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to build a filtering model using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `TensorFilter.py` file that''s already provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load the data contained in the MovieLens 1M dataset that''s already
    provided for you (`ratings.csv`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'The following returns are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform data scaling, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build the user item matrix, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can set some network parameters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will initialize the TensorFlow placeholder. Then, `weights` and `biases`
    are randomly initialized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the encoder and decoder model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'We will construct the model and predict the value, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define `loss` and `optimizer`, and minimize the squared error and
    the evaluation metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now initialize the variables, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can start to train our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: These are the top 10 results for the `1` user.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The collaborative filter approach focuses on finding users who have made similar
    judgments to the same objects, thus creating a link between users, to whom will
    be suggested objects that one of the two has reviewed in a positive way, or simply
    with which they have interacted. In this way, we look for associations between
    users, and no longer between objects.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The user item matrix represents a user's preferences for an object, but, if
    read by columns, highlights who a certain movie was liked or disliked by. In this
    way, you can see how a similarity between two objects can also be expressed without
    the object matrix, simply by observing that the films that are liked by the same
    people are probably similar in some way.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Collaborative filtering* (from The University of Texas at Dallas):
    [https://www.utdallas.edu/~nrr150130/cs6375/2015fa/lects/Lecture_23_CF.pdf](https://www.utdallas.edu/~nrr150130/cs6375/2015fa/lects/Lecture_23_CF.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Matrix Factorization and Collaborative Filtering* (from Carnegie
    Mellon University): [https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture25-mf.pdf](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture25-mf.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the *TensorFlow Tutorial* (from Stanford University): [https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
