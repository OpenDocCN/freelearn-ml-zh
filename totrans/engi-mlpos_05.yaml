- en: 'Chapter 4: Machine Learning Pipelines'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore and implement **machine learning** (**ML**)
    pipelines by going through hands-on examples using the MLOps approach. We will
    learn more by solving the business problem that we've been working on in [*Chapter
    3*](B16572_03_Final_JM_ePub.xhtml#_idTextAnchor053), *Code Meets Data*. This theoretical
    and practical approach to learning will ensure that you will have comprehensive
    knowledge of architecting and implementing ML pipelines for your problems or your
    company's problems. A ML pipeline has modular scripts or code that perform all
    the traditional steps in ML, such as data preprocessing, feature engineering,
    and feature scaling before training or retraining any model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin this chapter by ingesting the preprocessed data we worked on in the
    last chapter by performing feature engineering and scaling it to get it in shape
    for the ML training. We will discover the principles of ML pipelines and implement
    them on the business problem. Going ahead, we''ll look into ML model training,
    hyperparameter tuning, and the testing of the trained models. Finally, we''ll
    learn about packaging the models and their needed artifacts. We''ll register the
    models for further evaluation and will deploy the ML models. We are going to cover
    the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Going through the basics of ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML training and hyperparameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model testing and defining metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model packaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registering models and production artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going through the basics of ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we jump into the implementation of the ML pipeline, let's get the basics
    right. We will reflect on ML pipelines and set up the needed resources for ML
    pipeline implementation and then we will get started with data ingestion. Let's
    demystify ML pipelines by reflecting on the ML pipeline we discussed in *Figure
    14* of [*Chapter 1*](B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015), *Fundamentals
    of MLOps Workflow*.
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 4.1 – Machine learning pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_04_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Machine learning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 4.1*, a comprehensive ML pipeline consists of the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model testing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model packaging
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model registering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will implement all these steps of the pipeline using the Azure ML service
    (cloud-based) and MLflow (open source) simultaneously for the sake of a diverse
    perspective. Azure ML and MLflow are a power couple for MLOps: they exhibit the
    features shown in *Table 4.1*. They are also unique in their capabilities, as
    we can see from the following table.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.2 – MLflow versus Azure ML service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/01.jpg)![Table 4.2 – MLflow versus Azure ML service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2 – MLflow versus Azure ML service
  prefs: []
  type: TYPE_NORMAL
- en: To implement the ML pipeline, we need a storage resource for our dataset and
    a computational resource for our ML models. As discussed before in [*Chapter 2*](B16572_02_Final_JM_ePub.xhtml#_idTextAnchor028),
    *Characterizing Your Machine Learning Problem*, we will perform the computation
    required to implement the ML pipeline and the business problem, as shown in *Figure
    4.2*.
  prefs: []
  type: TYPE_NORMAL
- en: We process the data on our local computer or PC to get started and preprocess
    the data for our ML training. For ML training and pipeline implementation, we
    use compute resources provisioned on the cloud (Microsoft Azure). Even though
    ML training for the pipeline can be done on your local computer, we will use compute
    resources on the cloud to learn how to provision and use the needed compute resources
    for the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 4.3 – Computation location for data and ML tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_04_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Computation location for data and ML tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Without further ado, let''s configure the needed compute resources for the
    ML pipeline using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to your ML workspace.![Figure 4.4 – Azure Machine Learning workspace
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_04_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.4 – Azure Machine Learning workspace
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the **Compute** option and click the **Create** button to explore compute
    options available on the cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the suitable compute option for the ML model training to be optimal and
    efficient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a suitable compute option based on your training needs and cost limitations
    and give it a name. For example, in *Figure 4.4*, a compute or virtual machine
    is selected for the experiment `unique_code-ml-compute1`). The selected compute
    option in *Figure 4.4* is one of the cheapest compute options and this is sufficient
    for implementing the ML pipeline for the business problem. For faster implementation
    and training ML models, it is recommended to use the `STANDARD_DS11_V2` (2 cores,
    14 GB RAM) virtual machine size. With this option, training a model will take
    around 12 minutes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Provision the compute resource created previously. After naming and creating
    the needed compute resource, your compute resource is provisioned, ready, and
    running for ML training on the cloud, as shown in *Figure 4.5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Provisioned compute in an AzureML workspace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_04_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Provisioned compute in an AzureML workspace
  prefs: []
  type: TYPE_NORMAL
- en: After it is provisioned, select the **JupyterLab** option. JupyterLab is an
    open source web-based user interface. It comes with features such as text editor,
    code editor, terminal, and custom components integrated in an extensible manner.
    We will use this as a programming interface connected to the provisioned compute
    to train the ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we''ll begin with the hands-on implementation of the ML pipeline. Follow
    these steps to implement the ML pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: To start the implementation, clone the repository you have imported into the
    Azure DevOps project. To clone the repository, click on the **Clone** button in
    the upper-right corner from the **Repos** menu and then click on the **Generate
    Git Credentials** button. A hash password will be created.![Figure 4.7 – Cloning
    an Azure DevOps Git repository (Generate Git Credentials)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_04_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.7 – Cloning an Azure DevOps Git repository (Generate Git Credentials)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Copy the HTTPS link from the **Command Line** section to get the Azure DevOps
    repository link, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Copy the password generated from *step 1* and add it to the link from *step
    2* by adding the password just after the first username separated by `:` before
    the `@` character. Then it is possible to use the following `git clone` command
    without getting permission errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you are running JupyterLab, we will access the terminal to clone the repository
    to the azure compute. To access the terminal, you must select the **Terminal**
    option from the **Launcher** tab. Another way to access the terminal directly
    is by using the Terminal link from the Application URI column in the list of compute
    instances in the Azure ML workspace. Go to the **Terminal** option of JupyterLab
    and implement the following (as shown in *Figure 4.7*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Clone the Azure DevOps Git repository on Azure compute'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16572_04_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.8 – Clone the Azure DevOps Git repository on Azure compute
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to the `04_MLpipelines` folder and follow the implementation steps on `ML-pipeline.ipynb`
    from the cloned repository. All of the following steps are implemented in `ML-pipeline.ipynb`.
    It is recommended to follow the file instructions to have a better understanding
    of the implementation and execute the code yourself in a new file as per your
    setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So far, we have provisioned the compute resource and cloned the GitHub repository
    in the compute.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we start implementing the `ML-pipeline.ipynb` file by importing the needed
    libraries, such as `pandas`, `numpy`, `azureml`, `pickle`, `mlflow`, and others,
    as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use setup MLflow (for tracking experiments). Use the `get_mlflow_tracking_url()`
    function to get a tracking ID for where MLflow experiments and artifacts should
    be logged (in this case, we get the tracking ID for the provisioned training compute).
    Then use the `set_tracking_uri()` function to connect to a tracking URI (the uniform
    resource identifier of a specific resource) for the provisioned training compute.
    The tracking URI can be either for a remote server, a database connection string,
    or a local path to log data in a local directory. In our case, we point the tracking
    URI to the local path by default (on the provisioned training compute):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By setting the tracking URI for your MLflow experiments, you have set the location
    for MLflow to save its artifacts and logs in the `mlruns` folder (on your provisioned
    compute). After executing these commands, check for the current path. You will
    find the `mlruns` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is essential to train ML models; without data, there is no ML. Data ingestion
    is a trigger step for the ML pipeline. It deals with the volume, velocity, veracity,
    and variety of data by extracting data from various data sources and ingesting
    the needed data for model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ML pipeline is initiated by ingesting the right data for training the ML
    models. We will start by accessing the preprocessed data we registered in the
    previous chapter. Follow these steps to access and import the preprocessed data
    and get it ready for ML training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `Workspace()` function from the Azure ML SDK, access the data from
    the datastore in the ML workspace as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the preprocessed dataset that was prepared in the previous chapter.
    The preprocessed dataset is imported using the `.get_by_name()` function from
    the `Dataset` function from the Azureml SDK and the function is used to retrieve
    the needed dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Upon successfully retrieving or mounting the dataset, you can confirm by printing
    `dataset.name` and `dataset.version`, which should print `processed_weather_data_portofTurku
    1` or as per the name you have given the dataset previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After retrieving the preprocessed data, it is vital to split it into training
    and validation sets in order to train the ML model and test or evaluate it in
    the training phase and later stages. Hence, we split it into the training and
    validation sets, by splitting it in the 80% (training set) and 20% (test set)
    split-ratio as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After successfully splitting the data, these two datasets are stored and registered
    to the datastore (connected to the Azure ML workspace) as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By using the `register()` function, we are able to register the training and
    test datasets, which can be imported later from the datastore.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will import the training data and ingest it into the ML pipeline and
    use the test dataset later to test the model's performance on unseen data in production
    or for model analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion (training dataset)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ingest training data into the ML pipeline, we start by importing it using
    the `get_by_name()` function and converting it to a pandas dataframe using the
    `to_pandas_dataframe()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The training dataset is now retrieved and will be used to further train the
    ML models. The goal is to train classification models to predict whether it will
    rain or not. Hence, select the `Temperature`, `Humidity`, `Wind_speed`, `Wind_bearing`,
    `Visibility`, `Pressure`, and `Current_weather_conditions` features to train the
    binary classification models to predict weather conditions in the future (4 hours
    ahead).
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to select features and scale them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training the ML models, selecting the right features and scaling the
    data is vital. Therefore, we select features as follows. The values in the variable
    `X` represent the independent variables and the variable `Y` is the dependent
    variable (forecasted weather):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the training data into the training and testing sets (for training validation
    after training) using the `train_test_split()` function from `sklearn`. Fixing
    the random seed (`random_state`) is needed to reproduce a training session by
    keeping the samples from the previous experiment with the same configuration.
    Hence, we will use `random_state=1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With an 80% (training data) and 20% (test data) split, the training and test
    datasets are now ready for feature scaling and ML model training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the ML model training to be optimal and efficient, the data needs to be
    on the same scale. Therefore, we scale the data using `StandardScalar()` from
    `sklearn` to calibrate all the numeric values in the data on the same scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this step, the numeric values of the training data are scaled using `StandardScalar`
    and all the values are transformed in the range of `-1` to `1`, based on `X_train
    values`. Now we are ready to train ML models (the fun part)!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Machine learning training and hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are all set to do the fun part, training ML models! This step enables model
    training; it has modular scripts or code that perform all the traditional steps
    in ML training, such as fitting and transforming data to train the model and hyperparameter
    tuning to converge the best model. The output of this step is a trained ML model.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the business problem, we will train two well-known models using the
    **Support Vector Machine** classifier and the **Random Forest** classifier. These
    are chosen based on their popularity and consistency of results; you are free
    to choose models of your choice – there are no limitations in this step. First,
    we will train the Support Vector Machine classifier and then the Random Forest
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Support Vector Machine (SVM)** is a popular supervised learning algorithm
    (used for classification and regression). The data points are classified using
    hyperplanes in an N-dimensional space. It is known for producing significant accuracy
    with less computation power. It is recommended to know SVM, in theory, to better
    understand the model training in practice. To learn more about SVM, head here:
    [https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html](https://www.kdnuggets.com/2017/02/yhat-support-vector-machine.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started with training the SVM classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by initiating the training or experiment using the `Experiment()`
    function from the Azure SDK. The purpose of this function is to start a training
    run or experiment in order to monitor and log the model training performance in
    the Azure ML workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, the MLflow experiment is also initiated to observe a different perspective:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we have initiated an experiment in both the Azure ML workspace and MLflow.
    The following training step will be monitored and logged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we do hyperparameter tuning to find the best parameters to converge the
    best model. This can be done manually, but more efficient and automatic solutions
    such as Grid Search or Random Search exist. For training, the SVM classifier uses
    Grid Search as follows. We proceed by using the `SVC()` and `Grid SearchCV()`
    functions from `sklearn` and logging the run on Azure ML and MLflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, using the best parameters, a new model is trained using `C=1` and
    `kernel=''rbf ''` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this, we have trained the SVM model! We will now train the Random Forest
    classifier model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Random Forest classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random Forest is another popular supervised learning model (used for classification
    and regression). Random Forest is an ensemble learning method that operates with
    a multitude of decision trees. Before performing the model training, it is recommended
    to know the theoretical working of the Random Forest model. To know more about
    the Random Forest model, visit [https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html](https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To start training the Random Forest classifier, initialize the experiment in
    the Azure ML workspace and the MLflow experiment as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the experiment is successfully initiated, training can be initiated by
    importing the `RandomForestClassifier()` function from `sklearn.ensemble` and
    calling the function with the needed parameters, shown as follows. These parameters
    are randomly chosen (no `Grid Search` is done). `Grid Search` or `RandomizedSearch`
    can be used to determine the best parameters and optimize the algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model training is done using the `fit(X_train, y_train)` function by passing
    the training data to it. The training dataset and parameters are logged to Azure
    ML and MLflow experiments as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After training, the output is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the expected result when finishing training the Random Forest model.
    With this, you have successfully finished training the Random Forest model and,
    in total, two ML models: the SVM classifier and the Random Forest classifier.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After training, it is vital to test the performance of the model in terms of
    accuracy and other metrics to know whether the model is fit enough for the production
    or testing environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will test the performance of the trained models on the test data that
    we split before training the models.
  prefs: []
  type: TYPE_NORMAL
- en: Model testing and defining metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, we evaluate the trained model performance on a separate set of
    data points, named test data (which was split and versioned earlier, in the data
    ingestion step). The inference of the trained model is evaluated according to
    the selected metrics as per the use case. The output of this step is a report
    on the trained model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To gain a comprehensive analysis of the model performance, we will measure
    the accuracy, precision, recall, and f-score. This is what they mean in practice
    in the context of the business problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Number of correct predictions by the total number of predictions
    of data test samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: Precision measures the proportion of positives that were correctly
    predicted as positive. *Precision = True Positives / (True Positives + False Positives)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: Recall measures the proportion of actual positives that were identified
    correctly. *Recall = True Positives / (True Positives + False Negatives)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-score**: Both precision and recall are taken into account in the calculation
    of the f-score. It is the harmonic mean (average) of precision and recall. *F1
    Score = 2*(Recall * Precision) / (Recall + Precision)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will measure these metrics for the trained model on the validation dataset.
    Let's see the results for the SVM classifier and the Random Forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the SVM classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using `sklearn.metrics`, we calculate the `accuracy`, `f1_score`, `precision`,
    and `recall` for the model performance on test data samples and log them to the
    Azure ML workspace and MLflow experiments using the `run.log()` function as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'From `sklearn.metrics`, import `accuracy_score`, `f1_score`, `precision_score`,
    and `recall_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The results of the test data metrics are logged in the Azure ML workspace as
    per the experiment. You can read these logs later after registering the model
    (we will register the model in *Registering models and production artifacts*).
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Random Forest classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to what we did for the SVM classifier model, using `sklearn.metrics`
    we calculate the `accuracy`, `f1_score`, `precision`, and `recall`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output of the model performance metrics on test data samples are logged
    to the Azure ML workspace and MLflow experiments using the `run.log()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Model packaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the trained model has been tested in the previous step, the model can
    be serialized into a file to be exported to the test or the production environment.
    Serialized files come with compatibility challenges, such as model interoperability,
    if not done right. Model interoperability is a challenge, especially when models
    are trained using different frameworks. For example, if model 1 is trained using
    `sklearn` and model 2 is trained using TensorFlow, then model 1 cannot be imported
    or exported using TensorFlow for further model fine-tuning or model inference.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this problem, ONNX offers an open standard for model interoperability.
    ONNX stands for Open Neural Network Exchange. It provides a serialization standard
    for importing and exporting models. We will use the ONNX format to serialize the
    models to avoid compatibility and interoperability issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using ONNX, the trained model is serialized using the `skl2onnx` library. The
    model is serialized as the file `svc.onnx` for further exporting and importing
    of the model into test and production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is a serialized `svc.onnx` file. Similarly, using ONNX,
    we will convert the Random Forest model into a serialized file named `rf.onnx`
    for further exporting and importing of the model into test and production environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output of this code is a serialized `rf.onnx` file. Next, we will register
    these serialized models to the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Registering models and production artifacts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, the model that has been serialized or containerized in the previous
    step is registered and stored in the model registry. A registered model is compiled
    as a logical container for one or more files that function as a model. For instance,
    a model made up of multiple files can be registered as a single model in the model
    registry. By downloading the registered model, all the files can be received.
    The registered model can be deployed and used for inference on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s register our serialized models in the previous section by using the
    `model .register()` function from the Azure ML SDK. By using this function, the
    serialized ONNX file is registered to the workspace for further use and deploying
    to the test and production environment. Let''s register the serialized SVM classifier
    model (`svc.onnx`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is registered by naming and tagging the model as per the need. We
    can confirm the successful registering of the model by checking the registered
    model name and version. The output will reflect the model name you used when registering
    (for example, `support-vector-classifier`) and will show the model version as
    `1`. Likewise, let''s register the serialized Random Forest classifier model (`rf.onnx`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After successful registering of the model, the output of the `print` function
    will reflect the model name you used while registering (`random-forest-classifier`)
    and will show the model version as `1`. Lastly, we will register production artifacts
    for inference. Now you can see both models in the **Models** section of the Azure
    ML workspace as shown in *Figure 4.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Registered SVM model (with test metrics)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_04_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Registered SVM model (with test metrics)
  prefs: []
  type: TYPE_NORMAL
- en: This way, you can visualize and analyze your training and testing logs for each
    model trained in the Azure ML workspace. It offers a bird's-eye view of training
    and testing the model while enabling traceability for registered models.
  prefs: []
  type: TYPE_NORMAL
- en: Registering production artifacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For model inference in real time, a scalar is needed in order to scale the incoming
    data on the scale at which the data was scaled for ML training. We will use the
    same scaler function used for `scaling X_train` using `sc.fit_transform(X_train)`
    and serialize this variable into a `pickle` file. Lastly, we register this `pickle`
    file to the workspace for further retrieval and usage as needed (especially for
    model inference in the test and production environment). Using `pickle`, write
    the scaler variable `sc` into a `pickle` file using the `pickle.dump()` function
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle` with `open(''./outputs/scaler.pkl'', ''wb'') as scaler_pkl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code will save a serialized `pickle` file for the scaler
    with the filename `scaler.pkl`. Next, we will register this file to the model
    registry to later download and deploy together with our models for inference.
    The scaler is registered using the `model .register()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon saving and registering the scaler object, a registered object can be found
    on the Azure ML workspace. Likewise, registered models can be tracked, as shown
    in *Figure 4.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Registered models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_04_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Registered models
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! Both the SVM classifier and Random Forest classifier, along
    with the serialized scaler, are registered in the model registry. These models
    can be downloaded and deployed later. This brings us to the successful implementation
    of the ML pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the theory of ML pipelines and practiced them
    by building ML pipelines for a business problem. We set up tools, resources, and
    the development environment for training these ML models. We started with the
    data ingestion step, followed by the model training step, testing step, and packaging
    step, and finally, we completed the registering step. Congrats! So far, you have
    implemented a critical building block of the MLOps workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into evaluating and packaging production models.
  prefs: []
  type: TYPE_NORMAL
