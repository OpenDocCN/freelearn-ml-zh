<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Evaluating Results with TensorBoard</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we understood how a neural network works, what the various hyper parameters in a neural network are, and how they can be tweaked further to improve our model's accuracy.</p>
<p>Google offers TensorBoard, a visualization of the model training logs. In this chapter, we show how to use TensorBoard for TensorFlow and Keras. We interpret the visualizations generated by TensorBoard to understand the performance of our models, and also understand the other functionalities in TensorBoard that can help visualize our dataset better.</p>
<p>As discussed in the previous chapter, Keras as a framework is a wrapper on top of either TensorFlow or Theano. The computations that you'll use TensorFlow for, such as training a massive deep neural network, can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, the creators of TensorFlow have included a suite of visualization tools called TensorBoard.</p>
<p>You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and also to see additional data such as images that were given as input. When TensorBoard is fully configured, it looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1f8752e6-b68a-47ca-9948-880978984c6c.png" style=""/></div>
<p>From this screenshot, you can note that the chart shows a reduction in mean cross-entropy error over an increasing number of epochs. In the later sections of the chapter, we will go through the following:</p>
<ul>
<li>Installing TensorBoard</li>
<li>Overview of the various summary operations captured by TensorBoard</li>
<li>Ways to debug the code</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up TensorBoard</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, we understood how Datalab can be set up. Installing TensorBoard in Datalab is as simple as specifying the following code:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/511333e7-ab7a-4ca9-bea2-b029111e5df1.png" style=""/></div>
<p class="mce-root">Note that we need not make any separate installations for TensorBoard and it comes in prebuilt within the <kbd>google.datalab.ml</kbd> package.</p>
<p>Once the package is imported, we need to start TensorBoard by specifying the location of logs that contain the summaries written by the model fitting process.</p>
<p>The <kbd>tb.start</kbd> method works as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9c3873ed-70ef-4ff0-8a78-176c7d92b25f.png" style=""/></div>
<p>Note that, in the first step, it checks whether the user is permitted to perform the calculation. Next, it picks up an unused port to open TensorBoard, and finally it starts TensorBoard along with printing the link to open TensorBoard.</p>
<p>We will learn more about writing to logs in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of summary operations</h1>
                </header>
            
            <article>
                
<p>Summaries provide a way to export condensed information about a model, which is then accessible in tools such as TensorBoard.</p>
<p>Some of the commonly used summary functions are:</p>
<ul>
<li><kbd>scalar</kbd></li>
<li><kbd>histogram</kbd></li>
<li><kbd>audio</kbd></li>
<li><kbd>image</kbd></li>
<li><kbd>merge</kbd></li>
<li><kbd>merge_all</kbd></li>
</ul>
<p class="mce-root">A <kbd>scalar</kbd> summary operation returns a scalar, that is, the value of a certain metric over an increasing number of epochs.</p>
<p class="mce-root">A <kbd>histogram</kbd> summary operation returns the histogram of various valuesâ€”potentially weights and biases at each layer.</p>
<p class="mce-root">The <kbd>image</kbd> and <kbd>audio</kbd> summary operations return images and audio, which can be <span>visualized and </span>played in TensorBoard respectively.</p>
<p class="mce-root">A <kbd>merge</kbd> operation returns the union of all the values of input summaries, while <kbd>merge_all</kbd> returns the union of all the summaries contained in the model specification.</p>
<p class="mce-root">A visualization of some of the summaries discussed here will be provided in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ways to debug the code</h1>
                </header>
            
            <article>
                
<p>In order to understand how TensorBoard helps, let's initialize a model structure as follows, one that is bound <span>not </span>to work:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3e3e0a0-cdf1-4c9f-8ff0-08992cd12529.png" style=""/></div>
<p>Note that, in this code snippet, the validation accuracy is only around 19%.</p>
<p>The reason for such a low validation accuracy is that the input dataset is not scaled and we are performing ReLU activation on top of an unscaled dataset.</p>
<p>Note that, in the preceding code, we are storing the logs of the model run in the directory <kbd>logs/tensor_new6</kbd> (the sub-directory could be named anything).</p>
<p>Once the logs are stored in this location, we start TensorBoard as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d6a09942-e1d6-4b66-90f0-9298a2042a29.png" style=""/></div>
<p>The preceding code starts TensorBoard, which looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7d81b685-ae67-4660-b155-8dae3419df78.png" style=""/></div>
<p>Note that, by default, the output gives a measure of the scalars, that is, the accuracy and loss values of both the train and test datasets.</p>
<p>The outputs can be visualized adjacent to each other using the regular expression <kbd>.*</kbd> in <kbd>Filter</kbd> tags, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d87aac5c-cfc5-4c46-98c1-63da4e1f35ee.png" style=""/></div>
<p>Note that the first two graphs in this screenshot represent the accuracy and loss of train datasets, while the next two graphs represent the accuracy and loss of validation datasets.</p>
<p>When we look at the histogram of weights and bias across various layers, we learn that the weights and biases do not change across epochs:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1ecc83cb-f675-4804-920f-46e33f2ac6e8.png" style=""/></div>
<p class="mce-root">This is an indication that no learning is happening in the network architecture.</p>
<p class="mce-root">The same can be noted when we look at the distribution of weights and biases across epochs in a different tab:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f4d12641-71a9-499b-9f6f-63fde2752466.png" style=""/></div>
<p>From this screenshot, we can conclude why the accuracy of the model is so low; it's because the model is not able to update the weights.</p>
<p>Now, by clicking on the <span class="packt_screen">GRAPHS</span> tab, let us explore whether the model was initialized correctly:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9593e451-3861-4eb7-b386-00c49adc8c6e.png"/></div>
<p>You should notice that the training block is connected to every other block in the graph. This is because, in order to compute the gradients, one needs to connect to every variable in the graph (as every variable contains weights that need to be adjusted).</p>
<p>Let us, for now, remove the <span class="packt_screen">training</span> block from the graph. This is done by right-clicking on the <span class="packt_screen">training</span> block, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9e44be15-e1ee-4505-924e-aef7c22bc855.png" style=""/></div>
<p>The resultant graph after removing the <span class="packt_screen">training</span> block is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dc17e10a-a576-45a9-9064-6f845e13dc31.png" style=""/></div>
<p>Note that the input layer is connected to hidden layer, which in turn is connected to output layer, from which the metrics and loss are calculated. Let us explore the connections by double-clicking on the individual blocks, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ca36387e-45c9-4775-83e5-2c4939cad734.png"/></div>
<p>A zoom-in of these connections helps us understand the shapes at various blocks:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/957aa933-eadd-4b45-920a-94f5db891ece.png" style=""/></div>
<p>The input layer is (784) in dimension, as there could be any number of input samples but each of them are 784-dimensional. Similarly, the kernel (weight matrix) is 784 x 784 in dimensions and the bias would have 784 initialized values, and so on.</p>
<p>Note that, in the <span>preceding diagram</span>, we take the values in input layer and perform matrix multiplication with the kernel that is initialized using <kbd>random_normal</kbd> initialization. Also note that <kbd>random_normal</kbd> initialization is not connected to the training block, while the kernel block is connected to the training block.</p>
<p>Let us also find out whether the output layer is connected to all the relevant blocks per expectations. Given that the graph looks very complicated, we can use another functionality provided in TensorBoard: <span class="packt_screen">Trace inputs</span>. <span class="packt_screen">Trace inputs</span> help in highlighting only those blocks that are connected to any block of interest. It is activated by selecting the block of interest and toggling the switch in the left-hand pane, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/92b85145-05ad-468f-9c22-cc3bffe95a54.png"/></div>
<p>Now all the connections look fine, but the gradients are <span>still</span><span> </span><span>not getting updated; let us change the activation function to sigmoid a</span>nd then check the weight histograms:</p>
<p>We build a neural network with sigmoid activation as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/546cb038-f973-42c9-8560-4998769eef5a.png"/></div>
<p>Once the neural network structure is defined and compiled, let us fit the model as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/81de7d25-9b97-4a62-b351-2bb125af4b68.png"/></div>
<p>In order to open TensorBoard, we will execute the following code:</p>
<pre>from google.datalab.ml import TensorBoard as tb<br/>tb.start('./logs/tensor_neww3')</pre>
<p>We will then receive the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f94c332-4bbb-407b-936a-db78f0c9c2cf.png" style=""/></div>
<p>At the same time, we should notice that the accuracy and loss metrics have improved considerably:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/108592f0-b4a0-44c1-b55a-bd973f9c8245.png" style=""/></div>
<p>One would also be able to visualize the histogram of gradients by specifying <span class="packt_screen">write_grads=True</span> in the TensorBoard function. The output would then be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/08f06dd8-d983-49f5-acf5-2c8127f65375.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up TensorBoard from TensorFlow</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we have seen that there are two ways to define a model in TensorFlow:</p>
<ul>
<li>Premade estimators</li>
<li>Building a custom estimator</li>
</ul>
<p>In the following code, we will consider one additional snippet of code that would enable us to visualize the various summary operations:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d6200657-4ef8-4d03-a86f-3f3ed8fecd99.png" style=""/></div>
<p>Note that we only need to specify the <kbd>model_dir</kbd> in the premade estimator to store the various log files generated from TensorFlow operations.</p>
<p>TensorBoard can then be initialized by referring to the model directory, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9662f9da-adb9-48f9-8d79-d1fe88ef3c8b.png" style=""/></div>
<p>The <span>preceding code </span>would result in a TensorBoard visualization that would have all the summaries built in.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summaries from custom estimator</h1>
                </header>
            
            <article>
                
<p>In the previous section, we looked at obtaining predefined summaries from premade estimators in TensorBoard. In this section, we will understand obtaining summaries in custom estimators so that they can be visualized in TensorBoard.</p>
<p>The summary operations that need to be captured should be specified in the custom estimator function, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f0b30c65-9d1a-4d65-9e46-f6b8bdde557a.png"/></div>
<p class="mce-root">Note that the model function remains very similar to what we defined in the previous section while learning about custom estimators; however, a few lines of code that write summary to log files are added.</p>
<p class="mce-root"><kbd>tf.summary.scalar</kbd> adds the accuracy metric. Similarly, we might have wanted to add loss (which is another scalar) to logs; however, it gets added by default (note that loss is displayed when we train the model).</p>
<p class="mce-root"><kbd>tf.summary.histogram</kbd> gives a distribution of weights within the network.</p>
<p class="mce-root">Once the model is trained, we should notice the scalars and histogram/distributions in the TensorBoard output. The code to train the model and start TensorBoard is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/35306ac3-eb5e-4b9f-a8c1-689db7b14520.png"/></div>
<p>In the <span>preceding </span>code snippet, we have specified the model function and parameters and the directory to which the log files would be written:</p>
<pre>model.train(input_fn=train_input_fn, steps=1000)</pre>
<p class="mce-root">The <span>preceding </span>code snippet trains the model for 1,000 batches of 1,024 (batch size) data points:</p>
<pre>from google.datalab.ml import TensorBoard as tb<br/>tb.start('/content/datalab/docs/log10/')</pre>
<p>This code snippet starts TensorBoard by using the log files written in the given folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we understood visualizing neural network models in TensorBoard, both from Keras and TensorFlow. We also considered how to visualize the models, distribution of weights, and loss/accuracy metrics in both premade estimators and custom defined estimators. And also the various metrics in neural networks.</p>


            </article>

            
        </section>
    </body></html>