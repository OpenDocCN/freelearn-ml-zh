- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unsupervised Learning: Clustering and PCA'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning models find patterns in unlabeled data. Clustering is
    a technique for finding groups of objects such that the objects in a group are
    like one another, yet objects in different groups are dissimilar. **Principal
    component analysis** (**PCA**) is a technique for reducing the dimensionality
    of data. We will discuss both techniques in the context of product clustering,
    which uses textual product descriptions to group similar products together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discuss two unsupervised learning techniques: clustering and principal component
    analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the K-means clustering algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking the problem down into features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To break down the problems into features, we need to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation:** Load the dataset and inspect the data to understand its
    structure, missing values, and overall characteristics. Preprocess the data, which
    may involve handling missing values, data type conversions, and data cleaning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature engineering:** Select relevant features, extract features from text,
    and derive new features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text data preprocessing:** Tokenize text, remove punctuation, and stop words.
    Convert text to numerical format using the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) technique.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Apply clustering algorithm:** Create a K-means clustering model and determine
    the optimal number of clusters using appropriate techniques like the elbow method
    and silhouette score.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate and visualize clustering results:** Assess clustering performance
    and visualize the results using PCA in reduced dimensionality space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use the TAG prompt pattern as described in *Chapter 2*, that is, specify
    the task, actions to take, and guidance needed.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we’re using the TAG pattern (Task-Action-Guidance) as described
    in *Chapter 2*. We know the following of our problem to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task**:Create a customer segmentation clustering model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: We need to ask for steps to take and techniques to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guidance**: Asking to learn step-by-step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering can help segment customers based on their purchasing behavior, preferences,
    or demographic information. By analyzing customer data such as browsing history,
    purchase history, location, and demographic details, you can apply clustering
    algorithms to identify distinct customer segments. This information can then be
    used to personalize marketing campaigns, recommend relevant products, or tailor
    the user experience to different customer groups.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the e-commerce dataset, which can be downloaded as a CSV file from
    the UCI Machine Learning Repository: [https://archive.ics.uci.edu/dataset/352/online+retail](https://archive.ics.uci.edu/dataset/352/online+retail).
    It contains data for all the transactions that occurred between 01/12/2010 and
    09/12/2011 for a UK-based and registered non-store online retailer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset contains the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`InvoiceNo`: A 6-digit integral number uniquely assigned to each transaction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StockCode`: A 5-digit integral number uniquely assigned to each distinct product'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Description`: A textual description of each product name: text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Quantity`: The quantities of each product (item) per transaction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InvoiceDate`: The day and time when each transaction was generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UnitPrice`: The product price per unit sterling no.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CustomerID`: A 5-digit integral number uniquely assigned to each customer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Country`: The name of the country where each customer resides'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding AI assistance to the unsupervised learning model development process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s leverage ChatGPT to add AI assistance to the unsupervised learning model
    development process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by outlining our task to ChatGPT. Let’s see how we can use the
    TAG pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task**: Our task is to create a Segmentation Clustering model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: We want to know from the AI assistant what actions to take'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guidance**: We need the AI assistant to be efficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on that, let’s write our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Can you please provide guidance on how to load and understand a dataset for
    creating a Customer Segmentation Clustering model using the data from the `data.csv`
    file? The dataset contains the following columns: `InvoiceNo` (integer), `StockCode`
    (string), `Description` (string), `Quantity` (integer), `InvoiceDate` (date),
    `UnitPrice` (decimal), `CustomerID` (integer), `Country` (string).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like to know the necessary steps and techniques to perform this task
    effectively using Python. I will be using Jupyter Notebook. The dataset is in
    the GitHub repository and is using latin1 encoding. Remember I want to learn it
    step by step where the steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the Dataset**: Read the dataset into a pandas DataFrame to work with
    the data effectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inspect the Data**: Display the first few rows of the DataFrame to get a
    glimpse of the data. Check the column names, data types, and any missing values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Summary Statistics**: Calculate summary statistics such as mean, median,
    minimum, maximum, and quartiles for numeric columns. This helps in understanding
    the distribution and range of values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Sure! Let’s go through the step-by-step process of loading the dataset, inspecting
    the data, and calculating summary statistics for the Customer Segmentation Clustering
    model using Python in a Jupyter Notebook. We will assume the dataset is in a file
    named “data.csv” located in a GitHub repository with the “latin1” encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, make sure you have the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s implement the three blocks of code as you requested:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the Dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the Data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Summary Statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let me explain the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the Dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We import the necessary libraries, mainly `pandas` for data manipulation and
    analysis.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We define the `data_url` variable to point to the raw CSV file on GitHub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `pd.read_csv` to read the data from the CSV file into a pandas DataFrame
    called `df`. We display the shape of the DataFrame using `df.shape` to get an
    idea of the number of rows and columns in the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We display the first few rows of the DataFrame using `df.head()` to take a quick
    look at the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inspect the Data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use `df.dtypes` to display the column names and their corresponding data
    types to understand the types of data we have.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `df.isnull().sum()` to check for missing values in each column. This
    will give us the count of missing values for each attribute.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Summary Statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate summary statistics using `df.describe(include='all')`. The `include='all'`
    argument ensures that statistics are calculated for both numeric and non-numeric
    columns.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We transpose the summary statistics using `summary_stats.transpose()` to get
    a more readable format where columns represent attributes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we display the summary statistics using `print(summary_stats)`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember to replace `"yourusername/yourrepository"` with the actual username
    and repository name in the `data_url` variable. Now you should be able to execute
    the code blocks sequentially in a Jupyter Notebook to load, inspect, and understand
    the dataset for your Customer Segmentation Clustering model.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After loading the dataset and running `df.head()`, you should see an output
    similar to this text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Inspect the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Running the code to inspect the data using `df.info()` will give us some initial
    information about the columns, data types, and missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Output of DataFrame after running df.info()'
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can get further information about the data by executing the `Summary Statistics`
    script, `df.describe()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Summary of statistics running df.describe()'
  prefs: []
  type: TYPE_NORMAL
- en: We will need to preprocess the data before we can use it for our Customer Segmentation
    Clustering model.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover preprocessing in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After inspecting the data, we can see that there are some missing values in
    the `CustomerID` column. We will need to remove these missing values before we
    can use the data for our Customer Segmentation Clustering model. Quantitative
    columns, such as `Quantity` and `UnitPrice`, also have some negative values, which
    we will need to remove. We will drop some of the columns that we will not use
    for our Customer Segmentation Clustering model: StockCode, InvoiceDate, Description,
    and Country.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create ChatGPT prompts for each of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like you to help me implement the following steps to preprocess the
    data for my Customer Segmentation Clustering model using Python in a Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Remove Missing Values**: Remove rows with missing values in the `CustomerID`
    column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remove Negative Values**: Remove rows with negative values in the `Quantity`
    and `UnitPrice` columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remove Unused Columns**: Remove the `StockCode`, `InvoiceDate`, `Description`,
    `Country` columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! I’ll guide you through the steps to preprocess the data for your Customer
    Segmentation Clustering model using Python in a Jupyter Notebook. Let’s start
    with the data preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `dropna()` function to remove rows with missing values in the `CustomerID`
    column. This step ensures that we have valid customer IDs for our customer segmentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use boolean indexing to filter out rows with negative values in the `Quantity`
    and `UnitPrice` columns. We only keep rows where both `Quantity` and `UnitPrice`
    are non-negative.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the `drop()` function to remove the columns ‘StockCode’, ‘InvoiceDate’,
    ‘Description’, and ‘Country’ as they are not used for the customer segmentation
    in this case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure to run this block of code after running the previous blocks to continue
    from the preprocessed DataFrame `df_cleaned`. Also, keep in mind that data preprocessing
    is an essential step to ensure the quality of the data used for clustering. These
    steps will help remove invalid or irrelevant data before applying the clustering
    algorithm for customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the preprocessing code and check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: Output of cleaned DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will need to perform some feature engineering to create new features that
    will help us in our Customer Segmentation Clustering model. We will create a new
    column called `Amount` by multiplying the `Quantity` and `UnitPrice` columns.
    We will then create a `TotalAmount` column by summing up `Amount` for each `CustomerID`.
    We will also create a `TotalTransactions` column for each customer by counting
    the number of invoices for this customer. The new dataset that we will use for
    customer segmentation will have two columns: `TotalAmount` and `TotalTransactions`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create ChatGPT prompts for each of these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create Amount Column**: Create a new column called `Amount` by multiplying
    the `Quantity` and `UnitPrice` columns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create TotalAmount Column**: Create a new column called `TotalAmount` by
    summing up `Amount` for each `CustomerID`. Drop the `Amount` column.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create TotalTransactions Column**: Create a new column called `TotalTransactions`
    by counting the number of invoices for each `CustomerID`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create Customer Segmentation Dataset**: Create a new DataFrame called `df_segmentation`
    with two columns: `TotalAmount` and `TotalTransactions`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! I’ll guide you through the feature engineering steps to create new features
    for your Customer Segmentation Clustering model using Python in a Jupyter Notebook.
    Let’s proceed with the feature engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a new column `Amount` by multiplying the `Quantity` and `UnitPrice`
    columns, capturing the total amount spent on each transaction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We group the data by `CustomerID` and calculate the `TotalAmount` for each customer
    by summing up the `Amount` for all their transactions. We use the `groupby()`
    and `sum()` functions for this step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We also group the data by `CustomerID` and count the number of unique invoices
    (transactions) for each customer to create the `TotalTransactions` column using
    the `groupby()` and `nunique()` functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create the final DataFrame `df_segmentation` by merging `df_total_amount`
    and `df_total_transactions` on the `CustomerID` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting `df_segmentation` DataFrame will have two columns: `TotalAmount`
    and `TotalTransactions`, which will be used as features for customer segmentation.
    These new features can provide valuable insights for clustering customers based
    on their spending behavior and transaction frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the code, we should be able to see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated](img/B21232_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: Result of df.segmentation, TotalAmount, and TotalTransactions'
  prefs: []
  type: TYPE_NORMAL
- en: Checking for outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we’re going to check for outliers, which are data points that significantly
    differ from other observations in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to perform segmentation based on the `TotalAmount` and `TotalTransactions`
    features. Clustering algorithms are sensitive to outliers. So, let’s check if
    we have any outliers in our data. Please help me to: drop `CustomerID` column,
    check if there are many outliers in the resultant dataset that will be used for
    Customer segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! Let’s proceed with dropping the `CustomerID` column from the `df_segmentation`
    DataFrame and then check for outliers in the resultant dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We drop the `CustomerID` column from the `df_segmentation` DataFrame using the
    `drop()` function with `axis=1` to specify that we are dropping a column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use box plots to visualize the distribution of `TotalAmount` and `TotalTransactions`
    and identify any potential outliers. The box plots will show the median, quartiles,
    and any data points beyond the whiskers, which can be considered as potential
    outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Inspect the box plot to see if there are any points significantly far from the
    central distribution. Outliers, if present, will appear as individual data points
    outside the whiskers of the box plot. Depending on the visualization, you can
    decide whether to handle the outliers’ using techniques like capping, removing
    them, or using robust clustering algorithms that are less sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the definition of an outlier depends on the context of your
    data and the requirements of your customer segmentation. Outliers may represent
    significant or rare customer behavior and removing them should be done with caution
    and based on domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a box plot  Description automatically generated](img/B21232_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Box plot of TotalAmount and TotalTransactions columns'
  prefs: []
  type: TYPE_NORMAL
- en: Removing outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s remove the outliers in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: The ‘Box plot of TotalAmount and TotalTransactions’ showed that there are outliers
    in the `TotalAmount` column of the dataset. I would like to remove the outliers
    from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! To remove outliers from the `df_segmentation` DataFrame based on the
    `TotalAmount` column, we can use various methods. One common approach is to use
    the Interquartile Range (IQR) method to detect and remove outliers. Here’s how
    you can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the Interquartile Range (IQR) for the `TotalAmount` column using
    the `quantile()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We define the lower and upper bounds for outlier detection as 1.5 times the
    IQR below the first quartile (Q1) and above the third quartile (Q3), respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use boolean indexing to keep only the rows where the `TotalAmount` values
    are within the defined bounds, effectively removing the outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we display the updated shape of the DataFrame after removing outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember that removing outliers can affect the representativeness of your data
    and the clustering results. Before removing outliers, it’s essential to understand
    the impact they might have on your customer segmentation and consider the underlying
    reasons for their presence in the data. Depending on the context and your specific
    use case, you may choose to handle outliers differently, such as capping them,
    transforming the data, or using robust clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the code for step 7 and check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a white background  Description automatically generated](img/B21232_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: Showing the difference in the DataFrame after having removed outliers'
  prefs: []
  type: TYPE_NORMAL
- en: Data scaling using standardization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, let’s carry out a preprocessing step called “data scaling using standardization.”
    It will involve transforming our data so that it has a mean of 0 and a standard
    deviation of 1\. It’s useful when our data have different units or scales and
    ensures each feature contributes equally to the analysis we’re carrying out.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering methods are sensitive to the data scaling. I would like to scale
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’re right! Clustering methods, particularly distance-based ones like K-means,
    are sensitive to the scale of the features. To ensure that all features have the
    same scale and contribute equally to the clustering process, we need to perform
    data scaling. A common approach is to use standardization (also known as z-score
    scaling) or normalization. Here, I’ll show you how to use standardization to scale
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We import `StandardScaler` from the `sklearn.preprocessing` module, which will
    help us perform the standardization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We initialize the `StandardScaler` as `scaler`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use `scaler.fit_transform()` to perform standardization on the `df_segmentation`
    DataFrame, which contains the `TotalAmount` and `TotalTransactions` columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result of the `fit_transform()` operation is an array, so we convert it
    back to a DataFrame using `pd.DataFrame()` with the original column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After scaling, both `TotalAmount` and `TotalTransactions` will have a mean of
    0 and a standard deviation of 1, which helps in the clustering process, ensuring
    that no single feature dominates the clustering algorithm due to a larger scale.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you have a scaled DataFrame `df_scaled`, which is ready to be used for
    customer segmentation using clustering methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running code for step 8 will result in this ouput:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Deciding on the number of clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-means is a popular clustering algorithm used to partition a dataset into k
    clusters, where each data point belongs to a cluster of the nearest mean. The
    goal of using K-means is to identify patterns, by grouping similar data points
    together. You also want to reduce the complexity of your dataset. Other benefits
    include improved analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To effectively apply a clustering method like K-means, I need to decide on
    the number of clusters. There are several methods we could use to find that number;
    let’s ask ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: How can I determine the optimum number of clusters for the dataset?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining the optimum number of clusters, often referred to as the “elbow”
    point, is a crucial step when applying clustering methods like K-means. There
    are several methods you can use to find the appropriate number of clusters for
    your dataset. One of the most common approaches is the “Elbow Method,” but other
    methods like the “Silhouette Score” and “Gap Statistics” can also be helpful.
    Here’s how you can use these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elbow Method**: The Elbow Method involves plotting the **within-cluster sum
    of squares** (**WCSS**) against the number of clusters. WCSS represents the sum
    of squared distances between each point and the centroid of its assigned cluster.
    The idea is to find the point on the plot where the decrease in WCSS starts to
    slow down, forming an “elbow” shape.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the Elbow Method graph, look for the “elbow” point, which is the point where
    the curve starts to flatten. This point indicates the optimal number of clusters
    for your dataset. However, keep in mind that the Elbow Method is not always definitive,
    and the elbow point might not be very pronounced in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Silhouette Score**: The Silhouette Score measures how well each data point
    fits its assigned cluster and ranges from -1 to 1\. A higher Silhouette Score
    indicates better-defined clusters. To use the Silhouette Score, loop through different
    values of K and calculate the average Silhouette Score for each value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The optimal number of clusters is often associated with the highest Silhouette
    Score.
  prefs: []
  type: TYPE_NORMAL
- en: '**Gap Statistics**: Gap Statistics compare the WCSS of the clustering algorithm
    with the WCSS of randomly generated data. It helps determine if the clustering
    structure in your data is significant compared to random data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implementing Gap Statistics is more involved, but libraries like `scikit-learn-extra`
    provide functions to calculate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, look for the point where the Gap Value starts to flatten, indicating
    the optimum number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the choice of the optimal number of clusters is often a subjective
    decision based on the insights you seek and the specific problem context. These
    methods can give you a starting point, but you may need to experiment and validate
    the clustering results based on your domain knowledge and business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s execute the code for the elbow method and the silhouette score and inspect
    the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing a number of clusters  Description automatically generated](img/B21232_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: Number of clusters using the elbow method'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with a line  Description automatically generated](img/B21232_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Number of clusters using the silhouette score method'
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method and the silhouette score method that we used to determine the
    optimal number of clusters suggested six clusters. We will use six clusters for
    our K-means model. Let’s create a ChatGPT prompt to generate a Customer Segmentation
    K-means clustering model with six clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a clustering model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve determined the number of clusters to use, let’s create a cluster
    model next.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Please help me to create a Customer Segmentation K-means clustering model with
    6 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! Let’s create a Customer Segmentation K-means clustering model with 6
    clusters using the scaled data `df_scaled`. We’ll use the `KMeans` class from
    scikit-learn to perform the clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We specify `num_clusters` as 6, which will create 6 clusters for customer segmentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create the K-means clustering model using `KMeans` with the desired number
    of clusters and set `random_state` for reproducibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is fitted to the scaled data `df_scaled` using `fit()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add the cluster labels to the original DataFrame `df_segmentation` using
    `kmeans_model.labels_`, and store it in a new column called ‘Cluster’.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting `df_segmentation` DataFrame will now include the ‘Cluster’ column
    indicating the cluster assignment for each customer based on their ‘TotalAmount’
    and ‘TotalTransactions’. Each customer will be associated with one of the six
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the interpretation of the clusters will depend on the context
    of your data and your domain knowledge. You may want to analyze the characteristics
    of each cluster to gain insights about different customer segments and tailor
    your marketing or business strategies accordingly. Visualization techniques, such
    as scatter plots or cluster profiles, can be helpful in understanding and interpreting
    the results of the customer segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: Output from running df.segmentation()'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize clustering results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we can visualize the results of the clustering. Let’s create the next
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like to visualize the results of the clustering. Please create a scatter
    plot with the following specifications: the horizontal axis should be “Total Amount”,
    the vertical axis should be “Total Transactions”, and the color of the dots should
    be determined by the cluster number. Please also add a legend to the plot.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Sure! Let’s create a scatter plot to visualize the results of the clustering.
    We’ll use the `matplotlib` library for plotting, and the `seaborn` library for
    color mapping and a more visually appealing plot. If you don’t have the `seaborn`
    library installed, you can install it using `pip install seaborn`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We use `seaborn.scatterplot()` to create the scatter plot. The ‘TotalAmount’
    is plotted on the horizontal axis (`x`), the ‘TotalTransactions’ on the vertical
    axis (`y`), and the ‘Cluster’ column is used for color mapping using the `hue`
    parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the color palette to ‘viridis’ using the `palette` parameter, which will
    provide distinct colors for each cluster. The `s=50` parameter sets the size of
    the dots in the scatter plot to 50 for better visibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add axis labels, a plot title, a legend, and grid lines to improve the plot’s
    readability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting scatter plot will display each customer as a dot in the 2-dimensional
    space defined by ‘TotalAmount’ and ‘TotalTransactions’. The dots will be colored
    according to their cluster assignment, allowing you to visually identify the different
    customer segments. The legend will help you understand which color corresponds
    to which cluster number.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a customer segmentation  Description automatically generated](img/B21232_16_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.10: Data points divided up into six clusters'
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts on clustering and the prompting process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have completed Customer Segmentation Clustering with the assistance of ChatGPT.
    To achieve our goal, we have used several different types of prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt often begins by describing the dataset to be used, such as the source,
    columns, and their types, which sets a high-level context. ChatGPT then requests
    assistance in implementing a Customer Segmentation clustering model using a specific
    algorithm, in this case, K-means clustering. To summarize the prompts used, we
    can see that it blends a set of different techniques, which mirrors how you would
    carry out data science without an AI assistant. You can see below the different
    types of prompt types and how they helped with the clustering process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step-by-step instructions**: These provided a step-by-step guide on how to
    approach the problem, which includes loading and understanding the dataset, data
    preprocessing (removing missing values, negative values, and unused columns),
    and feature engineering (creating new features like Amount, TotalAmount, and TotalTransactions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering algorithm choice**: This specified the choice of the clustering
    algorithm, which is K-means clustering in this case, along with the number of
    clusters to be used for segmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier handling**: addressed the sensitivity of clustering algorithms to
    outliers and requests to remove outliers from the data using the **interquartile
    range** (**IQR**) method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data scaling**: This emphasized the importance of scaling the data to ensure
    clustering accuracy and instructs to use standardization to scale the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster visualization**: This asked for visualization techniques to display
    the results of the clustering model, typically through scatter plots, with TotalAmount
    on the x-axis, TotalTransactions on the y-axis, and color-coded points representing
    different clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal number of clusters**: The prompt guides in determining the optimal
    number of clusters using the elbow method and silhouette score, which helps in
    finding the right number of clusters that best represent the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combined script request**: The prompt requests to combine all the generated
    code into a single script, including detailed explanations as comments, to provide
    a comprehensive solution for implementing the Customer Segmentation clustering
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product clustering for an e-commerce project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this task, we will perform product clustering using the same e-commerce dataset.
    The goal is to create an unsupervised learning model to cluster products based
    on their textual descriptions using Python and common libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your initial prompt: Set context'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, when using ChatGPT, it’s important to set a high-level context. For
    data science, high-level context means describing things like the data used (e-commerce
    data), what we’re trying to achieve (get data insights), and how we’re looking
    to achieve it (clustering).
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example of a prompt providing all the suggested context we just
    described in the preceding paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like you to act as an experienced data scientist and help me implement
    Product Categorization model using clustering. The dataset is e-commerce data
    from the UCI Machine Learning Repository. It is a transnational dataset that contains
    all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based
    and registered non-store online retailer. The dataset contains the following columns:
    `InvoiceNo` (integer), `StockCode` (string), `Description` (string), `Quantity`
    (integer), `InvoiceDate` (date), `UnitPrice` (decimal), `CustomerID` (integer),
    `Country` (string). Can you suggest the steps needed to perform Product Categorization
    using the data from a csv file.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should receive a response like the below response:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly! Product categorization using clustering is a great approach to group
    similar products together based on their attributes. Here’s a step-by-step guide
    to perform product categorization using the given e-commerce dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Load and Preprocess the Data'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset from the CSV file into a suitable data structure, like a Pandas
    DataFrame in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inspect the data to understand its structure, missing values, and overall characteristics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the data, which may involve handling missing values, data type conversions,
    and data cleaning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 2: Feature Engineering'
  prefs: []
  type: TYPE_NORMAL
- en: Select relevant features for clustering. In this case, you can consider features
    like ‘StockCode’, ‘Description’, ‘Quantity’, ‘UnitPrice’, etc. Customer-related
    features like ‘CustomerID’ might not be directly relevant for product categorization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a feature matrix, where each row corresponds to a product and each column
    corresponds to a feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 3: Text Data Preprocessing'
  prefs: []
  type: TYPE_NORMAL
- en: For text-based features like ‘Description’, perform text preprocessing steps
    such as lowercasing, tokenization, removing punctuation, and stopwords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the text data into numerical format using techniques like TF-IDF (Term
    Frequency-Inverse Document Frequency) or word embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 4: Choose Clustering Algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Select an appropriate clustering algorithm. Common choices include K-means,
    Hierarchical clustering, and DBSCAN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune hyperparameters if needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 5: Feature Scaling'
  prefs: []
  type: TYPE_NORMAL
- en: Scale the numerical features to have similar ranges. This is important for algorithms
    like K-means that are distance-based.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 6: Apply Clustering Algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the chosen clustering algorithm to the preprocessed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the optimal number of clusters. You can use techniques like the Elbow
    Method, Silhouette Score, or Gap Statistic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 7: Interpret Clusters'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have clusters, analyze the products within each cluster to understand
    the common characteristics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign meaningful labels to the clusters based on your analysis. These labels
    will represent the product categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 8: Visualize Clusters (Optional)'
  prefs: []
  type: TYPE_NORMAL
- en: Use visualization techniques such as scatter plots, t-SNE, or PCA to visualize
    the clusters in a reduced dimensional space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 9: Assign Categories to Products'
  prefs: []
  type: TYPE_NORMAL
- en: Assign each product to its respective category based on the cluster it belongs
    to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 10: Evaluate and Refine'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the quality of your clustering results. You can use domain knowledge
    or external validation metrics if available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refine the process by adjusting preprocessing steps, algorithm parameters, or
    features as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember that clustering is an iterative process, and the quality of your results
    may depend on various factors such as data quality, feature selection, and algorithm
    choice. It’s also important to consider the business context and domain knowledge
    when interpreting and labeling the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Having reviewed the steps in the response, you can now proceed with the next
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Load and preprocess the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a series of steps described to us how to perform clustering,
    we want to carry out the first step from the response namely loading and preprocessing
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like to do it step by step. Please guide me through Step 1: Load and
    Preprocess the Data. My data URL is “../Datasets/data.csv” and the CSV file is
    using latin1 encoding. I would use the column ‘Description’ for clustering to
    identify product categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should receive a response like the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course! Let’s start with Step 1: Load and Preprocess the Data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This code will load the dataset from the provided CSV file using the Latin1
    encoding. It will then display basic information about the dataset, such as the
    data types of each column and the number of non-null values.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, it will display the first few rows of the dataset to give you
    a sense of the data’s structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.11: First few rows of the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s preprocess the data and focus on the ‘Description’ column for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we first drop rows with missing ‘Description’ values since
    we’ll be using this column for clustering. Next, we display basic statistics of
    the numerical columns to get an overview of the data’s distribution. After that,
    we count the number of unique descriptions to understand the variety of products.
    Finally, we display the top 10 most common descriptions to see the most frequent
    products in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have loaded the data, dropped missing values in the ‘Description’
    column, and gained some insights into the data’s characteristics. The next step
    will involve text data preprocessing for the ‘Description’ column. Let me know
    if you’re ready to move on to Step 3, or if you have any questions about the current
    steps!
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: After copying the code to a Jupyter Notebook, we find that it works perfectly
    fine, and we learn that 541,909 dataset records contain 4,223 unique product descriptions.
    The next step is to perform text data preprocessing for the ‘Description’ column.
    Let’s proceed with the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and text data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, the data has been read and we know the size of the dataset as
    well. Let’s carry on with the next two steps, feature engineering and text data
    preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains 541909 entries and there are 4223 unique product descriptions.
    I would like to drop duplicate ‘Descriptions’ and perform other text data preprocessing
    and feature engineering. Please create a script with comments and include libraries
    and `nltk` resources to download.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure, I’ll provide you with a script that covers text data preprocessing and
    feature engineering steps. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In this script:'
  prefs: []
  type: TYPE_NORMAL
- en: We import necessary libraries including `pandas`, `nltk`, and `TfidfVectorizer`
    from `sklearn.feature_extraction.text`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We download NLTK resources (`punkt` for tokenization and `stopwords` for removing
    stopwords) if they are not already downloaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We load the dataset and drop rows with missing ‘Description’ values and duplicate
    descriptions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We perform text preprocessing steps on the ‘Description’ column: converting
    to lowercase, tokenizing, removing punctuation, and stopwords.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We convert the tokenized descriptions back to strings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use TF-IDF vectorization to convert the preprocessed text data into numerical
    features. You can adjust the `max_features` parameter to control the number of
    features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TF-IDF matrix is converted to a DataFrame for further analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This script will preprocess the text data and perform feature engineering using
    TF-IDF vectorization. The resulting TF-IDF matrix can be used as input for clustering
    algorithms in the subsequent steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: After pasting the script to Jupyter Notebook, we find that it works perfectly
    fine, and we learn that the TF-IDF matrix has 4,223 rows (one for each unique
    product description) and 1,000 columns (one for each feature).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.12: TF-IDF matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will continue with feature engineering. Let’s visualize the most frequent
    features in the TF-IDF matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I would like to visualize the top 100 most frequent features from the TF-IDF
    matrix. Please create a bar chart with words on the vertical axis.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly! To visualize the top 100 most frequent features from the TF-IDF
    matrix, you can create a bar chart using libraries like `matplotlib`. Here’s how
    you can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This script calculates the sum of TF-IDF scores for each feature and selects
    the top 100 features by frequency. It then creates a horizontal bar chart using
    `matplotlib` to visualize the top features. Adjust the figure size, labels, and
    other parameters as needed to customize the visualization to your preferences.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this code in Jupyter Notebook will produce the following bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a number of people  Description automatically generated with medium
    confidence](img/B21232_16_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.13: Most frequent features'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s analyze this chart and proceed with the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the features contain numbers and words representing colors,
    like red, blue, and other colors. These words will not be useful for product categorization.
    Can you create a list of words that can represent color.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Certainly! Here’s a list of common words that represent colors. You can use
    this list to filter out color-related words from your features before performing
    product categorization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use this list to filter out color-related words from your features
    during the text preprocessing step. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: By removing color-related words, you can help improve the quality of your product
    categorization by focusing on more meaningful features.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the script to remove `color_words` and words that are less than three
    characters long. Refactor code to encapsulate data preprocessing and vectorisation
    into a function `preprocess_and_vectorize_data`.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly! Here’s the modified script with the requested changes, including
    encapsulating the data preprocessing and vectorization steps into a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This script encapsulates the data preprocessing and vectorization steps into
    the `preprocess_and_vectorize_data` function. The function takes the raw data
    as input, performs the required preprocessing, and returns the TF-IDF matrix.
    The rest of the script remains similar to before.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: After running the script, we can visualize the updated set of the most frequent
    words from the TF-IDF matrix output.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a number of people  Description automatically generated with medium
    confidence](img/B21232_16_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.14: Most frequent features that are not colors'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed with our feature engineering. We will use ChatGPT to suggest options
    that can be explored with TF-IDF vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: The TF-IDF vectorization is initialized with `max_features` parameter. What
    are other initialization options?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: When initializing the TF-IDF vectorizer in scikit-learn, you can set several
    parameters to customize its behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the `max_features` parameter, here are some other important
    parameters you can consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_df` and `min_df`: These parameters control the threshold for the document
    frequency of terms. For example, you can set `max_df` to ignore terms that appear
    in a high percentage of documents, and `min_df` to ignore terms that appear in
    a low percentage of documents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ngram_range`: This parameter specifies the range of n-grams to consider during
    tokenization. For example, setting `ngram_range=(1, 2)` will consider both individual
    words and pairs of consecutive words (bigrams).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`stop_words`: You can provide a list of stopwords to be ignored during tokenization.
    This can be a custom list or one of the built-in lists like `''english''`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`tokenizer`: You can specify a custom tokenizer function if you want to perform
    additional text preprocessing or tokenization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`use_idf`: This parameter controls whether to use the inverse-document-frequency
    (IDF) re-weighting of the term frequencies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`smooth_idf`: This parameter controls whether to add a smoothing term to the
    IDF to avoid division by zero errors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sublinear_tf`: If set to `True`, it applies sublinear scaling to the term
    frequency, which can help to prevent very frequent terms from dominating the calculation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s an example of how you can initialize the TF-IDF vectorizer with some
    of these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to experiment with these parameters based on your specific dataset
    and goals. The choice of parameters may depend on the nature of the text data
    and the desired outcomes of your clustering or categorization task.
  prefs: []
  type: TYPE_NORMAL
- en: Choose clustering algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are quite a few clustering algorithms we could choose. There is, for example,
    K-means, that we used for another use case; there’s also hierarchical clustering,
    **DBSCAN** (**Density-based clustering algorithm**), **Gaussian mixture models**
    (**GMMs**), and agglomerative clustering. We’re going to go with K-means, but
    why is that? The reason is that we’re working with customers and want to divide
    them up into groups based on purchasing behavior, demographic properties, and
    so on. Another reason for choosing K-means is its simplicity and being able to
    work with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might wonder when to apply the other mentioned clustering algorithms in
    the context of e-commerce. Well, let’s look at some interesting areas in this
    domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Product recommendation**: Here, it would be more appropriate to use hierarchical
    clustering as products usually come in hierarchies and we want to understand the
    relationship between products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Another important area is anomaly detection, especially
    fraudulent transactions. Here, you want to understand unusual customer behavior
    to identify outliers; for that use case, DBSCAN is the best algorithm to go with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalization**: You want to ensure customers feel like they have an experience
    uniquely fitted to their behaviors on the website. GMM is a great choice for this
    use case when a customer’s behavior is a bit more complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature scaling and TF-IDF are both preprocessing techniques and it only makes
    sense to choose one of them. Which one to choose depends on what we want to use
    it for. In the context of e-commerce, here’s how you could reason:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose feature scaling if you’re dealing with numerical features like customer
    age, purchase amounts, and purchase frequency (e.g., grouping customers based
    on their purchasing behavior).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the TF-IDF matrix if you’re looking at other aspects of e-commerce like
    customer reviews, product descriptions, and search queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can skip this step as we are using a TF-IDF matrix that is already scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Apply clustering algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let’s apply our chosen clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like to use english `stop_words`, no `max_features`**,** and pass `min_df`
    to `preprocess_and_vectorize_data` function to set minimum number of documents
    the feature appears in. I would like to try a range of values for `min_df: 5,
    10, 20, 30` and compare the silhouette score from the K-means output. I would
    like to visualize the result.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! Let’s modify the script to accommodate your requirements for `min_df`
    and the comparison of silhouette scores for different values of `min_df` using
    `KMeans` clustering. Here’s the updated script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this script, we’re looping through different values of `min_df` (5, 10,
    20, and 30) and performing the following steps for each value:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess and vectorize the data using the `preprocess_and_vectorize_data`
    function with the specified `min_df` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform K-means clustering with 10 clusters (you can adjust the number of clusters
    as needed).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the silhouette score for the clustering results and store it in the
    `silhouette_scores` list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we visualize the silhouette scores for different `min_df` values using
    a line plot. This will help you compare how different values of `min_df` impact
    the quality of the clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with a line  Description automatically generated](img/B21232_16_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.15: Silhouette score versus min_df'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will investigate the impact of the number of clusters on the quality
    of the clustering results. Let’s proceed with the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I would also like to try different number of clusters `num_clusters = [3,4,
    5,6,7]` and visually compare silhouette score outputs for `min_df`, `num_clusters`
    combinations. Please add the script that generates a plot of cluster and silhouette
    score values for each of the values from `num_clusters`.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly! Here’s the updated script that generates a plot of silhouette scores
    for different combinations of `min_df` and `num_clusters`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this script, we use the `itertools.product` function to create combinations
    of `min_df` and `num_clusters` values. We calculate silhouette scores for each
    combination and store them in the `silhouette_scores` list. Then, we convert the
    silhouette scores to a DataFrame for easier visualization. The resulting plot
    shows how the silhouette score varies for different values of `num_clusters` while
    considering different `min_df` values.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing the number of clusters  Description automatically generated](img/B21232_16_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.16: Silhouette score versus different min_df values'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s refine the plot to show the number of features corresponding to the `min_fd`
    parameter value.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: The plot is good. I would like a small change by adding the number of features
    for each of the `min_df` values to the `label=f'min_df = {min_df}`
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Certainly! You can add the number of features corresponding to each `min_df`
    value to the legend labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the modified code with that change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this modified script, we calculate the number of features for each `min_df`
    value using the `shape[1]` attribute of the TF-IDF DataFrame. Then, we include
    this information in the legend labels for each line on the plot. This change will
    help you visualize the impact of both `min_df` and the number of features on the
    silhouette scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph showing the number of clusters  Description automatically generated](img/B21232_16_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.17: Silhouette score versus different min_df values with number of
    features'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed with the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I will use the value of `min_df_value = 20`, `num_clusters= [3, 4, 5, 6]`. I
    would like to know the number of products in each cluster. Can you add a plot
    `Products per Cluster` to the previous code to show number of products per cluster
    for `num_clusters_values` in [3, 4, 5, 6]
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly! You can add a plot to visualize the number of products in each cluster
    for different `num_clusters` values. Here’s the modified script with the requested
    changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this modified script, we calculate the number of products in each cluster
    using `pd.Series(cluster_labels).value_counts().sort_index()` and store the results
    along with silhouette scores in the `results` list. We then create a DataFrame
    from the results for easier visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script includes two plots side by side: the first plot displays the silhouette
    scores for different `num_clusters` values, and the second plot displays the number
    of products in each cluster for the same `num_clusters` values. This will help
    you understand the relationship between the number of clusters, silhouette scores,
    and the distribution of products among clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the script, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A comparison of graphs with lines  Description automatically generated with
    medium confidence](img/B21232_16_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.18: Added plot of products per cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We will now select the number of clusters and `min_df` value and proceed with
    the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Interpret clusters and visualize results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll be looking into interpreting clusters and visualizing the results. What
    are the things we should bear in mind?
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For cluster interpretation, we should consider things like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cohesion: Ensure data points are similar to each other. You can find this out
    by examining terms that are the most representative in each cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Separation: Clusters should be well separated ensuring they are distinct and
    meaningful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labels: Clusters should also have descriptive labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Outliers: It’s important to identify outliers that don’t fit well in any cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For visualizing clusters, here are some things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction: You want to reduce the high-dimensional TF-IDF matrix
    to 2D or 3D to make it easier to visualize. For this, a technique like PCA comes
    in handy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scatter plots: Scatter plots are great for visualizing data points; a point
    can be a document, for example, and colors or shapes could represent different
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word clouds: You could also use word clouds if you look at text, to see what
    the most common words used are.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heat maps: Heat maps are another great way to visualize frequency and will
    help highlight the most important terms per cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interactivity: Ensure you allow for exploration to help you drill down into
    specific data points by using, for example, Plotly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s craft a prompt for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I will use the value of `min_df_value = 20` and 5 clusters. I would like to
    use PCA with 3 components to visualize product clusters on a 3D graph. The colors
    of clusters should be mapped from the seaborn `color_palette`.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly! In order to visualize the product clusters in a 3D graph using PCA
    with 3 components and mapping cluster colors from a seaborn color palette, you
    can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the data and perform clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply PCA to reduce the dimensionality to 3 components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the clusters in a 3D scatter plot with seaborn color palette.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the code to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: This code will preprocess the data, perform clustering, apply PCA to reduce
    dimensionality to 3 components, and then plot the product clusters in a 3D scatter
    plot with cluster colors mapped from a seaborn color palette. Adjust the figure
    size, labels, and other plot parameters as needed to customize the visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of clustering data  Description automatically generated](img/B21232_16_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.19: Product clusters visualized in 3D space'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a word cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, we will examine the most frequent words in each of the clusters. Let’s
    proceed with the next prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I would like to visualize the most frequent words occurring in product descriptions
    assigned to clusters. I would like you to use the word cloud library and create
    a function that will output word clouds for each of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! To visualize the most frequent words occurring in product descriptions
    assigned to clusters, you can use the `WordCloud` library. Here’s how you can
    create a function that generates word clouds for each of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a function `generate_wordclouds` that generates word clouds
    for each cluster based on the product descriptions assigned to that cluster. Adjust
    the parameters of the `WordCloud` object (e.g., `width`, `height`, `background_color`,
    `max_words`) as needed for your visualization preferences.
  prefs: []
  type: TYPE_NORMAL
- en: The function will plot the word clouds for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of words  Description automatically generated](img/B21232_16_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.20: Word cloud for the first cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '![A word cloud with different colored words  Description automatically generated](img/B21232_16_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.21: Word cloud for the second cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '![A word cloud with different colored text  Description automatically generated](img/B21232_16_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.22: Word cloud for the third cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '![A word cloud with different colored words  Description automatically generated](img/B21232_16_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.23: Word cloud for the fourth cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Assigning categories to products and evaluating and refining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neither assigning categories to products nor evaluating and refining steps were
    in scope for this chapter to solve but let’s describe what they are to give you
    an idea of how you could explore those should you want to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset contains invoices, and for us to categorize products, we would
    first need to understand the use cases we could possibly have. Those could be,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Organizing inventory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving search functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing the user experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, this is very centered on product data, which means we would
    probably need a completely different dataset containing product titles, descriptions,
    and more. Let’s describe at a high level what our approach could be using ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: I’m looking at an e-commerce dataset. What can you tell me about the process
    of assigning categories to products?
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assigning categories to products in an e-commerce dataset is a crucial task
    for organizing inventory, improving search functionality, and enhancing the user
    experience. Here’s a step-by-step guide to help you through the process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps to Assign Categories to Products:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understand the Data**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Review Product Descriptions**: Examine the product titles, descriptions,
    and any other relevant attributes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identify Existing Categories**: If there are existing categories, review
    them to understand the current classification system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define Categories**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a Category Hierarchy**: Develop a hierarchical structure of categories
    and subcategories that makes sense for your products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensure Completeness**: Make sure your categories cover all types of products
    in your dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Extraction**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text Features**: Extract relevant text features from product titles and descriptions
    using techniques like TF-IDF or word embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other Features**: Consider other attributes like price, brand, and specifications
    that might help in categorization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manual Labeling**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample Labeling**: Manually label a sample of products to create a training
    dataset. This helps in understanding the nuances of each category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Selection**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Choose a Classification Model**: Select a machine learning model suitable
    for text classification, such as Naive Bayes, SVM, or deep learning models like
    LSTM or BERT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training the Model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train the Model**: Use the labeled data to train your classification model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluate Performance**: Evaluate the model’s performance using metrics like
    accuracy, precision, recall, and F1-score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assign Categories**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predict Categories**: Use the trained model to predict categories for the
    unlabeled products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Review Predictions**: Manually review a subset of the predictions to ensure
    accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterate and Improve**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Refine Categories**: Based on the results, refine your category definitions
    and model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous Learning**: Continuously update the model with new data to improve
    its accuracy over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simplified example using a Naive Bayes classifier for text classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This code provides a basic framework for categorizing products based on their
    names. For a real-world application, you would need a larger dataset and more
    sophisticated preprocessing and modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any specific questions or need further details, feel free to ask!
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll leave it up to you to pursue this further. From what you’ve hopefully
    seen in this chapter already, start with a high-level prompt explaining what dataset
    you have (you have to find a dataset for the above problem) and what you aim to
    do. On the latter, you already have the steps, so you’re suggested to craft a
    prompt explaining what to do for each step and what code to run.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate and refine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is also out of scope, but let’s get you started on a ChatGPT prompt that
    you can then keep working on.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: Describe evaluate and refine step in the context of e-commerce data on invoices.
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of prompt]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Prompt response]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sure! Let’s break down the evaluate and refine steps in the context of e-commerce
    data on invoices:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluate Step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Performance Metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accuracy**: Measure how often the model correctly predicts the category of
    an invoice.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision and Recall**: Evaluate the precision (correct positive predictions)
    and recall (ability to find all relevant instances) for each category.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-Score**: Combine precision and recall into a single metric to balance
    both aspects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Confusion Matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Visualize Predictions**: Use a confusion matrix to see where the model is
    making correct and incorrect predictions. This helps identify specific categories
    where the model may be struggling.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross-Validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Robust Evaluation**: Perform cross-validation to ensure the model’s performance
    is consistent across different subsets of the data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Error Analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Analyze Misclassifications**: Examine the invoices that were misclassified
    to understand why the model made errors. Look for patterns or common issues.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refine Step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature Engineering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enhance Features**: Create new features or improve existing ones to provide
    more relevant information to the model. For example, extracting keywords from
    invoice descriptions or adding metadata like invoice date and amount.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyperparameter Tuning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Optimize Model Parameters**: Use techniques like grid search or random search
    to find the best hyperparameters for your model, improving its performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model Selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Try Different Models**: Experiment with different algorithms (e.g., decision
    trees, random forests, neural networks) to see if another model performs better.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Increase Data Variety**: Augment your dataset by generating synthetic data
    or using techniques like oversampling for underrepresented categories.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regular Updates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continuous Learning**: Regularly update the model with new invoice data to
    keep it current and improve its accuracy over time.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simplified example of evaluating and refining a model using cross-validation
    and hyperparameter tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This code provides a framework for evaluating and refining a model using cross-validation
    and hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any specific questions or need further details, feel free to ask!
  prefs: []
  type: TYPE_NORMAL
- en: '**[End of response]**'
  prefs: []
  type: TYPE_NORMAL
- en: Reflection on prompts for this use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like with our first use case in this chapter, we followed a specific method
    of first setting high-level context of describing our role, the shape of the dataset,
    and what we were looking to do. Then, we followed the below process to first breaking
    down the problem in steps, getting code and how we could continue to improve and
    refine and finally visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step-by-step guidance:** The requests are structured as a series of step-by-step
    tasks, breaking down the larger problem into manageable components. This makes
    it easier to follow and implement the solution incrementally.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Specify inputs and requirements:** Provide clear and specific information
    about the dataset, its columns, and the requirements for preprocessing, clustering,
    and visualization. This helps ensure that the assistance received is tailored
    to the particular needs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Request for code with comments:** Code snippets request to include comments
    to explain each step and clarify the purpose of the code. This helps in understanding
    the code and code validation and facilitates learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iteration and refinement:** Iteration of prompts, asking for additional modifications
    and visualizations after the initial guidance. This iterative approach allows
    for refining and improving the solution progressively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Visualization and interpretation:** Visualizing and interpreting the results
    allows focusing on deriving meaningful insights from the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assignment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we used traditional embedding with TF-IDF to transform
    text data into numerical representations, which can then be used for various **natural
    language processing** (**NLP**) tasks such as clustering. Let’s now try and improve
    the clustering results by using a more advanced embedding technique. We will use
    the Hugging Face Transformers library to get pre-trained embeddings for our product
    descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Ask ChatGPT to explain Hugging Face Transformers’ advantages over TF-IDF vectorization
    for clustering use cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use ChatGPT to generate and create product clusters using Hugging Face Transformers
    embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the results with the previous clustering results using TF-IDF vectorization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'See the solution in the repository: https://github.com/PacktPublishing/AI-Assisted-Software-Development-with-GitHub-Copilot-and-ChatGPT'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focused on clustering and how it could be used to group your data
    into separate areas. Creating these areas made it easier to understand our data
    points. Through visualization like heat maps, word clouds, and more, you were
    given the insight that data benefits from being shown in different ways. You also
    saw how the clustering process helped identify outliers, that is, data that vastly
    differs and can’t easily be assigned to any one cluster. For the ChatGPT and prompting
    part, you saw how setting a high-level context describing the dataset helped generate
    a suitable set of steps you could follow from top to bottom. The same high-level
    context also helped ChatGPT recommend a clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/aicode](https://packt.link/aicode)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code510410532445718281.png)'
  prefs: []
  type: TYPE_IMG
