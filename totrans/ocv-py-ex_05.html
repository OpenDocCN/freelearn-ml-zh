<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Extracting Features from an Image</h1></div></div></div><p>In this chapter, we are going to learn how to detect salient points, also known as keypoints, in an image. We will discuss why these keypoints are important and how we can use them to understand the image content. We will talk about different techniques that can be used to detect these keypoints, and understand how we can extract features from a given image.</p><p>By the end of this chapter, you will know:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What are keypoints and why do we care about them</li><li class="listitem" style="list-style-type: disc">How to detect keypoints</li><li class="listitem" style="list-style-type: disc">How to use keypoints for image content analysis</li><li class="listitem" style="list-style-type: disc">The different techniques to detect keypoints</li><li class="listitem" style="list-style-type: disc">How to build a feature extractor</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec46"/>Why do we care about keypoints?</h1></div></div></div><p>Image content analysis<a id="id174" class="indexterm"/> refers <a id="id175" class="indexterm"/>to the process of understanding the content of an image so that we can take some action based on that. Let's take a step back and talk about how humans do it. Our brain is an extremely powerful machine that can do complicated things very quickly. When we look at something, our brain automatically creates a footprint based on the "interesting" aspects of that image. We will discuss what interesting means as we move along this chapter. </p><p>For now, an interesting aspect is something that's distinct in that region. If we call a point interesting, then there shouldn't be another point in its neighborhood that satisfies the constraints. Let's consider the following image:</p><div><img src="img/B04554_05_01.jpg" alt="Why do we care about keypoints?"/></div><p>Now close your<a id="id176" class="indexterm"/> eyes and try to visualize this image. Do you see something specific? Can you recollect what's in the left half of the image? Not really! The reason for this is that the image doesn't have any interesting information. When our brain looks at something like this, there's nothing to make note of. So it tends to wander around! Let's take a look at the following image:</p><div><img src="img/B04554_05_02.jpg" alt="Why do we care about keypoints?"/></div><p>Now close your eyes and try to visualize this image. You will see that the recollection is vivid and you remember a lot of details about this image. The reason for this is that there are a lot of interesting regions in the image. The human eye is more sensitive to high frequency content as compared to low frequency content. This is the reason we tend to recollect the second image better than the first one. To further demonstrate this, let's look at the following image:</p><div><img src="img/B04554_05_03.jpg" alt="Why do we care about keypoints?"/></div><p>If you notice, your <a id="id177" class="indexterm"/>eye immediately went to the TV remote, even though it's not at the center of the image. We automatically tend to gravitate towards the interesting regions in the image because that is where all the information is. This is what our brain needs to store in order to recollect it later.</p><p>When we build object recognition systems, we need to detect these "interesting" regions to create a signature for the image. These interesting regions are characterized by keypoints. This is why keypoint detection is critical in many modern computer vision systems.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec47"/>What are keypoints?</h1></div></div></div><p>Now that we know that <a id="id178" class="indexterm"/>keypoints refer to the interesting regions in the image, let's dig a little deeper. What are keypoints made of? Where are these points? When we say "interesting", it means that something is happening in that region. If the region is just uniform, then it's not very interesting. For example, corners are interesting because there is sharp change in intensity in two different directions. Each corner is a unique point where two edges meet. If you look at the preceding images, you will see that the interesting regions are not completely made up of "interesting" content. If you look closely, we can still see plain regions within busy regions. For example, consider the following image:</p><div><img src="img/B04554_05_04.jpg" alt="What are keypoints?"/></div><p>If you look at the preceding object, the interior parts of the interesting regions are "uninteresting".</p><div><img src="img/B04554_05_05.jpg" alt="What are keypoints?"/></div><p>So, if we were to<a id="id179" class="indexterm"/> characterize this object, we would need to make sure that we picked the interesting points. Now, how do we define "interesting points"? Can we just say that anything that's not uninteresting can be an interesting point? Let's consider the following example:</p><div><img src="img/B04554_05_06.jpg" alt="What are keypoints?"/></div><p>Now, we can <a id="id180" class="indexterm"/>see that there is a lot of high frequency content in this image along the edge. But we cannot call the whole edge "interesting". It is important to understand that "interesting" doesn't necessarily refer to color or intensity values. It can be anything, as long as it is distinct. We need to isolate the points that are unique in their neighborhood. The points along the edge are not unique with respect to their neighbors. So, now that we know what we are looking for, how do we pick an interesting point?</p><p>What about the corner of the table? That's pretty interesting, right? It's unique with respect to its neighbors and we don't have anything like that in its vicinity. Now this point can be chosen as one of our keypoints. We take a bunch of these keypoints to characterize a particular image.</p><p>When we do image analysis, we need to convert it into a numerical form before we deduce something. These keypoints are represented using a numerical form and a combination of these keypoints is then used to create the image signature. We want this image signature to represent a given image in the best possible way.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec48"/>Detecting the corners</h1></div></div></div><p>Since we know that the<a id="id181" class="indexterm"/> corners are "interesting", let's see how we can detect them. In computer vision, there is a popular corner detection technique called<a id="id182" class="indexterm"/> <strong>Harris Corner Detector</strong>. We basically construct a 2x2 matrix based on partial derivatives of the grayscale image, and then analyze the eigenvalues. This is actually an oversimplification of the actual algorithm, but it covers the gist. So, if you want to understand the underlying mathematical details, you can look into the original paper by <a id="id183" class="indexterm"/>Harris and Stephens at <a class="ulink" href="http://www.bmva.org/bmvc/1988/avc-88-023.pdf">http://www.bmva.org/bmvc/1988/avc-88-023.pdf</a>. A corner point <a id="id184" class="indexterm"/>is a point where both the eigenvalues would have large values.</p><p>Let's consider the following image:</p><div><img src="img/B04554_05_07.jpg" alt="Detecting the corners"/></div><p>If you run the Harris<a id="id185" class="indexterm"/> corner detector on this image, you will see something like this:</p><div><img src="img/B04554_05_08.jpg" alt="Detecting the corners"/></div><p>As you can see, all the black dots correspond to the corners in the image. If you notice, the corners at the bottom of the box are not detected. The reason for this is that the corners are not sharp enough. You can adjust the thresholds in the corner detector to identify these corners. The <a id="id186" class="indexterm"/>code to do this is as follows:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('box.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

gray = np.float32(gray)

dst = cv2.cornerHarris(gray, 4,5, 0.04)      # to detect only sharp corners
#dst = cv2.cornerHarris(gray, 14, 5, 0.04)    # to detect soft corners

# Result is dilated for marking the corners
dst = cv2.dilate(dst,None)

# Threshold for an optimal value, it may vary depending on the image.
img[dst &gt; 0.01*dst.max()] = [0,0,0]

cv2.imshow('Harris Corners',img)
cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec49"/>Good Features To Track</h1></div></div></div><p>Harris corner detector performs well in many cases, but it misses out on a few things. Around six years after the original paper by Harris and Stephens, Shi-Tomasi came up with a better corner detector. You can read the original paper at <a class="ulink" href="http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf">http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf</a>. They used a different scoring function to improve the overall quality. Using this method, we can find the 'N' strongest corners in the given image. This is very useful when we don't want to use every single corner to extract information from the image.</p><p>If you apply the Shi-Tomasi corner detector<a id="id187" class="indexterm"/> to the image shown earlier, you will see something like this:</p><div><img src="img/B04554_05_09.jpg" alt="Good Features To Track"/></div><p>Following <a id="id188" class="indexterm"/>is the code:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('box.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

corners = cv2.goodFeaturesToTrack(gray, 7, 0.05, 25)
corners = np.float32(corners)

for item in corners:
    x, y = item[0]
    cv2.circle(img, (x,y), 5, 255, -1)

cv2.imshow("Top 'k' features", img)
cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec50"/>Scale Invariant Feature Transform (SIFT)</h1></div></div></div><p>Even though corner <a id="id189" class="indexterm"/>features are "interesting", they are not good enough to characterize the truly interesting parts. When we talk about image content analysis, we want the image signature to be invariant to things such as scale, rotation, illumination, and so on. Humans are very good at these things. Even if I show you an image of an apple upside down that's dimmed, you will still recognize it. If I show you a really enlarged version of that image, you will still recognize it. We want our image recognition systems to be able to do the same.</p><p>Let's consider the corner features. If you enlarge an image, a corner might stop being a corner as shown below.</p><div><img src="img/B04554_05_10.jpg" alt="Scale Invariant Feature Transform (SIFT)"/></div><p>In the second case, the detector will not pick up this corner. And, since it was picked up in the original image, the second image will not be matched with the first one. It's basically the same image, but the corner features based method will totally miss it. This means that corner detector is not exactly scale invariant. This is why we need a better method to characterize an image.</p><p>SIFT is one of the most popular<a id="id190" class="indexterm"/> algorithms in all of computer vision. You can read David Lowe's original paper at <a class="ulink" href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf</a>. We can use this algorithm to extract keypoints and build the corresponding feature descriptors. There is a lot of good documentation available online, so we will keep our discussion brief. To identify a potential keypoint, SIFT builds a pyramid by downsampling an image and taking the difference of Gaussian. This means that we run a Gaussian filter at each level and take the difference to build the successive levels in the pyramid. In order to see if the current point is a keypoint, it looks at the neighbors as well as the pixels at the same location in neighboring levels of the pyramid. If it's a maxima, then the current point is picked up as a keypoint. This ensures that we keep the keypoints scale invariant.</p><p>Now that we know how it achieves scale invariance, let's see how it achieves rotation invariance. Once we identify the keypoints, each keypoint is assigned an orientation. We take the neighborhood around each keypoint and compute the gradient magnitude and direction. This gives us a sense of the direction of that keypoint. If we have this information, we will be able to match this keypoint to the same point in another image even if it's rotated. Since we know the orientation, we will be able to normalize those keypoints before making the comparisons.</p><p>Once we have all this<a id="id191" class="indexterm"/> information, how do we quantify it? We need to convert it to a set of numbers so that we can do some kind of matching on it. To achieve this, we just take the 16x16 neighborhood around each keypoint, and divide it into 16 blocks of size 4x4. For each block, we compute the orientation histogram with 8 bins. So, we have a vector of length 8 associated with each block, which means that the neighborhood is represented by a vector of size 128 (8x16). This is the final keypoint descriptor that will be used. If we extract <code class="literal">N</code> keypoints from an image, then we will have <code class="literal">N</code> descriptors of length 128 each. This array of <code class="literal">N</code> descriptors characterizes the given image.</p><p>Consider the following image:</p><div><img src="img/B04554_05_11.jpg" alt="Scale Invariant Feature Transform (SIFT)"/></div><p>If you extract the <a id="id192" class="indexterm"/>keypoint locations using SIFT, you will see something like the following, where the size of the circle indicates the strength of the keypoints, and the line inside the circle indicates the orientation:</p><div><img src="img/B04554_05_12.jpg" alt="Scale Invariant Feature Transform (SIFT)"/></div><p>Before we <a id="id193" class="indexterm"/>look at the code, it is important to know that SIFT is patented and it's not freely available for commercial use. Following is the code to do it:</p><div><pre class="programlisting">import cv2
import numpy as np

input_image = cv2.imread('input.jpg')
gray_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)

sift = cv2.SIFT()
keypoints = sift.detect(gray_image, None)

input_image = cv2.drawKeypoints(input_image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

cv2.imshow('SIFT features', input_image)
cv2.waitKey()</pre></div><p>We can also <a id="id194" class="indexterm"/>compute the descriptors. OpenCV lets us do it separately or we can combine the detection and computation parts in the same step by using the following:</p><div><pre class="programlisting">keypoints, descriptors = sift.detectAndCompute(gray_image, None)</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec51"/>Speeded Up Robust Features (SURF)</h1></div></div></div><p>Even though SIFT is nice <a id="id195" class="indexterm"/>and useful, it's computationally intensive. This means that it's slow and we will have a hard time implementing a real-time system if it uses SIFT. We need a system that's fast and has all the advantages of SIFT. If you remember, SIFT uses the difference of Gaussian to build the pyramid and this process is slow. So, to overcome this, SURF uses a simple box filter to approximate the Gaussian. The good thing is that this is really easy to compute and it's reasonably fast. There's a lot <a id="id196" class="indexterm"/>of documentation available online on SURF at <a class="ulink" href="http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf">http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf</a>. So, you can go through it to see how they construct a descriptor. You can refer to the original paper at <a class="ulink" href="http://www.vision.ee.ethz.ch/~surf/eccv06.pdf">http://www.vision.ee.ethz.ch/~surf/eccv06.pdf</a>. It is important to know that SURF is also patented and it is not freely available for commercial use.</p><p>If you run the SURF keypoint detector on the earlier image, you will see something like the following one:</p><div><img src="img/B04554_05_13.jpg" alt="Speeded Up Robust Features (SURF)"/></div><p>Here is the <a id="id197" class="indexterm"/>code:</p><div><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('input.jpg')
gray= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

surf = cv2.SURF()

# This threshold controls the number of keypoints
surf.hessianThreshold = 15000

kp, des = surf.detectAndCompute(gray, None)

img = cv2.drawKeypoints(img, kp, None, (0,255,0), 4)

cv2.imshow('SURF features', img)
cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec52"/>Features from Accelerated Segment Test (FAST)</h1></div></div></div><p>Even though SURF is faster than SIFT, it's just not fast enough for a real-time system, especially when there are resource constraints. When you are building a real-time application on a mobile device, you won't have the luxury of using SURF to do computations in real time. We need something that's really fast and computationally inexpensive. Hence, Rosten and Drummond came up with FAST. As the name indicates, it's really fast!</p><p>Instead of going<a id="id198" class="indexterm"/> through all the expensive calculations, they came up with a high-speed test to quickly determine if the current point is a potential keypoint. We need to note that FAST is just for keypoint detection. Once keypoints are detected, we need to use SIFT or SURF to compute the descriptors. Consider the following image:</p><div><img src="img/B04554_05_14.jpg" alt="Features from Accelerated Segment Test (FAST)"/></div><p>If we run the FAST keypoint detector on this image, you will see something like this:</p><div><img src="img/B04554_05_15.jpg" alt="Features from Accelerated Segment Test (FAST)"/></div><p>If we clean it up and<a id="id199" class="indexterm"/> suppress the unimportant keypoints, it will look like this:</p><div><img src="img/B04554_05_16.jpg" alt="Features from Accelerated Segment Test (FAST)"/></div><p>Following is the<a id="id200" class="indexterm"/> code for this:</p><div><pre class="programlisting">import cv2
import numpy as np

gray_image = cv2.imread('input.jpg', 0)

fast = cv2.FastFeatureDetector()

# Detect keypoints
keypoints = fast.detect(gray_image, None)
print "Number of keypoints with non max suppression:", len(keypoints)

# Draw keypoints on top of the input image
img_keypoints_with_nonmax = cv2.drawKeypoints(gray_image, keypoints, color=(0,255,0))
cv2.imshow('FAST keypoints - with non max suppression', img_keypoints_with_nonmax)

# Disable nonmaxSuppression
fast.setBool('nonmaxSuppression', False)

# Detect keypoints again
keypoints = fast.detect(gray_image, None)

print "Total Keypoints without nonmaxSuppression:", len(keypoints)

# Draw keypoints on top of the input image
img_keypoints_without_nonmax = cv2.drawKeypoints(gray_image, keypoints, color=(0,255,0))
cv2.imshow('FAST keypoints - without non max suppression', img_keypoints_without_nonmax)
cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec53"/>Binary Robust Independent Elementary Features (BRIEF)</h1></div></div></div><p>Even though we have <a id="id201" class="indexterm"/>FAST to quickly detect the keypoints, we still have to use SIFT or SURF to compute the descriptors. We need a way to quickly compute the descriptors as well. This is where BRIEF comes into the picture. BRIEF is a method for extracting feature descriptors. It cannot detect the keypoints by itself, so we need to use it in conjunction with a keypoint detector. The good thing about BRIEF is that it's compact and fast.</p><p>Consider the following image:</p><div><img src="img/B04554_05_17.jpg" alt="Binary Robust Independent Elementary Features (BRIEF)"/></div><p>BRIEF takes the <a id="id202" class="indexterm"/>list of input keypoints and outputs an updated list. So if you run BRIEF on this image, you will see something like this:</p><div><img src="img/B04554_05_18.jpg" alt="Binary Robust Independent Elementary Features (BRIEF)"/></div><p>Following is the code:</p><div><pre class="programlisting">import cv2 
import numpy as np

gray_image = cv2.imread('input.jpg', 0)

# Initiate FAST detector
fast = cv2.FastFeatureDetector()

# Initiate BRIEF extractor
brief = cv2.DescriptorExtractor_create("BRIEF")

# find the keypoints with STAR
keypoints = fast.detect(gray_image, None)

# compute the descriptors with BRIEF
keypoints, descriptors = brief.compute(gray_image, keypoints)

gray_keypoints = cv2.drawKeypoints(gray_image, keypoints, color=(0,255,0))
cv2.imshow('BRIEF keypoints', gray_keypoints)
cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec54"/>Oriented FAST and Rotated BRIEF (ORB)</h1></div></div></div><p>So, now we have arrived <a id="id203" class="indexterm"/>at the best combination out of all the combinations that we have discussed so far. This algorithm came out of the OpenCV Labs. It's fast, robust, and open-source! Both SIFT and SURF algorithms are patented and you can't use them for commercial purposes. This is why ORB is good in many ways.</p><p>If you run the ORB keypoint extractor on one of the images shown earlier, you will see something like the following:</p><div><img src="img/B04554_05_19.jpg" alt="Oriented FAST and Rotated BRIEF (ORB)"/></div><p>Here is the <a id="id204" class="indexterm"/>code:</p><div><pre class="programlisting">import cv2
import numpy as np

input_image = cv2.imread('input.jpg')
gray_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)

# Initiate ORB object
orb = cv2.ORB()

# find the keypoints with ORB
keypoints = orb.detect(gray_image, None)

# compute the descriptors with ORB
keypoints, descriptors = orb.compute(gray_image, keypoints)

# draw only the location of the keypoints without size or orientation
final_keypoints = cv2.drawKeypoints(input_image, keypoints, color=(0,255,0), flags=0)

cv2.imshow('ORB keypoints', final_keypoints)
cv2.waitKey()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec55"/>Summary</h1></div></div></div><p>In this chapter, we learned about the importance of keypoints and why we need them. We discussed various algorithms to detect keypoints and compute feature descriptors. We will be using these algorithms in all the subsequent chapters in various different contexts. The concept of keypoints is central to computer vision, and plays an important role in many modern systems.</p><p>In the next chapter, we are going to discuss how to stitch multiple images of the same scene together to create a panoramic image.</p></div></div>
</body></html>