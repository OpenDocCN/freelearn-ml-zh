<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Blend It with Stacking</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">In this chapter, we will cover the following recipes:</span></p>
<ul class="calibre10">
<li class="calibre11">Understanding stacked generalization</li>
<li class="calibre11">Implementing <span>stacked generalization</span> by combining the predictions</li>
<li class="calibre11"><span>Implementing </span><span>stacked generalization</span><span> for marketing campaign outcome prediction using H2O</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2">The technical requirements for this chapter remain the same as those we detailed in earlier chapters.</p>
<p class="calibre2">Visit the<span class="calibre5"> </span>GitHub repository to find the dataset and the code. The datasets and code files are arranged according to chapter numbers, and by the name of the topic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding stacked generalization</h1>
                </header>
            
            <article>
                
<p class="calibre2">Stacked generalization is an ensemble of a diverse group of models that introduces the concept of a meta-learner. A meta-learner is a second-level machine learning algorithm that learns from an optimal combination of base learners:</p>
<div class="packtquote"><span>"Stacked generalization is a means of non-linearly combining generalizers to make a new generalizer, to try to optimally integrate what each of the original generalizers has to say about the learning set. The more each generalizer has to say (which isn't duplicated in what the other generalizers have to say), the better the resultant stacked generalization." </span>
<div class="CDPAlignRight"><span>- </span><span>Wolpert (1992)<em class="calibre23">,</em> Stacked Generalization</span></div>
</div>
<p class="calibre2"/>
<p class="calibre2">The steps for stacking are as follows:</p>
<ol class="calibre14">
<li class="calibre11">Split your dataset into a training set and a testing set.</li>
<li class="calibre11">Train several base learners on the training set.</li>
<li class="calibre11">Apply the base learners on the testing set to make predictions.</li>
<li class="calibre11">Use the predictions as inputs and the actual responses as outputs to train a higher-level learner.</li>
</ol>
<p class="calibre2">Because the predictions from the base learners are blended together, stacking is also referred to as blending. </p>
<p class="calibre2"><span class="calibre5">The following diagram gives us a conceptual representation of stacking:</span></p>
<p class="CDPAlignCenter"><img class="aligncenter81" src="assets/addebcb4-b1e7-44d6-ad30-af0d0ddefebf.png"/></p>
<p class="calibre2"/>
<p class="calibre2">It's of significance for stack generalization that the predictions from the base learners are not correlated with each other. In order to get uncorrelated predictions from the base learners, algorithms that use different approaches internally may be used to train the base learners. Stacked generalization is used mainly for minimizing the generalization error of the base learners, and can be seen as a refined version of cross-validation. It uses a strategy that's more sophisticated than cross-validation's <strong class="calibre4">winner-takes-all</strong> approach for combining the predictions from the base learners.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing stacked generalization by combining predictions</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we'll look at how to implement stacked generalization from scratch.</p>
<p class="calibre2">We will carry out the following steps to get started:</p>
<ol class="calibre14">
<li class="calibre11">Build three base learners for stacking.</li>
<li class="calibre11">Combine the predictions from each of the base learners.</li>
<li class="calibre11">Build the meta-learner using another algorithm.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this example, we use a dataset from the UCI ML Repository on credit card defaults. This dataset contains information on default payments, demographic factors, credit data, history of payments, and bill statements of credit card clients. The data and the data descriptions are provided in the GitHub.</p>
<p class="calibre2">We will start by loading the required libraries and reading our dataset:</p>
<pre class="calibre15">import os<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>from sklearn.metrics import accuracy_score</pre>
<p class="calibre2">We set our working folder as follows:</p>
<pre class="calibre15"># Set your working directory according to your requirement<br class="title-page-name"/>os.chdir(".../Chapter 8/")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Let's now read our data. We will prefix the DataFrame name with <kbd class="calibre12">df_</kbd> so that we can understand it easily:</p>
<pre class="calibre15">df_creditcarddata = pd.read_csv("UCI_Credit_Card.csv")</pre>
<p class="calibre2"><span class="calibre5">We drop the <kbd class="calibre12">ID</kbd> column, as this isn't required:</span></p>
<pre class="calibre15">df_creditcarddata = df_creditcarddata.drop("ID", axis= 1) </pre>
<p class="calibre2">We check the shape of the dataset:</p>
<pre class="calibre15">df_creditcarddata.shape</pre>
<p class="calibre2">We notice that the dataset now has 30,000 observations and 24 columns. <span class="calibre5">Let's now move on to training our models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it... </h1>
                </header>
            
            <article>
                
<ol class="calibre14">
<li class="calibre11">We split our target and feature variables:</li>
</ol>
<pre class="calibre18">from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/>X = df_creditdata.iloc[:,0:23]<br class="title-page-name"/>Y = df_creditdata['default.payment.next.month']</pre>
<ol start="2" class="calibre14">
<li class="calibre11"><span>Split the data into training, validation, and testing subsets:</span></li>
</ol>
<pre class="calibre18"># We first split the dataset into train and test subset<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)<br class="title-page-name"/><br class="title-page-name"/># Then we take the train subset and carve out a validation set from the same<br class="title-page-name"/>X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)</pre>
<ol start="2" class="calibre14"/>
<ol start="3" class="calibre14">
<li class="calibre11">Check the dimensions of each subset to ensure that our splits are correct:</li>
</ol>
<pre class="calibre18"># Dimensions for train subsets<br class="title-page-name"/>print(X_train.shape)<br class="title-page-name"/>print(Y_train.shape)<br class="title-page-name"/><br class="title-page-name"/># Dimensions for validation subsets<br class="title-page-name"/>print(X_val.shape)<br class="title-page-name"/>print(Y_val.shape)<br class="title-page-name"/><br class="title-page-name"/># Dimensions for test subsets<br class="title-page-name"/>print(X_test.shape)<br class="title-page-name"/>print(Y_test.shape)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">Import the required libraries for the base learners and the meta-learner:</li>
</ol>
<pre class="calibre18"># for the base learners<br class="title-page-name"/>from sklearn.naive_bayes import GaussianNB<br class="title-page-name"/>from sklearn.neighbors import KNeighborsClassifier<br class="title-page-name"/>from sklearn.tree import DecisionTreeClassifier<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># for the meta learner<br class="title-page-name"/>from sklearn.linear_model import LogisticRegression</pre>
<ol start="5" class="calibre14">
<li class="calibre11">Create instances of the base learners and fit the model on our training data:</li>
</ol>
<pre class="calibre18"># The base learners<br class="title-page-name"/>model_1 = GaussianNB()<br class="title-page-name"/>model_2 = KNeighborsClassifier(n_neighbors=1)<br class="title-page-name"/>model_3 = DecisionTreeClassifier()<br class="title-page-name"/><br class="title-page-name"/># Now we train a list of models<br class="title-page-name"/>base_learner_1 = model_1.fit(X_train, Y_train)<br class="title-page-name"/>base_learner_2 = model_2.fit(X_train, Y_train)<br class="title-page-name"/>base_learner_3 = model_3.fit(X_train, Y_train)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">Use the base learners on our validation subset to make predictions:</li>
</ol>
<pre class="calibre18"># We then use the models to make predictions on validation data<br class="title-page-name"/>val_prediction_base_learner_1 = base_learner_1.predict(X_val)<br class="title-page-name"/>val_prediction_base_learner_2 = base_learner_2.predict(X_val)<br class="title-page-name"/>val_prediction_base_learner_3 = base_learner_3.predict(X_val)</pre>
<ol start="7" class="calibre14">
<li class="calibre11">We have three sets of prediction results from three base learners. We use them to create a stacked array:</li>
</ol>
<pre class="calibre18"># And then use the predictions to create a new stacked dataset<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>prediction_test_stack = np.dstack([val_prediction_base_learner_1, val_prediction_base_learner_2, val_prediction_base_learner_3])<br class="title-page-name"/><br class="title-page-name"/># Now we stack the actual outcomes i.e. Y_Test with the prediction_stack<br class="title-page-name"/>final_train_stack = np.dstack([prediction_test_stack, Y_val])</pre>
<ol start="8" class="calibre14">
<li class="calibre11">We convert the <kbd class="calibre12">final_train_stack</kbd> stacked array to a DataFrame and add column names to each of the columns. Verify the dimensions and take a look at the first few rows:</li>
</ol>
<pre class="calibre18">stacked_train_dataframe = pd.DataFrame(final_train_stack[0,0:5400], columns='NB_VAL KNN_VAL DT_VAL Y_VAL'.split())<br class="title-page-name"/><br class="title-page-name"/>print(stacked_train_dataframe.shape)<br class="title-page-name"/>print(stacked_train_dataframe.head(5))</pre>
<p class="calibre20">In the following image, we see that the stacked array now has 5,400 observations and 4 columns:</p>
<p class="CDPAlignCenter"><img class="aligncenter82" src="assets/56917bad-d1a0-46f2-9aef-12025ce62841.png"/></p>
<ol start="9" class="calibre14">
<li class="calibre11">Train the meta-learner using the stacked array that we created in <em class="calibre23">Step 8</em>:</li>
</ol>
<pre class="calibre18"># Build the Mata-learner<br class="title-page-name"/>meta_learner = LogisticRegression()<br class="title-page-name"/>meta_learner_model = meta_learner.fit(stacked_train_dataframe.iloc[:,0:3], stacked_train_dataframe['Y_VAL'])</pre>
<ol start="10" class="calibre14">
<li class="calibre11">Create the stacked test set with the testing subset:</li>
</ol>
<pre class="calibre18"># Take the test data (new data)<br class="title-page-name"/># Apply the base learners on this new data to make predictions<br class="title-page-name"/><br class="title-page-name"/># We now use the models to make predictions on the test data and create a new stacked dataset<br class="title-page-name"/>test_prediction_base_learner_1 = base_learner_1.predict(X_test)<br class="title-page-name"/>test_prediction_base_learner_2 = base_learner_2.predict(X_test)<br class="title-page-name"/>test_prediction_base_learner_3 = base_learner_3.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/># Create the stacked data<br class="title-page-name"/>final_test_stack = np.dstack([test_prediction_base_learner_1, test_prediction_base_learner_2, test_prediction_base_learner_3])</pre>
<p class="calibre2"/>
<ol start="11" class="calibre14">
<li class="calibre11">Convert the<span> </span><kbd class="calibre12">final_test_stack</kbd><span> stacked array </span>to a <span>DataFrame </span>and add column names to each of the columns. Verify the dimensions and take a look at the first few rows:</li>
</ol>
<pre class="calibre18">stacked_test_dataframe = pd.DataFrame(final_test_stack[0,0:3000], columns='NB_TEST KNN_TEST DT_TEST'.split())<br class="title-page-name"/>print(stacked_test_dataframe.shape)<br class="title-page-name"/>print(stacked_test_dataframe.head(5))</pre>
<p class="calibre20">We see that the stacked array now has 3,000 observations and 3 columns in <kbd class="calibre12">stacked_test_dataframe</kbd>:</p>
<p class="CDPAlignCenter"><img class="aligncenter83" src="assets/26e39687-b793-40a5-85a8-4b9d6989a2a0.png"/></p>
<ol start="12" class="calibre14">
<li class="calibre11">Check the accuracy of <kbd class="calibre12">base_learner</kbd> on our original test data:</li>
</ol>
<pre class="calibre18">test_prediction_base_learner_1 = base_learner_1.predict(X_test)<br class="title-page-name"/>test_prediction_base_learner_2 = base_learner_2.predict(X_test)<br class="title-page-name"/>test_prediction_base_learner_3 = base_learner_3.predict(X_test)<br class="title-page-name"/><br class="title-page-name"/>print("Accuracy from GaussianNB:", accuracy_score(Y_test, test_prediction_base_learner_1))<br class="title-page-name"/>print("Accuracy from KNN:", accuracy_score(Y_test, test_prediction_base_learner_2))<br class="title-page-name"/>print("Accuracy from Decision Tree:", accuracy_score(Y_test, test_prediction_base_learner_3))</pre>
<p class="calibre20">We notice that the accuracy is as follows. Note that based on the sampling strategy and hyperparameters, the results may vary:</p>
<p class="CDPAlignCenter"><img class="aligncenter84" src="assets/587145bc-00ad-429b-a493-115eb20bacc3.png"/></p>
<ol start="13" class="calibre14">
<li class="calibre11">Use the meta-learner on the stacked test data and check the accuracy:</li>
</ol>
<pre class="calibre15">test_predictions_meta_learner = meta_learner_model.predict(stacked_test_dataframe)<br class="title-page-name"/>print("Accuracy from Meta Learner:", accuracy_score(Y_test, test_predictions_meta_learner))</pre>
<p class="calibre20">We see the following output returned by the meta-learner applied on the stacked test data. This accuracy is higher than the individual base learners:</p>
<p class="CDPAlignCenter"><img class="aligncenter85" src="assets/23f76ead-80a5-407b-a4cd-ccf09120bfc5.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we split our dataset into target and feature sets. In <em class="calibre13">Step 2</em>, we created our training, validation, and testing subsets. We took a look at the dimensions of each of the subset in <em class="calibre13">Step 3</em> to verify that the splits were done correctly.</p>
<p class="calibre2">We then moved on to building our base learners and the meta-learner. In <em class="calibre13">Step 4</em>, we imported the required libraries for the base learners and the meta-learner. For the base learners, we used Gaussian Naive Bayes, KNN, and a decision tree, while for the meta-learner we used logistic regression.</p>
<p class="calibre2">In <em class="calibre13">Step 5</em>, we fitted the base learners to our train dataset. <span class="calibre5">Single models, including Gaussian Naive Bayes, KNN, and a decision tree,</span><span class="calibre5"> are established in the level 0 space. </span>We then had three base models.</p>
<p class="calibre2">In <em class="calibre13">Step 6</em>, we used these three base models on our validation subset to predict the target variable. We then had three sets of predictions given by the respective base learners.</p>
<p class="calibre2"><span class="calibre5">Now </span><span class="calibre5">the base learners will be integrated by logistic regression in the level 1 space via stacked generalization. In <em class="calibre13">Step 7</em>, </span><span class="calibre5">we stacked the three sets of predicted values to create an array. We also stacked the actual target variable of our training dataset to the array. We then had four columns in our array: three columns from the three sets of predicted values of the base learners and a fourth column from the target variable of our training dataset. We called </span>it <span class="calibre5"><kbd class="calibre12">final_train_stack</kbd> </span><span class="calibre5">known as</span> <span class="calibre5"><kbd class="calibre12">stacked_train_dataframe</kbd>, </span><span class="calibre5">and we named the columns according to the algorithm used for the base learners. In our case, we used the names </span><kbd class="calibre12">NB_VAL</kbd><span class="calibre5">,</span> <kbd class="calibre12">KNN_VAL</kbd><span class="calibre5">, and</span> <kbd class="calibre12">DT_VAL</kbd> <span class="calibre5">since we used Gaussian Naive Bayes, KNN, and a decision tree classifier, respectively. Because the base learners are fitted to our validation subset, we suffixed the column names with</span> <kbd class="calibre12">_VAL</kbd><span class="calibre5"> to make them easier to understand.</span></p>
<p class="calibre2">In <em class="calibre13">Step 9</em>, we built the meta-learner with logistic regression and fitted it to our stacked dataset, <span class="calibre5"><kbd class="calibre12">stacked_train_dataframe</kbd>. N</span>otice that we moved away from our original dataset to a stacked dataset, which contains the predicted values from our base learners.</p>
<p class="calibre2"/>
<p class="calibre2">In <em class="calibre13">Step 10</em>, we used the base models on our test subset to get the predicted results. We called it <span class="calibre5"><kbd class="calibre12">final_test_stack</kbd>. In <em class="calibre13">Step 11</em>, we converted the <kbd class="calibre12">final_test_stack</kbd> array to a DataFrame called </span><span class="calibre5"><kbd class="calibre12">stacked_test_dataframe</kbd>. Note that in our <kbd class="calibre12">stacked_test_dataframe</kbd>, we only had three columns, which held the predicted values returned by the base learners applied on our test subset. The three columns were named after the algorithm used, suffixed with <kbd class="calibre12">_TEST</kbd>, so we have </span><span class="calibre5"><kbd class="calibre12">NB_TEST</kbd>, <kbd class="calibre12">KNN_TEST</kbd>, and <kbd class="calibre12">DT_TEST</kbd> as the three columns in <kbd class="calibre12">stacked_test_dataframe</kbd>.</span></p>
<p class="calibre2">In <em class="calibre13">Step 12</em>, we checked the accuracy of the base models on our original test subset. The Gaussian Naive Bayes, KNN, and decision tree classifier models gave us accuracy ratings of 0.39, 0.69, and 0.73, respectively.</p>
<p class="calibre2">In <em class="calibre13">Step 13</em>, we checked the accuracy that we get by applying the meta-learner model on our stacked test data. This gave us an accuracy of 0.77, which we can see is higher than the individual base learners. However, bear in mind <span class="calibre5">that simply adding more base learners to your stacking algorithm doesn't guarantee that you'll get better accuracy.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Creating a stacking model can be tedious. The </span><kbd class="calibre12">mlxtend</kbd> library provides tools that simplify building the stacking model. It provides StackingClassifier, which is <span class="calibre5">the ensemble-learning meta-classifier for stacking, and it also provides StackingCVClassifier, which uses cross-validation to prepare the input for the second level meta-learner to prevent overfitting.</span></p>
<p class="calibre2">You can download the library from <a href="https://pypi.org/project/mlxtend/" class="calibre9">https://pypi.org/project/mlxtend/</a> or use the <span class="calibre5"><kbd class="calibre12">pip install mlxtend</kbd> command to install it. You can find some great examples of simple stacked classification and stacked classification with grid search at <a href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/" class="calibre9">http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="calibre2">You can also take a look at the <kbd class="calibre12">ML-Ensemble</kbd> library. To find out more about <kbd class="calibre12">ML-Ensemble</kbd>, visit <a href="http://ml-ensemble.com/" class="calibre9">http://ml-ensemble.com/</a>. A guide to using <kbd class="calibre12">ML-Ensemble</kbd> is available at <a href="https://bit.ly/2GFsxJN" class="calibre9">https://bit.ly/2GFsxJN</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing stacked generalization for campaign outcome prediction using H2O</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">H2O is an open source platform for building machine learning and predictive analytics models. The algorithms are written on H2O's distributed map-reduce framework. With H2O, the data is distributed across nodes, read in parallel, and stored in the memory in a compressed manner. This makes H2O extremely fast.</span></p>
<p class="calibre2"><span class="calibre5">H2O's stacked ensemble method is an ensemble machine learning algorithm for supervised problems that finds the optimal combination of a collection of predictive algorithms using stacking. H2O's stacked ensemble supports regression, binary classification, and multiclass classification.</span></p>
<p class="calibre2">In this example, we'll take a look at how to use H2O's stacked ensemble to build a stacking model. We'll use the bank marketing dataset which is available in the Github.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready...</h1>
                </header>
            
            <article>
                
<p class="calibre2">First, import the <kbd class="calibre12">h2o</kbd> library and other modules from H2O:</p>
<pre class="calibre18">import h2o<br class="title-page-name"/>from h2o.estimators.random_forest import H2ORandomForestEstimator<br class="title-page-name"/>from h2o.estimators.gbm import H2OGradientBoostingEstimator<br class="title-page-name"/>from h2o.estimators.glm import H2OGeneralizedLinearEstimator<br class="title-page-name"/>from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator<br class="title-page-name"/>from h2o.grid.grid_search import H2OGridSearch</pre>
<p class="calibre2">Initialize the <kbd class="calibre12">h2o</kbd> instance using the <kbd class="calibre12">init()</kbd> function:</p>
<pre class="calibre18">h2o.init()</pre>
<p class="calibre2">Once we run the preceding code, the <kbd class="calibre12">h2o</kbd> instance gets initialized and we will see the following output:</p>
<p class="CDPAlignCenter"><img class="aligncenter86" src="assets/ecafcf23-ff57-404b-8924-0eb395f3964f.png"/></p>
<p class="calibre2">Now that we have instantiated an <kbd class="calibre12">H2O</kbd> instance, we move onto reading our dataset and building stacking models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol start="1" class="calibre14">
<li class="calibre11">We read our data using the <kbd class="calibre12">h2o.import_file()</kbd> function. We pass the filename to the function as the parameter:</li>
</ol>
<pre class="calibre18">df_bankdata = h2o.import_file("bank-full.csv")</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We split our data into training and testing subsets:</li>
</ol>
<pre class="calibre18"># split into train and validation sets<br class="title-page-name"/>train, test = df_bankdata.split_frame(ratios = [.8], seed = 1234)</pre>
<p class="calibre2"/>
<ol start="3" class="calibre14">
<li class="calibre11">We check the dimensions of the training and testing subsets to verify that the splits are OK:</li>
</ol>
<pre class="calibre18">train.shape, test.shape</pre>
<ol start="4" class="calibre14">
<li class="calibre11">We take a look at the first few rows to ensure that data is loaded correctly:</li>
</ol>
<pre class="calibre18">df_bankdata.head()</pre>
<ol start="5" class="calibre14">
<li class="calibre11">We separate the target and predictor column names, which are the <kbd class="calibre12">response</kbd> and <kbd class="calibre12">predictors</kbd>, respectively:</li>
</ol>
<pre class="calibre18"># Set the predictor names <br class="title-page-name"/>predictors = train.columns<br class="title-page-name"/><br class="title-page-name"/># Set the response column name<br class="title-page-name"/>response = "y"<br class="title-page-name"/><br class="title-page-name"/># Remove the 'y' variable from the predictors<br class="title-page-name"/>predictors.remove(response)<br class="title-page-name"/><br class="title-page-name"/>print(predictors)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">We convert the <kbd class="calibre12">response</kbd> variable to a categorical type with the <kbd class="calibre12">asfactor()</kbd> function:</li>
</ol>
<pre class="calibre18">train[response] = train[response].asfactor()<br class="title-page-name"/>test[response] = test[response].asfactor()</pre>
<ol start="7" class="calibre14">
<li class="calibre11">We will train our base learners using cross-validation. We set the <kbd class="calibre12">nfolds</kbd> value to <kbd class="calibre12">5</kbd>.We also set a variable 'encoding' to 'OneHotExplicit'. We will use this variable to encode our categorical variables.</li>
</ol>
<pre class="calibre18"># Number of CV folds<br class="title-page-name"/>nfolds = 5<br class="title-page-name"/><br class="title-page-name"/># Using the `categorical_encoding` parameter<br class="title-page-name"/>encoding = "OneHotExplicit"</pre>
<ol start="8" class="calibre14">
<li class="calibre11">We start training our base learners. We choose the Gradient Boosting Machine algorithm to build our first base learner:</li>
</ol>
<pre class="calibre18"># Train and cross-validate a GBM<br class="title-page-name"/>base_learner_gbm = H2OGradientBoostingEstimator(distribution="bernoulli",\<br class="title-page-name"/>                                                ntrees=100,\<br class="title-page-name"/>                                                max_depth=5,\<br class="title-page-name"/>                                                min_rows=2,\<br class="title-page-name"/>                                                learn_rate=0.01,\<br class="title-page-name"/>                                                nfolds=nfolds,\<br class="title-page-name"/>                                                fold_assignment="Modulo",\<br class="title-page-name"/>                                                categorical_encoding = encoding,\<br class="title-page-name"/>                                                keep_cross_validation_predictions=True)<br class="title-page-name"/><br class="title-page-name"/>base_learner_gbm.train(x=predictors, y=response, training_frame=train)</pre>
<ol start="9" class="calibre14">
<li class="calibre11">For our second base learner, we use a Random Forest:</li>
</ol>
<pre class="calibre18"># Train and cross-validate a RF<br class="title-page-name"/>base_learner_rf = H2ORandomForestEstimator(ntrees=250,\<br class="title-page-name"/>                                           nfolds=nfolds,\<br class="title-page-name"/>                                           fold_assignment="Modulo",\<br class="title-page-name"/>                                           categorical_encoding = encoding,\<br class="title-page-name"/>                                           keep_cross_validation_predictions=True)<br class="title-page-name"/>base_learner_rf.train(x=predictors, y=response, training_frame=train)</pre>
<ol start="10" class="calibre14">
<li class="calibre11">For our third base learner, we implement a <span><strong class="calibre1">Generalized Linear Model</strong> (<strong class="calibre1">GLM</strong>)</span>:</li>
</ol>
<pre class="calibre18"># Train and cross-validate a GLM<br class="title-page-name"/>base_learner_glm = H2OGeneralizedLinearEstimator(family="binomial",\<br class="title-page-name"/>                                                 model_id="GLM",\<br class="title-page-name"/>                                                 lambda_search=True,\<br class="title-page-name"/>                                                 nfolds = nfolds,\<br class="title-page-name"/>                                                 fold_assignment = "Modulo",\<br class="title-page-name"/>                                                 keep_cross_validation_predictions = True)<br class="title-page-name"/>                                                <br class="title-page-name"/><br class="title-page-name"/>base_learner_glm.train(x = predictors, y = response,training_frame = train)</pre>
<ol start="11" class="calibre14">
<li class="calibre11">Get the best-performing base learner on the test set in terms of the <kbd class="calibre12">test AUC</kbd>. Compare this with the <kbd class="calibre12">test AUC</kbd> of the stacked ensemble model:</li>
</ol>
<pre class="calibre18"># Compare to base learner performance on the test set<br class="title-page-name"/>gbm_test_performance = base_learner_gbm.model_performance(test)<br class="title-page-name"/>rf_test_performance = base_learner_rf.model_performance(test)<br class="title-page-name"/>glm_test_performance = base_learner_glm.model_performance(test)<br class="title-page-name"/><br class="title-page-name"/>print("Best AUC from the GBM", gbm_test_performance.auc())<br class="title-page-name"/>print("Best AUC from the Random Forest", rf_test_performance.auc())<br class="title-page-name"/>print("Best AUC from the GLM", glm_test_performance.auc())<br class="title-page-name"/><br class="title-page-name"/>baselearner_best_auc_test = max(gbm_test_performance.auc(), rf_test_performance.auc(), glm_test_performance.auc())<br class="title-page-name"/>print("Best AUC from the base learners", baselearner_best_auc_test)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>stack_auc_test = perf_stack_test.auc()<br class="title-page-name"/>print("Best Base-learner Test AUC: ", baselearner_best_auc_test)<br class="title-page-name"/>print("Ensemble Test AUC: ", stack_auc_test)</pre>
<ol start="12" class="calibre14">
<li class="calibre11">We train a stacked ensemble using the base learners we built in the preceding steps:</li>
</ol>
<pre class="calibre18">all_models = [base_learner_glm, base_learner_gbm, base_learner_rf]<br class="title-page-name"/><br class="title-page-name"/># Set up Stacked Ensemble. Using Deep Learning as the meta learner<br class="title-page-name"/>ensemble_deep = H2OStackedEnsembleEstimator(model_id ="stack_model_d", base_models = all_models, metalearner_algorithm = 'deeplearning')<br class="title-page-name"/><br class="title-page-name"/>ensemble_deep.train(y = response, training_frame = train)<br class="title-page-name"/><br class="title-page-name"/># Eval ensemble performance on the test data<br class="title-page-name"/>perf_stack_test = ensemble_deep.model_performance(test)<br class="title-page-name"/>stack_auc_test = perf_stack_test.auc()<br class="title-page-name"/>print("Ensemble_deep Test AUC: {0}".format(stack_auc_test))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we used the <kbd class="calibre12">h2o.import_file()</kbd> function to read our dataset. </p>
<div class="packtinfobox">The <kbd class="calibre19">h2o.import_file()</kbd> function returns an <span><kbd class="calibre19">H2OFrame</kbd> instance.</span></div>
<p class="calibre2">In <em class="calibre13">Step 2</em>, we split our <kbd class="calibre12">H2OFrame</kbd> into training and testing subsets. In <em class="calibre13">Step 3</em>, we checked the dimensions of these subsets to verify that our split is adequate for our requirements.</p>
<p class="calibre2">In <em class="calibre13">Step 4</em>, we took a look at the first few rows to check if the data is correctly loaded. In <em class="calibre13">Step</em> <em class="calibre13">5</em>, we separated out the column names of our response and predictor variables, and in <em class="calibre13">Step 6</em>, we converted the response variables into a categorical type with the <kbd class="calibre12">asfactor()</kbd> function.</p>
<p class="calibre2"/>
<p class="calibre2">We defined a variable called <kbd class="calibre12">nfolds</kbd> in <em class="calibre13">Step 7</em>, which we used for cross-validation. We have also defined a variable <kbd class="calibre12">encoding</kbd> which we used in the next steps to instruct H2O to use one-hot encoding for categorical variables. In <em class="calibre13">Step 8</em> to <em class="calibre13"><span class="calibre5">Step </span>10</em>, we built our base learners. </p>
<p class="calibre2"><span class="calibre5">In </span><em class="calibre13">Step<span class="calibre5"> 11</span></em><span class="calibre5">, we trained a </span><span class="calibre5">Gradient Boosting Machine model. </span>We passed some values to a few hyperparameters as follows:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">nfolds</kbd>: <span>Number of folds for K-fold cross-validation.</span></li>
<li class="calibre11"><kbd class="calibre12">fold_assignment</kbd>: <span>This option specifies the scheme to use for cross-validation</span> <span>fold assignment. This option is only applicable if a value for <kbd class="calibre12">nfolds</kbd> is specified and a <kbd class="calibre12">fold_column</kbd> isn't specified. </span></li>
<li class="calibre11"><kbd class="calibre12">distribution</kbd>:<span> </span><span>Specifies the distribution. In our case, since the response variable has two classes, we set <kbd class="calibre12">distribution</kbd> to <kbd class="calibre12">"bernoulli"</kbd></span>.</li>
<li class="calibre11"><kbd class="calibre12">ntrees</kbd>: Number of trees.</li>
<li class="calibre11"><kbd class="calibre12">max_depth</kbd>: Denotes the maximum tree depth.</li>
<li class="calibre11"><kbd class="calibre12">min_rows</kbd>:<span> </span><span>Fewest allowed observations in a leaf.</span></li>
<li class="calibre11"><kbd class="calibre12">learn_rate</kbd>:<span> </span><span>Learning rate takes value from <kbd class="calibre12">0.0</kbd> to <kbd class="calibre12">1.0</kbd>.</span></li>
</ul>
<div class="packtinfobox"><span>Note that for all base learners, cross-validation folds must be the same and</span> <kbd class="calibre19">keep_cross_validation_predictions</kbd> must be set to <kbd class="calibre19">True</kbd>.</div>
<p class="calibre2">In <em class="calibre13">Step 9</em>, we trained a random forest base learner using the following hyperparameters: <kbd class="calibre12">ntrees</kbd>, <kbd class="calibre12">nfolds</kbd>, <kbd class="calibre12">fold_assignment</kbd>.</p>
<p class="calibre2">In <em class="calibre13">Step 10</em>, we trained our algorithm with a GLM. Note that we have not encoded the categorical variables in GLM.</p>
<div class="packtinfobox"><span>H2O recommends users to allow GLM handle categorical columns, as it can take advantage of the categorical column for better performance and efficient memory utilization.<br class="title-page-name"/></span><span>From H2o.ai: "We strongly recommend avoiding one-hot encoding categorical columns with any levels into many binary columns, as this is very inefficient. This is especially true for Python users who are used to expanding their categorical variables manually for other frameworks".</span></div>
<p class="calibre2"><span class="calibre5">In </span><em class="calibre13">Step 11</em><span class="calibre5">, we generated the test AUC values for each of the base learners and printed the best AUC.</span></p>
<p class="calibre2">In <em class="calibre13">Step 12</em>, we trained a stacked ensemble model by combining the output of the base learners using <span class="calibre5"><kbd class="calibre12">H2OStackedEnsembleEstimator</kbd>. We used the trained ensemble model on our test subset. Note that by default GLM is used as the meta-learner for <kbd class="calibre12">H2OStackedEnsembleEstimator</kbd>. However, we have used deep learning as the meta-learner in our example.</span></p>
<div class="packtinfobox">Note that we have used default hyperparameters values for our meta-learner. We can specify the hyperparameter values with <kbd class="calibre19">metalearner_params</kbd>. <span>The <kbd class="calibre19">metalearner_params</kbd></span><span> option allows you to pass in a dictionary/list of hyperparameters to use for the algorithm that is used as meta-learner.</span></div>
<p class="calibre2">Fine-tuning the hyperparameters can deliver better results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">You may also assemble a list of models to stack together in different ways. In the preceding example, we</span> <span class="calibre5">trained individual models and put them in a list to ensemble them. We can also train a grid of models:</span></p>
<ol class="calibre14">
<li class="calibre11">We specify the random forest hyperparameters for the grid:</li>
</ol>
<pre class="calibre18">hyper_params = {"max_depth": [3, 4, 5, 8, 10],<br class="title-page-name"/>                "min_rows": [3,4,5,6,7,8,9,10],<br class="title-page-name"/>                "mtries": [10,15, 20],<br class="title-page-name"/>                "ntrees": [100,250,500, 750],<br class="title-page-name"/>                "sample_rate": [0.7, 0.8, 0.9, 1.0],<br class="title-page-name"/>                "col_sample_rate_per_tree": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}<br class="title-page-name"/><br class="title-page-name"/>search_criteria = {"strategy": "RandomDiscrete", "max_models": 3, "seed": 1}</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We train the grid using the hyperparameters defined in the preceding code:</li>
</ol>
<pre class="calibre18"># Train the grid<br class="title-page-name"/>grid = H2OGridSearch(model=H2ORandomForestEstimator(nfolds=nfolds,\<br class="title-page-name"/>                                                    fold_assignment="Modulo",\<br class="title-page-name"/>                                                    keep_cross_validation_predictions=True),\<br class="title-page-name"/>                     hyper_params=hyper_params,\<br class="title-page-name"/>                     search_criteria=search_criteria,\<br class="title-page-name"/>                     grid_id="rf_grid_binomial")<br class="title-page-name"/><br class="title-page-name"/>grid.train(x=predictors, y=response, training_frame=train)</pre>
<p class="calibre2"/>
<ol start="3" class="calibre14">
<li class="calibre11">We train the ensemble using the random forest grid:</li>
</ol>
<pre class="calibre18"># Train a stacked ensemble using the RF grid<br class="title-page-name"/>ensemble = H2OStackedEnsembleEstimator(model_id="ensemble_rf_grid_binomial_9", base_models=grid.model_ids)<br class="title-page-name"/><br class="title-page-name"/>ensemble.train(x=predictors, y=response, training_frame=train)<br class="title-page-name"/><br class="title-page-name"/># Evaluate ensemble performance on the test data<br class="title-page-name"/>perf_stack_test = ensemble.model_performance(test)<br class="title-page-name"/><br class="title-page-name"/># Compare to base learner performance on the test set<br class="title-page-name"/>baselearner_best_auc_test = max([h2o.get_model(model).model_performance(test_data=test).auc() for model in grid.model_ids])<br class="title-page-name"/><br class="title-page-name"/>stack_auc_test = perf_stack_test.auc()<br class="title-page-name"/><br class="title-page-name"/>print("Best Base-learner Test AUC: ", baselearner_best_auc_test)<br class="title-page-name"/>print("Ensemble Test AUC: ", stack_auc_test)</pre>
<p class="calibre2">The preceding code will give the best base-learner test AUC and test the AUC from the ensemble model. If the response variable is highly imbalanced, consider fine-tuning the following hyperparameters to control oversampling and under-sampling:</p>
<ul class="calibre10">
<li class="calibre11"><span><kbd class="calibre12">balance_classes</kbd>: This option can be used to <span>balance</span> the <span>class</span> distribution. When enabled, H2O will either under-sample the majority classes or oversample the minority classes. If this option is enabled, you can also specify a value for the <kbd class="calibre12">class_sampling_factors</kbd></span><span> and </span><span><kbd class="calibre12">max_after_balance_size</kbd> options.</span></li>
<li class="calibre11"><span><kbd class="calibre12">class_sampling_factors</kbd>: By default, sampling factors will be automatically computed to obtain class balance during training. This behavior may be changed using the <kbd class="calibre12">class_sampling_factors</kbd> </span><span>parameter. This option sets an over- or under-sampling ratio for each class and requires <kbd class="calibre12">balance_classes=true</kbd>.</span><span><br class="title-page-name"/></span></li>
<li class="calibre11"><span><kbd class="calibre12">max_after_balance_size</kbd>: In most cases, setting <kbd class="calibre12">balance_classes</kbd> to true</span><span> will increase the size of the DataFrame. To reduce the DataFrame size, you can use the <kbd class="calibre12">max_after_balance_size</kbd></span><span><span> parameter. This specifies the maximum relative size of the training data after balancing the class counts and defaults to <kbd class="calibre12">5.0</kbd>.</span></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="calibre2">Take a look at <kbd class="calibre12">StackNet</kbd>, which was developed by Marios Michailidis as part of his PhD. <kbd class="calibre12">StackNet</kbd> is available under the MIT licence. It'<span class="calibre5">s a scalable and analytical framework that resembles a feed-forward neural network, and uses Wolpert's stacked-generalization concept to improve accuracy in machine learning predictive tasks. </span><span class="calibre5">It uses the notion of meta-learners, in that it uses the predictions of some algorithms as features for other algorithms. StackNet can also generalize stacking on multiple levels. </span><span class="calibre5">It is, however, computationally intensive. It was </span><span class="calibre5">originally developed in Java, but a</span><span class="calibre5"> lighter Python version of <kbd class="calibre12">StackNet</kbd>, named <kbd class="calibre12">pystacknet</kbd>, is now available as well.</span></p>
<p class="calibre2">Let's think about how StackNet works. In the case of a neural network, the output of one layer is inserted as an input to the next layer and an activation function, such as sigmoid, tanh, or relu, is applied. Similarly, in the case of StackNet, the activation functions can be replaced with any supervised machine learning algorithm.</p>
<p class="calibre2">The stacking element can be run on two modes: a normal stacking mode and a re-stacking mode. In the case of a <span class="calibre5">normal stacking mode, each layer uses the predictions of the previous one. In the case of re-stacking mode, each layer uses the neurons and activations of the previous layers. </span></p>
<p class="calibre2">Sample code that uses StackNet would consist of the following steps:</p>
<ol class="calibre14">
<li class="calibre11">Import the required libraries (note that we have imported <kbd class="calibre12">StackNetClassifier</kbd> and <kbd class="calibre12">StackNetRegressor</kbd> from the <kbd class="calibre12">pystacknet</kbd> library):</li>
</ol>
<pre class="calibre18">import numpy as np<br class="title-page-name"/><br class="title-page-name"/># import required libraries from sklearn<br class="title-page-name"/>from sklearn.tree import DecisionTreeClassifier<br class="title-page-name"/>from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier<br class="title-page-name"/>from sklearn.linear_model import LogisticRegression<br class="title-page-name"/><br class="title-page-name"/>from sklearn.metrics import roc_auc_score, log_loss<br class="title-page-name"/>from sklearn.model_selection import StratifiedKFold<br class="title-page-name"/><br class="title-page-name"/># import StackNetClassifier and StackNetRegressor from pystacknet<br class="title-page-name"/>from pystacknet.pystacknet import StackNetClassifier,StackNetRegressor<br class="title-page-name"/>from pystacknet.metrics import rmse,mae</pre>
<p class="calibre2"/>
<ol start="2" class="calibre14">
<li class="calibre11">We read the data, drop the <kbd class="calibre12">ID</kbd> column, and check the dimensions of the dataset:</li>
</ol>
<pre class="calibre18">df_creditcarddata = pd.read_csv("UCI_Credit_Card.csv")<br class="title-page-name"/><br class="title-page-name"/>#dropping the ID column, as it would not be required<br class="title-page-name"/>df_creditcarddata.drop(["ID"],axis=1,inplace=True)<br class="title-page-name"/><br class="title-page-name"/># Check the shape of the data<br class="title-page-name"/>df_creditcarddata.shape</pre>
<ol start="3" class="calibre14">
<li class="calibre11">We separate our target and predictor variables. We also split the data into training and testing subsets:</li>
</ol>
<pre class="calibre18">#create the predictor &amp; target set<br class="title-page-name"/>X = df_creditcarddata.iloc[:,0:23]<br class="title-page-name"/>Y = df_creditcarddata['default.payment.next.month']<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = \<br class="title-page-name"/>train_test_split(X, Y, test_size=0.20, random_state=1)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">We define the models for the base learners and the meta-learner:</li>
</ol>
<pre class="calibre18">models=[[DecisionTreeClassifier(criterion="entropy", max_depth=5, max_features=0.5, random_state=1),<br class="title-page-name"/>GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1),<br class="title-page-name"/>LogisticRegression(random_state=1)],<br class="title-page-name"/>[RandomForestClassifier (n_estimators=500, criterion="entropy", max_depth=5, max_features=0.5, random_state=1)]]</pre>
<ol start="5" class="calibre14">
<li class="calibre11">We now use <kbd class="calibre12">StackNetClassifier</kbd> to build the stacking ensemble. However, note that we use <kbd class="calibre12">restacking=False</kbd>, which means that it uses the <span>normal stacking mode:</span></li>
</ol>
<pre class="calibre18">model=StackNetClassifier(models, metric="accuracy", folds=4, restacking=False, use_retraining=True, use_proba=True, random_state=12345, n_jobs=1, verbose=1)<br class="title-page-name"/> <br class="title-page-name"/>model.fit(X_train,Y_train )<br class="title-page-name"/><br class="title-page-name"/># Uses the meta-learner model to predict the outcome<br class="title-page-name"/>preds=model.predict_proba(X_test)[:,1]<br class="title-page-name"/>print ("TEST ACCURACY without RESTACKING, auc %f " % (roc_auc_score(Y_test,preds)))</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">With <kbd class="calibre12">restacking=True</kbd>, <kbd class="calibre12">StackNetClassifier</kbd> would use the <span class="calibre5">re-stacking mode to build the models.</span></p>
<p class="calibre2">There are various case studies of StackNet being used in winning competitions in Kaggle. An example of how <kbd class="calibre12">StackNet</kbd> can be used is available at <a href="https://bit.ly/2T7339y" class="calibre9">https://bit.ly/2T7339y</a>.</p>


            </article>

            
        </section>
    </body></html>