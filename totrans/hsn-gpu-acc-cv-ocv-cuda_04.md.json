["```py\ncudaEvent_t e_start, e_stop;\ncudaEventCreate(&e_start);\ncudaEventCreate(&e_stop);\ncudaEventRecord(e_start, 0);\n//All GPU code for which performance needs to be measured allocate the memory\ncudaMalloc((void**)&d_a, N * sizeof(int));\ncudaMalloc((void**)&d_b, N * sizeof(int));\ncudaMalloc((void**)&d_c, N * sizeof(int));\n\n  //Copy input arrays from host to device memory\ncudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);\ncudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);\n\ngpuAdd << <512, 512 >> >(d_a, d_b, d_c);\n//Copy result back to host memory from device memory\ncudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);\ncudaDeviceSynchronize();\ncudaEventRecord(e_stop, 0);\ncudaEventSynchronize(e_stop);\nfloat elapsedTime;\ncudaEventElapsedTime(&elapsedTime, e_start, e_stop);\nprintf(\"Time to add %d numbers: %3.1f ms\\n\",N, elapsedTime);\n```", "```py\ncudaError_t cudaStatus;\ncudaStatus = cudaMalloc((void**)&d_a, sizeof(int));\nif (cudaStatus != cudaSuccess) {\n        fprintf(stderr, \"cudaMalloc failed!\");\n        goto Error;\n}\n```", "```py\ncudaStatus = cudaMemcpy(d_a,&h_a, sizeof(int), cudaMemcpyHostToDevice);\nif (cudaStatus != cudaSuccess) {\n  fprintf(stderr, \"cudaMemcpy failed!\");\n  goto Error;\n  }\n```", "```py\ngpuAdd<<<1, 1>>>(d_a, d_b, d_c);\n// Check for any errors launching the kernel\ncudaStatus = cudaGetLastError();\nif (cudaStatus != cudaSuccess) {\n  fprintf(stderr, \"addKernel launch failed: %s\\n\", cudaGetErrorString(cudaStatus));\n  goto Error;\n}\n```", "```py\nError:\n    cudaFree(d_a);\n```", "```py\nCoalesce Memory Access: d_a[i] = a\nStrided Memory Access: d_a[i*2] = a \n```", "```py\nThread divergence by way of branching\ntid = ThreadId\nif (tid%2 == 0)\n{ \n  Some Branch code;\n}\nelse\n{\n  Some other code; \n}\nThread divergence by way of looping \nPre-loop code\nfor (i=0; i<tid;i++)\n{\n  Some loop code;\n}\nPost loop code;\n```", "```py\nAllocate Memory: cudaHostAlloc ( (void **) &h_a, sizeof(*h_a), cudaHostAllocDefault);\nFree Memory: cudaFreeHost(h_a); \n\n```", "```py\n#include \"stdio.h\"\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n//Defining number of elements in Array\n#define N 50000\n\n//Defining Kernel function for vector addition\n__global__ void gpuAdd(int *d_a, int *d_b, int *d_c) {\n  //Getting block index of current kernel\n\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  while (tid < N)\n  {\n    d_c[tid] = d_a[tid] + d_b[tid];\n    tid += blockDim.x * gridDim.x;\n  }\n}\n```", "```py\nint main(void) {\n  //Defining host arrays\n  int *h_a, *h_b, *h_c;\n  //Defining device pointers for stream 0\n  int *d_a0, *d_b0, *d_c0;\n  //Defining device pointers for stream 1\n int *d_a1, *d_b1, *d_c1;\n cudaStream_t stream0, stream1;\n cudaStreamCreate(&stream0);\n cudaStreamCreate(&stream1);\n\ncudaEvent_t e_start, e_stop;\n cudaEventCreate(&e_start);\n  cudaEventCreate(&e_stop);\n  cudaEventRecord(e_start, 0);\n\n```", "```py\n  //Allocate memory for host pointers\n  cudaHostAlloc((void**)&h_a, 2*N* sizeof(int),cudaHostAllocDefault);\n cudaHostAlloc((void**)&h_b, 2*N* sizeof(int), cudaHostAllocDefault);\n cudaHostAlloc((void**)&h_c, 2*N* sizeof(int), cudaHostAllocDefault);\n  //Allocate memory for device pointers\n  cudaMalloc((void**)&d_a0, N * sizeof(int));\n  cudaMalloc((void**)&d_b0, N * sizeof(int));\n  cudaMalloc((void**)&d_c0, N * sizeof(int));\n  cudaMalloc((void**)&d_a1, N * sizeof(int));\n  cudaMalloc((void**)&d_b1, N * sizeof(int));\n  cudaMalloc((void**)&d_c1, N * sizeof(int));\n  for (int i = 0; i < N*2; i++) {\n    h_a[i] = 2 * i*i;\n    h_b[i] = i;\n  }\n```", "```py\n//Asynchrnous Memory Copy Operation for both streams\ncudaMemcpyAsync(d_a0, h_a , N * sizeof(int), cudaMemcpyHostToDevice, stream0);\ncudaMemcpyAsync(d_a1, h_a+ N, N * sizeof(int), cudaMemcpyHostToDevice, stream1);\ncudaMemcpyAsync(d_b0, h_b , N * sizeof(int), cudaMemcpyHostToDevice, stream0);\ncudaMemcpyAsync(d_b1, h_b + N, N * sizeof(int), cudaMemcpyHostToDevice, stream1);\n\n//Kernel Call     \ngpuAdd << <512, 512, 0, stream0 >> > (d_a0, d_b0, d_c0);\ngpuAdd << <512, 512, 0, stream1 >> > (d_a1, d_b1, d_c1);\n\n//Copy result back to host memory from device memory\ncudaMemcpyAsync(h_c , d_c0, N * sizeof(int), cudaMemcpyDeviceToHost, stream0);\ncudaMemcpyAsync(h_c + N, d_c1, N * sizeof(int), cudaMemcpyDeviceToHost, stream0);\n```", "```py\ncudaDeviceSynchronize();\ncudaStreamSynchronize(stream0);\ncudaStreamSynchronize(stream1);\n```", "```py\ncudaEventRecord(e_stop, 0);\ncudaEventSynchronize(e_stop);\nfloat elapsedTime;\ncudaEventElapsedTime(&elapsedTime, e_start, e_stop);\nprintf(\"Time to add %d numbers: %3.1f ms\\n\",2* N, elapsedTime);\n```", "```py\nint Correct = 1;\nprintf(\"Vector addition on GPU \\n\");\n//Printing result on console\nfor (int i = 0; i < 2*N; i++) \n{\n  if ((h_a[i] + h_b[i] != h_c[i]))\n  {\n    Correct = 0;\n  }\n}\n\nif (Correct == 1)\n{\n  printf(\"GPU has computed Sum Correctly\\n\");\n}\nelse\n{\n  printf(\"There is an Error in GPU Computation\\n\");\n}\n//Free up memory\ncudaFree(d_a0);\ncudaFree(d_b0);\ncudaFree(d_c0);\ncudaFree(d_a0);\ncudaFree(d_b0);\ncudaFree(d_c0);\ncudaFreeHost(h_a);\ncudaFreeHost(h_b);\ncudaFreeHost(h_c);\nreturn 0;\n}\n```", "```py\n#include \"device_launch_parameters.h\"\n#include <stdio.h>\n\n#define arraySize 5\n#define threadPerBlock 5\n//Kernel Function for Rank sort\n__global__ void addKernel(int *d_a, int *d_b)\n{\n  int count = 0;\n  int tid = threadIdx.x;\n  int ttid = blockIdx.x * threadPerBlock + tid;\n  int val = d_a[ttid];\n  __shared__ int cache[threadPerBlock];\n  for (int i = tid; i < arraySize; i += threadPerBlock) {\n    cache[tid] = d_a[i];\n    __syncthreads();\n    for (int j = 0; j < threadPerBlock; ++j)\n      if (val > cache[j])\n        count++;\n        __syncthreads();\n  }\n  d_b[count] = val;\n}\n```", "```py\nint main()\n{\n    //Define Host and Device Array\n  int h_a[arraySize] = { 5, 9, 3, 4, 8 };\n  int h_b[arraySize];\n  int *d_a, *d_b;\n\n    //Allocate Memory on the device \n  cudaMalloc((void**)&d_b, arraySize * sizeof(int));\n  cudaMalloc((void**)&d_a, arraySize * sizeof(int));\n\n    // Copy input vector from host memory to device memory.\n  cudaMemcpy(d_a, h_a, arraySize * sizeof(int), cudaMemcpyHostToDevice);\n\n    // Launch a kernel on the GPU with one thread for each element.\n  addKernel<<<arraySize/threadPerBlock, threadPerBlock>>>(d_a, d_b);\n\n    //Wait for device to finish operations\n  cudaDeviceSynchronize();\n    // Copy output vector from GPU buffer to host memory.\n  cudaMemcpy(h_b, d_b, arraySize * sizeof(int), cudaMemcpyDeviceToHost);\n  printf(\"The Enumeration sorted Array is: \\n\");\n  for (int i = 0; i < arraySize; i++) \n  {\n    printf(\"%d\\n\", h_b[i]);\n  }\n    //Free up device memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  return 0;\n}\n```", "```py\nfor (int i=0; i < image_height; i++)\n{\n   for (int j=0; j < image_width; j++)\n   {\n      //Pixel Processing code for pixel located at (i,j)\n   }\n}\n```", "```py\nint i = blockIdx.y * blockDim.y + threadIdx.y;\nint j = blockIdx.x * blockDim.x + threadIdx.x;\n\n```", "```py\nint h_a[1000] = Random values between 0 and 15\n\nint histogram[16];\nfor (int i = 0; i<16; i++)\n{ \n   histogram[i] = 0;\n}\nfor (i=0; i < 1000; i++)\n{\n   histogram[h_a[i]] +=1;\n} \n```", "```py\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define SIZE 1000\n#define NUM_BIN 16\n\n__global__ void histogram_without_atomic(int *d_b, int *d_a)\n{\n  int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int item = d_a[tid];\n  if (tid < SIZE)\n  {\n    d_b[item]++;\n  }\n }\n\n__global__ void histogram_atomic(int *d_b, int *d_a)\n{\n  int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int item = d_a[tid];\n  if (tid < SIZE)\n  {\n    atomicAdd(&(d_b[item]), 1);\n  }\n}\n```", "```py\nint main()\n{\n\n  int h_a[SIZE];\n  for (int i = 0; i < SIZE; i++) {\n\n  h_a[i] = i % NUM_BIN;\n  }\n  int h_b[NUM_BIN];\n  for (int i = 0; i < NUM_BIN; i++) {\n    h_b[i] = 0;\n  }\n\n  // declare GPU memory pointers\n  int * d_a;\n  int * d_b;\n\n  // allocate GPU memory\n  cudaMalloc((void **)&d_a, SIZE * sizeof(int));\n  cudaMalloc((void **)&d_b, NUM_BIN * sizeof(int));\n\n  // transfer the arrays to the GPU\n  cudaMemcpy(d_a, h_a, SIZE * sizeof(int), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, h_b, NUM_BIN * sizeof(int), cudaMemcpyHostToDevice);\n\n  // launch the kernel\n\n  //histogram_without_atomic << <((SIZE+NUM_BIN-1) / NUM_BIN), NUM_BIN >> >(d_b, d_a);\n  histogram_atomic << <((SIZE+NUM_BIN-1) / NUM_BIN), NUM_BIN >> >(d_b, d_a);\n\n  // copy back the sum from GPU\n  cudaMemcpy(h_b, d_b, NUM_BIN * sizeof(int), cudaMemcpyDeviceToHost);\n  printf(\"Histogram using 16 bin without shared Memory is: \\n\");\n  for (int i = 0; i < NUM_BIN; i++) {\n    printf(\"bin %d: count %d\\n\", i, h_b[i]);\n  }\n\n  // free GPU memory allocation\n  cudaFree(d_a);\n  cudaFree(d_b);\n  return 0;\n}\n```", "```py\n#include <stdio.h>\n#include <cuda_runtime.h>\n#define SIZE 1000\n#define NUM_BIN 256\n__global__ void histogram_shared_memory(int *d_b, int *d_a)\n{\n  int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int offset = blockDim.x * gridDim.x;\n  __shared__ int cache[256];\n  cache[threadIdx.x] = 0;\n  __syncthreads();\n\n  while (tid < SIZE)\n  {\n    atomicAdd(&(cache[d_a[tid]]), 1);\n    tid += offset;\n  }\n  __syncthreads();\n  atomicAdd(&(d_b[threadIdx.x]), cache[threadIdx.x]);\n}\n```"]