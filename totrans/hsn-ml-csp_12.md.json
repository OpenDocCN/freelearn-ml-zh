["```py\npublic abstract NdArray[] Forward(params NdArray[] xs);\npublic virtual void Backward([CanBeNull] params NdArray[] ys){}\n```", "```py\nFunctionStack nn = new FunctionStack(\n                 new Linear(2, 2, name: \"l1 Linear\"),\n                 new Sigmoid(name: \"l1 Sigmoid\"),\n                 new Linear(2, 2, name: \"l2 Linear\"));\n```", "```py\nFunctionStack nn = new FunctionStack(\n                 new Convolution2D(1, 2, 3, name: \"conv1\", gpuEnable: true),// Do not forget the GPU flag if necessary\n                 new ReLU(),\n                 new MaxPooling(2, 2),\n                 new Convolution2D(2, 2, 2, name: \"conv2\", gpuEnable: true),\n                 new ReLU(),\n                 new MaxPooling(2, 2),\n                 new Linear(8, 2, name: \"fl3\"),\n                 new ReLU(),\n                 new Linear(2, 2, name: \"fl4\")\n             );\n```", "```py\nFunctionStack nn = new FunctionStack(\n                 new Linear(neuronCount * neuronCount, N, name: \"l1 Linear\"), // L1\n                 new BatchNormalization(N, name: \"l1 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l1 LeakyReLU\"),\n                 new Linear(N, N, name: \"l2 Linear\"), // L2\n                 new BatchNormalization(N, name: \"l2 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l2 LeakyReLU\"),\n                 new Linear(N, N, name: \"l3 Linear\"), // L3\n                 new BatchNormalization(N, name: \"l3 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l3 LeakyReLU\"),\n                 new Linear(N, N, name: \"l4 Linear\"), // L4\n                 new BatchNormalization(N, name: \"l4 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l4 LeakyReLU\"),\n                 new Linear(N, N, name: \"l5 Linear\"), // L5\n                 new BatchNormalization(N, name: \"l5 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l5 LeakyReLU\"),\n                 new Linear(N, N, name: \"l6 Linear\"), // L6\n                 new BatchNormalization(N, name: \"l6 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l6 LeakyReLU\"),\n                 new Linear(N, N, name: \"l7 Linear\"), // L7\n                 new BatchNormalization(N, name: \"l7 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l7 ReLU\"),\n                 new Linear(N, N, name: \"l8 Linear\"), // L8\n                 new BatchNormalization(N, name: \"l8 BatchNorm\"),\n                 new LeakyReLU(slope: 0.000001, name: \"l8 LeakyReLU\"),\n                 new Linear(N, N, name: \"l9 Linear\"), // L9\n                 new BatchNormalization(N, name: \"l9 BatchNorm\"),\n                 new PolynomialApproximantSteep(slope: 0.000001, name: \"l9 PolynomialApproximantSteep\"),\n                 new Linear(N, N, name: \"l10 Linear\"), // L10\n                 new BatchNormalization(N, name: \"l10 BatchNorm\"),\n                 new PolynomialApproximantSteep(slope: 0.000001, name: \"l10 PolynomialApproximantSteep\"),\n                 new Linear(N, N, name: \"l11 Linear\"), // L11\n                 new BatchNormalization(N, name: \"l11 BatchNorm\"),\n                 new PolynomialApproximantSteep(slope: 0.000001, name: \"l11 PolynomialApproximantSteep\"),\n                 new Linear(N, N, name: \"l12 Linear\"), // L12\n                 new BatchNormalization(N, name: \"l12 BatchNorm\"),\n                 new PolynomialApproximantSteep(slope: 0.000001, name: \"l12 PolynomialApproximantSteep\"),\n                 new Linear(N, N, name: \"l13 Linear\"), // L13\n                 new BatchNormalization(N, name: \"l13 BatchNorm\"),\n                 new PolynomialApproximantSteep(slope: 0.000001, name: \"l13 PolynomialApproximantSteep\"),\n                 new Linear(N, N, name: \"l14 Linear\"), // L14\n                 new BatchNormalization(N, name: \"l14 BatchNorm\"),\n                 new PolynomialApproximantSteep(slope: 0.000001, name: \"l14 PolynomialApproximantSteep\"),\n                 new Linear(N, 10, name: \"l15 Linear\") // L15\n             );\n```", "```py\npublic static void Run()\n         {\n             const int learningCount = 10000;\n\n             Real[][] trainData =\n             {\n                 new Real[] { 0, 0 },\n                 new Real[] { 1, 0 },\n                 new Real[] { 0, 1 },\n                 new Real[] { 1, 1 }\n             };\n\n             Real[][] trainLabel =\n             {\n                 new Real[] { 0 },\n                 new Real[] { 1 },\n                 new Real[] { 1 },\n                 new Real[] { 0 }\n             };\n\n             FunctionStack nn = new FunctionStack(\n                 new Linear(2, 2, name: \"l1 Linear\"),\n                 new ReLU(name: \"l1 ReLU\"),\n                 new Linear(2, 1, name: \"l2 Linear\"));\n\n             nn.SetOptimizer(new AdaGrad());\n\n             RILogManager.Default?.SendDebug(\"Training...\");\n             for (int i = 0; i < learningCount; i++)\n             {\n                 //use MeanSquaredError for loss function\n                 Trainer.Train(nn, trainData[0], trainLabel[0], new MeanSquaredError(), false);\n                 Trainer.Train(nn, trainData[1], trainLabel[1], new MeanSquaredError(), false);\n                 Trainer.Train(nn, trainData[2], trainLabel[2], new MeanSquaredError(), false);\n                 Trainer.Train(nn, trainData[3], trainLabel[3], new MeanSquaredError(), false);\n\n                 //If you do not update every time after training, you can update it as a mini batch\n                 nn.Update();\n             }\n\n             RILogManager.Default?.SendDebug(\"Test Start...\");\n             foreach (Real[] val in trainData)\n             {\n                 NdArray result = nn.Predict(val)[0];\n                 RILogManager.Default?.SendDebug($\"{val[0]} xor {val[1]} = {(result.Data[0] > 0.5 ? 1 : 0)} {result}\");\n             }\n         }\n```", "```py\nWeaver.Initialize(ComputeDeviceTypes.Gpu);\n```", "```py\n /// <summary>   The context. </summary>\n         internal static ComputeContext Context;\n         /// <summary>   The devices. </summary>\n         private static ComputeDevice[] Devices;\n         /// <summary>   Queue of commands. </summary>\n         internal static ComputeCommandQueue CommandQueue;\n         /// <summary>   Zero-based index of the device. </summary>\n         private static int DeviceIndex;\n         /// <summary>   True to enable, false to disable. </summary>\n         internal static bool Enable;\n         /// <summary>   The platform. </summary>\n         private static ComputePlatform Platform;\n         /// <summary>   The kernel sources. </summary>\n         private static readonly Dictionary<string, string> KernelSources = new Dictionary<string, string>();\n```", "```py\nReal[][] trainData = new Real[N][];\n             Real[][] trainLabel = new Real[N][];\n\n             for (int i = 0; i < N; i++)\n             {\n                 //Prepare Sin wave for one cycle\n                 Real radian = -Math.PI + Math.PI * 2.0 * i / (N - 1);\n                 trainData[i] = new[] { radian };\n                 trainLabel[i] = new Real[] { Math.Sin(radian) };\n             }\n```", "```py\nFunctionStack nn = new FunctionStack(\n                 new Linear(1, 4, name: \"l1 Linear\"),\n                 new Tanh(name: \"l1 Tanh\"),\n                 new Linear(4, 1, name: \"l2 Linear\")\n             );\n```", "```py\n nn.SetOptimizer(new SGD());\n```", "```py\nfor (int i = 0; i < EPOCH; i++)\n            {\n                Real loss = 0;\n                for (int j = 0; j < N; j++)\n                {\n                    //When training is executed in the network, an error is returned to the return value\n                    loss += Trainer.Train(nn, trainData[j], trainLabel[j], new MeanSquaredError());\n                }\n                if (i % (EPOCH / 10) == 0)\n                {\n                    RILogManager.Default?.SendDebug(\"loss:\" + loss / N);\n                    RILogManager.Default?.SendDebug(\"\");\n                }\n            }\n```", "```py\n    RILogManager.Default?.SendDebug(\"Test Start...\");\n            foreach (Real[] val in trainData)\n            {\n                RILogManager.Default?.SendDebug(val[0] + \":\" + nn.Predict(val)[0].Data[0]);\n            }\n```"]