- en: Chapter 11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Best of Both Worlds: Hybrid Architectures'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unity makes strength.*'
  prefs: []
  type: TYPE_NORMAL
- en: — English aphorism
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, we have a solid understanding of both classical and quantum neural
    networks. In this chapter, we will leverage this knowledge to explore an interesting
    kind of model: hybrid architectures of quantum neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss what these models are and how they can be useful,
    and we will also learn how to implement and train them with PennyLane and Qiskit.
    The whole chapter is going to be very hands-on, and we will also take the time
    to fill in some gaps regarding the actual practice of training models in real-world
    scenarios. In addition to this — and just to spice things up a bit — we will go
    beyond our usual binary classifiers and also consider other kinds of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The what and why of hybrid architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid architectures in PennyLane (with a brief overview of best practices for
    training models in real-world scenarios and an introduction to multi-class classification
    problems)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid architectures in Qiskit (with an introduction to PyTorch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is going to be a very exciting chapter. Let’s begin by giving meaning to
    these hybrid architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1 The what and why of hybrid architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we’ve used the adjective ”hybrid” to describe algorithms that
    rely on both classical and quantum processing; algorithms such as QAOA or VQE
    fit in this category, as well as the training of QSVMs and QNNs. When we talk
    about **hybrid architectures** or **hybrid models**, however, we refer to something
    more specific: we speak about models that combine classical models with other
    quantum-based models by joining them together and training them as a single unit.
    Of course, the training of hybrid models will itself be a hybrid algorithm. We
    know that the terminology might be confusing, but what can we do? Hybrid is too
    versatile a word to give it up.'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we will combine quantum neural networks with classical neural
    networks, for they are the two models that fit more naturally together. The way
    we will go about doing this will be by taking a usual classical neural network
    and plugging in a quantum neural network as one of its layers. In this way, the
    ”quantum layer” will take as input the outputs of the previous layer (or the inputs
    to the model, if there’s no layer before it) and will feed its output to the next
    layer (should there be any). The output of the quantum neural network will be
    a numerical array of length ![k](img/file317.png "k"); thus, in the eyes of the
    next layer, the quantum layer will behave as if it were a classical layer with
    ![k](img/file317.png "k") neurons.
  prefs: []
  type: TYPE_NORMAL
- en: These hybrid architectures combining classical and quantum neural networks are
    said to be, to the surprise of no one, **hybrid quantum neural** **networks**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a hybrid QNN is a classical neural network in which one or more
    of its layers have been replaced by quantum layers. These are quantum neural networks
    that get inputs from the outputs of the previous layer and feed their outputs
    to the next one. Of course, if there’s no next layer, the output of the quantum
    layer will be the output of the network. Analogously, if there’s no previous layer,
    the input to the quantum network will be the model’s input.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ve already hinted, a hybrid neural network is trained as a single unit:
    the training process involves the optimization of both the parameters of the classical
    layers and those of the quantum neural networks inside the quantum layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the whole definition of hybrid QNNs more clear, let us consider a simple
    example of how one such network may be constructed:'
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid QNN must begin taking some classical inputs. Let’s say it takes ![16](img/file619.png
    "16").
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We may then feed the input into a usual classical layer with ![8](img/file506.png
    "8") neurons and use the sigmoid activation function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will add a quantum layer. This quantum layer will have to accept ![8](img/file506.png
    "8") inputs from the previous layer. For example, we could use a QNN with three
    qubits using amplitude encoding. The output of this quantum layer could be, for
    instance, the expectation values of the first and second qubits, both measured
    on the computational basis. In this case, this quantum layer that we have added
    will return two numeric values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we may add a classical layer with a single neuron that uses the sigmoid
    activation function. This layer will take inputs from the quantum layer, so it
    will accept two inputs. It will essentially treat the quantum layer as if it were
    a classical layer with two neurons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And that’s how you can build yourself a simple hybrid QNN — at least in theory!
    But the question is... why would we want to do such a thing? What are these hybrid
    models good for? Let’s illustrate it with a typical example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned how to use a QNN to tackle a (binary) classification
    task. But, due to the limitations of current quantum hardware and simulators,
    we were forced to apply some dimensionality reduction techniques on our data before
    we could use it. That’s a situation where hybrid QNNs may prove useful: why not
    combine, in a single model, classical dimensionality reduction carried out by
    a classical neural network with classification performed by a quantum neural network?'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, instead of first reducing the dimensionality of our data and then
    classifying it with a quantum neural network, we could consider a hybrid QNN with
  prefs: []
  type: TYPE_NORMAL
- en: a bunch of classical layers that would reduce the dimensionality of our data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: joined to a quantum layer that would be in charge of making the classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, since the whole network would be trained as a single unit, there
    would be no way to truly tell whether the classical part of the network is only
    doing dimensionality reduction and the quantum part is only doing classification.
    Most likely, both parts will work on both tasks to some degree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding any further, a few disclaimers are in order. First and foremost:
    quantum layers are not any sort of magical tool that will surely lead to great
    improvements in the performance of a classical neural network. Actually, if used
    unwisely, quantum layers could very easily have a negative impact on your model!
    The key takeaway is that you shouldn’t blindly use a quantum layer solely as a
    replacement for a classical layer in a network. Be intentional. If you are going
    to include a quantum layer in your model, think about the role it’s going to play
    in it.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, when working with hybrid QNNs, you should watch out for how you
    are joining classical and quantum layers together. For instance, if you have a
    quantum layer using a feature map that requires its inputs to be normalized, maybe
    using an ELU activation function in the previous layer isn’t the best of ideas,
    because it is in no way bounded. On the other hand, in that case, a sigmoid activation
    function could be a great fit for the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: In the use case that we discussed a few paragraphs ago (combining classical
    data reduction with quantum classification), we can witness the ”intentionality”
    that we’ve just talked about. We do know that, in principle, a neural network
    can do a good job at reducing data dimensionality; in case you didn’t know, it’s
    a known fact that, using something called **autoencoders** [[104](ch030.xhtml#Xhandsonml),
    Chapter 17], one can train an **encoder** network that can reduce the dimensionality
    of a dataset. And we know that a quantum neural network can do a good job at classifying
    data coming from a dimensionality reduction technique (just have a look at the
    previous chapter!). So there must be some choice of parameters such that the combined
    hybrid model will successfully accomplish both tasks. Hence, with the right training,
    our hybrid model should be able to perform at least as well as it would if a classical
    encoder and a quantum classifier were trained separately. And the important bit
    is the ”at least,” because when training the classical encoder and the quantum
    classifier together we can join their powers!
  prefs: []
  type: TYPE_NORMAL
- en: And that’s the heuristic justification behind this interesting application of
    hybrid neural networks. Actually, this is the use case that we will devote this
    chapter to. However, this is by no means the only application of hybrid models!
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid architectures can also be used in regression problems, as we will later
    see in an exercise. In fact, this is a very interesting application, for Skolit
    et al. [[91](ch030.xhtml#Xskolik2022quantum)] have shown that adding a final layer
    with trainable parameters that transform the output of a quantum neural network
    can be very beneficial in certain reinforcement learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Now we promised that this chapter would be very hands-on, and we are going to
    honor that. That should have been enough of a theoretical introduction, so let’s
    gear up! Get ready to train a bunch of hybrid QNNs to classify some data.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Hybrid architectures in PennyLane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to use PennyLane to implement and train a couple
    of hybrid QNNs in order to solve some classification problems. Firstly, we will
    tackle a binary classification problem, just to better understand how hybrid QNNs
    work in a familiar setting. Then, we will take one step further and do the same
    for a multi-class classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get to the problems, though, let us set things up.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.1 Setting things up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As on previous occasions, we shall begin by importing NumPy and TensorFlow
    and setting a seed for both packages — all to ensure the reproducibility of our
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we can also import some useful functions from scikit-learn. We’ve already
    used them extensively — they need no introduction!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this chapter, we will generate our own datasets to have more flexibility.
    In order to create them, we will rely on the `make_classification` function in
    the scikit-learn package. Remember that we introduced it in *Chapter* [*8*](ch017.xhtml#x1-1390008),
    *What Is* *Quantum Machine Learning?*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, in this section, we will use the Lightning simulator with adjoint differentiation
    in order to get a good performance. Thus, we need to change the default datatype
    used by Keras models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can now import PennyLane and define the hermitian matrix ![M](img/file704.png
    "M") that we used in the previous chapter. Recall that it corresponds to the observable
    that assigns the eigenvalue ![1](img/file13.png "1") to ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle") and the eigenvalue ![0](img/file12.png "0") to ![\left|
    1 \right\rangle](img/file14.png "\left| 1 \right\rangle"); that is, ![M = \left|
    0 \right\rangle\left\langle 0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle
    0 \right|").
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we may import Matplotlib and reuse the function that we defined in
    the previous chapter for plotting training and validation losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And that’s all we need to get started. Let’s go for our first problem.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.2 A binary classification problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now ready to build our first hybrid QNN and train it to solve a binary
    classification task. Of course, the first thing we need is data and, as we discussed
    in the previous section, we shall generate it using the `make_classification`
    function. Using a hybrid QNN that will ”combine classical encoding with quantum
    classification” can make sense if, for instance, we have a large number of variables
    (features) in our dataset, so we will generate a dataset with ![20](img/file588.png
    "20") variables — that might already be quite large for current quantum hardware!
    Just to make sure that we have enough data, we will generate ![1000](img/file790.png
    "1000") samples. This is how we can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `make_classification` functions generate datasets with two possible
    classes. Just what we wanted!
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we will have to split this dataset into some training, validation,
    and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With our data ready, we need to think about the model that we will use. Let’s
    begin by constructing the quantum layer (the QNN) that we will include at the
    end of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this problem, we will use the two-local variational form that we introduced
    in the previous chapter (see *Figure* [*10.2*](ch019.xhtml#Figure10.2)). As you
    surely remember, we can implement it in PennyLane as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will take the quantum layer to be a simple QNN on four qubits using angle
    embedding as a feature map followed by the two-local variational form that we
    have just implemented. The measurement operation in the QNN will be the computation
    of the expectation value of ![M](img/file704.png "M") on the first qubit; that’s
    a sensible choice for binary classifiers in general, because it returns a value
    between ![0](img/file12.png "0") and ![1](img/file13.png "1"). The QNN can be
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we have already declared the weights dictionary that we will have
    to send to the TensorFlow interface in order to create the quantum layer. In it,
    we’ve specified that our variational form uses ![4 \cdot (2 + 1) = 12](img/file1398.png
    "4 \cdot (2 + 1) = 12") weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will define our hybrid QNN to have an input layer with ![20](img/file588.png
    "20") inputs in order to match the dimension of our data. This will be followed
    by a classical layer, which will be immediately followed by the quantum neural
    network (the quantum layer). Since our QNN accepts ![4](img/file143.png "4") inputs,
    the classical layer will have ![4](img/file143.png "4") neurons itself. Moreover,
    for the QNN to work optimally, we need the data to be normalized, so the classical
    layer will use a sigmoid activation function. We can define this model in Keras
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: 'When defining the Keras model, you may be tempted to store the quantum layer
    in a variable and then use it in the model definition, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This code will work and, a priori, there’s nothing wrong with it. However, if
    you decide to reset or modify your model, you will also have to rerun the first
    line, with the definition of `qlayer`, if you want to re-initialize the optimizable
    parameters (weights) in the quantum neural network!
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the model ready, we can also define our usual early stopping callback:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We’ve set the patience to ![2](img/file302.png "2") epochs in order to speed
    up the training; having a higher patience may easily lead to better results!
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, all it takes for us to train our model is to — just as we’ve always
    done on TensorFlow — pick an optimizer, compile our model with the binary cross
    entropy loss function, and call the `fit` method with the appropriate arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Et voilà! In just a matter of minutes, your flashy hybrid model will have finished
    training. Take a moment to reflect on how easy this was. You have been able to
    train a hybrid QNN with full ease, just as if it were a simple QNN. With PennyLane,
    quantum machine learning is a piece of cake.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check how well the training went, we can plot the training and validation
    losses with our custom function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The generated plot can be found in *Figure* [*11.1*](#Figure11.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1: Evolution of the training and validation loss functions in the
    training of a hybrid QNN binary classifier ](img/file1399.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.1**: Evolution of the training and validation loss functions in
    the training of a hybrid QNN binary classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'Those losses look really good; there don’t seem to be signs of overfitting
    and the model appears to be learning. In any case, let’s compute the test accuracy.
    We may also compute the training and validation accuracies, just for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When running the preceding code, we can see how our model has a training accuracy
    of ![95\%](img/file1400.png "95\%"), a validation accuracy of ![90\%](img/file1401.png
    "90\%"), and a test accuracy of ![96\%](img/file1402.png "96\%").
  prefs: []
  type: TYPE_NORMAL
- en: That’s a very satisfactory result. We have just trained our first hybrid QNN
    binary classifier, and we’ve seen how it can be effectively used to solve classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.1
  prefs: []
  type: TYPE_NORMAL
- en: Try to solve this problem using two additional (dense) classical layers, with
    ![16](img/file619.png "16") and ![8](img/file506.png "8") neurons each. Compare
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we said that this chapter was going to be hands-on and we truly meant it.
    So far, we have just trained models and gotten them right in one shot, but that’s
    something that rarely happens in practice. That’s why we’ve put together a small
    subsection on how to optimize models in real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.3 Training models in the real world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether you believe it or not, we care for you, our dear reader. All this time,
    behind each and every model that we’ve trained, we’ve invested hours of meticulous
    parameter selection and model preparation — all to make sure that the results
    we give you are good enough, if not optimal.
  prefs: []
  type: TYPE_NORMAL
- en: When you set out to train models on your own, you will soon find out that things
    don’t always work as well as you expected. For each well-performing model, there
    will be tens or even hundreds of discarded ones. And that’s something you need
    to prepare yourself for.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the early stages of a machine learning project in general — and a quantum
    machine learning project in particular — you should address two main following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How will you log all your results?** When you train lots of models, you need
    to find a way to log their performances together with their architectures and
    the parameters used in their training. That way, you can easily identify what
    works and what doesn’t, and you can avoid repeating the same mistakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How will you explore variations of your models?** Keeping a separate script
    for every model can be manageable when you are not training many models, but this
    isn’t a solution for large-scale projects. Oftentimes, you want to try a wide
    range of configurations and see which one works best. And automation can truly
    make your life easier in this regard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We leave the first question to you. In truth, there’s no universal way to address
    it — it all depends on the problem at hand and on the training strategy that you
    take. However, in regard to the second question, we do have something to offer.
  prefs: []
  type: TYPE_NORMAL
- en: When training a model, choosing good hyperparameters — such as a good batch
    size or learning rate — is not an easy task, but it is a crucial one. Should you
    use a smaller or a larger learning rate? How many layers should you use? Of what
    type? Decisions, decisions, decisions! The number of possibilities grows exponentially,
    so it is impossible to explore every one of them. But, in machine learning, finding
    a good configuration can be the difference between success and failure. How can
    we do this systematically and (kind of) effortlessly?
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few packages and utilities out there that can help you automate
    the search for optimal training parameters. One of the most popular ones is the
    Optuna package, which we are about to demonstrate. Please refer to *Appendix*
    [*D*](ch027.xhtml#x1-240000D), *Installing the Tools*, for installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: The process of automatically searching for optimal training parameters in a
    machine learning problem fits into what is known as **automated machine learning**,
    usually abbreviated as **AutoML**. This refers to the use of automation in order
    to solve machine learning problems. Having machines in charge of training other
    machines!
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve installed Optuna, you can import it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are going to use Optuna to find the best possible learning rate between the
    values ![0.001](img/file1165.png "0.001") and ![0.1](img/file1163.png "0.1").
    In order to do this, we need to define a function (which we shall call `objective`)
    with a single argument (`trial`). The objective function should use the training
    parameters that we want to optimize — in a manner that we will soon make precise
    — and it should return whichever metric we want to optimize. For instance, in
    our case, we would like to maximize the validation accuracy, so the objective
    function should train a model and return the validation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `trial` argument of the `objective` function is meant to represent an object
    of the `Trial` class that can be found in the `optuna``.``trial` module. We will
    use this object to define, within the objective function itself, the training
    parameters that we want to optimize, while also specifying their constraints:
    whether we want them to be integers or floats, the ranges within which we want
    our values to be, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our case, this is the objective function that we would have to define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we have defined the learning rate as an optimizable parameter by
    calling the `trial``.``suggest_float``(``"``learning_rate``"``,` `0.001,` `0.1)`
    method. In general, if you want to optimize a parameter named `"``parameter``"`,
    the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: If the data type of the parameter is a float and the parameter is bounded between
    `m` and `M`, you should call the `suggest_float``(``"``parameter``"``,` `m``,`
    `M``)` method. If you only want your parameter to take discrete values between
    `m` and `M` separated by a step `s`, you can send the optional argument `step`
    `=` `s`, which defaults to `None` (by default, the parameter will take continuous
    values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data type of the parameter is an integer bounded between `m` and `M`,
    you should call `suggest_int``(``"``parameter``"``,` `m``,` `M``)`. Also, if the
    values of the parameter should be separated by a step `s` from `m` to `M`, you
    can send in `step` `=` `s`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If your parameter takes values out of a list `values` of possible values, you
    should call `suggest_categorical``(``"``parameter``"``,` `values``)`. For instance,
    if we wanted to try out different activation functions on a layer of a neural
    network, we could use something like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Of course, a single objective function can have as many optimizable parameters
    as desired. They would just be defined with separate invocations of the methods
    that we’ve just outlined.
  prefs: []
  type: TYPE_NORMAL
- en: 'So that’s how you can create an objective function and specify the parameters
    that you want to optimize within it. Now, how do we optimize them? The first step
    is to create a `Study` object with the `create_study` function, just as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have specified that we want to create a study in order to maximize
    some objective function and using `TPESampler` with a seed. By default, Optuna
    will try to minimize objective functions — that’s why we had to send in that argument.
    The sampler that we’ve passed is just the object that, during the optimization
    process, is going to look for values to try. The one we’ve selected is the default
    one, but we have passed it manually so that we could give it a seed and get reproducible
    results. There are many other samplers. Most notably, `GridSampler` allows you
    to try all the combinations of parameters out of a pre-defined ”search space.”
    For instance, we could use the following sampler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This would make Optuna try out the values ![0.001](img/file1165.png "0.001"),
    ![0.003](img/file1403.png "0.003"), ![0.005](img/file1389.png "0.005"), ![0.008](img/file1404.png
    "0.008"), and ![0.01](img/file1093.png "0.01") — and no others.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about how these samplers work, you may have a look
    at their online documentation ([https://optuna.readthedocs.io/en/stable/reference/samplers/index.html](https://optuna.readthedocs.io/en/stable/reference/samplers/index.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `Study` object ready, all we have to do is call the `optimize` method
    specifying the objective function and the number of trials that we will let Optuna
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon running this (it can take a while), you will get an output similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: With the parameter variations that we have considered, we haven’t seen any significant
    differences in performance. But, still, at least we’ve learned how to use Optuna!
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.2
  prefs: []
  type: TYPE_NORMAL
- en: Use Optuna to simultaneously optimize the learning rate and the batch size of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: As a final remark, notice how, in the objective function, we have used the validation
    accuracy and not the test accuracy. The test dataset, remember, should only be
    used once we’ve already picked our best model. Otherwise, its independence is
    compromised. For instance, if we had saved the models following each Optuna trial,
    now it would make sense for us to compute the test accuracy on the trial 4 model
    in order to make sure that we have a low generalization error.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.3
  prefs: []
  type: TYPE_NORMAL
- en: Optuna can be used on any framework, not just TensorFlow — it can be used to
    optimize any parameters that you want for any purpose! All you have to do is build
    a suitable objective function. To further illustrate this, use Optuna to find
    the minimum of the function ![f(x) = {(x - 3)}^{2}](img/file1405.png "f(x) = {(x
    - 3)}^{2}").
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: In these few pages, we haven’t been able to cover all there is to know about
    Optuna. If you would like to learn more, you should have a look at its online
    documentation. You can find it at [https://optuna.readthedocs.io/en/stable/index.html](https://optuna.readthedocs.io/en/stable/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'That was a short overview of how to train (quantum) machine learning models
    in real-world scenarios. In the following subsection, we will leave our comfort
    zone and use PennyLane to solve a new problem for us: a multi-class classification
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2.4 A multi-class classification problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is going to be an exciting subsection, for we are about to consider a
    new kind of problem on which to apply our QML knowledge. Nonetheless, every long
    journey begins with a first step, and ours shall be to reset the seeds of NumPy
    and TensorFlow, just to make reproducibility easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We are about to consider a multi-class classification problem and, of course,
    the first thing we need is data. Our good old `make_classification` function can
    help us here, for we can give it the optional argument `n_classes` `=` `3` in
    order for it to generate a dataset with ![3](img/file472.png "3") distinct classes,
    which will be labeled as ![0](img/file12.png "0"), ![1](img/file13.png "1"), and
    ![2](img/file302.png "2"). However, there’s a catch. Increasing the number of
    classes means that, as per the function’s requirements, we will also have to tweak
    some of the default parameters; a valid configuration can be reached by setting
    the argument `n_clusters_per_class` to ![1](img/file13.png "1"). Thus, we can
    generate our dataset for ternary classification as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have data, it’s time for us to think about the model. We are approaching
    a new kind of problem, so we need to go back to the basics. For now, let’s forget
    about the hybrid component of the network, and let’s try to think about how we
    could design a QNN capable of solving a ternary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: A general perspective on multi-class classification tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this regard, it is useful to look at how this kind of problem is handled
    with classical neural networks. We know that, when solving binary classification
    problems, we consider neural networks having a single neuron in the final layer
    with a bounded activation function; in this way, we assign a label depending on
    whether the output is closer to ![0](img/file12.png "0") or ![1](img/file13.png
    "1"). Such an approach might not be as effective, in general, when having multiple
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: When working with ![k](img/file317.png "k")-class classification problems, neural
    networks are usually designed to have ![k](img/file317.png "k") neurons in their
    final layer — again, with bounded activation functions that make the values lie
    between ![0](img/file12.png "0") and ![1](img/file13.png "1"). And how is a label
    assigned from the output of these neurons? Easy. Each neuron is associated to
    a label, so we just assign the label of the neuron that has the highest output.
    Heuristically, you may think of each of these ![k](img/file317.png "k") neurons
    in the final layer as light bulbs — whose brightness is determined by their output
    — indicating how likely it is that the input will belong to a certain category.
    All we do in the end is assigning the category of the light bulb that shines the
    most!
  prefs: []
  type: TYPE_NORMAL
- en: Porting this idea to quantum neural networks is easy. Instead of taking the
    expectation value of the observable ![M](img/file704.png "M") on the first qubit,
    we return an array of values with the expectation values of the ![M](img/file704.png
    "M") observable on the first ![k](img/file317.png "k") qubits — assigning to each
    qubit a label. It couldn’t be easier.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to build classifiers in problems with multiple classes.
    For instance, two popular approaches are the **one-versus-all** and **one-versus-one**
    methods. They involve training multiple binary classifiers and combining their
    results. We invite you to have a look at chapter 3 of Geron’s book if you are
    curious [[104](ch030.xhtml#Xhandsonml)].
  prefs: []
  type: TYPE_NORMAL
- en: 'That solves the problem of designing a QNN that can handle our task, but we
    still have an issue left: we don’t yet have a suitable loss function for this
    kind of problem. In binary classification, we could rely on the binary cross-entropy
    function, but it doesn’t work for problems with multiple categories. Luckily for
    us, there’s a loss function that generalizes the binary cross entropy. Please,
    let us introduce you to the **categorical cross-entropy** loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider an arbitrary neural network ![N](img/file784.png "N") that,
    for any choice of parameters ![\theta](img/file89.png "\theta") and any input
    ![x](img/file269.png "x"), returns an array ![N_{\theta}(x)](img/file1406.png
    "N_{\theta}(x)") with ![k](img/file317.png "k") entries, all of them between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"). The categorical cross-entropy loss function
    depends on the parameters of the neural network ![\theta](img/file89.png "\theta"),
    the inputs ![x](img/file269.png "x"), and the targets ![y](img/file270.png "y"),
    but there is an important subtlety: the loss function expects the targets ![y](img/file270.png
    "y") to be in **one-hot** **form**. This means that ![y](img/file270.png "y")
    shouldn’t be a number representing a label (![0,1,\ldots,k - 1](img/file1407.png
    "0,1,\ldots,k - 1")). Instead, it should be a vector (an array) with ![k](img/file317.png
    "k") entries that are all set to ![0](img/file12.png "0") except for the entry
    in the position of the label, which should be set to ![1](img/file13.png "1").
    Thus, instead of having ![y = 0](img/file1408.png "y = 0"), we would have ![y
    = (1,0,\ldots,0)](img/file1409.png "y = (1,0,\ldots,0)"), or, instead of having
    ![y = k - 1](img/file1410.png "y = k - 1"), we would have ![y = (0,\ldots,0,1)](img/file1411.png
    "y = (0,\ldots,0,1)"). Under these assumptions, the categorical cross-entropy
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![H(\theta;x,y) = - \sum\limits_{j = 1}^{k}y_{j}{\log}(N_{\theta}(x)_{j}).](img/file1412.png
    "H(\theta;x,y) = - \sum\limits_{j = 1}^{k}y_{j}{\log}(N_{\theta}(x)_{j}).")'
  prefs: []
  type: TYPE_IMG
- en: Of course, we have used the subindex ![j](img/file258.png "j") in ![y](img/file270.png
    "y") and ![N_{\theta}(x)](img/file1406.png "N_{\theta}(x)") to denote their ![j](img/file258.png
    "j")-th entries. Notice how, in this definition, we have implicitly assumed that
    the first neuron in the final layer is associated to the label ![0](img/file12.png
    "0"), the second neuron is associated to ![1](img/file13.png "1"), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.4
  prefs: []
  type: TYPE_NORMAL
- en: Prove that the binary cross-entropy loss is a particular case of the categorical
    cross-entropy loss for ![k = 2](img/file1413.png "k = 2").
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the categorical cross-entropy function is a reasonable loss function
    for multi-class classification, and it shares some nice properties with the binary
    cross-entropy loss function. For instance, it is zero if a classifier gets an
    output completely right (it assigns ![1](img/file13.png "1") to the correct output
    and ![0](img/file12.png "0") to the rest), but it diverges if a classifier assigns
    ![1](img/file13.png "1") to a wrong output and ![0](img/file12.png "0") to the
    rest.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we already know how to implement our QNN and we have a loss function,
    so we just have to finalize the details of our architecture. Regarding the quantum
    layer, we already know which observable we are going to use, so that’s not a problem.
    For the feature map, we will rely on angular encoding and, for the variational
    form, we shall use the two-local variational form. To keep things somewhat efficient,
    we will take our QNN to have four qubits, and we will leave the rest of the hybrid
    architecture just as it was in the previous subsection.
  prefs: []
  type: TYPE_NORMAL
- en: That’s enough abstract thinking for now; let’s get to the code. And be prepared,
    because things are about to get hot.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a QNN for a ternary classification problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to our plan, the first thing that we need to do is encode our array
    of targets `y` in one-hot form.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.5
  prefs: []
  type: TYPE_NORMAL
- en: There is a variation of the categorical cross entropy loss that doesn’t require
    the targets to be in one-hot form. It is the **sparse** **categorical cross entropy
    loss**. Try to replicate what follows using this loss function and the unencoded
    targets. You may access it as `tf``.``keras``.``losses``.` `SparseCategoricalCrossentropy`.
  prefs: []
  type: TYPE_NORMAL
- en: We could implement our own one-hot encoder, but there’s no need to. The scikit-learn
    package — once again to our rescue! — already implements a `OneHotEncoder` class,
    which you can import from `sklearn``.``preprocessing`. You can work with this
    class just as you would with other familiar scikit-learn classes, such as `MaxAbsScaler`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to one-hot-encode an array of targets, you would need a `OneHotEncoder`
    object and you would just have to pass the array to the `fit_transform` method.
    But with a catch: the array should be a column vector! Our array of targets `y`
    is one-dimensional, so we will have to reshape it before we can feed it to the
    `fit_transform` method. Thus, this is how we can encode our array of targets in
    one-hot form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we have added the argument `sparse` `=` `False`. This Boolean value,
    which defaults to `True`, determines whether or not the encoder should return
    sparse matrices. Sparse matrices are datatypes that can be very memory-efficient
    when storing matrices with lots of zeros, such as one-hot encoded arrays. Essentially,
    instead of logging the value of each entry in a matrix, a sparse matrix only keeps
    track of the non-zero entries in it. When working with very large matrices, it
    can save a ton of memory, but, sadly, using sparse matrices would lead to problems
    in the training, so we need our one-hot encoder to give us an ordinary array.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: 'The neat thing about the `OneHotEncoder` class is that, once we have encoded
    an array of targets with representatives from each class using `fit_transform`,
    we can use the `transform` method on any array of targets. In our case, the `hot`
    object will remember that there are ![3](img/file472.png "3") classes in our dataset,
    and hence `hot``.` `transform` will encode any targets correctly: even if it’s
    given an input with nothing other than zeros, it will still encode them as arrays
    of length ![3](img/file472.png "3").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to do nothing more to our data, so we can now split it into some training,
    validation, and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can now implement the QNN that will constitute the quantum layer of
    our model. In truth, there’s nothing particularly special about this quantum neural
    network other than the fact that it will return an array of values rather than
    a single one. We can define it, according to our previous specification, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The code is pretty self-explanatory. Notice that, as usual, we have taken the
    chance to define the weights dictionary that we will use in the definition of
    the quantum Keras layer. In this case, we will be using ![12](img/file601.png
    "12") weights, exactly as in the case of our model in *Subsection* * [*11.2.2*](#x1-19900011.2.2),
    because we are using the same variational form and the same number of qubits and
    repetitions.*
  prefs: []
  type: TYPE_NORMAL
- en: '*With our QNN ready, we can define the Keras model for our hybrid QNN. This
    is just analogous to what we did in the previous subsection, with a few important
    differences — don’t copy-paste so fast! First of all, in this case, we need to
    set the output dimension of the quantum layer to three, not one. And, much more
    importantly, we need to add an extra activation function on the QNN output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The categorical cross entropy loss function expects probability distributions.
    In principle, it assumes that the output of the ![j](img/file258.png "j")-th neuron
    is the probability that the input belong to category ![j](img/file258.png "j").
    Thus, the data that the model outputs should be normalized: it should add up to
    ![1](img/file13.png "1"). Nevertheless, a priori, there’s no way for us to guarantee
    that the QNN will return some normalized outputs with our current setup. In order
    to ensure this, we may use the **softmax** activation function, which is defined
    as'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\sigma(x_{1},\ldots,x_{n}) = \frac{1}{\sum\limits_{j = 1}^{n}e^{x_{j}}}(e^{x_{1}},\ldots,e^{x_{n}}).](img/file1414.png
    "\sigma(x_{1},\ldots,x_{n}) = \frac{1}{\sum\limits_{j = 1}^{n}e^{x_{j}}}(e^{x_{1}},\ldots,e^{x_{n}}).")
    |'
  prefs: []
  type: TYPE_TB
- en: It’s easy to check that ![\sigma](img/file1415.png "\sigma") is a vector with
    components bounded by ![0](img/file12.png "0") and ![1](img/file13.png "1") which
    add up to ![1](img/file13.png "1") and, hence, is a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these modifications, we will add an extra classical layer with
    ![8](img/file506.png "8") neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: And we can now compile our model with the Adam optimizer and the categorical
    cross-entropy loss before training it with the `fit` method; nothing particularly
    exciting here. As a fun fact, if you were forgetful enough to tell TensorFlow
    to use the binary cross-entropy loss instead of the categorical cross-entropy
    one, it would still use the categorical cross-entropy loss (don’t look at us;
    we don’t say it from experience, right?). This is a rather nice and thoughtful
    feature from the guys behind TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes of training, we may get a plot of the evolution of the
    training and validation losses with the following instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot can be found in *Figure* [*11.2*](#Figure11.2), which shows
    the evolution of both losses.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2: Evolution of the training and validation loss functions in the
    training of a hybrid QNN multi-class classifier ](img/file1416.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.2**: Evolution of the training and validation loss functions in
    the training of a hybrid QNN multi-class classifier'
  prefs: []
  type: TYPE_NORMAL
- en: 'We may now compute the training, validation, and test accuracies of our freshly-trained
    models, but, in order to do so, the `accuracy_score` function needs the predicted
    and actual labels to be represented by numbers, not encoded in one-hot form as
    arrays. Hence, we need to undo the one-hot encoding. For this purpose, we can
    just use the `argmax` method, which returns the entry of the maximum value in
    an array, and it can be given an optional `axis` argument for it to be applied
    only in one axis. Thus, we may compute the accuracy scores as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This returns a training accuracy of ![67\%](img/file1417.png "67\%"), a validation
    accuracy of ![53\%](img/file1418.png "53\%"), and a test accuracy of ![60\%](img/file1419.png
    "60\%"). Notice that the low accuracy on the validation dataset — compared to
    that of the training dataset — seems to indicate an overfitting problem. This
    might be fixed, for example, by using a larger training dataset; of course, this
    would lead to longer training times.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 11.6
  prefs: []
  type: TYPE_NORMAL
- en: Just to further leave our ”classifier comfort zone,” try to implement a hybrid
    model able to do regression. This model should be trained on some data with inputs
    ![x](img/file269.png "x") and target values ![y](img/file270.png "y") for which
    there is a continuous function ![f(x)](img/file800.png "f(x)") such that ![f(x)
    \simeq y](img/file1420.png "f(x) \simeq y") (you can create such a dataset, for
    instance, with the `make_regression` method from scikit-learn). The model should
    try to learn the function ![f](img/file778.png "f") for all the points in the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You may design this model using some classical layers, followed by a quantum
    layer like the ones that we have considered, and a final classical layer with
    no activation functions and just one neuron. You should train it with the mean
    squared error loss.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our study of hybrid architectures in PennyLane. It’s time for
    us to get to Qiskit, and that’s going to be a very different adventure!
  prefs: []
  type: TYPE_NORMAL
- en: 11.3 Hybrid architectures in Qiskit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed how hybrid QNNs could be implemented and
    trained using PennyLane in conjunction with TensorFlow, an ML framework that we
    already know how to use. We will devote this section to studying how to work with
    these hybrid architectures in Qiskit, and in this mission we will need to face
    a new challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'For better or for worse, Qiskit doesn’t have a built-in TensorFlow interface
    at the time of writing. It only has native support for a different ML framework:
    PyTorch. So, if we want to get those hybrid NNs working on Qiskit, we better learn
    a thing or two about PyTorch. As daunting as this task may seem, it won’t be such
    a hassle and it will greatly pay off in the future — and, yes, the future is our
    next chapter on QGANs.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We will be using **version 1.13** of the PyTorch package. If you are using a
    different version, things may be slightly different!
  prefs: []
  type: TYPE_NORMAL
- en: What’s so special about PyTorch to be worth our time beyond this short section?
    Come and see.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.1 Nice to meet you, PyTorch!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have worked with TensorFlow. In our experience, this framework provides
    a very easy and streamlined experience for the implementation and training of
    all sorts of network-based models. However, there’s a small catch behind all that
    ease of use. In this book, we haven’t been using ”pure TensorFlow,” but we have
    been relying heavily on Keras. In spite of being fully integrated into TensorFlow,
    Keras is a component that creates some additional layers of abstraction in order
    to simplify the handling of neural-network models in TensorFlow. All this time,
    Keras has been taking care of lots of things for us behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, there are two very popular ML frameworks out there:
    TensorFlow and PyTorch. The former we already know, the latter we soon will. PyTorch,
    unlike TensorFlow, doesn’t come with its own Keras (although there are some third-party
    packages that provide similar functionalities). In PyTorch, we will have to take
    care of many details ourselves. And that’s great. Granted, learning how to use
    PyTorch will require a tiny bit more effort on our part, but PyTorch will offer
    us a level of flexibility that TensorFlow’s Keras simply can’t. Let’s get started
    then.'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using version 1.13 of the PyTorch package. Please refer to *Appendix*
    [*D*](ch027.xhtml#x1-240000D), *Installing the Tools*, for instructions on how
    to install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we shall begin by importing NumPy and a few utilities from scikit-learn.
    We will also set a seed for NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'With those imports out of the way, we can get to our main dish. This is how
    you can import PyTorch and give it a seed to ensure reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Most functionality related to the implementation of models is in the `torch``.``nn`
    module, and most activation functions can be found in the `torch``.``nn``.``functional`
    module, so let’s import these as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Those are all the imports that we need for now.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a model in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to understand how the PyTorch package works, we will implement and
    train a simple binary classifier as a (classical) neural network. This neural
    network will take ![16](img/file619.png "16") inputs and return a unique output
    between ![0](img/file12.png "0") and ![1](img/file13.png "1"). As usual, the two
    possible labels will be ![0](img/file12.png "0") and ![1](img/file13.png "1")
    and the output label will be decided based on whether the network output is closer
    to ![0](img/file12.png "0") or ![1](img/file13.png "1").
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can implement this neural network classifier. In PyTorch,
    model architectures are defined as subclasses of the `nn``.``Module` class, and
    individual models are objects of these subclasses. When defining subclasses of
    `nn``.``Module`, you should implement an initializer that first calls the parent’s
    initializer and then prepares all the variables of the model architecture; for
    instance, all the network layers should be initialized here. In addition, you
    need to provide a `forward` method that defines the behavior of the network: this
    method should take any input to the network as an argument and return its output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our desired neural network could be implemented as follows (don’t worry, we
    will discuss this piece of code right away):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few things to digest in this implementation. Let us first look
    at the initializer. As expected, we are defining a subclass of `nn``.``Module`
    and we are first calling the parent’s initializer; so far, so good. Then we are
    defining what seem to be the layers of the neural network, and here is where some
    confusion may arise. Our first issue arises from terminology: ”linear layers”
    are PyTorch’s equivalent of Keras’ ”dense” layers — not a big deal. But then we
    have a deeper issue. Back in our Keras days, we defined the layers of a neural
    network by specifying the number of neurons they had and their activation function.
    But here there’s no trace of activation functions and the layers take what seem
    to be two-dimensional arguments. What’s going on?'
  prefs: []
  type: TYPE_NORMAL
- en: In a neural network, you have a bunch of neurons that are arranged into arrays,
    and these arrays are connected by some ”linear wiring” between them. In addition,
    each array of neurons has a (usually non-linear) activation function. In Keras,
    layers were associated to these arrays of neurons themselves (with their activation
    functions) and to the ”linear wiring” before them. In PyTorch, on the other hand,
    when we speak of layers, we only refer to the linear wiring between these arrays
    of neurons. Hence, `nn``.``Linear``(16,` `8)` is nothing more than the linear
    wiring — with its weights and biases — between an array of ![16](img/file619.png
    "16") neurons and an array of ![8](img/file506.png "8") neurons. This will make
    more sense when we look at the `forward` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward` method defines what happens to any input that gets into the network.
    In its implementation, we can see how any input, which will be a PyTorch tensor
    of length ![16](img/file619.png "16"), goes through the first layer. This first
    layer is the ”linear wiring” between an array of ![16](img/file619.png "16") neurons
    and an array of ![8](img/file506.png "8") neurons; it has its own weights ![w_{jk}](img/file1421.png
    "w_{jk}") and biases ![b_{k}](img/file1422.png "b_{k}") and, for any input ![(x_{1},\ldots,x_{16})](img/file1423.png
    "(x_{1},\ldots,x_{16})"), it returns a vector ![({\hat{x}}_{1},\ldots,{\hat{x}}_{8})](img/file1424.png
    "({\hat{x}}_{1},\ldots,{\hat{x}}_{8})") with
  prefs: []
  type: TYPE_NORMAL
- en: '| ![{\hat{x}}_{k} = \sum\limits_{j = 1}^{16}w_{jk}x_{j} + b_{k}.](img/file1425.png
    "{\hat{x}}_{k} = \sum\limits_{j = 1}^{16}w_{jk}x_{j} + b_{k}.") |'
  prefs: []
  type: TYPE_TB
- en: Then, each entry in the resulting tensor goes through the ELU activation function.
    The rest of the code is self-explanatory and simply defines a neural network that
    matches our specifications.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Layers in PyTorch define their own weights and biases. If you wish to remove
    the biases — setting them to zero for all eternity — you may do so by sending
    the optional argument `bias` `=` `False` when initializing a layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our model architecture defined, we can instantiate it into
    an individual model by initializing an object of the `TorchClassifier` class.
    A nice thing about PyTorch models, by the way, is that they can be printed; their
    output gives you an overview of the different model components. Let’s create our
    model object and see this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon running this, we get the following output from the print instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is somewhat analogous to the model summaries that we could print in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the weights and biases of models are random, so our newly-created
    `model` should already be ready to be used. Let’s try it out! The `torch``.``rand`
    function can create a random tensor of any specified size. We will use it to feed
    our model some random data and see if it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output that we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'And there we have it! As expected, our model returns a value between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"). By the way, notice one little thing in the
    output: right next to the tensor value, there is a `grad_fn` value that somehow
    remembers that this output was last obtained from the application of a sigmoid
    function. Interesting, isn’t it? Well, you may remember that TensorFlow used its
    own tensor datatype, and PyTorch has its own tensors too. The cool thing about
    them is that every PyTorch tensor keeps track of how it was computed in order
    to enable gradient computation through backpropagation. We will further discuss
    this later on in this subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, now that our network is all set up, let us generate some data
    and split it into some training, validation, and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Training a model in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In principle, we could work with this raw data just as we did in TensorFlow
    — perhaps converting it to PyTorch tensors, but still. However, we know that PyTorch
    will require us to take care of many things ourselves; one of which will be splitting
    our data into batches should we want to. Doing that ourselves could be tedious
    to say the least. Thankfully, PyTorch comes with some tools that can assist us
    in the process, so we better give them a shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to deal with datasets in PyTorch is by storing data in subclasses
    of a `Dataset` class, which can be found in the `torch``.``utils``.``data` module.
    Any subclasses of `Dataset` should implement an initializer, a `__getitem__` method
    (to access data items by indexing), and a `__len__` method (returning the number
    of items in the dataset). For our purposes, we can create a subclass in order
    to create datasets from our NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we have added some size-checking to ensure that the data array and
    the labels vector have matching dimensions, and how we have reshaped the array
    of targets — that’s in order to avoid problems with the loss functions, which
    expect them to be column vectors. With this class set up, we may create dataset
    objects for the training, validation and test datasets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to check whether our implementation was successful, let us try to access
    the first element in `tr_data` and get the length of the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output returned by these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can see how, indeed, it gave us a tuple with a tensor of length ![16](img/file619.png
    "16") and its corresponding label. Also, a call to the `len` function did return
    the correct number of items in our dataset. Now, you may reasonably wonder why
    we should bother with all this mess of creating dataset classes. There are a couple
    of reasons. For one, this allows us to have our data organized and structured
    in a more orderly manner. What is more, using dataset objects, we can create data
    loaders. The `DataLoader` class can be imported from `torch``.``utils``.``data`
    and its objects allow us to easily iterate through batches of data. An example
    may help clarify this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we want to iterate over the training dataset in batches of ![2](img/file302.png
    "2"). All we would have to do is to create a data loader with the `tr_data` dataset
    specifying the batch size and the fact that we would like it to shuffle the data.
    Then, we could create an iterator object out of the data loader with the `iter`
    function and iterate over all the batches. This is shown in the following piece
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'You may recall from Python 101 that calling `next``(``tr_loader``)` for the
    first time would be equivalent to running a `for` `x` `in` `tr_loader` loop and
    extracting the value of `x` in the first iteration. This is the output that we
    get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: And there you have it! In each iteration of the data loader, we get an array
    with the training data in the batch and its corresponding array of targets. All
    is shuffled and taken care of by PyTorch automatically. Neat, isn’t it? That can
    and will save us a good deal of effort.
  prefs: []
  type: TYPE_NORMAL
- en: We must say that, in truth, you could technically use data loaders without going
    through the whole process of defining datasets — just sending in the numpy arrays.
    But it wouldn’t be the most ”PyTorchy” of practices. Anyhow, this settles our
    preparation of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the training process, we will use, as always, the binary cross-entropy loss.
    We can save its function in a variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the `get_loss` function will take a tensor of values between ![0](img/file12.png
    "0") and ![1](img/file13.png "1") and a matching tensor of labels, and will use
    them to compute the binary cross entropy loss. To see if it works as expected,
    we may compute a simple loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Since the only value in the tensor matches the expected value, we should get
    a loss of ![0](img/file12.png "0") and, indeed, this instruction returns `tensor`
  prefs: []
  type: TYPE_NORMAL
- en: '`(0.)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are already preparing ourselves for the training. In our case, since our
    dataset has ![1000](img/file790.png "1000") elements, it could make sense to use
    a batch size of ![100](img/file389.png "100"), so let us prepare the training
    data loader to that effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we will rely on the Adam optimizer for the training. The optimizer
    is implemented as a class in the `torch``.``optim` module, and, in order to use
    it, we need to specify which parameters it is going to optimize; in our case,
    that will be the parameters in our model, which we can retrieve with the `parameters`
    method. In addition, we can further configure the optimizer by passing optional
    arguments for the learning rate, among other adjustable parameters. We will use
    a learning rate of ![0.005](img/file1389.png "0.005") and trust the default values
    of the remaining parameters. Thus, we can define our optimizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have all the ingredients ready and we can finally get to the training
    itself. In Keras, this would’ve been as easy as calling a method with a bunch
    of parameters, but here we have to work the training out ourselves! We will begin
    by defining a function that will perform one full training epoch. It will be the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is pretty much self-explanatory, but a few details deserve clarification.
    We have used two new methods: `backward` and `step`. Oversimplifying a bit, the
    `backward` method on `loss` computes the gradient of the loss by tracing back
    how it was computed and saving the partial derivatives in the optimizable parameters
    of the model on which the loss depends. This is the famous backpropagation technique
    that we talked about in *Chapter* *[*8*](ch017.xhtml#x1-1390008), *What Is Quantum*
    *Machine Learning?*. Then, `opt``.``step``()` prompts the optimizer to update
    the optimizable parameters using the derivatives that `loss``.``backward``()`
    computed.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To learn more…'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are curious about how differentiation works with the `backward` method
    on PyTorch tensors, we can run a quick example to illustrate. We may define two
    variables, `a` and `b`, taking the values ![2](img/file302.png "2") and ![3](img/file472.png
    "3") respectively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how we set `requires_grad` `=` `True` to tell PyTorch that these are
    variables it should keep track of. We may then define the function ![f(a,b) =
    a^{2} + b](img/file1426.png "f(a,b) = a^{2} + b") and compute its gradient as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We know that ![\left. \partial f\slash\partial a = (\partial\slash\partial a)a^{2}
    + b = 2a \right.](img/file1427.png "\left. \partial f\slash\partial a = (\partial\slash\partial
    a)a^{2} + b = 2a \right."), which in our case is equal to ![2a = 2 \cdot 2 = 4](img/file1428.png
    "2a = 2 \cdot 2 = 4"). When we run the `backward` method, PyTorch has already
    computed this partial derivative for us, and we can access it by calling `a``.``grad`,
    which, as expected, returns `tensor``([4.])`. Analogously, ![\left. \partial f\slash\partial
    b = 1 \right.](img/file1429.png "\left. \partial f\slash\partial b = 1 \right."),
    and, as expected, `b``.``grad` returns `tensor` `([1.])`.
  prefs: []
  type: TYPE_NORMAL
- en: In principle, we could train our model by calling `run_epoch` manually as many
    times as we wanted, but why suffer like that when we can leave Python in charge?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define a training loop in which, at each iteration, we will run an epoch
    and log the training and validation loss obtained over the whole dataset. Instead
    of fixing a specific number of epochs, we will keep iterating until the validation
    loss increases — this will be our own version of the early stopping callback that
    we used in TensorFlow. The following piece of code gets the job done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how, when logging the losses in `tr_losses`, we have converted the PyTorch
    tensors to floats. This is the output that we get after executing this loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'An image is worth a thousand words, so, just to get a visual overview of the
    performance of our training, let us recycle the `plot_losses` function that we
    had for TensorFlow and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot can be found in *Figure* [*11.3*](#Figure11.3). The plot
    does show some signs of overfitting, but likely not something to be concerned
    about; in any case, let’s wait until we get the accuracy over the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3: Evolution of the training and validation losses over the training
    of a classical binary classifier with PyTorch ](img/file1430.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.3**: Evolution of the training and validation losses over the training
    of a classical binary classifier with PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get the accuracy of our classifier on the training, validation,
    and test datasets, we can run the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This returns a training accuracy of ![94\%](img/file1431.png "94\%"), a validation
    accuracy of ![92\%](img/file1432.png "92\%"), and a test accuracy of ![96\%](img/file1402.png
    "96\%").
  prefs: []
  type: TYPE_NORMAL
- en: We have just concluded our not-that-short introduction to PyTorch. Let’s go
    quantum!
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.2 Building a hybrid binary classifier with Qiskit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will implement our first hybrid QNN with Qiskit. The
    process will be fairly straightforward, and we will be able to rely on a good
    deal of the code that we already have. To get started, let us import the Qiskit
    package and the ZZ feature map and two-local variational form that come bundled
    with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'With a QNN, it will be advisable to use smaller datasets in order for the training
    time to be reasonable on our simulators. We can prepare them, along with the corresponding
    dataset and data loader objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Our quantum layer will be a simple ![4](img/file143.png "4")-qubit QNN with
    one instance of the ZZ feature map and the two-local variational form. Thus, the
    components that we will use in our QNN circuit will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have instantiated the two-local form as in *Chapter* [*10*](ch019.xhtml#x1-18100010),
    *Quantum* *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, just as we did in the previous chapter, we could use the `TwoLayerQNN`
    class in order to generate our quantum neural network according to our specifications.
    We may import it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to define our model architecture with PyTorch. Its structure
    will be analogous to that of a classical architecture. The only difference is
    that we will have to define a quantum neural network object in the initializer,
    and we will have to rely on the `TorchConnector` in order to use the QNN in the
    `forward` method. This `TorchConnector` is analogous to the `qml``.``qnn``.``KerasLayer`
    that we used in PennyLane — only that it’s for Qiskit and PyTorch! This is how
    we may then define our hybrid network and instantiate a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we’ve passed the optional argument `input_gradients` `=` `True` to
    the `TwoLayer` initializer; that is required for the PyTorch interface to work
    properly. Apart from that, the construction of the quantum neural network was
    fully analogous to what we did in *Chapter* [*10*](ch019.xhtml#x1-18100010), *Quantum
    Neural Networks*. A detail that perhaps deserves an explanation is the reason
    why we have included a final classical layer after the quantum one. This is because
    our QNN will return values between ![- 1](img/file312.png "- 1") and ![1](img/file13.png
    "1"), not between ![0](img/file12.png "0") and ![1](img/file13.png "1"); by including
    this final layer followed by the classical sigmoid activation function, we can
    ensure that the output of our network will be bounded between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"), as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now all we have left to do before we can start the training is prepare the
    optimizer, and send the model parameters to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can simply reuse the `run_epoch` function to complete the training,
    just as we did in the previous subsection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output that the execution will yield:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we can get a plot of the loss evolution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: This returns the plot shown in *Figure* [*11.4*](#Figure11.4). There does seem
    to be some overfitting, which could likely be fixed by giving more data to the
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4: Evolution of the training and validation losses over the training
    of a hybrid binary classifier with PyTorch ](img/file1433.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 11.4**: Evolution of the training and validation losses over the training
    of a hybrid binary classifier with PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, let’s compute the training, validation, and test accuracies to
    get a better insight into the performance of the classifier. We may do that by
    executing the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Upon running this, we get a training accuracy of ![92\%](img/file1432.png "92\%"),
    a validation accuracy of ![86\%](img/file1434.png "86\%"), and a test accuracy
    of ![74\%](img/file1435.png "74\%"). This confirms our suspicions regarding the
    existence of overfitting. As in other cases, should we want to fix this, we could
    try training the model with additional data, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all that we’ve learned about how to train hybrid QNNs with PyTorch
    and Qiskit also works for ordinary QNNs. If you want to train a simple Qiskit
    QNN using PyTorch, you’ve just learned how to do it; all it will take is defining
    a model with no classical layers.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our study of hybrid neural networks in Qiskit. But we still have
    one thing left before bringing this section to an end.
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of Qiskit is its tight integration with IBM’s quantum
    hardware. Nevertheless, as was the case in our study of quantum optimization,
    queueing times make the training of any QNN model on real hardware unfeasible
    through the usual interfaces to IBM’s hardware — that is, just using a real hardware
    backend, as we discussed in *Chapter* [*2*](ch009.xhtml#x1-400002), *The* *Tools
    of the Trade in Quantum Computing*. Thankfully, there’s a better way.
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.3 Training Qiskit QNNs with Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Qiskit’s Runtime service, as we did in *Chapters* [*5*](ch013.xhtml#x1-940005)
    and [*7*](ch015.xhtml#x1-1190007), we can effectively train any QNN model defined
    in PyTorch through a Qiskit Torch connector on any of the devices and simulators
    provided by IBM Quantum. All it takes is waiting on a single queue, and the whole
    training process is executed as a unit — with all the executions on quantum hardware
    included. The folks at IBM refer to this use case of Qiskit Runtime as ”Torch
    Runtime.”
  prefs: []
  type: TYPE_NORMAL
- en: 'That is very convenient. However, we must warn you that, at the time of writing,
    the queuing times to run these Torch Runtime programs can be somewhat long: around
    the order of a few hours. Also, you should keep in mind that — again, at the time
    of writing — this service enables you to train QNNs defined on PyTorch, but not
    hybrid QNNs! That is, your PyTorch model should not have any classical layers
    whatsoever.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train a simple QNN model on a real device. As usual, we should firstly
    load our IBMQ account and pick a device. We will pick the least busy device among
    all the real devices with at least four qubits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We may define a simple QNN model with the PyTorch connector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we may generate some data on which to train this model using the `make_classification`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we have adjusted some of the parameters of the `make_classification`
    function in order to comply with its requirements (check its documentation at
    [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)
    for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model should return values between ![0](img/file12.png "0") and ![1](img/file13.png
    "1"), but the observable that we have chosen for our circuit — the default one,
    the parity observable (check *Chapter* [*10*](ch019.xhtml#x1-18100010), *Quantum
    Neural Networks*, for reference) — returns two possible values: ![1](img/file13.png
    "1") or ![- 1](img/file312.png "- 1"), not ![0](img/file12.png "0") and ![1](img/file13.png
    "1"). Thus we need to update the targets mapping ![\left. 0\mapsto - 1 \right.](img/file1436.png
    "\left. 0\mapsto - 1 \right.") and ![\left. 1\mapsto 1 \right.](img/file1437.png
    "\left. 1\mapsto 1 \right."). This can be done with the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now set up some data loaders for the training, validation, and test
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'And the only ingredients that we have left to define are the optimizer and
    the loss function. We can still rely on Adam as an optimizer, but the binary cross
    entropy loss will no longer work since our labels are now ![- 1](img/file312.png
    "- 1") and ![1](img/file13.png "1") instead of ![0](img/file12.png "0") and ![1](img/file13.png
    "1"); thus, we will use the mean squared error loss instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to be able to use our model with Torch Runtime, we will have to define
    a Torch Runtime Client, `client`, specifying a few self-explanatory parameters.
    This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: We have set the number of epochs to ![5](img/file296.png "5") in order to get
    some quick results, but feel free to increase it.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now this is the instruction that we need to execute if we want to train
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: This will likely take a while because of the queue time required to run a Torch
    Runtime program. Sit back and relax. Eventually, your model will be trained. Once
    that happens, you can get information about the training from the `result` object,
    whose type is `TorchRuntimeResult`. In particular, the attributes `train_history`
    and `val_history` will show you the evolution of the training and validation losses
    throughout the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’d like to get the model’s prediction on some data — for instance, the
    test dataset — all you have to do is send a data loader object with the data to
    the `predict` method. And this is how you can get your predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t expect to get great results! The model that we have defined is not very
    powerful and we only trained for a few epochs. As if that were not enough, when
    you run on real hardware, there’s always the issue of having to deal with noise.
    Of course, you could use error mitigation as we did back in *Chapter* [*7*](ch015.xhtml#x1-1190007),
    *VQE: Variational Quantum Eigensolver*, by setting `measurement_error_mitigation`
    `=` `True` in the `TorchRuntimeClient` instantiation.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3.4 A glimpse into the future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way in which we have worked with Torch Runtime is supported by IBM at the
    time of writing, but change is the only constant in Qiskit land.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, Torch Runtime will no longer be supported and, instead, it will
    be necessary to use a different interface in order to train quantum neural networks
    with Qiskit Runtime. This interface — which, at the time of writing, is still
    in active development — will rely on the `Sampler` and `Estimator` objects that
    we mentioned in *Section* [*7.3.7*](ch015.xhtml#x1-1320007.3.7). In this subsection,
    we will present to you a simple example that will showcase how to work with this
    new interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following piece of code can be used to train a simple variational quantum
    classifier (a `VQC` object) using the ”new” Qiskit Runtime on the `ibmq_lima`
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Please note that you need to install the `qiskit_ibm_runtime` package (refer
    to *Appendix* *[*D*](ch027.xhtml#x1-240000D), *Installing the Tools*, for instructions)
    and replace `"``TOKEN``"` with your actual IBM Quantum token.*
  prefs: []
  type: TYPE_NORMAL
- en: '*As a matter of fact, when you send a program through this new Qiskit Runtime
    interface, you will likely see a fairly big collection of jobs on your IBM Quantum
    dashboard. Don’t worry, Runtime is working just fine. All those jobs correspond
    to different calls to the quantum computer, but they are all executed without
    the need to wait in the queue after each and every job execution.'
  prefs: []
  type: TYPE_NORMAL
- en: And that’s all we wanted to share with you about the Torch Runtime utility.
    Let’s wrap up this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This has been a long and intense chapter. We began by learning what hybrid
    neural networks actually are and in which use cases they can be useful. We then
    explored how to implement and train these hybrid networks in PennyLane and, along
    the way, we discussed a few good practices that apply to any machine learning
    project. In addition, we left our comfort zone and considered a new kind of QML
    problem: the training of multi-class classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we finished our study of PennyLane, we dived into Qiskit, and a big surprise
    was waiting for us there. Since Qiskit relied on an interface with the PyTorch
    ML package for the implementation of hybrid QNNs, we invested a good deal of effort
    in learning how to use PyTorch. In the process, we saw how PyTorch provided us
    with a level of flexibility that we simply couldn’t get using TensorFlow and Keras.
    At the point where we had a solid understanding of the PyTorch package, we got
    to work with Qiskit and its PyTorch connector and we trained a hybrid QNN with
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we concluded the chapter by fulfilling a promise we made in *Chapter*
    [*10*](ch019.xhtml#x1-18100010), *Quantum Neural Networks*, and we discussed how
    to train quantum neural networks on IBM’s quantum hardware using Torch Runtime.***
  prefs: []
  type: TYPE_NORMAL
