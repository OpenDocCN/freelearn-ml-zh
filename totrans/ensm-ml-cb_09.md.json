["```py\n# Instantiate a sequential model\nseqmodel = Sequential()\n\n# Add layers using the Dense class\nseqmodel.add(Dense8, activation='relu')\n\n# Compile the model\nseqmodel.compile(loss='binary_crossentropy, optimizer='adam', metric=['accuracy'])\n\n# Fit the model\nseqmodel.fit(X_train, Y_train, batch_size=10)\n```", "```py\npip install tensorflow\n```", "```py\npip install tensorflow-gpu\n```", "```py\nsudo pip install keras\n```", "```py\nsudo pip install --upgrade keras\n```", "```py\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n```", "```py\nos.chdir(\"..../Chapter 9\")\nos.getcwd()\n```", "```py\ndf_energydata = pd.read_csv(\"energydata.csv\")\n```", "```py\ndf_energydata.isnull().sum() \n```", "```py\ndf_traindata, df_testdata = train_test_split(df_energydata, test_size=0.3)\n```", "```py\nprint(df_traindata.shape)\nprint(df_testdata.shape)\n```", "```py\nX_test = df_testdata.iloc[:,3:27] \nY_test = df_testdata.iloc[:,28] \n```", "```py\nprint(X_test.shape)\nprint(Y_test.shape)\n```", "```py\nensemble = 20\nfrac = 0.7\n\npredictions_total = np.zeros(5921, dtype=float)\n\nfor i in range(ensemble):\n    print(\"number of iteration:\", i)\n    print(\"predictions_total\", predictions_total)\n\n    # Sample randomly the train data\n    Traindata = df_traindata.sample(frac=frac)\n    X_train = Traindata.iloc[:,3:27] \n    Y_train = Traindata.iloc[:,28] \n\n    ############################################################\n\n    model = Sequential()\n    # Adding the input layer and the first hidden layer\n    model.add(Dense(units=16, kernel_initializer = 'normal', activation = 'relu', input_dim = 24))\n\n    # Adding the second hidden layer\n    model.add(Dense(units = 24, kernel_initializer = 'normal', activation = 'relu'))\n\n    # Adding the third hidden layer\n    model.add(Dense(units = 32, kernel_initializer = 'normal', activation = 'relu'))\n\n    # Adding the output layer\n    model.add(Dense(units = 1, kernel_initializer = 'normal', activation = 'relu'))\n\n    # Compiling the ANN\n    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.9, epsilon=None, decay=0.0)\n    model.compile(loss='mse', optimizer=adam, metrics=['mean_squared_error'])\n    # Fitting the ANN to the Training set\n\n    model.fit(X_train, Y_train, batch_size = 16, epochs = 25)\n\n    ############################################################\n\n    # We use predict() to predict our values\n    model_predictions = model.predict(X_test)\n\n    model_predictions = model_predictions.flatten()\n    print(\"TEST MSE for individual model: \", mean_squared_error(Y_test, model_predictions))\n    print(\"\")\n    print(model_predictions)\n    print(\"\")\n\npredictions_total = np.add(predictions_total, model_predictions)\n```", "```py\npredictions_total = predictions_total/ensemble\nprint(\"MSE after ensemble: \", mean_squared_error(np.array(Y_test), predictions_total))\n```", "```py\n# Example code to set activation function through the activation layer\n\nfrom keras.layers import Activation, Dense \n\nmodel.add(Dense(64)) \nmodel.add(Activation('tanh'))\n```", "```py\nfrom google.colab import drive\n\n# This can be your folder path as per your drive\ndrive.mount('/content/drive')\n```", "```py\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import array\n\nfrom sklearn.metrics import accuracy_score\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation\n\n```", "```py\nfrom google.colab import drive\ndrive.mount('/content/drive')\n```", "```py\nimport h5py\n\n# Open the file as readonly\nh5f = h5py.File('/content/drive/My Drive/DLCP/SVHN_single_grey.h5', 'r')\n```", "```py\n# Load the training and test set\nx_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nx_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\n\n# Close this file\nh5f.close()\n```", "```py\nx_train = x_train.reshape(x_train.shape[0], 1024)\nx_test = x_test.reshape(x_test.shape[0], 1024)\n```", "```py\n# normalize inputs from 0-255 to 0-1\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n```", "```py\nprint(\"X_train shape\", x_train.shape)\nprint(\"y_train shape\", y_train.shape)\nprint(\"X_test shape\", x_test.shape)\nprint(\"y_test shape\", y_test.shape)\n```", "```py\n# Visualizing the 1st 10 images in our dataset\n# along with the labels\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 1))\nfor i in range(10):\n plt.subplot(1, 10, i+1)\n plt.imshow(x_train[i].reshape(32,32), cmap=\"gray\")\n plt.title(y_train[i], color='r')\n plt.axis(\"off\")\nplt.show()\n```", "```py\n# Let us store the original y_test to another variable y_test_actuals\ny_test_actuals = y_test\n\n# one-hot encoding using keras' numpy-related utilities\nn_classes = 10\n\nprint(\"Before one-hot encoding:\")\nprint(\"Shape of Y_TRAIN before one-hot encoding: \", y_train.shape)\nprint(\"Shape of Y_TEST before one-hot encoding: \", y_test.shape)\n\ny_train = np_utils.to_categorical(y_train, n_classes)\ny_test = np_utils.to_categorical(y_test, n_classes)\n\nprint(\"After one-hot encoding:\")\nprint(\"Shape of Y_TRAIN after one-hot encoding: \", y_train.shape)\nprint(\"Shape of Y_TRAIN after one-hot encoding: \", y_test.shape)\n```", "```py\n# building a linear stack of layers with the sequential model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(1024,)))\nmodel.add(Activation('relu')) \n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n```", "```py\n# compiling the sequential model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n```", "```py\n# training the model and saving metrics in history\nsvhn_model = model.fit(x_train, y_train,\n          batch_size=128, epochs=100,\n          verbose=2,\n          validation_data=(x_test, y_test))\n```", "```py\n# plotting the metrics\nfig = plt.figure(figsize=(12,4))\n\n#plt.subplot(2,1,1)\nplt.plot(svhn_model.history['acc'])\nplt.plot(svhn_model.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Test'], loc='uppper left')\n\nplt.tight_layout()\n```", "```py\n# plotting the metrics\nfig = plt.figure(figsize=(12,4))\n\nplt.plot(svhn_model.history['loss'])\nplt.plot(svhn_model.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Train', 'Test'], loc='upper right')\n\nplt.tight_layout()\n```", "```py\n# code from http://scikit-learn.org\ndef plot_confusion_matrix(cm, classes,\nnormalize=False,\ntitle='Confusion matrix',\ncmap=plt.cm.Blues):\n\"\"\"\nThis function prints and plots the confusion matrix.\n\"\"\"\nplt.imshow(cm, cmap=cmap)\nplt.title(title)\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\nplt.text(j, i, cm[i, j],\nhorizontalalignment=\"center\",\ncolor=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel('Actuals')\nplt.xlabel('Predicted')\n```", "```py\ntarget_names = [ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n# Formulating the Confusion Matrix\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_actuals, predicted_classes)\nprint(cm)\n\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cm, classes=target_names, normalize=False)\nplt.show()\n```", "```py\n# fit model on dataset\ndef train_models(x_train, y_train):\n  # building a linear stack of layers with the sequential model\n  model = Sequential()\n  model.add(Dense(512, input_shape=(1024,)))\n  model.add(Activation('relu')) \n  model.add(Dropout(0.2))\n\n  model.add(Dense(512))\n  model.add(Activation('relu'))\n  model.add(Dropout(0.2))\n\n  model.add(Dense(10))\n  model.add(Activation('softmax'))\n\n  # compiling the sequential model\n  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n  # training the model and saving metrics in history\n  svhn_model = model.fit(x_train, y_train, batch_size=32, epochs=25)\n\n  return model\n```", "```py\n# make an ensemble prediction for multi-class classification\ndef ensemble_predictions(models, x_test):\n  # make predictions\n  y_predicted = [model.predict(x_test) for model in models]\n  y_predicted = np.array(y_predicted)\n\n  # sum predictions from all ensemble models\n  predicted_total = np.sum(y_predicted, axis=0)\n\n  # argmax across classes\n  result = np.argmax(predicted_total, axis=1)\n\n  return result\n```", "```py\n# evaluate a specific number of members in an ensemble\ndef evaluate_models(models, no_of_models, x_test, y_test):\n # select a subset of members\n subset = models[:no_of_models]\n\n # make prediction\n y_predicted_ensemble = ensemble_predictions(subset, x_test)\n\n # calculate accuracy\n return accuracy_score(y_test_actuals, y_predicted_ensemble)\n```", "```py\n# fit all models\nno_of_models = 50\n\nmodels = [train_models(x_train, y_train) for _ in range(no_of_models)]\n\n# evaluate different numbers of ensembles\nall_scores = list()\nfor i in range(1, no_of_models+1):\n  score = evaluate_models(models, i, x_test, y_test)\n  print(\"Accuracy Score of model \", i, \" \", score)\n  all_scores.append(score)\n```", "```py\n# plot score vs number of ensemble members\nx_axis = [i for i in range(1, no_of_models+1)]\nplt.plot(x_axis, all_scores)\nplt.show()\n```"]