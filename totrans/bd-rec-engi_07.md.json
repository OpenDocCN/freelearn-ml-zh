["```py\ntar xfvz spark-2.0.0-bin-hadoop2.7.tgz \ncd spark-2.0.0-bin-hadoop2.7\n\n```", "```py\n------------------ \n./bin/run-example org.apache.spark.examples.SparkPi \n16/09/26 15:20:36 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.845103 s \nPi is roughly 3.141071141071141 \n--------------------\n\n```", "```py\n./bin/spark-shell --master local[2]\n\n```", "```py\nspark = SparkSession\\ \n    .builder\\ \n    .appName(\"recommendationEngine\")\\ \n     config(\"spark.some.config.option\", \"some-value\")\\ \n    .getOrCreate() \n\n```", "```py\ncoll = List(\"a\", \"b\", \"c\", \"d\", \"e\") \n\nrdd_from_coll = sc.parallelize(coll) \n\n```", "```py\nrdd_from_Text_File = sc.textFile(\"testdata.txt\") \n\n```", "```py\npyspark\n```", "```py\ndata = sc.textFile(\"~/ml-100k/udata.csv\")\n```", "```py\ntype(data)\n<class 'pyspark.rdd.RDD'>\n\n```", "```py\ndata.count()\n100001\n```", "```py\ndata.first()\n'UserID\\tItemId \\tRating\\tTimestamp'\n\n```", "```py\ndata.take(5)\n['UserID\\tItemId \\tRating\\tTimestamp', '196\\t242\\t3\\t881250949', '186\\t302\\t3\\t891717742', '22\\t377\\t1\\t878887116', '244\\t51\\t2\\t880606923']\n\n```", "```py\nheader = data.first()\n```", "```py\ndata = data.filter(lambda l:l!=header)\n```", "```py\ndata.count()\n100000\n```", "```py\ndata.first()\n'196\\t242\\t3\\t881250949'\n```", "```py\ntype(ratings)\n<class 'pyspark.rdd.PipelinedRDD'>\n```", "```py\nratings.take(5)\n\n[Rating(user=196, product=242, rating=3.0), Rating(user=186, product=302, rating=3.0), Rating(user=22, product=377, rating=1.0), Rating(user=244, product=51, rating=2.0), Rating(user=166, product=346, rating=1.0)]\n```", "```py\ndf.select('user').distinct().show(5)\n```", "```py\ndf.select('user').distinct().count()\n943\n```", "```py\ndf.select('product').distinct().count()\n1682\n```", "```py\ndf.select('product').distinct().show(5)\n```", "```py\ndf.groupBy(\"user\").count().take(5)\n\n[Row(user=26, count=107), Row(user=29, count=34), Row(user=474, count=327), Row(user=191, count=27), Row(user=65, count=80)]\n```", "```py\ndf.groupBy(\"rating\").count().show()\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt\nn_groups = 5 \nx = df.groupBy(\"rating\").count().select('count') \nxx = x.rdd.flatMap(lambda x: x).collect() \nfig, ax = plt.subplots() \nindex = np.arange(n_groups) \nbar_width = 1 \nopacity = 0.4 \nrects1 = plt.bar(index, xx, bar_width, \n                 alpha=opacity, \n                 color='b', \n                 label='ratings') \nplt.xlabel('ratings') \nplt.ylabel('Counts') \nplt.title('Distribution of ratings') \nplt.xticks(index + bar_width, ('1.0', '2.0', '3.0', '4.0', '5.0')) \nplt.legend() \nplt.tight_layout() \nplt.show() \n\n```", "```py\ndf.groupBy(\"UserID\").count().select('count').describe().show()\n```", "```py\n df.stat.crosstab(\"UserID\", \"Rating\").show() \n\n```", "```py\ndf.groupBy('UserID').agg({'Rating': 'mean'}).take(5)\n\n[Row(UserID=148, avg(Rating)=4.0), Row(UserID=463, avg(Rating)=2.8646616541353382), Row(UserID=471, avg(Rating)=3.3870967741935485), Row(UserID=496, avg(Rating)=3.0310077519379846), Row(UserID=833, avg(Rating)=3.056179775280899)] \n\n```", "```py\ndf.groupBy('ItemId ').agg({'Rating': 'mean'}).take(5)\n\n[Row(ItemId =496, avg(Rating)=4.121212121212121), Row(ItemId =471, avg(Rating)=3.6108597285067874), Row(ItemId =463, avg(Rating)=3.859154929577465), Row(ItemId =148, avg(Rating)=3.203125), Row(ItemId =1342, avg(Rating)=2.5)] \n\n```", "```py\ntraining.count()\n80154    \n\n```", "```py\ntest.count()\n19846\n```", "```py\nrank = 10\nnumIterations = 10\n\n```", "```py\n16/10/04 11:01:34 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n16/10/04 11:01:34 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n16/10/04 11:01:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n16/10/04 11:01:34 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 122:\n[rdd_221_0]\n16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 123:\n[rdd_222_0]\n16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 124:\n[rdd_221_0]\n16/10/04 11:01:37 WARN Executor: 1 block locks were not released by TID = 125:\n[rdd_222_0]\n\n```", "```py\nmodel\n\n```", "```py\ntestdata = test.map(lambda p: (p[0], p[1]))\n\ntype(testdata)\n<class 'pyspark.rdd.PipelinedRDD'>\n\n```", "```py\ntest.take(5)\n\n[Rating(user=119, product=392, rating=4.0), Rating(user=38, product=95, rating=5.0), Rating(user=63, product=277, rating=4.0), Rating(user=160, product=234, rating=5.0), Rating(user=225, product=193, rating=4.0)]\n\n```", "```py\ntestdata.take(5)\n\n[(119, 392), (38, 95), (63, 277), (160, 234), (225, 193)]\n\n```", "```py\npred_ind = model.predict(119, 392) \n\n```", "```py\npred_ind \n\n4.3926091845289275 \n\n```", "```py\npredictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2])) \n\n```", "```py\ntype(predictions) \n<class 'pyspark.rdd.PipelinedRDD'> \n\n```", "```py\n predictions.take(5) \n\n[((268, 68), 3.197299431949281), ((200, 68), 3.6296857016488357), ((916, 68), 3.070451877410571), ((648, 68), 2.165520614428771), ((640, 68), 3.821666263132798)] \n\n```", "```py\nrecommedItemsToUsers = model.recommendProductsForUsers(10)\n```", "```py\nrecommedItemsToUsers.count()  \n943\n```", "```py\nrecommedItemsToUsers.take(2)\n\n[\n(96, (Rating(user=96, product=1159, rating=11.251653489172302), Rating(user=96, product=962, rating=11.1500279633824), Rating(user=96, product=534, rating=10.527262244626867), Rating(user=96, product=916, rating=10.066351313580977), Rating(user=96, product=390, rating=9.976996795233937), Rating(user=96, product=901, rating=9.564128162876036), Rating(user=96, product=1311, rating=9.13860044421153), Rating(user=96, product=1059, rating=9.081563794413025), Rating(user=96, product=1178, rating=9.028685203289745), Rating(user=96, product=968, rating=8.844312806737918)\n)),\n (784, (Rating(user=784, product=904, rating=5.975314993539809), Rating(user=784, product=1195, rating=5.888552423210881), Rating(user=784, product=1169, rating=5.649927493462845), Rating(user=784, product=1446, rating=5.476279163198376), Rating(user=784, product=1019, rating=5.303140289874016), Rating(user=784, product=1242, rating=5.267858336331315), Rating(user=784, product=1086, rating=5.264190584020031), Rating(user=784, product=1311, rating=5.248377920702441), Rating(user=784, product=816, rating=5.173286729120303), Rating(user=784, product=1024, rating=5.1253425029498985)\n))\n]\n\n```", "```py\nratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n```", "```py\nMSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n\n[Stage 860:>                                               (0 + 4) / 6]\n\nMean Squared Error = 1.1925845065690288  \n\nfrom math import sqrt\n\nrmse = sqrt(MSE)\nrmse\n1.092055175606539\n\n```", "```py\ntype(ratings) \n<class 'pyspark.rdd.PipelinedRDD'> \n\n```", "```py\nsqlContext \n<pyspark.sql.context.SQLContext object at 0x7f24c94f7d68> \n\n```", "```py\ndf = sqlContext.createDataFrame(ratings) \n\ntype(df) \n\n<class 'pyspark.sql.dataframe.DataFrame'> \n\n```", "```py\ndf.show() \n\n```", "```py\n(training, test) = df.randomSplit([0.8, 0.2]) \n\n```", "```py\nfrom pyspark.ml.recommendation import ALS \n\n```", "```py\nals = ALS(userCol=\"user\", itemCol=\"product\", ratingCol=\"rating\") \nals \n\nALS_45108d6e011beae88f4c \n\n```", "```py\ntype(als) \n<class 'pyspark.ml.recommendation.ALS'> \n\n```", "```py\nals.explainParams()\n\"alpha: alpha for implicit preference (default: 1.0)\\ncheckpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\\nfinalStorageLevel: StorageLevel for ALS model factors. (default: MEMORY_AND_DISK)\\nimplicitPrefs: whether to use implicit preference (default: False)\\nintermediateStorageLevel: StorageLevel for intermediate datasets. Cannot be 'NONE'. (default: MEMORY_AND_DISK)\\nitemCol: column name for item ids. Ids must be within the integer value range. (default: item, current: ItemId )\\nmaxIter: max number of iterations (>= 0). (default: 10)\\nnonnegative: whether to use nonnegative constraint for least squares (default: False)\\nnumItemBlocks: number of item blocks (default: 10)\\nnumUserBlocks: number of user blocks (default: 10)\\npredictionCol: prediction column name. (default: prediction)\\nrank: rank of the factorization (default: 10)\\nratingCol: column name for ratings (default: rating, current: Rating)\\nregParam: regularization parameter (>= 0). (default: 0.1)\\nseed: random seed. (default: -1517157561977538513)\\nuserCol: column name for user ids. Ids must be within the integer value range. (default: user, current: UserID)\"\n\n```", "```py\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=[als])\n type(pipeline)\n<class 'pyspark.ml.pipeline.Pipeline'>\n\n```", "```py\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nparamMapExplicit = ParamGridBuilder() \\\n                    .addGrid(als.rank, [8, 12]) \\\n                    .addGrid(als.maxIter, [10, 15]) \\\n                    .addGrid(als.regParam, [1.0, 10.0]) \\\n                    .build()\n\n```", "```py\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n```", "```py\nevaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n\n```", "```py\ncvExplicit = CrossValidator(estimator=als, estimatorParamMaps=paramMap, evaluator=evaluatorR,numFolds=5)\n\n```", "```py\ncvModel = cvExplicit.fit(training)\n\n[Stage 897:============================>                           (5 + 4) / 10]\n[Stage 938:==================================================>     (9 + 1) / 10]\n[Stage 1004:>(0 + 4) / 10][Stage 1005:> (0 + 0) / 2][Stage 1007:>(0 + 0) / 10]  \n[Stage 1008:>                                                     (3 + 4) / 200]\n\npreds = cvModel.bestModel.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\nrmse = evaluator.evaluate(pred)\nprint(\"Root-mean-square error = \" + str(rmse))\n rmse                                                                        \n\n0.924617823674082\n\n```"]