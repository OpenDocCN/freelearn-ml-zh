- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exporting and Importing Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll discuss how to save and load model parameters during
    and after training. This is important because model training can take days or
    even weeks. Saving intermediate results allows us to load them later for evaluation
    or production use.
  prefs: []
  type: TYPE_NORMAL
- en: Such regular save operations can be beneficial in the case of a random application
    crash. Another substantial feature of any **machine learning** (**ML**) framework
    is its ability to export the model architecture, which allows us to share models
    between frameworks and makes model deployment easier. The main topic of this chapter
    is to show how to export and import model parameters such as weights and bias
    values with different C++ libraries. The second part of this chapter is all about
    the **Open Neural Network Exchange** (**ONNX**) format, which is currently gaining
    popularity among different ML frameworks and can be used to share trained models.
    This format is suitable for sharing model architectures as well as model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: ML model serialization APIs in C++ libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delving into the ONNX format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Dlib` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mlpack` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `F``lashlight` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pytorch` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `onnxruntime` framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modern C++ compiler with C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter12](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter12).'
  prefs: []
  type: TYPE_NORMAL
- en: ML model serialization APIs in C++ libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss the ML model sharing APIs that are available
    in the `Dlib`, `F``lashlight`, `mlpack`, and `pytorch` libraries. There are three
    main types of sharing ML models among the different C++ libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Share model parameters (weights)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share the entire model’s architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share both the model architecture and its trained parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we’ll look at what API is available in each library
    and emphasize what type of sharing it supports.
  prefs: []
  type: TYPE_NORMAL
- en: Model serialization with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Dlib` library uses the serialization API for `decision_function` and neural
    network objects. Let’s learn how to use it by implementing a real example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll define the types for the neural network, regression kernel, and
    training sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we’ll generate the training data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `x` represents the predictor variable, while `y` represents the target
    variable. The target variable, `y`, is salted with uniform random noise to simulate
    real data. These variables have a linear dependency, which is defined with the
    following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve generated the data, we normalize it using the `vector_normalizer`
    type object. Objects of this type can be reused after training to normalize data
    with the learned mean and standard deviation. The following snippet shows how
    it’s implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we train the `decision_function` object for kernel ridge regression
    with the `krr_trainer` type object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that we initialized the trainer object with the instance of the `KernelType`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the trained `decision_function` object, we can serialize it
    into a file with a stream object that’s returned by the `serialize` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function takes the name of the file for storage as an input argument and
    returns an output stream object. We used the `<<` operator to put the learned
    weights of the regression model into the file. The serialization approach we used
    in the preceding code example only saves model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same approach can be used to serialize almost all ML models in the `D``lib`
    library. The following code shows how to use it to serialize the parameters of
    a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For neural networks, there’s also the `net_to_xml` function, which saves the
    model structure. However, there’s no function to load this saved structure into
    our program in the library API. It’s the user’s responsibility to implement a
    loading function.
  prefs: []
  type: TYPE_NORMAL
- en: The `net_to_xml` function exists if we wish to share the model between frameworks,
    as depicted in the `Dlib` documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check that parameter serialization works as expected, we can generate new
    test data to evaluate a loaded model on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’ve reused the `normalizer` object. In general, the parameters of
    the `normalizer` object should also be serialized and loaded because, during evaluation,
    we need to transform new data into the same statistical characteristics that we
    used for the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a serialized object in the `Dlib` library, we can use the `deserialize`
    function. This function takes the filename and returns the input stream object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we discussed previously, in the `Dlib` library, serialization only stores
    model parameters. So, to load them, we need to use the model object with the same
    properties that it had before serialization was performed.
  prefs: []
  type: TYPE_NORMAL
- en: For a regression model, this means that we should instantiate a decision function
    object with the same kernel type.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a neural network model, this means that we should instantiate a network
    object of the same type that we used for serialization, as can be seen in the
    following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we saw that the `Dlib` serialization API allows us to save
    and load ML model parameters but has limited options to serialize and load model
    architectures. In the next section, we’ll look at the `Shogun` library model’s
    serialization API.
  prefs: []
  type: TYPE_NORMAL
- en: Model serialization with Flashlight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Flashlight` library can save and load models and parameters into a binary
    format. It uses the `Cereal` C++ library internally for serialization. An example
    of this functionality is shown in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous example, we’ll start by creating some sample training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created a vector, `x`, with random data and used it to create our target
    variable, `y`, by applying a linear dependency formula. We wrapped our independent
    and target vectors into a `BatchDataset` object called `batch_dataset`, which
    we’ll use to train a sample neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows our neural network definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it’s the same feedforward network that we used in the previous
    example, but this time for Flashlight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows how to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the same training approach that we used previously. First, we
    defined the `loss` object and the `sgd` optimizer object. Then, we used the two
    loops over epochs and over batches to train the model. In the internal loop, we
    applied the model to get new predicted values from training batch data. Then,
    we used the `loss` object to calculate the MSE value with the batch target values.
    We also used the `backward` method of the loss value variable to calculate the
    gradients. Finally, we used the `sgd` optimizer object to update the model parameters
    with the `step` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the trained model, we have two ways to save it in the `F``lashlight`
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: Serialize the whole model with the architecture and weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serialize only the model weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the first option—that is, serialize the whole model with the architecture—we
    can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `model.dat` is the name of the file where we’ll save the model. To load
    such a file, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we created a new empty object called `model_loaded` . This new
    object is just the `fl::Sequential` container object without particular layers.
    All the layers and parameter values were loaded with the `fl::load` function.
    Once we’ve loaded the model, we can use it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, `new_x` is some new data that we’re using for evaluation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach when you store the whole model can be useful for applications
    that contain different models but have the same input and output interfaces as
    it can help you easily change or upgrade a model in production, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second option, which involves only saving the parameter (weight) values
    of a network, can be useful if we need to retrain a model regularly or if we share
    or reuse only some part of the model or its parameters. To do this, we can use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the `params` method of the `model` object to get all the model’s
    parameters. This method returns the `std::vector` sequence of parameters for all
    model sub-modules. So, you can only manage some of them. To load saved parameters,
    we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: First, we created the empty `params` container. Then, with the `fl::load` function,
    we loaded parameter values into it. To be able to update particular sub-module
    parameter values, we used the `setParams` method. The `'setParams'` method takes
    a value and an integer position where we want to set this value. We saved all
    the model parameters so that we could take them back into the model sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there’s no way to load models and weights from other formats
    into the `Flashlight` library. So, if you need to load from another format, you
    have to write a converter and use the `setParams` method to set particular values.
    In the next section, we’ll delve into the `mlpack` library’s serialization API.
  prefs: []
  type: TYPE_NORMAL
- en: Model serialization with mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `mlpack` library only implements model parameter serialization. This serialization
    is based on functionality that exists in the Armadillo math library, which is
    used as the backend for mlpack. This means we can save parameter values in different
    file formats using the mlpack API. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.csv`, or optionally `.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.pgm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.ppm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.bin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.bin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.hdf5`, `.hdf`, `.h5`, or `.he5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at a minimal example of model creation and parameter management
    with mlpack. First, we need a model. The following code shows the function we
    can use to create one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_model` function creates the feedforward network with several linear
    layers. Note that we made this model use `MSE` as the loss function and added
    the zero parameter initializer. Now that we have a model, we need some data to
    train it. The following code shows how to create linear dependent data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created two single-dimensional vectors, similar to what we did for
    the `Flashlight` sample but using the Armadillo matrix API. Notice that we used
    the `t()` transpose method for the `x` vector since mlpack uses the column dimension
    for its training features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can connect all the components and perform model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we created the `Adam` algorithm optimizer object and used it in the model’s
    `Train` method with the two data vectors we created previously. Now, we have the
    trained model and are ready to save its parameters. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `data::Save` function automatically determines the file format
    to save with based on the provided filename extension. Here, we used the `Parameters`
    method of the model object to get the parameter values. This method returns a
    big matrix with all values. We also passed `true` as the third parameter to make
    the `save` function throw an exception in case of failure. By default, it will
    just return `false`; this is something you have to check manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `mlpack::data::Load` function to load parameter values, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created the `new_model` object; this is the same model but with parameters
    initialized as zero. Then, we used the `mlpack::data::Load` function to load parameter
    values from the file. Once again, we used the `Parameters` method to get the reference
    to the internal parameter values matrix and passed it to the `load` function.
    The third argument of the `load` function we set to `true` so that we can get
    an exception in case of errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve initialized the model, we can use it to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created an output matrix, `prediction`, and used the `Predict` method
    of the `new_model` object for model evaluation. Note that `new_x` is some new
    data that we wish to get predictions for.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can’t load other frameworks’ file formats into mlpack, so you
    have to create converters if you need them. In the next section, we’ll look at
    the `pytorch` library’s serialization API.
  prefs: []
  type: TYPE_NORMAL
- en: Model serialization with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss two approaches to network parameter serialization
    that are available in the `pytorch` C++ library:'
  prefs: []
  type: TYPE_NORMAL
- en: The `torch::save` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An object of the `torch::serialize::OutputArchive` type for writing parameters
    into the `OutputArchive` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by preparing the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start by generating the training data. The following code snippet shows
    how we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Usually, we want to utilize as many hardware resources as possible. So, first,
    we checked whether a GPU with CUDA technology was available in the system by using
    the `torch::cuda::is_available()` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined the `dist` object so that we could generate the uniformly distributed
    real values in the `–1` to `1` range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we generated 1,000 predictor variable values and shuffled them. For each
    value, we calculated the target value with the linear function that we used in
    the previous examples—that is, `func`. Here’s what this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, all the values were moved into the `torch::Tensor` objects with `torch::tensor`
    function calls. Notice that we used a previously detected device for tensor creation.
    Once we moved all the values to tensors, we used the `torch::stack` function to
    concatenate the predictor and target values in two distinct single tensors. This
    was required so that we could perform data normalization with the `pytorch` library’s
    linear algebra routines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we used the `torch::mean` and `torch::std` functions to calculate the
    mean and standard deviation of the predictor values and normalized them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we’re defining the `NetImpl` class, which implements
    our neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here, we defined our neural network model as a network with three fully connected
    neuron layers with a linear activation function. Each layer is of the `torch::nn::Linear`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: In the constructor of our model, we initialized all the network parameters with
    small random values. We did this by iterating over all the network modules (see
    the `modules` method call) and applying the `torch::nn::init::normal_` function
    to the parameters that were returned by the `named_parameters()` module’s method.
    Biases were initialized to zeros with the `torch::nn::init::zeros_` function.
    The `named_parameters()` method returned objects consisting of a string name and
    a tensor value, so for initialization, we used its `value` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can train the model with our generated training data. The following
    code shows how we can train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: To utilize all our hardware resources, we moved the model to the selected computational
    device. Then, we initialized an optimizer. In our case, the optimizer used the
    `Adam` algorithm. After, we ran a standard training loop over the epochs where,
    for each epoch, we took the training batch, cleared the optimizer’s gradients,
    performed a forward pass, computed the loss, performed a backward pass, and updated
    the model weights with the optimizer step.
  prefs: []
  type: TYPE_NORMAL
- en: To select a batch of training data from the dataset, we used the tensor’s `narrow`
    method, which returned a new tensor with a reduced dimension. This function takes
    a new number of dimensions as the first parameter, the start position as the second
    parameter, and the number of elements to remain as the third parameter. We also
    used the `unsqueeze` method to add a batch dimension; this is required by the
    PyTorch API for the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, there are two approaches we can use to serialize
    model parameters in `pytorch` in the C++ API (the Python API provides even more
    reach). Let’s look at them.
  prefs: []
  type: TYPE_NORMAL
- en: Using the torch::save and torch::load functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first approach we can take to save model parameters is using the `torch::save`
    function, which recursively saves parameters from the passed module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To use it correctly with our custom modules, we need to register all the sub-modules
    in the parent one with the `register_module` module’s method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the saved parameters, we can use the `torch::load` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The function fills the passed module parameters with the values that are read
    from a file.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch archive objects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second approach is to use an object of the `torch::serialize::OutputArchive`
    type and write the parameters we want to save into it. The following code shows
    how to implement the `SaveWeights` method for our model. This method writes all
    the parameters and buffers that exist in our module to the `archive` object, and
    then it uses the `save_to` method to write them in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: It’s also important to save buffer tensors. Buffers can be retrieved from a
    module with the `named_buffers` module’s method. These objects represent the intermediate
    values that are used to evaluate different modules. For example, we can be running
    mean and standard deviation values for the batch normalization module. In this
    case, we need them to continue being trained if we used serialization to save
    the intermediate steps and if our training process was stopped for some reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load parameters that have been saved this way, we can use the `torch::serialize::InputArchive`
    object. The following code shows how to implement the `LoadWeights` method for
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `LoadWeights` method uses the `load_from` method of the `archive`
    object to load parameters from the file. First, we took the parameters and buffers
    from our module with the `named_parameters` and `named_buffers` methods and filled
    in their values incrementally with the `read` method of the `archive` object.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we used an instance of the `torch::NoGradGuard` class to tell the
    `pytorch` library that we won’t be performing any model calculation or graph-related
    operations. It’s essential to do this because the `pytorch` library’s construct
    calculation graph and any unrelated operations can lead to errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the new instance of our `model_loaded` model with `load` parameters
    to evaluate the model on some test data. Note that we need to switch the model
    to the evaluation model with the `eval` method. Generated test data values should
    also be converted into tensor objects with the `torch::tensor` function and moved
    to the same computational device that our model uses. The following code shows
    how we can implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we looked at two types of serialization in the `pytorch` library.
    The first approach involved using the `torch::save` and `torch::load` functions,
    which easily save and load all the model parameters, respectively. The second
    approach involved using objects of the `torch::serialize::InputArchive` and `torch::serialize::OutputArchive`
    types so that we can select what parameters we want to save and load.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss the ONNX file format, which allows us to
    share our ML model architecture and model parameters among different frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Delving into the ONNX format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ONNX format is a special file format that’s used to share neural network
    architectures and parameters between different frameworks. It’s based on Google’s
    Protobuf format and library. The reason why this format exists is to test and
    run the same neural network model in different environments and on different devices.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, researchers use a programming framework that they know how to use to
    develop a model, and then run this model in a different environment for production
    purposes or if they want to share their model with other researchers or developers.
    This format is supported by all leading frameworks, including PyTorch, TensorFlow,
    MXNet, and others. However, there’s a lack of support for this format from the
    C++ API of these frameworks and at the time of writing, they only have a Python
    interface for dealing with the ONNX format. Despite this, Microsoft provides the
    `onnxruntime` framework to run inference with this format directly with different
    backbends, such as CUDA, CPUs, or even NVIDIA TensorRT.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the specifics of using the framework for our use case, it’s
    important to consider certain limitations so that we can approach the problem
    statement in a well-rounded way. Sometimes, exporting to the ONNX format can be
    problematic due to the lack of certain operators or functions, which can limit
    the types of models that can be exported. Also, there can be limited support for
    dynamic dimensions for tensors and limited support for conditional operators,
    which limits our ability to use models with dynamic computational graphs and implement
    complex algorithms. These limitations depend on the target hardware. You’ll find
    that embedded devices have the most restrictions and that some of these problems
    can only be found in the inference runtime. However, there is one big advantage
    of using ONNX—usually, it’s possible to run such a model on a variety of different
    tensor math acceleration hardware.
  prefs: []
  type: TYPE_NORMAL
- en: TorchScript has fewer limitations for model operators and structure than ONNX.
    It’s usually possible to export models with dynamic computational graphs that
    have been traced with all the required branches. However, there can be restrictions
    on hardware where you’ll have to infer your model. For example, usually, it’s
    not possible to use mobile GPUs or NPUs for inference with TorchScript. ExecuTorch
    should solve this problem in the future.
  prefs: []
  type: TYPE_NORMAL
- en: To utilize the available hardware as much as possible, we can use different
    inference engines from particular vendors. Usually, it’s possible to convert a
    model in the ONNX format or that’s using another method into its internal format
    to perform inference on a specific GPU or NPU. Examples of such engines include
    OpenVINO for Intel hardware, TensorRT from NVIDIA, ArmNN for ARM-based processors,
    and QNN for Qualcomm NPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve understood the best way in which we can utilize the framework,
    let’s understand how to use the ResNet neural network architecture for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Using the ResNet architecture for image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generally, we, as developers, don’t need to know how the ONNX format works
    internally because we’re only interested in the files where the model has been
    saved. As mentioned previously, internally, the ONNX format is a Protobuf-formatted
    file. The following code shows the first part of the ONNX file, which describes
    how to use the ResNet neural network architecture for image classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Usually, ONNX files come in binary format to reduce file size and increase loading
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to use the `onnxruntime` API to load and run ONNX models.
    The ONNX community provides pre-trained models for the most popular neural network
    architectures in the publicly available Model Zoo ([https://github.com/onnx/models](https://github.com/onnx/models)).
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of ready-to-use models that can be used to solve different ML
    tasks. For example, we can use the `ResNet-50` model for image classification
    tasks ([https://github.com/onnx/models/tree/main/validated/vision/classification/resnet/model/resnet50-v1-7.onnx](https://github.com/onnx/models/tree/main/validated/vision/classification/resnet/model/resnet50-v1-7.onnx)).
  prefs: []
  type: TYPE_NORMAL
- en: For this model, we have to download the corresponding `synset` file with image
    class descriptions to be able to return classification results in a human-readable
    manner. You can find the file at [https://github.com/onnx/models/blob/main/validated/vision/classification/synset.txt](https://github.com/onnx/models/blob/main/validated/vision/classification/synset.txt).
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to use the `onnxruntime` C++ API, we have to use the following header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to create the global shared `onnxruntime` environment and a model
    evaluation session, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `session` object takes a model’s filename as its input argument and automatically
    loads it. Here, we passed the name of the downloaded model. The last parameter
    is the `SessionOptions` type object, which can be used to specify a particular
    device executor, such as CUDA. The `env` object holds some shared runtime state.
    The most valuable state is the logging data and the logging level, which can configured
    with a constructor argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve loaded a model, we can access its parameters, such as the number
    of model inputs, the number of model outputs, and parameter names. Such information
    will be very useful if you didn’t know it beforehand because you need input parameter
    names to run inference. We can discover such model information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we created a function header and initialized the memory allocator for
    strings. Now, we can print the input parameter information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we’ve discovered the input parameters, we can print the output parameter
    information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the `session` object to discover model properties. Using the `GetInputCount`
    and `GetOutputCount` methods, we got the number of corresponding input and output
    parameters. Then, we used the `GetInputNameAllocated` and `GetOutputNameAllocated`
    methods to get the parameter names by their indices. Notice that these methods
    require the `allocator` object. Here, we used the default one that was initialized
    at the top of the `show_model_info` function.
  prefs: []
  type: TYPE_NORMAL
- en: We can get the additional parameter type information with the `GetInputTypeInfo`
    and `GetOutputTypeInfo` methods by using their corresponding parameter indices.
    Then, by using these parameter type information objects, we can get the tensor
    information with the `GetTensorTypeAndShapeInfo` method. The most important piece
    of information here is the tensor shape that we got with the `GetShape` method
    of the `tensor_onfo` object. It’s important because we need to use particular
    shapes for the model input and output tensors. The shape is represented as a vector
    of integers. Now, using the `show_model_info` function, we can get model input
    and output parameter information, create the corresponding tensors, and fill them
    with data.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the input is a tensor of size `1 x 3 x 224 x 224`, which represents
    the RGB image for classification. The `onnxruntime` session object takes `Ort::Value`
    type objects as input and fills them as outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows how to prepare the input tensor for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we defined constants that represent an input image’s width and height.
    Then, we created the `input_shape` object, which defines the full shape of the
    tensor, including its batch dimension. With the shape, we created the `input_image`
    vector to hold the exact image data. This data container was filled with the `read_image`
    function, something we’ll take a closer look at shortly. Finally, we created the
    `input_tensor` object with the `Ort::Value::CreateTensor` function, which takes
    the `memory_info` object and the references to the data and shape containers.
    The `memory_info` object was created with parameters to allocate the input tensor
    on the host CPU device. The output tensor can be created in the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `onnxruntime` API allows us to create an empty output tensor
    that will be initialized automatically. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `Run` method for evaluation purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Here, we defined the input and output parameters’ names and constants and created
    the `run_options` object with default initialization. The `run_options` object
    can be used to log verbosity configuration, while the `Run` method can be used
    to evaluate the model. Notice that the input and output tensors were passed as
    pointers to arrays with corresponding element numbers. In our case, we specified
    single input and output elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of this model is image scores (probabilities) for each of the 1,000
    classes of the `ImageNet` dataset, which was used to train the model. The following
    code shows how to decode the model’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Here, we iterated over each element of the result tensor data—that is, the `result`
    vector object we initialized earlier. This `result` object was filled with actual
    data values during model evaluation. Then, we placed the score values and class
    indices in the vector of corresponding pairs. This vector was sorted by score,
    in descending order. Then, we printed five classes with the maximum score.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at an example of how to deal with the ONNX format
    with the `onnxruntime` framework. However, we still need to learn how to load
    input images into tensor objects, something we use for the model’s input.
  prefs: []
  type: TYPE_NORMAL
- en: Loading images into onnxruntime tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s learn how to load image data according to the model’s input requirements
    and memory layout. Previously, we initialized an `input_image` vector of the corresponding
    size. The model expects the input images to be normalized and three-channel RGB
    images whose shapes are `N x 3 x H x W`, where *N* is the batch size and *H* and
    *W* are expected to be at least 224 pixels wide. Normalization assumes that the
    images are loaded into the`[0, 1]` range and then normalized using means equal
    to `[0.485, 0.456, 0.406]` and standard deviations equal to `[0.229,` `0.224,
    0.225]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume that we have the following function definition to load images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s write its implementation. To load images, we’ll use the `OpenCV` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here, we read the image from a file with the `cv::imread` function. If the image
    dimensions aren’t equal to the ones that have been specified, we need to resize
    the image with the `cv::resize` function and then crop the image if the image’s
    dimensions exceed the ones that have been specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must convert the image into the floating-point type and RGB format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once formatting is complete, we can split the image into three separate channels
    with red, green, and blue colors. We should also normalize the color values. The
    following code shows how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Here, each channel was subtracted by the corresponding mean and divided by the
    corresponding standard deviation for the normalization process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we should concatenate the channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the normalized channels were concatenated into one contiguous
    image with the `cv::vconcat` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to copy an OpenCV image into the `image_data`
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, the image data was copied into a vector of floats, which was initialized
    with the specified dimensions. The OpenCV image data was accessed with the `cv::Mat::data`
    type member. We cast the image data into the floating-point type because this
    member variable is of the `unsigned char *` type. The pixel’s data was copied
    with the standard `std::copy_n` function. This function was used to fill the `input_image`
    vector with actual image data. Then, the reference to the `input_image` vector
    data was used in the `CreateTensor` function to initialize the `Ort::Value` object.
  prefs: []
  type: TYPE_NORMAL
- en: Another important function that was used in the ONNX format example was a function
    that can read class definitions from a `synset` file. We’ll take a look at this
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the class definition file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we used the `read_classes` function to load the map of objects.
    Here, the key was an image class index and the value was a textual class description.
    This function is trivial and reads the `synset` file line by line. In such a file,
    each line contains a number and a class description string, separated by a space.
    The following code shows its definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we used the `std::getline` function in the internal `while` loop
    to tokenize a single line string. We did this by specifying the third parameter
    that defines the delimiter character value.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to load the `synset` file, which represents
    the correspondence between class names and their IDs. We used this information
    to map a class ID that we got as a classification result to its string representation,
    which we showed to a user.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to save and load model parameters in different
    ML frameworks. We saw that all the frameworks we used in the `Flashlight`, `mlpack`,
    `Dlib`, and `pytorch` libraries have an API for model parameter serialization.
    Usually, these are quite simple functions that work with model objects and some
    input and output streams. We also discussed the serialization API, which can be
    used to save and load the overall model architecture. At the time of writing,
    some of the frameworks we used don’t fully support such functionality. For example,
    the `Dlib` library can export neural networks in XML format but can’t load them.
    The PyTorch C++ API lacks exporting functionality, but it can load and evaluate
    model architectures that have been exported from the Python API with its TorchScript
    functionality. However, the `pytorch` library does provide access to the library
    API, which allows us to load and evaluate models saved in the ONNX format from
    C++. However, note that you can export a model into the ONNX format from the PyTorch
    Python API that was previously exported into TorchScript and loaded, for example.
  prefs: []
  type: TYPE_NORMAL
- en: We also briefly looked at the ONNX format and realized that it’s quite a popular
    format for sharing models among different ML frameworks. It supports almost all
    operations and objects that are used to serialize complex neural network models
    effectively. At the time of writing, it’s supported by all popular ML frameworks,
    including TensorFlow, PyTorch, MXNet, and others. Also, Microsoft provides the
    ONNX runtime implementation, which allows us to run the ONNX model’s inference
    without having to depend on any other frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we developed a C++ application that can be used
    to run inference on the ResNet-50 model, which was trained and exported in ONNX
    format. This application was made with the onnxruntime C++ API so that we could
    load the model and evaluate it on the loaded image for classification.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll discuss how to deploy ML models that have been developed
    with C++ libraries to mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dlib documentation: [http://Dlib.net/](http://dlib.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch C++ API: [https://pytorch.org/cppdocs/](https://pytorch.org/cppdocs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ONNX official page: [https://onnx.ai/](https://onnx.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ONNX Model Zoo: [https://github.com/onnx/models](https://github.com/onnx/models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ONNX ResNet models for image classification: [https://github.com/onnx/models/blob/main/validated/vision/classification/resnet](https://github.com/onnx/models/blob/main/validated/vision/classification/resnet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onnxruntime` C++ examples: [https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flashlight documentation: [https://fl.readthedocs.io/en/stable/index.html](https://fl.readthedocs.io/en/stable/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'mlpack documentation: [https://rcppmlpack.github.io/mlpack-doxygen/](https://rcppmlpack.github.io/mlpack-doxygen/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
