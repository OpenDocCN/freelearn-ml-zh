- en: '*Chapter 8*: Model Scoring and Deployment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to use outputs generated by DataRobot
    to understand models and why a model provides a particular prediction. We will
    now learn how to use models to score input datasets and create predictions to
    be used in the intended applications. DataRobot automates many tasks that are
    required for scoring and generating row-level explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating predictions, however, is not where these tasks end. In most cases,
    these predictions need to be transformed into actions for consumption by people
    or applications. This mapping of predictions to actions requires an understanding
    of business and therefore needs a person to interpret the results (in most use
    cases). In this chapter, we will discuss how this is done. We''re going to cover
    the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scoring and prediction methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating prediction explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing predictions and postprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying DataRobot models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring deployed models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring and prediction methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataRobot provides multiple methods to score datasets using models that have
    been created. One of the easiest methods is batch scoring via the DataRobot **user
    interface** (**UI**). For this, we need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a file with the dataset to be scored. Given that we are using a public
    dataset, we will simply use the same dataset to score. In a real project, you
    will have access to a new dataset for which you want to create predictions. For
    our purposes, we simply created a copy of our `imports-85-data.xlsx` dataset file
    and named it `imports-85-data-score.xlsx`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's select the **Predict** tab and then the **Test Predictions** tab
    for the **XGBoost** (**XGB**) models, as shown in the following screenshot:![Figure
    8.1 – Batch scoring
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.1_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.1 – Batch scoring
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the preceding screenshot, you will see that you have an option to **drag
    and drop a new dataset** to add the scoring file to the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's select our `imports-85-data-score.xlsx` scoring file and drop it into
    the **Drag and drop a new dataset** box. Once you drop the file, it will get uploaded
    and you can see it in the interface, as shown in the following screenshot:![Figure
    8.2 – Computing predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.2_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.2 – Computing predictions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can now click on the `.csv` file that you can view in Excel, as shown in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Downloaded predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.3_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Downloaded predictions
  prefs: []
  type: TYPE_NORMAL
- en: The downloaded predictions file can now be joined with the original dataset
    for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The second method for scoring a dataset is via the DataRobot batch prediction
    **application programming interface** (**API**), which will be discussed in the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating prediction explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will focus on how to generate explanations along with predictions
    for the scoring dataset. After uploading the scoring dataset (as we discussed
    in the preceding section), you can now go to the **Understand** tab and then select
    the **Prediction Explanations** tab, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Prediction explanations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.4_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Prediction explanations
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, you can see that it now shows the scoring dataset
    that was uploaded. You can now click on the icon next to the dataset filename
    to compute the explanations. Once the computation is complete, you will see the
    download icon. You can use the download icon to download the generated explanations
    for the predictions made by the model. The explanations come in the form of a
    `.csv` file that can be opened using Excel, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Prediction explanations file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.5_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Prediction explanations file
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, we see that the file contains the predictions,
    as well as an explanation for each prediction. For example, if we look at row
    `make` explains 5.17% of the value difference from the base for this automobile.
    Similarly, you can see the relative contribution of each feature value. Notice
    that the features in the file are not sorted by the most important feature and
    also that the most important feature for a given row is not the same as some other
    row. The feature importance will change from row to row.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the predictions and their explanations, let's look at how to
    analyze these and determine how to use them to take actions or make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing predictions and postprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we charge off to deploy the model, it would be advisable to analyze
    the predictions and see if they make sense, whether there are some patterns in
    the errors, and also how to turn the predictions into something actionable. These
    are aspects where traditional data science tools and methods are not of much help,
    and you need to rely on judgment and methods from other disciplines to help formulate
    the next steps. For this, let''s start by combining the scoring dataset file with
    the explanations file. This can be done in **Structured Query Language** (**SQL**),
    Python, or Excel. The combined file looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Combined scoring data and predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.6_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Combined scoring data and predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'We also created a new **ERROR** column that simply subtracts **prediction**
    from **price**. We can now use Excel to create a pivot table and look at the results
    from multiple perspectives. For example, let''s create a pivot table and look
    at the **Average of ERROR** value by **symboling**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Average of ERROR value by symboling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.7_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – Average of ERROR value by symboling
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows that errors are much higher for the value **-2**.
    Looking at the dataset, we find that we have only three data points for **-2**,
    thus it is not a surprise that the model performs poorly. This tells us that we
    cannot trust the results when the **symboling** value is **-2** and that we should
    try to get more data for this value. Analysis such as this can point to areas
    of improvement and where to focus your efforts. We also realize that since this
    is an average error, we should use the average of the absolute percentage value
    of the error to prevent incorrect conclusions, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Average of abs perc ERROR value by symboling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.8_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – Average of abs perc ERROR value by symboling
  prefs: []
  type: TYPE_NORMAL
- en: Now, we see that the absolute percent error decreases as the **symboling** value
    increases. At this point, there is no hard and fast way to find insights except
    exploring the output data and looking at it from different perspectives to see
    what you can find. Typically, it is a good idea to sort the errors and look at
    rows that have unusually large errors, and then see if you can determine why this
    is so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, on to one of the most important aspects of building a data science model—understanding
    which actions to take. Now that we have a reasonable model to predict price, a
    question arises: *What should we do with this information?* Hopefully, the answer
    was determined at the start of the project as to what was the goal of this exercise.
    Let''s assume that the objective is to set the price of a new vehicle by looking
    at the prediction of the model and providing all the parameters such as `engine_size`,
    and so on. We could also imagine that a model such as this could be useful even
    during the design stage when designers are trying to determine trade-offs between
    different parameters such as `bore` or `width`. This goes on to say that a predictive
    model can many times be applied to use cases that were not considered while building
    the model.'
  prefs: []
  type: TYPE_NORMAL
- en: This, however, requires us to understand the broader context of the business
    problem. This is the primary reason we took time to discuss and understand the
    business context in [*Chapter 3*](B17159_03_Final_NM_ePub.xhtml#_idTextAnchor073),
    *Understanding and Defining Business Problems*. It might be useful to revisit
    that chapter to refresh the concepts discussed there as we will use some of the
    techniques that were introduced there, such as causal modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine how we use price prediction, let''s review what we know about
    how price relates to other parameters. In [*Chapter 5*](B17159_05_Final_NM_ePub.xhtml#_idTextAnchor097),
    *Exploratory Data Analysis with DataRobot*, we looked at association analysis
    information. Association strengths using mutual information were generated by
    DataRobot. We can use that information to draw a network graph between different
    features, as shown in the following screenshot. You can do this by drawing a circle
    for each feature, and then creating lines between features that have high association
    strengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Network graph of associations between features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.9_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Network graph of associations between features
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B17159_07_Final_NM_ePub.xhtml#_idTextAnchor110), *Model Understanding
    and Explainability*, we saw the feature importance for price in terms of **SHapley
    Additive exPlanations** (**SHAP**) values is specific to the model we selected.
    The following might represent a causal diagram for this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Causal diagram for the XGB model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.10_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Causal diagram for the XGB model
  prefs: []
  type: TYPE_NORMAL
- en: The left side of the diagram represents the most important features from the
    SHAP values. Let's imagine that the actual price charged is a bit different from
    the prediction. The **Price Delta** feature reflects a decision someone might
    make to charge a price different from the prediction. The **Price** feature impacts
    **Units Sold**, which ultimately affects the profitability. Note that this reflects
    just one possible way of using this model to help make pricing decisions.
  prefs: []
  type: TYPE_NORMAL
- en: If, on the other hand, we imagine that we are trying to help the car design
    team come up with the best car configuration that will also be the most profitable
    one, then we might look at the diagram a bit differently. This is because different
    choices of car or engine design will also impact the cost of the car. Also, we
    know from *Figure 8.9* that the features are not independent. Changing the **bore**
    feature will change the **Engine Size** and the **Horsepower** features. Hence,
    when we are looking into making decisions, we have to think about the causal impacts
    as well. This is a very simplified view, and you can imagine that for a real problem,
    these diagrams will be a lot more complex. Imagine business leaders making those
    decisions by taking into account all of these relationships in their heads. This
    is one of the reasons that many times, models are not used by business users.
  prefs: []
  type: TYPE_NORMAL
- en: In our example problem, the causal diagram shown in *Figure 8.10* is fairly
    simple. You can imagine real-world problems where this diagram will be a lot more
    complex. In such cases, it is very difficult to assess the impact the deployment
    of a model will have on the ecosystem. This includes users and other stakeholders.
    Complex problems tend to have many unanticipated consequences, especially when
    the affected parties are people.
  prefs: []
  type: TYPE_NORMAL
- en: In such situations, if the potential impact may be large, it is advisable to
    test the new model in a synthetic or simulated environment. With the testing and
    impact analyses complete, we are now ready to deploy our model.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying DataRobot models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataRobot makes it pretty easy to deploy the models you have developed. To
    prepare a model for deployment, here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's unlock the project so that we can see the metrics for the holdout datasets,
    as shown in the following screenshot:![Figure 8.11 – Unlocking DataRobot models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.11_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.11 – Unlocking DataRobot models
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see the **Unlock project Holdout for all
    models** option on the right side of the interface.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should unlock the project only after you have selected the model that you
    are choosing for deployment. In our case, we have selected the XGB model that
    uses the **FL1 top23** feature list. Clicking on this option brings up a dialog
    box, as shown in the following screenshot:![Figure 8.12 – Unlocking project holdout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.12_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.12 – Unlocking project holdout
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unlocking the project is an irreversible process. Let's unlock the project and
    see the holdout metrics, as shown in the following screenshot:![Figure 8.13 –
    Unlocked project view
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.13_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.13 – Unlocked project view
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 8.13* shows that the holdout values are higher than the cross-validation
    values, as expected. The holdout values are a better representation of the kind
    of performance you should expect from a model after deployment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the project is unlocked, let's retrain the selected model with 100%
    of the data to improve this model's performance. For that, click on the orange
    **+** sign for the model, as shown in *Figure 8.13*. This will bring up a dialog
    box for changing the sample size, as shown in the following screenshot:![Figure
    8.14 – Defining new sample size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.14_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.14 – Defining new sample size
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the preceding screenshot, you see options to change the sample size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Drag the slider bar all the way to **100%** to indicate that you want to train
    the model with 100% of the data, as shown in the following screenshot:![Figure
    8.15 – Setting new sample size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.15_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.15 – Setting new sample size
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can now click the **Run with new sample size** button. DataRobot will now
    retrain the XGB model with 100% of the data. For the XGB model, you can now click
    on the **Predict** tab and then the **Deploy** tab, as shown in the following
    screenshot:![Figure 8.16 – Deploying a model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.16_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.16 – Deploying a model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, click on the **Deploy model** button. This will bring up a new page, as
    shown in the following screenshot:![Figure 8.17 – Creating a deployment for the
    model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.17_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.17 – Creating a deployment for the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can now give a name to your deployed model. You can also select your prediction
    environment where the deployed model is hosted, as set up by your administrator.
    Under the **Data Drift** section, you can specify if you want to track data drift
    or enable challenger models. You can also enable the storage of prediction rows,
    which allows DataRobot to analyze performance over time. Similarly, you can enable
    the tracking of attributes for segment-based analysis of model performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can now click the `https://app2.datarobot.com`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can now invoke this API to generate predictions. You can also see other
    information about your deployment by clicking on different tabs. If you click
    on the **Service Health** tab, you will see a page like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Service health of deployments'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.19_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.19 – Service health of deployments
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the status of the price prediction model. It
    shows how many predictions have been done, the response time for a prediction,
    and the error rates. The screenshot does not show any values because we just deployed
    this model.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to start monitoring this deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring deployed models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you will have guessed by now, the job of the data science team does not
    end once a model is deployed. We now have to monitor this model to see how it
    is performing, whether it is working as intended, and if we need to intervene
    and make any changes. We''ll proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To see how that works, let's click on the **Predictions** tab, as shown in the
    following screenshot:![Figure 8.20 – Making predictions using the deployed model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.20_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.20 – Making predictions using the deployed model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can now upload a dataset to be scored, by dragging and dropping a file (here,
    we will use the same file that we used before during model training) into the
    **Prediction source** box. We can now see other options becoming available, as
    shown in the following screenshot:![Figure 8.21 – Computing predictions for a
    dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.21_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.21 – Computing predictions for a dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting the options, we can click on the **Compute and download predictions**
    button. After DataRobot finishes the computations, we will see the output file
    becoming available, as shown in the following screenshot:![Figure 8.22 – Downloading
    predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_8.22_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.22 – Downloading predictions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output file can now be downloaded and analyzed. Since we are interested
    in monitoring the model, let''s click on the **Service Health** tab, as shown
    in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Service health of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_8.23_B17159.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 8.23 – Service health of the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can now see that the model has serviced 15 requests with a median response
    time of 325 **milliseconds** (**ms**) and an error rate of 0%. The overall service
    health looks good.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now look at the data drift for the model by clicking on the **Data Drift**
    tab, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Data drift for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.24_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.24 – Data drift for the model
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, at the top of the `price`. If you scroll down
    the page, you will see additional graphs, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Data drift for the model: additional information'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_8.25_B17159.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.25 – Data drift for the model: additional information'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the average prediction values over time. This
    will indicate whether the predictions have been stable or if they have changed
    over time. You will have to rely on your understanding of the business problem
    to determine whether the amount of drift is acceptable or not. DataRobot will
    also give you an indication by showing a red, yellow, or green status. A red status
    would indicate that there is an issue that needs to be resolved; similarly, yellow
    means that you should be aware of potential issues, and green indicates that everything
    looks fine. In general, the issue could be errors in the data pipeline or a change
    in the business environment. A change in the business environment would be an
    indication that the model needs to be retrained.
  prefs: []
  type: TYPE_NORMAL
- en: If the model needs to be retrained or if you need to rebuild the model, you
    can follow the steps that we have outlined in the preceding chapters. This completes
    a basic view of how you use DataRobot to build and deploy a model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use models after training. We discussed the
    methods that are used to score a dataset and also methods that are used for analyzing
    the resulting outputs. We also covered methods and considerations for turning
    predictions into actions or decisions. This is a critical step whereby you have
    to engage with your business stakeholders to make sure that introducing this model
    will not cause unforeseen problems. This is also the time to work on change management
    tasks such as communicating changes to people who are impacted by the change and
    ensure that users are trained in the new process and know how to use the new capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed how to use DataRobot capabilities to rapidly deploy a model
    and then monitor the model performance. It is easy to underestimate the importance
    of this capability. Model deployment and monitoring are not easy, and many organizations
    spend a lot of time and effort trying to deploy a model. Hopefully, we have shown
    how easily this can be accomplished with DataRobot.
  prefs: []
  type: TYPE_NORMAL
- en: We have now completed the basic steps needed to build and deploy a model and
    can now go over some advanced concepts and capabilities of DataRobot. You are
    now ready to dive into advanced topics based on your interest or based on the
    type of project you will be working on. For example, if you are working on a time
    series problem, then you can review [*Chapter 9*](B17159_09_Final_NM_ePub.xhtml#_idTextAnchor125),
    *Forecasting and Time Series Modeling*.
  prefs: []
  type: TYPE_NORMAL
