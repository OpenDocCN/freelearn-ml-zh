- en: Chapter 9. Building Scalable Recommendation Engines with Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that you have just launched an online e-commerce website to sell clothes
    designed by you and you are lucky enough to make your business kick-start well
    and make it a successful venture. With more web traffic coming to your site, the
    most obvious choice is to implement a recommendation engine on your website with
    features such as people who visited something also visited something else, items
    similar to the current item, and so on. Since your website is new and successful,
    you have implemented a recommendation engine using popular tools, such as R and
    Python. The recommendation functionality is deployed and works well, adding more
    value to the success of the business. Now with more business coming in and with
    an increase in your user base, the most likely problem you might face with the
    website is that your customers start complaining that your website is becoming
    slow.
  prefs: []
  type: TYPE_NORMAL
- en: Upon analyzing the root cause, the obvious reason would be that the recommender
    features that are added to the website are slowing down the site. This is bound
    to happen because of the limitation of collaborative filtering algorithms used
    to cater for recommendations. Every time we calculate the similarity between users,
    the entire user base will be loaded into the memory and the similarity values
    would be calculated. This operation will be fast with a small user base. Assume
    that with a large use base, such as one million users, the collaborative filtering
    model will be thrown out of the memory exception. By increasing the RAM capability,
    we might address this to some extent, but it still won't help us. Increasing the
    RAM would be bad idea as it shoots up the infrastructure cost.
  prefs: []
  type: TYPE_NORMAL
- en: The best way is to redesign the recommender engine on a distributed platform,
    such as Hadoop. This is where Apache Mahout will come in handy as it is an open
    source machine learning library built for the distributed platform, Apache Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Mahout general introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Mahout standalone and distributed mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core building blocks of Mahout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and evaluating recommendation engines with Mahout such as user-based
    collaborative filtering, item-based collaborative filtering, SVD recommendation
    engines, and ALS recommendation engines.![Building Scalable Recommendation Engines
    with Mahout](img/image00459.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahout - a general introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Mahout** is an open source java library built on top of Apache Hadoop,
    which provides large-scale machine learning algorithms. Though this library was
    originally started with the MapReduce paradigm, the framework currently offers
    bindings to Apache Spark, H2O, and Apache Flink. The latest version of Mahout
    supports collaborative filtering recommendation engines, clustering, classification,
    dimensionality reduction, H2O, and spark bindings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The major features of Mahout 0.12.2 are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An extensible programming environment and framework for the building of scalable
    algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for Apache Spark, Apache Flink, and H2O algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samsara, a vector Math environment similar to the R programming language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned in the previous section, though many things are possible with Mahout,
    we will be limiting our discussion to building recommendation engines using Mahout.
    Mahout provides support for both the standalone mode, where the recommendation
    model or application can be deployed on a single server, and the distributed mode,
    where the recommendation model can be deployed on a distributed platform.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we shall look at setting up Mahout in standalone and distributed
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: The standalone mode - using Mahout as a library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The standalone mode of Mahout usually involves two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding Mahout libraries to the Java application that wants to use the Mahout
    capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling Mahout recommendation engine functions to build the recommender application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running an application that uses Mahout requires the following dependencies
    to be added to the `pom.xml` file of your Java Maven project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00460.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding dependencies will download all the required jars or libraries
    required to run the Mahout functionalities, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00461.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another step is to go to the official Apache Mahout website and download the
    required Mahout jar files, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest Mahout library can be downloaded from the Apache Mahout official
    website at [http://mahout.apache.org/general/downloads.html](http://mahout.apache.org/general/downloads.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the screenshot of the above mentioned URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00462.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Download the tar file(tar files are just executable) instead of source files,
    as we just need the jar files of Mahout to build recommendation engines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00463.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After downloading the tar file, just extract all the files and add the required
    jars to the Java application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00464.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: With this minimal setup, let's build a very basic recommendation engine using
    Java Eclipse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimal setup just requires the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Java Maven project in Eclipse with the following attribute selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following image shows the screenshot of creating a new Maven project setup
    step 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00465.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'In the following image, add the **Artifact Id** "`recommendations`":'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00466.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: A Maven project will be created with `app.java` as the default class. We can
    make changes in this class to build our standalone recommendation engine:![The
    standalone mode - using Mahout as a library](img/image00467.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set Java runtime as 1.7 or higher, as shown in the next screenshot:![The standalone
    mode - using Mahout as a library](img/image00468.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the required Maven dependencies listed as **mahout-mr**, **mahout-math**,
    **slf4j-log4j**, **commons-math3**, and **guava**; this will download the required
    jars for the application to run, as shown in the following screenshot:![The standalone
    mode - using Mahout as a library](img/image00469.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These dependencies can be seen in the following screenshot:![The standalone
    mode - using Mahout as a library](img/image00470.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder called `data` in the project and create a sample dataset, as
    shown in the following screenshot:![The standalone mode - using Mahout as a library](img/image00471.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now rename `app.java` to the `UserbasedRecommender.java` file. Write the code
    in the java class to build the basic user-based recommender system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding code will generate the recommendations for user 2, as
    shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![The standalone mode - using Mahout as a library](img/image00472.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Boom! We have created our first user-based recommendation engine. Don't worry
    about what we have done or what's happening; everything will become clearer in
    the next few sections. For now, just try to understand how the Mahout library
    can be used in the standalone mode to build recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Mahout for the distributed mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen how to use Mahout libraries in the standalone mode. In this section,
    let''s see how to setup Mahout on a distributed platform, such as HDFS. The following
    are the requirements in order to set up Mahout:'
  prefs: []
  type: TYPE_NORMAL
- en: Java 7 and higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting up Java 7 and installing Hadoop is out of the scope of the is book.
    We can find very good resources online on how to set up Hadoop. Assuming Hadoop
    is already set up, follow these steps to set up Mahout:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and extract the latest Mahout distribution from Apache Mahout website,
    as explained earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up environment values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unset `MAHOUT_LOCAL` in order to run it on the Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once the environment variables are set up, use the following commands in the
    command line to run a recommendation engine on the distributed platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following code, we are generating item-based recommendations using
    the log likelihood similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Mahout for the distributed mode](img/image00473.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Core building blocks of Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like any other recommendation engine framework, Mahout also provides a rich
    set of components to build customized recommender systems that are enterprise-ready,
    scalable, flexible, and that perform well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key components of Mahout are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: DataModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarity: UserSimilarity, ItemSimilarity'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: User neighborhood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommender
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommender evaluator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of a user-based collaborative recommendation engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we shall cover the components required for building a user-based
    collaborative filtering system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Components of a user-based collaborative recommendation engine](img/image00474.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The components of a user-based collaborative recommendation engine are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataModel**: A DataModel implementation allows us to store and provide access
    to the user, item, and preference data required for computation. The DataModel
    component allows us to pull data from the data source. Mahout provides **MySQLJDBCDataModel**,
    which allows us to pull data from the database via JDBC and MySQL. For the purpose
    of our example, we use the **FileDataModel** interface to access data from files
    that Mahout exposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some other DataModels exposed by Mahout are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**HBaseDataModel**: ([http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.html](http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.html))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GenericJDBCDataModel**: ([http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/GenericJDBCDataModel.html](http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/GenericJDBCDataModel.html))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PostgreSQLJDBCDataModel**: ([http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/PostgreSQLJDBCDataModel.html](http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/PostgreSQLJDBCDataModel.html))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDBDataModel**: ([http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.html](http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.html))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahout expects the user data to be in the format of a userID, itemID, preference
    triplet. The preference values can be either continuous or Boolean. Mahout has
    support for both continuous and Boolean preference values. Each input triplet,
    containing userID, itemID, and preference, which we supply to the DataModel, will
    be represented in a memory-efficient **Preference object** or a **PreferenceArray**
    object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**UserSimilarity**: The UserSimilarity interface calculates the similarity
    between two users. The implementations of UserSimilarity return values in the
    range of -1.0 to 1.0 usually, with 1.0 being the perfect similarity. In previous
    chapters, we saw the multiple ways in which we can calculate the similarity between
    users, such as Euclidean Distance, Pearson Coefficient, cosine distance, and so
    on. There are many implementations of the UserSimilarity interface to calculate
    the User Similarity, which are listed as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CachingUserSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CityBlockSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: EuclideanDistanceSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenericUserSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LogLikelihoodSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PearsonCorrelationSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SpearmanCorrelationSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TanimotoCoefficientSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: UncenteredCosineSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ItemSimilarity**: Similar to UserSimilarity, Mahout also provides the ItemSimilarity
    interface, analogous to UserSimilarity, which can be used to calculate the similarity
    between items. The implementations of UserSimilarity return values in the range
    of -1.0 to 1.0 usually, with 1.0 being the perfect similarity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AbstractItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AbstractJDBCItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CachingItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CityBlockSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: EuclideanDistanceSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: FileItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenericItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LogLikelihoodSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQLJDBCInMemoryItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQLJDBCItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PearsonCorrelationSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL92JDBCInMemoryItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL92JDBCItemSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TanimotoCoefficientSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: UncenteredCosineSimilarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UserNeighborhood**: In a user-based recommender, recommendations generated
    for the active user are produced by finding a neighborhood of similar users. UserNeighborhood
    usually refers to a way to determine the neighborhood for a given active user,
    for example, the ten nearest neighbors to take into account while generating recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These neighborhood classes implement the UserSimilarity interface for their
    operation. The following are the implementations of the neighborhood interface:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: CachingUserNeighborhood
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NearestNUserNeighborhood
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ThresholdUserNeighborhood
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender**: A recommender is the core abstraction in Mahout. Given the
    `DataModel` object as the input, it produces recommendations for items to users.
    The implementations of the recommender interface are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AbstractRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CachingRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenericBooleanPrefItemBasedRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenericBooleanPrefUserBasedRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenericItemBasedRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GenericUserBasedRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ItemAverageRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ItemUserAverageRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RandomRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RecommenderWrapper
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SVDRecommender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Building recommendation engines using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have covered the core building blocks of the Mahout recommendation
    engine framework, let's start building recommendations. In this section, we will
    look at a series of different recommendation engines implemented using the standalone
    mode. The recommendation engine capabilities are using implementations of the
    `org.apache.mahout.cf.taste.impl` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommendation engines we see in this section are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: User-based collaborative filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item-based collaborative filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD recommenders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we get into recommender implementations, let''s look at the dataset
    we use in this section. For this section, we use the restaurant and consumer data
    dataset available from the UCI machine learning dataset repository from the following
    URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data](https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data)'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset can be used to build collaborative filtering applications using
    consumer preference information. The dataset, the file downloaded from the previous
    link, contains nine files listed in the following figure. Of all the files in
    this exercise, we use the `rating_final.csv` file, which contains attributes such
    as userID, placeID, rating, food_rating, and service_rating. But for our use cases,
    we only use userID, placeID, and rating. We can think of the data as a preference
    value given to Place by a given user.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will have to make use of the previously created project in the setup session.
  prefs: []
  type: TYPE_NORMAL
- en: Add the input `ratings_final.csv` file to the *data* folder to the current project
    structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'So first, let''s preprocess the original raw data into the required format
    of the userID, placeID, and rating triplet. Here''s the raw dataset used for this
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset description](img/image00475.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following program will prepare the required triplet dataset, implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read each line from the `ratings_final.csv` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the first three columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the extracted columns from the previous step to a new `recoDataset.csv`
    file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following java program implements the previously explained steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon running the preceding java program, the final dataset that we use to build
    recommendation engines will be created under the *data* folder as the `recoDataset.csv`
    file. The following is a sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset description](img/image00476.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have preprocessed the required data, let's start building our recommendation
    engines with the Mahout framework.
  prefs: []
  type: TYPE_NORMAL
- en: User-based collaborative filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just for the sake of a refresher: the user-based recommender system generates
    recommendations based on the UserSimilarity calculation between users and then
    uses UserNeighborhood to choose top-N users and then generate recommendations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first execute the following code and then we shall look at the code
    line by line. We will use the Euclidean Distance similarity and Nearest Neighborhood
    methods to generate recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this program generates recommendations shown in the following figures.
    We are generating the top three user-based item recommendations to `UserId - 1068`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the result, we can conclude that for `UserId - 1068`, the top three recommended
    places along with similarity values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![User-based collaborative filtering](img/image00477.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now look at the code line by line; just recall the core building blocks
    of the Mahout recommendations section. We need DataModel, Similarity calculation,
    UserNeighborhood, recommender, and generating recommendations. This order is used
    in the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in the `UserbasedRecommender.main` method creates a data source from
    the `data/recoDataset.csv` CSV file using `org.apache.mahout.cf.taste.impl.model.file.FileDataModel.FileDataModel
    class`. This class constructor gets the `Java.io.File` instance containing the
    preferences data and creates the `DataModel` class instance model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the UserSimilarity instance: the similarity calculation
    between all users using `org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarity
    class`, which takes the `FileDataModel` instance created in the previous step
    as the constructor parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the UserNeighborhood instance: the neighborhood using
    `org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood` class,
    and it takes three parameters: the number of nearest neighbors to be considered,
    the UserSimilarity instance-similarity, the DataModel instance which is the model
    created in the previous steps as inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next step is to generate a recommender model. This is achieved using the
    `org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender class`
    instance. A `GenericUserBasedRecommender` instance-the recommender is created
    by passing the DataModel instance model, the UserNeighborhood instance neighborhood,
    the UserSimilarity instance similarity as inputs to the constructer while creating
    the recommender object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Kudos! We have created our user-based recommender system using the Euclidean
    Distance similarity and the `NearestNNeighborhhood` method to create a recommender
    model. Now the next step would be to generate recommendations; for this, we call
    the `recommend()` method available in the recommender object, which takes `UserId`
    for which the recommendations and the number of recommendations have to be generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This step has generated three item recommendations to the `UserId` 1068 along
    with the strength of the preference.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we generated the following recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Item-based collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Item-based recommenders recommend similar items to users by considering the
    similarity between items instead of the similarity of users, as shown in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the given java program to build item-based collaborative filtering.
    We have used `LogLikelihoodSimilarity` to calculate `ItemSimilarity`, and then
    we used the `GenericItemBasedRecommender` class to recommend items to users. In
    addition, we can see how to check similar items for a given item using the `mostSimilarItems`
    method present in `GenericItemBasedRecommender`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the previous program will generate three items most similar to the
    input item, in our case, for the placeID `135104`, the most similar placeID attributes
    along with the strength of the similarity is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Item-based collaborative filtering](img/image00478.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at each step of the preceding program in order to understand what''s
    happening in the preceding implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step, like in the previous section, is to create the DataModel instance
    using the `org.apache.mahout.cf.taste.impl.model.file.FileDataModel` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the ItemSimilarity instance, the similarity calculation
    between all users using the `org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity`
    class, which takes the `FileDataModel` instance created in the previous step as
    the constructor parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next step is to generate a recommender model. This is achieved using the
    `org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender` class
    instance. A `GenericItemBasedRecommender` instance recommender is created passing
    the DataModel instance which is the model ItemSimilarity instance-similarity as
    inputs to the constructer while creating the recommender object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The choice of the similarity metric is left to you; it is set as per your requirement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Kudos! We have created our item-based recommender system using the `LogLikelihood`
    similarity to create a recommender model. Now the next step would be to generate
    recommendations, and for this, we call the `recommend()` method available in the
    recommender object, which takes `UserId` for which the recommendations and the
    number of recommendations have to be generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This step has generated three item recommendations to the UserID 1068 along
    with the strength of the preference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In our case, we generated the following recommendations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Imagine that we want to see items similar to a particular item; recommender
    interfaces, such as the `GenericItemBasedRecommender` class in our example, provide
    the `mostSimilarItems()` method, which takes `UserId`, the number of items to
    be displayed as inputs, and extracts `similarItems` for a given item:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, the three places most similar to `PlaceId` 135104 are shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, let's evaluate the recommendations we have created
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how to build recommendations using collaborative filtering approaches.
    But the key thing is to build efficient recommendations. Evaluating the accuracy
    of the recommender models - what we built - is a very crucial step in building
    recommendation engines. In this section, we will look at how to evaluate both
    user-based recommenders and item-based recommenders.
  prefs: []
  type: TYPE_NORMAL
- en: Mahout provides components that enable us to evaluate the accuracy of the recommendation
    models we have built so far. We can evaluate how closely our recommendation engine
    estimates the preferences against the actual preference values. We can instruct
    Mahout to use part of the original training data to set aside and use this test
    dataset in order to calculate the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use any of the following listed recommender evaluators provided by Mahout
    as per our requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating collaborative filtering](img/image00479.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Recommender evaluation using Mahout usually requires two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an instance of the `org.apache.mahout.cf.taste.impl.eval.RMSRecommenderEvaluator`
    class available from the preceding list, which will create the accuracy score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the inner interface for `org.apache.mahout.cf.taste.eval.RecommenderBuilder`
    so as to create the recommender that the `RecommenderEvaluator` class instance
    from the previous step can use to produce the accuracy score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The listing shows the java implementation for user-based recommender model evaluation.
    For this exercise, we have used the root mean squared error evaluation technique.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating user-based recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we shall see the code for evaluating the user-based recommendations
    we built in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the preceding program will give us the model accuracy: `0.692216091226208`.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating item-based recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Below code snippet will be used in evaluating the item-based recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the previous program will give us the model accuracy: `0.6041129199039021`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at this evaluation implementation step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a DataModel instance model using the `org.apache.mahout.cf.taste.impl.model.file.FileDataModel`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the `org.apache.mahout.cf.taste.impl.eval.RMSRecommenderEvaluator`
    instance evaluator, which will calculate the recommendation engine accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we implement the `org.apache.mahout.cf.taste.eval.RecommenderBuilder`
    interface to create the recommender of our choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s use the same recommender models we used for both user-based and item-based
    recommenders in the previous section:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we are ready to calculate the recommendation accuracy. For this, we use
    the `evaluate()` method from the evaluator instance. The `evaluate()` method does
    not accept a recommender instance--which we created in user-based/item-based recommenders
    directly--but it accepts RecommenderBuilder, created in step 3 of our examples/index,
    which can build the recommender to test the accuracy on top of a given DataModel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `evaluate()` method takes four parameters: the recommender builder created
    in step 3, the DataModel object created in step 1, the DataModel builder object
    that we don''t need for our example, the training percentage--in our case, we
    used 0.7 % as the training dataset and 0.3 as the test dataset--, evaluation percentage,
    the percentage of users to be used in evaluation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `evaluate()` method returns the accuracy score of the model, which is how
    well the recommender-predicted preferences match the real values. Lower values
    indicate a better match, with 0 being the perfect match:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: SVD recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the item-based and user-based recommender systems explained earlier,
    we can also use model-based recommender implementations in Mahout, such as `SVDRecommender`,
    which uses matrix factorization methods to generate recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are similar to previous implementations. Two important steps that
    need to be understood here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizer` class,
    which factorizes the user rating matrix using *Alternating-Least-Squares with
    Weighted-λ-Regularization*. The `ALSWRFactorizer` class constructor takes parameters
    such as DataModel, the number of features, the regularization parameter, and the
    number of iterations as inputs. This `ALSWRFactorizer` class instance is passed
    as the input parameter to the recommender object: the `SVDRecommender` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommender` class generates
    the recommendation model by taking `DataModel` and `ALSWRFactorizer` objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The rest of the other steps are very similar to what we saw in the previous
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to build SVD recommender systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Distributed recommendations using Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to now, we have seen how to build recommendation engines in the standalone
    mode. In most cases, the standalone implementations are very handy and they work
    quite efficiently in handling a million records provided we supply the dataset
    format, such as the userID, itemID, and preference triplet.
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the data increases, the standalone mode might not be able to
    address the requirements. We need to look for ways to handle the enormous amount
    of data and be able to process the data to build recommendations. One approach
    is to port our standalone solution to the distributed mode, an example of which
    is Hadoop platforms.
  prefs: []
  type: TYPE_NORMAL
- en: The porting of the recommender solution to Hadoop is not straight forward, as
    the data will be distributed across the nodes. The memory-based models, such as
    neighbourhood recommenders, or model-based recommenders, such as Alternating Least
    Squares, requires the entire data to be available while generating the model,
    which will not be available on a distributed platform. Hence we need an entirely
    new design to build recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, Mahout has removed the headaches in designing recommender implementations
    that can be distributed. These Mahout-distributed recommendation engine implementations
    are provided as jobs that internally run a series of map-reduce phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, Mahout-distributed recommendations using Alternating Least Squares
    consists of two jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: A parallel matrix factorization job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A recommendation job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix factorization job takes the user-item-rating file as the input and
    creates the user latent matrix that is a user feature matrix and an item feature
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation job uses the latent feature matrices created using the matrix
    factorization job and computes Top-N recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: The two jobs are executed sequentially, the input data is read from HDFS, and
    final recommendations are written to HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we shall look at how to generate recommendations using the
    item-based recommendation engine and Alternating Least Squares methods using Hadoop.
    Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: ALS recommendation on Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build the recommendation using ALS implementations, the following are the
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load data to the Hadoop platform. The ALS implementation of Mahout expects
    the input to be a triplet: userID, itemID, and preference value (explicit rating/implicit
    rating).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the ALS recommendation engine implementation job; this job will create
    user and item latent matrices by taking the input dataset from step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the recommender job that takes the user-item latent feature matrices
    created in step 2 and generate Top-N recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's execute all the steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the following exercise, we are using CDH 5 and Centos 6. This is assuming
    `JAVA_HOME` is set and Mahout is installed properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load data to the Hadoop platform as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s check whether we have created the directory properly using the `ls`
    command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s load data to the HDFS using the `copyFromLocal` command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The input data is the MovieLens dataset that consists of one million rating
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s verify that the data is loaded properly using the `ls` command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have seen that the data is loaded properly, let''s look at the
    first few records of the input data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![ALS recommendation on Hadoop](img/image00480.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Create **User** and **Item latent** matrices. To create the latent feature
    matrices, we need to run the following commands from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s look at each of the command parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**$MAHOUT_HOME\bin\mahout**: This is the executable file that runs the underlying
    matrix factorization job.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**parallelALS**: This is the name of the algorithm to be applied on the input
    dataset. The `parallelALS` command invokes the underlying `ParallelALSFactorizationJob
    class object`, which is a map-reduce implementation of the factorization algorithms
    described in *Large-scale Parallel Collaborative Filtering for the Netflix Prize*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--input**: This is the HDFS input path of the input ratings data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--output**: This is the path where the output latent matrices for the user
    and item will be generated.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--lambda**: This is the regularization parameter given in order to avoid
    overfitting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--alpha**: This is the confidence parameter used for implicit feedback only.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--implicitFeatures**: This is the Boolean value to state whether the preference
    values are true or false. In our case, they are false.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--numIterations**: This is the total number of times the model gets recomputed
    by applying the learnings from the previous model to the new model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--tempDir**: This is the path to the temporary directory where the intermediate
    results are written.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On executing the command we saw, three datasets are created in the *output*
    directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**U**: This contains the user latent feature matrix'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M**: The contains the item latent feature matrix'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**userRatings:** All the outputs are of a sequence file format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate recommendations for all the users. This step takes the *output* results
    stored to HDFS from the previous step as the input, generates recommendations,
    and writes the final recommendations to the *recommendations output* directory
    on HDFS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following command will invoke the `org.apache.mahout.cf.taste.hadoop.als.RecommenderJob`
    recommender job, which internally calls an `org.apache.mahout.cf.taste.hadoop.als.PredictionMapper`
    class to generate recommendations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s look at each of the parameters in detail:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**-- input**: This is the HDFS path containing the list of the userID file
    to be used to generate recommendations in the sequence file format. In our example,
    the *output/userRatings* directory contains all the userID to be used to generate
    recommendations in the sequence file format; this file is the output of step 2.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--userFeatures**: This is the HDFS path containing user latent features generated
    as the output in step 2.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--itemFeatures**: This is the HDFS path containing item latent features generated
    as the output in step 2.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--numRecommendations**: The number of recommendations to be generated per
    user.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--output recommendations**: This is the HDFS path where the final recommendations
    have to be generated.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**--maxRating**: This is the maximum rating that the generated recommendations
    should contain.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upon running the previous commands in the command line, recommendations are
    generated into the recommendations folder on HDFS, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ALS recommendation on Hadoop](img/image00481.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the earlier result, we can see the first ten user recommendations in order.
    Each user vector contains itemID and the rating that the algorithm has predicted.
    While serving recommendations, we can just send recommendations as is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you may get a question like this: what if I want to generate recommendations
    to specific users? Mahout supports such scenarios as well. Remember the input
    parameter in step 3? Just provide the HDFS path containing the userID which we
    need for recommendations. But make sure that the input path containing the userID
    are in a sequence file format.'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture for a scalable system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Taking the recommendation engine system to production is the same as any other
    system. The previous figure shows a very simple recommendation engine deployed
    on a production system:'
  prefs: []
  type: TYPE_NORMAL
- en: The production system is Centos 6 with Java 8 and the Apache Tomcat server installed
    on the system. CDH 5 and the Mahout 0.12 version is also installed on it so that
    the recommender jobs we have built so far can be deployed:![The architecture for
    a scalable system](img/image00482.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Java code we have written so far can be made as jar files and deployed on
    the production system. Schedule all the jobs at a regular interval as per our
    requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At a defined scheduled time, the recommender jobs start executing and the data
    is pulled from data sources, computes recommendation models, and generates recommendations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data for the recommendation module will be read and written back to the
    HDFS file system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The frontend applications will read the final outputs from HDFS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to build recommendations using Apache Mahout. We
    looked at how we can leverage Mahout for both the standalone and the distributed
    mode. We have written Java code for user-based, item-based, and SVD-based recommendation
    engines in the standalone mode and Alternating Least Squares recommendations in
    the distributed mode. We also saw how we can evaluate the recommendation engine
    models. In the final section, we explored a very basic system of how to take Mahout
    to production.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we shall cover the future of recommendation engines, where
    the recommendation engines are heading, and the promising use cases to lookout
    for.
  prefs: []
  type: TYPE_NORMAL
