<html><head></head><body><div><h1 class="header-title">Using Decision Trees to Make a Medical Diagnosis</h1>
                
            
            
                
<p>Now that we know how to handle data in all shapes and forms, be it numerical, categorical, text, or image data, it is time to put our newly gained knowledge to good use.</p>
<p>In this chapter, we will learn how to build a machine learning system that can make a medical diagnosis. We aren't all doctors, but we've probably all been to one at some point in our lives. Typically, a doctor would gain as much information as possible about a patient's history and symptoms to make an informed diagnosis. We will mimic a doctor's decision-making process with the help of what is known as <strong>decision trees</strong>. We will also cover the Gini coefficient, information gain, and variance reduction, along with overfitting and pruning.</p>
<p>A decision tree is a simple yet powerful supervised learning algorithm that resembles a flow chart; we will talk more about this in just a minute. Other than in medicine, decision trees are commonly used in fields such as astronomy (for example, for filtering noise from the Hubble Space Telescope images or to classify star-galaxy clusters), manufacturing and production (for example, by Boeing to discover flaws in the manufacturing process), and object recognition (for example, for recognizing 3D objects).</p>
<p>Specifically, we want to learn about the following in this chapter:</p>
<ul>
<li>Building simple decision trees from data and using them for either classification or regression</li>
<li>Deciding which decision to make next using the Gini coefficient, information gain, and variance reduction</li>
<li>Pruning a decision tree and its benefits</li>
</ul>
<p>But first, let's talk about what decision trees actually are.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You can refer to the code for this chapter from the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter05</a>.</p>
<p>Here is a summary of the software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>You will need Python version 3.6 (any Python version 3.x will be fine).</li>
<li>You will need Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any OS—macOS, Windows, and Linux-based OSes, along with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided along with this book.</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding decision trees</h1>
                
            
            
                
<p>A decision tree is a simple yet powerful model for supervised learning problems. As the name suggests, we can think of it as a tree in which information flows along different branches—starting at the trunk and going all of the way to the individual leaves, making decisions about which branch to take at each junction. </p>
<p>This is basically a decision tree! Here is a simple example of a decision tree:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-884 image-border" src="img/10e1cb79-f006-49d7-aab2-6b1b877e39f4.png" style="width:28.92em;height:20.08em;" width="514" height="356"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A decision tree is made of a hierarchy of questions or tests about the data (also known as <strong>decision nodes</strong>) and their possible consequences.</p>
<p>One of the true difficulties with building decision trees is how to pull out suitable features from the data. To make this clear, let's use a concrete example. Let's say we have a dataset consisting of a single email:</p>
<pre>In [1]: data = [<br/>...       'I am Mohammed Abacha, the son of the late Nigerian Head of '<br/>...       'State who died on the 8th of June 1998. Since i have been '<br/>...       'unsuccessful in locating the relatives for over 2 years now '<br/>...       'I seek your consent to present you as the next of kin so '<br/>...       'that the proceeds of this account valued at US$15.5 Million '<br/>...       'Dollars can be paid to you. If you are capable and willing '<br/>...       'to assist, contact me at once via email with following '<br/>...       'details: 1. Your full name, address, and telephone number. '<br/>...       '2. Your Bank Name, Address. 3.Your Bank Account Number and '<br/>...       'Beneficiary Name - You must be the signatory.'<br/>...     ]</pre>
<p>This email can be vectorized in much the same way as we did in the previous chapter, using scikit-learn's <kbd>CountVectorizer</kbd>:</p>
<pre>In [2]: from sklearn.feature_extraction.text import CountVectorizer<br/>... vec = CountVectorizer()<br/>... X = vec.fit_transform(data)</pre>
<p>From the previous chapter, we know that we can have a look at the feature names in <kbd>X</kbd> using the following function:</p>
<pre>In [3]: function:vec.get_feature_names()[:5]<br/>Out[3]: ['15', '1998', '8th', 'abacha', 'account']</pre>
<p>For the sake of clarity, we focus on the first five words only, which are sorted alphabetically. Then, the corresponding number of occurrences can be found as follows:</p>
<pre>In [4]: X.toarray()[0, :5]<br/>Out[4]: array([1, 1, 1, 1, 2], dtype=int64)</pre>
<p>This tells us that four out of five words show up just once in the email, but the word, <kbd>account</kbd> (the last one listed in <kbd>Out[3]</kbd>), actually shows up twice. In the last chapter, we typed <kbd>X.toarray()</kbd> to convert the sparse array, <kbd>X</kbd>, into a human-readable array. The result is a 2D array, where rows correspond to the data samples, and columns correspond to the feature names described in the preceding command. Since there is only one sample in the dataset, we limit ourselves to row 0 of the array (that is, the first data sample) and the first five columns in the array (that is, the first five words).</p>
<p>So, how do we check whether the email is from a Nigerian prince?</p>
<p>One way to do this is to look at whether the email contains both the words, <kbd>nigerian</kbd> and <kbd>prince</kbd>:</p>
<pre>In [5]: 'nigerian' in vec.get_feature_names()<br/>Out[5]: True<br/>In [6]: 'prince' in vec.get_feature_names()<br/>Out[6]: False</pre>
<p>What do we find to our surprise? The word <kbd>prince</kbd> does not occur in the email.</p>
<p>Does this mean the message is legit?</p>
<p>No, of course not. Instead of <kbd>'prince'</kbd>, the email went with the words, <kbd>head of state</kbd>, effectively circumventing our all-too-simple spam detector.</p>
<p>Similarly, how do we even start to model the second decision in the tree: <em>wants me to send him money?</em> There is no straightforward feature in the text that answers this question. Hence, this is a problem of feature engineering, of combining the words that actually occur in the message in such a way that allows us to answer this question. Sure, a good sign would be to look for strings such as <kbd>US$</kbd> and <kbd>money</kbd>, but then we still wouldn't know the context in which these words were mentioned. For all we know, perhaps they were part of the sentence: <em>Don't worry, I don't want you to send me any money.</em></p>
<p>To make matters worse, it turns out that the order in which we ask these questions can actually influence the final outcome. For example, what if we asked the last question first: <em>do I actually know a Nigerian prince?</em> Suppose we had a Nigerian prince for an uncle, then finding the words <em>Nigerian prince</em> in the email might no longer be suspicious.</p>
<p>As you can see, this seemingly simple example got quickly out of hand.</p>
<p>Luckily, the theoretical framework behind decision trees helps us to find both the right decision rules as well as which decisions to tackle next.</p>
<p>However, to understand these concepts, we have to dig a little deeper.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Building our first decision tree</h1>
                
            
            
                
<p>I think we are ready for a more complex example. As promised earlier, let's now move into the medical domain.</p>
<p>Let's consider an example where several patients have suffered from the same illness, such as a rare form of basorexia. Let's further assume that the true causes of the disease remain unknown to this day and that all of the information that is available to us consists of a bunch of physiological measurements. For example, we might have access to the following information:</p>
<ul>
<li>A patient's blood pressure (<kbd>BP</kbd>)</li>
<li>A patient's cholesterol level (<kbd>cholesterol</kbd>)</li>
<li>A patient's gender (<kbd>sex</kbd>)</li>
<li>A patient's age (<kbd>age</kbd>)</li>
<li>A patient's blood sodium concentration (<kbd>Na</kbd>)</li>
<li>A patient's blood potassium concentration (<kbd>K</kbd>)</li>
</ul>
<p>Based on all of ...</p></div>



  
<div><h1 class="header-title">Generating new data</h1>
                
            
            
                
<p>Before proceeding with the further steps, let's quickly understand one very crucial step for every machine learning engineer—data generation. We know that all machine learning and deep learning techniques require a huge amount of data—in simple terms: the bigger, the better. But what if you don't have enough data? Well, you can end up with a model that doesn't have enough accuracy. The common technique employed (if you are not able to generate any new data) is to use the majority of the data for training. The major downside of this is that you have a model that is not generalized or, in other terms, suffers from overfitting. </p>
<p>One solution to deal with the preceding issue is to generate new data or, as it is commonly referred to, synthetic data. The key point to note here is that the synthetic data should have similar features to your real data. The more similar they are to the real data, the better it is for you as an ML engineer. This technique is referred to as <strong>data augmentation</strong>, where we use various techniques such as rotation and mirror images to generate new data that is based upon the existing data. </p>
<p>Since we are dealing with a hypothetical case here, we can write simple Python code to generate random data—since there are no set features for us here. In a real-world case, you would use data augmentation to generate realistic-looking new data samples. Let's see how to approach this for our case.</p>
<p>Here, the dataset is actually a list of dictionaries, where every dictionary constitutes a single data point that contains a patient's blood work, age, and gender, as well as the drug that was prescribed. So, we know that we want to create new dictionaries and we know the keys to use in this dictionary. The next thing to focus on is the data type of the values in the dictionaries. </p>
<p>We start with <kbd>age</kbd>, which is an integer, then we have the gender, which is either <kbd>M</kbd> or <kbd>F</kbd>. Similarly, for other values, we can infer the data types and, in some cases, using common sense, we can even infer the range of the values to use.</p>
<p>It is very important to note that common sense and deep learning don't go well together most of the time. This is because you want your model to understand when something is an outlier. For example, we know that it's highly unlikely for someone to have an age of 130 but a generalized model should understand that this value is an outlier and should not be taken into account. This is why you should always have a small portion of data with such illogical values.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's see how we can generate some synthetic data for our case:</p>
<pre>import random<br/><br/>def generateBasorexiaData(num_entries):<br/>    # We will save our new entries in this list <br/>    list_entries = []<br/>    for entry_count in range(num_entries):<br/>        new_entry = {}<br/>        new_entry['age'] = random.randint(20,100)<br/>        new_entry['sex'] = random.choice(['M','F'])<br/>        new_entry['BP'] = random.choice(['low','high','normal'])<br/>        new_entry['cholestrol'] = random.choice(['low','high','normal'])<br/>        new_entry['Na'] = random.random()<br/>        new_entry['K'] = random.random()<br/>        new_entry['drug'] = random.choice(['A','B','C','D'])<br/>        list_entries.append(new_entry)<br/>    return list_entries</pre>
<p>We can call the preceding function using <kbd>entries = generateBasorexiaData (5)</kbd> if we want to generate five new entries.</p>
<p>Now that we know how to generate the data, let's have a look at what we can do with this data. Can we figure out the doctor's reasoning for prescribing drugs <kbd>A</kbd>, <kbd>B</kbd>, <kbd>C</kbd>, or <kbd>D</kbd>? Can we see a relationship between a patient's blood values and the drug that the doctor prescribed?</p>
<p>Chances are, it is as difficult a question to answer for you as it is for me. Although the dataset might look random at first glance, I have, in fact, put in some clear relationships between a patient's blood values and the prescribed drug. Let's see whether a decision tree can uncover these hidden relationships.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding the task by understanding the data</h1>
                
            
            
                
<p>What is always the first step in tackling a new machine learning problem?</p>
<p>You are absolutely right: getting a sense of the data. The better we understand the data, the better we understand the problem we are trying to solve. In our future endeavors, this will also help us to choose an appropriate machine learning algorithm.</p>

<p>The first thing to realize is that the <kbd>drug</kbd> column is actually not a feature value like all of the other columns. Since it is our goal to predict which drug will be prescribed based on a patient's blood values, the <kbd>drug</kbd> column effectively becomes the target label. In other words, the inputs to our machine learning algorithm will be the blood values, age, and gender of a ...</p></div>



  
<div><h1 class="header-title">Preprocessing the data</h1>
                
            
            
                
<p>For our data to be understood by the decision tree algorithm, we need to convert all categorical features (<kbd>sex</kbd>, <kbd>BP</kbd>, and <kbd>cholesterol</kbd>) into numerical features. What is the best way to do that?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Exactly: we use scikit-learn's <kbd>DictVectorizer</kbd>. Like we did in the previous chapter, we feed the dataset that we want to convert to the <kbd>fit_transform</kbd> method:</p>
<pre>In [10]: from sklearn.feature_extraction import DictVectorizer<br/>...      vec = DictVectorizer(sparse=False)<br/>...      data_pre = vec.fit_transform(data)</pre>
<p>Then, <kbd>data_pre</kbd> contains the preprocessed data. If we want to look at the first data point (that is, the first row of <kbd>data_pre</kbd>), we match the feature names with the corresponding feature values:</p>
<pre>In [12]: vec.get_feature_names()<br/>Out[12]: ['BP=high', 'BP=low', 'BP=normal', 'K', 'Na', 'age',<br/>...       'cholesterol=high', 'cholesterol=normal',<br/>...       'sex=F', 'sex=M']<br/>In [13]: data_pre[0]<br/>Out[13]: array([ 1. , 0. , 0. , 0.06, 0.66, 33. , 1. , 0. ,<br/>                 1. , 0. ])</pre>
<p>From this, we can see that the three categorical variables—blood pressure (<kbd>BP</kbd>), cholesterol level (<kbd>cholesterol</kbd>), and gender (<kbd>sex</kbd>)—have been encoded using one-hot coding.</p>
<p>To make sure that our data variables are compatible with OpenCV, we need to convert everything into floating point values:</p>
<pre>In [14]: import numpy as np<br/>...      data_pre = np.array(data_pre, dtype=np.float32)<br/>...      target = np.array(target, dtype=np.float32)</pre>
<p>Then, all that's left to do is to split the data into training and tests sets, like we did in <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>. Remember that we always want to keep the training and test sets separate. Since we only have 20 data points to work with in this example, we should probably reserve more than 10 percent of the data for testing. A 15-5 split seems appropriate here. We can be explicit and order the <kbd>split</kbd> function to yield exactly five test samples:</p>
<pre>In [15]: import sklearn.model_selection as ms<br/>...      X_train, X_test, y_train, y_test =<br/>...      ms.train_test_split(data_pre, target, test_size=5,<br/>...      random_state=42)</pre>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Constructing the tree</h1>
                
            
            
                
<p>Building the decision tree with OpenCV works in much the same way as in <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>. Recall that all of the machine learning functions reside in OpenCV 3.1's <kbd>ml</kbd> module:</p>
<ol>
<li>We can create an empty decision tree using the following code:</li>
</ol>
<pre style="padding-left: 60px">In [16]: import cv2...      dtree = cv2.ml.dtree_create()</pre>
<ol start="2">
<li>To train the decision tree on the training data, we use the <kbd>train</kbd> method. That's why we converted the data to float before—so that we could use it in the <kbd>train</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [17]: dtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train)</pre>
<p style="padding-left: 60px">Here, we have to specify whether the data samples in <kbd>X_train</kbd> occupy the rows (using <kbd>cv2.ml.ROW_SAMPLE</kbd>) or the columns (<kbd>cv2.ml.COL_SAMPLE</kbd>).</p>
<ol start="3">
<li>Then, we can predict the labels of the ...</li></ol></div>



  
<div><h1 class="header-title">Visualizing a trained decision tree</h1>
                
            
            
                
<p>OpenCV's implementation of decision trees is good enough if you are just starting out and don't care too much about what's going on under the hood. However, in the following sections, we will switch to scikit-learn. Its implementation allows us to customize the algorithm and makes it a lot easier to investigate the inner workings of the tree. Its usage is also much better documented.</p>
<p>In scikit-learn, decision trees can be used for both classification and regression. They reside in the <kbd>tree</kbd> module:</p>
<ol>
<li>Let's first import the <kbd>tree</kbd> module from <kbd>sklearn</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In [21]: from sklearn import tree</pre>
<p class="mce-root"/>
<ol start="2">
<li>Similar to OpenCV, we then create an empty decision tree using the <kbd>DecisionTreeClassifier</kbd> constructor:</li>
</ol>
<pre style="padding-left: 60px">In [22]: dtc = tree.DecisionTreeClassifier()</pre>
<ol start="3">
<li>The tree can then be trained using the <kbd>fit</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [23]: dtc.fit(X_train, y_train)<br/>Out[23]: DecisionTreeClassifier(class_weight=None, criterion='gini',<br/>            max_depth=None, max_features=None, max_leaf_nodes=None,<br/>            min_impurity_split=1e-07, min_samples_leaf=1,<br/>            min_samples_split=2, min_weight_fraction_leaf=0.0,<br/>            presort=False, random_state=None, splitter='best')</pre>
<ol start="4">
<li>We can then compute the accuracy score on both the training and the test sets using the <kbd>score</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [24]: dtc.score(X_train, y_train)<br/>Out[24]: 1.0<br/>In [25]: dtc.score(X_test, y_test)<br/>Out[25]: 0.40000000000000002</pre>
<p style="padding-left: 60px">Now, here's the cool thing: if you want to know what the tree looks like, you can do so using GraphViz to create a PDF file (or any other supported file type) from the tree structure. For this to work, you need to install GraphViz first. Worry not as it is already present in the environment we created at the beginning of this book. </p>
<ol start="5">
<li>Then, back in Python, you can export the tree in GraphViz format to a file, <kbd>tree.dot</kbd>, using scikit-learn's <kbd>export_graphviz</kbd> exporter:</li>
</ol>
<pre style="padding-left: 60px">In [26]: with open("tree.dot", 'w') as f:<br/>... tree.export_graphviz(clf, out_file=f)</pre>
<ol start="6">
<li>Then, back on the command line, you can use GraphViz to turn <kbd>tree.dot</kbd> into (for example) a PNG file:</li>
</ol>
<pre style="padding-left: 60px">$ dot -Tpng tree.dot -o tree.png</pre>
<p>Alternatively, you can also specify <kbd>-Tpdf</kbd> or any other supported image format. The result for the preceding tree looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-890 image-border" src="img/3fa7b1d2-d71b-4531-b853-c1d3e05174cc.png" style="width:38.83em;height:27.00em;" width="743" height="517"/></p>
<p>What does this all mean? Let's break the diagram down step by step.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Investigating the inner workings of a decision tree</h1>
                
            
            
                
<p>We established earlier that a decision tree is basically a flow chart that makes a series of decisions about the data. The process starts at the root node (which is the node at the very top), where we split the data into two groups (only for binary trees), based on some decision rule. Then, the process is repeated until all remaining samples have the same target label, at which point we have reached a leaf node.</p>

<p>In the spam filter example earlier, decisions were made by asking true/false questions. For example, we asked whether an email contained a certain word. If it did, we followed the edge labeled true and asked the next question. However, this works not just for categorical features, ...</p></div>



  
<div><h1 class="header-title">Rating the importance of features</h1>
                
            
            
                
<p>What I haven't told you yet is how you pick the features along which to split the data. The preceding root node split the data according to <em>Na &lt;= 0.72,</em> but who told the tree to focus on sodium first? Also, where does the number 0.72 come from anyway?</p>
<p>Apparently, some features might be more important than others. In fact, scikit-learn provides a function to rate feature importance, which is a number between <em>0</em> and <em>1</em> for each feature, where <em>0</em> means <em>not used at all in any decisions made</em> and <em>1</em> means <em>perfectly predicts the target</em>. The feature importances are normalized so that they all sum to 1:</p>
<pre>In [27]: dtc.feature_importances_<br/>Out[27]: array([ 0.        , 0.   , 0.        , 0.13554217, 0.29718876,<br/>                 0.24096386, 0.   , 0.32630522, 0.        , 0. ])</pre>
<p class="mce-root"/>
<p>If we remind ourselves of the feature names, it will become clear which feature seems to be the most important. A plot might be most informative:</p>
<pre>In [28]: plt.barh(range(10), dtc.feature_importances_, align='center',<br/>...      tick_label=vec.get_feature_names())</pre>
<p>This will result in the following bar plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-892 image-border" src="img/448d58c4-1960-44f1-847b-e1e2552d81d9.png" style="width:94.08em;height:44.33em;" width="1129" height="532"/></p>
<p>Now, it becomes evident that the most important feature for knowing which drug to administer to patients was actually whether the patient had a normal cholesterol level. Age, sodium levels, and potassium levels were also important. On the other hand, gender and blood pressure did not seem to make any difference at all. However, this does not mean that gender or blood pressure are uninformative. It only means that these features were not picked by the decision tree, likely because another feature would have led to the same splits.</p>
<p>But, hold on. If cholesterol level is so important, why was it not picked as the first feature in the tree (that is, in the root node)? Why would you choose to split on the sodium level first? This is where I need to tell you about that ominous <kbd>gini</kbd> label in the diagram earlier.</p>
<p>Feature importances tell us which features are important for classification, but not which class label they are indicative of. For example, we only know that the cholesterol level is important, but we don't know how that led to different drugs being prescribed. In fact, there might not be a simple relationship between features and classes.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding the decision rules</h1>
                
            
            
                
<p>To build the perfect tree, you would want to split the tree at the most informative feature, resulting in the purest daughter nodes. However, this simple idea leads to some practical challenges:</p>
<ul>
<li>It's not actually clear what most informative means. We need a concrete value, a score function, or a mathematical equation that can describe how informative a feature is.</li>
<li>To find the best split, we have to search over all of the possibilities at every decision node.</li>
</ul>
<p>Fortunately, the decision tree algorithm actually does these two steps for you. Two of the most commonly used criteria that scikit-learn supports are the following:</p>
<ul>
<li><kbd>criterion='gini'</kbd>: The Gini impurity is a measure of misclassification, with the aim ...</li></ul></div>



  
<div><h1 class="header-title">Controlling the complexity of decision trees</h1>
                
            
            
                
<p>If you continue to grow a tree until all leaves are pure, you will typically arrive at a tree that is too complex to interpret. The presence of pure leaves means that the tree is 100 percent correct on the training data, as was the case with our tree shown earlier. As a result, the tree is likely to perform very poorly on the test dataset, as was the case with our tree shown earlier. We say the tree overfits the training data.</p>
<p>There are two common ways to avoid overfitting:</p>
<ul>
<li><strong>Pre-pruning</strong>: This is the process of stopping the creation of the tree early.</li>
<li><strong>Post-pruning</strong> <strong>(or just pruning)</strong>: This is the process of first building the tree but then removing or collapsing nodes that contain only a little information.</li>
</ul>
<p>There are several ways to pre-prune a tree, all of which can be achieved by passing optional arguments to the <kbd>DecisionTreeClassifier</kbd> constructor:</p>
<ul>
<li>Limiting the maximum depth of the tree via the <kbd>max_depth</kbd> parameter</li>
<li>Limiting the maximum number of leaf nodes via <kbd>max_leaf_nodes</kbd></li>
<li>Requiring a minimum number of points in a node to keep splitting it via <kbd>min_samples_split</kbd></li>
</ul>
<p>Often pre-pruning is sufficient to control overfitting.</p>
<p>Try it out on our toy dataset! Can you get the score on the test set to improve at all? How does the tree layout change as you start playing with the earlier parameters?</p>
<p>In more complicated real-world scenarios, pre-pruning is no longer sufficient to control overfitting. In such cases, we want to combine multiple decision trees into what is known as a <strong>random forest</strong>. We will talk about this in <a href="10a94a5b-e700-4fae-b9f6-f6b77a1580f6.xhtml" target="_blank">Chapter 10</a>, <em>Ensemble Methods for Classification</em>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using decision trees to diagnose breast cancer</h1>
                
            
            
                
<p>Now that we have built our first decision tree, it's time to turn our attention to a real dataset: the Breast Cancer Wisconsin dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)</a>).</p>
<p>This dataset is a direct result of medical imaging research and is considered a classic today. The dataset was created from digitized images of healthy (benign) and cancerous (malignant) tissues. Unfortunately, I wasn't able to find any public-domain examples from the original study, but the images look similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-893 image-border" src="img/dafbad1a-c5df-4a4e-b9ec-e0f78e3d247d.png" style="width:108.33em;height:58.08em;" width="1300" height="697"/></p>
<p>The goal of the research was to classify tissue ...</p></div>



  
<div><h1 class="header-title">Loading the dataset</h1>
                
            
            
                
<p>The full dataset is part of scikit-learn's example datasets. We can import it using the following commands:</p>
<ol>
<li>First, let's load the dataset using the <kbd>load_breast_cancer</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In [1]: from sklearn import datasets<br/>...     data = datasets.load_breast_cancer()</pre>
<ol start="2">
<li>As in the previous examples, all data is contained in a 2D feature matrix, <kbd>data.data</kbd>, where the rows represent data samples and the columns are the feature values:</li>
</ol>
<pre style="padding-left: 60px">In [2]: data.data.shape<br/>Out[2]: (569, 30)</pre>
<ol start="3">
<li>With a look at the provided feature names, we recognize some that we mentioned earlier:</li>
</ol>
<pre style="padding-left: 60px">In [3]: data.feature_names<br/>Out[3]: array(['mean radius', 'mean texture', 'mean perimeter',<br/>               'mean area', 'mean smoothness', 'mean compactness',<br/>               'mean concavity', 'mean concave points',<br/>               'mean symmetry', 'mean fractal dimension',<br/>               'radius error', 'texture error', 'perimeter error',<br/>               'area error', 'smoothness error',<br/>               'compactness error', 'concavity error',<br/>               'concave points error', 'symmetry error',<br/>               'fractal dimension error', 'worst radius',<br/>               'worst texture', 'worst perimeter', 'worst area',<br/>               'worst smoothness', 'worst compactness',<br/>               'worst concavity', 'worst concave points',<br/>               'worst symmetry', 'worst fractal dimension'], <br/>              dtype='&lt;U23')</pre>
<ol start="4">
<li>Since this is a binary classification task, we expect to find exactly two target names:</li>
</ol>
<pre style="padding-left: 60px">In [4]: data.target_names<br/>Out[4]: array(['malignant', 'benign'], dtype='&lt;U9')</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Let's reserve some 20 percent of all data samples for testing:</li>
</ol>
<pre style="padding-left: 60px">In [5]: import sklearn.model_selection as ms<br/>...     X_train, X_test, y_train, y_test =<br/>...     ms.train_test_split(data_pre, target, test_size=0.2,<br/>...     random_state=42)</pre>
<ol start="6">
<li>You could certainly choose a different ratio, but most commonly people use something like 70-30, 80-20, or 90-10. It all depends a bit on the dataset size but, in the end, should not make too much of a difference. Splitting the data 80-20 should result in the following set sizes:</li>
</ol>
<pre style="padding-left: 60px">In [6]: X_train.shape, X_test.shape<br/>Out[6]: ((455, 30), (114, 30))</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Building the decision tree</h1>
                
            
            
                
<p>As shown earlier, we can create a decision tree using scikit-learn's <kbd>tree</kbd> module. For now, let's not specify any optional arguments:</p>
<ol>
<li>We will start off by creating a decision tree:</li>
</ol>
<pre style="padding-left: 60px">In [5]: from sklearn import tree...     dtc = tree.DecisionTreeClassifier()</pre>
<ol start="2">
<li>Do you remember how to train the decision tree? We will use the <kbd>fit</kbd> function for that:</li>
</ol>
<pre style="padding-left: 60px">In [6]: dtc.fit(X_train, y_train)Out[6]: DecisionTreeClassifier(class_weight=None, criterion='gini',                               max_depth=None, max_features=None,                               max_leaf_nodes=None,                               min_impurity_split=1e-07,                               min_samples_leaf=1,                               min_samples_split=2,                               min_weight_fraction_leaf=0.0,                               presort=False, random_state=None,                               splitter='best')</pre>
<ol start="3">
<li>Since we did not specify any pre-pruning parameters, we would expect this ...</li></ol></div>



  
<div><h1 class="header-title">Using decision trees for regression</h1>
                
            
            
                
<p>Although we have so far focused on using decision trees in classification tasks, you can also use them for regression. But you will need to use scikit-learn again, as OpenCV does not provide this flexibility. We will therefore only briefly review its functionality here:</p>
<ol>
<li>Let's say we wanted to use a decision tree to fit a sin wave. To make things interesting, we will also add some noise to the data points using NumPy's random number generator:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import numpy as np<br/>...     rng = np.random.RandomState(42)</pre>
<ol start="2">
<li>We then create 100 randomly spaced <em>x</em> values between 0 and 5 and calculate the corresponding sin values:</li>
</ol>
<pre style="padding-left: 60px">In [2]: X = np.sort(5 * rng.rand(100, 1), axis=0)<br/>...     y = np.sin(X).ravel()</pre>
<ol start="3">
<li>We then add noise to every other data point in <kbd>y</kbd> (using <kbd>y[::2]</kbd>), scaled by <kbd>0.5</kbd> so we don't introduce too much jitter:</li>
</ol>
<pre style="padding-left: 60px">In [3]: y[::2] += 0.5 * (0.5 - rng.rand(50))</pre>
<p class="mce-root"/>
<ol start="4">
<li>You can then create a regression tree like any other tree before.</li>
</ol>
<p style="padding-left: 60px">A small difference is that the <kbd>gini</kbd> and <kbd>entropy</kbd> split criteria do not apply to regression tasks. Instead, scikit-learn provides two different split criteria:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>mse (also known as variance reduction)</strong>: This criterion calculates the <strong>Mean Squared Error</strong> (<strong>MSE</strong>) between ground truth and prediction and splits the node that leads to the smallest MSE.</li>
<li><strong>mae</strong>: This criterion calculates the <strong>Mean Absolute Error</strong> (<strong>MAE</strong>) between ground truth and prediction and splits the node that leads to the smallest MAE.</li>
</ul>
</li>
</ul>
<ol start="5">
<li>Using the MSE criterion, we will build two trees. Let's first build a tree with depth 2:</li>
</ol>
<pre style="padding-left: 60px">In [4]: from sklearn import tree<br/>In [5]: regr1 = tree.DecisionTreeRegressor(max_depth=2,<br/>...     random_state=42)<br/>...     regr1.fit(X, y)<br/>Out[5]: DecisionTreeRegressor(criterion='mse', max_depth=2,<br/>                              max_features=None, max_leaf_nodes=None,<br/>                              min_impurity_split=1e-07,<br/>                              min_samples_leaf=1, min_samples_split=2,<br/>                              min_weight_fraction_leaf=0.0,<br/>                              presort=False, random_state=42,<br/>                              splitter='best')</pre>
<ol start="6">
<li>Next, we will build a decision tree with a maximum depth of 5:</li>
</ol>
<pre style="padding-left: 60px">In [6]: regr2 = tree.DecisionTreeRegressor(max_depth=5,<br/>...     random_state=42)<br/>...     regr2.fit(X, y)<br/>Out[6]: DecisionTreeRegressor(criterion='mse', max_depth=5,<br/>                              max_features=None, max_leaf_nodes=None,<br/>                              min_impurity_split=1e-07,<br/>                              min_samples_leaf=1, min_samples_split=2,<br/>                              min_weight_fraction_leaf=0.0,<br/>                              presort=False, random_state=42,<br/>                              splitter='best')</pre>
<p style="padding-left: 90px">We can then use the decision tree like a linear regressor from <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>.</p>
<ol start="7">
<li>For this, we create a test set with <em>x</em> values densely sampled in the whole range from 0 through 5:</li>
</ol>
<pre style="padding-left: 60px">In [7]: X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</pre>
<ol start="8">
<li>The predicted <em>y</em> values can then be obtained with the <kbd>predict</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">In [8]: y_1 = regr1.predict(X_test)<br/>...     y_2 = regr2.predict(X_test)</pre>
<ol start="9">
<li>If we plot all of these together, we can see how the decision trees differ:</li>
</ol>
<pre style="padding-left: 60px">In [9]: import matplotlib.pyplot as plt<br/>... %matplotlib inline<br/>... plt.style.use('ggplot')<br/><br/>... plt.scatter(X, y, c='k', s=50, label='data')<br/>... plt.plot(X_test, y_1, label="max_depth=2", linewidth=5)<br/>... plt.plot(X_test, y_2, label="max_depth=5", linewidth=3)<br/>... plt.xlabel("data")<br/>... plt.ylabel("target")<br/>... plt.legend()<br/>Out[9]: &lt;matplotlib.legend.Legend at 0x12d2ee345f8&gt;</pre>
<p>This will produce the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-902 image-border" src="img/fb76b100-17ab-494a-9e71-210bfbfdb4a8.png" style="width:42.42em;height:26.50em;" width="902" height="563"/></p>
<p>Here, the thick red line represents the regression tree with depth 2. You can see how the tree tries to approximate the data using these crude steps. The thinner blue line belongs to the regression tree with depth 5; the added depth has allowed the tree to make many finer-grained approximations. Therefore, this tree can approximate the data even better. However, because of this added power, the tree is also more susceptible to fitting noisy values, as can be seen especially from the spikes on the right-hand side of the plot.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned all about decision trees and how to apply them to both classification and regression tasks. We talked a bit about data generation, overfitting, and ways to avoid this phenomenon by tweaking pre-pruning and post-pruning settings. We also learned about how to rate the quality of a node split using metrics such as the Gini impurity and information gain. Finally, we applied decision trees to medical data to detect cancerous tissues. We will come back to decision trees towards the end of this book when we will combine multiple trees into what is known as a random forest. But for now, let's move on to a new topic.</p>
<p>In the next chapter, we will introduce another staple of the machine learning world: support vector ...</p></div>



  </body></html>