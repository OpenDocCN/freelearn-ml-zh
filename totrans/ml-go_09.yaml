- en: Deploying and Distributing Analyses and Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have implemented all sorts of models in Go, including regressions, classifications,
    clustering, and more. You have also learned about some of the process around developing
    a machine learning model. Our models have successfully predicted disease progression,
    flower species, and objects within images. Yet, we are still missing an important
    piece of the machine learning puzzle: deployment, maintenance, and scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: If our models just stay on our laptops, they are not doing any good or creating
    value within a company. We need to know how to take our machine learning workflows
    and integrate them into the systems that are already deployed in our organization,
    and we need to know how to scale, update, and maintain these workflows over time.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that our machine learning workflows are, by their very nature, multi-stage
    workflows might make this deployment and maintenance a little bit of a challenge.
    We need to train, test, and utilize our models and, in some cases, we need to
    preprocess and/or postprocess our data. We may also need to chain certain models
    together. How can we deploy and connect all of these stages while maintaining
    the simplicity and integrity of our applications? For example, how can we update
    a training dataset over time while still knowing which training dataset produced
    which models, and which of those models produced which results? How can we easily
    scale our predictions as the demand for those predictions scales up and down?
    Finally, how can we integrate our machine learning workflows with other applications
    in our infrastructure or with pieces of the infrastructure itself (databases,
    queues, and so on)?
  prefs: []
  type: TYPE_NORMAL
- en: We will tackle all of these questions in this chapter. As it turns out, Go and
    infrastructure tooling written in Go provide an excellent platform to deploy and
    manage machine learning workflows. We will use a completely Go-based approach
    from bottom to top and illustrate how each of those pieces helps us do data science
    at scale!
  prefs: []
  type: TYPE_NORMAL
- en: Running models reliably on remote machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether your company uses on--premise infrastructure or cloud infrastructure,
    you will need to run your machine learning models somewhere other than your laptop
    at some point. These models might need to be ready to serve fraud predictions,
    or they may need to process user-uploaded images in real time. You cannot have
    the model sitting on your laptop and successfully serve this information.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as we get our data processing and machine learning applications off
    of our laptops, we should ensure the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We should not complicate our applications just for the sake of deployment and
    scaling. We should keep our applications simple, which will help us maintain them
    and ensure integrity over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We should ensure that our applications behave like they did on our local machine,
    where we developed them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we deploy our machine learning workflows and they do no perform or behave
    like they did during development, they will not be able to produce their anticipated
    value. We should be able to understand how our models will perform locally and
    assume that they will perform the same way in production. This becomes increasingly
    hard to accomplish as you add unnecessary complexity to your applications during
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: One way to keep your deployments simple, portable, and reproducible is with
    Docker ([https://www.docker.com/](https://www.docker.com/)), and we will utilize
    it here to deploy our machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is itself written in Go, making it the first Go-based infrastructure
    tool that we will make use of in our machine learning deployment stack.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to Docker and Docker jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker and the whole container ecosystem has its own set of jargon, which can
    be confusing, especially for those with experience in things like virtual machines.
    Before we continue, let''s solidify this jargon as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Docker image** is a collection of data layers that together define a filesystem,
    libraries, environmental variables, and so on that will be seen by your application
    running inside a software container. Think of the image as a package that includes
    your application, other related libraries or packages, and other portions of an
    environment that your applications, need to run. A Docker image does not include
    a full operating system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Dockerfile** is a file in which you define the various layers of your Docker
    image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Docker engine** helps you build, manage, and run Docker images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of building a Docker image for your application is commonly referred
    to as **Docker-izing** your application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **container** or **software container** is a running instance of a Docker
    image. Essentially, this running container includes all of the layers of your
    Docker image plus a read/write layer allowing your application to run, input/output
    data, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Docker registry** is a place where you keep Docker images. This registry
    could be local or it could be running on a remote machine. It also could be a
    hosted registry service such as **Docker Hub** or Quay, **Amazon Web Service**
    (**AWS**), **Amazon EC2 Container Registry** (**ECR**), and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note**: A Docker image is not the same as a virtual machine. A Docker image
    includes your application, a filesystem, and various libraries and packages, but
    it does not actually include a guest operation system. Moreover, it does not take
    up a set amount of memory, disk, and CPU on the host machine when running. Docker
    containers share the resources of the underlying kernel on which the Docker engine
    is running.'
  prefs: []
  type: TYPE_NORMAL
- en: We hope that all of this jargon is solidified in the examples that follow this
    section, but as with other subjects in this book, you can dive into Docker and
    software containers deeper. We will include some links in this chapter's references
    to more Docker resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the following examples, we will assume that you are able to build and run
    Docker images locally. To install Docker, you can follow the detailed instructions
    at [https://www.docker.com/community-edition#/download](https://www.docker.com/community-edition#/download).
  prefs: []
  type: TYPE_NORMAL
- en: Docker-izing a machine learning application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The machine learning workflow that we will be deploying and scaling in this
    chapter will be the linear regression workflow to predict diabetes disease progression
    that we developed in [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*. In our deployment, we will consider three different pieces of the
    workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: The training and exporting of a single regression model (modeling disease progression
    with body mass index)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training and exporting of a multiple regression model (modeling disease
    progression with body mass index and the blood measurement LTG)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An inference of disease progression based on one of the trained models and input
    attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later in the chapter, it will become clear why we might want to split the workflow
    into these pieces. For now, let's focus on getting these pieces of the workflow
    Docker-ized (building Docker images that can run these portions of our workflow,
    that is).
  prefs: []
  type: TYPE_NORMAL
- en: Docker-izing the model training and export
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to utilize essentially the same code for model training as that
    from [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml), *Regression*. However,
    we are going to make a few tweaks to the code to make it more user-friendly and
    able to interface with other portions of our workflow. We would not consider these
    complications to the actual modeling code. Rather, these are things that you would
    probably do to any application that you are getting ready to utilize more generally.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to add some command line flags to our application that
    will allow us to specify the input directory where our training dataset will be
    located, and an output directory where we are going to export a persisted representation
    of our model. You can implement these command line arguments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will create a couple of struct types that will allow us to export the
    coefficient and intercept of our model to a JSON file. This exported JSON file
    is essentially a persisted version of our trained model, because the coefficients
    and intercept fully parameterize our model. These structs are defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Other than that, our training code is the same as it was from [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*. We will still use `github.com/sajari/regression` to train our model.
    We will just export the model to the JSON file. The training and exporting of
    the single regression model is included in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for the multiple regression model, the process looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To Docker-ize these training processes, we need to create a Dockerfile for
    each of the single regression training program and the multiple regression training
    program. It turns out, however, that we can use essentially the same Dockerfile
    for each of these. This Dockerfile, which should be placed in the same directory
    as our program, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pretty simple, right? Let's explore the purpose of these two lines. Remember
    how a Docker image is a series of layers that specify the environment in which
    your application will run? Well, this Dockerfile builds up two of those layers
    with two Dockerfile commands, `FROM` and `ADD`.
  prefs: []
  type: TYPE_NORMAL
- en: '`FROM alpine` specifies that we want our Docker image filesystem, applications,
    and libraries to be based on the official Alpine Linux Docker image available
    on Docker Hub. The reason that we are using `alpine` as a base image is that it
    is a very small Docker image (making it very portable) and it includes a few Linux
    shell niceties.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ADD goregtrain /` specifies that we want to add a `goregtrain` file to the
    `/` directory in the Docker image. This `goregtrain` file is actually the Go binary
    that we are going to build from our Go code. Thus, all the Dockerfile is saying
    is that we want to run our Go binary in Alpine Linux.'
  prefs: []
  type: TYPE_NORMAL
- en: Unless you are using `cgo` and depend on a bunch of external C libraries, always
    build your Go binary before building your Docker image and copy this statically
    linked Go binary into the Docker image. This will speed up your Docker builds
    and make your Docker images extremely small, which means that you will be able
    to port them easily to any system and start them super quick.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to build our Go binary before we build our Docker image, because
    we are copying that Go binary into the image. To do this, we will use a `Makefile`
    that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '`make compile` will compile our Go binary for the target architecture and name
    it `goregtrain`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`make docker` will use the Docker engine (via the `docker` CLI) to build an
    image based on our Dockerfile, and it will **tag** our image, `dwhitena/goregtrain:single`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `dwhitena` portion of the tag specifies the Docker Hub username under which
    we will store our image (in this case, `dwhitena`), `goregtrain` specifies the
    name of the image, and `:single` specifies a tagged version of this image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the image is built, `make push` will push the newly built Docker image
    to a registry. In this case, it will push it to Docker Hub under the username
    of `dwhitena` (of course, you could push to any other private or public registry).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `make clean` will clean up our binary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned, this `Dockerfile` and `Makefile` are the same for both the single
    and multiple regression models. However, we will utilize different Docker image
    tags to differentiate the two models. We will utilize `dwhitena/goregtrain:single`
    for the single regression model and `dwhitena/goregtrain:multi` for the multiple
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: In these examples and in the rest of the chapter, you can follow along locally
    using either the public Docker images under `dwhitena` on Docker Hub, which would
    not require you to modify the examples printed here. Just note that you will not
    be able to build and push your own image to `dwhitena` on Docker Hub, as you are
    not `dwhitena`. Alternatively, you could replace `dwhitena` everywhere in the
    examples with your own Docker Hub username. This would allow you to build, push,
    and utilize your own images.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build, push, and clean up after building the Docker image for either the
    single or multiple regression models, we can just run `make`, as shown in the
    following code, which is itself written in Go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see in the preceding output that the Docker engine built the two layers
    of our image, tagged the image, and pushed the image up to Docker Hub. We can
    now see the Docker image in our local registry via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see it on Docker Hub ([https://hub.docker.com/r/dwhitena/goregtrain/](https://hub.docker.com/r/dwhitena/goregtrain/))
    as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b62cc44-5fd4-4a69-aa54-7a451709f74f.png)'
  prefs: []
  type: TYPE_IMG
- en: We will see how to run and utilize this Docker image shortly, but let's first
    build another Docker image that will generate predictions based on our JSON-persisted
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Docker-izing model predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with the training of our model, we are going to utilize command line arguments
    to specify the input directories and output directories utilized by our prediction
    program. This time we will have two input directories; one for the persisted model
    and one for a directory that will contain attributes from which we are to make
    predictions. Our program will, thus, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Read in the model from the model input directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Walk over files in the attributes input directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each file in the attributes input directory (containing attributes with
    no corresponding prediction of disease progression), utilize our loaded model
    to make a prediction of disease progression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output the disease progression to an output file in the directory specified
    as a command line argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of this process as follows. We have trained our model on historical data
    to predict disease progression and we want doctors or clinics to utilize this
    prediction in some way for new patients. They send us those patients attributes
    (**body mass index** (**BMI**) or body mass index and **long term growth** (**LTG**))
    and we make predictions based on these input attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will assume that the input attributes come to our program in the form of
    JSON files (which could also be thought of as a JSON message off a queue or a
    JSON API response) that look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As such, let''s create the following struct types in our prediction program
    to decode the input attributes and marshal the output prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also create a function that will allow us to make a prediction based
    on a `ModelInfo` value that we read in from the model input directory (our persisted
    model, that is). This `prediction` function is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This prediction function, along with the types, will then allow us to walk
    over any attribute JSON files in a specified input directory and output disease
    predictions for each of those files. This process is implemented in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same `Dockerfile` and `Makefile` that we used in the preceding
    section to Docker-ize this prediction program. The only difference is that we
    will name the Go binary `goregpredict` and we will tag the Docker image `dwhitena/goregpredict`.
    Building the Docker image with `make` should return a similar output to that in
    the preceding section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Testing the Docker images locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before pushing our Docker-ized modeling processes to any servers, it's wise
    to test them locally to ensure that we are seeing the behavior that we expect.
    Then, once we are satisfied with that behavior, we can rest assured that these
    Docker images will run exactly the same on any other host that is running Docker.
    This assurance makes the use of Docker images a significant contributor to maintaining
    reproducibility with our deploys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that we have our training data and some input attribute files
    in the following directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can run our Docker images as software containers locally using the `docker
    run` command. We will also utilize the `-v` flag that will let us mount local
    directories inside of a running container, allowing us to read and write files
    to and from our local filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s run our single regression model training inside of the Docker
    container as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we look at what is in our `model` directory, we will see the newly
    trained model coefficients and intercept in `model.json`, which was output from
    our program running in the Docker image. This is illustrated in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! We trained our model using the Docker container. Now let''s utilize
    this model to make predictions. Specifically, let''s run our `goregpredict` Docker
    image using the trained model to make predictions for the three attribute files
    in `attributes`. In the following code, you will see that the attribute files
    do not have corresponding predictions before running the Docker image, but they
    do after running the Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Running the Docker images on remote machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be thinking, *what's the big deal, you get the same functionality
    with Docker that we could get by just building and running your Go program*. Well,
    that's true locally, but the magic of Docker happens when we want to run our same
    functionality in an environment other than our laptop.
  prefs: []
  type: TYPE_NORMAL
- en: We can take those Docker images and run them in the exact same way on any host
    that is running Docker, and they will produce the same exact behavior. This is
    true for any Docker image. You don't have to install any of the dependencies that
    might be layered inside of the Docker image. All you need is Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Guess what? Our Docker images are just about 3 MB in size! This means that they
    will download to any host super fast and start up and run super fast. You don't
    have to worry about lugging around multiple gigabyte-sized virtual machines and
    manually specifying resources.
  prefs: []
  type: TYPE_NORMAL
- en: Building a scalable and reproducible machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker sets up quite a bit of the way towards having our machine learning workflows
    deployed in our company''s infrastructure. However, there are still a few missing
    pieces, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we string the various stages of our workflow together? In this simple
    example, we have a training stage and a prediction stage. In other pipelines,
    you might also have data preprocessing, data splitting, data combining, visualization,
    evaluation, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to get the right data to the right stages of our workflow, especially as
    we receive new data and/or our data changes? It's not sustainable to manually
    copy new attributes over to a folder that is co-located with our prediction image
    every time we need to make new predictions, and we cannot log in to a server every
    time we need to update our training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will we be able to track and reproduce the various runs of our workflow
    for maintenance, continued development, or debugging? If we are making predictions
    over time and updating our model and/or training set over time, we need to understand
    which models produced which results to maintain reproducibility (and to meet compliance,
    in some cases).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we scale our processing over multiple machines and, in some cases, over
    multiple shared resources? We likely need to run our processing on some shared
    set of resources in our company. It would be nice for us to be able to scale our
    processing over these resources as we need more computation power and/or as other
    applications are scheduled on these resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thankfully, there are a couple more open source tools (both written in Go) that
    solve these issues for us. Not only that, they let us use the Docker images that
    we already built as the main units of data processing. These tools are **Kubernetes**
    (**k8s**) and **Pachyderm**.
  prefs: []
  type: TYPE_NORMAL
- en: You have already been exposed to Pachyderm in [Chapter 1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml),
    *Gathering and Organizing Data*. In that chapter, we utilized Pachyderm to illustrate
    the idea of data versioning and, as you might be guessing, Pachyderm will help
    us solve some of the issues around managing data, tracking, and reproducibility.
    Pachyderm will also solve all of the remaining issues that we need to address
    around scalability and pipelining because Pachyderm provides both data management/versioning
    capabilities and data pipelining capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is a container orchestration engine, which is amazing at scheduling
    containerized workloads (like our Docker images) across a cluster of shared resources.
    It is wildly popular right now, and Pachyderm utilizes Kubernetes under the hood
    to manage containerized data processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Pachyderm and Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pachyderm, which runs on Kubernetes, can be deployed almost anywhere because
    Kubernetes can be deployed anywhere. You can deploy Pachyderm on any of the popular
    cloud providers, on premise, or even on your own laptop. All you need to do to
    deploy a Pachyderm cluster is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a running Kubernetes cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have access to an object store of your choice (for example, S3, **Glasglow Coma
    Scale** (**GCS**), or Minio). This object store will serve as the storage backing
    for the Pachyderm cluster where all the versioning and processed data is stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy Pachyderm on the Kubernetes cluster using the Pachyderm CLI `pachctl`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detailed instructions to deploy Pachyderm on any cloud provider or on-premise
    can be found at [http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html](http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html).
    Alternatively, you can easily experiment with and develop against a local Pachyderm
    cluster using `minikube`, as further detailed at [http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html](http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Following one set of those instructions should get you to a state where you
    have a Kubernetes cluster running in the following state (which can be checked
    using Kubernetes CLI tool called `kubectl`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see that the Pachyderm daemon (or server), `pachd`, is running
    in the Kubernetes cluster as a **pod**, which is simply a group of one or more
    containers. Our pipeline stages will also run as Kubernetes pods, but you won't
    have to worry too much about that as Pachyderm will take care of the details.
  prefs: []
  type: TYPE_NORMAL
- en: The ports and IPs listed in the above output may vary depending on your deployment
    and various configurations. However, a healthy Pachyderm cluster should look very
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your Pachyderm cluster is running and you followed one of the Pachyderm
    deploy guides, you should also have the `pachctl` CLI tool installed. When `pachctl`
    is successfully connected to the Pachyderm cluster, you can run the following
    as a further sanity check (where the version number may change based on the Pachyderm
    version you are running):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you have any trouble getting Pachyderm deployed or have any trouble building
    or running pipelines, the Pachyderm community has a great public Pachyderm Slack
    channel. You can join by visiting [http://slack.pachyderm.io/](http://slack.pachyderm.io/).
    The community is active there every day, and any questions you have will be welcome.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Pachyderm machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you have already seen, our example machine learning pipeline has two stages.
    The model stage of our pipeline will train a model and export a persisted version
    of this model to a file. The prediction stage of our pipeline will utilize the
    trained model to make predictions for input attribute files. Overall, this pipeline
    will look like the following in Pachyderm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb8d094-54f6-4338-b91a-4d3008681c50.png)'
  prefs: []
  type: TYPE_IMG
- en: Each of the cylinders in the preceding figure represents a Pachyderm data repository
    of versioned data. You were already exposed to these data repositories in [Chapter
    1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml), *Gathering and Organizing Data*.
    Each of the boxes represents a containerized data pipeline stage. The basic units
    of data processing in Pachyderm are Docker images. Thus, we can utilize the Docker
    images that we created in preceding sections in this data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: By stringing together versioned collections of data (again, think about these
    as a sort of *Git for data*), processing these collections of data with containers,
    and saving results to other versioned collections of data, Pachyderm pipelines
    have some pretty interesting and useful implications.
  prefs: []
  type: TYPE_NORMAL
- en: First, we can go back at any point in time to see the state of any portion of
    our data at that point in time. This might help us as we further develop our model
    if we are wanting to develop a certain state of our data. It might also help us
    to collaboratively develop as a team by tracking the team's data over time. Not
    all changes to data are good, and we need the ability to revert after bad or corrupt
    data is committed into the system.
  prefs: []
  type: TYPE_NORMAL
- en: Next, all of our pipeline results are linked to the exact Docker images and
    exact states of other data that produced these results. The Pachyderm team calls
    this **provenance**. It is the ability to understand what pieces of processing
    and which pieces of data contributed to a particular result. For example, with
    this pipeline, we are able to determine the exact version of our persisted model
    that produced a particular result, along with the exact Docker image that produced
    that model and the exact state of the training data that was input to that exact
    Docker image. We are, thus, able to exactly reproduce an entire run of any pipeline,
    debug strange model behavior, and attribute results to the corresponding input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, because Pachyderm knows which portions of your data repositories are
    new or different, our data pipeline can process data incrementally and can be
    triggered automatically. Let's say that we have already committed one million
    attribute files into the `attributes` data repository, and then we commit ten
    more attribute files. We don't have to reprocess the first million to update our
    results. Pachyderm knows that we only need to process the ten new files and will
    send these to the `prediction` stage to be processed. Moreover, Pachyderm will
    (by default) automatically trigger this update because it knows that the update
    is needed to keep the results in sync with the input.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental processing and automatic triggers are some of the default behaviors
    of Pachyderm pipelines, but they are not the only things you can do with Pachyderm.
    In fact, we will only scratch the surface here of what is possible with Pachyderm.
    You can do distributed map/reduce style operations, distributed image processing,
    periodic processing of database tables, and much, much more. Thus, the techniques
    here should be transferable to a variety of domains.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and filling the input repositories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create the Pachyderm pipeline on our deployed Pachyderm cluster, we first
    need to create the input repositories for the pipeline. Remember, our pipeline
    has the `training` and `attributes` data repositories that drive the rest of the
    pipeline. As we put data into these, Pachyderm will trigger the downstream portions
    of the pipeline and calculate the results.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in [Chapter 1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml), *Gathering
    and Organizing Data*, how we can create data repositories in Pachyderm and commit
    data into these repositories using `pachctl`, but let's try to do this directly
    from a Go program here. In this particular example, there is not any real advantage
    to doing this, due to the fact that we already have our training and test data
    in files and we can easily commit those files into Pachyderm by name.
  prefs: []
  type: TYPE_NORMAL
- en: However, imagine a more realistic scenario. We might want our data pipeline
    to be integrated with some other Go services that we have already written at our
    company. One of these services might process incoming patient medical attributes
    from doctors or clinics, and we want to feed these attributes to our data pipeline,
    such that the data pipeline can make the corresponding predictions of disease
    progression.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such a scenario, we would ideally like to pass the attributes directly into
    Pachyderm from our service instead of manually copying all that data. This is
    where the Pachyderm Go client, `github.com/pachyderm/pachyderm/src/client`, can
    come in very handy. Using the Pachyderm Go client, we can create our input repositories
    as follows (after connecting to the Pachyderm cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling this and running it creates the `repos` as expected, which can be
    confirmed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, the above code just creates and checks the repo once. If you
    ran the code again, you would get an error because the repo was already created.
    To improve on this, you could check if the repo exists and then create it if it
    does not exist. You can also check that the repo exists by name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go ahead and put an attributes file in the `attributes` repository
    and the `diabetes.csv` training data set into the `training` repository. This
    is also very easily done directly in Go, via the Pachyderm client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this does indeed commit the data into Pachyderm in the proper data
    repositories, as can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It is important to finish the commits that we started above. If you leave an
    orphaned commit hanging you might block other commits of data. Thus, you might
    consider deferring the finish of the commit, just to be safe.
  prefs: []
  type: TYPE_NORMAL
- en: Good! Now we have data in the proper input data repositories on our remote Pachyderm
    cluster. Next, we can actually process this data and produce results.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running the processing stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing stages are created declaratively via a **pipeline specification**
    in Pachyderm. If you have worked with Kubernetes before, this type of interaction
    should be familiar. Basically, we declare to Pachyderm what processing we want
    to take place, and Pachyderm handles all the details to make sure that this processing
    happens as declared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create the `model` stage of our pipeline using a JSON pipeline
    specification that is stored in `train.json`. This JSON specification is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This might look somewhat complicated, but there are really only a few things
    happening here. Let''s dissect the specification to learn about the different
    pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we are telling Pachyderm to name our pipeline `model`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we are telling Pachyderm that we want this `model` pipeline to process
    data using our `dwhitena/goregtrain:single`, and we want the pipeline to run our
    `goregtrain` binary in the container when processing the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the specification tells Pachyderm that we intend to process the `training`
    data repository as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To understand some of the other details of the specification, we need to first
    discuss what will happen when we create the pipeline on our Pachyderm cluster
    using the specification. When this happens, Pachyderm will use Kubernetes to schedule
    one or more worker pods on the Kubernetes cluster. These worker pods will then
    wait, ready to process any data supplied to them by the Pachyderm daemon. When
    there is data that needs to be processed in the input repository, `training`,
    the Pachyderm daemon will trigger a job in which these workers will process the
    data. Pachyderm will automatically mount the input data in the container at `/pfs/<input
    repo name>` (`/pfs/training` in this case). Data the container writes out to `/pfs/out`
    will be automatically versioned in an output data repository with the same name
    as the pipeline (`model` in this case).
  prefs: []
  type: TYPE_NORMAL
- en: The `parallelism_spec` portion of the specification lets us tell Pachyderm how
    many workers it should spin up to process the input data. Here we will just spin
    up a single worker and process the data serially. Later in the chapter, we will
    discuss parallelizing our pipeline and the associated data sharding, which is
    controlled by the `glob` pattern in the `input` section of the specification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough talk though! Let''s actually create this `model` pipeline stage on our
    Pachyderm cluster and get to processing some data. The pipeline can easily be
    created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When we do this, we can see the worker(s) that Pachyderm creates on the Kubernetes
    cluster using `kubectl` as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also see that Pachyderm has automatically triggered a job to perform
    our model training. Remember, this happens because Pachyderm knows that there
    is data in the input repository, `training`, that has not been processed. You
    can see the triggered job along with its results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Great! We can see that our model training produced the results that we expected.
    However, Pachyderm was smart enough to trigger the pipeline, and it will update
    this model whenever we modify the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `prediction` pipeline can be created in a similar manner. Its pipeline
    specification, `pipeline.json`, is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The major difference in this pipeline specification is that we have two input
    repositories rather than one. These two input repositories are the `attributes`
    repository that we previously created and the `model` repository that contains
    the output of the `model` pipeline. These are combined using a `cross` primitive.
    `cross` ensures that all relevant pairs of our model and attribute files are processed.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested, you can find out more about the other ways to combine
    data in Pachyderm in the Pachyderm docs; go to [http://pachyderm.readthedocs.io/en/latest/](http://pachyderm.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating this pipeline and examining the results (corresponding to `1.json`)
    can be accomplished as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You can see that another data repository, `prediction`, was created to version
    the output of the `prediction` pipeline. In this manner, Pachyderm gradually builds
    up links between input data repositories, data processing stages, and output data
    repositories, such that it always knows what data and processing components are
    linked.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full machine learning pipeline is now deployed and we have run each stage
    of the pipeline once. However, the pipeline is ready and waiting to process more
    data. All we have to do is put data at the top of the pipeline and Pachyderm will
    automatically trigger any downstream processing that needs to take place to update
    our results. Let''s illustrate this by committing two more files into the `attributes`
    repository, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will automatically trigger another prediction job to update our results.
    The new job can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can see that the `prediction` repository has two new results from
    running our prediction code again on the two new input files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note that Pachyderm did not reprocess `1.json` here even though it is shown
    with the latest results. Under the hood, Pachyderm knows that there was already
    a result corresponding to `1.json` in `attributes`, so there is no need to reprocess
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Updating pipelines and examining provenance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over time, we are likely going to want to update our machine learning models.
    The data that they are processing may have changed over time or maybe we have
    just developed a different or better model. In any event, it is likely that we
    are going to need to update our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that our pipeline is in the state described in the previous section,
    where we have created the full pipeline, the training process has run once, and
    we have processed two commits into the `attributes` repository. Now, let's update
    the model pipeline to train our multiple regression
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s update the model pipeline to train our multiple regression models
    instead of our single regression model. This is actually really easy. We just
    need to change `"image": "dwhitena/goregtrain:single"` in our `model.json` pipeline
    specification to `"image": "dwhitena/goregtrain:multi"` and then update the pipeline
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When we give Pachyderm this new specification, Pachyderm will automatically
    update the worker pods such that they are running the containers with the multiple
    regression model. Also, because we supplied the `--reprocess` flag, Pachyderm
    will trigger one or more new jobs to reprocess any input commits with the new
    image that was previously processed with the old image. We can see these reprocessing
    jobs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Looking at this output, we can notice something interesting and very useful.
    When our `model` stage trained our new multiple regression model and output this
    model as `model.json`, Pachyderm saw that the results of our `prediction` pipeline
    were now out of sync with the latest model. As such, Pachyderm automatically triggered
    another job to update our previous results using the new multiple regression models.
  prefs: []
  type: TYPE_NORMAL
- en: This workflow is super useful to manage both deployed models and during the
    development of models, because you do not need to manually shuffle around a bunch
    of model versions and related data. Everything happens automatically. However,
    a natural question arises from this update. How are we to know which models produced
    which results?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, this is where Pachyderm''s data versioning combined with the pipelining
    really shines. We can look at any prediction result and inspect that commit to
    determine the commit''s provenance. This is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we can see that prediction `ef32a74b04ee4edda7bc2a2b469f3e03`
    resulted from model `f54ca1a0142542579c1543e41f5e9403`, which can be retrieved
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we look at a previous prediction, we will see that it resulted
    from a different model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can always trace back the provenance of any piece of data in our pipeline,
    no matter how many jobs are run and how many times we update our pipeline. This
    is very important in creating sustainable machine learning workflows that can
    be maintained over time, improved upon, and updated without major headaches.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling pipeline stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each pipeline specification in Pachyderm can have a corresponding `parallelism_spec`
    field. This field, along with the `glob` patterns in your inputs, lets you parallelize
    your pipeline stages over their input data. Each pipeline stage is individually
    scalable independent of all the other pipeline stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `parallelism_spec` field in the pipeline specification lets you control
    how many workers Pachyderm will spin up to process data in that pipeline stage.
    For example, the following `parallelism_spec` would tell Pachyderm to spin up
    10 workers for a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `glob` patterns in your inputs tell Pachyderm how it can share your input
    data over the workers declared by the `parallelism_spec`. For example, a `glob`
    pattern of `/*` would tell Pachyderm that it can split up your input data into
    datums consisting of any files of directories at the root of your repository.
    A glob pattern of `/*/*` would tell Pachyderm that it can split up your data into
    datums consisting of files or directories at a level deep into your repository
    directory structure. If you are new to glob patterns, check out this description
    with some examples: [https://en.wikipedia.org/wiki/Glob_(programming)](https://en.wikipedia.org/wiki/Glob_(programming)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of our example pipeline, we could imagine that we need to scale
    the `prediction` stage of our pipeline because we are seeing a huge influx of
    attributes that need corresponding predictions. If this was the case, we could
    change `parallelism_spec` in our `prediction.json` pipeline specification to `"constant":
    "10"`. As soon as we update the pipeline with the new specification, Pachyderm
    will spin up 10 new workers for the `prediction` pipeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, any new commits of attributes into the `attributes` repo will be processed
    in parallel by the 10 workers. For example, if we committed 100 more attribute
    files into `attributes`, Pachyderm would send 1/10 of those files to each of the
    10 workers to process in parallel. All of the results would still be seen the
    same way in the `prediction` repo.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, setting a constant number of workers is not the only way you can
    do scaling with Pachyderm and Kubernetes. Pachyderm will let you scale down workers
    as well by setting a scale down threshold for idle workers. In addition, Pachyderm
    auto-scaling of workers can be combined with auto-scaling of Kubernetes cluster
    resources to deal with bursts of data and batch processing. Still, further, Pachyderm
    will allow you to specify the resources needed for pipelines and, using this resource
    specification, you can offload machine learning workloads to accelerators (GPUs,
    for example).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction to Docker: [https://training.docker.com/introduction-to-docker](https://training.docker.com/introduction-to-docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building minimal Docker images for Go apps: [https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/](https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pachyderm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Public Slack channel: [http://slack.pachyderm.io/](http://slack.pachyderm.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Getting started guide: [http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html](http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go client docs: [https://godoc.org/github.com/pachyderm/pachyderm/src/client](https://godoc.org/github.com/pachyderm/pachyderm/src/client)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning with Pachyderm: [http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html](http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning examples: [http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning](http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributed processing with Pachyderm: [http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html](http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Common Pachyderm deployments: [http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html](http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Auto-scaling Pachyderm clusters: [http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html](http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From merely gathering CSV data to a fully deployed, distributed machine learning
    pipeline, we did it! We can now build machine learning models with Go and deploy
    these models with confidence on a cluster of machines. These patterns should allow
    you to build and deploy a whole variety of intelligent applications, and I can't
    wait to hear about what you create! Please stay in touch.
  prefs: []
  type: TYPE_NORMAL
