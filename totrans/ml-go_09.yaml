- en: Deploying and Distributing Analyses and Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署和分发分析和模型
- en: 'We have implemented all sorts of models in Go, including regressions, classifications,
    clustering, and more. You have also learned about some of the process around developing
    a machine learning model. Our models have successfully predicted disease progression,
    flower species, and objects within images. Yet, we are still missing an important
    piece of the machine learning puzzle: deployment, maintenance, and scaling.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 Go 中实现了各种模型，包括回归、分类、聚类等。您也已经了解了一些关于开发机器学习模型的过程。我们的模型已经成功地预测了疾病进展、花卉种类和图像中的物体。然而，我们仍然缺少机器学习拼图中的一块重要部分：部署、维护和扩展。
- en: If our models just stay on our laptops, they are not doing any good or creating
    value within a company. We need to know how to take our machine learning workflows
    and integrate them into the systems that are already deployed in our organization,
    and we need to know how to scale, update, and maintain these workflows over time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型仅仅停留在我们的笔记本电脑上，它们在公司中就不会产生任何好处或创造价值。我们需要知道如何将我们的机器学习工作流程集成到我们组织内已经部署的系统，并且我们需要知道如何随着时间的推移进行扩展、更新和维护这些工作流程。
- en: The fact that our machine learning workflows are, by their very nature, multi-stage
    workflows might make this deployment and maintenance a little bit of a challenge.
    We need to train, test, and utilize our models and, in some cases, we need to
    preprocess and/or postprocess our data. We may also need to chain certain models
    together. How can we deploy and connect all of these stages while maintaining
    the simplicity and integrity of our applications? For example, how can we update
    a training dataset over time while still knowing which training dataset produced
    which models, and which of those models produced which results? How can we easily
    scale our predictions as the demand for those predictions scales up and down?
    Finally, how can we integrate our machine learning workflows with other applications
    in our infrastructure or with pieces of the infrastructure itself (databases,
    queues, and so on)?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的机器学习工作流程本质上就是多阶段工作流程，这可能会使得部署和维护变得有些挑战。我们需要训练、测试和利用我们的模型，在某些情况下，我们还需要预处理和/或后处理我们的数据。我们可能还需要将某些模型串联起来。我们如何在保持应用程序简单性和完整性的同时部署和连接所有这些阶段呢？例如，我们如何在时间上更新训练数据集的同时，仍然知道哪个训练数据集产生了哪些模型，以及哪些模型产生了哪些结果？我们如何轻松地随着预测需求的上下波动而扩展我们的预测？最后，我们如何将我们的机器学习工作流程集成到我们基础设施中的其他应用程序或基础设施本身（数据库、队列等）？
- en: We will tackle all of these questions in this chapter. As it turns out, Go and
    infrastructure tooling written in Go provide an excellent platform to deploy and
    manage machine learning workflows. We will use a completely Go-based approach
    from bottom to top and illustrate how each of those pieces helps us do data science
    at scale!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中解决所有这些问题。事实证明，Go 和用 Go 编写的基础设施工具提供了一个出色的平台来部署和管理机器学习工作流程。我们将从底到顶使用完全基于
    Go 的方法，并展示每个组件如何帮助我们进行大规模的数据科学！
- en: Running models reliably on remote machines
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在远程机器上可靠地运行模型
- en: Whether your company uses on--premise infrastructure or cloud infrastructure,
    you will need to run your machine learning models somewhere other than your laptop
    at some point. These models might need to be ready to serve fraud predictions,
    or they may need to process user-uploaded images in real time. You cannot have
    the model sitting on your laptop and successfully serve this information.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您的公司使用的是本地基础设施还是云基础设施，您都可能在某个时候需要在除笔记本电脑之外的地方运行您的机器学习模型。这些模型可能需要准备好提供欺诈预测，或者它们可能需要实时处理用户上传的图像。您不能让模型停留在您的笔记本电脑上，并成功提供这些信息。
- en: 'However, as we get our data processing and machine learning applications off
    of our laptops, we should ensure the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们将数据处理和机器学习应用程序从笔记本电脑上移除时，我们应该确保以下几点：
- en: We should not complicate our applications just for the sake of deployment and
    scaling. We should keep our applications simple, which will help us maintain them
    and ensure integrity over time.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不应该为了部署和扩展而使我们的应用程序变得复杂。我们应该保持应用程序的简单性，这将帮助我们维护它们并确保随着时间的推移保持完整性。
- en: We should ensure that our applications behave like they did on our local machine,
    where we developed them.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该确保我们的应用程序的行为与我们在本地机器上开发它们时的行为一致。
- en: If we deploy our machine learning workflows and they do no perform or behave
    like they did during development, they will not be able to produce their anticipated
    value. We should be able to understand how our models will perform locally and
    assume that they will perform the same way in production. This becomes increasingly
    hard to accomplish as you add unnecessary complexity to your applications during
    deployment.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们部署我们的机器学习工作流程，并且它们没有像开发期间那样执行或表现，它们将无法产生预期的价值。我们应该能够理解我们的模型在本地如何表现，并假设它们在生产环境中将以相同的方式表现。随着您在部署过程中向应用程序添加不必要的复杂性，这变得越来越难以实现。
- en: One way to keep your deployments simple, portable, and reproducible is with
    Docker ([https://www.docker.com/](https://www.docker.com/)), and we will utilize
    it here to deploy our machine learning applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一种保持您的部署简单、便携和可重复的方法是使用 Docker ([https://www.docker.com/](https://www.docker.com/))，我们在这里将利用它来部署我们的机器学习应用程序。
- en: Docker is itself written in Go, making it the first Go-based infrastructure
    tool that we will make use of in our machine learning deployment stack.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 本身是用 Go 编写的，这使得它成为我们将在机器学习部署堆栈中使用的第一个基于 Go 的基础设施工具。
- en: A brief introduction to Docker and Docker jargon
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 和 Docker 术语的简要介绍
- en: 'Docker and the whole container ecosystem has its own set of jargon, which can
    be confusing, especially for those with experience in things like virtual machines.
    Before we continue, let''s solidify this jargon as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 和整个容器生态系统有其自己的术语集，这可能会令人困惑，尤其是对于那些有虚拟机等经验的人来说。在我们继续之前，让我们如下巩固这些术语：
- en: A **Docker image** is a collection of data layers that together define a filesystem,
    libraries, environmental variables, and so on that will be seen by your application
    running inside a software container. Think of the image as a package that includes
    your application, other related libraries or packages, and other portions of an
    environment that your applications, need to run. A Docker image does not include
    a full operating system.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker 镜像**是一组数据层，这些层共同定义了一个文件系统、库、环境变量等，这些将在运行在软件容器中的应用程序中看到。将镜像视为一个包含您的应用程序、其他相关库或包以及应用程序运行所需的环境其他部分的包。Docker
    镜像不包括完整的操作系统。'
- en: A **Dockerfile** is a file in which you define the various layers of your Docker
    image.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dockerfile** 是一个文件，您在其中定义 Docker 镜像的各个层。'
- en: The **Docker engine** helps you build, manage, and run Docker images.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker 引擎**帮助您构建、管理和运行 Docker 镜像。'
- en: The process of building a Docker image for your application is commonly referred
    to as **Docker-izing** your application.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的应用程序构建 Docker 镜像的过程通常被称为**Docker 化**您的应用程序。
- en: A **container** or **software container** is a running instance of a Docker
    image. Essentially, this running container includes all of the layers of your
    Docker image plus a read/write layer allowing your application to run, input/output
    data, and so on.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**或**软件容器**是 Docker 镜像的运行实例。本质上，这个运行的容器包括了您的 Docker 镜像的所有层，以及一个读写层，允许您的应用程序运行、输入/输出数据等。'
- en: A **Docker registry** is a place where you keep Docker images. This registry
    could be local or it could be running on a remote machine. It also could be a
    hosted registry service such as **Docker Hub** or Quay, **Amazon Web Service**
    (**AWS**), **Amazon EC2 Container Registry** (**ECR**), and so on.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker 仓库**是您存放 Docker 镜像的地方。这个仓库可以是本地的，也可以是在远程机器上运行的。它还可以是一个托管仓库服务，如**Docker
    Hub**、Quay、**亚马逊网络服务**（**AWS**）、**亚马逊 EC2 容器注册库**（**ECR**）等等。'
- en: '**Note**: A Docker image is not the same as a virtual machine. A Docker image
    includes your application, a filesystem, and various libraries and packages, but
    it does not actually include a guest operation system. Moreover, it does not take
    up a set amount of memory, disk, and CPU on the host machine when running. Docker
    containers share the resources of the underlying kernel on which the Docker engine
    is running.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Docker 镜像与虚拟机不同。Docker 镜像包括您的应用程序、文件系统以及各种库和包，但它实际上并不包括一个客户操作系统。此外，它在主机机器上运行时不会占用固定数量的内存、磁盘和
    CPU。Docker 容器共享 Docker 引擎运行的底层内核的资源。'
- en: We hope that all of this jargon is solidified in the examples that follow this
    section, but as with other subjects in this book, you can dive into Docker and
    software containers deeper. We will include some links in this chapter's references
    to more Docker resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望本节之后的示例能够巩固所有这些术语，但与其他本书中的主题一样，你可以更深入地了解Docker和软件容器。我们将在本章的参考文献中包含一些链接，以提供更多Docker资源。
- en: In the following examples, we will assume that you are able to build and run
    Docker images locally. To install Docker, you can follow the detailed instructions
    at [https://www.docker.com/community-edition#/download](https://www.docker.com/community-edition#/download).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将假设你能够本地构建和运行Docker镜像。要安装Docker，你可以按照[https://www.docker.com/community-edition#/download](https://www.docker.com/community-edition#/download)上的详细说明进行操作。
- en: Docker-izing a machine learning application
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将机器学习应用程序Docker化
- en: 'The machine learning workflow that we will be deploying and scaling in this
    chapter will be the linear regression workflow to predict diabetes disease progression
    that we developed in [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*. In our deployment, we will consider three different pieces of the
    workflow:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将部署和扩展的机器学习工作流程将是我们在[第4章](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml)中开发的线性回归工作流程，用于预测糖尿病疾病进展。在我们的部署中，我们将考虑工作流程的三个不同部分：
- en: The training and exporting of a single regression model (modeling disease progression
    with body mass index)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和导出单一回归模型（使用体重指数建模疾病进展）
- en: The training and exporting of a multiple regression model (modeling disease
    progression with body mass index and the blood measurement LTG)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和导出多重回归模型（使用体重指数和血液测量LTG建模疾病进展）
- en: An inference of disease progression based on one of the trained models and input
    attributes
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于训练模型之一和输入属性对疾病进展进行推断
- en: Later in the chapter, it will become clear why we might want to split the workflow
    into these pieces. For now, let's focus on getting these pieces of the workflow
    Docker-ized (building Docker images that can run these portions of our workflow,
    that is).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，将会清楚为什么我们可能想要将这些工作流程分成这些部分。现在，让我们专注于将这些工作流程的部分Docker化（构建可以运行这些工作流程部分的Docker镜像）。
- en: Docker-izing the model training and export
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型训练和导出Docker化
- en: We are going to utilize essentially the same code for model training as that
    from [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml), *Regression*. However,
    we are going to make a few tweaks to the code to make it more user-friendly and
    able to interface with other portions of our workflow. We would not consider these
    complications to the actual modeling code. Rather, these are things that you would
    probably do to any application that you are getting ready to utilize more generally.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与[第4章](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml)中“回归”相同的代码进行模型训练。然而，我们将对代码进行一些调整，使其更加用户友好，并能与其他工作流程部分进行接口。我们不会将这些复杂性视为实际建模代码的复杂性。相反，这些是你可能对任何准备更广泛使用的应用程序所做的事情。
- en: 'First, we are going to add some command line flags to our application that
    will allow us to specify the input directory where our training dataset will be
    located, and an output directory where we are going to export a persisted representation
    of our model. You can implement these command line arguments as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将向我们的应用程序添加一些命令行标志，这将允许我们指定训练数据集所在的输入目录，以及我们将导出模型持久化表示的输出目录。你可以如下实现这些命令行参数：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we will create a couple of struct types that will allow us to export the
    coefficient and intercept of our model to a JSON file. This exported JSON file
    is essentially a persisted version of our trained model, because the coefficients
    and intercept fully parameterize our model. These structs are defined here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建几个结构类型，这将使我们能够将模型的系数和截距导出到JSON文件中。这个导出的JSON文件本质上是我们训练模型的持久化版本，因为系数和截距完全参数化了我们的模型。这些结构在这里定义：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Other than that, our training code is the same as it was from [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*. We will still use `github.com/sajari/regression` to train our model.
    We will just export the model to the JSON file. The training and exporting of
    the single regression model is included in the following snippet:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个之外，我们的训练代码与[第4章](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml)中的回归相同。我们仍然会使用`github.com/sajari/regression`来训练我们的模型。我们只是将模型导出为JSON文件。单变量回归模型的训练和导出包含在以下片段中：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, for the multiple regression model, the process looks like the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于多元回归模型，过程看起来如下：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To Docker-ize these training processes, we need to create a Dockerfile for
    each of the single regression training program and the multiple regression training
    program. It turns out, however, that we can use essentially the same Dockerfile
    for each of these. This Dockerfile, which should be placed in the same directory
    as our program, looks like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些训练过程Docker化，我们需要为每个单变量回归训练程序和多元回归训练程序创建一个Dockerfile。然而，实际上我们可以为这些程序使用几乎相同的Dockerfile。这个Dockerfile应该放在我们的程序相同的目录中，看起来如下：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pretty simple, right? Let's explore the purpose of these two lines. Remember
    how a Docker image is a series of layers that specify the environment in which
    your application will run? Well, this Dockerfile builds up two of those layers
    with two Dockerfile commands, `FROM` and `ADD`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很简单，对吧？让我们来探讨这两行代码的目的。记得Docker镜像是一系列层，这些层指定了您的应用程序将运行的环境吗？嗯，这个Dockerfile通过两个Dockerfile命令`FROM`和`ADD`构建了这两个层。
- en: '`FROM alpine` specifies that we want our Docker image filesystem, applications,
    and libraries to be based on the official Alpine Linux Docker image available
    on Docker Hub. The reason that we are using `alpine` as a base image is that it
    is a very small Docker image (making it very portable) and it includes a few Linux
    shell niceties.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`FROM alpine`指定我们希望我们的Docker镜像文件系统、应用程序和库基于Docker Hub上可用的官方Alpine Linux Docker镜像。我们之所以使用`alpine`作为基础镜像，是因为它是一个非常小的Docker镜像（使其非常便携），并且包含了一些Linux
    shell的便利功能。'
- en: '`ADD goregtrain /` specifies that we want to add a `goregtrain` file to the
    `/` directory in the Docker image. This `goregtrain` file is actually the Go binary
    that we are going to build from our Go code. Thus, all the Dockerfile is saying
    is that we want to run our Go binary in Alpine Linux.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`ADD goregtrain /`指定我们希望在Docker镜像的`/`目录中添加一个`goregtrain`文件。这个`goregtrain`文件实际上是我们将从Go代码构建的Go二进制文件。因此，Dockerfile实际上只是在说我们希望在Alpine
    Linux上运行我们的Go二进制文件。'
- en: Unless you are using `cgo` and depend on a bunch of external C libraries, always
    build your Go binary before building your Docker image and copy this statically
    linked Go binary into the Docker image. This will speed up your Docker builds
    and make your Docker images extremely small, which means that you will be able
    to port them easily to any system and start them super quick.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您使用`cgo`并依赖于大量外部C库，否则在构建Docker镜像之前，始终先构建Go二进制文件，并将此静态链接的Go二进制文件复制到Docker镜像中。这将加快您的Docker构建速度，并使您的Docker镜像非常小，这意味着您将能够轻松地将它们移植到任何系统并快速启动。
- en: 'Now, we need to build our Go binary before we build our Docker image, because
    we are copying that Go binary into the image. To do this, we will use a `Makefile`
    that looks like the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们构建Docker镜像之前，我们需要先构建Go二进制文件，因为我们将要将其复制到镜像中。为此，我们将使用一个如下所示的`Makefile`：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见：
- en: '`make compile` will compile our Go binary for the target architecture and name
    it `goregtrain`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make compile`将为目标架构编译我们的Go二进制文件，并将其命名为`goregtrain`。'
- en: '`make docker` will use the Docker engine (via the `docker` CLI) to build an
    image based on our Dockerfile, and it will **tag** our image, `dwhitena/goregtrain:single`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make docker`将使用Docker引擎（通过`docker` CLI）根据我们的Dockerfile构建一个镜像，并将我们的镜像`dwhitena/goregtrain:single`进行标记。'
- en: The `dwhitena` portion of the tag specifies the Docker Hub username under which
    we will store our image (in this case, `dwhitena`), `goregtrain` specifies the
    name of the image, and `:single` specifies a tagged version of this image.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签中的`dwhitena`部分指定了我们将存储我们的镜像的Docker Hub用户名（在这种情况下，`dwhitena`），`goregtrain`指定了镜像的名称，而`:single`指定了此镜像的标记版本。
- en: Once the image is built, `make push` will push the newly built Docker image
    to a registry. In this case, it will push it to Docker Hub under the username
    of `dwhitena` (of course, you could push to any other private or public registry).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦镜像构建完成，`make push` 命令会将新构建的 Docker 镜像推送到一个注册库。在这种情况下，它将推送到名为 `dwhitena` 的
    Docker Hub 用户名下（当然，你也可以推送到任何其他私有或公共注册库）。
- en: Finally, `make clean` will clean up our binary.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`make clean` 命令将清理我们的二进制文件。
- en: As mentioned, this `Dockerfile` and `Makefile` are the same for both the single
    and multiple regression models. However, we will utilize different Docker image
    tags to differentiate the two models. We will utilize `dwhitena/goregtrain:single`
    for the single regression model and `dwhitena/goregtrain:multi` for the multiple
    regression model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个 `Dockerfile` 和 `Makefile` 对于单变量和多元回归模型都是相同的。然而，我们将使用不同的 Docker 镜像标签来区分这两个模型。我们将使用
    `dwhitena/goregtrain:single` 作为单变量回归模型的标签，使用 `dwhitena/goregtrain:multi` 作为多元回归模型的标签。
- en: In these examples and in the rest of the chapter, you can follow along locally
    using either the public Docker images under `dwhitena` on Docker Hub, which would
    not require you to modify the examples printed here. Just note that you will not
    be able to build and push your own image to `dwhitena` on Docker Hub, as you are
    not `dwhitena`. Alternatively, you could replace `dwhitena` everywhere in the
    examples with your own Docker Hub username. This would allow you to build, push,
    and utilize your own images.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例和本章的其余部分，你可以使用 Docker Hub 上 `dwhitena` 下的公共 Docker 镜像进行本地操作，这样你就不需要修改这里打印的示例。只需注意，你将无法在
    Docker Hub 的 `dwhitena` 下构建和推送自己的镜像，因为你不是 `dwhitena`。或者，你可以在示例中的所有地方用你自己的 Docker
    Hub 用户名替换 `dwhitena`。这将允许你构建、推送并使用你自己的镜像。
- en: 'To build, push, and clean up after building the Docker image for either the
    single or multiple regression models, we can just run `make`, as shown in the
    following code, which is itself written in Go:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建、推送并清理用于单变量或多元回归模型的 Docker 镜像，我们只需运行 `make`，如下面的代码所示，该代码本身是用 Go 编写的：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can see in the preceding output that the Docker engine built the two layers
    of our image, tagged the image, and pushed the image up to Docker Hub. We can
    now see the Docker image in our local registry via the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在前面的输出中看到，Docker 引擎构建了我们的镜像的两个层，标记了镜像，并将其推送到 Docker Hub。现在我们可以通过以下方式在我们的本地注册库中看到
    Docker 镜像：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can also see it on Docker Hub ([https://hub.docker.com/r/dwhitena/goregtrain/](https://hub.docker.com/r/dwhitena/goregtrain/))
    as illustrated here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在这里看到它（[https://hub.docker.com/r/dwhitena/goregtrain/](https://hub.docker.com/r/dwhitena/goregtrain/)）的
    Docker Hub 上，如图所示：
- en: '![](img/1b62cc44-5fd4-4a69-aa54-7a451709f74f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1b62cc44-5fd4-4a69-aa54-7a451709f74f.png)'
- en: We will see how to run and utilize this Docker image shortly, but let's first
    build another Docker image that will generate predictions based on our JSON-persisted
    models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快看到如何运行和利用这个 Docker 镜像，但首先让我们构建另一个 Docker 镜像，该镜像将根据我们 JSON 持久化的模型生成预测。
- en: Docker-izing model predictions
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型预测 Docker 化
- en: 'As with the training of our model, we are going to utilize command line arguments
    to specify the input directories and output directories utilized by our prediction
    program. This time we will have two input directories; one for the persisted model
    and one for a directory that will contain attributes from which we are to make
    predictions. Our program will, thus, do the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的模型训练一样，我们将利用命令行参数来指定我们的预测程序使用的输入目录和输出目录。这次我们将有两个输入目录；一个用于持久化的模型，另一个用于包含我们将从中进行预测的属性的目录。因此，我们的程序将执行以下操作：
- en: Read in the model from the model input directory.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型输入目录中读取模型。
- en: Walk over files in the attributes input directory.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历属性输入目录中的文件。
- en: For each file in the attributes input directory (containing attributes with
    no corresponding prediction of disease progression), utilize our loaded model
    to make a prediction of disease progression.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于属性输入目录中的每个文件（包含没有相应疾病进展预测的属性），使用我们加载的模型来预测疾病进展。
- en: Output the disease progression to an output file in the directory specified
    as a command line argument.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将疾病进展输出到由命令行参数指定的目录中的输出文件。
- en: Think of this process as follows. We have trained our model on historical data
    to predict disease progression and we want doctors or clinics to utilize this
    prediction in some way for new patients. They send us those patients attributes
    (**body mass index** (**BMI**) or body mass index and **long term growth** (**LTG**))
    and we make predictions based on these input attributes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个过程想象成这样。我们已经在历史数据上训练了我们的模型来预测疾病进展，我们希望医生或诊所以某种方式利用这种预测为新患者提供服务。他们发送给我们这些患者的属性（**体质指数**（**BMI**）或体质指数和**长期增长**（**LTG**）），然后我们根据这些输入属性进行预测。
- en: 'We will assume that the input attributes come to our program in the form of
    JSON files (which could also be thought of as a JSON message off a queue or a
    JSON API response) that look like the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设输入属性以JSON文件的形式进入我们的程序（这也可以被视为来自队列或JSON API响应的JSON消息），如下所示：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As such, let''s create the following struct types in our prediction program
    to decode the input attributes and marshal the output prediction:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们在我们的预测程序中创建以下结构类型，以解码输入属性并序列化输出预测：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s also create a function that will allow us to make a prediction based
    on a `ModelInfo` value that we read in from the model input directory (our persisted
    model, that is). This `prediction` function is shown in the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再创建一个函数，允许我们根据从模型输入目录（即我们持久化的模型）读取的`ModelInfo`值进行预测。以下代码展示了这个`prediction`函数：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This prediction function, along with the types, will then allow us to walk
    over any attribute JSON files in a specified input directory and output disease
    predictions for each of those files. This process is implemented in the following
    code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测函数以及类型将允许我们在指定的输入目录中遍历任何属性JSON文件，并为每个文件输出疾病预测。这个过程在以下代码中实现：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will use the same `Dockerfile` and `Makefile` that we used in the preceding
    section to Docker-ize this prediction program. The only difference is that we
    will name the Go binary `goregpredict` and we will tag the Docker image `dwhitena/goregpredict`.
    Building the Docker image with `make` should return a similar output to that in
    the preceding section:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与上一节相同的`Dockerfile`和`Makefile`来将此预测程序Docker化。唯一的区别是，我们将Go二进制文件命名为`goregpredict`，并将Docker镜像标记为`dwhitena/goregpredict`。使用`make`构建Docker镜像应该会返回与上一节类似的结果：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Testing the Docker images locally
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地测试Docker镜像
- en: Before pushing our Docker-ized modeling processes to any servers, it's wise
    to test them locally to ensure that we are seeing the behavior that we expect.
    Then, once we are satisfied with that behavior, we can rest assured that these
    Docker images will run exactly the same on any other host that is running Docker.
    This assurance makes the use of Docker images a significant contributor to maintaining
    reproducibility with our deploys.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在将我们的Docker化建模过程推送到任何服务器之前，在本地测试它们是明智的，以确保我们看到我们预期的行为。然后，一旦我们对这种行为感到满意，我们可以确信这些Docker镜像将在任何运行Docker的其他主机上以完全相同的方式运行。这种保证使得Docker镜像的使用成为我们部署中保持可重复性的重要贡献者。
- en: 'Let''s suppose that we have our training data and some input attribute files
    in the following directories:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的训练数据和一些输入属性文件位于以下目录中：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can run our Docker images as software containers locally using the `docker
    run` command. We will also utilize the `-v` flag that will let us mount local
    directories inside of a running container, allowing us to read and write files
    to and from our local filesystem.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`docker run`命令在本地以软件容器的形式运行我们的Docker镜像。我们还将利用`-v`标志，它允许我们将本地目录挂载到正在运行的容器中，使我们能够从本地文件系统中读取和写入文件。
- en: 'First, let''s run our single regression model training inside of the Docker
    container as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们按照以下方式在Docker容器中运行我们的单个回归模型训练：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, if we look at what is in our `model` directory, we will see the newly
    trained model coefficients and intercept in `model.json`, which was output from
    our program running in the Docker image. This is illustrated in the following
    code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们查看我们的`model`目录，我们将看到新训练的模型系数和截距在`model.json`中，这是我们的程序在Docker镜像中运行时输出的。以下代码展示了这一点：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Excellent! We trained our model using the Docker container. Now let''s utilize
    this model to make predictions. Specifically, let''s run our `goregpredict` Docker
    image using the trained model to make predictions for the three attribute files
    in `attributes`. In the following code, you will see that the attribute files
    do not have corresponding predictions before running the Docker image, but they
    do after running the Docker image:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 极好！我们使用Docker容器训练了我们的模型。现在让我们利用这个模型进行预测。具体来说，让我们使用训练好的模型运行我们的`goregpredict`
    Docker镜像，对`attributes`中的三个属性文件进行预测。在下面的代码中，你将看到在运行Docker镜像之前，属性文件没有相应的预测，但在运行Docker镜像之后，它们就有了：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running the Docker images on remote machines
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在远程机器上运行Docker镜像
- en: You might be thinking, *what's the big deal, you get the same functionality
    with Docker that we could get by just building and running your Go program*. Well,
    that's true locally, but the magic of Docker happens when we want to run our same
    functionality in an environment other than our laptop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能正在想，“这有什么大不了的，你通过构建和运行你的Go程序就能得到相同的功能”。这在本地是正确的，但Docker的魔力在于我们想要在除了笔记本电脑之外的环境中运行相同的功能时。
- en: We can take those Docker images and run them in the exact same way on any host
    that is running Docker, and they will produce the same exact behavior. This is
    true for any Docker image. You don't have to install any of the dependencies that
    might be layered inside of the Docker image. All you need is Docker.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些Docker镜像在任何运行Docker的主机上以完全相同的方式运行，并且它们将产生完全相同的行为。这对于任何Docker镜像都是成立的。你不需要安装Docker镜像内部可能包含的任何依赖项。你需要的只是Docker。
- en: Guess what? Our Docker images are just about 3 MB in size! This means that they
    will download to any host super fast and start up and run super fast. You don't
    have to worry about lugging around multiple gigabyte-sized virtual machines and
    manually specifying resources.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 猜猜看？我们的Docker镜像大小大约是3 MB！这意味着它们将非常快地下载到任何主机上，并且启动和运行也非常快。你不必担心携带多个多吉字节大小的虚拟机，并手动指定资源。
- en: Building a scalable and reproducible machine learning pipeline
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建可扩展且可重复的机器学习流程
- en: 'Docker sets up quite a bit of the way towards having our machine learning workflows
    deployed in our company''s infrastructure. However, there are still a few missing
    pieces, as outlined here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Docker为公司的基础设施中部署我们的机器学习工作流程做了很多工作。然而，正如这里概述的那样，还有一些缺失的部分。
- en: How do we string the various stages of our workflow together? In this simple
    example, we have a training stage and a prediction stage. In other pipelines,
    you might also have data preprocessing, data splitting, data combining, visualization,
    evaluation, and so on.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将工作流程的各个阶段连接起来？在这个简单的例子中，我们有一个训练阶段和一个预测阶段。在其他流程中，你可能还会有数据预处理、数据拆分、数据合并、可视化、评估等等。
- en: How to get the right data to the right stages of our workflow, especially as
    we receive new data and/or our data changes? It's not sustainable to manually
    copy new attributes over to a folder that is co-located with our prediction image
    every time we need to make new predictions, and we cannot log in to a server every
    time we need to update our training set.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将正确的数据传输到工作流程的各个阶段，尤其是在我们接收新数据或数据发生变化时？每次我们需要进行新的预测时，手动将新属性复制到与我们的预测镜像位于同一文件夹中是不可持续的，并且我们不能每次更新训练集时都登录到服务器。
- en: How will we be able to track and reproduce the various runs of our workflow
    for maintenance, continued development, or debugging? If we are making predictions
    over time and updating our model and/or training set over time, we need to understand
    which models produced which results to maintain reproducibility (and to meet compliance,
    in some cases).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将如何跟踪和重现我们的工作流程的各种运行，以进行维护、持续开发或调试？如果我们随着时间的推移进行预测并更新我们的模型和/或训练集，我们需要了解哪些模型产生了哪些结果，以保持可重复性（并在某些情况下满足合规性）。
- en: How can we scale our processing over multiple machines and, in some cases, over
    multiple shared resources? We likely need to run our processing on some shared
    set of resources in our company. It would be nice for us to be able to scale our
    processing over these resources as we need more computation power and/or as other
    applications are scheduled on these resources.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将我们的处理过程扩展到多台机器上，在某些情况下，扩展到多个共享资源上？我们可能需要在公司的一些共享资源集上运行我们的处理过程。如果我们需要更多的计算能力，或者当其他应用程序在这些资源上被调度时，我们能够扩展我们的处理过程将会很理想。
- en: Thankfully, there are a couple more open source tools (both written in Go) that
    solve these issues for us. Not only that, they let us use the Docker images that
    we already built as the main units of data processing. These tools are **Kubernetes**
    (**k8s**) and **Pachyderm**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，还有一些开源工具（都是用Go编写的）为我们解决了这些问题。不仅如此，它们还允许我们使用已经构建好的Docker镜像作为数据处理的主要单元。这些工具是**Kubernetes**（**k8s**）和**Pachyderm**。
- en: You have already been exposed to Pachyderm in [Chapter 1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml),
    *Gathering and Organizing Data*. In that chapter, we utilized Pachyderm to illustrate
    the idea of data versioning and, as you might be guessing, Pachyderm will help
    us solve some of the issues around managing data, tracking, and reproducibility.
    Pachyderm will also solve all of the remaining issues that we need to address
    around scalability and pipelining because Pachyderm provides both data management/versioning
    capabilities and data pipelining capabilities.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在[第1章](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml)，《收集和组织数据》中接触到了Pachyderm。在那章中，我们利用Pachyderm来阐述数据版本化的概念，正如您可能猜到的，Pachyderm将帮助我们解决一些关于数据管理、跟踪和可重复性的问题。Pachyderm还将解决我们需要解决的关于可扩展性和管道的所有剩余问题，因为Pachyderm提供了数据管理/版本化能力和数据处理管道能力。
- en: Kubernetes is a container orchestration engine, which is amazing at scheduling
    containerized workloads (like our Docker images) across a cluster of shared resources.
    It is wildly popular right now, and Pachyderm utilizes Kubernetes under the hood
    to manage containerized data processing pipelines.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个容器编排引擎，擅长在共享资源集群中调度容器化工作负载（如我们的Docker镜像）。它目前非常流行，Pachyderm在底层使用Kubernetes来管理容器化数据处理管道。
- en: Setting up a Pachyderm and Kubernetes cluster
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Pachyderm和Kubernetes集群
- en: 'Pachyderm, which runs on Kubernetes, can be deployed almost anywhere because
    Kubernetes can be deployed anywhere. You can deploy Pachyderm on any of the popular
    cloud providers, on premise, or even on your own laptop. All you need to do to
    deploy a Pachyderm cluster is the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上运行的Pachyderm几乎可以部署在任何地方，因为Kubernetes可以部署在任何地方。您可以在任何流行的云服务提供商、本地或甚至在自己的笔记本电脑上部署Pachyderm。部署Pachyderm集群您需要做的只是以下几步：
- en: Have a running Kubernetes cluster.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有一个运行的Kubernetes集群。
- en: Have access to an object store of your choice (for example, S3, **Glasglow Coma
    Scale** (**GCS**), or Minio). This object store will serve as the storage backing
    for the Pachyderm cluster where all the versioning and processed data is stored.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有权访问您选择的对象存储（例如，S3、**Glasglow Coma Scale**（**GCS**）或Minio）。这个对象存储将作为Pachyderm集群的存储后端，所有版本化和处理后的数据都存储在这里。
- en: Deploy Pachyderm on the Kubernetes cluster using the Pachyderm CLI `pachctl`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Pachyderm CLI `pachctl`在Kubernetes集群上部署Pachyderm。
- en: Detailed instructions to deploy Pachyderm on any cloud provider or on-premise
    can be found at [http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html](http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html).
    Alternatively, you can easily experiment with and develop against a local Pachyderm
    cluster using `minikube`, as further detailed at [http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html](http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html](http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html)可以找到部署Pachyderm到任何云服务提供商或本地环境的详细说明。或者，您可以使用`minikube`轻松地实验和开发本地Pachyderm集群，具体细节请参阅[http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html](http://pachyderm.readthedocs.io/en/latest/getting_started/local_installation.html)。
- en: 'Following one set of those instructions should get you to a state where you
    have a Kubernetes cluster running in the following state (which can be checked
    using Kubernetes CLI tool called `kubectl`):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些说明中的一套，您应该可以达到以下状态：有一个Kubernetes集群正在以下状态下运行（可以使用名为`kubectl`的Kubernetes CLI工具进行检查）：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, you can see that the Pachyderm daemon (or server), `pachd`, is running
    in the Kubernetes cluster as a **pod**, which is simply a group of one or more
    containers. Our pipeline stages will also run as Kubernetes pods, but you won't
    have to worry too much about that as Pachyderm will take care of the details.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到Pachyderm守护程序（或服务器），`pachd`，作为**pod**在Kubernetes集群中运行，pod只是由一个或多个容器组成的一组。我们的管道阶段也将作为Kubernetes
    pod运行，但您不必过于担心这一点，因为Pachyderm会处理细节。
- en: The ports and IPs listed in the above output may vary depending on your deployment
    and various configurations. However, a healthy Pachyderm cluster should look very
    similar.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的输出中列出的端口和IP地址可能因你的部署和不同的配置而异。然而，一个健康的Pachyderm集群看起来应该非常相似。
- en: 'If your Pachyderm cluster is running and you followed one of the Pachyderm
    deploy guides, you should also have the `pachctl` CLI tool installed. When `pachctl`
    is successfully connected to the Pachyderm cluster, you can run the following
    as a further sanity check (where the version number may change based on the Pachyderm
    version you are running):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的Pachyderm集群正在运行，并且你遵循了Pachyderm部署指南之一，那么你也应该已经安装了`pachctl` CLI工具。当`pachctl`成功连接到Pachyderm集群时，你可以运行以下命令作为进一步的合理性检查（版本号可能根据你运行的Pachyderm版本而变化）：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you have any trouble getting Pachyderm deployed or have any trouble building
    or running pipelines, the Pachyderm community has a great public Pachyderm Slack
    channel. You can join by visiting [http://slack.pachyderm.io/](http://slack.pachyderm.io/).
    The community is active there every day, and any questions you have will be welcome.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到部署Pachyderm或构建和运行流程的任何问题，Pachyderm社区有一个优秀的公共Pachyderm Slack频道。你可以通过访问[http://slack.pachyderm.io/](http://slack.pachyderm.io/)加入。社区每天都在那里活跃，你提出的任何问题都将受到欢迎。
- en: Building a Pachyderm machine learning pipeline
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建Pachyderm机器学习流程
- en: 'As you have already seen, our example machine learning pipeline has two stages.
    The model stage of our pipeline will train a model and export a persisted version
    of this model to a file. The prediction stage of our pipeline will utilize the
    trained model to make predictions for input attribute files. Overall, this pipeline
    will look like the following in Pachyderm:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们的示例机器学习流程有两个阶段。我们的流程中的模型阶段将训练一个模型并将该模型的持久化版本导出到文件中。我们的流程中的预测阶段将利用训练好的模型对输入属性文件进行预测。总的来说，这个流程在Pachyderm中看起来如下：
- en: '![](img/cfb8d094-54f6-4338-b91a-4d3008681c50.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cfb8d094-54f6-4338-b91a-4d3008681c50.png)'
- en: Each of the cylinders in the preceding figure represents a Pachyderm data repository
    of versioned data. You were already exposed to these data repositories in [Chapter
    1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml), *Gathering and Organizing Data*.
    Each of the boxes represents a containerized data pipeline stage. The basic units
    of data processing in Pachyderm are Docker images. Thus, we can utilize the Docker
    images that we created in preceding sections in this data pipeline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个图中的每个圆柱体代表一个Pachyderm版本化数据的数据存储库。你已经在[第1章](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml)，“收集和组织数据”中接触到了这些数据存储库。每个框代表一个容器化的数据处理阶段。Pachyderm中数据处理的基本单元是Docker镜像。因此，我们可以利用在前几节中创建的Docker镜像来利用这个数据流程。
- en: By stringing together versioned collections of data (again, think about these
    as a sort of *Git for data*), processing these collections of data with containers,
    and saving results to other versioned collections of data, Pachyderm pipelines
    have some pretty interesting and useful implications.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将版本化的数据集合（再次，将这些视为一种“数据Git”），使用容器处理这些数据集合，并将结果保存到其他版本化的数据集合中，Pachyderm流程有一些相当有趣和有用的含义。
- en: First, we can go back at any point in time to see the state of any portion of
    our data at that point in time. This might help us as we further develop our model
    if we are wanting to develop a certain state of our data. It might also help us
    to collaboratively develop as a team by tracking the team's data over time. Not
    all changes to data are good, and we need the ability to revert after bad or corrupt
    data is committed into the system.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以回到任何时间点，查看我们数据在那个时间点的任何部分的状态。如果我们想要开发我们数据的一个特定状态，这可能会在进一步开发我们的模型时帮助我们。这也可能帮助我们通过跟踪团队随时间变化的数据来协作开发。并不是所有的数据更改都是好的，我们需要在将不良或损坏的数据提交到系统中后能够回滚的能力。
- en: Next, all of our pipeline results are linked to the exact Docker images and
    exact states of other data that produced these results. The Pachyderm team calls
    this **provenance**. It is the ability to understand what pieces of processing
    and which pieces of data contributed to a particular result. For example, with
    this pipeline, we are able to determine the exact version of our persisted model
    that produced a particular result, along with the exact Docker image that produced
    that model and the exact state of the training data that was input to that exact
    Docker image. We are, thus, able to exactly reproduce an entire run of any pipeline,
    debug strange model behavior, and attribute results to the corresponding input
    data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们所有管道的结果都与产生这些结果的精确Docker镜像和其他数据的精确状态相关联。Pachyderm团队称这为**溯源**。这是理解哪些处理部分和哪些数据部分贡献了特定结果的能力。例如，使用这个管道，我们能够确定产生特定结果的持久化模型的精确版本，以及产生该模型的精确Docker镜像和输入到该精确Docker镜像的训练数据的精确状态。因此，我们能够精确地重现任何管道的整个运行过程，调试奇怪模型行为，并将结果归因于相应的输入数据。
- en: Finally, because Pachyderm knows which portions of your data repositories are
    new or different, our data pipeline can process data incrementally and can be
    triggered automatically. Let's say that we have already committed one million
    attribute files into the `attributes` data repository, and then we commit ten
    more attribute files. We don't have to reprocess the first million to update our
    results. Pachyderm knows that we only need to process the ten new files and will
    send these to the `prediction` stage to be processed. Moreover, Pachyderm will
    (by default) automatically trigger this update because it knows that the update
    is needed to keep the results in sync with the input.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为Pachyderm知道你的数据存储库中哪些部分是新的或不同的，我们的数据处理管道可以增量处理数据，并且可以自动触发。假设我们已经将一百万个属性文件提交到`attributes`数据存储库中，然后我们提交了十个更多的属性文件。我们不需要重新处理前一百万个文件来更新我们的结果。Pachyderm知道我们只需要处理这十个新文件，并将它们发送到`prediction`阶段进行处理。此外，Pachyderm将（默认情况下）自动触发这个更新，因为它知道需要更新以保持结果与输入同步。
- en: Incremental processing and automatic triggers are some of the default behaviors
    of Pachyderm pipelines, but they are not the only things you can do with Pachyderm.
    In fact, we will only scratch the surface here of what is possible with Pachyderm.
    You can do distributed map/reduce style operations, distributed image processing,
    periodic processing of database tables, and much, much more. Thus, the techniques
    here should be transferable to a variety of domains.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 增量处理和自动触发是Pachyderm管道的一些默认行为，但它们并不是你可以用Pachyderm做的唯一事情。实际上，我们在这里只是触及了Pachyderm可能性的表面。你可以进行分布式map/reduce风格的操作，分布式图像处理，定期处理数据库表，以及更多更多。因此，这里的技巧应该可以转移到各种领域。
- en: Creating and filling the input repositories
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和填充输入存储库
- en: To create the Pachyderm pipeline on our deployed Pachyderm cluster, we first
    need to create the input repositories for the pipeline. Remember, our pipeline
    has the `training` and `attributes` data repositories that drive the rest of the
    pipeline. As we put data into these, Pachyderm will trigger the downstream portions
    of the pipeline and calculate the results.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们部署的Pachyderm集群上创建Pachyderm管道，我们首先需要为管道创建输入存储库。记住，我们的管道有`training`和`attributes`数据存储库，它们驱动着管道的其余部分。当我们将这些数据放入这些存储库时，Pachyderm将触发管道的下游部分并计算结果。
- en: We saw in [Chapter 1](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml), *Gathering
    and Organizing Data*, how we can create data repositories in Pachyderm and commit
    data into these repositories using `pachctl`, but let's try to do this directly
    from a Go program here. In this particular example, there is not any real advantage
    to doing this, due to the fact that we already have our training and test data
    in files and we can easily commit those files into Pachyderm by name.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](4f556f8e-6876-48ca-9ac5-f897b733e23e.xhtml)“收集和组织数据”中，我们看到了如何在Pachyderm中创建数据存储库，并使用`pachctl`将这些数据提交到这些存储库。但在这里，我们尝试直接从一个Go程序中完成这个操作。在这个特定的例子中，这样做并没有任何实际优势，因为我们已经将训练和测试数据存储在文件中，我们可以很容易地通过名称将这些文件提交到Pachyderm。
- en: However, imagine a more realistic scenario. We might want our data pipeline
    to be integrated with some other Go services that we have already written at our
    company. One of these services might process incoming patient medical attributes
    from doctors or clinics, and we want to feed these attributes to our data pipeline,
    such that the data pipeline can make the corresponding predictions of disease
    progression.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，想象一个更现实的场景。我们可能希望我们的数据管道与我们公司已经编写的某些其他Go服务集成。这些服务之一可能会处理来自医生或诊所的传入患者医疗属性，我们希望将这些属性馈送到我们的数据管道中，以便数据管道可以做出相应的疾病进展预测。
- en: 'In such a scenario, we would ideally like to pass the attributes directly into
    Pachyderm from our service instead of manually copying all that data. This is
    where the Pachyderm Go client, `github.com/pachyderm/pachyderm/src/client`, can
    come in very handy. Using the Pachyderm Go client, we can create our input repositories
    as follows (after connecting to the Pachyderm cluster):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们理想情况下希望直接从我们的服务将属性传递到Pachyderm，而不是手动复制所有这些数据。这就是Pachyderm Go客户端，`github.com/pachyderm/pachyderm/src/client`能派上大用场的地方。使用Pachyderm
    Go客户端，我们可以创建我们的输入仓库，如下所示（在连接到Pachyderm集群之后）：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Compiling this and running it creates the `repos` as expected, which can be
    confirmed as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行它确实创建了预期的`repos`，如下所示：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For simplicity, the above code just creates and checks the repo once. If you
    ran the code again, you would get an error because the repo was already created.
    To improve on this, you could check if the repo exists and then create it if it
    does not exist. You can also check that the repo exists by name.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，上面的代码只是创建并检查了一次仓库。如果你再次运行代码，你会得到一个错误，因为仓库已经被创建了。为了改进这一点，你可以检查仓库是否存在，如果不存在则创建它。你也可以通过名称检查仓库是否存在。
- en: 'Now, let''s go ahead and put an attributes file in the `attributes` repository
    and the `diabetes.csv` training data set into the `training` repository. This
    is also very easily done directly in Go, via the Pachyderm client:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将一个属性文件放入`attributes`仓库，并将`diabetes.csv`训练数据集放入`training`仓库。这也可以通过Pachyderm客户端直接在Go中轻松完成：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Running this does indeed commit the data into Pachyderm in the proper data
    repositories, as can be seen here:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作确实将数据提交到Pachyderm的正确数据仓库中，如下所示：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It is important to finish the commits that we started above. If you leave an
    orphaned commit hanging you might block other commits of data. Thus, you might
    consider deferring the finish of the commit, just to be safe.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 完成我们上面开始提交的提交是很重要的。如果你留下一个孤立的提交悬而未决，你可能会阻塞其他数据提交。因此，你可能考虑推迟提交的完成，以确保安全。
- en: Good! Now we have data in the proper input data repositories on our remote Pachyderm
    cluster. Next, we can actually process this data and produce results.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！现在我们已经在远程Pachyderm集群的适当输入数据仓库中有数据了。接下来，我们实际上可以处理这些数据并生成结果。
- en: Creating and running the processing stages
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和运行处理阶段
- en: Processing stages are created declaratively via a **pipeline specification**
    in Pachyderm. If you have worked with Kubernetes before, this type of interaction
    should be familiar. Basically, we declare to Pachyderm what processing we want
    to take place, and Pachyderm handles all the details to make sure that this processing
    happens as declared.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pachyderm中，通过一个**管道规范**声明性地创建处理阶段。如果你之前使用过Kubernetes，这种交互应该很熟悉。基本上，我们向Pachyderm声明我们想要进行哪些处理，然后Pachyderm处理所有细节以确保这些处理按声明执行。
- en: 'Let''s first create the `model` stage of our pipeline using a JSON pipeline
    specification that is stored in `train.json`. This JSON specification is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用存储在`train.json`中的JSON管道规范来创建我们管道的`model`阶段。这个JSON规范如下：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This might look somewhat complicated, but there are really only a few things
    happening here. Let''s dissect the specification to learn about the different
    pieces:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有些复杂，但实际上这里只发生了几件事情。让我们分析这个规范，了解不同的部分：
- en: First, we are telling Pachyderm to name our pipeline `model`.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们告诉Pachyderm将我们的管道命名为`model`。
- en: Next, we are telling Pachyderm that we want this `model` pipeline to process
    data using our `dwhitena/goregtrain:single`, and we want the pipeline to run our
    `goregtrain` binary in the container when processing the data.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们告诉Pachyderm，我们希望这个`model`管道使用我们的`dwhitena/goregtrain:single`来处理数据，并且我们希望在处理数据时在容器中运行我们的`goregtrain`二进制文件。
- en: Finally, the specification tells Pachyderm that we intend to process the `training`
    data repository as input.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，规范告诉 Pachyderm 我们打算将 `training` 数据存储库作为输入处理。
- en: To understand some of the other details of the specification, we need to first
    discuss what will happen when we create the pipeline on our Pachyderm cluster
    using the specification. When this happens, Pachyderm will use Kubernetes to schedule
    one or more worker pods on the Kubernetes cluster. These worker pods will then
    wait, ready to process any data supplied to them by the Pachyderm daemon. When
    there is data that needs to be processed in the input repository, `training`,
    the Pachyderm daemon will trigger a job in which these workers will process the
    data. Pachyderm will automatically mount the input data in the container at `/pfs/<input
    repo name>` (`/pfs/training` in this case). Data the container writes out to `/pfs/out`
    will be automatically versioned in an output data repository with the same name
    as the pipeline (`model` in this case).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解规范的一些其他细节，我们首先需要讨论当我们使用规范在我们的 Pachyderm 集群上创建管道时会发生什么。当这种情况发生时，Pachyderm
    将使用 Kubernetes 在 Kubernetes 集群上调度一个或多个工作 pod。这些工作 pod 将等待，准备好处理由 Pachyderm 守护进程提供给它们的任何数据。当输入存储库
    `training` 中有需要处理的数据时，Pachyderm 守护进程将触发一个作业，这些工作进程将处理数据。Pachyderm 将自动将输入数据挂载到容器的
    `/pfs/<input repo name>`（在本例中为 `/pfs/training`）。容器写入 `/pfs/out` 的数据将被自动版本化到具有与管道相同名称的输出数据存储库（在本例中为
    `model`）。
- en: The `parallelism_spec` portion of the specification lets us tell Pachyderm how
    many workers it should spin up to process the input data. Here we will just spin
    up a single worker and process the data serially. Later in the chapter, we will
    discuss parallelizing our pipeline and the associated data sharding, which is
    controlled by the `glob` pattern in the `input` section of the specification.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 规范中的 `parallelism_spec` 部分让我们可以告诉 Pachyderm 它应该启动多少个工作进程来处理输入数据。在这里，我们只启动一个工作进程并串行处理数据。在章节的后面，我们将讨论并行化我们的管道和相关的数据分片，这由规范中
    `input` 部分的 `glob` 模式控制。
- en: 'Enough talk though! Let''s actually create this `model` pipeline stage on our
    Pachyderm cluster and get to processing some data. The pipeline can easily be
    created as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然说得够多了！让我们实际上在我们的 Pachyderm 集群上创建这个 `model` 管道阶段，并开始处理一些数据。管道可以轻松地按以下方式创建：
- en: '[PRE24]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'When we do this, we can see the worker(s) that Pachyderm creates on the Kubernetes
    cluster using `kubectl` as shown in the following code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们这样做时，我们可以使用 `kubectl` 在 Kubernetes 集群中看到 Pachyderm 创建的工作进程（如下面的代码所示）：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will also see that Pachyderm has automatically triggered a job to perform
    our model training. Remember, this happens because Pachyderm knows that there
    is data in the input repository, `training`, that has not been processed. You
    can see the triggered job along with its results as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将看到 Pachyderm 已经自动触发了一个作业来执行我们的模型训练。记住，这是因为 Pachyderm 知道输入存储库 `training`
    中有尚未处理的数据。你可以如下查看触发的作业及其结果：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Great! We can see that our model training produced the results that we expected.
    However, Pachyderm was smart enough to trigger the pipeline, and it will update
    this model whenever we modify the training dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们可以看到我们的模型训练产生了我们预期的结果。然而，Pachyderm 足够聪明，足以触发管道，并且每当我们在训练数据集中修改模型时，它都会更新这个模型。
- en: 'The `prediction` pipeline can be created in a similar manner. Its pipeline
    specification, `pipeline.json`, is shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`prediction` 管道也可以以类似的方式创建。其管道规范 `pipeline.json` 如下所示：'
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The major difference in this pipeline specification is that we have two input
    repositories rather than one. These two input repositories are the `attributes`
    repository that we previously created and the `model` repository that contains
    the output of the `model` pipeline. These are combined using a `cross` primitive.
    `cross` ensures that all relevant pairs of our model and attribute files are processed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个管道规范中的主要区别是我们有两个输入存储库而不是一个。这两个输入存储库是我们之前创建的 `attributes` 存储库和包含 `model` 管道输出的
    `model` 存储库。这些是通过 `cross` 原始操作结合的。`cross` 确保处理我们模型和属性文件的所有相关对。
- en: If you are interested, you can find out more about the other ways to combine
    data in Pachyderm in the Pachyderm docs; go to [http://pachyderm.readthedocs.io/en/latest/](http://pachyderm.readthedocs.io/en/latest/).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，你可以在 Pachyderm 文档中了解更多关于在 Pachyderm 中组合数据的其他方法；请访问 [http://pachyderm.readthedocs.io/en/latest/](http://pachyderm.readthedocs.io/en/latest/)。
- en: 'Creating this pipeline and examining the results (corresponding to `1.json`)
    can be accomplished as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建此流水线并检查结果（对应于`1.json`）可以按照以下方式完成：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see that another data repository, `prediction`, was created to version
    the output of the `prediction` pipeline. In this manner, Pachyderm gradually builds
    up links between input data repositories, data processing stages, and output data
    repositories, such that it always knows what data and processing components are
    linked.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，为了版本控制`prediction`流水线的输出，创建了一个新的数据仓库`prediction`。以这种方式，Pachyderm逐渐建立起输入数据仓库、数据处理阶段和输出数据仓库之间的链接，这样它总是知道哪些数据和数据处理组件是链接在一起的。
- en: 'The full machine learning pipeline is now deployed and we have run each stage
    of the pipeline once. However, the pipeline is ready and waiting to process more
    data. All we have to do is put data at the top of the pipeline and Pachyderm will
    automatically trigger any downstream processing that needs to take place to update
    our results. Let''s illustrate this by committing two more files into the `attributes`
    repository, as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的机器学习流水线现在已部署，我们已经运行了流水线的每个阶段。然而，流水线已经准备好并等待处理更多数据。我们只需将数据放在流水线的顶部，Pachyderm就会自动触发任何需要发生的下游处理来更新我们的结果。让我们通过将两个更多文件提交到`attributes`仓库来展示这一点，如下所示：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will automatically trigger another prediction job to update our results.
    The new job can be seen in the following code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将自动触发另一个预测作业来更新我们的结果。新的作业可以在以下代码中看到：
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Also, we can see that the `prediction` repository has two new results from
    running our prediction code again on the two new input files:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以看到，在再次对两个新的输入文件运行预测代码后，`prediction`仓库出现了两个新的结果：
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that Pachyderm did not reprocess `1.json` here even though it is shown
    with the latest results. Under the hood, Pachyderm knows that there was already
    a result corresponding to `1.json` in `attributes`, so there is no need to reprocess
    it.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管它显示了最新的结果，但Pachyderm在这里并没有重新处理`1.json`。在底层，Pachyderm知道在`attributes`中已经有一个与`1.json`相对应的结果，因此没有必要重新处理它。
- en: Updating pipelines and examining provenance
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新流水线和检查溯源
- en: Over time, we are likely going to want to update our machine learning models.
    The data that they are processing may have changed over time or maybe we have
    just developed a different or better model. In any event, it is likely that we
    are going to need to update our pipeline.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们很可能会想要更新我们的机器学习模型。它们处理的数据可能已经改变，或者我们可能已经开发了一个不同的或更好的模型。无论如何，我们很可能会需要更新我们的流水线。
- en: Let's assume that our pipeline is in the state described in the previous section,
    where we have created the full pipeline, the training process has run once, and
    we have processed two commits into the `attributes` repository. Now, let's update
    the model pipeline to train our multiple regression
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的流水线处于前一个章节所描述的状态，其中我们已经创建了完整的流水线，训练过程已经运行过一次，并且我们已经将两个提交处理到了`attributes`仓库中。现在，让我们更新模型流水线以训练我们的多元回归
- en: 'Now, let''s update the model pipeline to train our multiple regression models
    instead of our single regression model. This is actually really easy. We just
    need to change `"image": "dwhitena/goregtrain:single"` in our `model.json` pipeline
    specification to `"image": "dwhitena/goregtrain:multi"` and then update the pipeline
    as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们更新模型流水线以训练我们的多元回归模型，而不是单个回归模型。这实际上非常简单。我们只需要将`model.json`流水线规范中的`"image":
    "dwhitena/goregtrain:single"`更改为`"image": "dwhitena/goregtrain:multi"`，然后按照以下方式更新流水线：'
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When we give Pachyderm this new specification, Pachyderm will automatically
    update the worker pods such that they are running the containers with the multiple
    regression model. Also, because we supplied the `--reprocess` flag, Pachyderm
    will trigger one or more new jobs to reprocess any input commits with the new
    image that was previously processed with the old image. We can see these reprocessing
    jobs as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向Pachyderm提供这个新的规范时，Pachyderm将自动更新工作节点，使它们运行带有多元回归模型的容器。此外，因为我们提供了`--reprocess`标志，Pachyderm将触发一个或多个新的作业来重新处理任何使用新图像处理过的旧图像的输入提交。我们可以按照以下方式查看这些重新处理作业：
- en: '[PRE33]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Looking at this output, we can notice something interesting and very useful.
    When our `model` stage trained our new multiple regression model and output this
    model as `model.json`, Pachyderm saw that the results of our `prediction` pipeline
    were now out of sync with the latest model. As such, Pachyderm automatically triggered
    another job to update our previous results using the new multiple regression models.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这个输出，我们可以注意到一些有趣且非常有用的东西。当我们的`model`阶段训练了新的多元回归模型并将该模型输出为`model.json`时，Pachyderm发现我们的`prediction`管道的结果现在与最新模型不同步。因此，Pachyderm自动触发了另一个作业，使用新的多元回归模型更新我们之前的结果。
- en: This workflow is super useful to manage both deployed models and during the
    development of models, because you do not need to manually shuffle around a bunch
    of model versions and related data. Everything happens automatically. However,
    a natural question arises from this update. How are we to know which models produced
    which results?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这种工作流程在管理已部署的模型以及在模型开发期间都非常有用，因为你不需要手动移动一大堆模型版本和相关数据。所有事情都是自动发生的。然而，从这个更新中自然会产生一个疑问。我们如何知道哪些模型产生了哪些结果？
- en: 'Well, this is where Pachyderm''s data versioning combined with the pipelining
    really shines. We can look at any prediction result and inspect that commit to
    determine the commit''s provenance. This is illustrated here:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这正是Pachyderm的数据版本控制与管道结合真正发光的地方。我们可以查看任何预测结果并检查相应的提交，以确定提交的来源。这在此处得到了说明：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In this case, we can see that prediction `ef32a74b04ee4edda7bc2a2b469f3e03`
    resulted from model `f54ca1a0142542579c1543e41f5e9403`, which can be retrieved
    here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到预测`ef32a74b04ee4edda7bc2a2b469f3e03`是由模型`f54ca1a0142542579c1543e41f5e9403`产生的，该模型可以在此处检索到：
- en: '[PRE35]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'However, if we look at a previous prediction, we will see that it resulted
    from a different model:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们查看之前的预测，我们会看到它是由不同的模型产生的：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We can always trace back the provenance of any piece of data in our pipeline,
    no matter how many jobs are run and how many times we update our pipeline. This
    is very important in creating sustainable machine learning workflows that can
    be maintained over time, improved upon, and updated without major headaches.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以始终追溯管道中任何数据片段的来源，无论运行了多少个作业以及我们更新管道的次数。这对于创建可持续的机器学习工作流程非常重要，这些工作流程可以随着时间的推移进行维护、改进和更新，而不会遇到大的麻烦。
- en: Scaling pipeline stages
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放管道阶段
- en: Each pipeline specification in Pachyderm can have a corresponding `parallelism_spec`
    field. This field, along with the `glob` patterns in your inputs, lets you parallelize
    your pipeline stages over their input data. Each pipeline stage is individually
    scalable independent of all the other pipeline stages.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm中的每个管道规范都可以有一个相应的`parallelism_spec`字段。这个字段，连同你的输入中的`glob`模式，允许你并行化你的管道阶段在其输入数据上。每个管道阶段都是独立可扩展的，与其他所有管道阶段无关。
- en: 'The `parallelism_spec` field in the pipeline specification lets you control
    how many workers Pachyderm will spin up to process data in that pipeline stage.
    For example, the following `parallelism_spec` would tell Pachyderm to spin up
    10 workers for a pipeline:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 管道规范中的`parallelism_spec`字段允许你控制Pachyderm将启动多少个工作进程来处理该管道阶段的数據。例如，以下`parallelism_spec`将告诉Pachyderm为管道启动10个工作进程：
- en: '[PRE37]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `glob` patterns in your inputs tell Pachyderm how it can share your input
    data over the workers declared by the `parallelism_spec`. For example, a `glob`
    pattern of `/*` would tell Pachyderm that it can split up your input data into
    datums consisting of any files of directories at the root of your repository.
    A glob pattern of `/*/*` would tell Pachyderm that it can split up your data into
    datums consisting of files or directories at a level deep into your repository
    directory structure. If you are new to glob patterns, check out this description
    with some examples: [https://en.wikipedia.org/wiki/Glob_(programming)](https://en.wikipedia.org/wiki/Glob_(programming)).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输入中的`glob`模式告诉Pachyderm如何根据`parallelism_spec`中声明的工怍进程共享你的输入数据。例如，`glob`模式`/*`会告诉Pachyderm它可以分割你的输入数据为包含任何文件或目录在仓库根目录的datums。`glob`模式`/*/*`会告诉Pachyderm它可以分割你的数据为包含文件或目录在仓库目录结构深处的datums。如果你对glob模式不熟悉，可以查看以下带有一些示例的描述：[https://en.wikipedia.org/wiki/Glob_(programming)](https://en.wikipedia.org/wiki/Glob_(programming))。
- en: 'In the case of our example pipeline, we could imagine that we need to scale
    the `prediction` stage of our pipeline because we are seeing a huge influx of
    attributes that need corresponding predictions. If this was the case, we could
    change `parallelism_spec` in our `prediction.json` pipeline specification to `"constant":
    "10"`. As soon as we update the pipeline with the new specification, Pachyderm
    will spin up 10 new workers for the `prediction` pipeline, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的示例管道中，我们可以想象我们需要扩展管道的 `prediction` 阶段，因为我们看到了大量需要相应预测的属性涌入。如果情况如此，我们可以在
    `prediction.json` 管道规范中的 `parallelism_spec` 将其更改为 `"constant": "10"`。一旦我们使用新的规范更新管道，Pachyderm
    将启动 10 个新的工作节点用于 `prediction` 管道，如下所示：'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, any new commits of attributes into the `attributes` repo will be processed
    in parallel by the 10 workers. For example, if we committed 100 more attribute
    files into `attributes`, Pachyderm would send 1/10 of those files to each of the
    10 workers to process in parallel. All of the results would still be seen the
    same way in the `prediction` repo.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，任何将属性提交到 `attributes` 仓库的新提交都将由 10 个工作节点并行处理。例如，如果我们向 `attributes` 提交了 100
    个更多的属性文件，Pachyderm 将将其中 1/10 的文件发送到每个工作节点以并行处理。所有结果仍然以相同的方式在 `prediction` 仓库中可见。
- en: Of course, setting a constant number of workers is not the only way you can
    do scaling with Pachyderm and Kubernetes. Pachyderm will let you scale down workers
    as well by setting a scale down threshold for idle workers. In addition, Pachyderm
    auto-scaling of workers can be combined with auto-scaling of Kubernetes cluster
    resources to deal with bursts of data and batch processing. Still, further, Pachyderm
    will allow you to specify the resources needed for pipelines and, using this resource
    specification, you can offload machine learning workloads to accelerators (GPUs,
    for example).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，设置固定数量的工作节点并不是使用 Pachyderm 和 Kubernetes 进行扩展的唯一方法。Pachyderm 还允许您通过为空闲工作节点设置缩减阈值来缩减工作节点。此外，Pachyderm
    的工作节点自动扩展可以与 Kubernetes 集群资源的自动扩展相结合，以处理数据突发和批量处理。更进一步，Pachyderm 还允许您指定管道所需的资源，并使用这种资源规范将机器学习工作负载卸载到加速器（例如
    GPU）。
- en: References
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Docker:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'Docker:'
- en: 'Introduction to Docker: [https://training.docker.com/introduction-to-docker](https://training.docker.com/introduction-to-docker)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 简介：[https://training.docker.com/introduction-to-docker](https://training.docker.com/introduction-to-docker)
- en: 'Building minimal Docker images for Go apps: [https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/](https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 Go 应用构建最小化 Docker 镜像：[https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/](https://blog.codeship.com/building-minimal-docker-containers-for-go-applications/)
- en: 'Pachyderm:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pachyderm:'
- en: 'Public Slack channel: [http://slack.pachyderm.io/](http://slack.pachyderm.io/)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共 Slack 频道：[http://slack.pachyderm.io/](http://slack.pachyderm.io/)
- en: 'Getting started guide: [http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html](http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入门指南：[http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html](http://pachyderm.readthedocs.io/en/latest/getting_started/getting_started.html)
- en: 'Go client docs: [https://godoc.org/github.com/pachyderm/pachyderm/src/client](https://godoc.org/github.com/pachyderm/pachyderm/src/client)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go 客户端文档：[https://godoc.org/github.com/pachyderm/pachyderm/src/client](https://godoc.org/github.com/pachyderm/pachyderm/src/client)
- en: 'Machine learning with Pachyderm: [http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html](http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pachyderm 进行机器学习：[http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html](http://pachyderm.readthedocs.io/en/latest/cookbook/ml.html)
- en: 'Machine learning examples: [http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning](http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习示例：[http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning](http://pachyderm.readthedocs.io/en/latest/examples/readme.html#machine-learning)
- en: 'Distributed processing with Pachyderm: [http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html](http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pachyderm 进行分布式处理：[http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html](http://pachyderm.readthedocs.io/en/latest/fundamentals/distributed_computing.html)
- en: 'Common Pachyderm deployments: [http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html](http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的 Pachyderm 部署：[http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html](http://pachyderm.readthedocs.io/en/latest/deployment/deploy_intro.html)
- en: 'Auto-scaling Pachyderm clusters: [http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html](http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '自动扩展 Pachyderm 集群: [http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html](http://pachyderm.readthedocs.io/en/latest/managing_pachyderm/autoscaling.html)'
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: From merely gathering CSV data to a fully deployed, distributed machine learning
    pipeline, we did it! We can now build machine learning models with Go and deploy
    these models with confidence on a cluster of machines. These patterns should allow
    you to build and deploy a whole variety of intelligent applications, and I can't
    wait to hear about what you create! Please stay in touch.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从仅仅收集 CSV 数据到完全部署的分布式机器学习管道，我们做到了！现在我们可以用 Go 语言构建机器学习模型，并在机器集群上自信地部署这些模型。这些模式应该允许你构建和部署各种智能应用，我迫不及待地想听听你创造了什么！请保持联系。
