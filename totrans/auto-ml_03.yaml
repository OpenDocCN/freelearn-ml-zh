- en: '*Chapter 2*: Automated Machine Learning, Algorithms, and Techniques'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*: 自动化机器学习、算法和技术'
- en: '"Machine intelligence is the last invention that humanity will ever need to
    make."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"机器智能是人类将永远需要的最后一种发明。"'
- en: – Nick Bostrom
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – 尼克·博斯特罗姆
- en: '"The key to artificial intelligence has always been the representation."'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '"人工智能的关键始终是表示。"'
- en: – Jeff Hawkins
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: – 杰夫·哈克
- en: '"By far, the greatest danger of artificial intelligence is that people conclude
    too early that they understand it."'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '"到目前为止，人工智能最大的危险是人们过早地得出他们理解它的结论。"'
- en: – Eliezer Yudkowsky
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: – 埃利泽·尤德科夫斯基
- en: Automating the automation sounds like one of those wonderful Zen meta ideas,
    but learning to learn is not without its challenges. In the last chapter, we covered
    the **Machine Learning** (**ML**) development life cycle, and defined automated
    ML, with a brief overview of how it works.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化自动化听起来像是一种美好的禅宗元概念，但学会学习并非没有挑战。在上章中，我们介绍了机器学习（ML）的开发生命周期，并简要概述了它是如何工作的。
- en: In this chapter, we will explore under-the-hood technologies, techniques, and
    tools used to make automated ML possible. Here, you will see how **AutoML** actually
    works, the algorithms and techniques of automated feature engineering, automated
    model and hyperparameter turning, and automated deep learning. You will learn
    about meta-learning as well as state-of-the-art techniques, including Bayesian
    optimization, reinforcement learning, evolutionary algorithms, and gradient-based
    approaches.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使自动化机器学习成为可能的技术、技术和工具。在这里，您将了解**AutoML**实际上是如何工作的，自动化特征工程、自动化模型和超参数调整以及自动化深度学习的算法和技术。您还将了解元学习和最先进的技术，包括贝叶斯优化、强化学习、进化算法和基于梯度的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Automated ML – Opening the hood
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化机器学习 – 打开引擎盖
- en: Automated feature engineering
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化特征工程
- en: Hyperparameter optimization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Neural architecture search
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经架构搜索
- en: Automated ML – Opening the hood
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化机器学习 – 打开引擎盖
- en: 'To oversimplify, a typical ML pipeline comprises data cleaning, feature selection,
    pre-processing, model development, deployment, and consumption steps, as seen
    in the following workflow:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，典型的机器学习（ML）流程包括数据清洗、特征选择、预处理、模型开发、部署和消费步骤，如下所示的工作流程：
- en: '![Figure 2.1 – The ML life cycle'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 – 机器学习生命周期'
- en: '](img/Figure_2.1_B16890.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.1_B16890.jpg)'
- en: Figure 2.1 – The ML life cycle
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 机器学习生命周期
- en: 'The goal of automated ML is to simplify and democratize the steps of this pipeline
    so that it is accessible by citizen data scientists. Originally, the key focus
    of the automated ML community was model selection and hyperparameter tuning, that
    is, finding the best-performing model for the job and the corresponding parameters
    that work best for the problem. However, in recent years, it has been shifted
    to include the entire pipeline as shown in the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习的目标是简化并民主化这个流程的步骤，使其对公民数据科学家可访问。最初，自动化机器学习社区的关键焦点是模型选择和超参数调整，即找到最适合工作的最佳性能模型及其对应的最优参数。然而，近年来，它已经扩展到包括以下所示图中的整个流程：
- en: '![Figure 2.2 – A simplified AutoML pipeline by Waring et al.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – Waring等人提出的简化AutoML流程'
- en: '](img/Figure_2.2_B16890.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.2_B16890.jpg)'
- en: Figure 2.2 – A simplified AutoML pipeline by Waring et al.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – Waring等人提出的简化AutoML流程
- en: The notion of meta-learning, that is, learning to learn, is an overarching theme
    in the automated ML landscape. Meta-learning techniques are used to learn optimal
    hyperparameters and architectures by observing learning algorithms, similar tasks,
    and those from previous models. Techniques such as learning task similarity, active
    testing, surrogate model transfer, Bayesian optimization, and stacking are used
    to learn these meta-features to improve the automated ML pipeline based on similar
    tasks; essentially, a warm start. The automated ML pipeline function does not
    really end at deployment – an iterative feedback loop is required to monitor the
    predictions that arise for drift and consistency. This feedback loop ensures that
    the outcome distribution of prediction matches the business metrics, and that
    there are anomalies in terms of hardware resource consumption. From an operational
    point of view, the logs of errors and warnings, including custom error logs, are
    audited and monitored in an automated manner. All these best practices also apply
    to the training cycle, where the concept drift, model drift, or data drift can
    wreak havoc on your predictions; heed the caveat emptor warning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习概念，即学习如何学习，是自动机器学习领域的主题。元学习技术通过观察学习算法、相似任务和先前模型来学习最佳超参数和架构。学习任务相似性、主动测试、代理模型迁移、贝叶斯优化和堆叠等技术用于学习这些元特征，以基于相似任务改进自动机器学习流程；本质上，这是一个热启动。自动机器学习流程功能并不真正在部署时结束——需要一个迭代反馈循环来监控预测的漂移和一致性。这个反馈循环确保预测结果的分布与业务指标相匹配，并且在硬件资源消耗方面存在异常。从运营的角度来看，错误和警告的日志，包括自定义错误日志，以自动化的方式进行审计和监控。所有这些最佳实践也适用于训练周期，其中概念漂移、模型漂移或数据漂移可能会对你的预测造成破坏；请注意“买者自慎”的警告。
- en: Now, let's explore some of the key automated ML terms you will see in this and
    future chapters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索一些你将在本章和未来章节中看到的自动机器学习的关键术语。
- en: The taxonomy of automated ML terms
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动机器学习术语的分类
- en: For newcomers to automated ML, one of the biggest challenges is to become familiar
    with the industry jargon – large numbers of new or overlapping terminologies can
    overwhelm and discourage those exploring the automated ML landscape. Therefore,
    in this book we try to keep things simple and generalize as much as possible without
    losing any depth. You will repeatedly see in this book, as well as in other automated
    ML literature, the emphasis being placed on three key areas – namely, automated
    feature engineering, automated hyperparameter turning, and automated neural architecture
    search methods.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动机器学习的新手来说，最大的挑战之一是熟悉行业术语——大量新术语或重叠术语可能会让探索自动机器学习领域的人感到不知所措并失去动力。因此，在这本书中，我们尽量使内容简单化，并在尽可能不失去深度的情况下进行概括。你将在本书以及其他自动机器学习文献中反复看到，重点放在三个关键领域——即自动特征工程、自动超参数调整和自动神经网络架构搜索方法。
- en: 'Automated feature engineering is further classified into feature extraction,
    selection, and generation or construction. Automated hyperparameter tuning, or
    the learning of hyperparameters for a specific model, sometimes gets bundled with
    learning the model itself, and hence becomes part of a larger neural architecture
    search area. This approach is known as the **Full Model Selection** (**FMS**)
    or **Combined Algorithm Selection and Hyperparameter** (**CASH**) optimization
    problem. Neural architecture search is also known as **automated deep** **learning**
    (abbreviated as **AutoDL**), or simply architecture search. The following diagram
    outlines how **data preparation**, **feature engineering**, **model generation**,
    and **evaluation**, along with their subcategories, become part of the larger
    ML pipeline:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动特征工程进一步分为特征提取、选择和生成或构建。自动超参数调整，即学习特定模型的超参数，有时与学习模型本身捆绑在一起，因此成为更大神经网络架构搜索领域的一部分。这种方法被称为**全模型选择**（**FMS**）或**结合算法选择和超参数**（**CASH**）优化问题。神经网络架构搜索也称为**自动深度学习**（缩写为**AutoDL**），或简单地称为架构搜索。以下图表概述了**数据准备**、**特征工程**、**模型生成**和**评估**，以及它们的子类别，如何成为更大的机器学习流程的一部分：
- en: '![Figure 2.3 – Automated ML pipeline via state-of-the-art AutoML survey, He,
    et al., 2019'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3 – 通过最先进的 AutoML 调查的自动机器学习流程，He 等人，2019年]'
- en: '](img/Figure_2.3_B16890.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_2.3_B16890.jpg)'
- en: Figure 2.3 – Automated ML pipeline via state-of-the-art AutoML survey, He, et
    al., 2019
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 通过最先进的 AutoML 调查的自动机器学习流程，He 等人，2019年
- en: 'The techniques used to perform these three key tenets of automated ML have
    a few things in common. Bayesian optimization, reinforcement learning, evolutionary
    algorithms, gradient-free, and gradient-based approaches are used in almost all
    these different areas, with variations as shown in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用于执行自动化机器学习这三个关键原则的技术有一些共同之处。贝叶斯优化、强化学习、进化算法、无梯度方法和基于梯度的方法几乎在所有这些不同的领域中都被使用，具体变化如以下图表所示：
- en: '![Figure 2.4 – Automated ML techniques'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 自动化机器学习技术'
- en: '](img/Figure_2.4_B16890.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.4_B16890.jpg)'
- en: Figure 2.4 – Automated ML techniques
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 自动化机器学习技术
- en: So, you may get perplexed looks if you refer to using genetic programming in
    automated feature engineering, while someone considers evolutionary hierarchical
    ML systems as a hyperparameter optimization algorithm. That is because you can
    apply the same class of techniques, such as reinforcement learning, evolutionary
    algorithms, gradient descent, or random search, to different parts of automated
    ML pipelines, and that works just fine.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您提到在自动化特征工程中使用遗传编程，而有人认为进化分层机器学习系统是一个超参数优化算法，您可能会得到困惑的表情。这是因为您可以将同一类技术，如强化学习、进化算法、梯度下降或随机搜索，应用于自动化机器学习管道的不同部分，并且效果很好。
- en: We hope that the information provided between *Figure 2.2* and *Figure 2.4*
    help you to understand the relationship between ML pipelines, automated ML salient
    traits, and techniques/algorithms used to achieve those three key characteristic
    traits. The mental model you will build in this chapter will go a long way, especially
    when you encounter preposterous terms coined by marketing (yes Todd, I am talking
    about you!), such as deep-learning-based-hyperparameter-optimization-product-with-bitcoins-and-hyperledger.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望*图2.2*和*图2.4*之间提供的信息能帮助您理解机器学习管道、自动化机器学习的显著特性和用于实现这三个关键特征特性的技术/算法之间的关系。在本章中构建的思维方式将大有裨益，尤其是在您遇到营销人员创造的荒谬术语时（是的，我指的是你，Todd！），例如基于深度学习的超参数优化产品，使用比特币和Hyperledger。
- en: The next stop is automated feature engineering, the first pillar of the automated
    ML pipeline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个步骤是自动化特征工程，这是自动化机器学习管道的第一个支柱。
- en: Automated feature engineering
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化特征工程
- en: 'Feature engineering is the art and science of extracting and selecting the
    right attributes from the dataset. It is an art because it not only requires subject
    matter expertise, but also domain knowledge and an understanding of ethical and
    social concerns. From a scientific perspective, the importance of a feature is
    highly correlated with its resulting impact on the outcome. Feature importance
    in predictive modeling measures how much a feature influences the target, hence
    making it easier in retrospect to assign ranking to attributes with the most impact.
    The following diagram explains how the iterative process of automated feature
    generation works, by generating candidate features, ranking them, and then selecting
    the specific ones to become part of the final feature set:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是从数据集中提取和选择正确属性的艺术和科学。它是一门艺术，因为它不仅需要专业知识，还需要领域知识，以及对伦理和社会问题的理解。从科学的角度来看，特征的重要性与其对结果的影响高度相关。预测模型中的特征重要性衡量一个特征对目标的影响程度，因此在事后更容易对最具影响力的属性进行排序。以下图表解释了自动化特征生成的迭代过程，通过生成候选特征，对它们进行排序，然后选择特定的特征成为最终特征集的一部分：
- en: '![Figure 2.5 – Iterative feature generation process by Zoller et al. Benchmark
    and survey of automated ML frameworks, 2020'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 – Zoller等人关于自动化机器学习框架的基准和调查中的迭代特征生成过程，2020](img/Figure_2.5_B16890.jpg)'
- en: '](img/Figure_2.5_B16890.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Figure_2.4_B16890.jpg)'
- en: Figure 2.5 – Iterative feature generation process by Zoller et al. Benchmark
    and survey of automated ML frameworks, 2020
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – Zoller等人关于自动化机器学习框架的基准和调查中的迭代特征生成过程，2020
- en: Extracting a feature from the dataset requires the generation of categorical
    binary features based on columns with multiple possible values, scaling the features,
    eliminating highly correlated features, adding feature interactions, substituting
    cyclic features, and handling data/time scenarios. Date fields, for instance,
    result in several features, such as year, month, day, season, weekend/weekday,
    holiday, and enrollment period. Once extracted, selecting a feature from a dataset
    requires the removal of sparse and low variance features, as well as applying
    dimensionality reduction techniques such as **Principal Component Analysis** (**PCA**)
    to make the number of features manageable. We will now investigate hyperparameter
    optimization, which used to be a synonym for automated ML, and is still a fundamental
    entity in the space.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据集中提取特征需要基于具有多个可能值的列生成分类二元特征，对特征进行缩放，消除高度相关的特征，添加特征交互，替换周期性特征，以及处理数据/时间场景。例如，日期字段会产生多个特征，如年份、月份、日期、季节、周末/工作日、假日和入学期间。一旦提取，从数据集中选择特征需要移除稀疏和低方差特征，以及应用降维技术，如**主成分分析**（PCA），以使特征数量可管理。我们现在将研究超参数优化，这曾经是自动化机器学习的同义词，并且仍然是该领域的一个基本实体。
- en: Hyperparameter optimization
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Due to its ubiquity and ease of framing, hyperparameter optimization is sometimes
    regarded as being synonymous with automated ML. Depending on the search space,
    if you include features, hyperparameter optimization, also dubbed hyperparameter
    tuning and hyperparameter learning, is known as automated pipeline learning. All
    these terms can be bit daunting for something as simple as finding the right parameters
    for a model, but graduating students must publish, and I digress.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其普遍性和易于构建，超参数优化有时被认为与自动化机器学习同义。根据搜索空间，如果你包括特征、超参数优化（也称为超参数调整和超参数学习），则被称为自动化管道学习。所有这些术语对于寻找模型正确参数这样简单的事情来说可能有点令人畏惧，但毕业生必须发表论文，我就不赘述了。
- en: 'There are a couple of key points regarding hyperparameters that are important
    to note as we look further into these constructs. It is well established that
    the default parameters are not optimized. Olson et al., in their NIH paper, demonstrated
    how the default parameters are almost always a bad idea. Olson mentions that "*Tuning
    often improves an algorithm''s accuracy by 3–5%, depending on the algorithm….
    In some cases, parameter tuning led to CV accuracy improvements of 50%*." This
    was observed in *Cross-validation accuracy improvement – Data-driven advice for
    applying ML to bioinformatics problems*, by Olson et al.: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究这些结构时，有几个关于超参数的关键点需要注意。默认参数没有经过优化是众所周知的事实。Olson等人在其NIH论文中展示了默认参数几乎总是糟糕的选择。Olson提到，“*调整参数通常可以提高算法的准确率3-5%，具体取决于算法……在某些情况下，参数调整导致了交叉验证准确率的50%提升*。”这一观察结果可以在Olson等人的文章《交叉验证准确率提升
    – 将机器学习应用于生物信息学问题的数据驱动建议》中找到：[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/).
- en: The second important point is that a comparative analysis of these models leads
    to greater accuracy; as you will see in forthcoming chapters, the entire pipeline
    (model, automated features, hyperparameters) are all key to getting the best accuracy
    trade-off. The *Heatmap for comparative analysis of algorithms* section in *Data-driven
    advice for applying ML to bioinformatics problems*, by Olson et al. ([https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/))
    shows the experiment performed by Olson et al., where 165 datasets were used against
    multiple different algorithms to determine the best accuracy, ranked from top
    to bottom based on performance. The takeaway from this experiment is that no single
    algorithm can be considered best-performing across all the datasets. Therefore,
    there is a definite need to consider different ML algorithms when solving these
    data science problems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要点是，对这些模型的比较分析会导致更高的准确性；正如你将在接下来的章节中看到的，整个流程（模型、自动特征、超参数）都是获得最佳准确度权衡的关键。在Olson等人撰写的《将机器学习应用于生物信息学问题的数据驱动建议》一书中，“算法比较分析的热图”部分（[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/））展示了Olson等人进行的实验，其中使用了165个数据集与多个不同的算法进行比较，以确定最佳准确性，并按性能从上到下排名。从这个实验中得出的结论是，没有单个算法可以在所有数据集上被认为是表现最佳的。因此，在解决这些数据科学问题时，确实有必要考虑不同的机器学习算法。
- en: Let's do a quick recap of what the hyperparameters are. Each model has its internal
    and external parameters. Internal parameters or model parameters are intrinsic
    to the model, such as weight or the predictor matrix, while external parameters
    also known as hyperparameters, are "outside" the model; for example learning rate
    and the number of iterations. For instance in k-means, k stands for the number
    of clusters required and epochs are used to specify the number of passes done
    over the training data. Both of these are examples of hyperparameters, that is,
    parameters that are not intrinsic to the model itself. Similarly, the learning
    rate for training a neural network, C and sigma for **Support Vector Machines**
    (**SVMs**), *k* number of leaves or depth of a tree, latent factors in a matrix
    factorization, the number of hidden layers in a deep neural network, and so on
    are all examples of hyperparameters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下超参数是什么。每个模型都有其内部和外部参数。内部参数或模型参数是模型本身的固有属性，例如权重或预测矩阵，而外部参数也称为超参数，是“模型外部”的；例如学习率和迭代次数。例如，在k-means中，k代表所需的聚类数量，epochs用于指定在训练数据上进行的遍历次数。这两个都是超参数的例子，即不是模型本身固有属性的参数。同样，训练神经网络的学习率，**支持向量机**（**SVMs**）中的C和sigma，树的数量或深度，矩阵分解中的潜在因素，深度神经网络中的隐藏层数量等等，都是超参数的例子。
- en: 'To find the correct hyperparameters, there are a number of approaches, but
    first let''s see what different types of hyperparameters there are. Hyperparameters
    can be continuous, for example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到正确的超参数，有几种方法，但首先让我们看看有哪些不同类型的超参数。超参数可以是连续的，例如：
- en: The learning rate of a model
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的学习率
- en: The number of hidden layers
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The number of iterations
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数
- en: Batch size
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小
- en: 'Hyperparameters can also be categorical, for example, the type of operator,
    activation function, or the choice of algorithm. They can also be conditional,
    for example, selecting the convolutional kernel size if a convolutional layer
    is used, or the kernel width if a **R****adial Basis Function** (**RBF**) kernel
    is selected in an SVM. Since there are multiple types of hyperparameters, there
    are also a variety of hyperparameter optimization techniques. Grid, random search,
    Bayesian optimization, evolutionary techniques, multi-arm bandit approaches, and
    gradient descent-based techniques are all used for hyperparameter optimization:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数也可以是分类的，例如操作符的类型、激活函数或算法的选择。它们也可以是条件性的，例如，如果使用了卷积层，则选择卷积核大小；或者在SVM中选择了**径向基函数**（**RBF**）核时，选择核宽度。由于存在多种类型的超参数，因此也有各种超参数优化技术。网格搜索、随机搜索、贝叶斯优化、进化技术、多臂老虎机方法和基于梯度的技术都被用于超参数优化：
- en: '![Figure 2.6 – Grid and random search layout. Bergstra and Bengio – JMLR 2012'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6 – 网格和随机搜索布局。伯格斯特拉和本吉奥 – JMLR 2012'
- en: '](img/Figure_2.6_B16890.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_2.6_B16890.jpg)'
- en: Figure 2.6 – Grid and random search layout. Bergstra and Bengio – JMLR 2012
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 网格和随机搜索布局。伯格斯特拉和本吉奥 – JMLR 2012
- en: The simplest techniques for hyperparameter tuning are manual, grid, and random
    search. Manual turning, as the name suggests, is based on intuition and guessing
    based on past experiences. Grid search and random search are slightly different,
    as you pick a set of hyperparameters either for each combination (grid), or randomly
    and iterate through to keep the best performing ones. However, as you can imagine,
    this can get computationally out of hand quickly as the search space gets bigger.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整的最简单技术是手动调整、网格搜索和随机搜索。正如其名所示，手动调整基于直觉和基于以往经验的猜测。网格搜索和随机搜索略有不同，因为你可以选择一组超参数，要么为每个组合（网格），要么随机选择并迭代以保留表现最好的那些。然而，正如你可以想象的那样，随着搜索空间的扩大，这可能会迅速变得计算上难以控制。
- en: The other prominent technique is Bayesian optimization, in which you start with
    a random combination of hyperparameters and use it to construct a surrogate model.
    Then you use this surrogate model to predict how other combinations of hyperparameters
    would work. As a general principle, Bayesian optimization builds a probability
    model to minimize the objective's function, using past performance to select the
    future values, and that is exactly what's Bayesian about it. As known in the Bayesian
    universe, your observations are less important than your prior belief.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种突出的技术是贝叶斯优化，其中你从一个随机的超参数组合开始，并使用它来构建一个代理模型。然后你使用这个代理模型来预测其他超参数组合将如何工作。作为一个基本原则，贝叶斯优化通过使用过去的表现来选择未来的值，构建一个概率模型以最小化目标函数，这正是贝叶斯优化之所以称为贝叶斯的原因。在贝叶斯宇宙中，你的观察不如你的先验信念重要。
- en: 'The greedy nature of Bayesian optimization is controlled by exploration and
    exploitation trade-off (expected improvement), allocating fixed-time evaluations,
    setting thresholds, and suchlike. There are variations of these surrogate models
    that exist, such as random forest surrogate and gradient boosting surrogate, which
    use the aforementioned techniques to minimize the surrogate''s function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化的贪婪性质由探索与利用的权衡（预期改进）、分配固定时间评估、设置阈值等来控制。存在这些代理模型的变体，例如随机森林代理和梯度提升代理，它们使用上述技术来最小化代理函数：
- en: '![Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi
    et al., 2019'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.7 – 超参数优化技术分类，Elshawi等人，2019](img/Figure_2.7_B16890.jpg)'
- en: '](img/Figure_2.7_B16890.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi
    et al., 2019](img/Figure_2.7_B16890.jpg)'
- en: Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi et
    al., 2019
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi et
    al., 2019
- en: The class of population-based methods (also called meta-heuristic techniques
    or optimization from samples methods) is also widely used to perform hyperparameter
    tuning, with genetic programming (evolutionary algorithms) being the most popular,
    where hyperparameters are added, mutated, selected, crossed over, and tuned. A
    particle swarm moves toward the best individual configurations when the configuration
    space is updated at each iteration. On the other hand, evolutionary algorithms
    work by maintaining a configuration space, and improve it by making smaller changes
    and combining individual solutions to build a new generation of hyperparameter
    configuration. Let's now explore the final piece of the automated ML puzzle –
    neural architecture search.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于种群的方法类（也称为元启发式技术或从样本中进行优化的方法）也被广泛用于执行超参数调整，其中遗传编程（进化算法）是最受欢迎的，超参数在此过程中被添加、变异、选择、交叉和调整。当配置空间在每次迭代中更新时，粒子群会朝着最佳个体配置移动。另一方面，进化算法通过维护一个配置空间，并通过进行较小的更改和组合个体解决方案来构建新一代的超参数配置来工作。现在让我们探索自动机器学习难题的最后一部分——神经架构搜索。
- en: Neural architecture search
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经架构搜索
- en: Selecting models can be challenging. In the case of regression, that is, predicting
    a numerical value, you have a choice of linear regression, decision trees, random
    forest, lasso versus ridge regression, k-means elastic net, gradient boosting
    methods, including **XGBoost**, and SVMs, among many others.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型可能具有挑战性。在回归的情况下，即预测数值，你有线性回归、决策树、随机森林、lasso与岭回归、k-means弹性网络、梯度提升方法（包括**XGBoost**）和SVMs等多种选择，等等。
- en: For classification, that in other words, separating out things by classes, you
    have **logistic regression**, **random forest**, **AdaBoost**, **gradient boost**,
    and **SVM-based classifiers** at your disposal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，换句话说，通过类别分离事物，你可以使用**逻辑回归**、**随机森林**、**AdaBoost**、**梯度提升**和基于**SVM**的分类器。
- en: Neural architecture has the notion of search space, which defines which architectures
    can be used in principle. Then, a search strategy must be defined that outlines
    how to explore using the exploration-exploitation trade-off. Finally, there has
    to be a performance estimation strategy, which estimates the candidate's performance.
    This includes training and validation of the architecture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构具有搜索空间的概念，这定义了原则上可以使用哪些架构。然后，必须定义一个搜索策略，概述如何通过探索-利用权衡来探索。最后，必须有一个性能估计策略，用于估计候选架构的性能。这包括架构的训练和验证。
- en: 'There are several techniques for performing the exploration of search space.
    The most common ones include chain structured neural networks, multi-branch networks,
    cell-based search, and optimizing approaches using existing architecture. Search
    strategies include random search, evolutionary approaches, Bayesian optimization,
    reinforcement learning, and gradient-free versus gradient-based optimization approaches,
    such as **Differentiable Architecture Search** (**DARTS**). The search strategy
    to hierarchically explore the architectural search spaces, using Monte Carlo tree
    search or hill climbing, is popular as it helps discover high-quality architectures
    by rapidly approaching better performing architectures. These are the gradient
    "free" methods. In gradient-based methods, the underlying assumption of a continuous
    search space facilitates DARTS, which, unlike traditional reinforcement learning
    or evolutionary search approaches, explores the search space using gradient descent.
    A visual taxonomy of neural architectural search can be seen in the following
    diagram:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以执行搜索空间的探索。最常见的方法包括链式结构神经网络、多分支网络、基于单元格的搜索以及使用现有架构的优化方法。搜索策略包括随机搜索、进化方法、贝叶斯优化、强化学习和无梯度与基于梯度的优化方法，如**可微分架构搜索**（**DARTS**）。使用蒙特卡洛树搜索或爬山来分层探索架构搜索空间，这种方法很受欢迎，因为它有助于通过快速接近性能更好的架构来发现高质量的架构。这些都是“免费”梯度方法。在基于梯度的方法中，连续搜索空间的基本假设促进了DARTS，与传统的强化学习或进化搜索方法不同，DARTS使用梯度下降来探索搜索空间。神经架构搜索的可视分类学可以在以下图表中看到：
- en: '![Figure 2.8 – A taxonomy of neural architecture search techniques, Elshawi
    et al., 2019'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 神经架构搜索技术分类学，Elshawi等人，2019
- en: '](img/Figure_2.8_B16890.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.8 – A taxonomy of neural architecture search techniques, Elshawi
    et al., 2019]'
- en: Figure 2.8 – A taxonomy of neural architecture search techniques, Elshawi et
    al., 2019
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 神经架构搜索技术分类学，Elshawi等人，2019
- en: To evaluate which approach works best for the specific dataset, the performance
    estimation strategies have a spectrum of simple to more complex (albeit optimized)
    approaches. Simplest among the estimation strategies is to just train the candidate
    architecture and evaluate its performance on test data – if it works out, great.
    Otherwise, toss it out and try a different architectural combination. This approach
    can quickly become prohibitively resource-intensive as the number of candidate
    architectures grows; hence, the low-fidelity strategies, such as shorter training
    times, subset training, and fewer filters per layer are introduced, which are
    not nearly as exhaustive. Early stopping, in other words, estimating an architecture's
    performance by extrapolating its learning curve, is also a helpful optimization
    for such an approximation. Morphing a trained neural architecture, and one-short
    searches treating all architectures as a subgraph of a super graph, are also effective
    approaches as regards one-shot architecture search.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估哪种方法最适合特定的数据集，性能估计策略包括从简单到更复杂（尽管是优化过的）的方法。在估计策略中最简单的是仅训练候选架构，并在测试数据上评估其性能——如果效果良好，那就太好了。否则，就将其丢弃并尝试不同的架构组合。随着候选架构数量的增加，这种方法可能会迅速变得资源密集，因此引入了低保真策略，如缩短训练时间、子集训练和每层更少的过滤器，这些方法并不那么全面。换句话说，早期停止，即通过外推学习曲线来估计架构的性能，也是这种近似的有用优化。将训练好的神经架构进行形态变化，以及将所有架构视为超图子图的单一搜索，也是有效的单次架构搜索方法。
- en: Several surveys have been conducted in relation to automated ML that provide
    an in-depth overview of these techniques. The specific techniques also have their
    own publications, with well-articulated benchmark data, challenges, and triumphs
    – all of which is beyond the scope of this manuscript. In the next chapter however,
    we will use the libraries that utilize these techniques, so you will get better
    hands-on exposure vis-à-vis their usability.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 已有几项关于自动化ML的调查，这些调查对这些技术提供了深入的概述。具体的技术也有它们自己的出版物，包括阐述良好的基准数据、挑战和成功——所有这些都超出了本手稿的范围。然而，在下一章中，我们将使用利用这些技术的库，这样你将更好地了解它们在实际中的可用性。
- en: Summary
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Today, the success of ML within an enterprise largely depends on human ML experts
    who can construct business-specific features and workflows. Automated ML aims
    to change this, as it aims to automate ML so as to provide off-the-shelf ML methods
    that can be utilized without expert knowledge. To understand how automated ML
    works, we need to review the underlying four subfields, or pillars, of automated
    ML: hyperparameter optimization; automated feature engineering; neural architecture
    search; and meta-learning.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，企业内部机器学习的成功在很大程度上取决于能够构建特定业务特征和工作流程的人类ML专家。自动化ML旨在改变这一点，因为它旨在自动化ML，以便提供现成的ML方法，这些方法可以在没有专业知识的情况下使用。为了理解自动化ML是如何工作的，我们需要回顾自动化ML的四个基础子领域或支柱：超参数优化；自动化特征工程；神经网络架构搜索；和元学习。
- en: In this chapter, we explained what is under the hood in terms of the technologies,
    techniques, and tools used to make automated ML possible. We hope that this chapter
    has introduced you to automated ML techniques and that you are now ready to do
    a deeper dive into the implementation phase.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了自动化机器学习（ML）背后的技术、技术和工具，以及它们是如何使自动化ML成为可能的。我们希望这一章能让你对自动化ML技术有所了解，并且你现在准备好深入到实施阶段。
- en: In the next chapter, we will review the open source tools and libraries that
    implement these algorithms to get a hands-on overview of how to use these concepts
    in practice, so stay tuned.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾实现这些算法的开源工具和库，以便获得如何在实际中应用这些概念的手动概述，所以请保持关注。
- en: Further reading
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on the following topics, refer to the suggested resources
    and links:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 关于以下主题的更多信息，请参阅建议的资源链接：
- en: '*Automated ML: Methods, Systems, Challenges*: Frank Hutter (Editor), Lars Kotthoff
    (Editor), and Joaquin Vanschoren (Editor). The Springer Series on Challenges in
    ML'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动化ML：方法、系统、挑战*：Frank Hutter（编辑），Lars Kotthoff（编辑），和 Joaquin Vanschoren（编辑）。Springer机器学习挑战系列'
- en: '*Hands-On Automated ML*: A Beginner''s Guide to Building Automated ML Systems
    Using AutoML and Python, by Sibanjan Das and Umit Mert Cakmak, Packt'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动手实践自动化ML*：使用AutoML和Python构建自动化ML系统的入门指南，作者：Sibanjan Das 和 Umit Mert Cakmak，Packt'
- en: '*Neural Architecture Search with Reinforcement Learning*: [https://arxiv.org/pdf/1611.01578.pdf](https://arxiv.org/pdf/1611.01578.pdf)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用强化学习进行神经网络架构搜索*：[https://arxiv.org/pdf/1611.01578.pdf](https://arxiv.org/pdf/1611.01578.pdf)'
- en: '*Learning Transferable Architectures for Scalable Image Recognition*: [https://arxiv.org/pdf/1707.07012.pdf](https://arxiv.org/pdf/1707.07012.pdf)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习可迁移的架构以实现可扩展的图像识别*：[https://arxiv.org/pdf/1707.07012.pdf](https://arxiv.org/pdf/1707.07012.pdf)'
- en: '*Progressive Neural Architecture Search*: [https://arxiv.org/pdf/1712.00559.pdf](https://arxiv.org/pdf/1712.00559.pdf)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*渐进式神经网络架构搜索*：[https://arxiv.org/pdf/1712.00559.pdf](https://arxiv.org/pdf/1712.00559.pdf)'
- en: '*Efficient Neural Architecture Search via Parameter Sharing*: [https://arxiv.org/pdf/1802.03268.pdf](https://arxiv.org/pdf/1802.03268.pdf)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过参数共享进行高效的神经网络架构搜索*：[https://arxiv.org/pdf/1802.03268.pdf](https://arxiv.org/pdf/1802.03268.pdf)'
- en: '*Efficient Architecture Search by Network Transformation*: [https://arxiv.org/pdf/1707.04873.pdf](https://arxiv.org/pdf/1707.04873.pdf)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过网络转换进行高效的架构搜索*：[https://arxiv.org/pdf/1707.04873.pdf](https://arxiv.org/pdf/1707.04873.pdf)'
- en: '*Network Morphism*: [https://arxiv.org/pdf/1603.01670.pdf](https://arxiv.org/pdf/1603.01670.pdf)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网络形态学*：[https://arxiv.org/pdf/1603.01670.pdf](https://arxiv.org/pdf/1603.01670.pdf)'
- en: '*Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution*:
    [https://arxiv.org/pdf/1804.09081.pdf](https://arxiv.org/pdf/1804.09081.pdf)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过拉马克进化进行高效的多目标神经网络架构搜索*：[https://arxiv.org/pdf/1804.09081.pdf](https://arxiv.org/pdf/1804.09081.pdf)'
- en: '*Auto-Keras*: *An Efficient Neural Architecture Search System*: [https://arxiv.org/pdf/1806.10282.pdf](https://arxiv.org/pdf/1806.10282.pdf)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Auto-Keras*：*一个高效的神经网络架构搜索系统*：[https://arxiv.org/pdf/1806.10282.pdf](https://arxiv.org/pdf/1806.10282.pdf)'
- en: '*Convolutional Neural Fabrics*: [https://arxiv.org/pdf/1606.02492.pdf](https://arxiv.org/pdf/1606.02492.pdf)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积神经网络织物*：[https://arxiv.org/pdf/1606.02492.pdf](https://arxiv.org/pdf/1606.02492.pdf)'
- en: '*DARTS*: *Differentiable Architecture Search*: [https://arxiv.org/pdf/1806.09055.pdf](https://arxiv.org/pdf/1806.09055.pdf)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DARTS*：可微架构搜索：[https://arxiv.org/pdf/1806.09055.pdf](https://arxiv.org/pdf/1806.09055.pdf)'
- en: '*Neural Architecture Optimization*: [https://arxiv.org/pdf/1808.07233.pdf](https://arxiv.org/pdf/1808.07233.pdf)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经架构优化*：[https://arxiv.org/pdf/1808.07233.pdf](https://arxiv.org/pdf/1808.07233.pdf)'
- en: '*SMASH*: One-Shot Model Architecture Search through HyperNetworks: [https://arxiv.org/pdf/1708.05344.pdf](https://arxiv.org/pdf/1708.05344.pdf)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SMASH*：通过超网络进行一次性模型架构搜索：[https://arxiv.org/pdf/1708.05344.pdf](https://arxiv.org/pdf/1708.05344.pdf)'
- en: '*DARTS in PyTorch*: [https://github.com/quark0/darts](https://github.com/quark0/darts)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DARTS in PyTorch*：[https://github.com/quark0/darts](https://github.com/quark0/darts)'
- en: 'Hyperparameter Tuning Using Simulated Annealing: [https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html](https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模拟退火进行超参数调整：[https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html](https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html)
- en: 'Bayesian Optimization: [http://krasserm.github.io/2018/03/21/bayesian-optimization/](http://krasserm.github.io/2018/03/21/bayesian-optimization/)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化：[http://krasserm.github.io/2018/03/21/bayesian-optimization/](http://krasserm.github.io/2018/03/21/bayesian-optimization/)
- en: 'Neural Architecture Search: A Survey: [https://www.jmlr.org/papers/volume20/18-598/18-598.pdf](https://www.jmlr.org/papers/volume20/18-598/18-598.pdf)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经架构搜索：综述：[https://www.jmlr.org/papers/volume20/18-598/18-598.pdf](https://www.jmlr.org/papers/volume20/18-598/18-598.pdf)
- en: 'Data-driven advice for applying ML to bioinformatics problems: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将机器学习应用于生物信息学问题的数据驱动建议：[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/)
