["```py\nimport lightgbm as lgbm\n# Construct a LGBM dataset\nlgbm.Dataset(..)\n# Train a LGBM predictor\nclf = lgbm.train(..)\n```", "```py\n    from azureml.core import Workspace, Experiment\n    ws = Workspace.from_config()\n    exp = Experiment(workspace=ws, name=\"titanic-lgbm\")\n    ```", "```py\n    import pandas as pd\n    # Read the data\n    df = pd.read_csv('data/titanic.csv')\n    # Prepare the data\n    df.drop(['PassengerId'], axis=1, inplace=True)\n    df.loc[df['Sex'] == 'female', 'Sex'] = 0\n    df.loc[df['Sex'] == 'male', 'Sex'] = 1\n    df['Sex'] = df['Sex'].astype('int8')\n    embarked_encoder = LabelEncoder()\n    embarked_encoder.fit(df['Embarked'].fillna('Null'))\n    df['Embarked'].fillna('Null', inplace=True)\n    df['Embarked'] = embarked_encoder.transform(\n        df['Embarked'])\n    df.drop(['Name', 'Ticket', 'Cabin'],\n        axis=1,\n        inplace=True)\n    ```", "```py\n    def df_to_dataset(ws, df, name):\n        datastore = ws.get_default_datastore()\n        dataset = Dataset.Tabular.register_pandas_dataframe(\n            df, datastore, name)\n        return dataset\n    ```", "```py\n    # Register the data\n    df_to_dataset(ws, df, 'titanic_cleaned')\n    ```", "```py\n    def get_aml_cluster(ws, cluster_name,\n                        vm_size='STANDARD_D2_V2',\n                        max_nodes=4):\n        try:\n            cluster = ComputeTarget(\n                 workspace=ws, name=cluster_name)\n        except ComputeTargetException:\n            config = AmlCompute.provisioning_configuration(\n                vm_size=vm_size, max_nodes=max_nodes)\n            cluster = ComputeTarget.create(\n                ws, cluster_name, config)\n        return cluster \n    ```", "```py\n    # Create or get training cluster\n    aml_cluster = get_aml_cluster(ws, \n                                  cluster_name=\"cpu-cluster\")\n    aml_cluster.wait_for_completion(show_output=True)\n    ```", "```py\n    def get_run_config(target, packages=None):\n        packages = packages or []\n        packages += ['azureml-defaults']\n        config = RunConfiguration()\n        config.target = target\n        config.environment.python.conda_dependencies = \\\n            CondaDependencies.create(pip_packages=packages)\n        return config\n    ```", "```py\n    # Create a remote run configuration\n    lgbm_config = get_run_config(aml_cluster, [\n        'numpy', 'pandas', 'matplotlib', 'seaborn',\n        'scikit-learn', 'joblib', 'lightgbm'\n    ])\n    ```", "```py\n    from azureml.core import Dataset, Run\n    run = Run.get_context()\n    ws = run.experiment.workspace\n    ```", "```py\n    parser.add_argument('--data', type=str)\n    parser.add_argument('--boosting', type=str)\n    parser.add_argument('--learning-rate', type=float)\n    parser.add_argument('--drop-rate', type=float)\n    args = parser.parse_args()\n    ```", "```py\n    # Get a dataset by id\n    dataset = Dataset.get_by_id(ws, id=args.data)\n    # Load a TabularDataset into pandas DataFrame\n    df = dataset.to_pandas_dataframe()\n    ```", "```py\n    y = df.pop('Survived')\n    # Split into training and testing set \n    X_train, X_test, y_train, y_test = train_test_split(\n        df, y, test_size=0.2, random_state=42) \n    ```", "```py\n    categories = ['Alone', 'Sex', 'Pclass', 'Embarked']\n    ```", "```py\n    # Create training set\n    train_data = lgbm.Dataset(data=X_train, label=y_train, \n        categorical_feature=categories, free_raw_data=False)\n    # Create testing set\n    test_data = lgbm.Dataset(data=X_test, label=y_test,\n        categorical_feature=categories, free_raw_data=False)\n    ```", "```py\n    lgbm_params = {\n        'application': 'binary',\n        'metric': 'binary_logloss',\n        'learning_rate': args.learning_rate,\n        'boosting': args.boosting,\n        'drop_rate': args.drop_rate,\n    }\n    ```", "```py\n    for k, v in params.items():\n        run.log(k, v)\n    ```", "```py\n    def azure_ml_callback(run):\n        def callback(env):\n            if env.evaluation_result_list:\n                for data_name, eval_name, result, _ in \\\n                    env.evaluation_result_list:\n                    run.log(\"%s (%s)\" % (eval_name, \n                                         data_name), result)\n        callback.order = 10\n        return callback \n    ```", "```py\n    clf = lgbm.train(train_set=train_data,\n                     params=lgbm_params,\n                     valid_sets=[train_data, test_data], \n                     valid_names=['train', 'val'],\n                     num_boost_round=args.num_boost_round,\n                     callbacks = [azure_ml_callback(run)])\n    ```", "```py\n    y_pred = clf.predict(X_test)\n    run.log(\"accuracy (test)\", accuracy_score(y_test, \n                                              y_pred))\n    run.log(\"precision (test)\", precision_score(y_test, \n                                                y_pred))\n    run.log(\"recall (test)\", recall_score(y_test, y_pred))\n    run.log(\"f1 (test)\", f1_score(y_test, y_pred))\n    ```", "```py\n    fig = plt.figure()\n    ax = plt.subplot(111)\n    lgbm.plot_importance(clf, ax=ax)\n    run.log_image(\"feature importance\", plot=fig)\n    ```", "```py\n    import joblib\n    joblib.dump(clf, 'outputs/lgbm.pkl')\n    run.upload_file('lgbm.pkl', 'outputs/lgbm.pkl')\n    run.register_model(model_name='lgbm_titanic', \n        model_path='lgbm.pkl')\n    ```", "```py\n    script_params = [\n      '--data', ds.as_named_input('titanic'),\n      '--boosting', 'dart',\n      '--learning-rate', '0.01',\n      '--drop-rate', '0.15',\n    ]\n    ```", "```py\n    from azureml.core import ScriptRunConfig\n    src = ScriptRunConfig(\n        source_directory=os.getcwd(),\n        script='train_lightgbm.py',\n        run_config= lgbm_config\n        arguments=script_params)\n    ```", "```py\n    from azureml.widgets import RunDetails\n    run = exp.submit(src)\n    RunDetails(run).show()\n    ```"]