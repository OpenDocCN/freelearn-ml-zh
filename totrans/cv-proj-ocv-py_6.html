<html><head></head><body><div><h1 class="header-title">Facial Feature Tracking and Classification with dlib</h1>
                
            
            
                
<p>In this chapter, we'll learn about dlib and how to locate faces from images and videos with the help of some examples. We will also learn about facial recognition using dlib.</p>
<p>We are going to cover the following topics:</p>
<ul>
<li>Introducing dlib</li>
<li>Facial landmarks</li>
<li>Finding 68 facial landmarks in images</li>
<li>Faces in videos</li>
<li>Facial recognition</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Introducing dlib</h1>
                
            
            
                
<p>dlib is a general-purpose, cross-platform software library written in the programming language C++. We are going to learn dlib and understand how to find and use human facial features from images and videos. According to their own website, <a href="http://dlib.net/" target="_blank">dlib.net</a>, dlib is a modern C++ tool containing machine learning algorithms and tools for creating complex software in C++ to solve real-world problems. It is a C++ toolkit and, just like OpenCV, it contains a very nice set of Python bindings that will work very well for our applications.</p>
<p>dlib is a very rich library and contains a whole lot of algorithms and features, which are very well documented on their website. This makes it easy to learn from, and it has a whole lot of examples similar to what we're going to do in this chapter and for your customized projects. It is recommended that you check their website if you're interested in dlib and want to learn how to use it for your applications. The <em>High Quality Portable Code</em> section on the <a href="http://dlib.net/" target="_blank">http://dlib.net/</a> website has efficient code for Microsoft Windows, Linux, and macOS, and just like Python, contains a very rich set of machine learning algorithms, including state-of-the-art deep learning, which we are using in this chapter, although we're going to use TensorFlow for our purposes. It also has <strong>Support Vector Machines</strong> (<strong>SVMs</strong>), which we saw in <a href="" target="_blank">Chapter 5</a>, <em>Handwritten Digit Recognition with scikit-learn and TensorFlow</em> on handwritten digit recognition, and a wide variety of other things for object detection and clustering, K-means, and so forth. It also has a rich set of numerical algorithms, linear algebra, <strong>singular value decomposition</strong> (<strong>SVD</strong>), and a whole lot optimization algorithms, as well as graphical model inference algorithms, and image processing (which is very useful for us). It has routines for reading and writing common image formats (although we won't use them as we're going to use the tools that we've already seen for reading and writing images) and <strong>Speeded-Up Robust Features</strong> (<strong>SURF</strong>), <strong>Histogram of Oriented Gradient</strong> (<strong>HOG</strong>), and FHOG, which are useful for image detection and recognition. What's interesting for now are the tools for detecting objects, including frontal face detection, pose estimation, and facial feature recognition. So, we'll talk about that in this chapter. There are some other features of dlib, such as threading, networking, <strong>Graphical User Interface</strong> (<strong>GUI</strong>) development, data compression, and a bunch of other utilities. <a href="http://dlib.net/" target="_blank">http://dlib.net/</a> includes examples in C++ and examples in Python. What we're going to be interested in is face detection, facial landmark detection, and recognition. So, we're going to go through similar examples to see what we have here.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Facial landmarks</h1>
                
            
            
                
<p>We're going to learn all about facial landmarks in dlib. Before we can run any code, we need to grab some data that's used for facial features themselves. We'll see what these facial features are and exactly what details we're looking for. This is not included with Python dlib distributions, so you will have to download this. We'll go to the <a href="http://dlib.net/files" target="_blank">dlib.net/files/</a> site, where you can see all the source code files; scroll to the bottom and you can see the <kbd>shape_predictor_68_face_landmarks.dat.bz2</kbd> file. Click on it and then save it wherever you keep your Jupyter Notebooks for this book.</p>
<p>Okay, so, what exactly is that? What are these 68 landmarks? Well, these landmarks are a common feature set that was generated by training alpha datasets from something called iBUG (<a href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/" target="_blank">https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/</a>), the intelligent behavior understanding group. So, this is a pre-trained model, a database of a whole bunch of human faces of people from all over the world, male/female, different age groups, and so forth.</p>
<p>So, we'll work on a variety of cases, and what we're looking for is a bunch of points around the outline of the face, as you can see in the following diagram:</p>
<div><img class="alignnone size-full wp-image-502 image-border" src="img/03f332b3-640a-46e9-a413-1d8ef79dca42.png" style="width:37.67em;height:34.33em;" width="541" height="493"/></div>
<p>Points <strong>1</strong> through <strong>17</strong> are the outline of the face, points <strong>18</strong> through <strong>22</strong> are the right eyebrow, <strong>23</strong> to <strong>27</strong> the left eyebrow, <strong>28</strong> to <strong>31</strong> the ridge of the nose, <strong>30</strong> to <strong>36</strong> the base of the nose, <strong>37</strong> to <strong>42</strong> forms the right eye, and <strong>43</strong> to <strong>48</strong> outlines the left eye, and then there are a whole bunch of points for the mouth, including both sides of the upper lip and both sides of the lower lip.</p>
<p>So, these are common features that all human faces will have, and this will allow us to do a whole lot of things like facial recognition and identification, pose estimation, possibly age estimation, gender estimation, and even neat things like facial stitching and facial blending. A lot of very interesting things can be done just with this information, and these are just based on pure intensity values on the face. So, there are no SURF features, <strong>Scale Invariant Feature Transform</strong> (<strong>SIFT</strong>) features, HOG features, or anything like that. These are just detectable from pixel values. So, effectively you can convert from RGB to black and white to monochrome, and you can run this model if it's an ensemble of regression trees.</p>
<p>You can download the iBUG dataset and train your own model, and you can actually vary the number of features. There are datasets with more features than this, but this is more than adequate for our purposes. You can train it if you want to run it on a variety of faces or particular faces, but you'll find that this pre-trained dataset is going to work in a wide variety of cases. So, iBUG is powerful in and of itself. We're going to use it here, and we're going to see how to run the code that will find all these features for some images and for some videos. Then, we're going to apply that to the facial recognition problem, where we differentiate between faces in a given set. After you have downloaded the <kbd>shape_predictor_68_face_landmarks.dat.bz2</kbd> file, you can put the file in your directory where you have your Jupyter Notebook and with that, we can get started with our code.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Finding 68 facial landmarks in images</h1>
                
            
            
                
<p>In this section, we're going to see our first example, where we find 68 facial landmarks and images with single people and with multiple people. So, let's open our Jupyter Notebook for this section. Take a look at this first cell:</p>
<pre>%pylab notebook<br/><br/>import dlib<br/>import cv2<br/>import os<br/>import tkinter<br/>from tkinter import filedialog<br/>from IPython import display<br/>root = tkinter.Tk()<br/>root.withdraw()<br/>#Go to your working directory (will be different for you)<br/>%cd /home/test/13293</pre>
<p>We've got to do some basic setup, as we did in the previous chapters. We're going to initialize <kbd>%pylab notebook</kbd>. Again, that will load NumPy and PyPlot and some other stuff, and we're going to perform <kbd>notebook</kbd> for now, which will be good for close-up views of images, though we're going to switch it to <kbd>inline</kbd> for the second example because we'll need that for looking at videos. Then, we have to import our other libraries. dlib is the focus of this section, of course.</p>
<p>We're going to use a few utilities from OpenCV, but it's just additional annotation and working with videos. We're going to use <kbd>tkinter</kbd> so we have a nice file dialog display. So, rather than hardcoding the filename into our code, we'll just prompt the user for the file that we want to analyze. We'll import <kbd>display</kbd> from <kbd>IPython</kbd> in order to watch the movie for the second example, and we have to set up <kbd>tkinter</kbd>; we want to make sure that we're in the working directory with all our files. You might not need this, but you can do it just to be sure.</p>
<p>So, we're going to select the cell, hit <em>Ctrl</em> + <em>Enter</em>, and then, if everything worked correctly, you should see the following output:</p>
<div><img class="alignnone size-full wp-image-503 image-border" src="img/b8e2d5f1-8a71-4a80-9fa6-856e48514edb.png" style="width:38.08em;height:2.67em;" width="503" height="35"/></div>
<p>You can see <kbd>Populating the interactive namespace</kbd> and your current working directory.</p>
<p>Okay, so let's see the first example now that we're set up, and we'll actually use those 68 features from that file that we downloaded; we'll see how easy it is to do this within dlib. Now, we're going to see that this is only just a little bit of code, but it does something really cool:</p>
<pre>imgname = filedialog.askopenfilename(parent = root,initialdir = os.getcwd(), title = 'Select image file...')<br/>img = imread(imgname)<br/>img.flags['WRITEABLE']=True<br/><br/>annotated = img.copy()<br/><br/>predictor_path = "./shape_predictor_68_face_landmarks.dat"<br/><br/>detector = dlib.get_frontal_face_detector()<br/><br/>predictor = dlib.shape_predictor(predictor_path)<br/>font = cv2.FONT_HERSHEY_SIMPLEX<br/><br/>dets = detector(img, 1)<br/>print("Number of faces detected: {}".format(len(dets)))<br/>for k, d in enumerate(dets):<br/>    print("Detection {}: Left: {} Top: {} Right: {} Bottom: {}".format(k, d.left(), d.top(), d.right(), d.bottom()))<br/>    shape = predictor(img,d)<br/>    print("Part 0: {}, Part 1:{} ...".format(shape.part(0),shape.part(1)))<br/>    head_width = shape.part(16).x-shape.part(0).y<br/>    fontsize = head_width/650<br/>    <br/>    for pt in range(68):<br/>        x,y = shape.part(pt).x, shape.part(pt).y<br/>        annotated = cv2.putText(annotated, str(pt), (x,y), font, fontsize, (255,255,255),2, cv2.LINE_AA)<br/>    <br/>figure(figsize = (8,6))<br/>imshow(annotated)</pre>
<p>First, we're going to ask the user for the filename. So, this is using <kbd>tkinter</kbd> and we're going to open a filename; it'll start searching in the current working directory using the <kbd>initialdir=os.getcwd()</kbd> function:</p>
<pre>imgname = filedialog.askopenfilename(parent = root,initialdir = os.getcwd(), title = 'Select image file...')</pre>
<p>We'll read that in using the following line:</p>
<pre>img = imread(imgname)<br/>img.flags['WRITEABLE']=True</pre>
<p>The <kbd>img.flags['WRITEABLE']=True</kbd> line is kind of a quirk of dlib, not really a big deal but, depending on how you loaded the file, <kbd>flags</kbd> for a <kbd>WRITEABLE</kbd> might be set <kbd>False</kbd>. That happens with <kbd>imread</kbd>. It depends on how you load it, but just to be sure, <kbd>WRITEABLE</kbd> needs to be set to <kbd>True</kbd>. Otherwise, dlib will throw an error. Depending on how you load, this might not be necessary.</p>
<p>We want to create an image that we can write on, where we can actually display where the landmarks were found, so we're going to create a copy of our image that we loaded earlier, the image that has the face in it, so we can write to it without clobbering the original image:</p>
<pre>annotated = img.copy()</pre>
<p>Now, we will load the data from the file we downloaded. <kbd>shape_predictor_68_face_landmarks.dat.bz2</kbd> comes in <kbd>.bz2</kbd> format; if you have not already unzipped it, you can unzip it to the <kbd>.dat</kbd> format. If you're on Windows, then it is recommend to use 7-zip. If you're on Linux or macOS, there should be a built-in utility you can just double-click on and it should be pretty straightforward to extract that.</p>
<p>So, we'll set the path and keep it in the current directory, and we need to initialize our objects:</p>
<pre>predictor_path = "./shape_predictor_68_face_landmarks.dat"</pre>
<p>Now, there are two stages here. First, you need to detect where the faces are. This is similar to what Haar cascades would do if you've used OpenCV and those examples before, but we use <kbd>dlib.get_frontal_face_detector</kbd>, which is just built-in:</p>
<pre>detector = dlib.get_frontal_face_detector()</pre>
<p>So, we create the <kbd>detector</kbd> object, get it from <kbd>dlib.get_frontal_face_detector</kbd>, initialize that, and then there's the <kbd>predictor</kbd>:</p>
<pre>predictor = dlib.shape_predictor(predictor_path)</pre>
<p>Once we've detected where the face is, we know how many faces there are, and there can be more than one. dlib works fine for multiple faces, as we'll see. Once you know where the faces are, then you can run the <kbd>predictor</kbd>, which actually finds where those 68 landmarks previously mentioned are. So, we create our <kbd>detector</kbd> object and our <kbd>predictor</kbd> object, again making sure <kbd>predictor_path</kbd> is set up correctly.</p>
<p>Then, we'll set our <kbd>font</kbd> here:</p>
<pre>font = cv2.FONT_HERSHEY_SIMPLEX </pre>
<p><kbd>font</kbd> is just displaying landmark data on the annotated image. So, you can change that if you want. Okay, now we get to the fun part of the code. First, do the detection, and find where exactly the faces are. Here's one really simple line of code:</p>
<pre>dets = detector(img,1) </pre>
<p>We're going to just print out the number of faces detected:</p>
<pre>print("Number of faces detected: {}".format(len(dets))) </pre>
<p>This can be useful for debugging purposes, although we'll see the output image where it actually detected the faces.</p>
<p>Now, we're going to do a <kbd>for</kbd> loop here, and this will handle the case where we could have more than one face:</p>
<pre>#1 detection = 1 face; iterate over them and display data 
for k, d in enumerate(dets):</pre>
<p>So, we're going to iterate over each one. The length of <kbd>dets</kbd> could be one, more than one, or zero, but we're not going to do that in this case. If you're not sure, then you might want to put this in a <kbd>try...catch</kbd> block, but we're only going to deal with images that have visible faces here.</p>
<p>So, we'll iterate over the faces, and display where exactly the bounding box is for each face on the <kbd>Left</kbd>, <kbd>Top</kbd>, <kbd>Right</kbd>, and <kbd>Bottom</kbd>; where exactly did those go? Note the following code:</p>
<pre>print("Detection {}: Left: {} Top:{} Right: {} Bottom: {}".format( 
   k, d.left(), d.top(), d.right(), d.bottom())) </pre>
<p>This is where the magic happens:</p>
<pre>shape = predictor(img, d) </pre>
<p>We're going to find the shape, then we're going to find those 68 landmarks, and just do a sanity check by printing out the first couple of landmarks just to make sure that it's working:</p>
<pre>print("Part 0: {}, Part 1: {} ...".format(shape.part(0), shape.part(1))) </pre>
<p>Okay, so, we have our landmarks for the face, and now we want to actually display it to understand what exactly we have here. We want to scale the <kbd>font</kbd> to make sure that fits the image because, depending on the size of the image, you could have a high resolution such as a 4,000 × 2,000 image, or you could have a low resolution such as a 300 × 200 (or something similar) and the heads could be very large in the image, as if the subject is close to the camera, or the reverse, small if it's far away.</p>
<p>So, we want to scale our <kbd>font</kbd> to the size of the head in the image:</p>
<pre>#We want to scale the font to be in proportion to the head 
#pts 16 and 0 correspond to the extreme points on the right/left side of head 
head_width = shape.part(16).x-shape.part(0).x 
fontsize = head_width/650 </pre>
<p>So, here we're just computing <kbd>head_width</kbd>. <kbd>shape</kbd> is a predictor object that has a <kbd>part</kbd> method, and you pass in the index of the landmark that you want to find and each landmark is going to have an <kbd>x</kbd> and a <kbd>y</kbd> part. So, <kbd>head_width</kbd> is <kbd>16</kbd> here, which is dependent on your perspective. <kbd>head_width</kbd> is just width in terms of pixels of the head. Then, we're going to scale the font size based on <kbd>head_width</kbd>, and <kbd>650</kbd> is just a nice factor that works well.</p>
<p>Now, we have all the data, we're going to iterate over each of the points:</p>
<pre>for pt in range(68); 
   x,y = shape.part(pt).x, shape.part(pt).y 
   annotated=cv2.putText(annotated, str(pt), (x,y), font, fontsize, (255,255,255),2, cv2.LINE_AA) </pre>
<p>So, we'll hardcode <kbd>68</kbd>, because we know that we have <kbd>68</kbd> points, but if you were using another kind of shape finder, such as a pre-trained shape finder, then you might want to change this number. We iterate over the points and then we get the <kbd>x</kbd> and the <kbd>y</kbd> coordinates for each of the landmarks that were shown before. We extract the <kbd>x</kbd> and <kbd>y</kbd> coordinates using <kbd>shape.part</kbd> and update the annotated image. We need <kbd>cv2</kbd> to put the text into the image. dlib does have something similar to this, but <kbd>cv2</kbd> is better, and we can have just one interface for that anyway. So, we're going to use OpenCV here and then we're going to create a figure and display it:</p>
<pre>figure(figsize=(8,6)) 
imshow(annotated) </pre>
<p>So, that's all regarding the code, and hopefully that seems pretty straightforward to you. Read it at your leisure. When we execute the code, we can see a dialog box of stock photos. We can select any photo among those; for instance, here is the photo of a man wearing a hat. So, it'll take just a little bit to compute that, and here you go:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-504 image-border" src="img/8131ab8a-d1ba-4a20-a662-b96e70e491fe.png" style="width:38.33em;height:25.00em;" width="703" height="458"/></p>
<p>We see this guy with all 68 points. We have labeled it from 0 to <strong>67</strong>, because of Python's index from 0 convention, but we can see that, just like before, we have all of the points; so, you can see point 0 on the left side, point 16 on the right side, depending on your perspective, and then it continues all the way around. Here's a zoomed view for clarity:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-505 image-border" src="img/e2c13edd-8383-479c-8956-52562790372c.png" style="width:35.25em;height:30.75em;" width="577" height="503"/></p>
<p>As we can see, some of the points are close together, but you can get the idea here of what's what. It looks pretty clear. So, this is pretty cool, and there's a whole lot you can do with this, as mentioned before. This guy is looking straight into the camera, so you might be asking what happens if somebody has their head tilted? Alright, we're going to run this again.</p>
<p>Let's select the stock photo woman here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-506 image-border" src="img/234d6149-587a-4198-b328-933943ad6e4a.png" style="width:31.00em;height:32.17em;" width="490" height="509"/></p>
<p>You can see her head is turned, and yet this still works fine. It won't always work in extreme cases; if somebody's head is turned so much that landmarks aren't there, then this can fail for reasonable cases, you can see that this actually works very nicely.</p>
<p>Okay, so what about multiple faces? Does this work for that? Let's have a look at another group photo:</p>
<div><img class="alignnone size-full wp-image-690 image-border" src="img/98189d50-d116-461c-aee8-f01e2699f76d.png" style="width:41.50em;height:19.92em;" width="861" height="414"/></div>
<p>We can see we have six people here in various poses. Given the resolution here, it's not possible to read those annotations, but that's perfectly okay because you have seen where they are, and we can see that we actually detected all six faces very nicely. So, hopefully you can get some ideas here of how you can use this in your own code, and just how easy dlib makes it for you in terms of detection phases.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Faces in videos</h1>
                
            
            
                
<p>We're going to see our second example from what we learned in the last section on faces in photos. The still image example was neat, but you might be asking about videos. Okay, let's look at that for our next example:</p>
<pre>%pylab inline<br/>%cd /home/test/13293<br/>   <br/>import dlib<br/>import cv2<br/>import os<br/>import tkinter<br/>from tkinter import filedialog<br/>from IPython import display<br/>root = tkinter.Tk()<br/>root.withdraw()</pre>
<p>We change to <kbd>%pylab inline</kbd> because having all those widgets can actually cause a problem with Jupyter when you want to display a video sequence. We'll need the same code to get started with as shown in the previous example, and only replace <kbd>notebook</kbd> with <kbd>inline</kbd>. Then, we run the same code again.</p>
<p>After its execution, we move on with the next part. This is actually very close to the same thing because all you have to do is iterate over each frame, and it will work just the same:</p>
<pre>predictor_path = "./shape_predictor_68_face_landmarks.dat" 
detector = dlib.get_frontal_face_detector() 
predictor = dlib.shape_predictor(predictor_path) </pre>
<p>So, you see this code is pretty much the same as the previous example. If you want, you can do this with your webcam. It's actually pretty neat to watch. We'll not be using a webcam here, but for your custom project, if you want to use a webcam you can add the following line:</p>
<pre>cap = cv2.VideoCapture(0)  
#0 is the first camera on your computer, change if you have more #than one camera </pre>
<p>We're assuming here that you only have one webcam. If you have more than one camera and you don't want to use the first one, then you might need to change that <kbd>0</kbd> to something else. If you don't want to use your webcam, add the following line:</p>
<pre>cap = cv2.Videocapture('./rollerc.mp4') </pre>
<p>Here, we are not using a webcam. We want to create a figure that we're going to display, and we'll name it <kbd>100</kbd> to make sure it has its own unique ID. We'll use the same <kbd>font</kbd> as in the previous example:</p>
<pre>font = cv2.FONT_HERSHEY_SIMPLEX </pre>
<p>It sounds really complicated, but it's just an ordinary font. We're going to create a <kbd>while</kbd> loop, which is going to go over each and every frame:</p>
<pre>while(True): 
   #capture frame-by-frame 
ret, img = cap.read 
img.flags['WRITEABLE']=True #just in case </pre>
<p>So, we have <kbd>cap</kbd> as our video capture object from OpenCV, and then all we have to do to read the frames is <kbd>cap.read()</kbd>. <kbd>ret</kbd> is just code that makes sure that we actually read a frame. Then, <kbd>img</kbd> is the actual image that is returned, and again make sure that the <kbd>WRITEABLE</kbd> flag is set, otherwise dlib could produce an error.</p>
<p>We're going to try to find a face and, if the face is not found, then we're going to release and break out of our loop:</p>
<pre>try: 
   dets = detector(img, 1) 
   shape = predictor(img, dets[0]) 
except: 
   print('no face detected', end='\r') 
   cap.release() 
   break </pre>
<p>You might not want this for your application, but one neat thing here is if you're using a webcam, an easy way to just stop this loop from running indefinitely is to just put your hand in front of the face. You put your hand in front of the camera, or turn your head, or whatever, and that will automatically stop it, hands-free. Otherwise, you can send a kernel interrupt and just make sure you do <kbd>cap.release()</kbd>, otherwise the video source will stay open and you might get an error later.</p>
<p>According to the preceding code block, we grab the image, detect the faces, and take the shape. For this code, we'll assume that there's only one face, but you can see from the previous example how to deal with multiple faces.</p>
<p>Then, we create the blank image or the image that's a copy of the original, which we can write without distorting the original. Set the <kbd>head_width</kbd> and <kbd>fontsize</kbd>, and then just do exactly what we did before. Find the <kbd>x</kbd> and <kbd>y</kbd> points, and then write to them:</p>
<pre>annotated=img.copy() 
head_width = shape.part(16).x-shape.part(0).x 
fontsize = head_width/650 
for pt in range(68): 
   x,y = shape.part(pt).x, shape.part(pt).y 
   annotated=cv2.putText(annotated, str(pt), (x,y), font, fontsize, (255,255,255),2, cv2.LINE_AA) 
 </pre>
<p>We are going to display our results, as shown in the following code:</p>
<pre>fig=imshow(cv2.cvtColor(annotated,cv2.COLOR_BGR2RGB </pre>
<p>Note the color, <kbd>BGR2RGB</kbd>. That's because OpenCV uses <strong>blue green red</strong> (<strong>BGR</strong>) by default, and the colors will look really funny if you don't change that for the display. Then, there's some stuff here that will make sure that our window is updating while the script is still running. Otherwise, it will actually just run the entire script and you won't see what's happening in real time.</p>
<p>We then hit <em>Shift</em> + <em>Enter</em>. It might take a second to load and then it'll run pretty slowly, largely because it's part of Jupyter Notebook. You can take the code out and run it as an independent program, and probably you'll want to create a <kbd>cv2</kbd> named window, but this will do for our purposes. When you execute the cell, you'll see two women:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-508 image-border" src="img/9a43cb3c-24ab-4896-ba42-1162aeaa7572.png" style="width:31.17em;height:18.17em;" width="476" height="277"/></p>
<p>One face is kind of obscured, so it's not going to detect her, but for the one who's in the foreground, as you can see, her face is being tracked pretty nicely and the landmarks are being found. This can work in real time depending on your hardware, and this isn't the kind of thing that you want to run in a Jupyter Notebook. You can watch this as long as you want to, but you get the idea.</p>
<p>So, that's how easy it is to work with video. Switch to the other woman in the background, and the first one's face is turned:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-509 image-border" src="img/80e7597f-ba7f-449f-957a-977d6f6f3974.png" style="width:30.00em;height:17.83em;" width="470" height="280"/></p>
<p>That's how easy it is to work with video, and you can detect multiple faces and do whatever you want with this information.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Facial recognition</h1>
                
            
            
                
<p>We're going to see how we can perform facial recognition with dlib with a relatively small amount of code. Facial recognition here means that we're going to look at an image and see whether or not this person is the same as the person in a different image. We're going to keep it simple here and just compare two faces to see whether they're the same, but this can easily be generalized, as we'll see later.</p>
<p>Here, we're going to do something similar to the first example, where we're going to prompt the user to open two files, each with a face that is going to be compared to another. For this, we are going to use some faces from <strong>Labeled Faces in the Wild</strong> (<strong>LFW</strong>). It's a nice database that has thousands of faces from various celebrities. You can download the entire set from <a href="http://vis-www.cs.umass.edu/lfw/" target="_blank">http://vis-www.cs.umass.edu/lfw/</a> and get a whole lot of examples that you can play with. So, we are just going to use a small subset of examples from the dataset to do our example here.</p>
<p>We prompt a user to select two different facial images. We're going to start the initial directory in the <kbd>faces</kbd> subdirectory of the project folder:</p>
<pre>#Prompt the user for two images with one face each 
imgname = filedialog.askopenfilename(parent=root, initialdir='faces', title='First face...') 
face1 = imread(imgname) 
face1.flags['WRITEABLE']=True 
#second face 
imgname = filedialog.askopenfilename(parent=root, initialdir='faces', title='Second face...') 
face2 = imread(imgname) 
face2.flags['WRITEABLE']=True </pre>
<p>There are two additional files that you're going to need from <a href="http://dlib.net/files" target="_blank">dlib.net/files</a>, and they are the <kbd>shape_predictor_5_face_landmarks.dat</kbd> file and the <kbd>dlib_face_recognition_resnet_model_v1.dat</kbd> file. Again, they're going to be in <kbd>bz2</kbd> format. So, interestingly, we're only using five facial landmarks for this, but combined with the descriptors that's actually very adequate for describing a human face. Hence, we are not using 68 face landmarks, but just 5. We'll see just how nicely that works. Download those files and unzip <kbd>bz2</kbd>, just as we did in the first example.</p>
<p>Now, we set the path to the proper file locations:</p>
<pre>predictor_path = './shape_predictor_5_face_landmarks.dat 
face_rec_model_path= './ dlib_face_recognition_resnet_model_v1.dat </pre>
<p>The <kbd>predictor</kbd> works similarly to the 68 face landmarks, but again a link comes up with five results, and we're going to use a pre-trained recognition model. It works on a variety of faces; you won't have to retrain it now. Here, we don't have to do any complicated deep learning modeling for this. There are ways to train your own models, but you'll see that this will actually work very nicely for a wide variety of applications.</p>
<p>So, we create our <kbd>detector</kbd>, as before. That doesn't require any additional data:</p>
<pre>detector = dlib.get_frontal_face_detector() </pre>
<p>We're going to create our shape finder, similar to the previous example, and again we're using the five facial landmarks detector. We're going to create a new <kbd>facerec</kbd> object that comes from <kbd>dlib.face_recognition_model_v1</kbd>, passing in the path as <kbd>face_rec_model_path</kbd>:</p>
<pre>sp = dlib.shape_predictor(predictor_path) 
facerec = dlib.face_recognition_model_v1(face_rec_model_path) </pre>
<p>Now, what <kbd>facerec</kbd> does is it takes a mapping, given our detected face and given the shapes and the location of where those landmarks are, and it's going to create a 128-length float vector, called a descriptor, that's going describe the face. So, it actually creates something that will be a description of a face, and is something that will capture the essence of a face. If you have the same person in two different pictures, where in one picture the person is far away from the camera and in another their face might be turned, it could be as many pictures and there could be different lighting conditions and so forth. The descriptor should be pretty much invariant to that. The descriptor is never exactly the same, but the same person should get a similar enough face descriptor, regardless of their orientation, the lighting conditions, and so forth. Even if they change their hair or they're wearing a hat, you should get a similar descriptor, and <kbd>facerec</kbd> actually does a good job of that.</p>
<p>The following code just performs the detection and the shape finding:</p>
<pre>dets1 = detector(face1, 1) 
shape1 = sp(face1, dets1[0]) 
dets2 = detector(face2, 1) 
shape2 = sp(face2, dets2[0])</pre>
<p>Then, we're going to perform the operation that we described previously: given the detection, spatial features, and the landmarks, we're going to compute the 128-point vector, and we can inspect it a little bit. Then, we're going to look at the faces side by side:</p>
<pre>figure(200) 
subplot(1,2,1) 
imshow(face1) 
subplot(1,2,2) 
imshow(face2) </pre>
<p>Now, we want to know how similar the faces are, so we're going to compute the Euclidean distance:</p>
<pre>euclidean_distance = np.linalg.norm(np.array(face_descriptor1)-np.array(face_descriptor2)) </pre>
<p>What that means is you take each point, 1 through 128, and you subtract the second one from the first one, you square each one, you sum them together, and take the square root, and that's going to give you a single number. That number is going to be used to determine whether or not these two images are of the same person's face.</p>
<p>There's a magic number, <kbd>0.6</kbd>, which we're going to use here, and which has been determined empirically to work very well. If the 128-dimensional distance is less than <kbd>0.6</kbd>, we say that these two images are of the same person. If it's more than <kbd>0.6</kbd>, or equal to <kbd>0.6</kbd> as in this case, we're going to say that these are different people. So, we look at the two images, compute all those metrics, and then we're going to say if it is <kbd>&lt;0.6</kbd>, the faces match, and if it is <kbd>&gt;0.6</kbd>, the faces are different:</p>
<pre>if euclidean_distance&lt;0.6: 
   print('Faces match') 
else: 
   print('Faces are different') </pre>
<p>Now, let's run the code. You'll see a dialog of celebrity photos from the LFW. We'll pick one of Alec Baldwin and one of Sylvester Stallone:</p>
<div><img class="alignnone size-full wp-image-510 image-border" src="img/cc0a96d9-6198-4055-9983-9231eeee5be9.png" style="width:32.25em;height:18.00em;" width="387" height="216"/></div>
<p>Baldwin and Sylvester Stallone are classified as two different people. That is exactly as we expected, as the faces are different. Now, let's do it for another pair. Let's compare Alec Baldwin to Alec Baldwin:</p>
<div><img class="alignnone size-full wp-image-511 image-border" src="img/a8b1916d-cc5f-4b2b-8de4-fba8bed71416.png" style="width:32.42em;height:18.33em;" width="389" height="220"/></div>
<p>Here, you can see that their faces match. Let's do a few more comparisons for fun. So, Yao Ming and Winona Ryder look different from each other:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-512 image-border" src="img/befb8a98-2baf-4d94-a90f-d9b7f9e57314.png" style="width:32.58em;height:18.50em;" width="391" height="222"/></p>
<p>Then, we take two different pictures of Winona Ryder, and the faces match:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-513 image-border" src="img/677b9fbb-8d52-4b3a-bffb-d7eafceb104d.png" style="width:32.58em;height:18.58em;" width="391" height="223"/></p>
<p>You can do all kinds of combinations of this. Okay, so this is pretty easy. It might be useful to take a look at the facial descriptor; you can just hit <em>Shift</em> + <em>Tab,</em> and you can see what the vectors look like, which is something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-514 image-border" src="img/520480df-9d83-4c50-b02f-2b4f8351549a.png" style="width:16.92em;height:25.58em;" width="230" height="350"/></p>
<p>It's not very human-understandable, but is available just in case you're curious about it. That is enough to capture the essence of a human face, and just using a simple comparison, we can actually do a pretty good job of telling whether the two pictures are of the same face. This actually has a greater than 99% accuracy on the LFW dataset. So, you'll actually have a difficult time finding two faces that get bad results, whether two faces of the same person that are said not to match, or two of different people that are said to match.</p>
<p>Therefore, if you wanted to adapt this to your own needs, what you can do is get your own database, just your own directory of faces of people that you want to recognize, and then when you have a new face, just go through each of the faces in your database. Just do a <kbd>for</kbd> loop and compare your new face to each one. For the Euclidean distance, computed here simply by using the NumPy linear algebra norm (<kbd>np.linalg.norm</kbd>), if that distance is less than 0.6, then you can say that you have a match. If you are concerned with false positives, you can have multiple faces of a person and compare to each one, then do a majority rule.</p>
<p>Otherwise, suppose you have ten faces and you want to make sure that all ten of them match. If you really want to make sure that you did not get a false positive, you can just get ten really good images and then compare your new test image to all ten. But in any case, you can see from this example that it does not take a whole lot of code, and this method can be adapted to a wide variety of applications.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we had a brief introduction to the dlib library and learned how to use it for facial recognition. We then learned how to generate the outline for a face using the 68 facial landmarks pre-trained model. Later, we learned how to find the facial landmarks for a single person, multiple people, and for people in videos.</p>
<p>In the next chapter, <a href="ffda7469-1745-4a0a-8375-43426248af4d.xhtml" target="_blank">Chapter 7</a>, <em>Deep Learning Image Classification with TensorFlow</em>, we'll learn how to classify images with TensorFlow using a pre-trained model, and later we'll use our own custom images.</p>


            

            
        
    </div>



  </body></html>