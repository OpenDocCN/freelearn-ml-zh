<html><head></head><body>
		<div><p><a id="_idTextAnchor984"/><a id="_idTextAnchor985"/><a id="_idTextAnchor986"/></p>
			<h1 id="_idParaDest-219" class="chapter-number"><a id="_idTextAnchor987"/><st c="0">8</st></h1>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor988"/><st c="2">Creating New Features</st></h1>
			<p><st c="23">Adding new features to a dataset can help machine learning models learn patterns and important details in the data. </st><st c="140">For example, in finance, the </st><strong class="bold"><st c="169">disposable income</st></strong><st c="186">, which is the </st><em class="italic"><st c="201">total income</st></em><st c="213"> minus the </st><em class="italic"><st c="224">acquired debt</st></em><st c="237"> for any</st><a id="_idIndexMarker560"/><st c="245"> one month, might be more relevant for credit risk than just the income or the acquired debt. </st><st c="339">Similarly, the </st><em class="italic"><st c="354">total acquired debt</st></em><st c="373"> of a person across financial products, such as a car loan, a mortgage, and credit cards, might be more important to estimate the credit risk than any debt considered individually. </st><st c="554">In these examples, we use domain knowledge to craft new variables, and these variables are created by adding or subtracting </st><st c="678">existing features.</st></p>
			<p><st c="696">In some cases, a variable may not have a linear or monotonic relationship with the target, but a polynomial combination might. </st><st c="824">For example, if our variable has a quadratic relationship with the target, </st><img src="img/36.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.257em;height:0.960em;width:2.392em"/><st c="899"/><st c="900">, we can convert that into a linear relationship by squaring the original variable. </st><st c="984">We can also help linear models better understand the relationships between variables and targets by transforming the predictors through splines, or by using </st><st c="1141">decision trees.</st></p>
			<p><st c="1156">The advantage of crafting additional features to train simpler models, such as linear or logistic regression, is that both the features and the models remain interpretable. </st><st c="1330">We can explain the reasons driving a model’s output to management, clients, and regulators, adding a layer of transparency to our machine learning pipelines. </st><st c="1488">In addition, simpler models tend to be faster to train and easier to deploy </st><st c="1564">and maintain.</st></p>
			<p><st c="1577">In this chapter, we will create new features by transforming or combining variables with mathematical functions, splines, and </st><st c="1704">decision trees.</st></p>
			<p><st c="1719">This chapter will cover the </st><st c="1748">following recipes:</st></p>
			<ul>
				<li><st c="1766">Combining features with </st><st c="1791">mathematical functions</st></li>
				<li><st c="1813">Comparing features to </st><st c="1836">reference variables</st></li>
				<li><st c="1855">Performing </st><st c="1867">polynomial expansion</st></li>
				<li><st c="1887">Combining features with </st><st c="1912">decision trees</st></li>
				<li><st c="1926">Creating periodic features from </st><st c="1959">cyclical variables</st></li>
				<li><st c="1977">Creating </st><st c="1987">spline features</st><a id="_idTextAnchor989"/><a id="_idTextAnchor990"/></li>
			</ul>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor991"/><st c="2002">Technical requirements</st></h1>
			<p><st c="2025">In this chapter, we will use the </st><code><st c="2059">pandas</st></code><st c="2065">, </st><code><st c="2067">numpy</st></code><st c="2072">, </st><code><st c="2074">matplotlib</st></code><st c="2084">, </st><code><st c="2086">scikit-learn</st></code><st c="2098">, and </st><code><st c="2104">feature-engine</st></code> <st c="2118">Python libraries.</st><a id="_idTextAnchor992"/><a id="_idTextAnchor993"/></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor994"/><st c="2136">Combining features with mathematical functions</st></h1>
			<p><st c="2183">New features</st><a id="_idIndexMarker561"/><st c="2196"> can be created by combining existing variables with mathematical and statistical functions. </st><st c="2289">Taking an example from</st><a id="_idIndexMarker562"/><st c="2311"> the finance industry, we can calcula</st><a id="_idTextAnchor995"/><st c="2348">te the total debt of a person by summing up their debt across individual financial products, such as car loan, mortgage, or credit </st><st c="2480">card debt:</st></p>
			<p><em class="italic"><st c="2490">Total debt = car loan debt + credit card debt + </st></em><em class="italic"><st c="2539">mortgage debt</st></em></p>
			<p><st c="2552">We can also derive other insightful features using alternative statistical operations. </st><st c="2640">For example, we can determine the maximum debt of a customer across financial products or the average time a user spends on </st><st c="2764">a website:</st></p>
			<p><em class="italic"><st c="2774">maximum debt = max(car loan balance, credit card balance, </st></em><em class="italic"><st c="2833">mortgage balance)</st></em></p>
			<p><em class="italic"><st c="2850">average time on website = mean(time spent on homepage, time spent on about page, time spent on </st></em><em class="italic"><st c="2946">FAQ page)</st></em></p>
			<p><st c="2955">We can, in principle, use any mathematical or statistical operation to create new features, such as the product, mean, standard deviation, or maximum or minimum values. </st><st c="3125">In this recipe, we will implement these mathematical operations using </st><code><st c="3195">pandas</st></code> <st c="3201">and </st><code><st c="3206">feature-engine</st></code><st c="3220">.</st></p>
			<p class="callout-heading"><st c="3221">Note</st></p>
			<p class="callout"><st c="3226">While, in the recipe, we can show you how to combine features with mathematical functions, we can’t do justice to the use of domain knowledge in deciding which function to apply, as that varies with every domain. </st><st c="3440">So, we will leave that </st><st c="3463">with yo</st><a id="_idTextAnchor996"/><a id="_idTextAnchor997"/><st c="3470">u.</st></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor998"/><st c="3473">Getting ready</st></h2>
			<p><st c="3487">In this recipe, we </st><a id="_idIndexMarker563"/><st c="3507">will use the breast cancer dataset from </st><code><st c="3547">scikit-learn</st></code><st c="3559">. The features are computed from digitized images of </st><a id="_idIndexMarker564"/><st c="3612">breast cells and describe the characteristics of their cell nuclei, in terms of smoothness, concavity, symmetry, and compactness, among others. </st><st c="3756">Each row contains information about the morphology of cell nuclei in a tissue sample. </st><st c="3842">The target variable indicates whether the tissue sample corresponds to cancerous cells. </st><st c="3930">The idea is to predict whether the tissue samples belong to benign or malignant breast cells, based on their cell </st><st c="4044">nuclei morphology.</st></p>
			<p><st c="4062">To become familiar with the dataset, run the following commands in a Jupyter notebook or </st><st c="4152">Python console:</st></p>
			<pre class="source-code"><st c="4167">
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
print(data.DESCR)</st></pre>			<p><st c="4261">The preceding code block should print out a description of the dataset and an interpretation of </st><st c="4358">its variab</st><a id="_idTextAnchor999"/><a id="_idTextAnchor1000"/><st c="4368">les.</st></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor1001"/><st c="4373">How to do it...</st></h2>
			<p><st c="4389">In </st><a id="_idTextAnchor1002"/><st c="4393">this recipe, we will create new features by combining variables using multiple </st><st c="4472">mathematical operations:</st></p>
			<ol>
				<li><st c="4496">Let’s begin by loading the necessary libraries, classes, </st><st c="4554">and data:</st><pre class="source-code"><st c="4563">
import pandas as pd
from feature_engine.creation import MathFeatures
from sklearn.datasets import load_breast_cancer</st></pre></li>				<li><st c="4680">Next, load the breast cancer dataset into a </st><code><st c="4725">pandas</st></code><st c="4731"> DataFrame:</st><pre class="source-code"><st c="4742">
data = load_breast_cancer()
df = pd.DataFrame(data.data,
    columns=data.feature_names)</st></pre><p class="list-inset"><st c="4827">In the following code lines, we will create new features by combining variables using multiple </st><st c="4923">mathematical operations.</st></p></li>				<li><st c="4947">Let’s</st><a id="_idIndexMarker565"/><st c="4953"> begin</st><a id="_idIndexMarker566"/><st c="4959"> by creating a list with the subset of the features that we want </st><st c="5024">to combine:</st><pre class="source-code"><st c="5035">
features = [
    «mean smoothness",
    «mean compactness",
    «mean concavity",
    «mean concave points",
    «mean symmetry",
]</st></pre><p class="list-inset"><st c="5147">The features in </st><em class="italic"><st c="5164">step 3</st></em><st c="5170"> represent the mean characteristics of cell nuclei in the images. </st><st c="5236">It might be useful to obtain the mean across all </st><st c="5285">examined characteristics.</st></p></li>				<li><st c="5310">Let</st><a id="_idTextAnchor1003"/><st c="5314">’s get the mean value of the features and then display the </st><st c="5374">resulting feature:</st><pre class="source-code"><st c="5392">
df["mean_features"] = df[features].mean(axis=1)
df["mean_features"].head()</st></pre><p class="list-inset"><st c="5467">The following output shows the mean value of the features from </st><em class="italic"><st c="5531">step 3</st></em><st c="5537">:</st></p><pre class="source-code"><strong class="bold"><st c="5539">0    0.21702</st></strong>
<strong class="bold"><st c="5548">1    0.10033</st></strong>
<strong class="bold"><st c="5558">2    0.16034</st></strong>
<strong class="bold"><st c="5568">3    0.20654</st></strong>
<strong class="bold"><st c="5578">4    0.14326</st></strong>
<strong class="bold"><st c="5588">Name: mean_features, dtype: float64</st></strong></pre></li>				<li><st c="5624">Similarly, to</st><a id="_idIndexMarker567"/><st c="5638"> capture the general </st><a id="_idIndexMarker568"/><st c="5659">variability of the cell nuclei, let’s determine the standard deviation of the mean characteristics, and then display the </st><st c="5780">resulting feature:</st><pre class="source-code"><st c="5798">
df["std_features"] = df[features].std(axis=1)
df["std_features"].head()</st></pre><p class="list-inset"><st c="5870">The following output shows the standard deviation of the features from </st><st c="5942">step 3:</st></p><pre class="source-code"><strong class="bold"><st c="5949">0    0.080321</st></strong>
<strong class="bold"><st c="5960">1    0.045671</st></strong>
<strong class="bold"><st c="5971">2    0.042333</st></strong>
<strong class="bold"><st c="5982">3    0.078097</st></strong>
<strong class="bold"><st c="5993">4    0.044402</st></strong>
<strong class="bold"><st c="6004">Name: std_features, dtype: float64</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="6039">Note</st></p>
			<p class="callout"><st c="6044">When we craft new features based on domain knowledge, we know exactly how we want to combine the variables. </st><st c="6153">We could also combine features with multiple operations and then evaluate whether they are predictive, using, for example, a feature selection algorithm or deriving feature importance from the machine </st><st c="6354">learning model.</st></p>
			<ol>
				<li value="6"><st c="6369">Le</st><a id="_idTextAnchor1004"/><st c="6372">t’s make a list containing mathematical functions that we want to use to combine </st><st c="6454">the features:</st><pre class="source-code"><st c="6467">
math_func = [
    "sum", "prod", "mean", "std", "max", "min"]</st></pre></li>				<li><st c="6525">Now, let’s apply the functions from </st><em class="italic"><st c="6562">step 6</st></em><st c="6568"> to combine the features from </st><em class="italic"><st c="6598">step 3</st></em><st c="6604">, capturing the</st><a id="_idIndexMarker569"/><st c="6619"> resulting variables in a </st><st c="6645">new DataFrame:</st><pre class="source-code"><st c="6659">
df_t = df[features].agg(math_func, axis="columns")</st></pre><p class="list-inset"><st c="6710">If we</st><a id="_idIndexMarker570"/><st c="6716"> execute </st><code><st c="6725">df_t.head()</st></code><st c="6736">, we will see the DataFrame with the newly </st><st c="6779">created </st><a id="_idTextAnchor1005"/><st c="6787">features:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_1.jpg" alt="Figure 8.1 – A DataFrame with the newly created features"/><st c="6796"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7074">Figure 8.1 – A DataFrame with the newly created features</st></p>
			<p class="callout-heading"><st c="7130">Note</st></p>
			<p class="callout"><code><st c="7135">pandas</st></code> <code><st c="7142">agg</st></code><st c="7146"> can apply multiple functions to combine features. </st><st c="7197">It can take a list of strings with the function names, as we did in </st><em class="italic"><st c="7265">step 7</st></em><st c="7271">; a list of NumPy functions, such as </st><code><st c="7309">np.log</st></code><st c="7315">; and Python functions that </st><st c="7344">you creat</st><a id="_idTextAnchor1006"/><st c="7353">e.</st></p>
			<p class="list-inset"><st c="7356">We can create the same features that we created with </st><code><st c="7410">pandas</st></code><st c="7416"> automatically by </st><st c="7434">using </st><code><st c="7440">feature-engine</st></code><st c="7454">.</st></p>
			<ol>
				<li value="8"><st c="7455">Let’s create a list by using the name of the </st><st c="7501">output features:</st><pre class="source-code"><st c="7517">
new_feature_names = [
    "sum_f", "prod_f", "mean_f",
    „std_f", „max_f", „min_f"]</st></pre></li>				<li><st c="7595">Let’s set up </st><code><st c="7609">MathFeatures()</st></code><st c="7623"> to apply the functions in </st><em class="italic"><st c="7650">step 6</st></em><st c="7656"> to the features from </st><em class="italic"><st c="7678">step 3</st></em><st c="7684">, naming the new features with the strings from </st><em class="italic"><st c="7732">step 8</st></em><st c="7738">:</st><pre class="source-code"><st c="7740">
create = MathFeatures(
    variables=features,
    func=math_func,
    new_variables_names=new_feature_names</st><a id="_idTextAnchor1007"/><st c="7837">,
)</st></pre></li>				<li><st c="7840">Let’s add </st><a id="_idIndexMarker571"/><st c="7851">the new features</st><a id="_idIndexMarker572"/><st c="7867"> to the original DataFrame, capturing the result in a </st><st c="7921">new variable:</st><pre class="source-code"><st c="7934">
df_t = create.fit_transform(df)</st></pre><p class="list-inset"><st c="7966">We can display the input and output features by executing </st><code><st c="8025">df_t[features + </st></code><code><st c="8041">new_feature_name</st><a id="_idTextAnchor1008"/><st c="8057">s].head()</st></code><st c="8067">:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_2.jpg" alt="Figure 8.2 – DataFrame with the input features and the newly created variables"/><st c="8069"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="8621">Figure 8.2 – DataFrame with the input features and the newly created variables</st></p>
			<p><st c="8699">While </st><code><st c="8706">pandas</st></code> <code><st c="8712">agg</st></code><st c="8716"> returns a DataFrame with the features resulting from the operation, </st><code><st c="8785">feature-engine</st></code><st c="8799"> goes one step further, by concatenating the new features to the </st><st c="8864">original </st><a id="_idTextAnchor1009"/><a id="_idTextAnchor1010"/><st c="8873">DataFrame.</st></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor1011"/><st c="8883">How it works...</st></h2>
			<p><code><st c="8899">pandas</st></code><st c="8906"> has many built-in operations to apply mathematical and statistical computations to a group of variables. </st><st c="9012">To combine features mathematically, we first made a list containing the names of the features we wanted to combine. </st><st c="9128">Then, we determined the mean and standard deviation of those features by using </st><code><st c="9207">pandas</st></code> <code><st c="9213">mean()</st></code><st c="9220"> and </st><code><st c="9225">std()</st></code><st c="9230">. We could also apply any of the </st><code><st c="9263">sum()</st></code><st c="9268">, </st><code><st c="9270">prod()</st></code><st c="9276">, </st><code><st c="9278">max()</st></code><st c="9283">, and </st><code><st c="9289">min()</st></code><st c="9294"> methods, which return the sum, product, maximum, and minimum values of those features, respectively. </st><st c="9396">To perform these operations across the columns, we added the </st><code><st c="9457">axis=1</st></code><st c="9463"> argument within </st><st c="9480">the methods.</st></p>
			<p><st c="9492">With pandas </st><code><st c="9505">agg()</st></code><st c="9510">, we </st><a id="_idIndexMarker573"/><st c="9515">applied several mathematical functions simultaneously. </st><st c="9570">It takes as arguments a list of strings, corresponding to the functions to apply and the </st><code><st c="9659">axis</st></code><st c="9663"> that the functions should be applied to, which can be either </st><code><st c="9725">1</st></code><st c="9726"> for columns or </st><code><st c="9742">0</st></code><st c="9743"> for rows. </st><st c="9754">As a result, pandas </st><code><st c="9774">agg()</st></code><st c="9779"> returned a </st><code><st c="9791">pandas</st></code><st c="9797"> DataFrame, resulting from applying the mathematical functions to the groups </st><st c="9874">of features.</st></p>
			<p><st c="9886">Finally,</st><a id="_idTextAnchor1012"/><st c="9895"> we </st><a id="_idIndexMarker574"/><st c="9899">created the same features by combining variables with </st><code><st c="9953">feature-engine</st></code><st c="9967">. We used the </st><code><st c="9981">MathFeatures()</st></code><st c="9995"> transformer, which takes the features to combine and the functions to apply as input; it also has the option to indicate the names of the resulting features. </st><st c="10154">When we used </st><code><st c="10167">fit()</st></code><st c="10172">, the transformer did not learn parameters but checked that the variables were indeed numerical. </st><st c="10269">The </st><code><st c="10273">transform()</st></code><st c="10284"> method triggered the use of </st><code><st c="10313">pandas.agg</st></code><st c="10323"> under the hood, applying the mathematical functions to create the </st><st c="10390">ne</st><a id="_idTextAnchor1013"/><a id="_idTextAnchor1014"/><st c="10392">w variables.</st></p>
			<h2 id="_idParaDest-226"><st c="10405">See also</st><a id="_idTextAnchor1015"/></h2>
			<p><st c="10414">To find out more about the mathematical operations supported by </st><code><st c="10479">pandas</st></code><st c="10485">, </st><st c="10487">visit </st><a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats"><st c="10493">https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#computations-descriptive-stats</st></a><st c="10589">.</st></p>
			<p><st c="10590">To learn more about </st><code><st c="10611">pandas</st></code> <code><st c="10617">aggregate</st></code><st c="10627">, check </st><st c="10635">out </st><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html"><st c="10639">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg</st><st c="10718">regate.html</st></a><st c="10730">.</st></p>
			<h1 id="_idParaDest-227"><st c="10731">Comparing features to reference vari</st><a id="_idTextAnchor1018"/><st c="10768">ables</st></h1>
			<p><st c="10774">In the </st><a id="_idIndexMarker575"/><st c="10782">previous recipe, </st><em class="italic"><st c="10799">Combining features with mathematical functions</st></em><st c="10845">, we created new features by applying mathematical</st><a id="_idIndexMarker576"/><st c="10895"> or statistical functions, such as the sum or the mean, to a group of variables. </st><st c="10976">Some mathematical operations, however, such as subtraction or division, are performed </st><em class="italic"><st c="11062">between</st></em><st c="11069"> features. </st><st c="11080">Th</st><a id="_idTextAnchor1019"/><st c="11082">ese operations are useful to derive ratios, such as the </st><em class="italic"><st c="11139">debt-to-income ratio</st></em><st c="11159">:</st></p>
			<p><em class="italic"><st c="11161">debt-to-income ratio = total debt / </st></em><em class="italic"><st c="11197">total income</st></em></p>
			<p><st c="11209">These operations </st><a id="_idIndexMarker577"/><st c="11227">are also useful to compute differences, such as the </st><em class="italic"><st c="11279">disposable income</st></em><st c="11296">:</st></p>
			<p><em class="italic"><st c="11298">disposable income = income - </st></em><em class="italic"><st c="11327">total debt</st></em></p>
			<p><st c="11337">In this recipe, we</st><a id="_idIndexMarker578"/><st c="11356"> will learn how to create new features by subtracting or dividing variables with </st><code><st c="11437">pandas</st></code> <st c="11443">and </st><code><st c="11448">feature-engine</st></code><st c="11462">.</st></p>
			<p class="callout-heading"><st c="11463">Note</st></p>
			<p class="callout"><st c="11468">In the recipe, we will show you how to create features with subtraction and division. </st><st c="11555">We hope that the examples, relating to the financial sector, shed some light on how to use domain knowledge to decide which features to </st><a id="_idTextAnchor1020"/><a id="_idTextAnchor1021"/><st c="11691">combine </st><st c="11699">and how.</st></p>
			<h2 id="_idParaDest-228"><st c="11707">How to do</st><a id="_idTextAnchor1022"/><st c="11717"> it…</st></h2>
			<p><st c="11721">Let’s begin by loading the necessary Python libraries and the breast cancer dataset </st><st c="11806">from scikit-learn:</st></p>
			<ol>
				<li><st c="11824">Load the necessary libraries, classes, </st><st c="11864">and data:</st><pre class="source-code"><st c="11873">
import pandas as pd
from feature_engine.creation import RelativeFeatures
from sklearn.datasets import load_breast_cancer</st></pre></li>				<li><st c="11994">Load the breast cancer dataset into a </st><code><st c="12033">pandas</st></code><st c="12039"> DataFrame:</st><pre class="source-code"><st c="12050">
data = load_breast_cancer()
df = pd.DataFrame(data.data,
    columns=data.feature_names)</st></pre><p class="list-inset"><st c="12135">In the breast cancer dataset, some features capture the worst and the mean characteristics of the cell nuclei of breast cells. </st><st c="12263">For example, for each image (that is, for each row), we have the worst compactness observed in all nuclei and the mean compactness of all nuclei. </st><st c="12409">A </st><a id="_idIndexMarker579"/><st c="12411">feature that captures the difference between the worst and the mean value</st><a id="_idIndexMarker580"/><st c="12484"> could </st><st c="12491">predict malignancy.</st><a id="_idTextAnchor1023"/></p></li>				<li><st c="12510">Let’s capture the difference between two features, the </st><code><st c="12566">worst compactness</st></code><st c="12583"> and </st><code><st c="12588">mean compactness</st></code><st c="12604"> of cel</st><a id="_idTextAnchor1024"/><st c="12611">l nuclei, in a new variable and display </st><st c="12652">its values:</st><pre class="source-code"><st c="12663">
df["difference"] = df["worst compactness"].sub(
    df["mean compactness"])
df["differen</st><a id="_idTextAnchor1025"/><st c="12748">ce"].head()</st></pre><p class="list-inset"><st c="12760">In the following output, we can see the difference between these </st><st c="12826">feature values:</st></p><pre class="source-code"><strong class="bold"><st c="12841">0    0.38800</st></strong>
<strong class="bold"><st c="12851">1    0.10796</st></strong>
<strong class="bold"><st c="12861">2    0.26460</st></strong>
<strong class="bold"><st c="12871">3    0.58240</st></strong>
<strong class="bold"><st c="12881">4    0.07220</st></strong>
<strong class="bold"><st c="12891">Name: difference, dtype: float64</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="12924">Note</st></p>
			<p class="callout"><st c="12929">We can perform the same calculation by executing </st><code><st c="12979">df["difference"] = df["worst compactness"] - (</st></code><code><st c="13025">df["mean compactness"])</st></code><st c="13049">.</st></p>
			<p class="list-inset"><st c="13050">Similarly, the ratio between the worst and the average characteristic of the cell nuclei might be indicative </st><st c="13160">of malignancy.</st></p>
			<ol>
				<li value="4"><st c="13174">Let’s create a new feature with the ratio between the worst and mean radius of the nuclei, and then display </st><st c="13283">its values:</st><pre class="source-code"><st c="13294">
df["quotient"] = df["worst radius"].div(
    df["mean radius"])
df["quotient"].head()</st></pre><p class="list-inset"><st c="13376">In the </st><a id="_idIndexMarker581"/><st c="13384">following output, we can see the </st><a id="_idIndexMarker582"/><st c="13417">values corresponding to the ratio between </st><st c="13459">the features:</st></p><pre class="source-code"><strong class="bold"><st c="13472">0    1.410784</st></strong>
<strong class="bold"><st c="13483">1    1.214876</st></strong>
<strong class="bold"><st c="13494">2    1.197054</st></strong>
<strong class="bold"><st c="13505">3    1.305604</st></strong>
<strong class="bold"><st c="13516">4    1.110892</st></strong>
<strong class="bold"><st c="13527">Name: quotient, dtype: float64</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="13558">Note</st></p>
			<p class="callout"><st c="13563">We can calculate the ratio by executing an alternative command, </st><code><st c="13628">df["quotient"] = df["worst radius"] / (</st></code><code><st c="13667">df["me</st><a id="_idTextAnchor1026"/><st c="13674">an radius"])</st></code><st c="13687">.</st></p>
			<p class="list-inset"><st c="13688">We can also capture the ratio and difference between every nuclei morphology characteristic and the mean radius or mean area of the nuclei. </st><st c="13829">Let’s begin by capturing these subsets of variables </st><st c="13881">into lists.</st></p>
			<ol>
				<li value="5"><st c="13892">Let’s make a list of the features in </st><st c="13930">the numerator:</st><pre class="source-code"><st c="13944">
features = [
    "mean smoothness",
    «mean compactness",
    "mean concavity",
    "mean symmetry"
]</st></pre></li>				<li><st c="14032">Let’s make a list of the features in </st><st c="14070">the denominator:</st><pre class="source-code"><st c="14086">
reference = ["mean radius", "mean area"]</st></pre></li>			</ol>
			<p class="callout-heading"><st c="14127">Note</st></p>
			<p class="callout"><st c="14132">We can create features by dividing the features in </st><em class="italic"><st c="14184">step 5</st></em><st c="14190"> by one of the features in </st><em class="italic"><st c="14217">step 6</st></em><st c="14223"> with </st><code><st c="14229">pandas</st></code><st c="14235">, by executing </st><code><st c="14250">df[features].div(df["mean radius"])</st></code><st c="14285">. For subtraction, we’d execute </st><code><st c="14317">df[features].sub(df["mean radius"])</st></code><st c="14352">.</st></p>
			<ol>
				<li value="7"><st c="14353">Let’s set </st><a id="_idIndexMarker583"/><st c="14364">up the </st><code><st c="14371">feature-engine</st></code><st c="14385"> library’s </st><code><st c="14396">RelativeFeatures()</st></code><st c="14414"> so that it subtracts or divides every feature </st><a id="_idIndexMarker584"/><st c="14461">from </st><em class="italic"><st c="14466">step 5</st></em><st c="14472"> with respect to the features from </st><em class="italic"><st c="14507">step 6</st></em><st c="14513">:</st><pre class="source-code"><st c="14515">
creator = RelativeFeatures(
    variables=features,
    reference=reference,
    func=["sub", "div"],
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="14607">Note</st></p>
			<p class="callout"><st c="14612">Subtracting the features from </st><em class="italic"><st c="14643">step 5</st></em><st c="14649"> and </st><em class="italic"><st c="14654">step 6</st></em><st c="14660"> does not make biological sense, but we will do it anyway to demonstrate the use of the </st><code><st c="14748">RelativeFeatures()</st></code> <a id="_idTextAnchor1027"/><st c="14766">transformer.</st></p>
			<ol>
				<li value="8"><st c="14779">Let’s add the new features to the DataFrame and capture the result in a </st><st c="14852">new variable:</st><pre class="source-code"><st c="14865">
df_t = creator.fit_transform(df)</st></pre></li>				<li><st c="14898">Let’s capture the names of the new features in </st><st c="14946">a list:</st><pre class="source-code"><st c="14953">
all_feat = creator.feature_names_in_
new_features = [
    f for f in df_t.columns if f not in all_feat]</st></pre></li>			</ol>
			<p class="callout-heading"><st c="15053">Note</st></p>
			<p class="callout"><code><st c="15058">feature_names_in_</st><a id="_idTextAnchor1028"/></code><st c="15076"> is a </st><a id="_idIndexMarker585"/><st c="15082">common attribute in </st><code><st c="15102">scikit-learn</st></code><st c="15114"> and </st><code><st c="15119">feature-engine</st></code><st c="15133"> transformers and stores the name of the variables from the DataFrame used to fit the transformer. </st><st c="15232">In other words, it stores the names of the input features. </st><st c="15291">When using </st><code><st c="15302">transform()</st></code><st c="15313">, the transformers check that the features from the new input dataset match those used during training. </st><st c="15417">In </st><em class="italic"><st c="15420">step 9</st></em><st c="15426">, we leverage this attribute to find the additional variables added to the data after </st><st c="15512">the transformation.</st></p>
			<p><st c="15531">If we</st><a id="_idIndexMarker586"/><st c="15537"> execute </st><code><st c="15546">print(new_features)</st></code><st c="15565">, we</st><a id="_idIndexMarker587"/><st c="15569"> will see a list with the names of the features created by </st><code><st c="15628">ReferenceFeatures()</st></code><st c="15647">. Note </st><a id="_idTextAnchor1029"/><st c="15654">that the features contain the variables on the left- and right-hand sides of the mathematical equation, plus the function that was applied to them to create the </st><st c="15815">new feature:</st></p>
			<pre class="console"><st c="15827">
['mean smoothness_sub_mean radius',
'mean compactness_sub_mean radius',
'mean concavity_sub_mean radius',
'mean symmetry_sub_mean radius',
'mean smoothness_sub_mean area',
'mean compactness_sub_mean area',
'mean concavity_sub_mean area',
'mean symmetry_sub_mean area',
'mean smoothness_div_mean radius',
'mean compactness_div_mean radius',
'mean concavity_div_mean radius',
'mean symmetry_div_mean radius',
'mean smoothness_div_mean area',
'mean compactness_div_mean area',
'mean concavity_div_mean area',
'mean symmetry_div_mean area']</st></pre>			<p><st c="16364">Finally, we can display the first five rows of the resulting variables by </st><st c="16439">executing </st><code><st c="16449">df</st><a id="_idTextAnchor1030"/><st c="16451">_t[new_features].head()</st></code><st c="16475">:</st></p>
			<div><div><img src="img/B22396_08_3.jpg" alt="Figure 8.3 – A DataFrame with the newly created features"/><st c="16477"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="16983">Figure 8.3 – A DataFrame with the newly created features</st></p>
			<p><code><st c="17039">feature-engine</st></code><st c="17054"> adds </st><a id="_idIndexMarker588"/><st c="17060">new features as columns</st><a id="_idIndexMarker589"/><st c="17083"> at the right of the original DataFrame and automatically adds variable names to those features. </st><st c="17180">By doing so, </st><code><st c="17193">feature-engine</st></code><st c="17207"> automates much of the manual work tha</st><a id="_idTextAnchor1031"/><a id="_idTextAnchor1032"/><st c="17245">t we would do </st><st c="17260">with </st><code><st c="17265">pandas</st></code><st c="17271">.</st></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor1033"/><st c="17272">How it works...</st></h2>
			<p><code><st c="17288">pandas</st></code><st c="17295"> has many built-in operations to compare a feature or a group of features to a reference variable. </st><st c="17394">In this recipe, we used pandas </st><code><st c="17425">sub()</st></code><st c="17430"> and </st><code><st c="17435">div()</st></code><st c="17440"> to determine the difference or the ratio between two variables, or a subset of variables and one </st><st c="17538">refe</st><a id="_idTextAnchor1034"/><st c="17542">rence feature.</st></p>
			<p><st c="17557">To subtract one variable from another, we applied </st><code><st c="17608">sub()</st></code><st c="17613"> to a </st><code><st c="17619">pandas</st></code><st c="17625"> series with the first variable, passing the </st><code><st c="17670">pandas</st></code><st c="17676"> series with the second variable as an argument to </st><code><st c="17727">sub()</st></code><st c="17732">. This operation returned a third </st><code><st c="17766">pandas</st></code><st c="17772"> series with the difference between the first and second variables. </st><st c="17840">To divide one variable from another, we used </st><code><st c="17885">div()</st></code><st c="17890">, which works identically to </st><code><st c="17919">sub()</st></code><st c="17924"> – that is, it divides the variable on the left by the variable passed as an argument </st><st c="18010">of </st><code><st c="18013">div()</st></code><st c="18018">.</st></p>
			<p><st c="18019">Then, we combined several variables with two reference variables automatically via subtraction or division, by utilizing </st><code><st c="18141">ReferenceFeatures()</st></code><st c="18160"> from </st><code><st c="18166">Feature-engine</st></code><st c="18180">. The </st><code><st c="18186">ReferenceFeatures()</st></code><st c="18205"> transformer takes the variables to be combined, the reference variables, and the functions to use to combine them. </st><st c="18321">When using </st><code><st c="18332">fit()</st></code><st c="18337">, the transformer did not learn about parameters but checked that the variables were numerical. </st><st c="18433">Executing </st><code><st c="18443">transform()</st></code><st c="18454"> added the new features to </st><st c="18481">the DataFrame.</st></p>
			<p class="callout-heading"><st c="18495">Note</st></p>
			<p class="callout"><code><st c="18500">ReferenceFeatures()</st></code><st c="18520"> can also add, multiply, get the modulo, or get the power of a group of variables relating to a second group of reference variables. </st><st c="18653">You can find out more in its </st><st c="18682">documentation: </st><a href="https://feature-engine.readthedocs.io/en/latest/api_doc/creation/RelativeFeatures.html"><st c="18697">https://feature-engine.readthedocs.io/en/latest/api_doc/creation/RelativeFeatures.html</st></a><st c="18783">.</st></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor1035"/><st c="18784">See also</st></h2>
			<p><st c="18793">To learn more about the binary operations supported by </st><code><st c="18849">pandas</st></code><st c="18855">, </st><st c="18857">visit </st><a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions"><st c="18863">https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions</st></a><st c="18954">.</st></p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor1036"/><st c="18955">Performing polynomial expansion</st></h1>
			<p><st c="18987">Simple </st><a id="_idIndexMarker590"/><st c="18995">models, such as linear and logistic regression, can capture complex patterns if we feed them the right features. </st><st c="19108">Sometimes, we can create powerful features by combining the variables in our datasets with themselves or with other variables. </st><st c="19235">For example, in the following figure, we can see that the target, </st><em class="italic"><st c="19301">y</st></em><st c="19302">, has a quadratic relation with the variable, </st><em class="italic"><st c="19348">x</st></em><st c="19349">, and as shown in the left panel, a linear model is not able to capture that </st><st c="19426">relationship accurately:</st></p>
			<div><div><img src="img/B22396_08_4.jpg" alt="Figure 8.4 – A linear model fit to predict a target, y, from a feature, x, which has a quadratic relationship to the target, before and after squaring x. In the left panel: the model offers a poor fit by using the original variable; in the right panel, the model offers a better fit, based on the squar﻿e of the original variable"/><st c="19450"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="19506">Figure 8.4 – A linear model fit to predict a target, y, from a feature, x, which has a quadratic relationship to the target, before and after squaring x. </st><st c="19660">In the left panel: the model offers a poor fit by using the original variable; in the right panel, the model offers a better fit, based on the squar</st><a id="_idTextAnchor1037"/><st c="19808">e of the original variable</st></p>
			<p><st c="19835">This linear </st><a id="_idIndexMarker591"/><st c="19848">model has a quadratic relationship to the target, before and after squaring </st><em class="italic"><st c="19924">x</st></em><st c="19925">. However, if we square </st><em class="italic"><st c="19949">x</st></em><st c="19950">, or, in other words, if we create a second-degree polynomial of the feature, the linear model can accurately predict the target, </st><em class="italic"><st c="20080">y</st></em><st c="20081">, from the square of </st><em class="italic"><st c="20102">x</st></em><st c="20103">, as we see in the </st><st c="20122">right panel.</st></p>
			<p><st c="20134">Another classical example in which a simple feature can make a simple model, such as logistic regression, understand the underlying relationship in the data is the </st><strong class="bold"><st c="20299">XOR</st></strong><st c="20302"> situation. </st><st c="20314">In</st><a id="_idIndexMarker592"/><st c="20316"> the left panel of the following diagram, we see how the target class is distributed across the values of </st><em class="italic"><st c="20422">x1</st></em><st c="20424"> and </st><em class="italic"><st c="20429">x2</st></em><st c="20431"> (the class is highlighted with different </st><st c="20473">color shades):</st></p>
			<div><div><img src="img/B22396_08_5.jpg" alt="Figure 8.5 – An illustration of the XOR relationship and how combining features allows a full class separation"/><st c="20487"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="20592">Figure 8.5 – An illustration of the XOR relationship and how combining features allows a full class separation</st></p>
			<p><st c="20702">If both features are positive, or both features are negative, then the class is 1, but if the features take different signs, then the class is 0 (left panel). </st><st c="20862">Logistic regression will not be able to pick this pattern from each individual feature because, as we can see in the middle panel, there</st><a id="_idIndexMarker593"/><st c="20998"> is significant class overlap across the values of the feature – in this case, x1. </st><st c="21081">However, multiplying x1 by x2 creates a feature that allows a logistic regression to predict the classes accurately because x3, as can we see in the right panel, allows the classes to be </st><st c="21268">clearly separated.</st></p>
			<p><st c="21286">With similar logic, polynomial combinations of the same or different variables can return new variables that convey additional information and capture feature interaction thereby resulting in useful inputs for linear models. </st><st c="21512">With huge datasets, analyzing every possible variable combination is not always possible. </st><st c="21602">But we can create several polynomial variables automatically, using, for example, </st><code><st c="21684">scikit-learn</st></code><st c="21696">, and we can let the model decide which variables are useful. </st><st c="21758">In this recipe, we will learn how to create multiple features through polynomial comb</st><a id="_idTextAnchor1038"/><a id="_idTextAnchor1039"/><st c="21843">inations </st><st c="21853">using scikit-learn.</st></p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor1040"/><st c="21872">Getting ready</st></h2>
			<p><st c="21886">Polynomial expansion serves to automate the creation of new features, capture feature interaction, and potential non-linear relationships between the original variables and the target. </st><st c="22072">To create polynomial features, we need to determine which features to combine and which polynomial degree </st><st c="22178">to use.</st></p>
			<p class="callout-heading"><st c="22185">Note</st></p>
			<p class="callout"><st c="22190">While determining the features to combine or the degree of the polynomial combination is not an easy task, keep in mind that high polynomial degrees will result in a lot of new features and may lead to overfitting. </st><st c="22406">In general, we keep the degree low, to a maximum of 2 </st><st c="22460">or 3.</st></p>
			<p><st c="22465">The </st><code><st c="22470">PolynomialFeatures()</st></code><st c="22490"> transformer from </st><code><st c="22508">scikit-learn</st></code><st c="22520"> creates polynomial combinations of the features with a degree less than or equal to a user-specified </st><st c="22622">degree, automatically.</st></p>
			<p><st c="22644">To follow up easily with this recipe, let’s first understand the output of </st><code><st c="22720">PolynomialFeatures()</st></code><st c="22740"> when used to create second- and third-degree polynomial combinations of </st><st c="22813">th</st><a id="_idTextAnchor1041"/><st c="22815">ree variables.</st></p>
			<p><st c="22830">Second-degree polynomial combinations of three variables – </st><em class="italic"><st c="22890">a</st></em><st c="22891">, </st><em class="italic"><st c="22893">b</st></em><st c="22894">, and </st><em class="italic"><st c="22900">c</st></em><st c="22901"> – will return the following </st><st c="22930">new features:</st></p>
			<p><em class="italic"><st c="22943">1, a, b, c, ab, ac, bc, a2, </st></em><em class="italic"><st c="22972">b2, c2</st></em></p>
			<p><st c="22978">From</st><a id="_idIndexMarker594"/><st c="22983"> the previous features, </st><em class="italic"><st c="23007">a</st></em><st c="23008">, </st><em class="italic"><st c="23010">b</st></em><st c="23011">, and </st><em class="italic"><st c="23017">c</st></em><st c="23018"> are the original variables; </st><em class="italic"><st c="23047">ab</st></em><st c="23049">, </st><em class="italic"><st c="23051">ac</st></em><st c="23053">, and </st><em class="italic"><st c="23059">bc</st></em><st c="23061"> are the products of those features; and </st><em class="italic"><st c="23102">a2</st></em><st c="23104">, </st><em class="italic"><st c="23106">b2</st></em><st c="23108">, and </st><em class="italic"><st c="23114">c2</st></em><st c="23116"> are the squared values of the original features. </st><code><st c="23166">PolynomialFeatures()</st></code><st c="23186"> also returns the bias term </st><em class="italic"><st c="23214">1</st></em><st c="23215">, which we would probably exclude when </st><st c="23254">creating features.</st></p>
			<p class="callout-heading"><st c="23272">Note</st></p>
			<p class="callout"><st c="23277">The resulting features – </st><em class="italic"><st c="23303">ab</st></em><st c="23305">, </st><em class="italic"><st c="23307">ac</st></em><st c="23309">, and </st><em class="italic"><st c="23315">bc</st></em><st c="23317"> – are </st><a id="_idIndexMarker595"/><st c="23324">called </st><strong class="bold"><st c="23331">interactions</st></strong><st c="23343"> or feature interactions of </st><strong class="bold"><st c="23371">degree 2</st></strong><st c="23379">. The degree </st><a id="_idIndexMarker596"/><st c="23392">reflects the number of variables combined. </st><st c="23435">The result combines a maximum of two variables because we indicated a second-degree polynomial as the maximum </st><st c="23545">allowed combination.</st></p>
			<p><st c="23565">Third-degree polynomial combinations of the three variables – </st><em class="italic"><st c="23628">a</st></em><st c="23629">, </st><em class="italic"><st c="23631">b</st></em><st c="23632">, and </st><em class="italic"><st c="23638">c</st></em><st c="23639"> – will return the following </st><st c="23668">new features:</st></p>
			<p><em class="italic"><st c="23681">1, a, b, c, ab, ac, bc, abc, a2b, a2c, b2a, b2c, c2a, c2b, a3, </st></em><em class="italic"><st c="23745">b3, c3</st></em></p>
			<p><st c="23751">Among the returned features, in addition to those returned by the second-degree polynomial combination, we now have the third-degree combinations of the features with themselves (</st><em class="italic"><st c="23931">a3</st></em><st c="23934">, </st><em class="italic"><st c="23936">b3</st></em><st c="23938">, and </st><em class="italic"><st c="23944">c3</st></em><st c="23946">), the squared values of every feature combined linearly with a second feature (</st><em class="italic"><st c="24027">a2b</st></em><st c="24031">, </st><em class="italic"><st c="24033">a2c</st></em><st c="24036">, </st><em class="italic"><st c="24038">b2a</st></em><st c="24041">, </st><em class="italic"><st c="24043">b2c</st></em><st c="24046">, </st><em class="italic"><st c="24048">c2a</st></em><st c="24051">, and </st><em class="italic"><st c="24057">c2b</st></em><st c="24060">), and the product of the three features (</st><em class="italic"><st c="24103">abc</st></em><st c="24107">). </st><st c="24111">Note how we have all possible interactions of degrees 1, 2, and 3 and the bias </st><st c="24190">term</st> <em class="italic"><st c="24194">1</st></em><st c="24196">.</st></p>
			<p><st c="24197">Now that we understand the output of the polynomial expansion implemented by </st><code><st c="24275">scikit-lea</st><a id="_idTextAnchor1042"/><a id="_idTextAnchor1043"/><st c="24285">rn</st></code><st c="24288">, let’s jump into </st><st c="24306">the recipe.</st></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor1044"/><st c="24317">How to do it...</st></h2>
			<p><st c="24333">In this </st><a id="_idIndexMarker597"/><st c="24342">recipe, we will create features with polynomial expansion using a toy dataset to become familiar with the resulting variables. </st><st c="24469">Creating features with the polynomial expansion of a real dataset is identical to what we will discuss in </st><st c="24575">this recipe:</st></p>
			<ol>
				<li><st c="24587">Let’s import the required libraries, classes, </st><st c="24634">and data:</st><pre class="source-code"><st c="24643">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import set_config
from sklearn.preprocessing import PolynomialFeatures</st></pre></li>				<li><st c="24798">Let’s set </st><code><st c="24809">scikit-learn</st></code><st c="24821"> library’s </st><code><st c="24832">set_output</st></code><st c="24842"> API globally so that all transformers return a DataFrame as a result of the </st><code><st c="24919">transform()</st></code><st c="24930"> method:</st><pre class="source-code"><st c="24938">
set_config(transform_output="pandas")</st></pre></li>				<li><st c="24976">Let’s </st><a id="_idIndexMarker598"/><st c="24983">create a DataFrame containing one variable, with values from 1 </st><st c="25046">to 10:</st><pre class="source-code"><st c="25052">
df = pd.DataFrame(np.linspace(
    0, 10, 11), columns=["var"])</st></pre></li>				<li><st c="25112">Let’s set up </st><code><st c="25126">PolynomialFeatures()</st></code><st c="25146"> to create all possible combinations up to a third-degree polynomial of the single variable and exclude the bias term from the result – that is, we will exclude the </st><st c="25311">value </st><em class="italic"><st c="25317">1</st></em><st c="25318">:</st><pre class="source-code"><st c="25319">
poly = PolynomialFeatures(
    degree=3,
    interaction_only=False,
    include_bias=False)</st></pre></li>				<li><st c="25400">Now, let’s create the </st><st c="25423">polynomial combinations:</st><pre class="source-code"><st c="25447">
dft = poly.fit_transform(df)</st></pre><p class="list-inset"><st c="25476">If we execute </st><code><st c="25491">dft</st></code><st c="25494">, we’ll </st><a id="_idTextAnchor1045"/><st c="25502">see a DataFrame with the original feature, followed by its values squared, and then its values to the power </st><st c="25610">of three:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_6.jpg" alt="Figure 8.6 – A DataFrame with the polynomial expansion of the third degree of a single variable"/><st c="25619"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="25812">Figure 8.6 – A DataFrame with the polynomial expansion of the third degree of a single variable</st></p>
			<p class="list-inset"><st c="25907">If, instead </st><a id="_idIndexMarker599"/><st c="25920">of returning a DataFrame, </st><code><st c="25946">PolynomialFeatures()</st></code><st c="25966"> returns a NumPy array and you want to obtain the names of the features in the array, you can do so by executing </st><code><st c="26079">poly.get_feature_names_out()</st></code><st c="26107">, which returns </st><code><st c="26123">array(['var', 'var^2', '</st></code><code><st c="26147">var^3'], dtype=object)</st></code><st c="26170">.</st></p>
			<ol>
				<li value="6"><st c="26171">Now, let’s plot the new feature values against the </st><st c="26223">original variable:</st><pre class="source-code"><st c="26241">
dft = pd.DataFrame(
    dft, columns=poly.get_feature_names_out())
plt.plot(df["var"], dft)
plt.legend(dft.columns)
plt.xlabel("original variable")
plt.ylabel("new variables")
plt.show()</st><a id="_idTextAnchor1046"/></pre><p class="list-inset"><st c="26424">In the following diagram, we can see the relationship between the polynomial fea</st><a id="_idTextAnchor1047"/><st c="26505">tures and the </st><st c="26520">original variable:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_7.jpg" alt="Figure 8.7 – The relationship between the features resulting from polynomial expansion and the original variable"/><st c="26538"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="26645">Figure 8.7 – The relationship between the features resulting from polynomial expansion and the original variable</st></p>
			<ol>
				<li value="7"><st c="26757">Let’s add </st><a id="_idIndexMarker600"/><st c="26768">two additional variables to our toy dataset, with values from 1 </st><st c="26832">to 10:</st><pre class="source-code"><st c="26838">
df["col"] = np.linspace(0, 5, 11)
df["feat"] = np.linspace(0, 5, 11)</st></pre></li>				<li><st c="26907">Next, let’s combine the three features in the </st><a id="_idTextAnchor1048"/><st c="26954">dataset with polynomial expansion up to the second degree, but this time, we will only return features produced by combining at least two different variables – that is, the </st><st c="27127">interaction features:</st><pre class="source-code"><st c="27148">
poly = PolynomialFeatures(
    degree=2, interaction_only=True,
    include_bias=False)
dft = poly.fit_transform(df)</st></pre><p class="list-inset"><st c="27257">If we execute </st><code><st c="27272">dft</st></code><st c="27275">, we will see the features resulting from the polynomial expansion, which contain the original features, plus all possible combinations of the three variables but without the quadratic terms, as we set the transformer to return only </st><a id="_idIndexMarker601"/><st c="27508">the interaction </st><st c="27524">between features:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_8.jpg" alt="Figure 8.8 – A DataFrame with the result of creating features with polynomial expansion but retaining only the interactio﻿n between variables"/><st c="27541"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27849">Figure 8.8 – A DataFrame with the result of creating features with polynomial expansion but retaining only the interactio</st><a id="_idTextAnchor1049"/><st c="27970">n between variables</st></p>
			<p class="callout-heading"><st c="27990">Note</st></p>
			<p class="callout"><st c="27995">Go ahead and create third-degree polynomial combinations of the features, returning only the interactions or all possible features to get a better sense of the output </st><st c="28163">of </st><code><st c="28166">PolynomialFeatures()</st></code><st c="28186">.</st></p>
			<p><st c="28187">With that, we’ve </st><a id="_idIndexMarker602"/><st c="28205">learned how to create new features by combining existing variables with themselves or other features in data. </st><st c="28315">Creating features via polynomial expansion using a real dataset is, in </st><st c="28386">essence, identical.</st></p>
			<p><st c="28405">If you want to combine only a subset of features, you can select the features to combine by utilizing </st><code><st c="28508">ColumnTransformer()</st></code><st c="28527">, as we will demonstrate in the </st><em class="italic"><st c="28559">There’s more…</st></em><st c="28572"> section ahead in this recipe, or by using </st><code><st c="28615">SklearnTransformerWrapper()</st></code><st c="28642"> from </st><code><st c="28648">feature-engine</st></code><st c="28662">, as you can see in the accompanying GitHub </st><st c="28706">repository: </st><a href="https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch08-creation/Recipe3-PolynomialExpansion.ipynb"><st c="28718">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch08-creation/</st><st c="28827">Recipe3-PolynomialExpansion.ipynb</st></a><st c="28861">.</st></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor1052"/><st c="28862">How it works...</st></h2>
			<p><st c="28878">In this recipe, we </st><a id="_idIndexMarker603"/><st c="28898">created features by using polynomial combinations of a feature with itself or among three variables. </st><st c="28999">To create these polynomial features, we used </st><code><st c="29044">PolynomialFeatures()</st></code><st c="29064"> from </st><code><st c="29070">scikit-learn</st></code><st c="29082">. By default, </st><code><st c="29096">Polynomia</st><a id="_idTextAnchor1053"/><st c="29105">lFeatures()</st></code><st c="29117"> generates a new feature matrix consisting of all polynomial combinations of the features in the data, with a degree less than or equal to the user-specified </st><code><st c="29275">degree</st></code><st c="29281">. By setting </st><code><st c="29294">degree</st></code><st c="29300"> to </st><code><st c="29304">3</st></code><st c="29305">, we created all possible polynomial combinations of a degree of 3 or smaller. </st><st c="29384">To retain the combination of a feature with itself, we set the </st><code><st c="29447">interaction_only</st></code><st c="29463"> parameter to </st><code><st c="29477">False</st></code><st c="29482">. To avoid returning the bias term, we set the </st><code><st c="29529">include_bias</st></code><st c="29541"> parameter </st><st c="29552">to </st><code><st c="29555">False</st></code><st c="29560">.</st></p>
			<p class="callout-heading"><st c="29561">Note</st></p>
			<p class="callout"><st c="29566">Setting the </st><code><st c="29579">interaction_only</st></code><st c="29595"> parameter to </st><code><st c="29609">True</st></code><st c="29613"> returns the interaction terms only – that is, the variables resulting from combinations of two or </st><st c="29712">more variables.</st></p>
			<p><st c="29727">The </st><code><st c="29732">fit()</st></code><st c="29737"> method determined all of the possible feature combinations based on the parameters specified. </st><st c="29832">At this stage, the transformer did not perform actual mathematical computations. </st><st c="29913">The </st><code><st c="29917">transform()</st></code><st c="29928"> method performed the mathematical computations with the features to create the new variables. </st><st c="30023">With the </st><code><st c="30032">get_feature_names()</st></code><st c="30051"> method, we could identify the terms of the expansion – that is, how each new feature </st><st c="30137">was calculated.</st></p>
			<p><st c="30152">In </st><em class="italic"><st c="30156">step 2</st></em><st c="30162">, we set </st><code><st c="30171">scikit-learn</st></code><st c="30183"> library’s </st><code><st c="30194">set_output</st></code><st c="30204"> API </st><code><st c="30228">pandas</st></code><st c="30234"> DataFrames as a result of the </st><code><st c="30265">transform()</st></code><st c="30276"> method. </st><st c="30285">scikit-learn transformers return </st><code><st c="30318">NumPy</st></code><st c="30323"> arrays by default. </st><st c="30343">The new </st><code><st c="30351">set_output</st></code><st c="30361"> API allows us to change the container of the result to a </st><code><st c="30419">pandas</st></code><st c="30425"> or a </st><code><st c="30431">polars</st></code><st c="30437"> DataFrame. </st><st c="30449">We can set the output individually every time we set up a transformer – for example, by using </st><code><st c="30543">poly = PolynomialFeatures().set_output(transform="pandas")</st></code><st c="30601">. Alternatively, as we did in this recipe, we can set the global configuration, and then every time we set up a new transformer,</st><a id="_idTextAnchor1054"/><a id="_idTextAnchor1055"/><st c="30729"> it will return a </st><code><st c="30747">pandas</st></code><st c="30753"> DataFrame.</st></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor1056"/><st c="30764">There’s more...</st></h2>
			<p><st c="30780">Let’s create features by</st><a id="_idIndexMarker604"/><st c="30805"> performing polynomial expansion on a subset of variables in the breast </st><st c="30877">cancer dataset:</st></p>
			<ol>
				<li><st c="30892">First, import the necessary libraries, classes, </st><st c="30941">and data:</st><pre class="source-code"><st c="30950">
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures</st></pre></li>				<li><st c="31170">Then, load the data and separate it into train and </st><st c="31222">test sets:</st><pre class="source-code"><st c="31232">
data = load_breast_cancer()
df = pd.DataFrame(data.data,
    columns=data.feature_names)
X_train, X_test, y_train, y_test = train_test_split(
    df, data.target, </st><a id="_idTextAnchor1057"/><st c="31388">test_size=0.3, random_state=0
)</st></pre></li>				<li><st c="31419">Make a list with the features </st><st c="31450">to combine:</st><pre class="source-code"><st c="31461">
features = [
    "mean smoothness",
    "mean compactness",
    "mean concavity"]</st></pre></li>				<li><st c="31531">Set up </st><code><st c="31539">PolynomialFeatures()</st></code><st c="31559"> to create all possible combinations up to the </st><st c="31606">third degree:</st><pre class="source-code"><st c="31619">
poly = PolynomialFeatures(
    degree=3,
    interaction_only=False,
    include_bias=False)</st></pre></li>				<li><st c="31700">Set up the column transformer to create features only from those specified in </st><em class="italic"><st c="31779">step 3</st></em><st c="31785">:</st><pre class="source-code"><st c="31787">
ct = ColumnTransformer([("poly", poly, features)])</st></pre></li>				<li><st c="31838">Create the </st><st c="31850">polynomial features:</st><pre class="source-code"><st c="31870">
train_t = ct.fit_transform(X_train)
test_t = ct.transform(X_test)</st></pre></li>			</ol>
			<p><st c="31936">And that’s it. </st><st c="31952">By </st><a id="_idIndexMarker605"/><st c="31955">executing </st><code><st c="31965">ct.get_featur</st><a id="_idTextAnchor1058"/><st c="31978">e_names_out()</st></code><st c="31992">, we obtain the names of the </st><st c="32021">new features.</st></p>
			<p class="callout-heading"><st c="32034">Note</st></p>
			<p class="callout"><code><st c="32039">ColumnTransformer()</st></code><st c="32059"> will append the word </st><code><st c="32081">poly</st></code><st c="32085"> to the resulting variables, which is the name we gave to the step within </st><code><st c="32159">ColumnTransformer()</st></code><st c="32178"> in </st><em class="italic"><st c="32182">step 5</st></em><st c="32188">. I am not a huge fan of this behavior because it makes data analysis harder, as you need to keep track of the variable name changes. </st><st c="32322">To avoid variable name changes, you can use </st><code><st c="32366">feature-engine</st></code><st c="32380">’s</st><a id="_idTextAnchor1059"/><a id="_idTextAnchor1060"/><a id="_idTextAnchor1061"/> <code><st c="32383">SklearnTransformerWrapper()</st></code><st c="32411"> instead.</st></p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor1062"/><st c="32420">Combining features with decision trees</st></h1>
			<p><st c="32459">In the</st><a id="_idIndexMarker606"/><st c="32466"> winning solution of the </st><strong class="bold"><st c="32491">Knowledge Discovery and Data</st></strong><st c="32519"> Mining (</st><strong class="bold"><st c="32528">KDD</st></strong><st c="32532">) competition in 200</st><a id="_idTextAnchor1063"/><st c="32553">9, the authors created new features by combining two or more variables using decision trees. </st><st c="32647">When examining the</st><a id="_idIndexMarker607"/><st c="32665"> variables, they noticed that some features had a high level of mutual information with the target yet low correlation, indicating that the relationship with the target was not linear. </st><st c="32850">While these features </st><a id="_idIndexMarker608"/><st c="32871">were predictive when used in tree-based algorithms, linear models could not take advantage of them. </st><st c="32971">Hence, to use these features in linear models, they replaced the features with the outputs of decision trees trained on the individual features, or combinations of two or three variables, to return new features with a monotonic relationship with </st><st c="33217">the target.</st></p>
			<p><st c="33228">In short, combining features with decision trees is useful for creating features that show a monotonic relationship with the target, which is useful for making accurate predictions</st><a id="_idIndexMarker609"/><st c="33409"> using linear models. </st><st c="33431">The procedure consists of training a decision tree using a subset of the </st><a id="_idIndexMarker610"/><st c="33504">features – typically, one, two, or three at a time – and then using the prediction of the tree as a </st><st c="33604">new feature.</st></p>
			<p class="callout-heading"><st c="33616">Note</st></p>
			<p class="callout"><st c="33621">You can find more details about this procedure and the overall winning solution of the 2009 KDD data competition in this </st><st c="33743">article: </st><a href="http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf"><st c="33752">http://proceedings.mlr.press/v7/niculescu09/niculescu09.pdf</st></a><st c="33811">.</st></p>
			<p><st c="33812">The good news is that we can automate the creation of features using trees, using </st><code><st c="33895">feature-engine</st></code><st c="33909">, and i</st><a id="_idTextAnchor1064"/><a id="_idTextAnchor1065"/><a id="_idTextAnchor1066"/><st c="33916">n this recipe, we will learn how to </st><st c="33953">do so.</st></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor1067"/><st c="33959">How to do it...</st></h2>
			<p><st c="33975">In this recipe, we’ll combine features with decision trees, using the California </st><st c="34057">housing dataset:</st></p>
			<ol>
				<li><st c="34073">Let’s begin by importing </st><code><st c="34099">pandas</st></code><st c="34105"> and the required functions, classes, </st><st c="34143">and dataset:</st><pre class="source-code"><st c="34155">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from </st><code><st c="34339">feature_engine.creation</st></code><st c="34362">, import DecisionTreeFeatures</st></pre></li>				<li><st c="34391">Let’s load the California housing dataset into a </st><code><st c="34441">pandas</st></code><st c="34447"> DataFrame and remove the </st><code><st c="34473">Latitude</st></code><st c="34481"> and </st><code><st c="34486">Longitude</st></code><st c="34495"> variables:</st><pre class="source-code"><st c="34506">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(labels=[
    "Latitude", "Longitude"], axis=1, inplace=True)</st></pre></li>				<li><st c="34635">Separate the dataset into train and </st><st c="34672">test sets:</st><pre class="source-code"><st c="34682">
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=0)</st></pre></li>				<li><st c="34772">Check out </st><a id="_idIndexMarker611"/><st c="34783">Pearson’s correlation coefficient </st><a id="_idIndexMarker612"/><st c="34817">between the features and the target, which is a measure of a </st><st c="34878">linear relationship:</st><pre class="source-code"><st c="34898">
for var in X_train.columns:
    pearson = np.corrcoef(X_train[var], y_train)[0, 1]
    pearson = np.round(pearson, 2)
    print(
        f"corr {var} vs target: {pearson}")</st></pre><p class="list-inset"><st c="35051">In the following output, we can see that, apart from </st><code><st c="35105">MedInc</st></code><st c="35111">, most variables do not show a strong linear relationship with the target; the correlation coefficient is smaller </st><st c="35225">than 0.5:</st></p><pre class="source-code"><strong class="bold"><st c="35234">corr MedInc vs target: 0.69</st></strong>
<strong class="bold"><st c="35262">corr HouseAge vs target: 0.1</st></strong>
<strong class="bold"><st c="35291">corr AveRooms vs target: 0.16</st></strong>
<strong class="bold"><st c="35321">corr AveBedrms vs target: -0.05</st></strong>
<strong class="bold"><st c="35353">corr Population vs target: -0.03</st></strong>
<code><st c="35422">feature-engine</st></code><st c="35436"> library’s </st><code><st c="35447">DecisionTreeFeatures()</st></code><st c="35469"> selects the best tree by </st><st c="35495">using cross-validation.</st></p></li>				<li><st c="35518">Create a grid of hyperparameters to optimize each </st><st c="35569">decision tree:</st><pre class="source-code"><st c="35583">
param_grid = {"max_depth": [2, 3, 4, None]}</st></pre><p class="list-inset"><st c="35627">The </st><code><st c="35632">feature-engine</st></code><st c="35646"> library’s </st><code><st c="35657">DecisionTreeFeatures()</st></code><st c="35679"> allows us to add features resulting from the predictions of a decision tree, trained on one or more features. </st><st c="35790">There</st><a id="_idIndexMarker613"/><st c="35795"> are many ways in which we can instruct the transformer to combine the features. </st><st c="35876">We’ll start by creating all possible combinations between </st><st c="35934">two variables.</st></p></li>				<li><st c="35948">Make a list</st><a id="_idIndexMarker614"/><st c="35960"> with the two features that we want to use </st><st c="36003">as inputs:</st><pre class="source-code"><st c="36013">
variables = ["AveRooms", "AveBedrms"]</st></pre></li>				<li><st c="36051">Set up </st><code><st c="36059">DecisionTreeFeatures()</st></code><st c="36081"> to create all possible combinations between the features from </st><em class="italic"><st c="36144">step 6</st></em><st c="36150">:</st><pre class="source-code"><st c="36152">
dtf = DecisionTreeFeatures(
    variables=variables,
    features_to_combine=None,
    cv=5,
    param_grid=param_grid,
    scoring="neg_mean_squared_error",
    regression=True,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="36309">Note</st></p>
			<p class="callout"><st c="36314">We set </st><code><st c="36322">regression</st></code><st c="36332"> to </st><code><st c="36336">True</st></code><st c="36340"> because the target in this dataset is continuous. </st><st c="36391">If you have a binary target or are performing classification, set it to </st><code><st c="36463">False</st></code><st c="36468">. Make sure to select an evaluation metric (</st><code><st c="36512">scoring</st></code><st c="36520">) that is suitable for </st><st c="36544">your model.</st></p>
			<ol>
				<li value="8"><st c="36555">Fit the transformer so that it trains the decision trees on the </st><st c="36620">input features:</st><pre class="source-code"><st c="36635">
dtf.fit(X_train, y_train)</st></pre></li>				<li><st c="36661">If you wonder </st><a id="_idIndexMarker615"/><st c="36676">which features have been used to train decision trees, you can inspect them </st><st c="36752">like this:</st><pre class="source-code"><st c="36762">
dtf.input_features_</st></pre><p class="list-inset"><st c="36782">In the</st><a id="_idIndexMarker616"/><st c="36789"> following output, we can see that </st><code><st c="36824">DecisionTreeFeatures()</st></code><st c="36846"> has trained three decision trees – two by using the single features, </st><code><st c="36916">AveRooms</st></code><st c="36924"> and </st><code><st c="36929">AveBedrms</st></code><st c="36938">, and one by using </st><st c="36957">both features:</st></p><pre class="source-code"><strong class="bold"><st c="36971">['AveRooms', 'AveBedrms', ['AveRooms', 'AveBedrms']]</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="37024">Note</st></p>
			<p class="callout"><code><st c="37029">DecisionTreeFeatures()</st></code><st c="37052"> also stores the decision trees. </st><st c="37085">You can check them out by executing  </st><code><st c="37121">dtf.estimators_</st></code><st c="37136">.</st></p>
			<ol>
				<li value="10"><st c="37137">Now, add the features to the training and </st><st c="37180">testing sets:</st><pre class="source-code"><st c="37193">
train_t = dtf.transform(X_train)
test_t = dtf.transform(X_test)</st></pre></li>				<li><st c="37257">Make a list with the name of the new features (the transformer appends the word </st><code><st c="37338">tree</st></code><st c="37342"> to the </st><st c="37350">feature names):</st><pre class="source-code"><st c="37365">
tree_features = [
    var for var in test_t.columns if "tree" in var ]</st></pre></li>				<li><st c="37432">Finally, display the features that were added to the </st><st c="37486">test set:</st><pre class="source-code"><st c="37495">
test_t[tree_features].head()</st></pre><p class="list-inset"><st c="37524">In the following output, we can see the first five rows of the new features, resulting from the decision trees trained in </st><em class="italic"><st c="37647">step 8</st></em><st c="37653">:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_9.jpg" alt="    Figure 8.9 – A portion of the testing set containing the features derived from the decision trees"/><st c="37655"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37883">    Figure 8.9 – A portion of the testing set containing the features derived from the decision trees</st></p>
			<ol>
				<li value="13"><st c="37980">To check the </st><a id="_idIndexMarker617"/><st c="37994">power of this transformation, calculate </st><a id="_idIndexMarker618"/><st c="38034">Pearson’s correlation coefficient between the new features and </st><st c="38097">the target:</st><pre class="source-code"><st c="38108">
for var in tree_features:
    pearson = np.corrcoef(test_t[var], y_test)[0, 1]
    pearson = np.round(pearson, 2)
    print(
        f"corr {var} vs target: {pearson}")</st></pre><p class="list-inset"><st c="38257">In the following output, we can see that the correlation between the new variables and the target is greater than the correlation shown by the original features (compare these values with those of </st><em class="italic"><st c="38455">step 4</st></em><st c="38461">):</st></p><pre class="source-code"><st c="38464">corr tree(AveRooms) vs target: 0.37
corr tree(AveBedrms) vs target: 0.12
corr tree(['AveRooms', 'AveBedrms']) vs target: 0.47</st></pre><p class="list-inset"><st c="38590">If you want to combine specific features instead of getting all possible combinations between variables, you can do so by specifying the input features </st><st c="38743">in tuples.</st></p></li>				<li><st c="38753">Create a tuple of tuples with the different features that we want to use as input for </st><st c="38840">decision trees:</st><pre class="source-code"><st c="38855">
features = (('Population'), ('Population','AveOccup'),
    ('Population', 'AveOccup', 'HouseAge'))</st></pre></li>				<li><st c="38950">Now, we </st><a id="_idIndexMarker619"/><st c="38959">need </st><a id="_idIndexMarker620"/><st c="38964">to pass these tuples to the </st><code><st c="38992">features_to_combine</st></code><st c="39011"> parameter </st><st c="39022">of </st><code><st c="39025">DecisionTreeFeatures()</st></code><st c="39047">:</st><pre class="source-code"><st c="39049">
dtf = DecisionTreeFeatures(
    variables=None,
    features_to_combine=features,
    cv=5,
    param_grid=param_grid,
    scoring="neg_mean_squared_error"
)
dtf.fit(X_train, y_train)</st></pre></li>				<li><st c="39213">We fitted the transformer in the previous step, so we can go ahead and add the features to training and </st><st c="39318">test sets:</st><pre class="source-code"><st c="39328">
train_t = dtf.transform(X_train)
test_t = dtf.transform(X_test)</st></pre></li>				<li><st c="39392">Display the </st><st c="39405">new features:</st><pre class="source-code"><st c="39418">
tree_features = [
    var for var in test_t.columns if "tree" in var]
test_t[tree_features].head()</st></pre><p class="list-inset"><st c="39513">In the following output, we can see the new features derived from predictions of decision trees in the </st><st c="39617">test set:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_10.jpg" alt="    Figure 8.10 – A portion of the testing set containing the features derived from the decision trees"/><st c="39626"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="39886">    Figure 8.10 – A portion of the testing set containing the features derived from the decision trees</st></p>
			<p class="list-inset"><st c="39984">To wrap up</st><a id="_idIndexMarker621"/><st c="39995"> the recipe, we’ll compare </st><a id="_idIndexMarker622"/><st c="40022">the performance of a Lasso linear regression model trained using the original features with one using the features derived from the </st><st c="40154">decision trees.</st></p>
			<ol>
				<li value="18"><st c="40169">Import </st><code><st c="40177">Lasso</st></code><st c="40182"> and the </st><code><st c="40191">cross_validate</st></code><st c="40205"> function </st><st c="40215">from </st><code><st c="40220">scikit-learn</st></code><st c="40232">:</st><pre class="source-code"><st c="40234">
from sklearn.linear_model import Lasso
from sklearn.model_selection import cross_validate</st></pre></li>				<li><st c="40324">Set up a Lasso </st><st c="40340">regression model:</st><pre class="source-code"><st c="40357">
lasso = Lasso(random_state=0, alpha=0.0001)</st></pre></li>				<li><st c="40401">Train and evaluate the model using the original data with cross-validation, and then print out the </st><st c="40501">resulting </st><em class="italic"><st c="40511">r</st></em><st c="40512">-squared:</st><pre class="source-code"><st c="40521">
cv_results = cross_validate(lasso, X_train, y_train,
    cv=3)
mean = cv_results['test_score'].mean()
std = cv_results['test_score'].std()
print(f"Results: {mean} +/- {std}")</st></pre><p class="list-inset"><st c="40692">In the following output, we can see the </st><em class="italic"><st c="40733">r</st></em><st c="40734">-squared of the Lasso regression model trained using the </st><st c="40791">original features:</st></p><pre class="source-code"><strong class="bold"><st c="40809">Results: 0.5480403481478856 +/- 0.004214649109293269</st></strong></pre></li>				<li><st c="40862">Finally, train a Lasso regression model with the features derived from the decision trees</st><a id="_idIndexMarker623"/><st c="40952"> and evaluate it </st><st c="40969">with cross-validation:</st><pre class="source-code"><st c="40991">
variables = ["AveRooms", "AveBedrms", "Population"]
train_t = train_t.drop(variables, axis=1)
cv_results = cross_validate(lasso, train_t, y_train,
    cv=3)
mean = cv_results['test_score'].mean()
std = cv_results['test_score'].std()
print(f"Results: {mean} +/- {std}")</st></pre><p class="list-inset"><st c="41256">In the</st><a id="_idIndexMarker624"/><st c="41263"> following output, we can see that the performance of the Lasso regression model trained based of the tree-derived features is b</st><a id="_idTextAnchor1068"/><st c="41391">etter; the </st><em class="italic"><st c="41403">r</st></em><st c="41404">-square is greater than that from </st><em class="italic"><st c="41438">step 20</st></em><st c="41445">:</st></p><pre class="source-code"><strong class="bold"><st c="41447">Results: 0.5800993721099441 +/- 0.002845475651622909</st></strong></pre></li>			</ol>
			<p><st c="41499">I hope I’ve given you a flavor of the power of combining features wit</st><a id="_idTextAnchor1069"/><a id="_idTextAnchor1070"/><st c="41569">h decision trees and how to do so </st><st c="41604">with </st><code><st c="41609">feature-engine</st></code><st c="41623">.</st></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor1071"/><st c="41624">How it works...</st></h2>
			<p><st c="41640">In this recipe, we created new features based on the predictions of decision trees trained on one or more variables. </st><st c="41758">We used </st><code><st c="41766">DecisionTreeFeatures()</st></code><st c="41788"> from</st><code><st c="41793"> Feature-engine</st></code><st c="41808"> to automate the process of training the decision trees with cross-validation and </st><st c="41890">hyperparameter optimization.</st></p>
			<p><code><st c="41918">DecisionTreeFeatures()</st></code><st c="41941"> trains decision trees using grid-search under the hood. </st><st c="41998">Hence, you can pass a grid of hyperparameters to optimize the tree, or the transformer will optimize just the depth, which, in any case, is the most important parameter in a decision tree. </st><st c="42187">You can also change the metric you want to optimize through the </st><code><st c="42251">scoring</st></code><st c="42258"> parameter and the cross-validation scheme you want to use through the </st><code><st c="42329">cv</st></code><st c="42331"> parameter.</st></p>
			<p><st c="42342">The most exciting feature of </st><code><st c="42372">DecisionTreeFeatures()</st></code><st c="42394"> is its ability to infer the feature combinations to create tree-derived features, which is regulated through the </st><code><st c="42508">features_to_combine</st></code><st c="42527"> parameter. </st><st c="42539">If you pass an integer to this parameter – say, for example, </st><code><st c="42600">3</st></code><st c="42601">, </st><code><st c="42603">DecisionTreeFeatures()</st></code><st c="42625"> will create all possible combinations of 1, 2, and 3 features and use these to train the decision trees. </st><st c="42731">Instead of an integer, you can pass a list of integers – say, </st><code><st c="42793">[2,3]</st></code><st c="42798"> – in which case, </st><code><st c="42816">DecisionTreeFeatures()</st></code><st c="42838"> will create all</st><a id="_idIndexMarker625"/><st c="42854"> possible combinations of 2 and 3 features. </st><st c="42898">You</st><a id="_idIndexMarker626"/><st c="42901"> can also specify which features you want to combine and how by passing the feature combinations in tuples, as we did in </st><em class="italic"><st c="43022">step 14</st></em><st c="43029">.</st></p>
			<p><st c="43030">With </st><code><st c="43036">fit()</st></code><st c="43041">, </st><code><st c="43043">DecisionTreeFeatures()</st></code><st c="43065"> finds the feature combinations and trains the decision trees. </st><st c="43128">With </st><code><st c="43133">transform()</st></code><st c="43144">, </st><code><st c="43146">DecisionTreeFeatures()</st></code><st c="43168"> adds the features resulting from the decision trees to </st><st c="43224">the DataFrame.</st></p>
			<p class="callout-heading"><st c="43238">Note</st></p>
			<p class="callout"><st c="43243">If you are training regression or multi-class classification, the new features will be either the prediction of the continuous target or the class. </st><st c="43392">If you are training a binary classification model, the new features will result from the probability of </st><st c="43496">class 1.</st></p>
			<p><st c="43504">After adding the new features, we compared their relationship to the target by analyzing Pearson’s correlation coefficient, which returned a measure of linear association. </st><st c="43677">We saw that the features derived from trees had a greater </st><st c="43735">correlation coefficient.</st></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor1072"/><st c="43759">See also</st></h2>
			<p><st c="43768">If you want to know more about what mutual information is and how to calculate it, check out this </st><st c="43867">article: </st><a href="https://www.blog.trainindata.com/mutual-information-with-python/"><st c="43876">https://www.blog.trainindata.com/mutual-information-with-python/</st></a><st c="43940">.</st></p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor1073"/><st c="43941">Creating periodic features from cyclical variables</st></h1>
			<p><st c="43992">Some features</st><a id="_idIndexMarker627"/><st c="44006"> are periodic – for e</st><a id="_idTextAnchor1074"/><st c="44027">xample, the</st><a id="_idIndexMarker628"/><st c="44039"> hours in</st><a id="_idTextAnchor1075"/><st c="44048"> a day, the months in a year, and the days in a week. </st><st c="44102">They all start at a certain value (say, January), go up to a certain other value (say, December), and then start over from the beginning. </st><st c="44240">Some features are numeric, such as the hours, and some can be represented with numbers, such as the months, with values of 1 to 12. </st><st c="44372">Yet, this numeric representation does not capture the periodicity or cyclical nature of the variable. </st><st c="44474">For example, December (12) is closer to January (1) than June (6); however, this relationship is not captured by the numerical representation of the feature. </st><st c="44632">But we could change it if we transformed these variables with the sine and cosine, two naturally </st><st c="44729">periodic functions.</st></p>
			<p><st c="44748">Encoding </st><a id="_idIndexMarker629"/><st c="44758">cyclical features </st><a id="_idIndexMarker630"/><st c="44776">with the sine and cosine functions allows linear models to leverage the cyclical nature of features and reduce their modeling error. </st><st c="44909">In this recipe, we will create new features from </st><a id="_idTextAnchor1076"/><a id="_idTextAnchor1077"/><st c="44958">periodic variables that capture the cyclical nature </st><st c="45010">of time.</st></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor1078"/><st c="45018">Getting ready</st></h2>
			<p><st c="45032">Trigonometric functions</st><a id="_idTextAnchor1079"/><st c="45056">, such as sine and cosine, are periodic, with valu</st><a id="_idTextAnchor1080"/><st c="45106">es cycling between -1 and 1 every 2π cycles, as </st><st c="45155">s</st><a id="_idTextAnchor1081"/><st c="45156">hown here:</st></p>
			<div><div><img src="img/B22396_08_11.jpg" alt="Figure 8.11 – Sine and cosine functions"/><st c="45166"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="45336">Figure 8.11 – Sine and cosine functions</st></p>
			<p><st c="45375">We can capture the periodicity of a cyclical variable by applying a trigonometric transformation after normalizing the variable values between 0 </st><st c="45521">and 2π:</st></p>
			<p><img src="img/37.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;sin&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;sin&lt;/mi&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mfrac&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.738em;height:1.983em;width:8.377em"/><st c="45528"/></p>
			<p><st c="45554">Dividing the variable’s values by its maximum will normalize it between 0 and 1 (assuming that the minimum value is 0), and multiplying it by 2π will rescale the variable between 0 </st><st c="45735">and 2π.</st></p>
			<p><st c="45742">Should we </st><a id="_idIndexMarker631"/><st c="45753">use sine? </st><st c="45763">Or should we use cosine? </st><st c="45788">The thing is, we need to use both to encode all the values of the variables unequivocally. </st><st c="45879">Since sine and cosine circle between 0 and 1, they will take a value of 0 for more than one value of </st><em class="italic"><st c="45980">x</st></em><st c="45981">. For example, the sine of 0 returns 0, and so does the sine of π. </st><st c="46048">So, if we encode a variable with just the sine, we wouldn’t be able to distinguish between the values 0 and π anymore. </st><st c="46167">However, because the sine and the cosine are out of phase, the cosine of 0 returns 1, whereas the </st><a id="_idIndexMarker632"/><st c="46265">cosine of π returns -1. </st><st c="46289">Hence, by encoding the variable with the two functions, we are now able to distinguish between 0 and 1, which would take (0,1) and (0,-</st><a id="_idTextAnchor1082"/><a id="_idTextAnchor1083"/><st c="46424">1) as values for the sine and cosine </st><st c="46462">functions, respectively.</st></p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor1084"/><st c="46486">How to do it…</st></h2>
			<p><st c="46500">In this recipe, we will first</st><a id="_idTextAnchor1085"/><st c="46530"> transform the </st><a id="_idTextAnchor1086"/><code><st c="46545">hour</st></code><st c="46549"> variable in a toy DataFrame with the sine and the cosine to get a sense of the new variable representation. </st><st c="46658">Then, we will automate feature creation from multiple cyclical variables </st><st c="46731">using </st><code><st c="46737">feature-engine</st></code><st c="46751">:</st></p>
			<ol>
				<li><st c="46753">Begin by importing the </st><st c="46776">necessary libraries:</st><pre class="source-code"><st c="46796">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</st></pre></li>				<li><st c="46867">Create a toy DataFrame with one variable – </st><code><st c="46911">hour</st></code><st c="46915"> – with values between 0 </st><st c="46940">and 23:</st><pre class="source-code"><st c="46947">
df = pd.DataFrame([i for i in range(24)],
    columns=["hour"])</st></pre></li>				<li><st c="47007">Next, create two features using the sine and cosine transformations, after normalizing the variable values between 0 </st><st c="47125">and 2π:</st><pre class="source-code"><st c="47132">
df["hour_sin"] = np.sin(
    df["hour"] / df["hour"].max() * 2 * np.pi)
df["hour_cos"] = np.cos(
    df["hour"] / df["hour"].max() * 2 * np.pi)</st></pre><p class="list-inset"><st c="47268">If we e</st><a id="_idTextAnchor1087"/><st c="47276">xecute </st><code><st c="47284">df.head()</st></code><st c="47293">, we will see the original and </st><st c="47324">new features:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_12.jpg" alt="Figure 8.12 – A DataFrame with the hour variable and the new features obtaine﻿d through the sine and cosine transformations"/><st c="47337"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="47462">Figure 8.12 – A DataFrame with the hour variable and the new features obtaine</st><a id="_idTextAnchor1088"/><st c="47539">d through the sine and cosine transformations</st></p>
			<ol>
				<li value="4"><st c="47585">Make</st><a id="_idIndexMarker633"/><st c="47590"> a scatter plot between the </st><a id="_idIndexMarker634"/><st c="47618">hour and its </st><st c="47631">sine-transformed values:</st><pre class="source-code"><st c="47655">
plt.scatter(df["hour"], df["hour_sin"])
plt.ylabel("Sine of hour")
plt.xlabel("Hour")
plt.title("Sine transformation")</st></pre><p class="list-inset"><st c="47774">In the following plot, we can see how the values of the hour circle between -1</st><a id="_idTextAnchor1089"/><st c="47853"> and 1, just like the sine function after </st><st c="47895">the transformation:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_13.jpg" alt="Figure 8.13 – A sca﻿tter plot of th﻿e hour versus its sine transformed values"/><st c="47914"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="48018">Figure 8.13 – A sca</st><a id="_idTextAnchor1090"/><st c="48037">tter plot of th</st><a id="_idTextAnchor1091"/><st c="48053">e hour versus its sine transformed values</st></p>
			<ol>
				<li value="5"><st c="48095">Now, make</st><a id="_idIndexMarker635"/><st c="48105"> a scatter plot between </st><a id="_idIndexMarker636"/><st c="48129">the hour and its </st><st c="48146">cosine transformation:</st><pre class="source-code"><st c="48168">
plt.scatter(df["hour"], df["hour_cos"])
plt.ylabel("Cosine of hour")
plt.xlabel("Hour")
plt.title("Cosine transformation")</st></pre><p class="list-inset"><st c="48291">In the following plot, we can see how the values of the hour circle between -1 </st><a id="_idTextAnchor1092"/><st c="48371">and 1, just like the cosine function after </st><st c="48414">the transformation:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_14.jpg" alt="Figure 8.14 – A scatter plot of the hour versus its cosine-transformed values"/><st c="48433"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="48541">Figure 8.14 – A scatter plot of the hour versus its cosine-transformed values</st></p>
			<p class="list-inset"><st c="48618">Finally, we</st><a id="_idIndexMarker637"/><st c="48630"> can reconstitute the cyclical nature o</st><a id="_idTextAnchor1093"/><st c="48669">f the hour,</st><a id="_idTextAnchor1094"/><st c="48681"> which is now captured by the two </st><st c="48715">new features.</st></p>
			<ol>
				<li value="6"><st c="48728">Plot the </st><a id="_idIndexMarker638"/><st c="48738">values of the sine versus the cosine of the hour, and overlay the original values of the hour using a </st><st c="48840">color map:</st><pre class="source-code"><st c="48850">
fig, ax = plt.subplots(figsize=(7, 5))
sp = ax.scatter(
    df["hour_sin"], df["hour_cos"], c=df["hour"])
ax.set(
    xlabel="s</st><a id="_idTextAnchor1095"/><st c="48970">in(hour)",
    ylab</st><a id="_idTextAnchor1096"/><st c="48986">el="cos(hour)",
)
_ = fig.colorbar(sp)</st></pre><p class="list-inset"><st c="49025">In the following plot, we can see how the two trigonometric transformations of the hour reflect</st><a id="_idIndexMarker639"/><st c="49121"> the cy</st><a id="_idTextAnchor1097"/><st c="49128">clical </st><a id="_idIndexMarker640"/><st c="49136">nature of the hour, in a plot that reminds us of </st><st c="49185">a clock:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_15.jpg" alt="Figure 8.15 – A scatter plot of the trigonometric transformation of the hour"/><st c="49193"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49357">Figure 8.15 – A scatter plot of the trigonometric transformation of the hour</st></p>
			<p class="callout-heading"><st c="49433">Note</st></p>
			<p class="callout"><st c="49438">The code implementation and idea for this plot were taken from scikit-learn’s </st><st c="49517">documentation: </st><a href="https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features"><st c="49532">https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features</st></a><st c="49652">.</st></p>
			<p class="list-inset"><st c="49653">Now that we understand the n</st><a id="_idTextAnchor1098"/><st c="49682">ature and effect of the transformation, let’s create new features using the sine and cosine transformations from multiple variables automatically. </st><st c="49830">We will use the </st><code><st c="49846">feature-engine</st></code> <st c="49860">library’s </st><code><st c="49871">CyclicalFeatures()</st></code><st c="49889">.</st></p>
			<ol>
				<li value="7"><st c="49890">Import </st><code><st c="49898">CyclicalFeatures()</st></code><st c="49916">:</st><pre class="source-code"><st c="49918">
from feature_engine.creation import CyclicalFeatures</st></pre></li>				<li><st c="49971">Let’s create</st><a id="_idIndexMarker641"/><st c="49984"> a toy DataFrame that contains the </st><code><st c="50019">hour</st></code><st c="50023">, </st><code><st c="50025">month</st></code><st c="50030">, and </st><code><st c="50036">week</st></code><st c="50040"> variables, whose</st><a id="_idIndexMarker642"/><st c="50057"> values vary between 0 and 23, 1 and 12, and 0 and </st><st c="50108">6, respectively:</st><pre class="source-code"><st c="50124">
df = pd.DataFrame()
df["hour"] = pd.Series([i for i in range(24)])
df["month"] = pd.Series([i for i in range(1, 13)]*2)
df["week"] = pd.Series([i for i in range(7)]*4)</st></pre><p class="list-inset"><st c="50292">If we execut</st><a id="_idTextAnchor1099"/><st c="50305">e </st><code><st c="50308">df.head()</st></code><st c="50317">, we will see the first five rows of the </st><st c="50358">toy DataFra</st><a id="_idTextAnchor1100"/><st c="50369">me:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_16.jpg" alt="Figure 8.16 – The﻿ toy DataFrame with three cyclical features"/><st c="50373"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="50390">Figure 8.16 – The</st><a id="_idTextAnchor1101"/><st c="50407"> toy DataFrame with three cyclical features</st></p>
			<ol>
				<li value="9"><st c="50450">Set up the transformer to create the sine and cosine features from </st><st c="50518">these variables:</st><pre class="source-code"><st c="50534">
cyclic = CyclicalFeatures(
    variables=None,
    drop_original=False,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="50600">Note</st></p>
			<p class="callout"><st c="50605">By setting </st><code><st c="50617">variables</st></code><st c="50626"> to </st><code><st c="50630">None</st></code><st c="50634">, </st><code><st c="50636">CyclicalFeatures()</st></code><st c="50654"> will create trigonometric features from all numerical variables. </st><st c="50720">To create trigonometric features from a subset of variables, we can pass the variables’ names in a list to the </st><code><st c="50831">variables</st></code><st c="50840"> parameter. </st><st c="50852">We can retain or drop the original variables after creating the cyclical features using the </st><code><st c="50944">drop_original</st></code><st c="50957"> parameter.</st></p>
			<ol>
				<li value="10"><st c="50968">To finish, add the </st><a id="_idIndexMarker643"/><st c="50988">features to the DataFrame and capture the result in a </st><st c="51042">new variable:</st><pre class="source-code"><st c="51055">
dft = cyclic.fit_transform(df)</st></pre><p class="list-inset"><st c="51086">If</st><a id="_idTextAnchor1102"/><st c="51089"> we execute </st><code><st c="51101">dft.head()</st></code><st c="51111">, we will see the original and </st><st c="51142">new features:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_17.jpg" alt="Figure 8.17 – DataFrame with cyclical features plus the features created through the sine and cosine functions"/><st c="51155"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="51559">Figure 8.17 – DataFrame with cyclical features plus the features created through the sine and cosine functions</st></p>
			<p><st c="51669">And that’s it – we’ve </st><a id="_idIndexMarker644"/><st c="51692">created features by using the sine and cosine transformation automatically from mu</st><a id="_idTextAnchor1103"/><a id="_idTextAnchor1104"/><st c="51774">ltiple variables and added them directly to the </st><st c="51823">original DataFrame.</st></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor1105"/><st c="51842">How it works…</st></h2>
			<p><st c="51856">In this recipe, we enco</st><a id="_idTextAnchor1106"/><st c="51880">ded cyclical features with</st><a id="_idTextAnchor1107"/><st c="51907"> values obtained from the sine and cosine functions, applied to the normalized values of the variable. </st><st c="52010">First, we normalized the variable values between 0 and 2 π. </st><st c="52070">To do this, we divided the variable values by the variable maximum value, which we obtained with </st><code><st c="52167">pandas.max()</st></code><st c="52179">, to scale the variables between 0 and 1. </st><st c="52221">Then, we multiplied those values by 2π, using </st><code><st c="52267">numpy.pi</st></code><st c="52275">. Finally, we used </st><code><st c="52294">np.sin</st></code><st c="52300"> and </st><code><st c="52305">np.cos</st></code><st c="52311"> to apply the sine and cosine </st><st c="52341">transformations, respectively.</st></p>
			<p><st c="52371">To automate this procedure for multiple variables, we used the </st><code><st c="52435">Feature-engine</st></code><st c="52449"> library’s </st><code><st c="52460">CyclicalFeatures()</st></code><st c="52478">. With </st><code><st c="52485">fit()</st></code><st c="52490">, the transformer learned the maximum values of each variable, and with </st><code><st c="52562">transform()</st></code><st c="52573">, it added the features resulting from the sine </st><a id="_idIndexMarker645"/><st c="52621">and cosine transformations to </st><st c="52651">the DataFrame.</st></p>
			<p class="callout-heading"><st c="52665">Note</st></p>
			<p class="callout"><st c="52670">In theory, to apply the sine and cosine transformation, we need to scale the original variable between 0 and 1. </st><st c="52783">Dividing by the maximum of the variable will only result in this scaling if the minimum value is 0. </st><st c="52883">scikit-learn’s documentation and </st><code><st c="52916">Feature-engine</st></code><st c="52930">’s current implementation divide the variable by its maximum value (or an arbitrary period), without paying too much attention to whether the variable starts at 0. </st><st c="53095">In practice, you won’t see a big difference in the resulting variables if you divide the hour feature by 23 or 24, or the month feature by 12 or 11. </st><st c="53244">Discussions are underway on whether Feature-engine’s implementation should be updated, so the default behavior might change by the time t</st><a id="_idTextAnchor1108"/><a id="_idTextAnchor1109"/><a id="_idTextAnchor1110"/><a id="_idTextAnchor1111"/><st c="53381">his book is published. </st><st c="53405">Check out the documentation for </st><st c="53437">more details.</st></p>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor1112"/><st c="53450">Creating spline features</st></h1>
			<p><st c="53475">Linear models</st><a id="_idIndexMarker646"/><st c="53489"> expect a linear relationship between the predictor variables and the target. </st><st c="53567">However, we can use linear models to model non-linear effects if we first transform the f</st><a id="_idTextAnchor1113"/><st c="53656">eatures. </st><st c="53666">In the </st><em class="italic"><st c="53673">Performing polynomial expansion</st></em><st c="53704"> recipe, we saw how we can unmask linear patterns by creating features with polynomial functions. </st><st c="53802">In this recipe, we will discuss the use </st><st c="53842">of splines.</st></p>
			<p><st c="53853">Splines are used to mathematically reproduce flexible shapes. </st><st c="53916">They consist of piecewise low-degree polynomial functions. </st><st c="53975">To create splines, we must place knots at several values of </st><em class="italic"><st c="54035">x</st></em><st c="54036">. These knots indicate where the pieces of the function join. </st><st c="54098">Then, we fit low-degree polynomials to the data between two </st><st c="54158">consecutive knots.</st></p>
			<p><st c="54176">There are several types of splines, such as smoothing splines, regression splines, and B-splines. </st><st c="54275">scikit-learn supports the use of B-splines to create features. </st><st c="54338">The procedure to fit and, therefore, return the spline values for a certain variable, based on a polynomial degree and the number of knots, exceeds the scope of this recipe. </st><st c="54512">For more details, check out the resources in the </st><em class="italic"><st c="54561">See also</st></em><st c="54569"> section of this recipe. </st><st c="54594">In this recipe, we’ll get a sense of what splines are</st><a id="_idTextAnchor1114"/><a id="_idTextAnchor1115"/><st c="54647"> and how we can use them to improve the performance of </st><st c="54702">linear models.</st></p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor1116"/><st c="54716">Getting ready</st></h2>
			<p><st c="54730">Let’s get a sense of what splines are. </st><st c="54770">In the following figure, on the left, we can see a spline with a degree of 1. </st><st c="54848">It consists of two linear pieces – one from 2.5 to 5 and the other from 5 to 7.5. </st><st c="54930">There are three knots – 2.5, 5, and 7.5. </st><st c="54971">Outside the interval between 2.5 and 7.5, the spline takes a value of 0. </st><st c="55044">The latter is characteristic of splines; they are only non-negative between certain values. </st><st c="55136">On the right panel of the figure, we can see three</st><a id="_idIndexMarker647"/><st c="55186"> splines of degree 1. </st><st c="55208">W</st><a id="_idTextAnchor1117"/><st c="55209">e can construct as many splines as we want by introducing </st><st c="55267">more knots:</st></p>
			<div><div><img src="img/B22396_08_18.jpg" alt="Figure 8.18 – The splines with a degree of 1"/><st c="55278"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="55401">Figure 8.18 – The splines with a degree of 1</st></p>
			<p><st c="55445">In the following figure, on the left, we can see</st><a id="_idTextAnchor1118"/><st c="55494"> a quadratic spline, also known as a spline with a degree of 2. </st><st c="55558">It is based on four adjacent knots – 0, 2.5, 5, and 7.5. </st><st c="55615">On the ri</st><a id="_idTextAnchor1119"/><st c="55624">ght-hand side of the figure, we can see several splines of </st><st c="55684">degree 2:</st></p>
			<div><div><img src="img/B22396_08_19.jpg" alt="Figure 8.19 – The splines with a degree of 2"/><st c="55693"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="55793">Figure 8.19 – The splines with a degree of 2</st></p>
			<p><st c="55837">We can use splines to model non-lin</st><a id="_idTextAnchor1120"/><a id="_idTextAnchor1121"/><st c="55873">ear functions, and we will learn how to do this in the </st><st c="55929">next section.</st></p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor1122"/><st c="55942">How to do it…</st></h2>
			<p><st c="55956">In this recipe, we</st><a id="_idIndexMarker648"/><st c="55975"> will use splines to model the sine function. </st><st c="56021">Once we get a sense of what splines are and how we can use them to fit non-linear relationships through a linear model, we will use splines for regression in a </st><st c="56181">real dataset:</st></p>
			<p class="callout-heading"><st c="56194">Note</st></p>
			<p class="callout"><st c="56199">The idea to model the sine function with splines was taken from scikit-learn’s </st><st c="56279">documentation: </st><a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html"><st c="56294">https://scikit-learn.org/stable/auto_</st><st c="56331">examples/linear_model/plot_polynomial_interpolation.html</st></a><st c="56388">.</st></p>
			<ol>
				<li><st c="56389">Let’s begin by importing the necessary libraries </st><st c="56439">and classes:</st><pre class="source-code"><st c="56451">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import SplineTransformer</st></pre></li>				<li><st c="56613">Create a training set, </st><code><st c="56637">X</st></code><st c="56638">, with 20 values between -1 and 11, and the target variable, </st><code><st c="56699">y</st></code><st c="56700">, which is the sine </st><st c="56720">of </st><code><st c="56723">X</st></code><st c="56724">:</st><pre class="source-code"><st c="56725">
X = np.linspace(-1, 11, 20)
y = np.sin(X)</st></pre></li>				<li><st c="56767">Plot the relationship between </st><code><st c="56798">X</st></code> <st c="56799">and </st><code><st c="56803">y</st></code><st c="56804">:</st><pre class="source-code"><st c="56805">
plt.plot(X, y)
plt.ylabel("y")
plt.x</st><a id="_idTextAnchor1124"/><st c="56842">label("X")</st></pre><p class="list-inset"><st c="56853">In the following plot, we can see the sine function </st><st c="56906">of </st><code><st c="56909">X</st></code><st c="56910">:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_20.jpg" alt="Figure 8.20 – The relationship ﻿between the predictor and the target variable, where y = sine(x)"/><st c="56911"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="56980">Figure 8.20 – The relationship </st><a id="_idTextAnchor1125"/><st c="57011">between the predictor and the target variable, where y = sine(x)</st></p>
			<ol>
				<li value="4"><st c="57075">Fit a linear</st><a id="_idIndexMarker649"/><st c="57088"> model to predict </st><code><st c="57106">y</st></code><st c="57107"> from </st><code><st c="57113">X</st></code><st c="57114"> by utilizing a Ridge regression, and then obtain the predictions of </st><st c="57183">the model:</st><pre class="source-code"><st c="57193">
linmod = Ridge(random_state=10)
linmod.fit(X.reshape(-1, 1), y)
pred = linmod.predict(X.reshape(-1, 1))</st></pre></li>				<li><st c="57297">Now, plot the relationship between </st><code><st c="57333">X</st></code><st c="57334"> and </st><code><st c="57339">y</st></code><st c="57340">, and overlay </st><st c="57354">the predictions:</st><pre class="source-code"><st c="57370">
plt.plot(X, y)
plt.plot(X, pred)
plt.ylabel("y")
plt.xlabel("X")
plt.legend(
    ["y", "linear"],
    bbox_to_anchor=(1, 1),
    loc="upper left")</st></pre><p class="list-inset"><st c="57505">In the following figure, we can see that the linear model m</st><a id="_idTextAnchor1126"/><st c="57565">akes a very poor fit of the non-linear relationship between </st><code><st c="57626">X</st></code> <st c="57627">and </st><code><st c="57631">y</st></code><st c="57632">:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_21.jpg" alt="Figure 8.21 – The linear fit between X and y"/><st c="57633"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="57716">Figure 8.21 – The linear fit between X and y</st></p>
			<ol>
				<li value="6"><st c="57760">Now, set</st><a id="_idIndexMarker650"/> <a id="_idTextAnchor1127"/><st c="57769">up </st><code><st c="57773">SplineTransformer()</st></code><st c="57792"> to obtain spline features from </st><code><st c="57824">X</st></code><st c="57825">, by utilizing third-degree polynomials and five knots at equidistant places within the values </st><st c="57920">of </st><code><st c="57923">X</st></code><st c="57924">:</st><pre class="source-code"><st c="57925">
spl = SplineTransformer(degree=3, n_knots=5)</st></pre></li>				<li><st c="57970">Obtain the spline features and convert the NumPy array into a </st><code><st c="58033">pandas</st></code><st c="58039"> DataFrame, adding the names of the spline </st><st c="58082">basis functions:</st><pre class="source-code"><st c="58098">
X_t = spl.fit_transform(X.reshape(-1, 1))
X_df = pd.DataFrame(
    X_t,
    columns=spl.get_feature_names_out(</st><a id="_idTextAnchor1128"/><st c="58201">["var"])
)</st></pre><p class="list-inset"><st c="58212">By executing </st><code><st c="58226">X_df.head()</st></code><st c="58237">, we can see the </st><st c="58254">spline features:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_22.jpg" alt="Figure 8.22 – A DataFrame with the splines"/><st c="58270"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="58585">Figure 8.22 – A DataFrame with the splines</st></p>
			<p class="callout-heading"><st c="58627">Note</st></p>
			<p class="callout"><code><st c="58632">SplineTransformer()</st></code><st c="58652"> returns a</st><a id="_idTextAnchor1129"/><st c="58662"> feature matrix consisting of </st><code><st c="58692">n_splines = n_knots + degree – </st></code><code><st c="58723">1</st></code><st c="58724">.</st></p>
			<ol>
				<li value="8"><st c="58725">Now, plot </st><a id="_idIndexMarker651"/><st c="58736">the splines against the values </st><st c="58767">of </st><code><st c="58770">X</st></code><st c="58771">:</st><pre class="source-code"><st c="58772">
plt.plot(X, X_t)
plt.legend(
    spl.get_feature_names_out(["var"]),
    bbox_to_anchor=(1, 1),
    loc="upper left")
plt.xlabel("X")
plt.ylabel("Splines values")
plt.title("Splines")
plt.show()</st></pre><p class="list-inset"><st c="58955">In the following figure, we can see the relationship betwee</st><a id="_idTextAnchor1130"/><st c="59015">n the different splines and the values of the predictor </st><st c="59072">variable, </st><code><st c="59082">X</st></code><st c="59083">:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_23.jpg" alt="Figure 8.23 – Spli﻿nes plotted against the values of the predictor variable, X"/><st c="59084"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="59216">Figure 8.23 – Spli</st><a id="_idTextAnchor1131"/><st c="59234">nes plotted against the values of the predictor variable, X</st></p>
			<ol>
				<li value="9"><st c="59294">Now, fit a </st><a id="_idIndexMarker652"/><st c="59306">linear model to predict </st><code><st c="59330">y</st></code><st c="59331"> from the spline features obtained from </st><code><st c="59371">X</st></code><st c="59372">, by utilizing a Ridge regression, and then obtain the predictions of </st><st c="59442">the model:</st><pre class="source-code"><st c="59452">
linmod = Ridge(random_state=10)
linmod.fit(X_t, y)
pred = linmod.predict(X_t)</st></pre></li>				<li><st c="59530">Now, plot the relationship between </st><code><st c="59566">X</st></code><st c="59567"> and </st><code><st c="59572">y</st></code><st c="59573">, and overlay </st><st c="59587">the predictions:</st><pre class="source-code"><st c="59603">
plt.plot(X, y)
plt.plot(X, pred)
plt.ylabel("y")
plt.xlabel("X")
plt.legend(
    ["y", </st><a id="_idTextAnchor1132"/><st c="59687">"splines"],
    bbox_to_anchor=(1, 1),
    loc="upper left")</st></pre><p class="list-inset"><st c="59739">In the following figure, we can see that by utilizing spline featur</st><a id="_idTextAnchor1133"/><st c="59807">es as input, the Ridge regression can better predict the shape </st><st c="59871">of </st><code><st c="59874">y</st></code><st c="59875">:</st></p></li>			</ol>
			<div><div><img src="img/B22396_08_24.jpg" alt="Figure 8.24 – The predictions of a linear model, based on splines overlaid over the true relationship between X and y"/><st c="59876"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="59961">Figure 8.24 – The predictions of a linear model, based on splines overlaid over the true relationship between X and y</st></p>
			<p class="callout-heading"><st c="60078">Note</st></p>
			<p class="callout"><st c="60083">Increasing the number of knots or the degree of the polynomial increases the flexibility of the spline curves. </st><st c="60195">Try creating splines from higher polynomial degrees and see how the Ridge regression </st><st c="60280">predictions change.</st></p>
			<p class="list-inset"><st c="60299">Now that we </st><a id="_idIndexMarker653"/><st c="60312">understand what the spline features are and how we can use them to predict non-linear effects, let’s try them out on a </st><st c="60431">real dataset.</st></p>
			<ol>
				<li value="11"><st c="60444">Import some additional classes and functions </st><st c="60490">from </st><code><st c="60495">scikit-learn</st></code><st c="60507">:</st><pre class="source-code"><st c="60509">
from sklearn.datasets import fetch_california_housing
from sklearn.compose import Colu</st><a id="_idTextAnchor1134"/><st c="60596">mnTransformer
from sklearn.model_selection import cross_validate</st></pre></li>				<li><st c="60661">Load the California housing dataset and drop two of the variables, which we won’t use </st><st c="60748">for modeling:</st><pre class="source-code"><st c="60761">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(["Latitude", "Longitude"], axis=1,
    inplace=True)</st></pre></li>				<li><st c="60882">First, we </st><a id="_idIndexMarker654"/><st c="60893">wil</st><a id="_idTextAnchor1135"/><st c="60896">l fit a Ridge regression to predict house prices based on the existing variables, by utilizing cross-validation, and then obtain the performance of the model to set up </st><st c="61065">the benchmark:</st><pre class="source-code"><st c="61079">
linmod = Ridge(random_state=10)
cv = cross_validate(linmod, X, y)
mean_, std_ = np.mean(
    cv[«test_score"]), np.std(cv["test_score"])
print(f"Model score: {mean_} +- {std_}")</st></pre><p class="list-inset"><st c="61253">In the following output, we can see the model performance, where the values are </st><st c="61334">the </st><em class="italic"><st c="61338">R</st></em><st c="61339">-squared:</st></p><pre class="source-code"><code><st c="61415">SplineTransformer()</st></code><st c="61434"> to obtain spline features from four variables by utilizing third-degree polynomials and 50 knots, and then fit the pipeline to </st><st c="61562">the data:</st><pre class="source-code"><st c="61571">
spl = SplineTransformer(degree=3, n_knots=50)
ct = ColumnTransformer(
    [("splines", spl, [
        "AveRooms", "AveBedrms", "Population",
        "AveOccup"]
    )],
    remainder="passthrough",
)
ct.fit(X, y)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="61756">Note</st></p>
			<p class="callout"><st c="61761">Remember that we need to use </st><code><st c="61791">ColumnTransformer()</st></code><st c="61810"> to obtain features from a subset of variables in the data. </st><st c="61870">With </st><code><st c="61875">remainder=passthrough</st></code><st c="61896">, we ensure that the variables that are not used as templates for the splines – that is, </st><code><st c="61985">MedInc</st></code><st c="61991"> and </st><code><st c="61996">HouseAge</st></code><st c="62004"> – are also returned in the resulting DataFrame. </st><st c="62053">To check out the features resulting from this step, </st><st c="62105">execute </st><code><st c="62113">ct.get_feature_names_out()</st></code><st c="62139">.</st></p>
			<ol>
				<li value="15"><st c="62140">Now, fit a Ridge </st><a id="_idIndexMarker655"/><st c="62158">regression to predict house prices based on </st><code><st c="62202">MedInc</st></code><st c="62208">, </st><code><st c="62210">HouseAge</st></code><st c="62218">, and the spline features, using cross-validation, and then obtain the performance of </st><st c="62304">the model:</st><pre class="source-code"><st c="62314">
cv = cross_validate(linmod, ct.transform(X), y)
mean_, std_ = np.mean(
    cv[«test_score"]), np.std(cv["test_score"])
print(f"Model score: {mean_} +- {std_}")</st></pre><p class="list-inset"><st c="62470">In the following output, we can see the model performance, where the values are </st><st c="62551">the </st><em class="italic"><st c="62555">R</st></em><st c="62556">-squared:</st></p><pre class="source-code"><strong class="bold"><st c="62565">Model score: 0.5553526813919297 +- 0.02244513992785257</st></strong></pre></li>			</ol>
			<p><st c="62620">As we can see, by using splines in place of some of the original va</st><a id="_idTextAnchor1136"/><a id="_idTextAnchor1137"/><st c="62688">riables, we can improve the performance of the linear </st><st c="62743">regression model.</st></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor1138"/><st c="62760">How it works…</st></h2>
			<p><st c="62774">In this recipe, we created new features based on splines. </st><st c="62833">First, we used a toy variable, with values from -1 to 11, and then we obtained splines from a real dataset. </st><st c="62941">The procedure in both cases was identical – we used </st><code><st c="62993">SplineTransformer()</st></code><st c="63012"> from </st><code><st c="63018">scikit-learn</st></code><st c="63030">. The </st><code><st c="63036">SplineTransformer()</st></code><st c="63055"> transformer takes the </st><code><st c="63078">degree</st></code><st c="63084"> property of the polynomial and the number of knots (</st><code><st c="63137">n_knots</st></code><st c="63145">) as input and returns the splines that better fit the data. </st><st c="63207">The knots are placed at equidistant values of </st><code><st c="63253">X</st></code><st c="63254"> by default, but through the </st><code><st c="63283">knots</st></code><st c="63288"> parameter, we can choose to uniformly distribute them to the quantiles of </st><code><st c="63363">X</st></code><st c="63364"> instead, or we can pass an array with the specific values of </st><code><st c="63426">X</st></code><st c="63427"> that should be used </st><st c="63448">as knots.</st></p>
			<p class="callout-heading"><st c="63457">Note</st></p>
			<p class="callout"><st c="63462">The number, spacing, and position of the knots are arbitrarily set by the user and are the parameters that influence the shape of the splines the most. </st><st c="63615">When using splines in regression models, we can optimize these parameters in a randomized search </st><st c="63712">with cross-validation.</st></p>
			<p><st c="63734">With </st><code><st c="63740">fit()</st></code><st c="63745">, the </st><a id="_idIndexMarker656"/><st c="63751">transformer computes the knots of the splines. </st><st c="63798">With </st><code><st c="63803">transform()</st></code><st c="63814">, it returns the array of B-splines. </st><st c="63851">The transformer returns </st><code><st c="63875">n_splines=n_knots + degree – </st></code><code><st c="63904">1</st></code><st c="63905">.</st></p>
			<p><st c="63906">Remember that, like most scikit-learn transformers, </st><code><st c="63959">SplineTransformer()</st></code><st c="63978"> also now has the option to return </st><code><st c="64013">pandas</st></code><st c="64019"> and polars DataFrames in addition to NumPy arrays, a behavior that can be modified through the </st><code><st c="64115">set_output()</st></code><st c="64127"> method.</st></p>
			<p><st c="64135">Finally, we used </st><code><st c="64153">ColumnTransformer()</st></code><st c="64172"> to derive splines from a subset of features. </st><st c="64218">Because we set </st><code><st c="64233">remainder</st></code><st c="64242"> to </st><code><st c="64246">passthrough</st></code><st c="64257">, </st><code><st c="64259">ColumnTransformer</st><a id="_idTextAnchor1139"/><st c="64276">() </st></code><st c="64280">concatenated the features that were not used to obtain splines to the resulting matrix of splines. </st><st c="64379">By doing this, we fitted a Ridge regression with the splines, plus the </st><code><st c="64450">MedInc</st></code><st c="64456"> and </st><code><st c="64461">Ho</st><a id="_idTextAnchor1140"/><a id="_idTextAnchor1141"/><st c="64463">useAge</st></code><st c="64470"> variables, and managed to improve the linear </st><st c="64516">model’s performance.</st></p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor1142"/><st c="64536">See also</st></h2>
			<p><st c="64545">To find out more about the math underlying B-splines, check out the </st><st c="64614">following articles:</st></p>
			<ul>
				<li><st c="64633">Perperoglou, et al. </st><em class="italic"><st c="64654">A review of spline function procedures in R</st></em><st c="64697"> (</st><a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3"><st c="64699">https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3</st></a><st c="64777">). </st><st c="64781">BMC Med Res Methodol 19, </st><st c="64806">46 (2019).</st></li>
				<li><st c="64816">Eilers and Marx. </st><em class="italic"><st c="64834">Flexible Smoothing with B-splines and </st></em><em class="italic"><st c="64872">Penalties</st></em><st c="64881"> (</st><a href="https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full"><st c="64883">https://projecteuclid.org/journals/statistical-science/volume-11/issue-2/Flexible-smoothing-with-B-splines-and-penalties/10.1214/ss/1038425655.full</st></a><st c="65030">).</st></li>
				<li><st c="65033">For an example of how to use B-splines to model time series data, check out the following page in the </st><code><st c="65136">scikit-learn</st></code><st c="65148"> library’s </st><st c="65159">documentation: </st><a href="https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#periodic-spline-features"><st c="65174">https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#periodic-spline-features</st></a><st c="65296">.</st></li>
			</ul>
		</div>
	<div></body></html>