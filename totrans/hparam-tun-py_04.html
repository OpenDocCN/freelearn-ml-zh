<html><head></head><body>
		<div id="_idContainer027">
			<h1 id="_idParaDest-31"><em class="italic"><a id="_idTextAnchor031"/>Chapter 3</em>: Exploring Exhaustive Search</h1>
			<p>Hyperparameter tuning doesn't always correspond to fancy and complex search algorithms. In fact, a simple <strong class="source-inline">for</strong> loop or manual search based on the developer's instinct can also be utilized to achieve the goal of hyperparameter tuning, which is to get the maximum evaluation score on the validation score without causing an overfitting issue. </p>
			<p>In this chapter, we'll discuss the first out of four groups of hyperparameter tuning, called an <strong class="bold">exhaustive search</strong>. This <a id="_idIndexMarker068"/>is the <em class="italic">most widely used</em> and <em class="italic">most straightforward</em> hyperparameter-tuning group in practice. As explained by its name, hyperparameter-tuning methods that belong to this group work by <em class="italic">exhaustively searching</em> through the hyperparameter space. Except for one method, all of the methods in this group <a id="_idIndexMarker069"/>are categorized as <strong class="bold">uninformed search</strong> algorithms, meaning they are not learning from previous iterations to have a better search space in the future. Three methods will be discussed in this chapter: manual search, grid search, and random search. </p>
			<p>By the end of this chapter, you will understand the concepts of each of the hyperparameter-tuning methods that belong to the exhaustive search group. You will be able to explain these methods with confidence when someone asks you about them, in both a high-level and detailed fashion, along with the pros and cons. More importantly, you will be able to apply all of the methods with high confidence in practice. You will also be able to understand what's happening if there are errors or unexpected results and understand how to set up the method configuration to match your specific problem.</p>
			<p>The following main topics will be covered in this chapter:</p>
			<ul>
				<li>Understanding manual search</li>
				<li>Understanding grid search</li>
				<li>Understanding random search</li>
			</ul>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor032"/>Understanding manual search</h1>
			<p><strong class="bold">Manual search</strong> is the <a id="_idIndexMarker070"/>most straightforward hyperparameter-tuning method that belongs to the exhaustive search group. In fact, it's not even an algorithm! There's no clear rule on how to perform this method. As its name would suggest, a manual search is performed based on your instinct. You simply have to tweak the hyperparameters until you are satisfied enough with the result.</p>
			<p>This method is <em class="italic">the one exception</em> mentioned before in the introduction of this chapter. Except for this method, other methods in the exhaustive search group are categorized as <strong class="bold">uninformed search methods</strong>. You <a id="_idIndexMarker071"/>may already know the reason why this method is the exception. It's because the developer themselves learn what is the impact of changing a particular or a set of hyperparameters in each iteration. In other words, they learn from previous iterations to have a (hopefully) better "hyperparameter space" in the next iterations.</p>
			<p>To perform a <a id="_idIndexMarker072"/>manual search, do the following:</p>
			<ol>
				<li>Split the original full data into train and test sets (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>).</li>
				<li>Specify initial hyperparameter values.</li>
				<li>Perform cross-validation on the train set (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>).</li>
				<li>Get the cross-evaluation score.</li>
				<li>Specify new hyperparameter values.</li>
				<li>Repeat <em class="italic">steps 3-5</em> until you are satisfied enough.</li>
				<li>Train on the full training set using the final hyperparameter values.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>Although this method seems very straightforward and easy to do, it is actually <em class="italic">the other way around for a beginner</em>. This is because you need to really understand how the model works and the usage of each hyperparameter. It is also worth noting that, when it comes to manual search, there is <em class="italic">no clear definition of the hyperparameter space</em>. The hyperparameter space can be surprisingly narrow or vast, based on the developer's willingness and initiative to experiment with it. </p>
			<p>Here is a list of <a id="_idIndexMarker073"/>pros and cons of the manual search hyperparameter-tuning <a id="_idIndexMarker074"/>method:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18753_03_001.jpg" alt="Figure 3.1 – Manual search: pros and cons&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Manual search: pros and cons</p>
			<p>Now that you are aware of how manual search works, along with the pros and cons, we will learn the simplest automated hyperparameter-tuning strategy, which will be discussed in the next section.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor033"/>Understanding grid search</h1>
			<p><strong class="bold">Grid search</strong> is the <em class="italic">simplest automated hyperparameter-tuning method</em> that ever existed. Apart from <a id="_idIndexMarker075"/>the fancy name, grid search is basically just a <em class="italic">nested</em> <strong class="source-inline">for</strong> loop that tests all possible hyperparameter values in the search space. Although many packages have grid search as one of their hyperparameter-tuning method implementations, it is super easy to write your own code from scratch to implement this method. The name <em class="italic">grid</em> comes from the fact that we have to test the whole hyperparameter space just like creating a grid, as illustrated in the following diagram.</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18753_03_002.jpg" alt="Figure 3.2 – Grid search illustration&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Grid search illustration</p>
			<p>For example, let's say we want to perform hyperparameter tuning using the grid search method on a <a id="_idIndexMarker076"/>random forest. We decide to focus only on the number of estimators, splitting criterion, and maximum tree-depth hyperparameters. Then, we can specify a list of possible values for each of the hyperparameters. Let's say we define the hyperparameter space as follows:</p>
			<ul>
				<li>Number of estimators: <strong class="source-inline">n_estimators = [25, 50, 100, 150, 200]</strong></li>
				<li>Splitting criterion: <strong class="source-inline">criterion = ["gini", "entropy"]</strong></li>
				<li>Maximum depth: <strong class="source-inline">max_depth = [3, 5, 10, 15, 20, None]</strong></li>
			</ul>
			<p>Notice that for the grid search method, we do not have to specify the underlying distribution of the hyperparameters. We simply create a list of all values that we want to test on for each hyperparameter. Then, we can call the grid search implementation from our favorite package or write the code for grid search by ourselves, as illustrated in the following snippet:</p>
			<pre class="source-code">for n_est in n_estimators:</pre>
			<pre class="source-code">          for crit in criterion:</pre>
			<pre class="source-code">          for m_depth in max_depth:</pre>
			<pre class="source-code">          #perform cross-validation here</pre>
			<p>In this example, we create a nested <strong class="source-inline">for</strong> loop consisting of three layers, each for the hyperparameter <a id="_idIndexMarker077"/>in our search space. To perform a grid search in general, do the following:</p>
			<ol>
				<li value="1">Split the original full data into train and test sets.</li>
				<li>Define the hyperparameter space.</li>
				<li>Construct a nested for loop of H layers, where H is the number of hyperparameters in the space.</li>
				<li>Within each loop, do the following:<ul><li>Perform cross-validation on the train set</li><li>Store the cross-validation score along with the hyperparameter combination in a data structure—for example, a dictionary</li></ul></li>
				<li>Train on the full training set using the best hyperparameter combination.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>As you can see from <a id="_idIndexMarker078"/>the detailed steps on how to perform a grid search, this method is actually a <em class="italic">brute-force</em> method since we have to test all possible combinations of the predefined hyperparameter space. That's why it is very important to have a proper or <em class="italic">well-defined hyperparameter space</em>. If not, then we will waste a lot of time testing all of the combinations.</p>
			<p>Here is a list of <a id="_idIndexMarker079"/>pros and <a id="_idIndexMarker080"/>cons of the grid search hyperparameter-tuning method:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18753_03_003.jpg" alt="Figure 3.3 – Grid search: pros and cons&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Grid search: pros and cons</p>
			<p>The <em class="italic">COD</em> in <em class="italic">Table 3.2</em> means that adding another value to the hyperparameter space will <em class="italic">exponentially increase</em> the experiment time. Let's use the preceding example where we performed hyperparameter tuning on a random forest. In our initial hyperparameter space, there are <img src="image/Formula_B18753_03_001.png" alt=""/> combinations we have to test. If we add just another value to our space—let's say we add <strong class="source-inline">30</strong> to the <strong class="source-inline">max_depth</strong> list—there will be <img src="image/Formula_B18753_03_002.png" alt=""/> combinations or an additional <strong class="source-inline">10</strong> combinations that we have to test. This exponential <a id="_idIndexMarker081"/>behavior will even become more apparent when we have a bigger hyperparameter space! Sadly, it is also possible that after defining a big hyperparameter space and spending a long time performing hyperparameter tuning, we can still <em class="italic">miss better hyperparameter values</em> since they are located outside of the predefined space!</p>
			<p>In this section, we have learned what grid search is, how it works, and what the pros and cons are. In the next section, we will discuss the last hyperparameter-tuning method that is categorized in the exhaustive search group: the random search method.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor034"/>Understanding random search</h1>
			<p><strong class="bold">Random search</strong> is the <a id="_idIndexMarker082"/>third and the last hyperparameter-tuning method that belongs in the exhaustive search group. It is a <em class="italic">simple method but works surprisingly well</em> in practice. As implied by its name, random search works by <em class="italic">randomly selecting hyperparameter values</em> in each iteration. There's nothing more to it. The selected set of hyperparameters in the previous iteration will not impact how the method selects another set of hyperparameters in the following iterations. That's why random search is also categorized as an <em class="italic">uninformed search</em> method. </p>
			<p>You can see an illustration of the random search method in the following diagram:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18753_03_004.jpg" alt="Figure 3.4 – Random search illustration&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Random search illustration</p>
			<p>Random search usually works better than grid search when we have little or no idea of the proper hyperparameter space for our case, and this applies most of the time. Compared to grid search, random search is also more efficient in terms of computing cost and in finding the optimal set of hyperparameters. This is because we do not have to test each of the hyperparameter combinations; we can just let it run stochastically—or, in layman's terms, we can just <em class="italic">let luck play its part</em>. </p>
			<p>You may wonder <a id="_idIndexMarker083"/>how picking a random set of hyperparameters can lead to a better tuning result compared to grid search most of the time. It is actually not the case if the predefined hyperparameter space is exactly the same as the one we provide to the grid search method. We have to provide a <em class="italic">bigger hyperparameter space</em> in order to support random search to play its role. A bigger search space doesn't always mean we have to <em class="italic">increase the dimensionality</em>, either by widening existing hyperparameters' range or adding new hyperparameters. We can also create a bigger hyperparameter space by <em class="italic">adding granularity</em> to it. </p>
			<p>It is also worth noting that, unlike grid search, which doesn't require defining the hyperparameter's distribution when defining a search space, in random search, it is suggested to <em class="italic">define the distribution of each hyperparameter</em>. In some package implementations, if you do not specify the distribution, it will default to the uniform distribution. We will discuss more on the implementation part from <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em> to <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a><em class="italic">, Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em></p>
			<p>Let's use a similar example to what we saw in the <em class="italic">Understanding grid search</em> section to get a better understanding of how random search works. Apart from focusing on the number of estimators, splitting criterion, and maximum tree depth, we will also add a minimum samples split hyperparameter to our space. Unlike grid search, we have to also provide a distribution of each of the hyperparameters when defining a search space. Let's say <a id="_idIndexMarker084"/>we define the hyperparameter space as follows:</p>
			<ul>
				<li>Number of estimators: <strong class="source-inline">n_estimators = randint(25,200)</strong></li>
				<li>Splitting criterion: <strong class="source-inline">criterion = ["gini", "entropy"]</strong></li>
				<li>Maximum depth: <strong class="source-inline">max_depth = [3, 5, 10, 15, 20, None]</strong></li>
				<li>Minimum samples split: <strong class="source-inline">min_samples_split = truncnorm(a=1, b=5, loc=2, scale=0.5)</strong></li>
			</ul>
			<p>As you can see, compared to the search space in the <em class="italic">Understanding grid search</em> section, we are <em class="italic">increasing the size of the space</em> by adding granularity and adding a new hyperparameter. We add granularity for the <strong class="source-inline">n_estimators</strong> hyperparameter by utilizing the <strong class="source-inline">randint</strong> uniform random integer distribution, ranging from <strong class="source-inline">25</strong> to <strong class="source-inline">200</strong>. This means we can test any value between <strong class="source-inline">25</strong> and <strong class="source-inline">200</strong>, where all of them will have the same probability of being tested. </p>
			<p>Apart from increasing the size of the search space by adding granularity, we also add a new hyperparameter called <strong class="source-inline">min_samples_split</strong>. This hyperparameter has the <strong class="source-inline">truncnorm</strong> distribution or <strong class="bold">truncated normal distribution</strong>, which basically—as implied by its <a id="_idIndexMarker085"/>name—is a modified normal distribution bounded on a particular range. In this case, the range is bounded on a range from <strong class="source-inline">a=1</strong> and <strong class="source-inline">b=5</strong>, with a mean of <strong class="source-inline">loc=2</strong> and standard deviation of <strong class="source-inline">scale=0.5</strong>.</p>
			<p>As for <strong class="source-inline">criterion</strong> and <strong class="source-inline">max_depth</strong>, we are still using the same configuration as the previous search space. Note that <em class="italic">not specifying any distribution</em> means we are applying uniform distribution to the hyperparameter, where all values will have the same probability of being tested. For now, you don't have to worry about what are the available distributions and how to implement them, since we will also discuss them from <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em> to <a href="B18753_10_ePub.xhtml#_idTextAnchor092"><em class="italic">Chapter 10</em></a><em class="italic">, Advanced Hyperparameter Tuning with DEAP and Microsoft NNI</em>.</p>
			<p>In random search, apart from the need to define a hyperparameter space, we also need to define a hyperparameter for this method itself, which <a id="_idIndexMarker086"/>is called the <strong class="bold">number of trials</strong>. This hyperparameter will <em class="italic">control how many trials or iterations</em> we want to perform on the predefined search space. This hyperparameter is needed since we <em class="italic">are not aiming to test all possible combinations</em> in the space; if we were, then it would be the same grid search method. It is also worth noting that since this method has a stochastic nature, we also need to specify a <em class="italic">random seed</em> to get the exact same result every time we run the code.</p>
			<p>Unlike grid search, it is quite cumbersome to implement this method from scratch, although it is possible to do so. Therefore, many packages support the implementation of the random search method. Regardless of the implementation variations, in general, random search <a id="_idIndexMarker087"/>works like this:</p>
			<ol>
				<li value="1">Split the original full data into train and test sets.</li>
				<li>Define the number of trials and a random seed.</li>
				<li>Define a hyperparameter space with the accompanied distributions.</li>
				<li>Generate an iterator consisting of random hyperparameter combinations with the number of elements equal to the defined number of trials in Step 2.</li>
				<li>Loop through the iterator, where the following actions will be performed within each loop:<ul><li>Getting the hyperparameter combination for this trial from the iterator</li><li>Performing cross-validation on the train set</li><li>Storing the cross-validation score along with the hyperparameter combination in a data structure—for example, a dictionary</li></ul></li>
				<li>Train on the full training set using the best hyperparameter combination.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>Please note that it is guaranteed there is <em class="italic">no duplicate</em> in the generated hyperparameter combinations in <em class="italic">Step 4</em>.  </p>
			<p>Here is a list of pros and cons of the random search hyperparameter-tuning method:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18753_03_005.jpg" alt="Figure 3.5 – Random search: pros and cons&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Random search: pros and cons</p>
			<p>The random <a id="_idIndexMarker088"/>search produces high variance during the process due <a id="_idIndexMarker089"/>to the property of <em class="italic">uninformed search</em> methods. There is no way for the random search to learn from past experiences so that it can learn better and be more effective in the next iterations. In <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a><em class="italic">, Exploring Multi-Fidelity Optimization</em>, we will learn other <em class="italic">variations of grid search and random search</em> that are categorized as informed search methods. </p>
			<p>In this section, we have learned all you need to know about random search, starting from what it is, how it works, what makes it different from grid search, and the pros and cons of this method. </p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Summary</h1>
			<p>In this chapter, we have discussed the first out of four groups of hyperparameter-tuning methods, called the exhaustive search group. We have discussed manual search, grid search, and random search. We not only discussed the definition of those methods, but also how those methods work at both a high level and a technical level, and what are the pros and cons for each of them. From now on, you should be able to explain these exhaustive search methods with confidence when someone asks you about them and apply all of the exhaustive search methods with high confidence in practice.</p>
			<p>In the next chapter, we will start discussing Bayesian optimization, the second group of hyperparameter-tuning methods. The goal of the next chapter is similar to this chapter, which is to give a better understanding of methods belonging to the Bayesian optimization group so that you can utilize those methods with high confidence in practice.</p>
		</div>
	</body></html>