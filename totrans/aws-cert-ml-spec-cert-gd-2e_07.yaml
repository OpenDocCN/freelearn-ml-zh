- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating and Optimizing Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is now time to learn how to evaluate and optimize machine learning models.
    During the process of modeling, or even after model completion, you might want
    to understand how your model is performing. Each type of model has its own set
    of metrics that can be used to evaluate performance, and that is what you are
    going to study in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from model evaluation, as a data scientist, you might also need to improve
    your model’s performance by tuning the hyperparameters of your algorithm. You
    will take a look at some nuances of this modeling task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing model evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, time to rock it!
  prefs: []
  type: TYPE_NORMAL
- en: Introducing model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several different scenarios in which you might want to evaluate model
    performance. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You are creating a model and testing different approaches and/or algorithms.
    Therefore, you need to compare these models to select the best one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have just completed your model and you need to document your work, which
    includes specifying the model’s performance metrics that you got from the modeling
    phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your model is running in a production environment, and you need to track its
    performance. If you encounter model drift, then you might want to retrain the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The term model drift is used to refer to the problem of model deterioration.
    When you are building a machine learning model, you must use data to train the
    algorithm. This set of data is known as training data, and it reflects the business
    rules at a particular point in time. If these business rules change over time,
    your model will probably fail to adapt to those changes. This is because it was
    trained on top of another dataset, which was reflecting another business scenario.
    To solve this problem, you must retrain the model so that it can consider the
    rules of the new business scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model evaluations are commonly inserted in the context of testing. You have
    learned about holdout validation and cross-validation before. However, both testing
    approaches share the same requirement: they need a metric in order to evaluate
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: These metrics are specific to the problem domain. For example, there are specific
    metrics for regression models, classification models, clustering, natural language
    processing, and more. Therefore, during the design of your testing approach, you
    have to consider what type of model you are building in order to define the evaluation
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, you will take a look at the most important metrics
    and concepts that you should know to evaluate your models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification models are one of the most traditional classes of problems that
    you might face, either during the exam or during your journey as a data scientist.
    A very important artifact that you might want to generate during the classification
    model evaluation is known as a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix compares your model predictions against the real values
    of each class under evaluation. *Figure 7**.1* shows what a confusion matrix looks
    like in a binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A confusion matrix](img/B21197_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – A confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'There are the following components in a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TP: This is the number of true positive cases. Here, you have to count the
    number of cases that have been predicted as true and are, indeed, true. For example,
    in a fraud detection system, this would be the number of fraudulent transactions
    that were correctly predicted as fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TN: This is the number of true negative cases. Here, you have to count the
    number of cases that have been predicted as false and are, indeed, false. For
    example, in a fraud detection system, this would be the number of non-fraudulent
    transactions that were correctly predicted as not fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FN: This is the number of false negative cases. Here, you have to count the
    number of cases that have been predicted as false but are, instead, true. For
    example, in a fraud detection system, this would be the number of fraudulent transactions
    that were wrongly predicted as not fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FP: This is the number of false positive cases. Here, you have to count the
    number of cases that have been predicted as true but are, instead, false. For
    example, in a fraud detection system, this would be the number of non-fraudulent
    transactions that were wrongly predicted as fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a perfect scenario, your confusion matrix will have only true positive and
    true negative cases, which means that your model has an accuracy of 100%. In practical
    terms, if that type of scenario occurs, you should be skeptical instead of happy,
    since it is expected that your model will contain some level of errors. If your
    model does not contain errors, you are likely to be suffering from overfitting
    issues, so be careful.
  prefs: []
  type: TYPE_NORMAL
- en: Once false negatives and false positives are expected, the best you can do is
    prioritize one of them. For example, you can reduce the number of false negatives
    by increasing the number of false positives and vice versa. This is known as the
    precision versus recall trade-off. Let’s take a look at these metrics next.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting metrics from a confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest metric that can be extracted from a confusion matrix is known
    as **accuracy**. Accuracy is given by the following equation, as shown in *Figure
    7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Formula for accuracy](img/B21197_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Formula for accuracy
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of demonstration, *Figure 7**.3* shows a confusion matrix with
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – A confusion matrix filled with some examples](img/B21197_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – A confusion matrix filled with some examples
  prefs: []
  type: TYPE_NORMAL
- en: According to *Figure 7**.3*, the accuracy would be (100 + 90) / 210, which is
    equal to 0.90\. There is a common issue that occurs when utilizing an accuracy
    metric, which is related to the balance of each class. Problems with highly imbalanced
    classes, such as 99% positive cases and 1% negative cases, will impact the accuracy
    score and make it useless.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if your training data has 99% positive cases (the majority class),
    your model is likely to correctly classify most of the positive cases but work
    badly in the classification of negative cases (the minority class). The accuracy
    will be very high (due to the correctness of the classification of the positive
    cases), regardless of the bad results in the minority class classification.
  prefs: []
  type: TYPE_NORMAL
- en: The point is that on highly imbalanced problems, you usually have more interest
    in correctly classifying the minority class, not the majority class. That is the
    case in most fraud detection systems, for example, where the minority class corresponds
    to fraudulent cases. For imbalanced problems, you should look for other types
    of metrics, which you will learn about next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important metric that you can extract from a confusion matrix is known
    as recall, which is given by the following equation, as shown in *Figure 7**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Formula for recall](img/B21197_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Formula for recall
  prefs: []
  type: TYPE_NORMAL
- en: In other words, recall is the number of true positives over the total number
    of positive cases. Recall is also known as sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the values in *Figure 7**.3*, recall is given by 100 / 112, which is equal
    to 0.89\. Precision, on the other hand, is given by the following formula, as
    shown in *Figure 7**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Formula for precision](img/B21197_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Formula for precision
  prefs: []
  type: TYPE_NORMAL
- en: In other words, precision is the number of true positives over the total number
    of predicted positive cases. Precision is also known as positive predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: With the values in *Figure 7**.3*, precision is given by 100 / 108, which is
    equal to 0.93\. In general, you can increase precision at the cost of decreasing
    recall and vice versa. There is another model evaluation artifact in which you
    can play around with this precision versus recall trade-off. It is known as a
    precision-recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision-recall curves summarize the precision versus recall trade-off by
    using different probability thresholds. For example, the default threshold is
    0.5, where any prediction above 0.5 will be considered true; otherwise, it is
    false. You can change the default threshold according to your need so that you
    can prioritize recall or precision. *Figure 7**.6* shows an example of a precision-recall
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – A precision-recall curve](img/B21197_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – A precision-recall curve
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 7**.6*, increasing the precision will reduce the amount
    of recall and vice versa. *Figure 7**.6* shows the precision/recall for each threshold
    for a **gradient boosting model** (as shown by the orange line) compared to a
    **no-skill model** (as shown by the blue dashed line). A perfect model will approximate
    the curve to the point (1,1), forming a squared corner on the top right-hand side
    of the chart.
  prefs: []
  type: TYPE_NORMAL
- en: Another visual analysis you can use on top of confusion matrixes is known as
    a **Receiver Operating Characteristic** (**ROC**) curve. ROC curves summarize
    the trade-off between the **true positive rate** and the **false positive rate**
    according to different thresholds, as in the precision-recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: You already know about the true positive rate, or sensitivity, which is the
    same as what you have just learned about with the precision-recall curve. The
    other dimension of an ROC curve is the **false positive rate**, which is the number
    of false positives over the number of false positives plus true negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In literature, you might find the false positive rate referred to as **inverted
    specificity**, represented by *1 – specificity*. Specificity is given as the number
    of true negatives over the number of true negatives plus false positives. Furthermore,
    false-positive rates or inverted specificity are the same. *Figure 7**.7* shows
    what an ROC curve looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – ROC curve](img/B21197_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: A perfect model will approximate the curve to the point (0,1), forming a squared
    corner on the top left-hand side of the chart. The orange line represents the
    trade-off between the true positive rate and the false positive rate of a gradient-boosting
    classifier. The dashed blue line represents a no-skill model, which cannot predict
    the classes properly.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, you can use ROC curves for fairly balanced datasets and precision-recall
    curves for moderate to imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing precision and recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, you might want to use a metric that summarizes precision and recall,
    instead of prioritizing one over the other. Two very popular metrics can be used
    to summarize precision and recall: **F1 score** and **Area Under** **Curve** (**AUC**).'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score, also known as the **F-measure**, computes the harmonic mean of
    precision and recall. AUC summarizes the approximation of the area under the precision-recall
    curve.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of this section on classification metrics. Let’s now
    take a look at the evaluation metrics for regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression models are quite different from classification models since the outcome
    of the model is a continuous number. Therefore, the metrics around regression
    models aim to monitor the difference between real and predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to check the difference between a predicted value (*yhat*)
    and its actual value (*y*) is by performing a simple subtraction operation, where
    the error will be equal to the absolute value of *yhat – y*. This metric is known
    as the **Mean Absolute** **Error** (**MAE**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you usually have to evaluate the error of each prediction, *i*, you have
    to take the mean value of the errors. *Figure 7**.8  *depicts formula that shows
    how this error can be formally defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Formula for error of each prediction](img/B21197_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Formula for error of each prediction
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, you might want to penalize bigger errors over smaller errors. To
    achieve this, you can use another metric, known as the **Mean Squared Error**
    (**MSE**). The MSE will square each error and return the mean value.
  prefs: []
  type: TYPE_NORMAL
- en: 'By squaring errors, the MSE will penalize bigger ones. *Figure 7**.9* depicts
    formula that shows how the MSE can be formally defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Formula for MSE](img/B21197_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Formula for MSE
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a potential interpretation problem with the MSE. Since it has to compute
    the squared error, it might be difficult to interpret the final results from a
    business perspective. The **Root Mean Squared Error** (**RMSE**) works around
    this interpretation issue, by taking the square root of the MSE. *Figure 7**.10*
    depicts the RMSE equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Formula for RMSE](img/B21197_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Formula for RMSE
  prefs: []
  type: TYPE_NORMAL
- en: The RMSE is one of the most used metrics for regression models, since it can
    penalize larger errors and remains easy to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring other regression metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many more metrics that are suitable for regression problems, in addition
    to the ones that you have just learned. You will not learn about most of them
    here, but you will be introduced to a few more metrics that might be important
    for you to know.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of these metrics is known as the **Mean Absolute Percentage Error** (**MAPE**).
    As the name suggests, the MAPE will compute the absolute percentage error of each
    prediction and then take the average value. *Figure 7**.11* depicts formula that
    shows how this metric is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Formula for MAPE](img/B21197_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Formula for MAPE
  prefs: []
  type: TYPE_NORMAL
- en: The MAPE is broadly used in forecasting models since it is very simple to interpret,
    and it provides a very good sense of how far (or close) the predictions are from
    the actual values (in terms of a percentage).
  prefs: []
  type: TYPE_NORMAL
- en: You have now completed this section on regression metrics. Next, you will learn
    about model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you know, understanding evaluation metrics is very important in order to
    measure your model’s performance and document your work. In the same way, when
    you want to optimize your current models, evaluating metrics also plays a very
    important role in defining the baseline performance that you want to challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The process of model optimization consists of finding the best configuration
    (also known as hyperparameters) of the machine learning algorithm for a particular
    data distribution. You do not want to find hyperparameters that overfit the training
    data, in the same way that you do not want to find hyperparameters that underfit
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about overfitting and underfitting in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*. In the same chapter, you also learned how to avoid
    these two types of modeling issues.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn about some techniques that you can use to find
    the best configuration for a particular algorithm and dataset. You can combine
    these techniques of model optimization with other methods, such as cross-validation,
    to find the best set of hyperparameters for your model and avoid fitting issues.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that you do not want to optimize your algorithm to the underlying
    training data but to the data distribution behind the training data, so that your
    model will work on the training data as well as the production data (the data
    that has never been exposed to your model during the training process). A machine
    learning model that works only on the training data is useless. That is why combining
    model-tuning techniques (such as the ones you will learn about next) with sampling
    techniques (such as cross-validation) makes all the difference when it comes to
    creating a good model.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Grid search** is probably the most popular method for model optimization.
    It consists of testing different combinations of the algorithm and selecting the
    best one. Here, there are two important points that you need to pay attention
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: How to define the best configuration of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many configurations should be tested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best model is defined based on an evaluation metric. In other words, you
    have to first define which metric you are going to use to evaluate the model’s
    performance. Secondly, you have to define how you are going to evaluate the model.
    Usually, cross-validation is used to evaluate the model on multiple datasets that
    have never been used for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the number of combinations/configurations, this is the most challenging
    part when playing with grid search. Each hyperparameter of an algorithm may have
    multiple or, sometimes, infinite possibilities of values. If you consider that
    an algorithm will usually have multiple hyperparameters, this becomes a function
    with quadratic cost, where the number of unique combinations to test is given
    as *the number of values of hyperparameter a * the number of values of hyperparameter
    b * the number of values of hyperparameter i*. *Table 7.1* shows how you could
    potentially set a grid search configuration for a decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Criterion** | **Max depth** | **Min** **samples leaf** |'
  prefs: []
  type: TYPE_TB
- en: '| Gini, Entropy | 2, 5 =, 10 | 10, 20, 30 |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Grid search configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Table 7.1*, there are three hyperparameters: **Criterion**, **Max depth**,
    and **Min samples leaf**. Each of these hyperparameters has a list of values for
    testing. That means by the end of the grid search process, you will have tested
    18 models (2 * 3 * 3), where only the best one will be selected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might have noticed, all the different combinations of those three hyperparameters
    will be tested. For example, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Criterion = Gini, Max depth = 2, Min samples leaf = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Criterion = Gini, Max depth = 5, Min samples leaf = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Criterion = Gini, Max depth = 10, Min samples leaf = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some other questions that you might have could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Considering that a particular algorithm might have several hyperparameters,
    which ones should I tune?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering that a particular hyperparameter might accept infinite values, which
    values should I test?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are good questions, and grid search will not give you a straight answer
    for them. Instead, this is closer to an empirical process, where you have to test
    as much as you need to achieve your target performance.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Of course, grid search cannot guarantee that you will come up with your target
    performance. That depends on the algorithm and the training data.
  prefs: []
  type: TYPE_NORMAL
- en: A common practice, though, is to define the values for testing by using a **linear
    space** or **log space**, where you can manually set the limits of the hyperparameter
    you want to test and the number of values for testing. Then, the intermediate
    values will be drawn by a linear or log function.
  prefs: []
  type: TYPE_NORMAL
- en: As you might imagine, grid search can take a long time to run. A number of alternative
    methods have been proposed to work around this time issue. **Random search** is
    one of them, where the list of values for testing is randomly selected from the
    search space.
  prefs: []
  type: TYPE_NORMAL
- en: Another method that has gained rapid adoption across the industry is known as
    **Bayesian optimization**. Algorithm optimizations, such as **gradient descent**,
    try to find what is called the **global minima**, by calculating derivatives of
    the cost function. The global minima are the points where you find the algorithm
    configuration with the least associated cost.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian optimization is useful when calculating derivatives is not an option.
    So you can use the **Bayes theorem**, a probabilistic approach, to find the global
    minima using the smallest number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, Bayesian optimization will start testing the entire search
    space to find the most promising set of optimal hyperparameters. Then, it will
    perform more tests specifically in the place where the global minima are likely
    to be.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the main metrics for model evaluation. You
    started with the metrics for classification problems and then you moved on to
    the metrics for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of classification metrics, you have been introduced to the well-known
    confusion matrix, which is probably the most important artifact for performing
    a model evaluation on classification models.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about true positives, true negatives, false positives, and false
    negatives. Then, you learned how to combine these components to extract other
    metrics, such as accuracy, precision, recall, the F1 score, and AUC.
  prefs: []
  type: TYPE_NORMAL
- en: You then went even deeper and learned about ROC curves, as well as precision-recall
    curves. You learned that you can use ROC curves to evaluate fairly balanced datasets
    and precision-recall curves for moderate to imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, when you are dealing with imbalanced datasets, remember that using
    accuracy might not be a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of regression metrics, you learned that the most popular ones, and
    the ones most likely to be present in the *AWS Machine Learning Specialty* exam,
    are the MAE, MSE, RMSE, and MAPE. Make sure you know the basics of each of them
    before taking the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about methods for hyperparameter optimization, such as
    grid search and Bayesian optimization. In the next chapter, you will have a look
    at AWS application services for AI/ML. But first, take a moment to practice these
    questions about model evaluation and model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH07](https://packt.link/MLSC01E2_CH07).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 7**.12*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.12 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 7**.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Chapter Review Questions for Chapter 7](img/B21197_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Chapter Review Questions for Chapter 7
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
