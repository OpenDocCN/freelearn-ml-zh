- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Evaluating and Optimizing Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估和优化模型
- en: It is now time to learn how to evaluate and optimize machine learning models.
    During the process of modeling, or even after model completion, you might want
    to understand how your model is performing. Each type of model has its own set
    of metrics that can be used to evaluate performance, and that is what you are
    going to study in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候学习如何评估和优化机器学习模型了。在建模过程中，甚至在模型完成之后，你可能想要了解你的模型表现如何。每种类型的模型都有自己的一套可以用来评估性能的指标，这就是你将在本章学习的内容。
- en: Apart from model evaluation, as a data scientist, you might also need to improve
    your model’s performance by tuning the hyperparameters of your algorithm. You
    will take a look at some nuances of this modeling task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型评估之外，作为一名数据科学家，你可能还需要通过调整算法的超参数来提高模型性能。你将了解这个建模任务的某些细微差别。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing model evaluation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍模型评估
- en: Evaluating classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: Evaluating regression models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Model optimization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型优化
- en: Alright, time to rock it!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，是时候摇滚起来啦！
- en: Introducing model evaluation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍模型评估
- en: 'There are several different scenarios in which you might want to evaluate model
    performance. Some of them are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的场景，你可能想要评估模型性能。以下是一些例子：
- en: You are creating a model and testing different approaches and/or algorithms.
    Therefore, you need to compare these models to select the best one.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在创建一个模型并测试不同的方法和/或算法。因此，你需要比较这些模型以选择最佳模型。
- en: You have just completed your model and you need to document your work, which
    includes specifying the model’s performance metrics that you got from the modeling
    phase.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你刚刚完成了你的模型，你需要记录你的工作，这包括指定你在建模阶段得到的模型性能指标。
- en: Your model is running in a production environment, and you need to track its
    performance. If you encounter model drift, then you might want to retrain the
    model.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的模型正在生产环境中运行，你需要跟踪其性能。如果你遇到模型漂移，那么你可能需要重新训练模型。
- en: Important note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The term model drift is used to refer to the problem of model deterioration.
    When you are building a machine learning model, you must use data to train the
    algorithm. This set of data is known as training data, and it reflects the business
    rules at a particular point in time. If these business rules change over time,
    your model will probably fail to adapt to those changes. This is because it was
    trained on top of another dataset, which was reflecting another business scenario.
    To solve this problem, you must retrain the model so that it can consider the
    rules of the new business scenario.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “模型漂移”这个术语用来指代模型退化的问题。当你构建机器学习模型时，你必须使用数据来训练算法。这组数据被称为训练数据，它反映了特定时间点的业务规则。如果这些业务规则随时间变化，你的模型可能无法适应这些变化。这是因为它是基于另一个数据集进行训练的，该数据集反映了另一个业务场景。为了解决这个问题，你必须重新训练模型，以便它能够考虑新业务场景的规则。
- en: 'Model evaluations are commonly inserted in the context of testing. You have
    learned about holdout validation and cross-validation before. However, both testing
    approaches share the same requirement: they need a metric in order to evaluate
    performance.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估通常在测试的上下文中进行。你之前已经学过保留验证和交叉验证。然而，这两种测试方法都有相同的要求：它们需要一个指标来评估性能。
- en: These metrics are specific to the problem domain. For example, there are specific
    metrics for regression models, classification models, clustering, natural language
    processing, and more. Therefore, during the design of your testing approach, you
    have to consider what type of model you are building in order to define the evaluation
    metrics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标是特定于问题域的。例如，回归模型、分类模型、聚类、自然语言处理等领域都有特定的指标。因此，在设计你的测试方法时，你必须考虑你正在构建哪种类型的模型，以便定义评估指标。
- en: In the following sections, you will take a look at the most important metrics
    and concepts that you should know to evaluate your models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将了解评估模型时应该知道的最重要指标和概念。
- en: Evaluating classification models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: Classification models are one of the most traditional classes of problems that
    you might face, either during the exam or during your journey as a data scientist.
    A very important artifact that you might want to generate during the classification
    model evaluation is known as a confusion matrix.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型是你可能会遇到的最传统的模型类别之一，无论是在考试中还是在作为数据科学家的旅程中。在分类模型评估期间你可能想要生成的非常重要的一项工具被称为混淆矩阵。
- en: 'A confusion matrix compares your model predictions against the real values
    of each class under evaluation. *Figure 7**.1* shows what a confusion matrix looks
    like in a binary classification problem:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵比较你的模型预测与每个评估类别的真实值。*图7.1*显示了在二元分类问题中混淆矩阵的形状：
- en: '![Figure 7.1 – A confusion matrix](img/B21197_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 混淆矩阵](img/B21197_07_01.jpg)'
- en: Figure 7.1 – A confusion matrix
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 混淆矩阵
- en: 'There are the following components in a confusion matrix:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵有以下组成部分：
- en: 'TP: This is the number of true positive cases. Here, you have to count the
    number of cases that have been predicted as true and are, indeed, true. For example,
    in a fraud detection system, this would be the number of fraudulent transactions
    that were correctly predicted as fraud.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TP：这是真正例的数量。在这里，你需要计算被预测为真且确实为真的案例数量。例如，在一个欺诈检测系统中，这将是正确预测为欺诈的欺诈交易数量。
- en: 'TN: This is the number of true negative cases. Here, you have to count the
    number of cases that have been predicted as false and are, indeed, false. For
    example, in a fraud detection system, this would be the number of non-fraudulent
    transactions that were correctly predicted as not fraud.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TN：这是真正例的数量。在这里，你需要计算被预测为假且确实为假的案例数量。例如，在一个欺诈检测系统中，这将是被正确预测为非欺诈的非欺诈交易数量。
- en: 'FN: This is the number of false negative cases. Here, you have to count the
    number of cases that have been predicted as false but are, instead, true. For
    example, in a fraud detection system, this would be the number of fraudulent transactions
    that were wrongly predicted as not fraud.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FN：这是假负例的数量。在这里，你需要计算被预测为假但实际上是真的案例数量。例如，在一个欺诈检测系统中，这将是被错误预测为非欺诈的欺诈交易数量。
- en: 'FP: This is the number of false positive cases. Here, you have to count the
    number of cases that have been predicted as true but are, instead, false. For
    example, in a fraud detection system, this would be the number of non-fraudulent
    transactions that were wrongly predicted as fraud.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP：这是假正例的数量。在这里，你需要计算被预测为真但实际上是假的案例数量。例如，在一个欺诈检测系统中，这将是被错误预测为欺诈的非欺诈交易数量。
- en: In a perfect scenario, your confusion matrix will have only true positive and
    true negative cases, which means that your model has an accuracy of 100%. In practical
    terms, if that type of scenario occurs, you should be skeptical instead of happy,
    since it is expected that your model will contain some level of errors. If your
    model does not contain errors, you are likely to be suffering from overfitting
    issues, so be careful.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，你的混淆矩阵将只有真正例和真正例，这意味着你的模型准确率为100%。从实际角度来说，如果出现这种情况，你应该持怀疑态度而不是高兴，因为预期你的模型将包含一定程度的错误。如果你的模型不包含错误，你很可能是过度拟合问题，所以请小心。
- en: Once false negatives and false positives are expected, the best you can do is
    prioritize one of them. For example, you can reduce the number of false negatives
    by increasing the number of false positives and vice versa. This is known as the
    precision versus recall trade-off. Let’s take a look at these metrics next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预期到假负例和假正例，你能做的最好的事情就是优先考虑其中之一。例如，你可以通过增加假正例的数量来减少假负例的数量，反之亦然。这被称为精确率与召回率的权衡。让我们接下来看看这些指标。
- en: Extracting metrics from a confusion matrix
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从混淆矩阵中提取指标
- en: 'The simplest metric that can be extracted from a confusion matrix is known
    as **accuracy**. Accuracy is given by the following equation, as shown in *Figure
    7**.2*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从混淆矩阵中提取的最简单指标被称为**准确率**。准确率由以下方程给出，如*图7.2*所示：
- en: '![Figure 7.2 – Formula for accuracy](img/B21197_07_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 准确率的公式](img/B21197_07_02.jpg)'
- en: Figure 7.2 – Formula for accuracy
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 准确率的公式
- en: For the sake of demonstration, *Figure 7**.3* shows a confusion matrix with
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，*图7.3*显示了包含数据的混淆矩阵。
- en: '![Figure 7.3 – A confusion matrix filled with some examples](img/B21197_07_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 填充了一些示例的混淆矩阵](img/B21197_07_03.jpg)'
- en: Figure 7.3 – A confusion matrix filled with some examples
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 填充了一些示例的混淆矩阵
- en: According to *Figure 7**.3*, the accuracy would be (100 + 90) / 210, which is
    equal to 0.90\. There is a common issue that occurs when utilizing an accuracy
    metric, which is related to the balance of each class. Problems with highly imbalanced
    classes, such as 99% positive cases and 1% negative cases, will impact the accuracy
    score and make it useless.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**图7.3**，准确度将是(100 + 90) / 210，等于0.90。当使用准确度指标时，会出现一个常见问题，这与每个类别的平衡有关。高度不平衡的类别问题，如99%的正例和1%的负例，将影响准确度得分并使其无用。
- en: For example, if your training data has 99% positive cases (the majority class),
    your model is likely to correctly classify most of the positive cases but work
    badly in the classification of negative cases (the minority class). The accuracy
    will be very high (due to the correctness of the classification of the positive
    cases), regardless of the bad results in the minority class classification.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的训练数据有99%的正例（多数类），你的模型可能正确分类大多数正例，但在负例（少数类）的分类上表现不佳。准确度将非常高（由于正例分类的正确性），而不管少数类分类的结果如何。
- en: The point is that on highly imbalanced problems, you usually have more interest
    in correctly classifying the minority class, not the majority class. That is the
    case in most fraud detection systems, for example, where the minority class corresponds
    to fraudulent cases. For imbalanced problems, you should look for other types
    of metrics, which you will learn about next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重点是，在高度不平衡的问题上，你通常更感兴趣的是正确分类少数类，而不是多数类。这在大多数欺诈检测系统中都是如此，例如，少数类对应于欺诈案例。对于不平衡问题，你应该寻找其他类型的指标，你将在下一节中了解到。
- en: 'Another important metric that you can extract from a confusion matrix is known
    as recall, which is given by the following equation, as shown in *Figure 7**.4*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中可以提取的另一个重要指标称为召回率，其公式如下，如**图7.4**所示：
- en: '![Figure 7.4 – Formula for recall](img/B21197_07_04.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 召回率公式](img/B21197_07_04.jpg)'
- en: Figure 7.4 – Formula for recall
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 召回率公式
- en: In other words, recall is the number of true positives over the total number
    of positive cases. Recall is also known as sensitivity.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，召回率是真实正例数除以正例总数。召回率也称为灵敏度。
- en: 'With the values in *Figure 7**.3*, recall is given by 100 / 112, which is equal
    to 0.89\. Precision, on the other hand, is given by the following formula, as
    shown in *Figure 7**.5*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图7.3**.3中的数值，召回率由100 / 112给出，等于0.89。另一方面，精确度由以下公式给出，如**图7.5**所示：
- en: '![Figure 7.5 – Formula for precision](img/B21197_07_05.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 精确度公式](img/B21197_07_05.jpg)'
- en: Figure 7.5 – Formula for precision
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 精确度公式
- en: In other words, precision is the number of true positives over the total number
    of predicted positive cases. Precision is also known as positive predictive power.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，精确度是真实正例数除以预测正例总数。精确度也被称为阳性预测力。
- en: With the values in *Figure 7**.3*, precision is given by 100 / 108, which is
    equal to 0.93\. In general, you can increase precision at the cost of decreasing
    recall and vice versa. There is another model evaluation artifact in which you
    can play around with this precision versus recall trade-off. It is known as a
    precision-recall curve.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图7.3**中的数值，精确度由100 / 108给出，等于0.93。一般来说，你可以通过降低召回率来提高精确度，反之亦然。还有一个模型评估的指标，你可以在这个精确度与召回率的权衡中玩转。它被称为精确度-召回率曲线。
- en: 'Precision-recall curves summarize the precision versus recall trade-off by
    using different probability thresholds. For example, the default threshold is
    0.5, where any prediction above 0.5 will be considered true; otherwise, it is
    false. You can change the default threshold according to your need so that you
    can prioritize recall or precision. *Figure 7**.6* shows an example of a precision-recall
    curve:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线通过使用不同的概率阈值来总结精确度与召回率的权衡。例如，默认阈值为0.5，其中任何高于0.5的预测将被认为是真实的；否则，它是假的。你可以根据需要更改默认阈值，以便你可以优先考虑召回率或精确度。**图7.6**显示了精确度-召回率曲线的一个示例：
- en: '![Figure 7.6 – A precision-recall curve](img/B21197_07_06.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 精确度-召回率曲线](img/B21197_07_06.jpg)'
- en: Figure 7.6 – A precision-recall curve
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 精确度-召回率曲线
- en: As you can see in *Figure 7**.6*, increasing the precision will reduce the amount
    of recall and vice versa. *Figure 7**.6* shows the precision/recall for each threshold
    for a **gradient boosting model** (as shown by the orange line) compared to a
    **no-skill model** (as shown by the blue dashed line). A perfect model will approximate
    the curve to the point (1,1), forming a squared corner on the top right-hand side
    of the chart.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*图 7.6*中看到的那样，提高精确率会减少召回量，反之亦然。*图 7.6*显示了**梯度提升模型**（由橙色线表示）与**无技能模型**（由蓝色虚线表示）在每个阈值下的精确率/召回率。一个完美的模型将曲线近似到点（1,1），在图表的右上角形成一个平方角。
- en: Another visual analysis you can use on top of confusion matrixes is known as
    a **Receiver Operating Characteristic** (**ROC**) curve. ROC curves summarize
    the trade-off between the **true positive rate** and the **false positive rate**
    according to different thresholds, as in the precision-recall curve.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在混淆矩阵之上使用另一种视觉分析，称为**接收者操作特征**（**ROC**）曲线。ROC 曲线总结了根据不同阈值，**真正例率**和**假正例率**之间的权衡，就像精确率-召回率曲线一样。
- en: You already know about the true positive rate, or sensitivity, which is the
    same as what you have just learned about with the precision-recall curve. The
    other dimension of an ROC curve is the **false positive rate**, which is the number
    of false positives over the number of false positives plus true negatives.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了真正例率，或称为灵敏度，这与你刚刚在精确率-召回率曲线中学到的是相同的。ROC 曲线的另一个维度是**假正例率**，它是假正例数除以假正例数加上真正例数的数量。
- en: 'In literature, you might find the false positive rate referred to as **inverted
    specificity**, represented by *1 – specificity*. Specificity is given as the number
    of true negatives over the number of true negatives plus false positives. Furthermore,
    false-positive rates or inverted specificity are the same. *Figure 7**.7* shows
    what an ROC curve looks like:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，你可能会发现假正例率被称为**倒数特异性**，表示为*1 – 特异性*。特异性给出的是真正例数除以真正例数加上假正例数的数量。此外，假正例率或倒数特异性是相同的。*图
    7.7*显示了 ROC 曲线的样子：
- en: '![Figure 7.7 – ROC curve](img/B21197_07_07.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – ROC 曲线](img/B21197_07_07.jpg)'
- en: Figure 7.7 – ROC curve
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – ROC 曲线
- en: A perfect model will approximate the curve to the point (0,1), forming a squared
    corner on the top left-hand side of the chart. The orange line represents the
    trade-off between the true positive rate and the false positive rate of a gradient-boosting
    classifier. The dashed blue line represents a no-skill model, which cannot predict
    the classes properly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美的模型将曲线近似到点（0,1），在图表的左上角形成一个平方角。橙色线代表梯度提升分类器的真正例率和假正例率之间的权衡。虚线蓝色线代表一个无技能模型，无法正确预测类别。
- en: To summarize, you can use ROC curves for fairly balanced datasets and precision-recall
    curves for moderate to imbalanced datasets.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你可以使用 ROC 曲线来评估相对平衡的数据集，而使用精确率-召回率曲线来评估中等至不平衡的数据集。
- en: Summarizing precision and recall
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结精确率和召回率
- en: 'Sometimes, you might want to use a metric that summarizes precision and recall,
    instead of prioritizing one over the other. Two very popular metrics can be used
    to summarize precision and recall: **F1 score** and **Area Under** **Curve** (**AUC**).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你可能想使用一个总结精确率和召回率的指标，而不是优先考虑其中一个。有两个非常流行的指标可以用来总结精确率和召回率：**F1 分数**和**曲线下面积**（**AUC**）。
- en: The F1 score, also known as the **F-measure**, computes the harmonic mean of
    precision and recall. AUC summarizes the approximation of the area under the precision-recall
    curve.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数，也称为**F-measure**，计算精确率和召回率的调和平均值。AUC 总结了精确率-召回率曲线下的近似面积。
- en: That brings us to the end of this section on classification metrics. Let’s now
    take a look at the evaluation metrics for regression models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了本节关于分类指标的讨论。现在让我们来看看回归模型的评估指标。
- en: Evaluating regression models
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Regression models are quite different from classification models since the outcome
    of the model is a continuous number. Therefore, the metrics around regression
    models aim to monitor the difference between real and predicted values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型与分类模型有很大不同，因为模型的输出是一个连续的数字。因此，围绕回归模型的指标旨在监控实际值和预测值之间的差异。
- en: The simplest way to check the difference between a predicted value (*yhat*)
    and its actual value (*y*) is by performing a simple subtraction operation, where
    the error will be equal to the absolute value of *yhat – y*. This metric is known
    as the **Mean Absolute** **Error** (**MAE**).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 检查预测值（*yhat*）与其实际值（*y*）之间差异的最简单方法是进行简单的减法运算，其中误差将等于*yhat – y*的绝对值。这个指标被称为**平均绝对误差**（**MAE**）。
- en: 'Since you usually have to evaluate the error of each prediction, *i*, you have
    to take the mean value of the errors. *Figure 7**.8  *depicts formula that shows
    how this error can be formally defined:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你通常必须评估每个预测的误差，*i*，你必须取误差的平均值。*图7.8*展示了如何正式定义此误差：
- en: '![Figure 7.8 – Formula for error of each prediction](img/B21197_07_08.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 每个预测误差的公式](img/B21197_07_08.jpg)'
- en: Figure 7.8 – Formula for error of each prediction
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 每个预测误差的公式
- en: Sometimes, you might want to penalize bigger errors over smaller errors. To
    achieve this, you can use another metric, known as the **Mean Squared Error**
    (**MSE**). The MSE will square each error and return the mean value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你可能想要对较大的误差进行惩罚，而不是较小的误差。为了实现这一点，你可以使用另一个指标，称为**平均平方误差**（**MSE**）。MSE将平方每个误差并返回平均值。
- en: 'By squaring errors, the MSE will penalize bigger ones. *Figure 7**.9* depicts
    formula that shows how the MSE can be formally defined:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过平方误差，MSE将对较大的误差进行惩罚。*图7.9*展示了如何正式定义MSE：
- en: '![Figure 7.9 – Formula for MSE](img/B21197_07_09.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – MSE的公式](img/B21197_07_09.jpg)'
- en: Figure 7.9 – Formula for MSE
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – MSE的公式
- en: 'There is a potential interpretation problem with the MSE. Since it has to compute
    the squared error, it might be difficult to interpret the final results from a
    business perspective. The **Root Mean Squared Error** (**RMSE**) works around
    this interpretation issue, by taking the square root of the MSE. *Figure 7**.10*
    depicts the RMSE equation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MSE存在一个潜在的解读问题。由于它必须计算平方误差，因此从商业角度解读最终结果可能很困难。**均方根误差**（**RMSE**）通过取MSE的平方根来解决这个问题。*图7.10*展示了RMSE的方程：
- en: '![Figure 7.10 – Formula for RMSE](img/B21197_07_10.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – RMSE的公式](img/B21197_07_10.jpg)'
- en: Figure 7.10 – Formula for RMSE
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – RMSE的公式
- en: The RMSE is one of the most used metrics for regression models, since it can
    penalize larger errors and remains easy to interpret.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE是回归模型中最常用的指标之一，因为它可以惩罚较大的误差，同时仍然易于解读。
- en: Exploring other regression metrics
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索其他回归指标
- en: There are many more metrics that are suitable for regression problems, in addition
    to the ones that you have just learned. You will not learn about most of them
    here, but you will be introduced to a few more metrics that might be important
    for you to know.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了你刚刚学到的那些之外，还有很多适合回归问题的指标。你在这里不会学习到它们中的大多数，但你会了解到一些可能对你很重要的额外指标。
- en: 'One of these metrics is known as the **Mean Absolute Percentage Error** (**MAPE**).
    As the name suggests, the MAPE will compute the absolute percentage error of each
    prediction and then take the average value. *Figure 7**.11* depicts formula that
    shows how this metric is computed:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标之一被称为**平均绝对百分比误差**（**MAPE**）。正如其名所示，MAPE将计算每个预测的绝对百分比误差，然后取平均值。*图7.11*展示了如何计算此指标：
- en: '![Figure 7.11 – Formula for MAPE](img/B21197_07_11.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – MAPE的公式](img/B21197_07_11.jpg)'
- en: Figure 7.11 – Formula for MAPE
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – MAPE的公式
- en: The MAPE is broadly used in forecasting models since it is very simple to interpret,
    and it provides a very good sense of how far (or close) the predictions are from
    the actual values (in terms of a percentage).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MAPE在预测模型中得到广泛应用，因为它非常简单易懂，并且可以很好地提供预测值（从百分比的角度）与实际值之间的距离（或接近程度）的感觉。
- en: You have now completed this section on regression metrics. Next, you will learn
    about model optimization.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经完成了关于回归指标的这一部分。接下来，你将学习关于模型优化的内容。
- en: Model optimization
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化
- en: As you know, understanding evaluation metrics is very important in order to
    measure your model’s performance and document your work. In the same way, when
    you want to optimize your current models, evaluating metrics also plays a very
    important role in defining the baseline performance that you want to challenge.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，为了衡量你的模型性能并记录你的工作，理解评估指标非常重要。同样，当你想要优化当前模型时，评估指标在定义你想要挑战的基线性能中也起着非常重要的作用。
- en: The process of model optimization consists of finding the best configuration
    (also known as hyperparameters) of the machine learning algorithm for a particular
    data distribution. You do not want to find hyperparameters that overfit the training
    data, in the same way that you do not want to find hyperparameters that underfit
    the training data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型优化的过程包括为特定数据分布找到机器学习算法的最佳配置（也称为超参数）。你不想找到过度拟合训练数据的超参数，就像你不想找到欠拟合训练数据的超参数一样。
- en: You learned about overfitting and underfitting in [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*,
    Machine Learning Fundamentals*. In the same chapter, you also learned how to avoid
    these two types of modeling issues.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你在[*第一章*](B21197_01.xhtml#_idTextAnchor018)《机器学习基础》中学习了过拟合和欠拟合。在同一章中，你也学习了如何避免这两种建模问题。
- en: In this section, you will learn about some techniques that you can use to find
    the best configuration for a particular algorithm and dataset. You can combine
    these techniques of model optimization with other methods, such as cross-validation,
    to find the best set of hyperparameters for your model and avoid fitting issues.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解一些你可以用来为特定算法和数据集找到最佳配置的技术。你可以将这些模型优化技术与其他方法（如交叉验证）结合起来，以找到最佳的超参数集，避免拟合问题。
- en: Important note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Always remember that you do not want to optimize your algorithm to the underlying
    training data but to the data distribution behind the training data, so that your
    model will work on the training data as well as the production data (the data
    that has never been exposed to your model during the training process). A machine
    learning model that works only on the training data is useless. That is why combining
    model-tuning techniques (such as the ones you will learn about next) with sampling
    techniques (such as cross-validation) makes all the difference when it comes to
    creating a good model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 总是记住，你不想优化你的算法以适应底层的训练数据，而是要适应训练数据背后的数据分布，这样你的模型才能在训练数据和生产数据（在训练过程中从未暴露给模型的那些数据）上工作。一个只在训练数据上工作的机器学习模型是无用的。这就是为什么将模型调整技术（如你接下来将要学习的）与采样技术（如交叉验证）结合起来，在创建一个好的模型时至关重要。
- en: Grid search
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: '**Grid search** is probably the most popular method for model optimization.
    It consists of testing different combinations of the algorithm and selecting the
    best one. Here, there are two important points that you need to pay attention
    to:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格搜索**可能是模型优化中最受欢迎的方法。它包括测试不同的算法组合并选择最佳的一个。在这里，有两个重要的点你需要注意：'
- en: How to define the best configuration of the model
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何定义模型的最佳配置
- en: How many configurations should be tested
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该测试多少配置
- en: The best model is defined based on an evaluation metric. In other words, you
    have to first define which metric you are going to use to evaluate the model’s
    performance. Secondly, you have to define how you are going to evaluate the model.
    Usually, cross-validation is used to evaluate the model on multiple datasets that
    have never been used for training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型是根据评估指标定义的。换句话说，你必须首先定义你将使用哪个指标来评估模型的性能。其次，你必须定义你将如何评估模型。通常，交叉验证用于在多个从未用于训练的数据集上评估模型。
- en: 'In terms of the number of combinations/configurations, this is the most challenging
    part when playing with grid search. Each hyperparameter of an algorithm may have
    multiple or, sometimes, infinite possibilities of values. If you consider that
    an algorithm will usually have multiple hyperparameters, this becomes a function
    with quadratic cost, where the number of unique combinations to test is given
    as *the number of values of hyperparameter a * the number of values of hyperparameter
    b * the number of values of hyperparameter i*. *Table 7.1* shows how you could
    potentially set a grid search configuration for a decision tree model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在组合/配置的数量方面，这是使用网格搜索时最具挑战性的部分。算法的每个超参数可能有多个或有时是无限的可能值。如果你考虑到一个算法通常会有多个超参数，这将成为一个二次成本函数，其中要测试的唯一组合数量给出为*超参数a的值数
    * 超参数b的值数 * 超参数i的值数*。*表7.1*展示了你可以如何为决策树模型设置潜在的网格搜索配置：
- en: '| **Criterion** | **Max depth** | **Min** **samples leaf** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **标准** | **最大深度** | **最小样本叶** |'
- en: '| Gini, Entropy | 2, 5 =, 10 | 10, 20, 30 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Gini, Entropy | 2, 5 =, 10 | 10, 20, 30 |'
- en: Table 7.1 – Grid search configuration
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 – 网格搜索配置
- en: 'In *Table 7.1*, there are three hyperparameters: **Criterion**, **Max depth**,
    and **Min samples leaf**. Each of these hyperparameters has a list of values for
    testing. That means by the end of the grid search process, you will have tested
    18 models (2 * 3 * 3), where only the best one will be selected.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在*表7.1*中，有三个超参数：**标准差**、**最大深度**和**最小叶子节点样本数**。每个超参数都有一个测试值的列表。这意味着在网格搜索过程结束时，你将测试18个模型（2
    * 3 * 3），其中只有最好的一个将被选中。
- en: 'As you might have noticed, all the different combinations of those three hyperparameters
    will be tested. For example, consider the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，所有这三个超参数的不同组合都将被测试。例如，考虑以下内容：
- en: Criterion = Gini, Max depth = 2, Min samples leaf = 10
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准差 = Gini，最大深度 = 2，最小叶子节点样本数 = 10
- en: Criterion = Gini, Max depth = 5, Min samples leaf = 10
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准差 = Gini，最大深度 = 5，最小叶子节点样本数 = 10
- en: Criterion = Gini, Max depth = 10, Min samples leaf = 10
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准差 = Gini，最大深度 = 10，最小叶子节点样本数 = 10
- en: 'Some other questions that you might have could be as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还有其他问题，例如以下内容：
- en: Considering that a particular algorithm might have several hyperparameters,
    which ones should I tune?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到特定的算法可能有多个超参数，我应该调整哪些？
- en: Considering that a particular hyperparameter might accept infinite values, which
    values should I test?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到特定的超参数可能接受无限值，我应该测试哪些值？
- en: These are good questions, and grid search will not give you a straight answer
    for them. Instead, this is closer to an empirical process, where you have to test
    as much as you need to achieve your target performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是很好的问题，网格搜索并不能直接给出答案。相反，这更接近于一个经验过程，你需要测试足够多的内容以达到你的目标性能。
- en: Important note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Of course, grid search cannot guarantee that you will come up with your target
    performance. That depends on the algorithm and the training data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，网格搜索不能保证你会达到你的目标性能。这取决于算法和训练数据。
- en: A common practice, though, is to define the values for testing by using a **linear
    space** or **log space**, where you can manually set the limits of the hyperparameter
    you want to test and the number of values for testing. Then, the intermediate
    values will be drawn by a linear or log function.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常的做法是使用**线性空间**或**对数空间**来定义测试值，这样你可以手动设置想要测试的超参数的界限和测试值的数量。然后，中间值将通过线性或对数函数生成。
- en: As you might imagine, grid search can take a long time to run. A number of alternative
    methods have been proposed to work around this time issue. **Random search** is
    one of them, where the list of values for testing is randomly selected from the
    search space.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，网格搜索可能需要很长时间才能运行。已经提出了许多替代方法来解决这个问题。**随机搜索**就是其中之一，其中测试值的列表是从搜索空间中随机选择的。
- en: Another method that has gained rapid adoption across the industry is known as
    **Bayesian optimization**. Algorithm optimizations, such as **gradient descent**,
    try to find what is called the **global minima**, by calculating derivatives of
    the cost function. The global minima are the points where you find the algorithm
    configuration with the least associated cost.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在业界迅速得到广泛采用的方法被称为**贝叶斯优化**。算法优化，如**梯度下降**，试图找到所谓的**全局最小值**，通过计算成本函数的导数来实现。全局最小值是找到算法配置与最低相关成本的位置。
- en: Bayesian optimization is useful when calculating derivatives is not an option.
    So you can use the **Bayes theorem**, a probabilistic approach, to find the global
    minima using the smallest number of steps.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算导数不是一种选择时，贝叶斯优化是有用的。因此，你可以使用**贝叶斯定理**，一种概率方法，通过最少的步骤来找到全局最小值。
- en: In practical terms, Bayesian optimization will start testing the entire search
    space to find the most promising set of optimal hyperparameters. Then, it will
    perform more tests specifically in the place where the global minima are likely
    to be.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，贝叶斯优化将从整个搜索空间开始测试，以找到最有希望的一组最优超参数。然后，它将在全局最小值可能存在的位置进行更具体的测试。
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about the main metrics for model evaluation. You
    started with the metrics for classification problems and then you moved on to
    the metrics for regression problems.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了模型评估的主要指标。你从分类问题的指标开始，然后转向回归问题的指标。
- en: In terms of classification metrics, you have been introduced to the well-known
    confusion matrix, which is probably the most important artifact for performing
    a model evaluation on classification models.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类指标方面，你已经介绍了众所周知的混淆矩阵，这可能是对分类模型进行模型评估最重要的工具。
- en: You learned about true positives, true negatives, false positives, and false
    negatives. Then, you learned how to combine these components to extract other
    metrics, such as accuracy, precision, recall, the F1 score, and AUC.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你学习了真阳性、真阴性、假阳性和假阴性。然后，你学习了如何将这些组件组合起来提取其他指标，例如准确率、精确率、召回率、F1 分数和 AUC。
- en: You then went even deeper and learned about ROC curves, as well as precision-recall
    curves. You learned that you can use ROC curves to evaluate fairly balanced datasets
    and precision-recall curves for moderate to imbalanced datasets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你进一步学习了 ROC 曲线和精确率-召回率曲线。你了解到可以使用 ROC 曲线来评估相当平衡的数据集，而对于中等到不平衡的数据集，则可以使用精确率-召回率曲线。
- en: By the way, when you are dealing with imbalanced datasets, remember that using
    accuracy might not be a good idea.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，当你处理不平衡数据集时，请记住使用准确率可能不是一个好主意。
- en: In terms of regression metrics, you learned that the most popular ones, and
    the ones most likely to be present in the *AWS Machine Learning Specialty* exam,
    are the MAE, MSE, RMSE, and MAPE. Make sure you know the basics of each of them
    before taking the exam.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归指标方面，你了解到最流行且最可能在*AWS 机器学习专业*考试中出现的指标是 MAE、MSE、RMSE 和 MAPE。在参加考试之前，确保你了解它们的基本知识。
- en: Finally, you learned about methods for hyperparameter optimization, such as
    grid search and Bayesian optimization. In the next chapter, you will have a look
    at AWS application services for AI/ML. But first, take a moment to practice these
    questions about model evaluation and model optimization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了超参数优化的方法，例如网格搜索和贝叶斯优化。在下一章中，你将了解 AWS 人工智能/机器学习应用服务。但在那之前，花点时间练习这些关于模型评估和模型优化的问题。
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试准备练习 – 复习题
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对关键概念有扎实的理解外，能够在时间压力下快速思考是一项帮助你通过认证考试的关键技能。这就是为什么在学习的早期阶段就培养这些技能至关重要。
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 复习题旨在随着你学习并复习每一章的内容，逐步提高你的应试技巧，同时复习章节中的关键概念。你可以在每一章的末尾找到这些练习。
- en: How To Access These Resources
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如何访问这些资源
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何访问这些资源，请转到名为[*第 11 章*](B21197_11.xhtml#_idTextAnchor1477)的章节，*访问在线练习资源*。
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开本章的复习题，请执行以下步骤：
- en: Click the link – [https://packt.link/MLSC01E2_CH07](https://packt.link/MLSC01E2_CH07).
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接 – [https://packt.link/MLSC01E2_CH07](https://packt.link/MLSC01E2_CH07)。
- en: 'Alternatively, you can scan the following **QR code** (*Figure 7**.12*):'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以扫描以下**二维码**（*图 7.12*）：
- en: '![Figure 7.12 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_07_12.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 为登录用户打开第 7 章复习题的二维码](img/B21197_07_12.jpg)'
- en: Figure 7.12 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 为登录用户打开第 7 章复习题的二维码
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 7**.13*:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，你将看到一个类似于*图 7.13*所示的页面：
- en: '![Figure 7.13 – Chapter Review Questions for Chapter 7](img/B21197_07_13.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 第 7 章复习题](img/B21197_07_13.jpg)'
- en: Figure 7.13 – Chapter Review Questions for Chapter 7
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 第 7 章复习题
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备就绪后，开始以下练习，多次尝试测验。
- en: Exam Readiness Drill
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考试准备练习
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三次尝试，不必担心时间限制。
- en: ATTEMPT 1
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 1
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次，目标至少达到**40%**。查看你答错的答案，并再次阅读章节中的相关部分，以修复你的学习差距。
- en: ATTEMPT 2
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试2
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次，目标至少达到**60%**。查看你答错的答案，并再次阅读章节中的相关部分，以修复任何剩余的学习差距。
- en: ATTEMPT 3
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试3
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次，目标至少达到**75%**。一旦你的得分达到75%或更高，你就可以开始练习时间管理了。
- en: Tip
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要超过**三次**尝试才能达到75%。这是可以的。只需复习章节中的相关部分，直到你达到目标。
- en: Working On Timing
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作在时间管理上
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：你的目标是保持分数不变，同时尽可能快地回答这些问题。以下是你下一次尝试应该看起来像的样子：
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **尝试** | **得分** | **用时** |'
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 尝试5 | 77% | 21分钟30秒 |'
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 尝试6 | 78% | 18分钟34秒 |'
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 尝试7 | 76% | 14分钟44秒 |'
- en: Table 7.2 – Sample timing practice drills on the online platform
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 – 在线平台上的样本时间练习练习
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 上表中显示的时间限制只是示例。根据网站上的测验时间限制，为每次尝试设定自己的时间限制。
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的尝试，你的得分应该保持在**75%**以上，而完成所需的时间应该“减少”。重复尽可能多的尝试，直到你对自己应对时间压力有信心。
