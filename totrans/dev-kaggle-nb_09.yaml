- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Can You Find Out Which Movie Is a Deepfake?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你能找出哪部电影是深度伪造的吗？
- en: 'In the previous chapters, we explored various data formats: tabular, geospatial,
    text, image, and acoustic, while working with Kaggle datasets, learning about
    shapefile visualization, building models for image or text classification, and
    acoustic signal analysis.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们探讨了各种数据格式：表格、地理空间、文本、图像和声学，同时使用Kaggle数据集，了解shapefile可视化，构建图像或文本分类模型，以及声学信号分析。
- en: In this chapter, we will introduce video data analysis. We will start by describing
    a Kaggle competition, *Deepfake Detection Challenge*.This competition challenged
    the participants to classify which videos were generated artificially to create
    realistic fake content convincingly. We will continue by quickly exploring the
    most used video formats, followed by introducing two utility scripts used for
    our data analysis. First, a utility script with functions for manipulating video
    content, i.e., reading, visualizing images from videos, and playing video files.
    Second, a utility script with functions for body, face, and face element detection.
    We will continue with metadata exploration from the competition dataset and then
    apply the utility scripts introduced to analyze the video data from the competition
    dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍视频数据分析。我们将首先描述一个Kaggle比赛，*深度伪造检测挑战*。这个挑战要求参与者对哪些视频是人工生成以创造逼真的虚假内容进行分类。接下来，我们将快速探索最常用的视频格式，然后介绍用于数据分析的两个实用脚本。首先是一个具有操作视频内容功能的实用脚本，即读取、从视频中可视化图像和播放视频文件。其次是一个具有身体、面部和面部元素检测功能的实用脚本。我们将继续从竞赛数据集中的元数据探索，然后应用所介绍的实用脚本分析竞赛数据集中的视频数据。
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: An introduction to the *Deepfake Detection Challenge* competition
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度伪造检测挑战**比赛的介绍'
- en: Utility scripts for video data manipulation and object detection in video data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于视频数据操作和视频数据中目标检测的实用脚本
- en: Metadata analysis and video data analysis from the competition dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞赛数据集的元数据分析和视频数据分析
- en: Introducing the competition
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍比赛
- en: In this chapter, we examine data from the well-known Kaggle competition the
    **Deepfake****Detection****Challenge** (**DFDC**). The competition, detailed in
    *Reference 1*, commenced on December 11, 2019, and concluded on March 31, 2020\.
    It attracted 2,265 teams comprising 2,904 participants, who collectively made
    8,951 submissions. Competitors vied for a total prize pool of $1,000,000, with
    the first prize being $500,000.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考察了来自知名Kaggle比赛**深度伪造检测挑战**（**DFDC**）的数据。该比赛在*参考文献1*中有详细描述，于2019年12月11日开始，并于2020年3月31日结束。它吸引了2,265个团队，共有2,904名参与者，他们共同提交了8,951份作品。竞争者争夺总额为100万美元的奖金池，其中一等奖为50万美元。
- en: The event was a collaborative effort involving AWS, Facebook, Microsoft, the
    Partnership on AI’s Media Integrity Steering Committee, and various academic entities.
    At the time, there was a widespread agreement among tech industry leaders and
    academics on the technical complexity and rapidly changing nature of media content
    manipulation. The competition’s aim was to encourage global researchers to devise
    innovative and effective technologies to detect deepfakes and media manipulation.
    Unlike later competitions that focused on code, this one required prize-eligible
    participants to test their code in a “black box” environment. The testing data,
    not available on Kaggle and necessitating a longer process, resulted in the private
    leaderboard being revealed later than usual, officially on June 12, 2020, although
    the competition ended on April 24, 2020.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动是AWS、Facebook、Microsoft、人工智能媒体完整性指导委员会的合作伙伴关系以及多个学术实体共同协作的结果。当时，技术行业领袖和学者们普遍认为媒体内容操纵的技术复杂性和快速变化性质。比赛的目的是鼓励全球研究人员设计创新和有效的技术来检测深度伪造和媒体操纵。与后来专注于代码的比赛不同，这次比赛要求获奖者在一个“黑盒”环境中测试他们的代码。测试数据不在Kaggle上可用，需要更长的处理过程，导致私人排行榜比平时晚公布，正式公布日期为2020年6月12日，尽管比赛已于2020年4月24日结束。
- en: The DFDC drew numerous high-ranking Kaggle Grandmasters who engaged in data
    analysis and developed models for submission. Notably, the initial first-prize
    winner was later disqualified by the organizers. This team, along with other top-ranked
    participants, had expanded their training sets using publicly available data.
    While they adhered to the competition’s rules regarding the use of external data,
    they failed to meet the documentation requirements for winning submissions. These
    rules included obtaining written consent from all individuals featured in the
    images used in the additional training data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DFDC吸引了众多高排名的Kaggle大师，他们参与数据分析并开发了提交的模型。值得注意的是，最初的第一名获奖者后来被组织者取消资格。这个团队以及其他排名靠前的参与者，通过使用公开数据扩展了他们的训练集。虽然他们遵守了关于使用外部数据的比赛规则，但他们未能满足获奖提交的文档要求。这些规则包括从所有出现在额外训练数据中的图像中的人物获得书面同意。
- en: The competition data was given in two separate sets. In the first set, 400 video
    samples for training and 400 videos for testing were provided in two folders,
    one for training data and one for testing data. These files are in the MP4 format,
    one of the most commonly used video formats.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛数据提供了两个单独的集合。在第一个集合中，提供了400个用于训练的视频样本和400个用于测试的视频，分别放在两个文件夹中，一个用于训练数据，一个用于测试数据。这些文件是MP4格式，这是最常用的视频格式之一。
- en: A much larger dataset, of over 470 GB, for training, was made available as a
    download link. Alternatively, the same data was also made available as 50 smaller
    files of around 10 GB each. For the current analysis, we will only use the data
    from the first set (containing the 400 training and 400 testing files, in the
    `.mp4` format).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练，提供了一个超过470 GB的大型数据集作为下载链接。或者，相同的数据也以50个大约10 GB的小文件的形式提供。对于当前的分析，我们只会使用第一组的数据（包含400个训练文件和400个测试文件，格式为`.mp4`）。
- en: '**Formats for video data**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**视频数据格式**'
- en: Video formats refer to standards used to encode, compress, and store video data.
    Currently, there are multiple formats that are used in parallel. Some of these
    video formats were created and promoted by technology companies like Microsoft,
    Apple, and Adobe. Their decision to develop proprietary formats might have been
    related to the need to control the quality of rendering on their own devices or
    devices running their operating systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 视频格式指的是用于编码、压缩和存储视频数据的标准。目前，存在多种并行使用的格式。其中一些视频格式是由微软、苹果和Adobe等科技公司创建和推广的。他们决定开发专有格式可能与其控制自身设备或运行其操作系统的设备上的渲染质量的需求有关。
- en: Additionally, proprietary formats can give you a competitive advantage and larger
    control over licensing and the royalties associated with the format. Some of these
    formats incorporate innovations and useful features not existent in previously
    used formats. In parallel with the development by technology leaders, other formats
    were developed in response to a combination of technology advancements, market
    demand, and the need to align industry standards.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，专有格式可以给你带来竞争优势和更大的对许可和与格式相关的版税的控制。其中一些格式包含了之前使用的格式中不存在的新颖功能和实用功能。与技术领导者的开发并行，其他格式是在技术进步、市场需求和对齐行业标准的需求的响应下开发的。
- en: To give just a few examples of frequently used formats, we can mention **Windows
    Media Video** (**WMV**) and **Audio Video Interleave** (**AVI**), both developed
    by Microsoft. The MOV (QuickTime Movie) format was developed by Apple to run on
    their macOS and iOS platforms. All these formats support multiple audio and video
    codecs. Then, we also have **Flash Video** (**FLV**), which was developed by Adobe.
    Additionally, one widely adopted format is **MPEG-4 Part 14** (**MP4**), which
    is open-source and can also hold many video and audio codecs. **Moving Picture
    Experts Group** (**MPEG**) refers to a group of industry experts that developed
    the standards for audio and video compressing and encoding/decoding. The successive
    standards, from MPEG-1 to MPEG-4, have had a massive impact on the development
    of the media industry.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 仅举几个常用格式的例子，我们可以提到**Windows Media Video**（**WMV**）和**Audio Video Interleave**（**AVI**），这两者都是由微软开发的。MOV（QuickTime
    Movie）格式是由苹果开发的，用于在其macOS和iOS平台上运行。所有这些格式都支持多种音频和视频编解码器。然后，我们还有由Adobe开发的**Flash
    Video**（**FLV**）。此外，一个广泛采用的格式是**MPEG-4 Part 14**（**MP4**），它是开源的，也可以包含许多视频和音频编解码器。**运动图像专家组**（**MPEG**）指的是一组开发音频和视频压缩以及编码/解码标准的行业专家。从MPEG-1到MPEG-4的后续标准，对媒体行业的发展产生了巨大影响。
- en: Introducing competition utility scripts
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍竞赛实用脚本
- en: Let’s begin by grouping the Python modules with reusable functions for video
    manipulation in two Kaggle utility scripts. The first utility script groups functions
    to load and display images from videos or play video files. The second one is
    geared toward object detection in videos – more specifically, to detect human
    faces and bodies – using a few alternative methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从两个Kaggle实用脚本中分组具有视频操作可重用功能的Python模块。第一个实用脚本将加载和显示视频或播放视频文件的功能分组。第二个则专注于视频中的对象检测——更具体地说，是检测人脸和身体——使用几种替代方法。
- en: Video data utils
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视频数据工具
- en: We developed a utility script to assist us in the manipulation of video data.
    Let’s introduce one utility script that we will use in the notebook associated
    with the current chapter to read video data, as well as visualize frames from
    a video file.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个实用脚本，帮助我们操作视频数据。让我们介绍一个实用脚本，我们将使用与当前章节相关的笔记本来读取视频数据，以及可视化视频文件的帧。
- en: The `video_utils` utility script includes functions to load, transform, and
    display images from videos. Additionally, it also contains a function to play
    video content. For video manipulation, we will use the OpenCV library. OpenCV
    is an open-source computer vision library widely used for image and video processing.
    Developed in C and C++, OpenCV has also a Python interface.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`video_utils`实用脚本包括加载、转换和显示视频图像的函数。此外，它还包含一个播放视频内容的函数。对于视频操作，我们将使用OpenCV库。OpenCV是一个开源的计算机视觉库，广泛用于图像和视频处理。OpenCV是用C和C++开发的，也提供了一个Python接口。'
- en: 'The following code block shows the included libraries and the function to display
    one image from a video file:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了包含的库以及从视频文件中显示一张图片的功能：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, the function `display_image_from_video` receives as a
    parameter the path to a video file, performs an image capture from the video,
    reads the image, creates a Matplotlib Pyplot image, converts it from BGR (Blue
    Green Red) to RGB (Red Green Blue), and displays it. RGB is a color model used
    to represent color in a digital image. The difference between RGB and BGR is in
    the order in which the color information is stored. In the case of RGB, blue is
    stored as the least significant area, followed by green, and then red. In the
    case of BGR, the order is reversed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，函数`display_image_from_video`接收一个参数，即视频文件的路径，从视频中捕获图像，读取图像，创建一个Matplotlib
    Pyplot图像，将其从BGR（蓝绿红）转换为RGB（红绿蓝），并显示。RGB是一种用于在数字图像中表示颜色的颜色模型。RGB和BGR之间的区别在于颜色信息存储的顺序。在RGB的情况下，蓝色存储为最低有效位，然后是绿色，最后是红色。在BGR的情况下，顺序相反。
- en: 'Next, we define a function to represent a group of image captures, from a list
    of video files:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来表示从视频文件列表中捕获的一组图像：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The function `display_images_from_video_list` receives as a parameter the path
    to a list of video filenames, relative to the path to its folder, and the path
    to the dataset. The function will perform the same processing as `display_image_from_video`
    does, for the first six video files in the list. We limit the number of images
    captured from video files for convenience.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`display_images_from_video_list`接收一个参数，即视频文件名列表的路径，相对于其文件夹的路径，以及数据集的路径。该函数将对列表中的前六个视频文件执行与`display_image_from_video`相同的处理。我们限制从视频文件中捕获的图像数量以方便操作。
- en: 'The utility script also includes a function to play videos. The function uses
    the `HTML` function from the IPython `display` module. The code will be:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 实用脚本还包括一个播放视频的函数。该函数使用IPython `display`模块中的`HTML`函数。代码如下：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The function `play_video` receives as parameters the name of the video file
    to play, the data folder, the folder contained in the data folder, and where the
    video file is located. The function uses the `b64encode` function from the `base64`
    library to decode the MP4 video format, and the decoded content is displayed in
    a video frame with a controlled width of 500 pixels, using the `HTML` control.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`play_video`接收要播放的视频文件名、数据文件夹、数据文件夹中的文件夹以及视频文件所在位置作为参数。该函数使用`base64`库中的`b64encode`函数解码MP4视频格式，并将解码后的内容以500像素的受控宽度显示在视频帧中，使用`HTML`控件。
- en: We have introduced the utility script for video image manipulation, loads the
    video, visualize images from videos, and play video files. In the next section,
    we introduce more utility scripts for object detection in images. These Python
    modules contain specialized classes for object detection. The modules implement
    two alternatives for face object detection, both based on computer vision algorithms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了用于视频图像处理的实用脚本，它可以加载视频，从视频中可视化图像，并播放视频文件。在下一节中，我们将介绍更多用于图像中对象检测的实用脚本。这些Python模块包含用于对象检测的专业类。这些模块实现了两种人脸对象检测的替代方案，两者都基于计算机视觉算法。
- en: Face and body detection utils
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人脸和身体检测工具
- en: In the detection of deepfake videos, the analysis of video features such as
    desynchronization between sound and lip movement or unnatural motions of parts
    of the faces of people appearing in the video were, at the time of this competition,
    valuable elements to train models to recognize deepfake videos. Therefore, we
    include here the utility script specialized to detect bodies and faces.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度伪造视频的检测中，分析视频特征，如声音与唇部动作不同步或视频中人物面部部分的不自然动作，在这次比赛时，是训练模型识别深度伪造视频的有价值元素。因此，我们在此包括专门用于检测身体和脸部的实用脚本。
- en: 'The first module for face detection used the **Haar cascade** algorithm. Haar
    cascade is a lightweight machine learning algorithm for object detection. It is
    usually trained to identify specific objects. The algorithm uses Haar-like features
    and the Adaboost classifier to create a strong classifier. The algorithm operates
    on a sliding window, applying a cascade of weak classifiers that rejects regions
    of the image less likely to contain the object of interest. In our case, we want
    to use the algorithm to identify details in video images that are usually altered
    in the case of a deepfake, such as the facial expression, the gaze, and the mouth
    shape. This module includes two classes. We start with one of these classes. **CascadeObjectDetector**
    is a generic class for the detection of objects using the **Haar** **cascade**
    algorithm. The **CascadeObjectDetector**class, which is modified from the code
    in *Reference 3*, has an `init` function where we initialize the object with the
    specific **Haar cascade** object that stores the trained model. The class also
    has a **detect** function. The following is the code for **CascadeObjectDetector**.
    In the `init` function, we initialize the `cascade` object:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个用于人脸检测的模块使用了**Haar级联**算法。Haar级联是一种轻量级的机器学习算法，用于对象检测。它通常被训练来识别特定对象。该算法使用Haar-like特征和Adaboost分类器来创建一个强大的分类器。算法在滑动窗口上操作，应用一系列弱分类器，拒绝图像中不太可能包含感兴趣对象的部分。在我们的案例中，我们希望使用该算法来识别视频图像中的细节，这些细节在深度伪造的情况下通常会被改变，例如面部表情、眼神和嘴型。此模块包括两个类。我们从其中一个类开始。**CascadeObjectDetector**是一个用于使用**Haar**
    **级联**算法检测对象的通用类。从*参考文献3*中的代码修改而来的**CascadeObjectDetector**类有一个`init`函数，其中我们使用存储训练模型的特定**Haar级联**对象初始化对象。该类还有一个**detect**函数。以下是**CascadeObjectDetector**的代码。在`init`函数中，我们初始化`cascade`对象：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For this competition, I created a dedicated Kaggle dataset, from the Haar cascade
    algorithms defined at [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades),
    as part of the OpenCV library distribution. The link to this database, called
    *Haar Cascades for Face Detection*, is given in *Reference 2*. The `init` function
    receives a path to one of the object detection models included in the database.
    The function `detect` receives, as parameters, the image to process for object
    extraction and a few parameters that can be used to adjust the detection. These
    parameters are the scale factor, the minimum number of neighbors used in detection,
    and the minimum size of the bounding box used for object detection. Inside the
    `detect` function we call the function `detectMultiscale` from the Haar cascade
    model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这次比赛，我创建了一个专门的Kaggle数据集，该数据集来源于在[https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades)定义的Haar级联算法，作为OpenCV库分发的一部分。这个数据库的链接，称为*Haar级联人脸检测*，在*参考文献2*中给出。`init`函数接收数据库中包含的一个目标检测模型的路径。`detect`函数接收用于对象提取的图像和处理检测的几个参数，这些参数可以用来调整检测。这些参数是缩放因子、检测中使用的最小邻居数以及用于对象检测的最小边界框大小。在`detect`函数内部，我们调用Haar级联模型中的`detectMultiscale`函数。
- en: The next class defined in the utility script is `FaceObjectDetector`. This class
    initializes four `CascadeObjectDetector`objects, for the face, face profile, eyes,
    and smile detection. The following code block shows the class definition with
    the `init` function, where these objects are defined.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在实用脚本中定义的下一个类是`FaceObjectDetector`。这个类初始化了四个`CascadeObjectDetector`对象，用于面部、面部侧面、眼睛和微笑检测。下面的代码块显示了带有`init`函数的类定义，其中定义了这些对象。
- en: 'For each face element, i.e., the frontal view of a person, profile view of
    a person, eye view, and smile view, we first initialize a dedicated variable with
    the value of the path to the Haar cascade resource. Then, for each of the resources,
    we initialize a `CascadeObjectDetector` object (see the code explanation for the
    `CascadeObjectDetector` class above):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个面部元素，即一个人的正面视图、侧面视图、眼睛视图和微笑视图，我们首先使用到Haar级联资源的路径初始化一个专用变量。然后，对于每个资源，我们初始化一个`CascadeObjectDetector`对象（参见上面关于`CascadeObjectDetector`类的代码解释）：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The objects are stored as the member variables `face_detector`, `eyes_detector`,
    `profile_detector`, and `smile_detector`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象存储为成员变量`face_detector`、`eyes_detector`、`profile_detector`和`smile_detector`。
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we apply the same approach to the `smile` objects in the image. We first
    detect the smile, and if detected, we display it using a rectangle, drawn with
    the `opencv` function over the bounding box of the detected object. Because this
    function tends to give a lot of false positives, by default, this functionality
    is deactivated, using a flag set to `False`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将相同的方法应用于图像中的`smile`对象。我们首先检测微笑，如果检测到，我们使用`opencv`函数在检测到的对象的边界框上绘制矩形来显示它。因为这个函数倾向于给出很多误报，所以默认情况下，这个功能是禁用的，使用一个设置为`False`的标志：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we extract the `profile` and `face` objects using the specialized
    Haar cascade algorithms. If detected, we draw rectangles to mark the bounding
    boxes of the detected objects:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用专门的Haar级联算法提取`profile`和`face`对象。如果检测到，我们绘制矩形来标记检测到的对象的边界框：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For each of the four specialized object detectors (the face, face profile, eyes,
    and smile) we call the detect function and the results (a list of rectangles with
    the bounding box of the detected object), and then we draw in the context of the
    initial image either circles (for eyes) or rectangles (for the smile, face, and
    face profile) around the detected object. Finally, the function displays the image,
    with the superposed layers marking the bounding boxes of detected objects. Because
    the model for `smile` gives many false positives, we set an additional parameter,
    a flag to decide whether we show the extracted bounding boxes with smiles.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于四个专门的对象检测器（面部、面部侧面、眼睛和微笑），我们调用检测函数，并获取结果（一个包含检测到的对象边界框的矩形列表），然后在初始图像的上下文中绘制围绕检测到的对象的圆（用于眼睛）或矩形（用于微笑、面部和面部侧面）。最后，该函数显示图像，叠加的层标记了检测到的对象的边界框。因为`smile`模型的误报很多，我们设置了一个额外的参数，一个标志，用来决定我们是否显示带有微笑的提取边界框。
- en: 'Next, the class has a function to extract image objects. The function receives
    a video path, captures an image from the video, and applies the `detect_objects`
    function on the image capture for detection of the face and face details (the
    eyes, smile, and so on) from that image. The following code block shows the function
    for extraction:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这个类有一个用于提取图像对象的功能。该函数接收一个视频路径，从视频中捕获图像，并在图像捕获上应用`detect_objects`函数以检测该图像中的面部和面部细节（眼睛、微笑等）。下面的代码块显示了提取函数：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We introduced a module for face detection using Haar cascade algorithms. Next,
    we will review an alternative approach, where we use the **MTCNN** model for face
    detection. We want to test multiple approaches to decide which one works better
    for face detection. **MTCNN** stands for **Multi-Task Cascaded Convolution Networks**
    and is based on a concept developed first in the paper *Joint Face Detection and
    Alignment using Multi-task Cascaded Convolutional Networks* (see *Reference 4*).
    In another article titled *Face Detection using MTCNN*, the authors propose a
    “cascaded multi-task framework using different features of sub-models” (see *Reference
    5*). The implementation of face element extraction using the MTCNN approach is
    done in the utility script `face_detection_mtcnn`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了一个使用Haar级联算法进行面部检测的模块。接下来，我们将回顾一种替代方法，其中我们使用**MTCNN**模型进行面部检测。我们想测试多种方法以决定哪种方法更适合面部检测。**MTCNN**代表**多任务级联卷积网络**，它基于在论文*使用多任务级联卷积网络进行联合面部检测和校准*中首先提出的概念（见*参考文献4*）。在另一篇题为*使用MTCNN进行面部检测*的文章中，作者提出了一种“使用子模型不同特征的级联多任务框架”（见*参考文献5*）。使用MTCNN方法进行面部元素提取的实现是在实用脚本`face_detection_mtcnn`中完成的。
- en: 'In this module, we define the class `MTCNNFaceDetector`. In the next code block,
    we show the class definition with the `init` function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模块中，我们定义了`MTCNNFaceDetector`类。在下一个代码块中，我们展示了带有`init`函数的类定义：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `init` function receives, as a parameter, an instance of the MTCNN model,
    imported and instantiated in the calling application from the `mtcnn` library.
    The class member variable detector is initialized with this object. The rest of
    the class variables are used for visualization of the detected objects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`init`函数接收MTCNN模型的一个实例作为参数，该实例在调用应用程序中从`mtcnn`库导入并实例化。类成员变量detector用此对象初始化。其余的类变量用于检测到的对象的可视化。'
- en: 'The class also has a `detect` function. The next code block shows the `detect`
    function implementation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该类还有一个`detect`函数。下一个代码块显示了`detect`函数的实现：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The function receives, as a parameter, the path to the video file. After capturing
    an image from the video file, we read it and transform it from the BGR format
    to the RGB format. The transformation is needed because we want to use library
    functions that expect the RGB color order. After we apply the `detect_faces` function
    of the MTCNN model to the transformed image, the detector returns a list of extracted
    JSONs. Each extraction JSON has the following format:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 函数接收视频文件的路径作为参数。从视频文件中捕获图像后，我们读取它并将其从BGR格式转换为RGB格式。这种转换是必要的，因为我们想使用期望RGB颜色顺序的库函数。在将MTCNN模型的`detect_faces`函数应用于转换后的图像后，检测器返回一个提取的JSON列表。每个提取的JSON具有以下格式：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the `''box''` field is the bounding box of the detected face area. In the
    `''``keypoints''` field are the keys and coordinates of the five objects detected:
    the left eye, right eye, nose, left-most mouth limit, and right-most mouth limit.
    There is an additional field, `''confidence''`, which gives the confidence factor
    of the model.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在`'box'`字段中是检测到的面部区域的边界框。在`'``keypoints'`字段中是五个检测到的对象的键和坐标：左眼、右眼、鼻子、最左侧的嘴部限制和最右侧的嘴部限制。还有一个额外的字段`'confidence'`，它给出了模型的置信因子。
- en: For real faces, the confidence factor is above 0.99 (the maximum is 1). If the
    model detected an artifact, or things like a poster with a face image, this factor
    could be as large as 0.9\. Confidence factors under 0.9 are most likely associated
    with artifact detection (or false positives).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于真实的人脸，置信因子高于0.99（最大值为1）。如果模型检测到伪影，或者像带有面部图像的海报这样的东西，这个因子可能高达0.9。低于0.9的置信因子最可能是与伪影检测（或假阳性）相关。
- en: In our implementation (see the preceding code), we parse the list of detection
    JSONs and add a rectangle for each face, and a point (or a very small rectangle)
    for each of the five face features. On the top of the face bounding box rectangle,
    we write the confidence factor (rounded to four decimals).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中（见前面的代码），我们解析检测JSON列表，并为每个面部添加一个矩形，并为五个面部特征中的每一个添加一个点（或一个非常小的矩形）。在面部边界框矩形的顶部，我们写下置信因子（四舍五入到小数点后四位）。
- en: Besides the utility scripts for image capture from video and playing videos,
    and for object detection from video data, we will also reuse the utility scripts
    for data quality and plotting that we started using in *Chapter 4*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于从视频捕获图像和播放视频的实用脚本，以及用于从视频数据中检测对象的实用脚本之外，我们还将重用我们在*第4章*中开始使用的用于数据质量和绘图的数据质量实用脚本。
- en: In the next section, we start with a few preparatory activities and continue
    with a metadata exploration of the competition data. We will cover, in this section,
    importing the libraries, a few checks of the data files, as well as a statistical
    analysis of the metadata files.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们开始进行一些准备工作，并继续对竞赛数据进行元数据探索。在本节中，我们将介绍导入库、对数据文件进行一些检查以及元数据文件的统计分析。
- en: Metadata exploration
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元数据探索
- en: 'We start by importing the utility functions and classes from the utility scripts
    for data quality, plot utils, video utils, and face object detection. The following
    code block shows what we import from the utility scripts:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从数据质量、绘图工具、视频工具和面部对象检测实用脚本中导入实用函数和类。以下代码块显示了从实用脚本中导入的内容：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After we load the data files (the train and test samples), we are ready to
    start our analysis. The following code block checks the types of files in `TRAIN_SAMPLE_FOLDER`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载完数据文件（训练和测试样本）后，我们就可以开始我们的分析了。以下代码块检查 `TRAIN_SAMPLE_FOLDER` 中文件的类型：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The result shows that there are two types of files, JSON files and MP4 files.
    The following code checks the content of the JSON file present in `TRAIN_SAMPLE_FOLDER`.
    It samples the first five records for files in `TRAIN_SAMPLE_FOLDER`, as included
    in the JSON file:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示有两种类型的文件，JSON文件和MP4文件。以下代码检查 `TRAIN_SAMPLE_FOLDER` 中存在的JSON文件的内容。它从包含在JSON文件中的
    `TRAIN_SAMPLE_FOLDER` 中的文件中采样前五条记录：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In *Figure 9.1,* we show the data sample obtained when we created the DataFrame
    `meta_train_df` from the JSON file. The index is the name of the file. **label**
    is either **FAKE** (for deepfake videos) or **REAL** (for real videos). The **split**
    field gives the set to which the video belongs (`train`). **original** is the
    name of the initial video, from which the deepfake was created.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图9.1* 中，我们展示了从JSON文件创建DataFrame `meta_train_df` 时获得的数据样本。索引是文件名。**label**
    是 **FAKE**（用于深度伪造视频）或 **REAL**（用于真实视频）。**split** 字段给出了视频所属的集合（`train`）。**original**
    是初始视频的名称，深度伪造是从该视频创建的。
- en: '![A screenshot of a computer code  Description automatically generated](img/B20963_09_01.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![计算机代码的截图  自动生成的描述](img/B20963_09_01.png)'
- en: 'Figure 9.1: Sample of files in the train sample folder'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：训练样本文件夹中的文件样本
- en: We also check a few stats about the metadata, using the `missing_data`, `unique_values`,
    and `most_frequent_values` functions from the utility script `data_quality_stats`.
    These functions were introduced in *Chapter 3*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用来自数据质量、绘图工具、视频工具和面部对象检测实用脚本的一些统计函数来检查元数据的一些统计信息。这些函数在 *第3章* 中介绍。
- en: '*Figure 9.2* shows the missing values from `meta_train_df`. As you can see,
    **19.25**% of the original fields are missing.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.2* 展示了 `meta_train_df` 中的缺失值。如图所示，**19.25**% 的原始字段是缺失的。'
- en: '![A screenshot of a computer  Description automatically generated](img/B20963_09_02.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![白色网格上黑色文本的截图  自动生成的描述](img/B20963_09_02.png)'
- en: 'Figure 9.2: Missing values in the sample train data'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：样本训练数据中的缺失值
- en: In *Figure 9.3*, we show the unique values from **meta_train_df**. There are
    **323** original values with **209** unique ones. The other two fields, **label**
    and **split**, have 400 values, with **2** unique values for **label** (FAKE and
    REAL) and **1** for **split** (train).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图9.3* 中，我们展示了 **meta_train_df** 中的唯一值。有 **323** 个原始值，其中 **209** 个是唯一的。其他两个字段
    **label** 和 **split** 有400个值，其中 **label** 有 **2** 个唯一值（伪造和真实），**split** 有 **1**
    个（训练）。
- en: '![A screenshot of a computer  Description automatically generated](img/B20963_09_03.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![计算机的截图  自动生成的描述](img/B20963_09_03.png)'
- en: 'Figure 9.3: Unique values in sample train data'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：样本训练数据中的唯一值
- en: '*Figure 9.4* displays the most frequent values from **meta_train_df**. From
    the total of **400** labels, **323** or **80.75**% are FAKE. The most frequent
    **original** value is **atvmxvwyns.mp4**, with a frequency of **6** (that is,
    it was used in 6 FAKE videos). All the values in the **split** column are **train**.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.4* 展示了 **meta_train_df** 中最频繁的值。在总共 **400** 个标签中，**323** 个或 **80.75**%
    是伪造的。最频繁的 **原始** 值是 **atvmxvwyns.mp4**，频率为 **6**（即，它在6个伪造视频中使用了）。**split** 列中的所有值都是
    **train**。'
- en: '![A screenshot of a white grid with black text  Description automatically generated](img/B20963_09_04.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![白色网格上黑色文本的截图  自动生成的描述](img/B20963_09_04.png)'
- en: 'Figure 9.4: Most frequent values in the sample train data'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：样本训练数据中的最频繁值
- en: 'In this analysis, we will use a custom color schema, with tones of blues and
    grays. The following code block shows the code for the generation of the custom
    color map:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分析中，我们将使用自定义颜色方案，包括蓝色和灰度的色调。以下代码块显示了生成自定义颜色图的代码：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In *Figure 9.5*, we show the color map.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.5*中，我们展示了颜色图。
- en: '![A blue and white rectangular object  Description automatically generated](img/B20963_09_05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![一个蓝白色矩形物体  自动生成的描述](img/B20963_09_05.png)'
- en: 'Figure 9.5: Most frequent values in the sample train data'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：样本训练数据中最频繁的值
- en: '*Figure 9.6* shows the **label** distribution in the sample train dataset.
    There are 323 records with the **FAKE** label, and the rest of the labels have
    a **REAL** value.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.6*显示了样本训练数据中的**标签**分布。有323条记录带有**FAKE**标签，其余标签的值为**REAL**。'
- en: '![A graph with blue and green bars  Description automatically generated](img/B20963_09_06.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![带有蓝色和绿色条的图表  自动生成的描述](img/B20963_09_06.png)'
- en: 'Figure 9.6: Most frequent values in the sample train data'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：样本训练数据中最频繁的值
- en: In the next section, we will start analyzing the video data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开始分析视频数据。
- en: Video data exploration
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频数据探索
- en: In this section, we will visualize a few samples of files, and then we will
    begin performing object detection to try to capture the features from the images
    that might have some anomalies when processed to create deepfakes. These are mostly
    the eyes, mouths, and figures.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将可视化一些文件样本，然后我们将开始执行目标检测，试图从图像中捕获在创建深度伪造时可能出现的异常特征。这些主要是眼睛、嘴巴和身体。
- en: We will start by visualizing sample files, both genuine images and deepfakes.
    We will then apply the first algorithm introduced previously for face, eye, and
    mouth detection, the one based on Haar cascade. We then follow with the alternative
    algorithm, based on MTCNN.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先可视化样本文件，包括真实图像和深度伪造图像。然后，我们将应用之前介绍的第一种算法，用于人脸、眼睛和嘴巴检测，即基于Haar级联的算法。接着，我们将使用基于MTCNN的替代算法。
- en: Visualizing sample files
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化样本文件
- en: 'The following code block selects a few video files from the set of fake videos
    and then visualizes an image capture from them, using the `display_image_from_video`
    function from the utility script `video_utils`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块从一组假视频中选取一些视频文件，然后使用来自实用脚本`video_utils`的`display_image_from_video`函数可视化它们的图像捕获：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code will plot one image capture per each of the three videos.
    In *Figure 9.7*, we only show one of these image captures, for the first video:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将为三个视频中的每一个绘制一个图像捕获。在*图9.7*中，我们只展示这些图像捕获中的一个，即第一个视频的：
- en: '![A person and person standing in front of a fireplace  Description automatically
    generated](img/B20963_09_07.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![一个人和一个站在壁炉前的人  自动生成的描述](img/B20963_09_07.png)'
- en: 'Figure 9.7: Example of an image capture from a fake video'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：伪造视频的图像捕获示例
- en: 'The next code block selects a sample of three real videos and then, for each
    selected video, creates and plots an image capture:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块选择三个真实视频的样本，然后为每个选定的视频创建并绘制一个图像捕获：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In *Figure 9.8*, we show one of the images captured from the first real video:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.8*中，我们展示了从第一段真实视频中捕获的一张图像：
- en: '![A person standing in a corner  Description automatically generated](img/B20963_09_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![站在角落里的人  自动生成的描述](img/B20963_09_08.png)'
- en: 'Figure 9.8: Example of an image capture from a real video'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：来自真实视频的图像捕获示例
- en: 'We would also like to inspect videos that are all derived from the same original
    video. We will pick six videos from the same original video and show one image
    capture from each video. The following code block performs this operation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望检查所有都源自同一原始视频的视频。我们将从同一原始视频中选取六个视频，并展示每个视频的一个图像捕获。以下代码块执行此操作：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In *Figure 9.9*, we show two of these image captures from several different
    videos, of which we used the same original file to deepfake.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.9*中，我们展示了来自几个不同视频的这些图像捕获中的两个，其中我们使用了相同的原始文件进行深度伪造。
- en: '![A person in a yellow shirt  Description automatically generated](img/B20963_09_09.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![穿黄色衬衫的人  自动生成的描述](img/B20963_09_09.png)'
- en: 'Figure 9.9: Image captures from faked videos modified from the same original
    file'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：从同一原始文件修改的伪造视频的图像捕获
- en: 'We performed similar checks with videos from the test set. Of course, in the
    case of the test set, we will not be able to know in advance which video is real
    or not. The following code selects image captures from two sample videos from
    the data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还对测试集的视频进行了类似的检查。当然，在测试集的情况下，我们无法事先知道哪个视频是真实的还是伪造的。以下代码从数据中的两个样本视频中选择了图像捕获：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Figure 9.10* displays these selected images:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.10* 显示了这些选定的图像：'
- en: '![A person sitting in a chair  Description automatically generated](img/B20963_09_10.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![坐在椅子上的一个人  自动生成的描述](img/B20963_09_10.png)'
- en: 'Figure 9.10: Image captures from faked videos modified from the same original
    file'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：从同一原始文件修改的伪造视频中的图像捕获
- en: Let’s now start to use the algorithms for face detection introduced in the **Face
    and body detection** **utils** section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在开始使用**人脸和身体检测**工具部分中介绍的人脸检测算法。
- en: Performing object detection
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行对象检测
- en: 'First, let’s use the **Haar cascade** algorithms from the `face_object_detection`
    module. We use the `FaceObjectDetector` object to extract the face, profile face,
    eyes, and smile. The class `CascadeObjectDetector`initializes the specialized
    cascade classifiers for the aforementioned people attributes (using the specialized
    imported resource). The function `detect` uses a method of the `CascadeClassifier`
    from OpenCV to detect objects in images. For each attribute, we will use a different
    shape and color to mark/highlight the extracted object, as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用来自`face_object_detection`模块的**Haar级联**算法。我们使用`FaceObjectDetector`对象提取面部、面部轮廓、眼睛和微笑。`CascadeObjectDetector`类初始化上述人员属性的专用级联分类器（使用专用导入的资源）。`detect`函数使用OpenCV中的`CascadeClassifier`方法在图像中检测对象。对于每个属性，我们将使用不同的形状和颜色来标记/突出显示提取的对象，如下所示：
- en: '**Frontal face**: Green rectangle'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正面面部**：绿色矩形'
- en: '**Eye**: Red circle'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**眼睛**：红色圆圈'
- en: '**Smile**: Red rectangle'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微笑**：红色矩形'
- en: '**Profile face**: Blue rectangle'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**侧面面部**：蓝色矩形'
- en: Note that due to the large amount of false positives, we deactivated the smile
    detector.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于大量误报，我们已禁用了微笑检测器。
- en: 'We apply the function for face detection to a selection of images from the
    train sample videos. The following code block performs this operation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将人脸检测函数应用于训练样本视频中的一些图像。以下代码块执行此操作：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding code run will yield three image captures for three different videos.
    Each image is decorated with the highlighted objects extracted. The following
    figuresshow the three image captures with the extracted objects. In *Figure 9.11a*,
    we see both the frontal and profile faces detected and one eye detected. *Figure
    9.11b* shows both the frontal and profile faces detected, and two eyes detected.
    *Figure* *9.11c* shows both the frontal and profile faces detected, the two eyes
    correctly detected, and one false positive (one of the nostrils is detected as
    an eye). The smile detection is not activated in this case (too many false positives).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码运行将生成三个不同视频的三个图像捕获。每个图像都装饰了提取的突出显示对象。以下图示显示了带有提取对象的三个图像捕获。在*图9.11a*中，我们看到了检测到的正面和侧面面部以及一个检测到的眼睛。*图9.11b*显示了检测到的正面和侧面面部以及两个检测到的眼睛。*图9.11c*显示了检测到的正面和侧面面部，正确检测到的两个眼睛，以及一个误报（其中一个鼻孔被检测为眼睛）。在这种情况下，微笑检测未激活（误报太多）。
- en: '![A person in yellow sweater  Description automatically generated](img/B20963_09_11.a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![身穿黄色毛衣的人  自动生成的描述](img/B20963_09_11.a.png)'
- en: a
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: '![A person in yellow sweater  Description automatically generated](img/B20963_09_11.b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![身穿黄色毛衣的人  自动生成的描述](img/B20963_09_11.b.png)'
- en: b
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: b
- en: '![A person in a yellow sweater  Description automatically generated](img/B20963_09_11.c.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![身穿黄色毛衣的人  自动生成的描述](img/B20963_09_11.c.png)'
- en: c
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: c
- en: 'Figure 9.11: Face, face profile, and eye detection in image captures from three
    different videos'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：从三个不同视频的图像捕获中检测到的面部、面部轮廓和眼睛
- en: Running these algorithms with other images as well, we can see that they are
    not very robust and frequently yield false positives, as well as incomplete results.
    In *Figure 9.12*, we show two examples of such incomplete detections. In *Figure
    9.12a*, only the face was detected. In *Figure 9.12b*, only one face profile was
    detected, although two people are present in the scene.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他图像运行这些算法，我们可以看到它们并不非常稳健，并且经常产生误报以及不完整的结果。在*图9.12*中，我们展示了这种不完整检测的两个示例。在*图9.12a*中，只检测到了面部。在*图9.12b*中，只检测到了一个面部轮廓，尽管场景中有两个人。
- en: '![A person in a black jacket  Description automatically generated](img/B20963_09_12.a.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![一个穿黑色夹克的人  自动生成的描述](img/B20963_09_12.a.png)'
- en: a
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: '*![A person talking to another person  Description automatically generated](img/B20963_09_12.b.png)*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*![一个人在和另一个人交谈  自动生成的描述](img/B20963_09_12.b.png)*'
- en: b
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: b
- en: 'Figure 9.12: Face, face profile, and eye detection in image captures from two
    different videos'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：从两个不同视频捕获的图像中的面部、面部轮廓和眼部检测
- en: In the preceding image, there is also a strange detection; the fire sprinkler
    in the ceiling is detected as an eye and so is the candle fixture on the far left.
    This type of false detection (false positives) is quite frequent with these filters.
    One common problem is that objects like eyes, noses, or lips are detected in areas
    where there is no face. Since the search is done independently for these different
    objects, the likelihood of getting such false positives is quite large.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，也存在一种奇怪的检测；天花板上的消防喷淋头被检测为眼睛，远左边的烛台也是如此。这类错误检测（假阳性）在这些过滤器中相当常见。一个常见问题是眼睛、鼻子或嘴唇等物体在没有人脸的区域被检测到。由于这些不同物体的搜索是独立进行的，因此出现这种假阳性的可能性相当大。
- en: With the alternative solution implemented by us in `face_detection_mtcnn`, a
    unique framework is used to detect simultaneously the face bounding box and the
    position of face elements like eyes, nose, and lips. Let’s compare the results
    obtained with the Haar cascade algorithm, as shown in *Figures 9.11* and *9.12*,
    with the results for the same images obtained with the MTCNN algorithm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`face_detection_mtcnn`中实施的替代解决方案使用了一个独特的框架来同时检测人脸边界框和面部元素（如眼睛、鼻子和嘴唇）的位置。让我们比较使用Haar级联算法获得的与使用MTCNN算法获得的相同图像的结果，如图*图9.11*和*图9.12*所示。
- en: 'In *Figure 9.13*, we show one image of the person dressed in yellow; this time,
    face detection is performed with our **MTCNNFaceDetector**:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.13*中，我们展示了一张身穿黄色衣服的人的图像；这次，我们使用的是我们的**MTCNNFaceDetector**进行人脸检测：
- en: '![A person in a yellow sweater  Description automatically generated](img/B20963_09_13.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![一个穿着黄色毛衣的人  自动生成的描述](img/B20963_09_13.png)'
- en: 'Figure 9.13: MTCNN face detection: one genuine face and one artifact detected'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13：MTCNN人脸检测：一个真实人脸和一个人工制品的检测
- en: 'Two face objects are detected. One is correct, and the second is an artifact.
    The detection JSONs are:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 检测到两个面部对象。一个是正确的，另一个是人工制品。检测的JSON如下：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: From the experiments we conducted with a considerable number of samples, we
    concluded that the real faces will have a confidence factor very close to 1\.
    Because the second detected “face” has a confidence of `0.87`, we can easily dismiss
    it. Only faces with a confidence factor above `0.99` are actually to be trusted.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对大量样本进行的实验中，我们得出结论，真实的人脸将有一个非常接近1的置信因子。因为第二个检测到的“人脸”置信度为`0.87`，我们可以很容易地将其排除。只有置信因子高于`0.99`的人脸才是可信的。
- en: Let’s see another example. In *Figure 9.14*, we compare the results for the
    same images from *Figure 9.12*. In both figures, all the faces of the people present
    in the scene are correctly identified. In all the cases, the confidence score
    is above 0.999\. No artifacts are incorrectly extracted as human figures. The
    algorithm appears to be more robust than the alternative implementations using
    Haar cascades.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看另一个例子。在*图9.14*中，我们比较了*图9.12*中相同图像的结果。在这两个图中，场景中所有人的面部都被正确识别。在所有情况下，置信度得分都高于0.999。没有错误地将人工制品提取为人像。该算法似乎比使用Haar级联的替代实现更稳健。
- en: '![A person in a black jacket  Description automatically generated](img/B20963_09_14.a.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![一个穿黑色夹克的人  自动生成的描述](img/B20963_09_14.a.png)'
- en: a
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: '![A group of women sitting on a couch  Description automatically generated](img/B20963_09_14.b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![一群女人坐在沙发上  自动生成的描述](img/B20963_09_14.b.png)'
- en: b
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: b
- en: 'Figure 9.14: MTCNN face detection: a scene with one person and a scene with
    two people'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14：MTCNN人脸检测：一个人和一个两个人的场景
- en: 'For the next example, we selected a case where, if there are two people present
    in the video from which we capture the image, the faces are correctly identified,
    and the confidence score is high. In the same image, an artifact is also identified
    as a human face:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个例子，我们选择了一个案例，如果视频中存在两个人，那么从视频中捕获的图像中的人脸被正确识别，置信度得分也较高。在同一图像中，还识别了一个被误认为是人脸的人工制品：
- en: '![A person and person standing under a porch  Description automatically generated](img/B20963_09_15.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![一个人和一个站在门廊下的人  自动生成的描述](img/B20963_09_15.png)'
- en: 'Figure 9.15: MTCNN face detection: a scene with two people'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：MTCNN面部检测：两人场景
- en: Besides the two real people, for which the confidence factors are 0.9995 and
    0.9999 (rounded to 1), respectively, the face of the *Dead Alive* character on
    the T-shirt of the first person in the scene is also detected as a face. The bounding
    box is correctly detected, and all the face elements are also detected correctly.
    The only indication that this is a false positive is the lower confidence factor,
    which in this case is 0.9075\. Such examples can help us to correctly calibrate
    our face detection approach. Only faces detected with a confidence above 0.95
    or even 0.99 should be considered.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了两个真实人物，其置信度因子分别为0.9995和0.9999（四舍五入为1）之外，场景中第一人T恤上的*Dead Alive*角色面部也被检测为面部。边界框被正确检测，所有面部元素也被正确检测。唯一表明这是一个误报的迹象是较低的置信度因子，在这种情况下为0.9075。这样的例子可以帮助我们正确校准我们的面部检测方法。只有置信度高于0.95或甚至0.99的面部检测应该被考虑。
- en: In the notebook associated with this chapter, *Deepfake Exploratory Data Analysis*
    ([https://www.kaggle.com/code/gpreda/deepfake-exploratory-data-analysis](https://www.kaggle.com/code/gpreda/deepfake-exploratory-data-analysis)),
    we provide more examples of face extraction with both the approaches introduced
    here.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在与本章相关的笔记本中，*Deepfake Exploratory Data Analysis* ([https://www.kaggle.com/code/gpreda/deepfake-exploratory-data-analysis](https://www.kaggle.com/code/gpreda/deepfake-exploratory-data-analysis))，我们提供了使用这里介绍的方法进行面部提取的更多示例。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we began by introducing a series of utility scripts, which
    are reusable Python modules on Kaggle designed for video data manipulation. One
    such script, `video_utils`, is used to visualize images from videos and play them.
    Another script, `face_object_detection`, utilizes Haar cascade models for face
    detection.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了一系列实用脚本，这些是Kaggle上可重用的Python模块，用于视频数据处理。其中一个脚本`video_utils`用于可视化视频中的图像并播放它们。另一个脚本`face_object_detection`使用Haar级联模型进行面部检测。
- en: The third script, `face_detection_mtcnn`, employs MTCNN models to identify faces
    and key points such as the eyes, nose, and mouth. We then examined the metadata
    and video data from the DFDC competition dataset. In this dataset, we applied
    the aforementioned face detection methods to images from training and test videos,
    finding the MTCNN model approach to be more robust and accurate, with fewer false
    positives.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第三段脚本`face_detection_mtcnn`使用了MTCNN模型来识别面部以及如眼睛、鼻子和嘴巴等关键点。然后我们检查了DFDC竞赛数据集的元数据和视频数据。在这个数据集中，我们将上述面部检测方法应用于训练和测试视频中的图像，发现MTCNN模型方法更稳健、更准确，且误报率更低。
- en: As we near the conclusion of our exploration of data, we will reflect on our
    journey through various data formats, including tabular, text, image, sound, and
    now video. We’ve delved into numerous Kaggle datasets and competition datasets,
    learning how to conduct exploratory data analysis, create reusable code, establish
    a visual identity for our notebooks, and weave a narrative with data. In some
    instances, we also introduced feature engineering elements and established a model
    baseline. In one case, we demonstrated the step-by-step refinement of our model
    to enhance validation metrics. The focus of the previous and current chapters
    has been on crafting high-quality notebooks on Kaggle.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们接近数据探索的尾声，我们将反思我们通过各种数据格式（包括表格、文本、图像、声音和现在视频）的旅程。我们深入研究了多个Kaggle数据集和竞赛数据集，学习了如何进行探索性数据分析、创建可重用代码、为我们的笔记本建立视觉身份，以及用数据编织故事。在某些情况下，我们还引入了特征工程元素并建立了模型基线。在一个案例中，我们展示了逐步细化我们的模型以增强验证指标的过程。前几章和当前章节的重点是制作高质量的Kaggle笔记本。
- en: In the upcoming chapter, we will explore the use of large language models from
    Kaggle, potentially in conjunction with other technologies such as LangChain and
    vector databases. This will demonstrate the vast potential of Generative AI in
    various applications.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨使用Kaggle的大型语言模型，可能还会结合其他技术，如LangChain和向量数据库。这将展示生成式AI在各个应用中的巨大潜力。
- en: References
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Deepfake Detection Challenge, Kaggle competition, Identify videos with facial
    or voice manipulations: [https://www.kaggle.com/competitions/deepfake-detection-challenge](https://www.kaggle.com/competitions/deepfake-detection-challenge)'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度伪造检测挑战，Kaggle竞赛，识别带有面部或声音操纵的视频：[https://www.kaggle.com/competitions/deepfake-detection-challenge](https://www.kaggle.com/competitions/deepfake-detection-challenge)
- en: 'Haar Cascades for Face Detection, Kaggle dataset: [https://www.kaggle.com/datasets/gpreda/haar-cascades-for-face-detection](https://www.kaggle.com/datasets/gpreda/haar-cascades-for-face-detection)'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人脸检测的 Haar 级联，Kaggle 数据集：[https://www.kaggle.com/datasets/gpreda/haar-cascades-for-face-detection](https://www.kaggle.com/datasets/gpreda/haar-cascades-for-face-detection)
- en: 'Serkan Peldek – Face Detection with OpenCV, Kaggle notebook: [https://www.kaggle.com/code/serkanpeldek/face-detection-with-opencv/](https://www.kaggle.com/code/serkanpeldek/face-detection-with-opencv/)'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Serkan Peldek – 使用 OpenCV 进行人脸检测，Kaggle 笔记本：[https://www.kaggle.com/code/serkanpeldek/face-detection-with-opencv/](https://www.kaggle.com/code/serkanpeldek/face-detection-with-opencv/)
- en: 'Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao – Joint Face Detection
    and Alignment using Multi-task Cascaded Convolutional Networks: [https://arxiv.org/abs/1604.02878](https://arxiv.org/abs/1604.02878)'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 张凯鹏，张占鹏，李志锋，乔宇 – 使用多任务级联卷积网络进行人脸检测与对齐：[https://arxiv.org/abs/1604.02878](https://arxiv.org/abs/1604.02878)
- en: 'Justin Güse – Face Detection using MTCNN — a guide for face extraction with
    a focus on speed: [https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49](https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49)'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Justin Güse – 使用 MTCNN 进行人脸检测 — 专注于速度的人脸提取指南：[https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49](https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49)
- en: Join our book’s Discord space
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并在以下地点与超过 5000 名成员一起学习：
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/kaggle](https://packt.link/kaggle)'
- en: '![](img/QR_Code9220780366773140.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code9220780366773140.png)'
