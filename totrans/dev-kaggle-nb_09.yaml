- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can You Find Out Which Movie Is a Deepfake?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we explored various data formats: tabular, geospatial,
    text, image, and acoustic, while working with Kaggle datasets, learning about
    shapefile visualization, building models for image or text classification, and
    acoustic signal analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce video data analysis. We will start by describing
    a Kaggle competition, *Deepfake Detection Challenge*.This competition challenged
    the participants to classify which videos were generated artificially to create
    realistic fake content convincingly. We will continue by quickly exploring the
    most used video formats, followed by introducing two utility scripts used for
    our data analysis. First, a utility script with functions for manipulating video
    content, i.e., reading, visualizing images from videos, and playing video files.
    Second, a utility script with functions for body, face, and face element detection.
    We will continue with metadata exploration from the competition dataset and then
    apply the utility scripts introduced to analyze the video data from the competition
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the *Deepfake Detection Challenge* competition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utility scripts for video data manipulation and object detection in video data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata analysis and video data analysis from the competition dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the competition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examine data from the well-known Kaggle competition the
    **Deepfake****Detection****Challenge** (**DFDC**). The competition, detailed in
    *Reference 1*, commenced on December 11, 2019, and concluded on March 31, 2020\.
    It attracted 2,265 teams comprising 2,904 participants, who collectively made
    8,951 submissions. Competitors vied for a total prize pool of $1,000,000, with
    the first prize being $500,000.
  prefs: []
  type: TYPE_NORMAL
- en: The event was a collaborative effort involving AWS, Facebook, Microsoft, the
    Partnership on AI’s Media Integrity Steering Committee, and various academic entities.
    At the time, there was a widespread agreement among tech industry leaders and
    academics on the technical complexity and rapidly changing nature of media content
    manipulation. The competition’s aim was to encourage global researchers to devise
    innovative and effective technologies to detect deepfakes and media manipulation.
    Unlike later competitions that focused on code, this one required prize-eligible
    participants to test their code in a “black box” environment. The testing data,
    not available on Kaggle and necessitating a longer process, resulted in the private
    leaderboard being revealed later than usual, officially on June 12, 2020, although
    the competition ended on April 24, 2020.
  prefs: []
  type: TYPE_NORMAL
- en: The DFDC drew numerous high-ranking Kaggle Grandmasters who engaged in data
    analysis and developed models for submission. Notably, the initial first-prize
    winner was later disqualified by the organizers. This team, along with other top-ranked
    participants, had expanded their training sets using publicly available data.
    While they adhered to the competition’s rules regarding the use of external data,
    they failed to meet the documentation requirements for winning submissions. These
    rules included obtaining written consent from all individuals featured in the
    images used in the additional training data.
  prefs: []
  type: TYPE_NORMAL
- en: The competition data was given in two separate sets. In the first set, 400 video
    samples for training and 400 videos for testing were provided in two folders,
    one for training data and one for testing data. These files are in the MP4 format,
    one of the most commonly used video formats.
  prefs: []
  type: TYPE_NORMAL
- en: A much larger dataset, of over 470 GB, for training, was made available as a
    download link. Alternatively, the same data was also made available as 50 smaller
    files of around 10 GB each. For the current analysis, we will only use the data
    from the first set (containing the 400 training and 400 testing files, in the
    `.mp4` format).
  prefs: []
  type: TYPE_NORMAL
- en: '**Formats for video data**'
  prefs: []
  type: TYPE_NORMAL
- en: Video formats refer to standards used to encode, compress, and store video data.
    Currently, there are multiple formats that are used in parallel. Some of these
    video formats were created and promoted by technology companies like Microsoft,
    Apple, and Adobe. Their decision to develop proprietary formats might have been
    related to the need to control the quality of rendering on their own devices or
    devices running their operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, proprietary formats can give you a competitive advantage and larger
    control over licensing and the royalties associated with the format. Some of these
    formats incorporate innovations and useful features not existent in previously
    used formats. In parallel with the development by technology leaders, other formats
    were developed in response to a combination of technology advancements, market
    demand, and the need to align industry standards.
  prefs: []
  type: TYPE_NORMAL
- en: To give just a few examples of frequently used formats, we can mention **Windows
    Media Video** (**WMV**) and **Audio Video Interleave** (**AVI**), both developed
    by Microsoft. The MOV (QuickTime Movie) format was developed by Apple to run on
    their macOS and iOS platforms. All these formats support multiple audio and video
    codecs. Then, we also have **Flash Video** (**FLV**), which was developed by Adobe.
    Additionally, one widely adopted format is **MPEG-4 Part 14** (**MP4**), which
    is open-source and can also hold many video and audio codecs. **Moving Picture
    Experts Group** (**MPEG**) refers to a group of industry experts that developed
    the standards for audio and video compressing and encoding/decoding. The successive
    standards, from MPEG-1 to MPEG-4, have had a massive impact on the development
    of the media industry.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing competition utility scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin by grouping the Python modules with reusable functions for video
    manipulation in two Kaggle utility scripts. The first utility script groups functions
    to load and display images from videos or play video files. The second one is
    geared toward object detection in videos – more specifically, to detect human
    faces and bodies – using a few alternative methods.
  prefs: []
  type: TYPE_NORMAL
- en: Video data utils
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We developed a utility script to assist us in the manipulation of video data.
    Let’s introduce one utility script that we will use in the notebook associated
    with the current chapter to read video data, as well as visualize frames from
    a video file.
  prefs: []
  type: TYPE_NORMAL
- en: The `video_utils` utility script includes functions to load, transform, and
    display images from videos. Additionally, it also contains a function to play
    video content. For video manipulation, we will use the OpenCV library. OpenCV
    is an open-source computer vision library widely used for image and video processing.
    Developed in C and C++, OpenCV has also a Python interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block shows the included libraries and the function to display
    one image from a video file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the function `display_image_from_video` receives as a
    parameter the path to a video file, performs an image capture from the video,
    reads the image, creates a Matplotlib Pyplot image, converts it from BGR (Blue
    Green Red) to RGB (Red Green Blue), and displays it. RGB is a color model used
    to represent color in a digital image. The difference between RGB and BGR is in
    the order in which the color information is stored. In the case of RGB, blue is
    stored as the least significant area, followed by green, and then red. In the
    case of BGR, the order is reversed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function to represent a group of image captures, from a list
    of video files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The function `display_images_from_video_list` receives as a parameter the path
    to a list of video filenames, relative to the path to its folder, and the path
    to the dataset. The function will perform the same processing as `display_image_from_video`
    does, for the first six video files in the list. We limit the number of images
    captured from video files for convenience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The utility script also includes a function to play videos. The function uses
    the `HTML` function from the IPython `display` module. The code will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The function `play_video` receives as parameters the name of the video file
    to play, the data folder, the folder contained in the data folder, and where the
    video file is located. The function uses the `b64encode` function from the `base64`
    library to decode the MP4 video format, and the decoded content is displayed in
    a video frame with a controlled width of 500 pixels, using the `HTML` control.
  prefs: []
  type: TYPE_NORMAL
- en: We have introduced the utility script for video image manipulation, loads the
    video, visualize images from videos, and play video files. In the next section,
    we introduce more utility scripts for object detection in images. These Python
    modules contain specialized classes for object detection. The modules implement
    two alternatives for face object detection, both based on computer vision algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Face and body detection utils
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the detection of deepfake videos, the analysis of video features such as
    desynchronization between sound and lip movement or unnatural motions of parts
    of the faces of people appearing in the video were, at the time of this competition,
    valuable elements to train models to recognize deepfake videos. Therefore, we
    include here the utility script specialized to detect bodies and faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first module for face detection used the **Haar cascade** algorithm. Haar
    cascade is a lightweight machine learning algorithm for object detection. It is
    usually trained to identify specific objects. The algorithm uses Haar-like features
    and the Adaboost classifier to create a strong classifier. The algorithm operates
    on a sliding window, applying a cascade of weak classifiers that rejects regions
    of the image less likely to contain the object of interest. In our case, we want
    to use the algorithm to identify details in video images that are usually altered
    in the case of a deepfake, such as the facial expression, the gaze, and the mouth
    shape. This module includes two classes. We start with one of these classes. **CascadeObjectDetector**
    is a generic class for the detection of objects using the **Haar** **cascade**
    algorithm. The **CascadeObjectDetector**class, which is modified from the code
    in *Reference 3*, has an `init` function where we initialize the object with the
    specific **Haar cascade** object that stores the trained model. The class also
    has a **detect** function. The following is the code for **CascadeObjectDetector**.
    In the `init` function, we initialize the `cascade` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For this competition, I created a dedicated Kaggle dataset, from the Haar cascade
    algorithms defined at [https://github.com/opencv/opencv/tree/master/data/haarcascades](https://github.com/opencv/opencv/tree/master/data/haarcascades),
    as part of the OpenCV library distribution. The link to this database, called
    *Haar Cascades for Face Detection*, is given in *Reference 2*. The `init` function
    receives a path to one of the object detection models included in the database.
    The function `detect` receives, as parameters, the image to process for object
    extraction and a few parameters that can be used to adjust the detection. These
    parameters are the scale factor, the minimum number of neighbors used in detection,
    and the minimum size of the bounding box used for object detection. Inside the
    `detect` function we call the function `detectMultiscale` from the Haar cascade
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The next class defined in the utility script is `FaceObjectDetector`. This class
    initializes four `CascadeObjectDetector`objects, for the face, face profile, eyes,
    and smile detection. The following code block shows the class definition with
    the `init` function, where these objects are defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each face element, i.e., the frontal view of a person, profile view of
    a person, eye view, and smile view, we first initialize a dedicated variable with
    the value of the path to the Haar cascade resource. Then, for each of the resources,
    we initialize a `CascadeObjectDetector` object (see the code explanation for the
    `CascadeObjectDetector` class above):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The objects are stored as the member variables `face_detector`, `eyes_detector`,
    `profile_detector`, and `smile_detector`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we apply the same approach to the `smile` objects in the image. We first
    detect the smile, and if detected, we display it using a rectangle, drawn with
    the `opencv` function over the bounding box of the detected object. Because this
    function tends to give a lot of false positives, by default, this functionality
    is deactivated, using a flag set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we extract the `profile` and `face` objects using the specialized
    Haar cascade algorithms. If detected, we draw rectangles to mark the bounding
    boxes of the detected objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For each of the four specialized object detectors (the face, face profile, eyes,
    and smile) we call the detect function and the results (a list of rectangles with
    the bounding box of the detected object), and then we draw in the context of the
    initial image either circles (for eyes) or rectangles (for the smile, face, and
    face profile) around the detected object. Finally, the function displays the image,
    with the superposed layers marking the bounding boxes of detected objects. Because
    the model for `smile` gives many false positives, we set an additional parameter,
    a flag to decide whether we show the extracted bounding boxes with smiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the class has a function to extract image objects. The function receives
    a video path, captures an image from the video, and applies the `detect_objects`
    function on the image capture for detection of the face and face details (the
    eyes, smile, and so on) from that image. The following code block shows the function
    for extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We introduced a module for face detection using Haar cascade algorithms. Next,
    we will review an alternative approach, where we use the **MTCNN** model for face
    detection. We want to test multiple approaches to decide which one works better
    for face detection. **MTCNN** stands for **Multi-Task Cascaded Convolution Networks**
    and is based on a concept developed first in the paper *Joint Face Detection and
    Alignment using Multi-task Cascaded Convolutional Networks* (see *Reference 4*).
    In another article titled *Face Detection using MTCNN*, the authors propose a
    “cascaded multi-task framework using different features of sub-models” (see *Reference
    5*). The implementation of face element extraction using the MTCNN approach is
    done in the utility script `face_detection_mtcnn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this module, we define the class `MTCNNFaceDetector`. In the next code block,
    we show the class definition with the `init` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `init` function receives, as a parameter, an instance of the MTCNN model,
    imported and instantiated in the calling application from the `mtcnn` library.
    The class member variable detector is initialized with this object. The rest of
    the class variables are used for visualization of the detected objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class also has a `detect` function. The next code block shows the `detect`
    function implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The function receives, as a parameter, the path to the video file. After capturing
    an image from the video file, we read it and transform it from the BGR format
    to the RGB format. The transformation is needed because we want to use library
    functions that expect the RGB color order. After we apply the `detect_faces` function
    of the MTCNN model to the transformed image, the detector returns a list of extracted
    JSONs. Each extraction JSON has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `''box''` field is the bounding box of the detected face area. In the
    `''``keypoints''` field are the keys and coordinates of the five objects detected:
    the left eye, right eye, nose, left-most mouth limit, and right-most mouth limit.
    There is an additional field, `''confidence''`, which gives the confidence factor
    of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: For real faces, the confidence factor is above 0.99 (the maximum is 1). If the
    model detected an artifact, or things like a poster with a face image, this factor
    could be as large as 0.9\. Confidence factors under 0.9 are most likely associated
    with artifact detection (or false positives).
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation (see the preceding code), we parse the list of detection
    JSONs and add a rectangle for each face, and a point (or a very small rectangle)
    for each of the five face features. On the top of the face bounding box rectangle,
    we write the confidence factor (rounded to four decimals).
  prefs: []
  type: TYPE_NORMAL
- en: Besides the utility scripts for image capture from video and playing videos,
    and for object detection from video data, we will also reuse the utility scripts
    for data quality and plotting that we started using in *Chapter 4*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we start with a few preparatory activities and continue
    with a metadata exploration of the competition data. We will cover, in this section,
    importing the libraries, a few checks of the data files, as well as a statistical
    analysis of the metadata files.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by importing the utility functions and classes from the utility scripts
    for data quality, plot utils, video utils, and face object detection. The following
    code block shows what we import from the utility scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After we load the data files (the train and test samples), we are ready to
    start our analysis. The following code block checks the types of files in `TRAIN_SAMPLE_FOLDER`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The result shows that there are two types of files, JSON files and MP4 files.
    The following code checks the content of the JSON file present in `TRAIN_SAMPLE_FOLDER`.
    It samples the first five records for files in `TRAIN_SAMPLE_FOLDER`, as included
    in the JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 9.1,* we show the data sample obtained when we created the DataFrame
    `meta_train_df` from the JSON file. The index is the name of the file. **label**
    is either **FAKE** (for deepfake videos) or **REAL** (for real videos). The **split**
    field gives the set to which the video belongs (`train`). **original** is the
    name of the initial video, from which the deepfake was created.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer code  Description automatically generated](img/B20963_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Sample of files in the train sample folder'
  prefs: []
  type: TYPE_NORMAL
- en: We also check a few stats about the metadata, using the `missing_data`, `unique_values`,
    and `most_frequent_values` functions from the utility script `data_quality_stats`.
    These functions were introduced in *Chapter 3*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.2* shows the missing values from `meta_train_df`. As you can see,
    **19.25**% of the original fields are missing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B20963_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Missing values in the sample train data'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9.3*, we show the unique values from **meta_train_df**. There are
    **323** original values with **209** unique ones. The other two fields, **label**
    and **split**, have 400 values, with **2** unique values for **label** (FAKE and
    REAL) and **1** for **split** (train).
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B20963_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Unique values in sample train data'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.4* displays the most frequent values from **meta_train_df**. From
    the total of **400** labels, **323** or **80.75**% are FAKE. The most frequent
    **original** value is **atvmxvwyns.mp4**, with a frequency of **6** (that is,
    it was used in 6 FAKE videos). All the values in the **split** column are **train**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a white grid with black text  Description automatically generated](img/B20963_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Most frequent values in the sample train data'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this analysis, we will use a custom color schema, with tones of blues and
    grays. The following code block shows the code for the generation of the custom
    color map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 9.5*, we show the color map.
  prefs: []
  type: TYPE_NORMAL
- en: '![A blue and white rectangular object  Description automatically generated](img/B20963_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Most frequent values in the sample train data'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.6* shows the **label** distribution in the sample train dataset.
    There are 323 records with the **FAKE** label, and the rest of the labels have
    a **REAL** value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with blue and green bars  Description automatically generated](img/B20963_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Most frequent values in the sample train data'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will start analyzing the video data.
  prefs: []
  type: TYPE_NORMAL
- en: Video data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will visualize a few samples of files, and then we will
    begin performing object detection to try to capture the features from the images
    that might have some anomalies when processed to create deepfakes. These are mostly
    the eyes, mouths, and figures.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by visualizing sample files, both genuine images and deepfakes.
    We will then apply the first algorithm introduced previously for face, eye, and
    mouth detection, the one based on Haar cascade. We then follow with the alternative
    algorithm, based on MTCNN.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing sample files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code block selects a few video files from the set of fake videos
    and then visualizes an image capture from them, using the `display_image_from_video`
    function from the utility script `video_utils`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will plot one image capture per each of the three videos.
    In *Figure 9.7*, we only show one of these image captures, for the first video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person and person standing in front of a fireplace  Description automatically
    generated](img/B20963_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Example of an image capture from a fake video'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next code block selects a sample of three real videos and then, for each
    selected video, creates and plots an image capture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 9.8*, we show one of the images captured from the first real video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person standing in a corner  Description automatically generated](img/B20963_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Example of an image capture from a real video'
  prefs: []
  type: TYPE_NORMAL
- en: 'We would also like to inspect videos that are all derived from the same original
    video. We will pick six videos from the same original video and show one image
    capture from each video. The following code block performs this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 9.9*, we show two of these image captures from several different
    videos, of which we used the same original file to deepfake.
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a yellow shirt  Description automatically generated](img/B20963_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Image captures from faked videos modified from the same original
    file'
  prefs: []
  type: TYPE_NORMAL
- en: 'We performed similar checks with videos from the test set. Of course, in the
    case of the test set, we will not be able to know in advance which video is real
    or not. The following code selects image captures from two sample videos from
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 9.10* displays these selected images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person sitting in a chair  Description automatically generated](img/B20963_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Image captures from faked videos modified from the same original
    file'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now start to use the algorithms for face detection introduced in the **Face
    and body detection** **utils** section.
  prefs: []
  type: TYPE_NORMAL
- en: Performing object detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s use the **Haar cascade** algorithms from the `face_object_detection`
    module. We use the `FaceObjectDetector` object to extract the face, profile face,
    eyes, and smile. The class `CascadeObjectDetector`initializes the specialized
    cascade classifiers for the aforementioned people attributes (using the specialized
    imported resource). The function `detect` uses a method of the `CascadeClassifier`
    from OpenCV to detect objects in images. For each attribute, we will use a different
    shape and color to mark/highlight the extracted object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frontal face**: Green rectangle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eye**: Red circle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smile**: Red rectangle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Profile face**: Blue rectangle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that due to the large amount of false positives, we deactivated the smile
    detector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the function for face detection to a selection of images from the
    train sample videos. The following code block performs this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code run will yield three image captures for three different videos.
    Each image is decorated with the highlighted objects extracted. The following
    figuresshow the three image captures with the extracted objects. In *Figure 9.11a*,
    we see both the frontal and profile faces detected and one eye detected. *Figure
    9.11b* shows both the frontal and profile faces detected, and two eyes detected.
    *Figure* *9.11c* shows both the frontal and profile faces detected, the two eyes
    correctly detected, and one false positive (one of the nostrils is detected as
    an eye). The smile detection is not activated in this case (too many false positives).
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in yellow sweater  Description automatically generated](img/B20963_09_11.a.png)'
  prefs: []
  type: TYPE_IMG
- en: a
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in yellow sweater  Description automatically generated](img/B20963_09_11.b.png)'
  prefs: []
  type: TYPE_IMG
- en: b
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a yellow sweater  Description automatically generated](img/B20963_09_11.c.png)'
  prefs: []
  type: TYPE_IMG
- en: c
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.11: Face, face profile, and eye detection in image captures from three
    different videos'
  prefs: []
  type: TYPE_NORMAL
- en: Running these algorithms with other images as well, we can see that they are
    not very robust and frequently yield false positives, as well as incomplete results.
    In *Figure 9.12*, we show two examples of such incomplete detections. In *Figure
    9.12a*, only the face was detected. In *Figure 9.12b*, only one face profile was
    detected, although two people are present in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a black jacket  Description automatically generated](img/B20963_09_12.a.png)'
  prefs: []
  type: TYPE_IMG
- en: a
  prefs: []
  type: TYPE_NORMAL
- en: '*![A person talking to another person  Description automatically generated](img/B20963_09_12.b.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.12: Face, face profile, and eye detection in image captures from two
    different videos'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, there is also a strange detection; the fire sprinkler
    in the ceiling is detected as an eye and so is the candle fixture on the far left.
    This type of false detection (false positives) is quite frequent with these filters.
    One common problem is that objects like eyes, noses, or lips are detected in areas
    where there is no face. Since the search is done independently for these different
    objects, the likelihood of getting such false positives is quite large.
  prefs: []
  type: TYPE_NORMAL
- en: With the alternative solution implemented by us in `face_detection_mtcnn`, a
    unique framework is used to detect simultaneously the face bounding box and the
    position of face elements like eyes, nose, and lips. Let’s compare the results
    obtained with the Haar cascade algorithm, as shown in *Figures 9.11* and *9.12*,
    with the results for the same images obtained with the MTCNN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.13*, we show one image of the person dressed in yellow; this time,
    face detection is performed with our **MTCNNFaceDetector**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a yellow sweater  Description automatically generated](img/B20963_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: MTCNN face detection: one genuine face and one artifact detected'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two face objects are detected. One is correct, and the second is an artifact.
    The detection JSONs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: From the experiments we conducted with a considerable number of samples, we
    concluded that the real faces will have a confidence factor very close to 1\.
    Because the second detected “face” has a confidence of `0.87`, we can easily dismiss
    it. Only faces with a confidence factor above `0.99` are actually to be trusted.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see another example. In *Figure 9.14*, we compare the results for the
    same images from *Figure 9.12*. In both figures, all the faces of the people present
    in the scene are correctly identified. In all the cases, the confidence score
    is above 0.999\. No artifacts are incorrectly extracted as human figures. The
    algorithm appears to be more robust than the alternative implementations using
    Haar cascades.
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a black jacket  Description automatically generated](img/B20963_09_14.a.png)'
  prefs: []
  type: TYPE_IMG
- en: a
  prefs: []
  type: TYPE_NORMAL
- en: '![A group of women sitting on a couch  Description automatically generated](img/B20963_09_14.b.png)'
  prefs: []
  type: TYPE_IMG
- en: b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.14: MTCNN face detection: a scene with one person and a scene with
    two people'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next example, we selected a case where, if there are two people present
    in the video from which we capture the image, the faces are correctly identified,
    and the confidence score is high. In the same image, an artifact is also identified
    as a human face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person and person standing under a porch  Description automatically generated](img/B20963_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: MTCNN face detection: a scene with two people'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the two real people, for which the confidence factors are 0.9995 and
    0.9999 (rounded to 1), respectively, the face of the *Dead Alive* character on
    the T-shirt of the first person in the scene is also detected as a face. The bounding
    box is correctly detected, and all the face elements are also detected correctly.
    The only indication that this is a false positive is the lower confidence factor,
    which in this case is 0.9075\. Such examples can help us to correctly calibrate
    our face detection approach. Only faces detected with a confidence above 0.95
    or even 0.99 should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook associated with this chapter, *Deepfake Exploratory Data Analysis*
    ([https://www.kaggle.com/code/gpreda/deepfake-exploratory-data-analysis](https://www.kaggle.com/code/gpreda/deepfake-exploratory-data-analysis)),
    we provide more examples of face extraction with both the approaches introduced
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began by introducing a series of utility scripts, which
    are reusable Python modules on Kaggle designed for video data manipulation. One
    such script, `video_utils`, is used to visualize images from videos and play them.
    Another script, `face_object_detection`, utilizes Haar cascade models for face
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: The third script, `face_detection_mtcnn`, employs MTCNN models to identify faces
    and key points such as the eyes, nose, and mouth. We then examined the metadata
    and video data from the DFDC competition dataset. In this dataset, we applied
    the aforementioned face detection methods to images from training and test videos,
    finding the MTCNN model approach to be more robust and accurate, with fewer false
    positives.
  prefs: []
  type: TYPE_NORMAL
- en: As we near the conclusion of our exploration of data, we will reflect on our
    journey through various data formats, including tabular, text, image, sound, and
    now video. We’ve delved into numerous Kaggle datasets and competition datasets,
    learning how to conduct exploratory data analysis, create reusable code, establish
    a visual identity for our notebooks, and weave a narrative with data. In some
    instances, we also introduced feature engineering elements and established a model
    baseline. In one case, we demonstrated the step-by-step refinement of our model
    to enhance validation metrics. The focus of the previous and current chapters
    has been on crafting high-quality notebooks on Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will explore the use of large language models from
    Kaggle, potentially in conjunction with other technologies such as LangChain and
    vector databases. This will demonstrate the vast potential of Generative AI in
    various applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deepfake Detection Challenge, Kaggle competition, Identify videos with facial
    or voice manipulations: [https://www.kaggle.com/competitions/deepfake-detection-challenge](https://www.kaggle.com/competitions/deepfake-detection-challenge)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Haar Cascades for Face Detection, Kaggle dataset: [https://www.kaggle.com/datasets/gpreda/haar-cascades-for-face-detection](https://www.kaggle.com/datasets/gpreda/haar-cascades-for-face-detection)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Serkan Peldek – Face Detection with OpenCV, Kaggle notebook: [https://www.kaggle.com/code/serkanpeldek/face-detection-with-opencv/](https://www.kaggle.com/code/serkanpeldek/face-detection-with-opencv/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao – Joint Face Detection
    and Alignment using Multi-task Cascaded Convolutional Networks: [https://arxiv.org/abs/1604.02878](https://arxiv.org/abs/1604.02878)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Justin Güse – Face Detection using MTCNN — a guide for face extraction with
    a focus on speed: [https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49](https://towardsdatascience.com/face-detection-using-mtcnn-a-guide-for-face-extraction-with-a-focus-on-speed-c6d59f82d49)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code9220780366773140.png)'
  prefs: []
  type: TYPE_IMG
