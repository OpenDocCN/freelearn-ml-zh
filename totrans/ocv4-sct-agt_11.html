<html><head></head><body>
        

                            
                    <h1 class="header-title">Stopping Time and Seeing like a Bee</h1>
                
            
            
                
<p>"You never can tell with bees."<br/>
                       – A. A. Milne, Winnie-the-Pooh (1926)</p>
<p>The silent threat of radiation is everywhere in James Bond's world. Of course, stolen nuclear warheads are one cause for concern, but the excessively sunny weather is almost as bad, exposing the hero and his lovely traveling companions to an overdose of UV rays. Then, in <em>Moonraker</em> (1979), there is a high-budget mission to outer space, where the radiation hazards include cosmic rays, solar flares, and the turquoise lasers that everyone is shooting.</p>
<p>James Bond is not afraid of all this radiation. Perhaps he is able to take a cool, rational view of it by reminding himself that <em>electromagnetic radiation</em> can refer to various kinds of waves that move at the speed of light, including the rainbow-colored range of <em>visible light</em> we all see and love, but also including radio waves, microwaves, thermal infrared emissions, near-infrared light, ultraviolet light, X-rays, and gamma rays.</p>
<p>With specialized cameras, it is possible to capture images of other kinds of radiation besides visible light. Moreover, it is possible to capture videos at high frame rates, revealing patterns of motion or of pulsing light that are too fast for human vision to perceive. These capabilities would nicely complement the <kbd>Lazy Eyes</kbd> application that we developed in <a href="7cc1c0b9-a764-4069-9d45-e8bf129efc57.xhtml">Chapter 7</a>, <em>Seeing a Heartbeat with a Motion-Amplifying Camera</em>. Recall that <kbd>Lazy Eyes</kbd> implements the Eulerian video magnification algorithm, which amplifies a specified range of frequencies of motion. If we can increase the frame rate, we can improve the precision of this range of frequencies; thus, we can isolate high frequencies (fast motion) more effectively. This could also be described as an improvement in <strong>selectivity</strong>.</p>
<p>From a programming perspective, our goal in this chapter is simply to develop a variant of <kbd>Lazy Eyes</kbd> with support for more types of cameras. We will name this variant <kbd>Sunbaker</kbd>. We will make <kbd>Sunbaker</kbd> compatible with the Point Grey brand of industrial cameras from FLIR Systems. These cameras can be controlled using a C++ library called <strong>Spinnaker SDK</strong>, which has a Python wrapper called <kbd>PySpin</kbd>. We will learn how to integrate PySpin (and, in principle, any Python module for camera control) seamlessly with OpenCV.</p>
<p>PySpin (with a capital <em>P</em> and capital <em>S</em>) should not be confused with pyspin (all lowercase letters). The latter is a different Python library that can display spinning icons in a Terminal.</p>
<p class="mce-root">More broadly, our objective is to learn about some of the specialized cameras available on the market today, work with images from them, and understand how these kinds of imaging relate to the natural world. Did you know that a honey bee flies at an average speed of 24 kilometers (15 miles) per hour, and that it can see ultraviolet patterns on flowers? A different camera can give us an appreciation of how this creature might perceive the passing of light and time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter's project has the following software dependencies:</p>
<ul>
<li><strong>A Python environment with the following modules</strong>: OpenCV, NumPy, SciPy, PyFFTW, and wxPython.</li>
<li><strong>Optional</strong>: Spinnaker SDK and PySpin. These are available for Windows and Linux, but not Mac.</li>
</ul>
<p>Where not otherwise noted, setup instructions are covered in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em>Preparing for the Mission</em>. Setup instructions for PyFFTW are covered in <a href="7cc1c0b9-a764-4069-9d45-e8bf129efc57.xhtml">Chapter 7</a>, <em>Seeing a Heartbeat with a Motion-Amplifying Camera</em>, in the <em>Choosing and setting up an FFT library</em> section. Setup instructions for Spinnaker SDK and PySpin are covered in the current chapter, in the <em>Installing Spinnaker SDK and PySpin</em> section. Always refer to the setup instructions for any version requirements. Basic instructions for running Python code are covered in <a href="c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml">Appendix C</a>, <em>Running with Snakes (or First Steps with Python)</em>.</p>
<p>The completed project for this chapter can be found in the book's GitHub repository, <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a>, in the <kbd>Chapter008</kbd> folder.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the Sunbaker app</h1>
                
            
            
                
<p>Compared to <kbd>Lazy Eyes</kbd>, <kbd>Sunbaker</kbd> has the same GUI and, substantially, the same implementation of Eulerian video magnification. However, <kbd>Sunbaker</kbd> can capture input from a Point Grey industrial camera if one is connected. The following screenshot shows <kbd>Sunbaker</kbd> running with a high-speed monochrome camera called the <strong>Point Grey Grasshopper 3 GS3-U3-23S6M-C</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c4b7657a-28b5-477d-a917-0c6e4e076d04.png"/></p>
<p>The preceding screenshot shows my monochromatic friend, Eiffel Einstein Rocket. The effect of the Eulerian video magnification is visible as a halo along the edge of his back, which is moving as he breathes. The frame rate (98.7 <strong>frames per second</strong> (<strong>FPS</strong>) as shown in the screenshot) happens to be limited by the processing of the images; on a faster system, the camera could potentially capture up to 163 FPS.</p>
<p>As a fallback, if PySpin is unavailable or no PySpin-compatible camera is connected, <kbd>Sunbaker</kbd> can also capture input from any OpenCV-compatible camera. The following screenshot shows <kbd>Sunbaker</kbd> running with an OpenCV-compatible ultraviolet webcam called the <strong>XNiteUSB2S-MUV</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e2656730-565a-4812-92c8-67693ce2865d.png" style="width:37.92em;height:32.08em;"/></p>
<p>The preceding screenshot shows a small dandelion. Of course, in visible light, a dandelion's petals are entirely yellow. However, to the ultraviolet camera, the dandelion petals look like a dark circle inside a bright circle. This bull's-eye pattern is what a bee would see. Note that, in this screenshot, <kbd>Sunbaker</kbd> is still building up its history of frames, so it does not yet show a frame rate yet or an Eulerian video magnification effect. Potentially, Eulerian video magnification could amplify the pattern of the petals' motion in the wind.</p>
<p>Next, let's take a moment to put the capabilities of an <em>ultraviolet webcam </em>in context by looking at the electromagnetic spectrum.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the spectrum</h1>
                
            
            
                
<p>The universe is flooded with light, or electromagnetic radiation, and astronomers can use all wavelengths to capture images of distant objects. However, the Earth's atmosphere partly or wholly reflects some wavelengths of light or radiation back into outer space, so typically we deal with more limited ranges of wavelengths in imaging on Earth. NASA provides the following illustration, showing various wavelengths of electromagnetic radiation, their day-to-day importance to human beings, and their ability (or inability) to penetrate the Earth's atmosphere:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/16c79807-4104-4a98-ade2-900a09cf7a08.png"/></p>
<p>Note that the axis in the preceding diagram runs from longer wavelengths on the left to shorter wavelengths on the right. The Earth's atmosphere is relatively opaque in the range from the longest radio wavelengths down to the short end of the shortwave band (10 m). This <strong>opacity</strong> or <strong>reflectivity</strong> is an important principle in worldwide radio broadcasting, as it enables certain radio wavelengths to propagate around the Earth by bouncing back and forth between the surface and the upper atmosphere.</p>
<p>Next in the spectrum, the atmosphere is relatively transparent in the so-called <strong>radio window</strong>, including very high frequency or FM radio (which does not propagate beyond the horizon), cellular and Wi-Fi ranges, and the longer part of the microwave range. Then, the atmosphere is relatively opaque to the shorter part of the microwave range and the longer part of the <strong>infrared</strong> (<strong>IR</strong>) range (which starts around 1 mm).</p>
<p>Longwave infrared is also called <strong>thermal infrared</strong> or <strong>far infrared</strong> (<strong>FIR</strong>), and shortwave infrared is also called <strong>near infrared</strong> (<strong>NIR</strong>). Here, the terms <em>far</em> and <em>near</em> mean <em>farther from visible light</em> and <em>nearer to visible light</em>, respectively. So, past the opposite end of the visible range, longwave ultraviolet is also called <strong>near ultraviolet</strong> (<strong>NUV</strong>), and shortwave ultraviolet is also called <strong>far ultraviolet</strong> (<strong>FUV</strong>).</p>
<p>The radio and microwave ranges have relatively poor potential for terrestrial (earthbound) imaging. Only low-resolution imaging would be possible in these ranges because wavelength is a limiting factor of resolution. On the other hand, starting in the IR range, it becomes feasible to capture recognizable images of human-sized or smaller objects. Fortunately, there are good sources of natural illumination in the IR and visible ranges. Warm-blooded animals and other warm objects emit FIR radiation, which makes them visible to thermal cameras (even at night and even behind cold obstacles such as trees or walls). Moreover, the atmosphere is relatively transparent in the so-called <strong>optical window</strong>, including the NIR range, the visible range, and to a lesser extent the NUV range. NIR and NUV cameras produce images that look fairly similar to visible-light images, but with some differences in objects' coloration, opacity, and sharpness.</p>
<p>Throughout most of the UV range, as well as the X-ray and gamma ranges, the Earth's atmosphere is relatively opaque again. This is also fortunate—perhaps not from a computer vision perspective, but certainly from a biological perspective. Shortwave radiation can penetrate unprotected skin, flesh, and even bone, quickly causing burns and more slowly causing cancer. However, in short, controlled exposures from an artificial source, <strong>ultraviolet</strong> (<strong>UV</strong>) and X-ray imaging can be very useful in medicine. For example, UV imaging can record invisible bruises that are deep beneath the surface of the skin, and this kind of image is often used as forensic evidence in domestic abuse cases. X-ray imaging, of course, can go even deeper to reveal the bones or the inside of the lungs. Shortwave or <em>hard</em> X-rays, as well as gamma rays, are widely used to scan the inside of containers and vehicles, for example at security checkpoints.</p>
<p>For many decades, X-ray images have been commonplace in much of the world. During the 1950s and 1960s in the Soviet Union, discarded X-ray slides were sufficiently plentiful that music bootleggers used them as a cheap substitute for vinyl records. People listened to <em>jazz on bones</em> or <em>rock on bones</em> because this banned, foreign music was unobtainable in any other form. However, in contrast to a world where an X-ray scan might be less troublesome than a jazz record, the world in 1895-1896 was astonished by the first X-ray images. <em>I have seen my death,</em> said Anna Bertha Ludwig, the wife of the pioneering X-ray scientist Wilhelm Röntgen, when she first saw a skeletal scan of her hand. She, and other viewers at the time, had never imagined that a photograph could uncover the skeleton of a living person.</p>
<p>Today, specialized imaging technology is continuing to become more pervasive, and it will continue to change the way people see themselves and the world. For example, IR and UV cameras are now widely used for surveillance and detection in police work, many members of the public are aware of this due to police dramas on television, and we might begin to question our old assumptions about what can and cannot be seen. Forget about secret agents and even police detectives for a moment; we might even see a thermal camera on a <strong>do-it-yourself</strong> (<strong>DIY</strong>) show, since FIR imaging can be used to locate a cold draft around a window or a hot water pipe inside a wall. IR and UV cameras are becoming more affordable even for home use, and we will consider some examples of these and other specialized cameras in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding specialized cameras</h1>
                
            
            
                
<p>The following table provides a few examples of cameras that can capture video at high frame rates, in IR, or in UV:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Name</strong></p>
</td>
<td>
<p><strong>Price</strong></p>
</td>
<td>
<p><strong>Purpose</strong></p>
</td>
<td>
<p><strong>Modes</strong></p>
</td>
<td>
<p><strong>Optics</strong></p>
</td>
<td>
<p><strong>Compatibility</strong></p>
</td>
</tr>
<tr>
<td>
<p>XNiteUSB2S-MUV</p>
</td>
<td>
<p>$135</p>
</td>
<td>
<p>Monochrome imaging in near ultraviolet</p>
</td>
<td>
<p>Monochrome <em>1920 x 1080</em> @ 30 FPS</p>
<p>Monochrome <em>1280 x 720</em> @ 60 FPS</p>
<p>Monochrome <em>640 x 480</em> @ 120 FPS</p>
<p>(and other modes)</p>
</td>
<td>
<p>Diagonal field of view—86 degrees</p>
<p>3.6 mm lens on 1/2.7" sensor</p>
</td>
<td>
<p>OpenCV on Windows, Mac, Linux</p>
</td>
</tr>
<tr>
<td>
<p>XNiteUSB2S-IR715</p>
</td>
<td>
<p>$135</p>
</td>
<td>
<p>Monochrome imaging in NIR</p>
</td>
<td>
<p>Monochrome <em>1920 x 1080</em> @ 30 FPS</p>
<p>Monochrome <em>1280 x 720</em> @ 60 FPS</p>
<p>Monochrome <em>640 x 480</em> @ 120 FPS</p>
<p>(and other modes)</p>
</td>
<td>
<p>Diagonal field of view—86 degrees</p>
<p>3.6 mm lens on 1/2.7" sensor</p>
</td>
<td>
<p>OpenCV on Windows, Mac, Linux</p>
</td>
</tr>
<tr>
<td>
<p>Sony PlayStation Eye</p>
</td>
<td>
<p>$10</p>
</td>
<td>
<p>High-speed color imaging in visible light</p>
</td>
<td>
<p>Color <em>640 x 480</em> @ 60 FPS</p>
<p>Color <em>320 x 240</em> @ 187 FPS</p>
</td>
<td>
<p>Diagonal field of view—75 degrees or 56 degrees (two zoom settings)</p>
</td>
<td>
<p>OpenCV on Linux only (V4L backend)</p>
</td>
</tr>
<tr>
<td>
<p>Point Grey Grasshopper 3 GS3-U3-23S6C-C</p>
</td>
<td>
<p>$1045</p>
</td>
<td>
<p>High-speed color imaging in visible light</p>
</td>
<td>
<p>Color <em>1920 x 1200</em> @ 162 FPS</p>
<p>(and other modes)</p>
</td>
<td>
<p>C-mount lens (not included) on 1/1.2" sensor</p>
</td>
<td>
<p>Spinnaker SDK and PySpin on Windows, Linux</p>
</td>
</tr>
<tr>
<td>
<p>Point Grey Grasshopper 3 GS3-U3-23S6M-C</p>
</td>
<td>
<p>$1045</p>
</td>
<td>
<p>High-speed monochrome imaging in visible light</p>
</td>
<td>
<p>Monochrome <em>1920 x 1200</em> @ 162 FPS</p>
<p>(and other modes)</p>
</td>
<td>
<p>C-mount lens (not included) on 1/1.2" sensor</p>
</td>
<td>
<p>Spinnaker SDK and PySpin on Windows, Linux</p>
</td>
</tr>
<tr>
<td>
<p>Point Grey Grasshopper 3 GS3-U3-41C6NIR-C</p>
</td>
<td>
<p>$1359</p>
</td>
<td>
<p>Monochrome imaging in NIR</p>
</td>
<td>
<p>Monochrome <em>2048 x 2048</em> @ 90 FPS</p>
<p>(and other modes)</p>
</td>
<td>
<p>C-mount lens (not included) on 1" sensor</p>
</td>
<td>
<p>Spinnaker SDK and PySpin on Windows, Linux</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Of course, there are many other specialized cameras on the market besides these few examples. Many industrial cameras, including the Point Grey cameras previously listed, conform to an industry standard called <strong>GenICam</strong>, which, in principle, makes them compatible with third-party software libraries that are based on this standard. Harvesters (<a href="https://github.com/genicam/harvesters">https://github.com/genicam/harvesters</a>) is an example of an open source Python library that can control GenICam-compliant cameras. You may want to look into Harvesters if you are interested in support for additional brands of industrial cameras and additional platforms (Mac as well as Windows and Linux). For now, though, let's discuss some of the cameras in the preceding table in more detail.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">XNiteUSB2S-MUV</h1>
                
            
            
                
<p>The XNiteUSB2S-MUV, available from MaxMax.com (<a href="https://maxmax.com/">https://maxmax.com/</a>), is a true UV camera in the sense that it blocks out visible and infrared light in order to capture ultraviolet light alone. This is accomplished by means of a permanently attached lens filter that is opaque to visible light but relatively transparent to part of the NUV range. The lens's glass itself also filters out some ultraviolet light, and the result is that the camera captures the range from 360 nm to 380 nm. The following photograph of the camera and a black-eyed Susan (a North American flower with yellow petals and black stamens) shows an opaque reflection of the flower in the lens filter:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/f007010c-0f0b-4094-91a9-edca3688ff7f.png" style="width:39.17em;height:26.08em;"/></p>
<p>The following photo, captured by the UV camera, shows the same flower with petals that are dark at the base and bright at the tip, forming a typical ultraviolet bull's-eye pattern:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0aed955c-35e4-4196-91e7-78137eb72666.png" style="width:35.92em;height:23.92em;"/></p>
<p>To a bee, this big splash of two contrasting colors would stand out like a fast-food logo. Pollen is here!</p>
<p>The XNiteUSB2S-MUV can capture images outdoors in sunlight, but if you want to use it indoors, you will need a UV light source that covers the camera's range of sensitivity, 360 nm to 380 nm. MaxMax.com can provide sales advice on UV light sources, and on options to customize the XNiteUSB2S-MUV with a quartz lens that extends the range of sensitivity down to approximately 300 nm (at a significantly higher cost). See the camera's product page at <a href="https://maxmax.com/maincamerapage/uvcameras/usb2-small">https://maxmax.com/maincamerapage/uvcameras/usb2-small</a> and MaxMax.com's contact page at <a href="https://maxmax.com/contact-us">https://maxmax.com/contact-us</a>.</p>
<p>MaxMax.com also offers a series of infrared cameras that have the same electronics and lens as the XNiteUSB2S-MUV, only they use a different filter in order to block out visible and ultraviolet light while capturing part of the NIR range. The XNiteUSB2S-IR715 captures the broadest part of the NIR range, down to a wavelength of approximately 715 nm (for comparison, visible red starts at 700 nm). The product lineup includes several similarly named alternatives for other wavelength cutoffs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Sony PlayStation Eye</h1>
                
            
            
                
<p>The PlayStation Eye holds a unique position as a low-cost camera with a high maximum frame rate (albeit at a low resolution). Sony released the Eye in 2007 as an accessory for the PlayStation 3 game console, and game developers used the camera to support motion tracking, face tracking, or simply video chat. Later, the Eye's driver was reverse-engineered for other platforms, and the device gained popularity among computer vision experimenters. The Linux kernel (specifically, the Video4Linux or V4L module) officially supports the Eye. So, on Linux (and only on Linux), OpenCV can use the Eye just like an ordinary webcam.</p>
<p>PS3EYE Driver (<a href="https://github.com/inspirit/PS3EYEDriver">https://github.com/inspirit/PS3EYEDriver</a>) is an open-source C++ library that can control the PlayStation Eye on Windows or Mac. Potentially, you could write your own wrapper around PS3EYEDriver to provide an OpenCV-friendly interface. PS3EYEDriver reuses a lot of code from the Eye's Linux driver, which is GPL-licensed, so be careful about the licensing implications of using PS3EYE Driver; it might not be right for you unless your project is also GPL-licensed.</p>
<p>Here is a screenshot of <kbd>Sunbaker</kbd> running at a high frame rate with a PlayStation Eye camera on Linux:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8dd357e8-6990-40af-afa6-bdddcdcda706.png" style="width:14.67em;height:14.58em;"/></p>
<p>The preceding photo shows my monochromatic friend, Eiffel Einstein Rocket, at rest. As he breathes, the effect of the Eulerian video magnification is visible as a halo along the edge of his back. Note that the frame rate (60.7 FPS, as displayed) is actually limited by the processing of the images; we could approach or even reach the camera's maximum rate of 187 FPS on a faster system.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Point Grey Grasshopper 3 GS3-U3-23S6M-C</h1>
                
            
            
                
<p>The Point Grey Grasshopper 3 GS3-U3-23S6M-C is a highly configurable, monochrome camera with interchangeable lenses and a high-speed USB 3 interface. Depending on the configuration and the attached lens, it can capture detailed images of a wide variety of subjects, under a wide variety of conditions, at a high frame rate. Consider the following set of images. We see a headshot of the author, a close-up shot of veins in the author's eye, and a long-distance shot of the moon, all captured with the GS3-U3-23S6M-C camera and various low-cost lenses (each $50 or less):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8323d73f-0484-4ed0-99c1-f32cdb13ede6.png" style="width:30.92em;height:33.58em;"/></p>
<p>The camera uses a type of lens mount called a <strong>C-mount</strong>, and its sensor size is the so-called <strong>1/1.2" format</strong>. This is the same lens mount and nearly the same sensor size as two formats called <strong>16 mm</strong> and Super 16, which have been popular in amateur movie cameras ever since 1923. So, the camera is compatible with a wide range of inexpensive, old <em>cine</em> (cinematography) lenses, as well as newer and more expensive machine vision lenses.</p>
<p>Before even sending the frames through USB, the camera itself can efficiently perform some image processing operations, such as cropping the image and binning (summing) neighboring pixels to increase brightness and reduce noise. We will see how to control these features later in this chapter, in the <em>Capturing images from industrial cameras using PySpin</em> section.</p>
<p>The Point Grey Grasshopper 3 GS3-U3-23S6C-C is the same as the camera described previously, except it captures visible light in color instead of in monochrome. The Point Grey Grasshopper 3 GS3-U3-41C6NIR-C also belongs to the same family of cameras, but it is a monochrome NIR camera with a larger sensor (1" format), higher resolution, and lower frame rate. There are many other interesting Point Grey cameras, and you can search through a list of the available models and features at <a href="https://www.flir.com/browse/camera-cores--components/machine-vision-cameras" target="_blank">https://www.flir.com/browse/camera-cores--components/machine-vision-cameras</a>.</p>
<p>Next, let's look at how we can set up software libraries to control Point Grey cameras.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing Spinnaker SDK and PySpin</h1>
                
            
            
                
<p>To obtain drivers and libraries that will enable us to interface with Point Grey cameras, let's take the following steps:</p>
<ol>
<li>Go to the Spinnaker SDK section of the FLIR website at <a href="https://www.flir.com/products/spinnaker-sdk/">https://www.flir.com/products/spinnaker-sdk/</a>. Click the DOWNLOAD NOW button. You will be prompted to go to a different download site. Click the DOWNLOAD FROM BOX button.</li>
<li>You will see a page that allows you to navigate a file structure to find the available downloads. Select the folder that matches your operating system, such as Windows or Linux/Ubuntu18.04.</li>
<li>Within the selected folder or its subfolders, find and download a version of Spinnaker SDK that matches your operating system and architecture. (For Windows, you may choose either the Web installer or the Full SDK.) Also, find and download a version of PySpin (the Python Spinnaker bindings) that matches your Python version, operating system, and architecture, such as <kbd>spinnaker_python-1.20.0.15-cp36-cp36m-win_amd64.zip</kbd> for 64-bit Python 3.6 on Windows.</li>
</ol>
<ol start="4">
<li>Close the web browser.</li>
<li>The instructions for installation for the various systems are as follows:</li>
</ol>
<ul>
<li style="padding-left: 30px">For Windows, the Spinnaker SDK installer is an <kbd>.exe</kbd> installer. Run it and follow the installer's prompts. If you are prompted to select an Installation Profile, choose Application Development. If you are prompted to select Installation Components, choose Documentation, Drivers, and any other components you want.</li>
<li style="padding-left: 30px">For Linux, the Spinnaker SDK download is a <kbd>TAR.GZ</kbd> archive. Unzip it to any destination, which we will refer to as <kbd>&lt;spinnaker_sdk_unzip_destination&gt;</kbd>. Open a Terminal, run <kbd>$ cd &lt;spinnaker_sdk_unzip_destination&gt; &amp;&amp; ./install_spinnaker.sh</kbd>, and answer all the installer's prompts by entering <kbd>Yes</kbd>.</li>
</ul>
<ol start="6">
<li>The Python Spinnaker download is a ZIP archive (for Windows) or a TAR archive (for Linux). Unzip it to any destination. We will refer to its unzip destination as <kbd>&lt;PySpin_whl_unzip_destination&gt;</kbd> because it contains a WHL file, such as <kbd>spinnaker_python-1.20.0.15-cp36-cp36m-win_amd64.whl</kbd>. We will refer to the <kbd>WHL</kbd> file as <kbd>&lt;PySpin_whl_file&gt;</kbd>. The WHL file is a package that can be installed using Python's package manager, <kbd>pip</kbd>. Open a Terminal and run the following commands (but substitute the actual folder name and filename):</li>
</ol>
<pre style="padding-left: 60px"><strong>$ cd &lt;PySpin_whl_unzip_destination&gt;</strong><br/><strong>$ pip install --user &lt;PySpin_whl_file&gt;</strong></pre>
<p>For some Python 3 environments, you may need to run <kbd>pip3</kbd> instead of <kbd>pip</kbd> in the preceding command.</p>
<p>At this point, we have all the software we need in order to control Point Grey cameras from Python scripts. Let's proceed to write a Python class that supports interoperability between PySpin and OpenCV.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Capturing images from industrial cameras using PySpin</h1>
                
            
            
                
<p>Let's create a file called <kbd>PySpinCapture.py</kbd>. Not surprisingly, we will begin its implementation with the following <kbd>import</kbd> statements:</p>
<pre>import PySpin<br/>import cv2</pre>
<p>As a practical introduction to <kbd>PySpin</kbd>, let's add the following function, which returns the number of PySpin-compatible cameras currently connected to the system:</p>
<pre>def getNumCameras():<br/>    system = PySpin.System.GetInstance()<br/>    numCameras = len(system.GetCameras())<br/>    system.ReleaseInstance()<br/>    return numCameras</pre>
<p>Here, we see that our standalone <kbd>getNumCameras</kbd> function (like any self-contained module of code that uses <kbd>PySpin</kbd>) is responsible for acquiring and releasing a reference to the <kbd>PySpin</kbd> system. We also see that the <kbd>PySpin</kbd> system is a gateway, providing access to any connected PySpin-compatible cameras.</p>
<p>Our primary goal in this file is to implement a class, <kbd>PySpinCapture</kbd>, which will provide some of the same public methods as the <kbd>cv2.VideoCapture</kbd> class in OpenCV's Python bindings. An instance of <kbd>PySpinCapture</kbd> will provide access to a single PySpin-compatible camera in a self-contained way. However, the class can be instantiated multiple times for simultaneous access to different cameras through different instances. <kbd>PySpinCapture</kbd> will implement the following methods to partly mimic the behavior of <kbd>cv2.VideoCapture</kbd>:</p>
<ul>
<li><kbd>get(propId)</kbd>: This method returns the value of the camera property identified by the <kbd>propId</kbd> argument. We will support two of OpenCV's <kbd>propId</kbd> constants, namely <kbd>cv2.CAP_PROP_FRAME_WIDTH</kbd> and <kbd>cv2.CAP_PROP_FRAME_HEIGHT</kbd>.</li>
<li><kbd>read(image=None)</kbd>: This method reads a camera frame and returns a tuple, <kbd>(retval, image_out)</kbd>, where <kbd>retval</kbd> is a Boolean indicating success (<kbd>True</kbd>) or failure (<kbd>False</kbd>), and <kbd>image_out</kbd> is the captured frame (or <kbd>None</kbd> if the capture failed). If the <kbd>image</kbd> argument is not <kbd>None</kbd> and the capture succeeds, then <kbd>image_out</kbd> is the same object as <kbd>image</kbd>, but it contains new data.</li>
</ul>
<ul>
<li><kbd>release()</kbd>: This method releases the camera's resources. <kbd>cv2.VideoCapture</kbd> is implemented such that the destructor calls <kbd>release</kbd>, and <kbd>PySpinCapture</kbd> will be implemented this way too.</li>
</ul>
<p>Other Python scripts will be able to call these methods on an object without needing to know whether the object is an instance of <kbd>cv2.VideoCapture</kbd>, <kbd>PySpinCapture</kbd>, or some other class that has the same methods. This is the case even though these classes have no relationship in terms of object-oriented inheritance. This feature of Python is called <strong>duck typing</strong>. <em>If it looks like a duck, swims like a duck, and quacks like a duck, then it probably is a duck,</em> or so the saying goes. If it provides a <kbd>read</kbd> method that returns a frame, then it probably <em>is</em> a frame-capture object. Later in this chapter, in the <em>Adapting the Lazy Eyes app to make Sunbaker</em> section, we will instantiate <kbd>PySpinCapture</kbd> if <kbd>PySpin</kbd> is available, and <kbd>cv2.VideoCapture</kbd> otherwise; then, we will use the instantiated object without further concern about its type.</p>
<p>Point Grey cameras are more configurable than most cameras supported by <kbd>cv2.VideoCapture</kbd>. Our <kbd>__init__</kbd> method for <kbd>PySpinCapture</kbd> will accept the following arguments:</p>
<ul>
<li><kbd>index</kbd>: This is the camera's device index.</li>
<li><kbd>roi</kbd>: This is a region of interest in the <kbd>(x, y, w, h)</kbd> format, relative to the camera's native image dimensions. Data outside the region of interest will not be captured. For example, if the native image dimensions are <em>800 x 600</em> pixels, and the <kbd>roi</kbd> is <kbd>(0, 300, 800, 300)</kbd>, the captured image will cover only the bottom half of the image sensor.</li>
<li><kbd>binningRadius</kbd>: This is <kbd>1</kbd> if an unfiltered image should be captured, and <kbd>2</kbd> or more if neighboring pixels in the specified radius should be summed to produce a smaller, brighter, less noisy image.</li>
<li><kbd>isMonochrome</kbd>: This is <kbd>True</kbd> if the captured image should be grayscale, and <kbd>False</kbd> if it should be BGR.</li>
</ul>
<p>The following code shows how we declare the <kbd>PySpinCapture</kbd> class and its <kbd>__init__</kbd> method:</p>
<pre>class PySpinCapture:<br/><br/><br/>    def __init__(self, index, roi, binningRadius=1,<br/>                 isMonochrome=False):</pre>
<p>PySpin and the underlying Spinnaker SDK are organized around a hierarchical model of a system, the cameras in the system, and the respective configurations of the cameras. Each camera's configuration is organized into a so-called <strong>node map</strong>, which defines properties, their supported values, and their current current values. To begin the implementation of our <kbd>__init__</kbd> method, we get an instance of the system, a list of cameras, and a specific camera by index. We initialize this camera and get its node map. All of this is seen in the following code:</p>
<pre>        self._system = PySpin.System.GetInstance()<br/><br/>        self._cameraList = self._system.GetCameras()<br/><br/>        self._camera = self._cameraList.GetByIndex(index)<br/>        self._camera.Init()<br/><br/>        self._nodemap = self._camera.GetNodeMap()</pre>
<p>We are interested in capturing a continuous series of video frames, rather than isolated still images. To support video capture, <kbd>PySpin</kbd> allows us to set a camera's <kbd>'AcquisitionMode'</kbd> property to a value for a <kbd>'Continuous'</kbd> capture: </p>
<pre>        # Enable continuous acquisition mode.<br/>        nodeAcquisitionMode = PySpin.CEnumerationPtr(<br/>                self._nodemap.GetNode('AcquisitionMode'))<br/>        nodeAcquisitionModeContinuous = \<br/>                nodeAcquisitionMode.GetEntryByName(<br/>                        'Continuous')<br/>        acquisitionModeContinuous = \<br/>                nodeAcquisitionModeContinuous.GetValue()<br/>        nodeAcquisitionMode.SetIntValue(<br/>                acquisitionModeContinuous)</pre>
<p>For more information about nodes, their names, and relevant documentation, see the technical note<em> Spinnaker Nodes</em> on the FLIR website at <a href="https://www.flir.com/support-center/iis/machine-vision/application-note/spinnaker-nodes/">https://www.flir.com/support-center/iis/machine-vision/application-note/spinnaker-nodes/</a>.</p>
<p>Next, we set a property called <kbd>'PixelFormat'</kbd> to either a value called <kbd>'Mono8'</kbd> or a value called <kbd>'BGR8'</kbd>, depending on whether our <kbd>__init__</kbd> method's <kbd>isMonochrome</kbd> argument is <kbd>True</kbd>. Here is the relevant code:</p>
<pre>        # Set the pixel format.<br/>        nodePixelFormat = PySpin.CEnumerationPtr(<br/>            self._nodemap.GetNode('PixelFormat'))<br/>        if isMonochrome:<br/>            # Enable Mono8 mode.<br/>            nodePixelFormatMono8 = PySpin.CEnumEntryPtr(<br/>                    nodePixelFormat.GetEntryByName('Mono8'))<br/>            pixelFormatMono8 = \<br/>                    nodePixelFormatMono8.GetValue()<br/>            nodePixelFormat.SetIntValue(pixelFormatMono8)<br/>        else:<br/>            # Enable BGR8 mode.<br/>            nodePixelFormatBGR8 = PySpin.CEnumEntryPtr(<br/>                    nodePixelFormat.GetEntryByName('BGR8'))<br/>            pixelFormatBGR8 = nodePixelFormatBGR8.GetValue()<br/>            nodePixelFormat.SetIntValue(pixelFormatBGR8)</pre>
<p>Similarly, we set a <kbd>'BinningVertical'</kbd> property based on our <kbd>binningRadius</kbd> argument (the horizontal binning radius is automatically set to the same value as the vertical binning radius). Here is the relevant code:</p>
<pre>        # Set the vertical binning radius.<br/>        # The horizontal binning radius is automatically set<br/>        # to the same value.<br/>        nodeBinningVertical = PySpin.CIntegerPtr(<br/>                self._nodemap.GetNode('BinningVertical'))<br/>        nodeBinningVertical.SetValue(binningRadius)</pre>
<p>Likewise, based on the <kbd>roi</kbd> argument, we set the values of properties named <kbd>'OffsetX'</kbd>, <kbd>'OffsetY'</kbd>, <kbd>'Width'</kbd>, and <kbd>'Height'</kbd>, as seen in the following code:</p>
<pre>        # Set the ROI.<br/>        x, y, w, h = roi<br/>        nodeOffsetX = PySpin.CIntegerPtr(<br/>                self._nodemap.GetNode('OffsetX'))<br/>        nodeOffsetX.SetValue(x)<br/>        nodeOffsetY = PySpin.CIntegerPtr(<br/>                self._nodemap.GetNode('OffsetY'))<br/>        nodeOffsetY.SetValue(y)<br/>        nodeWidth = PySpin.CIntegerPtr(<br/>                self._nodemap.GetNode('Width'))<br/>        nodeWidth.SetValue(w)<br/>        nodeHeight = PySpin.CIntegerPtr(<br/>                self._nodemap.GetNode('Height'))<br/>        nodeHeight.SetValue(h)</pre>
<p><kbd>cv2.VideoCapture</kbd> starts a capture session as soon as it is constructed, so we want to do the same thing in <kbd>PySpinCapture</kbd>. So, we finish the <kbd>__init__</kbd> method's implementation with the following line of code, which tells the camera to start acquiring frames:</p>
<pre>        self._camera.BeginAcquisition()</pre>
<p>We use the node map again in the implementation of the <kbd>get</kbd> method. If <kbd>cv2.CAP_PROP_FRAME_WIDTH</kbd> is requested, we return the value of the <kbd>'Width'</kbd> property. If, instead, <kbd>cv2.CAP_PROP_FRAME_HEIGHT</kbd> is requested, we return the value of the <kbd>'Height'</kbd> property. For any other request, we return <kbd>0.0</kbd>. Here is the method's implementation:</p>
<pre>    def get(self, propId):<br/>        if propId == cv2.CAP_PROP_FRAME_WIDTH:<br/>            nodeWidth = PySpin.CIntegerPtr(<br/>                    self._nodemap.GetNode('Width'))<br/>            return float(nodeWidth.GetValue())<br/>        if propId == cv2.CAP_PROP_FRAME_HEIGHT:<br/>            nodeHeight = PySpin.CIntegerPtr(<br/>                    self._nodemap.GetNode('Height'))<br/>            return float(nodeHeight.GetValue())<br/>        return 0.0</pre>
<p>We begin the implementation of the <kbd>read</kbd> method by telling the camera to capture a frame. If this fails, we return <kbd>False</kbd> and <kbd>None</kbd> (no image). Otherwise, we get the frame's dimensions and number of channels, get its data as a NumPy array, and reshape this array to match the format that OpenCV expects. We copy the data, release the original frame, and then return <kbd>True</kbd> and the copied image. Here is the method's implementation:</p>
<pre>    def read(self, image=None):<br/><br/>        cameraImage = self._camera.GetNextImage()<br/>        if cameraImage.IsIncomplete():<br/>            return False, None<br/><br/>        h = cameraImage.GetHeight()<br/>        w = cameraImage.GetWidth()<br/>        numChannels = cameraImage.GetNumChannels()<br/>        if numChannels &gt; 1:<br/>            cameraImageData = cameraImage.GetData().reshape(<br/>                    h, w, numChannels)<br/>        else:<br/>            cameraImageData = cameraImage.GetData().reshape(<br/>                    h, w)<br/><br/>        if image is None:<br/>            image = cameraImageData.copy()<br/>        else:<br/>            image[:] = cameraImageData<br/><br/>        cameraImage.Release()<br/><br/>        return True, image</pre>
<p>We implement the <kbd>release</kbd> method by telling the camera to stop acquiring frames, de-initializing and deleting the camera, clearing the list of cameras, and releasing the <kbd>PySpin</kbd> system. Here is the relevant code:</p>
<pre>    def release(self):<br/><br/>        self._camera.EndAcquisition()<br/>        self._camera.DeInit()<br/>        del self._camera<br/><br/>        self._cameraList.Clear()<br/><br/>        self._system.ReleaseInstance()</pre>
<p>To complete the implementation of the <kbd>PySpinCapture</kbd> class, we provide the following destructor or <kbd>__del__</kbd> method, which simply calls the <kbd>release</kbd> method that we implemented previously:</p>
<pre>    def __del__(self):<br/>        self.release()</pre>
<p class="mce-root">Next, let's look at how to use <kbd>PySpinCapture</kbd> or <kbd>cv2.VideoCapture</kbd> interchangeably in our application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adapting the Lazy Eyes app to make Sunbaker</h1>
                
            
            
                
<p>As we discussed at the start of this chapter, <kbd>Sunbaker</kbd> is a variant of <kbd>Lazy Eyes</kbd> with support for more cameras. As a starting point, make a copy of the completed <kbd>LazyEyes.py</kbd> script from <a href="7cc1c0b9-a764-4069-9d45-e8bf129efc57.xhtml"/><a href="7cc1c0b9-a764-4069-9d45-e8bf129efc57.xhtml">Chapter 7</a>, <em>Seeing a Heartbeat with a Motion-Amplifying Camera</em>, and rename it <kbd>Sunbaker.py</kbd>. The supported cameras in <kbd>Sunbaker</kbd> will vary depending on the modules that are available at runtime.</p>
<p>Add the following <kbd>try</kbd>/<kbd>except</kbd> block after the other <kbd>import</kbd> statements in <kbd>Sunbaker.py</kbd>:</p>
<pre>try:<br/>    import PySpinCapture<br/>except ImportError:<br/>    PySpinCapture = None</pre>
<p>The preceding block of code tries to import our <kbd>PySpinCapture</kbd> module, which contains our <kbd>getNumCameras</kbd> function and our <kbd>PySpinCapture</kbd> class. The <kbd>PySpinCapture</kbd> module, in turn, imports the <kbd>PySpin</kbd> module, as we saw earlier in this chapter in the <em>Capturing images from industrial cameras using PySpin</em> section. If the <kbd>PySpin</kbd> module is not found, <kbd>ImportError</kbd> is thrown. The preceding block of code catches this error and it defines <kbd>PySpinCapture = None</kbd> as a way to note that we failed to import an optional dependency, namely the <kbd>PySpinCapture</kbd> module. Later in <kbd>Sunbaker.py</kbd>, we will use the <kbd>PySpinCapture</kbd> module only when <kbd>PySpinCapture</kbd> is not <kbd>None</kbd>.</p>
<p>We must modify the <kbd>__init__</kbd> method of the <kbd>Sunbaker</kbd> class to remove the <kbd>cameraDeviceID</kbd> and <kbd>imageSize</kbd> arguments, and instead add a <kbd>capture</kbd> argument and an <kbd>isCaptureMonochrome</kbd> argument. The <kbd>capture</kbd> argument can be either a <kbd>cv2.VideoCapture</kbd> object or a <kbd>PySpinCapture</kbd> object. We assume that <kbd>capture</kbd> argument's width, height, and other properties are already fully configured before capture is passed to <kbd>__init__</kbd>. So, we have no need to call <kbd>ResizeUtils.cvResizeCapture</kbd> in <kbd>__init__</kbd> (and we can remove <kbd>ResizeUtils</kbd> from the list of imports). We attempt to get the image dimensions and format (grayscale or not) from an actual frame. If this fails, we will instead rely on getting the dimensions from the <kbd>capture</kbd> argument's properties and the format from the <kbd>isCaptureMonochrome</kbd> argument. The modifications to <kbd>__init__</kbd> are marked in bold in the following code:</p>
<pre>class Sunbaker(wx.Frame):<br/><br/>    def __init__(self, <strong>capture, isCaptureMonochrome=False,</strong><br/>                 maxHistoryLength=360,<br/>                 minHz=5.0/6.0, maxHz=1.0,<br/>                 amplification=32.0, numPyramidLevels=2,<br/>                 useLaplacianPyramid=True,<br/>                 useGrayOverlay=True,<br/>                 numFFTThreads=4, numIFFTThreads=4,<br/>                 title='Sunbaker'):<br/><br/>        self.mirrored = True<br/><br/>        self._running = True<br/><br/>        self._capture = capture<br/><br/><strong>        # Sometimes the dimensions fluctuate at the start of</strong><br/><strong>        # capture.</strong><br/><strong>        # Discard two frames to allow for this.</strong><br/><strong>        capture.read()</strong><br/><strong>        capture.read()</strong><br/><br/><strong>        success, image = capture.read()</strong><br/><strong>        if success:</strong><br/><strong>            # Use the actual image dimensions.</strong><br/><strong>            h, w = image.shape[:2]</strong><br/><strong>            isCaptureMonochrome = (len(image.shape) == 2)</strong><br/><strong>        else:</strong><br/><strong>            # Use the nominal image dimensions.</strong><br/><strong>            w = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))</strong><br/><strong>            h = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))</strong><br/><strong>        size = (w, h)</strong><br/><strong>        if isCaptureMonochrome:</strong><br/><strong>            useGrayOverlay = True</strong><br/><strong>        self._isCaptureMonochrome = isCaptureMonochrome</strong><br/><br/>        # ... The rest of the method is unchanged ...</pre>
<p>The <kbd>_applyEulerianVideoMagnification</kbd> method needs minor modifications to support the possibility that the input is not a BGR image, but rather a grayscale image from a monochrome camera. Again, the modifications are marked in bold in the following code:</p>
<pre class="mce-root">    def _applyEulerianVideoMagnification(self):<br/><br/>        timestamp = timeit.default_timer()<br/><br/>        <strong>if self._useGrayOverlay and \</strong><br/><strong>               not self._isCaptureMonochrome:</strong><br/><strong>            smallImage = cv2.cvtColor(</strong><br/><strong>                    self._image, cv2.COLOR_BGR2GRAY).astype(</strong><br/><strong>                            numpy.float32)</strong><br/><strong>        else:</strong><br/><strong>            smallImage = self._image.astype(numpy.float32)</strong><br/><br/>        # ... The middle part of the method is unchanged ...<br/><br/>        # Amplify the result and overlay it on the<br/>        # original image.<br/>        overlay = numpy.real(ifftResult[-1]) * \<br/>                          self._amplification<br/>        i = 0<br/>        while i &lt; self._numPyramidLevels:<br/>            overlay = cv2.pyrUp(overlay)<br/>            i += 1<br/><strong>        if self._useGrayOverlay and \</strong><br/><strong>                not self._isCaptureMonochrome:</strong><br/><strong>            overlay = cv2.cvtColor(overlay,</strong><br/><strong>                                   cv2.COLOR_GRAY2BGR)</strong><br/>        cv2.add(self._image, overlay, self._image,<br/>                dtype=cv2.CV_8U)</pre>
<p>Finally, the <kbd>main</kbd> function needs modifications to provide appropriate <kbd>capture</kbd> and <kbd>isCaptureMonochrome</kbd> arguments to <kbd>Sunbaker</kbd> application's <kbd>__init__</kbd> method. As an example, let's suppose that if <kbd>PySpin</kbd> is available, we want to use a monochrome camera with a binning radius of <kbd>2</kbd> and a capture resolution of <em>960 x 600</em>. (The GS3-U3-23S6M-C camera supports this configuration.) Alternatively, if <kbd>PySpin</kbd> is unavailable or if no PySpin-compatible camera is connected, let's use an OpenCV-compatible camera with a capture resolution of <em>640 x 480</em> at 60 FPS. The relevant modifications are marked in bold in the following code:</p>
<pre>def main():<br/><br/>    app = wx.App()<br/><br/><strong>    if PySpinCapture is not None and \<br/>            PySpinCapture.getNumCameras() &gt; 0:</strong><br/><strong>        isCaptureMonochrome = True</strong><br/><strong>        capture = PySpinCapture.PySpinCapture(</strong><br/><strong>                0, roi=(0, 0, 960, 600), binningRadius=2,</strong><br/><strong>                isMonochrome=isCaptureMonochrome)</strong><br/><strong>    else:</strong><br/><strong>        # 320x240 @ 187 FPS</strong><br/><strong>        #capture.set(cv2.CAP_PROP_FRAME_WIDTH, 320)</strong><br/><strong>        #capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)</strong><br/><strong>        #capture.set(cv2.CAP_PROP_FPS, 187)</strong><br/><br/><strong>        # 640x480 @ 60 FPS</strong><br/><strong>        capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)</strong><br/><strong>        capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)</strong><br/><strong>        capture.set(cv2.CAP_PROP_FPS, 60)</strong><br/><br/><strong>    # Show motion at edges with grayscale contrast.</strong><br/><strong>    sunbaker = Sunbaker(capture, isCaptureMonochrome)</strong><br/><br/><strong>    sunbaker.Show()</strong><br/>    app.MainLoop()</pre>
<p>You might need to modify the preceding code based on the capture modes that are supported by your cameras. If you are interested in using the PlayStation Eye camera at its maximum frame rate, you should comment out the lines of code that pertain to <em>640 x 480</em> resolution at 60 FPS, and uncomment the lines of code that pertain to <em>320 x 240</em> resolution at 187 FPS.</p>
<p>This brings us to the end of the code revisions. Now, you can test <kbd>Sunbaker</kbd> with either a Point Grey camera or an OpenCV-compatible camera such as a USB webcam. Take some time to adjust the camera parameters, as well as the parameters of the Eulerian video magnification algorithm (the latter are described in detail in <a href="7cc1c0b9-a764-4069-9d45-e8bf129efc57.xhtml">Chapter 7</a>, <em>Seeing a Heartbeat with a Motion-Amplifying Camera</em>, in the <em>Configuring and testing the app for various motions</em> section). Experiment with a variety of subjects and lighting conditions, including outdoor sunlight. If you are using a UV camera, remember to look at the flowers!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter has broadened our view of the things cameras can see. We have considered video capture at high frame rates and at wavelengths of light that are invisible to the human eye. As programmers, we have learned to wrap a third-party camera API in a way that allows us to use industrial cameras and OpenCV-compatible webcams interchangeably, thanks to Python's duck typing. As experimenters, we have extended our study of Eulerian video magnification into higher frequencies of motion, as well as more surprising patterns of pulsing light beyond the visible spectrum.</p>
<p>Let's reflect on all our progress. From finding the head of SPECTRE to exploring the electromagnetic spectrum, our journey as secret agents has taken us far. At this proud moment, however, our adventure must reach its conclusion. We will meet again. Look out for future books, webcasts, and presentations, to be announced on my website at <a href="http://nummist.com/opencv">http://nummist.com/opencv</a>. Also, email me at <a href="mailto:josephhowse@nummist.com">josephhowse@nummist.com</a> to report issues, ask questions, and tell me how you are using OpenCV.</p>
<p>The book is ending now and I am waiting to find out whether I disappear into the sunset with a femme fatale or whether I have a melancholy debriefing with M.</p>


            

            
        
    </body></html>