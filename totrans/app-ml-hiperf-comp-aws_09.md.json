["```py\n    …\n    ```", "```py\n    from torchvision.models import resnet18, ResNet18_Weights\n    ```", "```py\n    #initialize the model\n    ```", "```py\n    weights = ResNet18_Weights.DEFAULT\n    ```", "```py\n    model = resnet18(weights)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    torch.save(model.state_dict(),\n    ```", "```py\n               './output/resnet18-model.pt')\n    ```", "```py\n    with tarfile.open(\"model.tar.gz\", \"w:gz\") as f:\n    ```", "```py\n        f.add(\"model.pth\")\n    ```", "```py\n    input_tensor = torch.zeros([1, 3, 224, 224])\n    ```", "```py\n    model_uri = sagemaker_session.upload_data(\n    ```", "```py\n        path=\"model.tar.gz\", key_prefix=key_prefix)\n    ```", "```py\n    print(\"S3 Path for Model: \", model_uri)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    compilation_job_name = name_from_base(\"image-classification-neo\")\n    ```", "```py\n    prefix = key_prefix+'/'+compilation_job_name+\"/model\"\n    ```", "```py\n    data_shape = '{\"input0\":[1,3,224,224]}'\n    ```", "```py\n    target_device = \"ml_c5\"\n    ```", "```py\n    framework = \"PYTORCH\"\n    ```", "```py\n    framework_version = \"1.8\"\n    ```", "```py\n    compiled_model_path = \"s3://{}/{}/output\".format(bucket, compilation_job_name)\n    ```", "```py\n    print(\"S3 path for compiled model: \", compiled_model_path)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    from sagemaker.pytorch.model import PyTorchModel\n    ```", "```py\n    from sagemaker.predictor import Predictor\n    ```", "```py\n    sagemaker_model = PyTorchModel(\n    ```", "```py\n        model_data=model_uri,\n    ```", "```py\n        predictor_cls=Predictor,\n    ```", "```py\n        framework_version=framework_version,\n    ```", "```py\n        role=role,\n    ```", "```py\n        sagemaker_session=sagemaker_session,\n    ```", "```py\n        entry_point=\"inference.py\",\n    ```", "```py\n        source_dir=\"code\",\n    ```", "```py\n        py_version=\"py3\",\n    ```", "```py\n        env={\"MMS_DEFAULT_RESPONSE_TIMEOUT\": \"500\"},\n    ```", "```py\n    )\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    sagemaker_client = boto3.client(\"sagemaker\",\n    ```", "```py\n        region_name=region)\n    ```", "```py\n    target_arch = \"X86_64\"\n    ```", "```py\n    target_os = 'LINUX'\n    ```", "```py\n    response = sagemaker_client.create_compilation_job(\n    ```", "```py\n        CompilationJobName=compilation_job_name,\n    ```", "```py\n        RoleArn=role,\n    ```", "```py\n        InputConfig={\n    ```", "```py\n            \"S3Uri\": sagemaker_model.model_data,\n    ```", "```py\n            \"DataInputConfig\": data_shape,\n    ```", "```py\n            \"Framework\": framework,\n    ```", "```py\n        },\n    ```", "```py\n        OutputConfig={\n    ```", "```py\n            \"S3OutputLocation\": compiled_model_path,\n    ```", "```py\n            \"TargetDevice\": 'jetson_nano',\n    ```", "```py\n            \"TargetPlatform\": {\n    ```", "```py\n                \"Arch\": target_arch,\n    ```", "```py\n                \"Os\": target_os\n    ```", "```py\n            },\n    ```", "```py\n        },\n    ```", "```py\n        StoppingCondition={\"MaxRuntimeInSeconds\": 900},\n    ```", "```py\n    )\n    ```", "```py\n    print(response)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    predictor = compiled_model.deploy(\n    ```", "```py\n        initial_instance_count=1,\n    ```", "```py\n        instance_type=\"ml.c5.2xlarge\")\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    import numpy as np\n    ```", "```py\n    import json\n    ```", "```py\n    with open(\"horse_cart.jpg\", \"rb\") as f:\n    ```", "```py\n        payload = f.read()\n    ```", "```py\n        payload = bytearray(payload)\n    ```", "```py\n    response = runtime.invoke_endpoint(\n    ```", "```py\n        EndpointName=ENDPOINT_NAME,\n    ```", "```py\n        ContentType='application/octet-stream',\n    ```", "```py\n        Body=payload,\n    ```", "```py\n        Accept = 'application/json')\n    ```", "```py\n    result = response['Body'].read()\n    ```", "```py\n    result = json.loads(result)\n    ```", "```py\n    print(result)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    # delete endpoint after testing the inference\n    ```", "```py\n    import boto3\n    ```", "```py\n    # Create a low-level SageMaker service client.\n    ```", "```py\n    sagemaker_client = boto3.client('sagemaker',\n    ```", "```py\n                                    region_name=region)\n    ```", "```py\n    # Delete endpoint\n    ```", "```py\n    sagemaker_client.delete_endpoint(\n    ```", "```py\n        EndpointName=ENDPOINT_NAME)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    # ML model details\n    ```", "```py\n    ml_domain = \"COMPUTER_VISION\"\n    ```", "```py\n    ml_task = \"IMAGE_CLASSIFICATION\"\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    from torchvision.models import resnet18, ResNet18_Weights\n    ```", "```py\n    #initialize the model\n    ```", "```py\n    weights = ResNet18_Weights.DEFAULT\n    ```", "```py\n    model = resnet18(weights)\n    ```", "```py\n    torch.save(model.state_dict(), './output/resnet18-model.pt')\n    ```", "```py\n    with tarfile.open(\"model.tar.gz\", \"w:gz\") as f:\n    ```", "```py\n        f.add(\"model.pth\")\n    ```", "```py\n    input_tensor = torch.zeros([1, 3, 224, 224])\n    ```", "```py\n    model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=key_prefix)\n    ```", "```py\n    print(\"S3 Path for Model: \", model_uri)\n    ```", "```py\n    …\n    ```", "```py\n    …\n    ```", "```py\n    instance_type = \"ml.c5.xlarge\"  # Note: you can use any CPU-based instance type here, this is just to get a CPU tagged image\n    ```", "```py\n    container_uri = image_uris.retrieve(\n    ```", "```py\n        \"pytorch\",\n    ```", "```py\n        region,\n    ```", "```py\n        version=framework_version,\n    ```", "```py\n        py_version=\"py3\",\n    ```", "```py\n        instance_type=instance_type,\n    ```", "```py\n        image_scope=\"inference\",\n    ```", "```py\n    )\n    ```", "```py\n    container_uri\n    ```", "```py\n    …\n    ```", "```py\n    !wget https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/019/1390196df443f2cf614f2255ae75fcf8.jpg -P /sample-payload\n    ```", "```py\n    !wget https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/015/1390157d4caaf290962de5c5fb4c42.jpg -P /sample-payload\n    ```", "```py\n    !wget https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/020/1390207be327f4c4df1259c7266473.jpg  -P /sample-payload\n    ```", "```py\n    !wget https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/028/139028d865bafa3de66568eeb499f4a6.jpg  -P /sample-payload\n    ```", "```py\ncd ./sample-payload/ && tar czvf ../{payload_archive_name} *\n```", "```py\n…\nsample_payload_url = sagemaker_session.upload_data(path=payload_archive_name, key_prefix=\"tf_payload\")\n…\n```", "```py\n        …\n        ```", "```py\n        model_package_group_input_dict = {\n        ```", "```py\n        \"ModelPackageGroupName\": model_package_group_name,\n        ```", "```py\n        \"ModelPackageGroupDescription\": model_package_group_description,\n        ```", "```py\n        }\n        ```", "```py\n        create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n        ```", "```py\n        …\n        ```", "```py\n        …\n        ```", "```py\n        model_approval_status = \"PendingManualApproval\"\n        ```", "```py\n        # provide an input dictionary with configuration for registering the model\n        ```", "```py\n        model_package_input_dict = {\n        ```", "```py\n            \"ModelPackageGroupName\": model_package_group_name,\n        ```", "```py\n            \"Domain\": ml_domain.upper(),\n        ```", "```py\n            \"Task\": ml_task.upper(),\n        ```", "```py\n            \"SamplePayloadUrl\": sample_payload_url,\n        ```", "```py\n            \"ModelPackageDescription\": model_package_description,\n        ```", "```py\n            \"ModelApprovalStatus\": model_approval_status,\n        ```", "```py\n        }# optional – provide a list of instances\n        ```", "```py\n        supported_realtime_inference_types = [\"ml.c5.xlarge\", \"ml.m5.large\", \"ml.inf1.xlarge\"]\n        ```", "```py\n        ...\n        ```", "```py\n        #create model inference specification object\n        ```", "```py\n        modelpackage_inference_specification = {\n        ```", "```py\n            \"InferenceSpecification\": {\n        ```", "```py\n                \"Containers\": [\n        ```", "```py\n                    {\n        ```", "```py\n                        \"Image\": container_uri,\n        ```", "```py\n                        \"Framework\": \"PYTORCH\",\n        ```", "```py\n                        \"FrameworkVersion\": framework_version,\n        ```", "```py\n                        \"ModelInput\": {\"DataInputConfig\": data_input_configuration},\n        ```", "```py\n                    }\n        ```", "```py\n                ],\n        ```", "```py\n                \"SupportedContentTypes\": \"application/image\",\n        ```", "```py\n                \"SupportedRealtimeInferenceInstanceTypes\": supported_realtime_inference_types,  # optional\n        ```", "```py\n            }\n        ```", "```py\n        }\n        ```", "```py\n        # Specify the model data\n        ```", "```py\n        modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = model_url\n        ```", "```py\n        create_model_package_input_dict.update(modelpackage_inference_specification)\n        ```", "```py\n        ...\n        ```", "```py\n        create_mode_package_response = sm_client.create_model_package(**model_package_input_dict)\n        ```", "```py\n        model_package_arn = create_mode_package_response[\"ModelPackageArn\"]\n        ```", "```py\n        …\n        ```", "```py\n…\nresponse = sagemaker_client.create_inference_recommendations_job(\n    JobName=str(default_job),\n    JobDescription=\"\",\n    JobType=\"Default\",\n    RoleArn=role,\n    InputConfig={\"ModelPackageVersionArn\": model_package_arn},\n)\nprint(response)\n…\n```", "```py\n…\ndata = [\n    {**x[\"EndpointConfiguration\"], **x[\"ModelConfiguration\"], **x[\"Metrics\"]}\n    for x in inference_recommender_job[\"InferenceRecommendations\"]\n]\ndf = pd.DataFrame(data)\n…\n```"]