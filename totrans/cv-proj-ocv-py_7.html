<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Learning Image Classification with TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn how to classify images using TensorFlow. First, we will use a pre-trained model, and then we'll proceed with training our own model using custom images.</p>
<p>Toward the end of the chapter, we will make use of the GPU to help us speed up our computations.</p>
<p><span>In this chapter, we will cover the following:</span></p>
<ul>
<li>A deep introduction to TensorFlow</li>
<li>Using a pre-trained model (Inception) for image classification</li>
<li>Retraining with our own images</li>
<li>Speeding up computation with the GPU</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Along with knowledge of Python and the basics of image processing and computer vision, you will need the following libraries:</p>
<ul>
<li>TensorFlow</li>
<li>NVIDIA CUDA® Deep Neural Network</li>
</ul>
<p>The code used in the chapter has been added to the following GitHub repository:<br/>
<a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3</a><a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">An introduction to TensorFlow</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we'll go deeper into TensorFlow and see how we can build a general-purpose image classifier using its deep learning method.</p>
<p class="mce-root">This will be an extension of what we learned in <a href="" target="_blank">Chapter 2</a>, <em>Handwritten Digit Recognition with scikit-learn and TensorFlow,</em> where we learned how to classify handwritten digits. However, this method is quite a bit more powerful, as it will work on general images of people, animals, food, everyday objects, and so on.</p>
<p class="mce-root">To start, let's talk a little bit about what TensorFlow does, and the general workflow of TensorFlow.</p>
<p class="mce-root">To begin, what is a tensor? Wikipedia states this:</p>
<p class="mce-root"><em>"In mathematics, tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors... Given a reference basis of vectors, a tensor can be represented as an organized multi-dimensional array of numerical values."</em></p>
<p class="mce-root">However, according to Google, the makers of TensorFlow, a tensor is any multi-dimensional array with any data type. Essentially, according to Google, a tensor can mean basically anything.</p>
<p class="mce-root">Google has generalized the word so much that it doesn't really mean a lot, and personally I don't like that (coming from an engineering and physics background). However, TensorFlow is so powerful and useful that I'm going to get over it. Just be aware that if you're ever worried about misusing the word <em>tensor</em>, don't be, because Google completely misuses it anyway.</p>
<p class="mce-root">For now, all we need to know is that within TensorFlow, a tensor is some sort of data; usually a multi-dimensional array, but it could be basically anything, such as images or text. With that in mind, TensorFlow is, in general, a high-performance numerical library. It is primarily geared toward machine learning, but that doesn't mean that it's exclusively made for machine learning.</p>
<p class="mce-root">TensorFlow can also be used for simulations, solving complex partial differential equations, and just about anything numerical. We're only concerned with machine learning and, in particular, deep learning in this chapter. We are going to be using it for its main purpose, but just be aware that it's generally used for constructing and analyzing complex numerical models.</p>
<p class="mce-root">Before we go into building a classifier, I want to share a little bit about how we would generally use TensorFlow for very basic usage. Start as follows:</p>
<ol>
<li class="mce-root">We're going to change directories, and make sure that we can load key libraries and display images and so forth, using the following code:</li>
</ol>
<pre style="padding-left: 60px" class="CDPAlignLeft CDPAlign">#Get started with needed libraries/settings/directory<br/>%pylab inline<br/>%cd C:\Users\mrever\Documents\packt_CV\tensclass</pre>
<ol start="2">
<li class="mce-root">Next, we import <kbd>tensorflow</kbd> and <kbd>numpy</kbd>, using the standard convention:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tf<br/>import numpy as np</pre>
<p style="padding-left: 60px" class="mce-root">Since we performed <kbd>pylab inline</kbd>, we don't explicitly need to import <kbd>numpy</kbd>, but it's a good practice in general. If we want to copy some of this code out to other scripts, we need to make sure that <kbd>numpy</kbd> is imported.</p>
<ol start="3">
<li class="mce-root">Let's start with a simple TensorFlow example. We're just going to perform some really basic arithmetic. Define some constants within TensorFlow, as follows:</li>
</ol>
<pre style="padding-left: 60px">#Arithmetic with TensorFlow<br/><br/>a = tf.constant(2)<br/>b = tf.constant(3)</pre>
<p style="padding-left: 60px" class="mce-root">These constants can be just scalars, as we defined, or they could be vectors or matrices. We're just going to add them together. When we do that, we can define our constants.</p>
<ol start="4">
<li class="mce-root">We define constants, and then we create a TensorFlow session using the <kbd>with</kbd> clause. When it goes outside the <kbd>with</kbd> clause, we'll close the TensorFlow session, as follows:</li>
</ol>
<pre style="padding-left: 60px">with tf.Session() as sess:<br/>    print("a=2, b=3")<br/>    print("a+b=" + str(sess.run(a+b)))<br/>    print("a*b=" + str(sess.run(a*b)))</pre>
<div class="mce-root packt_infobox"><kbd>Session</kbd> can be important depending on what resources we're using, for example, if we're using a GPU and we want to release it, but in this section, we're just going to be talking about the <kbd>Session</kbd> using the CPU.</div>
<p style="padding-left: 60px" class="mce-root">Within our <kbd>Session</kbd>, TensorFlow has operator overloading where it makes sense. It understands what is meant by <kbd>a+b</kbd>, where <kbd>a</kbd> and <kbd>b</kbd> are both TensorFlow constants. It also understands arithmetic operations such as multiply (<kbd>*</kbd>), minus (<kbd>-</kbd>), divide (<kbd>/</kbd>), and so on.</p>
<ol start="5">
<li class="mce-root">Now, we're going to do the same thing using a different method, by creating <kbd>placeholder</kbd> variables, as follows:</li>
</ol>
<pre style="padding-left: 60px">a = tf.placeholder(tf.int16)<br/>b = tf.placeholder(tf.int16)</pre>
<p style="padding-left: 60px" class="mce-root">Often, we need to construct our model. That's what TensorFlow is based on, it's basically an input-output model. So, we have our input, which could be numbers, images, words, or whatever. We generally need to find placeholders before we input our data, and then define and construct our model.</p>
<ol start="6">
<li class="mce-root">In our case, we're just defining addition, just as we would normally define it, as follows:</li>
</ol>
<pre style="padding-left: 60px">add = tf.add(a, b)<br/>mul = tf.multiply(a, b)</pre>
<p style="padding-left: 60px" class="mce-root">This could be something more complex such as building a neural network, a <strong>convolutional neural network</strong><span> (</span><strong>CNN</strong><span>),</span> and so on.</p>
<p style="padding-left: 60px" class="mce-root">We'll see a bit of that momentarily, but for now we define our inputs, our model, our operations, and so on, and we create what is called a <em>graph</em>, which will take our inputs and map them to the desired outputs.</p>
<ol start="7">
<li class="mce-root">Similarly, we're going to create a <kbd>session</kbd>, and then we're going to run our operations:</li>
</ol>
<pre style="padding-left: 60px">with tf.Session() as sess:<br/>    print("a+b=" + str(sess.run(add, feed_dict={a: 2, b: 3})))<br/>    print("a*b=" + str(sess.run(mul, feed_dict={a: 2, b: 3}))) </pre>
<p style="padding-left: 60px" class="mce-root">In this case, we have to tell it what the values are, and then it does exactly what we expect, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c00971ea-14ad-4385-8110-bc8d991bd7a4.png" style="width:37.75em;height:10.00em;" width="687" height="182"/></p>
<p class="mce-root">Nothing too exciting<span>—</span>this is just so we understand a little bit about what TensorFlow is doing. We'll take advantage of some higher-level libraries for this chapter, but this is important if we want to go further in the future.</p>
<p class="mce-root">Similarly, we're going to do matrix multiplication. As mentioned earlier, the constants can be more than just scalars. In this case, we're defining matrices, a 2 by 2 matrix and a 2 by 1 matrix, using the following steps:</p>
<ol>
<li>We define our matrices as follows:</li>
</ol>
<pre style="padding-left: 60px">#Matrix multiplication<br/>matrix1 = tf.constant([[1., 2.],[9.0,3.14159]])<br/>matrix2 = tf.constant([[3.],[4.]])</pre>
<ol start="2">
<li class="mce-root">Then, we tell it to multiply matrices, as follows:</li>
</ol>
<pre style="padding-left: 60px">product = tf.matmul(matrix1, matrix2)</pre>
<ol start="3">
<li class="mce-root">We create our session:</li>
</ol>
<pre style="padding-left: 60px">with tf.Session() as sess:<br/>    result = sess.run(product)<br/>    print(result)</pre>
<p style="padding-left: 60px" class="mce-root">Now we run it, and then print the results. The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/5d42cb84-37b0-4191-9e76-247f5a004f9a.png" style="width:28.67em;height:16.17em;" width="516" height="292"/></p>
<p class="mce-root">Again, very basic, but very important in the future. We're not going to define our full network in this lesson, because that's very complex and very time-consuming to execute, but just mention the general steps for creating our own CNN.</p>
<p class="mce-root">We're going to create what is known as layers, define our input, and then we create a bunch of layers and stack them together and define how they're connected. We then find the output layer and then we have to define some other things like how we're going to train and how we're going to evaluate it.</p>
<p>The code for this is as follows:</p>
<pre>#creating a convolutional neural network (skeleton--not complete code!)<br/><br/># create a convolutional (not fully connected) layer...<br/>conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)<br/># and down-sample<br/>conv1 = tf.layers.max_pooling2d(conv1, 2, 2)<br/><br/># create second layer<br/>conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)<br/>conv2 = tf.layers.max_pooling2d(conv2, 2, 2)<br/><br/># flatten to 1D<br/>fc1 = tf.contrib.layers.flatten(conv2)<br/><br/># create fully-connected layer<br/>fc1 = tf.layers.dense(fc1, 1024)<br/><br/><br/># final (output/prediction) layer<br/>out = tf.layers.dense(fc1, n_classes)<br/><br/>#...training code etc.</pre>
<p class="mce-root">Again, this is just for our knowledge. Deep learning is a difficult subject, figuring out the necessary architecture and exactly how to train, which is beyond the scope of this chapter (although I would invite you to learn more about it). Here, we're just going to see how we can utilize what's already done<span>—</span>but if you want to go further, this is where you would start.</p>
<p class="mce-root">In the next section, we're going to see how to use a pre-trained model, Inception, to perform our image classification.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using Inception for image classification</h1>
                </header>
            
            <article>
                
<p>In this section, we're going to use a pre-trained model, Inception, from Google to perform image classification. We'll then move on and build our own model<span>—</span>or, at least do some retraining on the model in order to train on our own images and classify our own objects.</p>
<p>For now, we want to see what we can do with a model that's already trained, which would take a lot of time to reproduce from scratch. Let's get started with the code.</p>
<p>Let's go back to Jupyter Notebook. The Notebook file can be found at <a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/Chapter04" target="_blank">https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/Chapter04</a>.<a href="https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3" target="_blank"/></p>
<p>In order to run the code, we're going to need to download a file from TensorFlow's website, from the following link: <a href="http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz" target="_blank">http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz</a>. This is the Inception model.</p>
<p>The model was trained in 2015. It contains a couple of files that define the model, the <em>graph</em> as it is called, defining the input-output relation between the input images that we provide it and the output classification.</p>
<p>It also contains some labeling data because the output isn't class names; it is numbers. This is modified from Google <span>own </span>TensorFlow's example, to make it easier to understand and run in Jupyter Notebook and reduce the amount of code. <span>However, w</span><span>e need to change that. </span></p>
<p>Get the file and completely unzip it. On Windows, readers might use 7-Zip, which will give a TGZ file. Make sure to then untar the TGZ file to get the TXT, PBTXT, and PB files, particularly the PB file, as that is the one that actually contains the trained model.</p>
<p>We create a file called <kbd><span>inceptiondict</span></kbd>, rather than using Google's own convoluted file for mapping the class numbers to the class name.</p>
<p>Let's take a look at the <kbd><span>inceptiondict</span></kbd> file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/496f7a1f-36c0-41db-8c62-cfb74534d5dc.png" width="958" height="643"/></p>
<p>This file has a thousand classes. It would take a very long time to train this yourself, but we don't have to<span>; </span>we can take advantage of this and build off it, as we'll see later.</p>
<p>This file is interesting to look at if we want to know what kinds of images we'll be able to recognize within this pre-built model. There are a lot of animals in the file, some common items, fruits, musical instruments, different kinds of fish; it even recognizes the Japanese game <em>shoji</em>, apparently.</p>
<p>We're going to import this file as a dictionary called <kbd>inceptiondict</kbd>, which will map numbers to their corresponding class descriptions; for example, class <kbd>1</kbd> maps to the description <kbd>"goldfish, Carassius auratus"</kbd>.</p>
<p>Let's explore the main code. Firstly, we import the file as <kbd>inceptiondict</kbd>:</p>
<pre>#The main code:<br/>#"image" is a filename for the image we want to classify<br/><br/>#load our inception-id to English description dictionary<br/>from inceptiondict import inceptiondict</pre>
<p>Now, we have our <kbd>run_inference_on_image</kbd> <span>function, </span>where <kbd>image</kbd> is a filename. It is not the file data<span>—</span>we haven't loaded that yet<span>—j</span>ust the filename for the image that we want to classify.</p>
<p>Then, we check to make sure that filename exists, and create an error if it doesn't. If it does exist, then we use TensorFlow's own loading mechanism in order to read that filename, as follows:</p>
<pre>def run_inference_on_image(image):<br/><br/>    #load image (making sure it exists)<br/>    if not tf.gfile.Exists(image):<br/>        tf.logging.fatal('File does not exist %s', image)<br/>    image_data = tf.gfile.FastGFile(image, 'rb').read()</pre>
<p>We were talking about the graph file before. Unzip the <span>the crucial</span> <kbd>classify_image_graph_def.pb</kbd> file from the TGZ file to the current directory. Open that as a binary by using TensorFlow's own file loading mechanism, and then we're going to create our graph definition from that, as follows:</p>
<pre>    # Load our "graph" file--<br/>    # This graph is a pretrained model that maps an input image<br/>    # to one (or more) of a thousand classes.<br/>    # Note: generating such a model from scratch is VERY computationally<br/>    # expensive<br/>    with tf.gfile.FastGFile('classify_image_graph_def.pb', 'rb') as f:<br/>        graph_def = tf.GraphDef()<br/>        graph_def.ParseFromString(f.read())<br/>        _ = tf.import_graph_def(graph_def, name='')</pre>
<p>Here, we are just loading the pre-trained model. Google already did the hard work for us, and we're going to read from that.</p>
<p>Then, as we did previously, we need to create our TensorFlow session. We do that with the <kbd>with</kbd> clause, as follows:</p>
<pre>   #create a TF session to actually apply our model<br/>    with tf.Session() as sess:<br/>        # Some useful tensors:<br/>        # 'softmax:0': A tensor containing the normalized prediction across<br/>        # 1000 labels.<br/>        # 'pool_3:0': A tensor containing the next-to-last layer containing 2048<br/>        # float description of the image.<br/>        # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG<br/>        # encoding of the image.<br/>        # Runs the softmax tensor by feeding the image_data as input to the graph.<br/>        softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')<br/>        predictions = sess.run(softmax_tensor,<br/>                           {'DecodeJpeg/contents:0': image_data})<br/>        predictions = np.squeeze(predictions)<br/>        <br/>        #The output here is a 1000 length vector, each element between 0 and 1, <br/>        #that sums to 1. Each element may be thought of as a probability<br/>        #that the image corresponds to a given class (object type, e.g. bird, <br/>        #plane, and so on).</pre>
<p>This model already has multiple layers called tensors. We need to extract the <kbd>softmax</kbd> layer.</p>
<p>The output of our model isn't just going to be that something was detected 100%; what it does is give a probability for each one. We might have, for example, a 90% probability that our image is some sort of cat, a 20% probability that it's a squirrel, and 0.01% that it's a chair or something. Yes, you do get some wild classifications sometimes, although typically those probabilities are very small.</p>
<p>Some fraction of probability is calculated for each one of the thousand classes. Of course, the vast majority of them are going to be zero or very, very close to zero.</p>
<p>We want to extract the next-to-last layer, containing 2048 close descriptions of the image and the input image that provides the JPEG encoding of the image. Note that we didn't load the raw image data in a two-dimensional or three-dimensional vector (or tensor as they call it)<span>—</span>we still have it in JPEG encoding. We're just defining our variables to extract the outputs and find the inputs.</p>
<p>NumPy's <kbd>squeeze</kbd> gets rid of all singleton dimensions. So, if we have a 1 by 1000, this will convert it to a 1000 by 1.</p>
<p>Okay, so, we understand the inputs and outputs within our session. Just for understanding's sake, we only want to extract the top five predictions, and we're going to filter out predictions that have a probability of less than 10%. At most, we're going to get five predictions, but i<span>t usually will be less as we disregard anything below 10%, as follows:</span></p>
<pre>        #We only care about the top 5 (at most) predictions, and ones that have<br/>        #at least a 10% probability of a match<br/>        num_top_predictions= 5<br/>        top_k = predictions.argsort()[-num_top_predictions:][::-1]<br/>        for node_id in top_k:<br/>            human_string = inceptiondict[node_id]<br/>            score = predictions[node_id]<br/>            if score &gt; 0.1:<br/>                print('%s (score = %.5f)' % (human_string, score))</pre>
<p>We run the model and get the output of the image, and then we sort by our top five. We then iterate over those top predictions and convert to a human string by running the output's <kbd>node_id</kbd> through our <kbd>inceptiondict</kbd> dictionary. We read the <kbd>score</kbd>, and then we only print the output if the <kbd>score</kbd> is greater than 10%.</p>
<p>We're just defining the function, we're not running it, so this should be instantaneous to run.</p>
<p>Now, we're going to run this on some images. There are some sample images in a <kbd>sample_imgs</kbd> subdirectory. What we want do is test this, so just uncomment out one of these following lines to define our <kbd>image</kbd> variable:</p>
<pre>#uncomment out one of these lines to test<br/><br/>image='sample_imgs/cropped_panda.jpg'<br/># image='sample_imgs/dog.jpg'<br/># image='sample_imgs/bicycle2.jpg'<br/># image='sample_imgs/garbagecan.jpg'<br/># image='sample_imgs/bunny.jpg'<br/># image='sample_imgs/trombone.jpg'<br/># image='sample_imgs/treasurechest.jpg'<br/># image='sample_imgs/hotdog.jpg'<br/>figure()<br/>imshow(imread(image))<br/>run_inference_on_image(image)</pre>
<p>Then, we're going to create a figure, look at what we see using the <kbd>imshow</kbd> function, and then use the <kbd>run_inference_on_image</kbd> function, which will output the results.</p>
<p>To run the preceding block of code with our <kbd>cropped_panda.jpg</kbd> picture, uncomment the panda picture line. We can see the picture in the following output. It has classified it with about 90% probability as a <kbd>panda</kbd>, <kbd>giant panda</kbd>, or other synonym, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/3e0d0533-7f24-4ce6-b682-6ba2cfd74dea.png" width="863" height="291"/></p>
<p>Let's try it on something else. How about our <kbd>bicycle2.jpg</kbd> file? Uncomment the <kbd>bicycle2.jpg</kbd> line while commenting back the <kbd>cropped_panda.jpg</kbd> line, and we get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d65a051f-efee-4c1e-9858-8828f8245b18.png" style="width:40.58em;height:19.25em;" width="671" height="318"/></p>
<p>It has classified the picture with 91% probability as a <kbd>mountain bike</kbd>.</p>
<p>We are getting a little specific here. Let's try now with the <kbd>garbagecan.jpg</kbd> file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/dfa410be-077c-4300-9284-bc03611180a3.png" width="871" height="387"/></p>
<p>It wasn't as confident here<span>, </span>only about 67% probability in its classification. Sometimes that's the best we can do, but that's not too bad. That was the most likely result.</p>
<p>Let's try the <kbd>bunny.jpg</kbd> file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/39d8e0ef-3a4f-4f22-846c-c5eedff42afd.png" style="width:39.33em;height:18.75em;" width="663" height="318"/></p>
<p class="CDPAlignLeft CDPAlign">Alright, 87% probability that we have a rabbit. Looks pretty good.</p>
<p class="CDPAlignLeft CDPAlign">Now, let's try the <kbd>trombone.jpg</kbd> file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/df167616-f3f9-46de-97fb-0c58d795e430.png" style="width:33.00em;height:19.67em;" width="537" height="321"/></p>
<p class="CDPAlignLeft CDPAlign">Wow, very certain. Over 99% probability that the picture is of a <kbd>trombone</kbd><span>—</span>very good.</p>
<p class="CDPAlignLeft CDPAlign">If you're a fan of a certain popular TV show, you might be wondering whether the classifier can recognize a hot dog. The answer to that is yes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/7085af23-dd09-4e00-b4b7-6d2dfa96cb8a.png" style="width:29.33em;height:19.00em;" width="495" height="320"/></p>
<p class="CDPAlignLeft CDPAlign">It does recognize a <kbd>hotdog</kbd>, with 97% confidence.</p>
<p class="CDPAlignLeft CDPAlign">Finally, we're going to run our classifier on <kbd>dog.jpg</kbd>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cdd42cb6-d6ac-418f-bfc9-9c791a2ec34d.png" style="width:30.00em;height:18.50em;" width="523" height="322"/></p>
<p class="CDPAlignLeft CDPAlign">Whoever trained this model was apparently a dog lover, so they defined a bunch of different dog classes. We get <kbd>Irish wolfhound</kbd>, <kbd>Russian wolfhound</kbd>, <kbd>gazelle hound</kbd>, and others returned. It seems to think that it's one of those!</p>
<p class="CDPAlignLeft CDPAlign">This is working pretty well. If what we need happens to fall within those 1,000 classes, then we're in good shape here. You should be able to adapt the code in the Jupyter Notebook to your needs. Hopefully, deep learning and image classification don't seem quite as intimidating as they did before.</p>
<p class="CDPAlignLeft CDPAlign">So, with that, we're going to move on to the next section, where we do some retraining with our own images and classify objects that are not already in Google's training database.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Retraining with our own images</h1>
                </header>
            
            <article>
                
<p>In this section, we're going to go beyond what we did with the pre-built classifier and use our own images with our own labels.</p>
<p>The first thing I should mention is that this isn't really training from scratch with deep learning<span>—</span>there are multiple layers and algorithms for training the whole thing, which are very time-consuming<span>—</span>but we can take advantage of something called <em>transfer learning</em>, where we take the first few layers that were trained with a very large number of images, as illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0e63aeb4-46f6-4056-9ebb-35158efdabb3.png" style="width:39.25em;height:34.08em;" width="764" height="663"/></p>
<p><span>It's one of the caveats of deep learning that having a few hundred or a few thousand images isn't enough. </span>You need hundreds of thousands or even millions of samples in order to get good results, and gathering that much data is very time-consuming. Also, running it on a personal computer, which I expect most people are using, is not computationally feasible.</p>
<p>But the good news is that we can take layers from our pre-existing model and just do some tweaking at the end, and get some very good results. We're taking advantage of the pre-training by using input features that were trained on hundreds of thousands or millions of images, and transferring them to image types that the model has never seen before.</p>
<p>To do this, we borrow some code from TensorFlow Hub (<a href="https://www.tensorflow.org/hub/" target="_blank">https://www.tensorflow.org/hub/</a>). But we have to make some tweaks to make it run more easily with reduced code and make it so that we can just drop it into our Jupyter Notebook and run it.</p>
<p>In order to get started, we need some images on which to train, and different ways of doing that. Google has kindly provided a sample <span>c</span><span>alled </span><kbd>flower_photos</kbd> at the following link: <a href="http://download.tensorflow.org/example_images/flower_photos.tgz" target="_blank">http://download.tensorflow.org/example_images/flower_photos.tgz</a>. Once again, it's a TGZ file, so download the file and thoroughly unzip it.</p>
<p>You'll get a <kbd>flower_photos</kbd> directory, which will contain some subdirectories of different kinds of flowers such as tulips, dandelions, and so on, which were not among the 1,000 original classes. Those directory names will serve as the labels for those images. All we have to do is unzip them and then input flower photos in the our code.</p>
<p>A cheap method to get a whole lot of photos is to use the Fatkun Batch Download plugin for Chrome (<a href="https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en" target="_blank">https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf?hl=en</a>). Using this, we can go somewhere like Google Image Search and search for whatever kind of object we want<span>—</span>animal, food, and so on<span>—and </span>grab hundreds of images pretty quickly.</p>
<p>There are similar plugins for Firefox, or whatever web browser you are using. As long as you don't mind using those kinds of images, if they will suit your needs then this is a good way to do it.</p>
<p>After you're finished with the flower photos, I would suggest grabbing your own images. Think of something that you'd like to train on, something that you think would be useful. Try to get at least 100 images of each class and grab multiple classes.</p>
<p>For illustration purposes, I decided to classify some toys. Maybe you're running a toy store and you're taking inventory, or you're a collector and you want to know what exactly is in there<span>—</span>you just have a bunch of photos, and you want to classify them.</p>
<p>I created four subfolders called <kbd>barbie</kbd>, <kbd>gi joe</kbd>, <kbd>my little pony</kbd>, and <kbd>transformers</kbd>, shown as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2cc11a67-080f-4674-90a6-1854428df2cc.png" style="width:40.67em;height:8.25em;" width="440" height="89"/></p>
<p>Each folder contains over 100 images of each type. The filenames are not important<span>—j</span>ust the directory names are going to be used for the labeling.</p>
<p>So you can test whether or not it's working, you need to separate out some images. If you test on images that you trained on, then you're kind of cheating<span>—y</span>ou don't really know whether or not your model has generalized. So, make sure to pull out some images from that directory and put them in a separate directory for now.</p>
<p>The code for retraining is introduced in the Jupyter Notebook file itself, so we're not going to go through the whole thing. We've created a file called <kbd>retrained.py</kbd>, which is based on the TensorFlow Hub version, but is more easily dropped into existing code and a lot of the variables are already taken care of. </p>
<p>All we need to do is import the <kbd>retrain</kbd> function, and then we retrain on our <kbd>toy_images</kbd> folder, as follows:</p>
<pre>#pull the function from our custom retrain.py file<br/>from retrain import retrain<br/><br/>#Now we'll train our model and generate our model/graph file 'output_graph.pb'<br/>retrain('toy_images')</pre>
<p>This generally takes a while. If you run the code on the <kbd>flower_photos</kbd> directory, that will take about half an hour, especially if doing it on a CPU and not a GPU. The <kbd>toy_images</kbd> example will take a little less time, because there aren't as many images.</p>
<p>Training in general with machine learning is the time-consuming portion; that's what's going to tie up your computer for long periods. Running images through a classifier is quick, as we saw before, but training can take minutes, hours, days, or possibly even longer. In this case, we're looking at up to half an hour, depending on how many images are present.</p>
<p>After a couple of minutes, our <kbd>retrained</kbd> function has run successfully, with the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/8d731320-2415-4076-af7c-1d63c38173bf.png" style="width:44.42em;height:10.08em;" width="854" height="194"/></p>
<p>I've turned down some of the verbosity in the <kbd>retrain</kbd> function, as otherwise it spits out a lot of messages that don't mean much. You can go into the code if you want and turn that up, if you're concerned it's not running successfully, but it should run just fine as long as everything's set up correctly.</p>
<p>Let's confirm that it works:</p>
<pre>#Confirm that it worked<br/>!ls *.pb<br/><br/>#should see file "output_graph.pb"</pre>
<p>We're going to look for that <kbd>.pb</kbd> file (Python binary file), which will be the output of what we did. So, that's the model, the input-output model, or graph as it's typically called in TensorFlow.</p>
<p>After running the code, we should get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0a5d97b2-603e-48c0-8898-c115bd18620b.png" style="width:18.17em;height:9.58em;" width="369" height="197"/></p>
<p>We have this file called <kbd>output_graph.pb</kbd>. That's the one we just created<span>; </span>you should see this file in your directory.</p>
<p>The code for running your images isn't quite as complicated. Loading our <kbd>output_graph.pb</kbd> graph file is similar to what we did before when we loaded the Inception model, as follows:</p>
<pre>#Let's load some code that will run our model on a specified image<br/><br/>def load_graph(model_file):<br/>    graph = tf.Graph()<br/>    graph_def = tf.GraphDef()<br/><br/>    with open(model_file, "rb") as f:<br/>        graph_def.ParseFromString(f.read())<br/>    with graph.as_default():<br/>        tf.import_graph_def(graph_def)<br/><br/>    return graph</pre>
<p>The <kbd>read_tensor_from_image_file</kbd> function helps in reading data from the image file, as follows:</p>
<pre>def read_tensor_from_image_file(file_name,<br/>                                input_height=299,<br/>                                input_width=299,<br/>                                input_mean=0,<br/>                                input_std=255):<br/>    input_name = "file_reader"<br/>    output_name = "normalized"<br/>    file_reader = tf.read_file(file_name, input_name)<br/>    if file_name.endswith(".png"):<br/>        image_reader = tf.image.decode_png(<br/>                file_reader, channels=3, name="png_reader")<br/>    elif file_name.endswith(".gif"):<br/>        image_reader = tf.squeeze(<br/>                tf.image.decode_gif(file_reader, name="gif_reader"))<br/>    elif file_name.endswith(".bmp"):<br/>        image_reader = tf.image.decode_bmp(file_reader, name="bmp_reader")<br/>    else:<br/>        image_reader = tf.image.decode_jpeg(<br/>                file_reader, channels=3, name="jpeg_reader")<br/>    float_caster = tf.cast(image_reader, tf.float32)<br/>    dims_expander = tf.expand_dims(float_caster, 0)<br/>    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])<br/>    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])<br/>    sess = tf.Session()<br/>    result = sess.run(normalized)<br/><br/>    return result</pre>
<p>There are some defaults here, but they don't matter. Images don't necessarily need to be <kbd>299</kbd> by <kbd>299</kbd>. We're just dealing with JPEGs here, but if we have files in PNG, GIF, or BMP formats, the model can handle that. We just decode the images, put them into our variable, and store and return them.</p>
<p>As said before, the labels come from the directories. The following code will load the created <kbd>output_labels.txt</kbd> i<span>t's going to load it from <kbd>output_labels.txt</kbd>, and that's going to be our dictionary of sorts, as defined by our subdirectory names</span>:</p>
<pre>def load_labels(label_file):<br/>    label = []<br/>    proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()<br/>    for l in proto_as_ascii_lines:<br/>        label.append(l.rstrip())<br/>    return label</pre>
<p>The following code shows the <kbd>label_image</kbd> function. To find an image you know, give the correct filename, but there is a default just in case:</p>
<pre>def label_image(file_name=None):<br/>    if not file_name:<br/>        file_name = "test/mylittlepony2.jpg"<br/>    model_file = "./output_graph.pb"<br/>    label_file = "./output_labels.txt"<br/>    input_height = 299<br/>    input_width = 299<br/>    input_mean = 0<br/>    input_std = 255<br/>    input_layer = "Placeholder"<br/>    output_layer = "final_result"</pre>
<p>I have hardcoded these in for simplicity. If you want to change stuff, you can, but I think that having it written there makes things easy to read and understand.</p>
<p>We load our graph file, read our data from the image file, and read layer names from the new model that we created, as follows:</p>
<pre>    graph = load_graph(model_file)<br/>    t = read_tensor_from_image_file(<br/>            file_name,<br/>            input_height=input_height,<br/>            input_width=input_width,<br/>            input_mean=input_mean,<br/>            input_std=input_std)<br/><br/>    input_name = "import/" + input_layer<br/>    output_name = "import/" + output_layer<br/>    input_operation = graph.get_operation_by_name(input_name)<br/>    output_operation = graph.get_operation_by_name(output_name)</pre>
<p>We're just going to read the input and output layers.</p>
<p>We define our session and get our results from <kbd>output_operation</kbd>. Again, we sort it to the <kbd>top_k</kbd> variable, and print the results:</p>
<pre>    with tf.Session(graph=graph) as sess:<br/>        results = sess.run(output_operation.outputs[0], {<br/>                input_operation.outputs[0]: t<br/>        })<br/>    results = np.squeeze(results)<br/><br/>    top_k = results.argsort()[-5:][::-1]<br/>    labels = load_labels(label_file)<br/>    for i in top_k:<br/>        print(labels[i], results[i])</pre>
<p>There are a lot of classes, but we're actually going to see it's always just going to be one result here.</p>
<p><span>Let's try our code again. As discussed, we separated a couple of images out into a separate directory, because we don't want to test on our training images, as that proves nothing.</span></p>
<p><span>Let's test the retrained model on our first <kbd>transformers1.jpg</kbd> image. The model is to display the image and tell us what the classification results were:</span></p>
<pre>#label_image will load our test image and tell us what class/type it is<br/><br/>#uncomment one of these lines to test<br/>#<br/>test_image='test/transformers1.jpg'<br/># test_image='test/transformers2.jpg'<br/># test_image='test/transformers3.jpg'<br/><br/># test_image='test/mylittlepony1.jpg'<br/># test_image='test/mylittlepony2.jpg'<br/># test_image='test/mylittlepony3.jpg'<br/><br/># test_image='test/gijoe1.jpg'<br/># test_image='test/gijoe2.jpg'<br/># test_image='test/gijoe3.jpg'<br/><br/># test_image='test/barbie1.jpg'<br/># test_image='test/barbie2.jpg'<br/># test_image='test/barbie3.jpg'<br/><br/>#display the image<br/>figure()<br/>imshow(imread(test_image))<br/><br/>#and tell us what the classification result is<br/>label_image(test_image)</pre>
<p>The output for the preceding code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/dcae58c2-27dc-481e-91d7-0ae2a5777014.png" style="width:22.58em;height:27.58em;" width="365" height="447"/></p>
<p><span>The model classified the image with a very high probability that this was a <kbd>transformers</kbd>. Since our images are distinct enough, and there are fewer classes, it's going to work very nicely. We see there is a 99.9% probability the picture is of a Transformer, a small probability that it is a GI Joe, and it's most definitely not a Barbie or a My Little Pony.</span></p>
<p><span>We can use <em>Ctrl + /</em> to comment and uncomment lines in the code in Jupyter Notebook, and press <em>Ctrl + Enter</em> to run the code again with the <kbd>transformer2.jpg</kbd> picture:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/3adb0534-4ca0-41d6-8677-fa47e1374330.png" style="width:19.08em;height:19.50em;" width="367" height="375"/></p>
<p><span>The output is <kbd>transformers</kbd> again. This time the model thinks it is slightly more likely to be a Barbie than a GI Joe, but the probability is insignificant.</span></p>
<p><span>Let's try again with the <kbd>mylittlepony1.jpg</kbd> picture:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/db308670-2a26-4ff4-8e9f-cd2a92732552.png" style="width:20.00em;height:23.42em;" width="388" height="455"/></p>
<p><span>And yes, it definitely looks like other images in the <kbd>my little pony</kbd> subfolder.</span></p>
<p><span>Let's take another picture, <kbd>mylittlepony3.jpg</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/1908b3b8-7218-43e2-bd3d-a0693597ea7f.png" style="width:20.33em;height:24.67em;" width="371" height="453"/></p>
<p><span>Again, no problem classifying the image. Let's take a look at <kbd>gijoe2.jpg</kbd> too:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d080b044-e64e-46fc-a8f9-9a0c5102b643.png" style="width:17.58em;height:20.83em;" width="312" height="368"/></p>
<p><span>There's a high probability of it being a <kbd>gi joe</kbd>, <kbd>transformers</kbd> and <kbd>barbie</kbd> are more likely than <kbd>my little pony</kbd>, but again all those probabilities are insignificant—it's definitely a <kbd>gi joe</kbd>.</span></p>
<p><span>Finally, let's try it on <kbd>barbie1.jpg</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/2074a3ed-55c3-4e38-beb4-419eb189c6eb.png" style="width:23.00em;height:27.50em;" width="381" height="455"/></p>
<p><span>Again, definitely classified as a <kbd>barbie</kbd>, and <kbd>my little pony</kbd> was the second most likely, perhaps because of the colors; there tends to be more pink and purple on Barbie and My Little Pony toys.</span></p>
<p><span>Now we know how we can use our own images to retrain a pre-existing model. With not a lot of coding or CPU time, we can create a custom image classifier for our own purposes.</span></p>
<p><span>In the next section, we're going to talk about speeding up the computations with the help of your GPU.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Speeding up computation with your GPU</h1>
                </header>
            
            <article>
                
<p>In this section, we'll talk briefly about speeding up computations with your GPU. The good news is that TensorFlow is actually very smart about using the GPU, so if you have everything set up, then it's pretty simple.</p>
<p>Let's see what things look like if we have the GPU properly set up. First, import TensorFlow as follows:</p>
<pre>import tensorflow</pre>
<p>Next, we print <kbd>tensorflow.Session()</kbd>. This just gives us information about our CPU and GPU (if it is properly set up):</p>
<pre class="CDPAlignLeft CDPAlign">print(<span>tensorflow.Session()</span>)</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d1703197-abb0-4678-a930-ad0f6aee9785.png" style="width:54.08em;height:6.75em;" width="1276" height="159"/></p>
<p>As we can see from the output, we're using a laptop with a GeForce GTX 970M, which is CUDA-compatible. This is needed in order to run TensorFlow with the GPU. If everything is set up properly, you will see a message very similar to the preceding output for your GPU, your card model, and details about it such as its memory and so forth.</p>
<p>TensorFlow is smart about it. We can override it ourselves, but that's only a good idea if we know what we're doing and we're willing to put in the extra work. Unless we know what we're doing, we're not going to get improved performance, so just leave the default settings.</p>
<p>Subsequent sections will run just fine on a CPU<span>, </span>just not quite as fast.</p>
<p>The bad news about TensorFlow using the GPU is that setting it up isn't quite as straightforward. We previously covered the <kbd>pip</kbd> command, for example, <kbd>pip install tensorflow</kbd> and <kbd>pip install tensorflow-gpu</kbd>, which is a starting point, but we'll still need CUDA to be installed.</p>
<p>I have version 9.0 installed. If you have a Quadro GPU or some sort of workstation, Tesla, or one of those specialized cards, you should use CUDA version 9.1. It's platform-dependent, depending on what kind of GPU you have and, more particularly, what kind of operating system, so we can't go into full details here.</p>
<p>What's important to know is that we can't just install <kbd>tensorflow-gpu</kbd>; we have to install CUDA. Download and install CUDA for your operating system from the NVIDIA website (<a href="https://developer.nvidia.com/cuda-toolkit" target="_blank">https://developer.nvidia.com/cuda-toolkit</a>).</p>
<p>In addition to that, TensorFlow requires the <strong>NVIDIA CUDA® Deep Neural Network</strong> (<strong>cuDNN</strong>) library, which is a big DLL file for Windows, or a shared object (<kbd>.SO</kbd>) file for Linux. It's similar for macOS as well. It's just one file, which needs to be in your path. I generally copy it over to my <kbd>CUDA</kbd> directory.</p>
<p>If you do have one, try to install CUDA, do try to install cuDNN, and try to get TensorFlow working. Hopefully, that will speed up computations for you.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to classify images using <span>a pre-trained model based on TensorFlow. We then retrained our model to work with custom images.</span></p>
<p><span>Finally, we had a brief overview of how to speed up the classification process by carrying out the computation on a GPU.</span></p>
<p><span>Using the examples covered in this book, you will be able to carry your our custom projects using Python, OpenCV, and TensorFlow.</span></p>


            </article>

            
        </section>
    </div>



  </body></html>