- en: Setting Up a Docker Container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker is a convenient platform that can package an application and its dependencies
    in a replicable virtual environment that can run on different operating systems.
    In particular, it is well integrated with any **Linux system**.
  prefs: []
  type: TYPE_NORMAL
- en: The replicable virtual environment is described in a **Dockerfile**that contains
    instructions that should be executed in order to achieve the desired virtual environment.
    These instructions mainly include the installation procedure, which is pretty
    much similar to the installation procedure with a Linux shell. Once the environment
    has been created, you can be sure that your app will have the same behavior on
    any other machine.
  prefs: []
  type: TYPE_NORMAL
- en: In Docker terminology, the resulting virtual environment is called a **Docker
    image**. You can create an instance of the virtual environment, which is called
    a **Docker container**. After the container is created, you can execute your code
    inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: Please follow the installation instructions on the official website in order
    to get Docker up and running on the operating system of your choice: [https://docs.docker.com/install/](https://docs.docker.com/install/)
  prefs: []
  type: TYPE_NORMAL
- en: For your convenience, we are including Dockerfiles, which will make it very
    easy to replicate the environment that we have used to run the code in this book,
    regardless of what operating system you have on your computer. First, we describe
    a Dockerfile that uses only the CPU without GPU acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Dockerfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instructions in the Dockerfile start from a base image, and then desired installations
    and modifications are done on top of that image.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, TensorFlow does not support **Python 3.8**. If you plan
    to run [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning to Recognize
    Traffic Signs*, or [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml), *Learning
    to Classify and Localize Objects*, where TensorFlow is used, you can start with **Python
    3.7** and then install TensorFlow with `pip`, or you can pick `tensorflow/tensorflow:latest-py3` as
    the base image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over the steps to create our environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start from a base image, which is the basic Python image that is based on
    **Debian**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We install useful packages that will particularly be used during the installation
    process of OpenCV and other dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We download **OpenCV 4.2** together with the contributor packages, which are
    required for non-free algorithms such as **scale-invariant feature transform** (**SIFT**)
    and **speeded-up robust features** (**SURF**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We install a version of NumPy that works with **OpenCV 4.2**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We compile OpenCV using appropriate flags:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We link the OpenCV Python binary to the appropriate location so that the interpreter
    can find it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This linking might be redundant or result in an error if you have used a base
    image that differs from `python:3.8`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We install other Python packages that are used in the book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now that we have composed the Dockerfile, we can build the corresponding
    Docker image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We have named the image `cv` and we have passed a Dockerfile located at `dockerfiles/Dockerfile` to
    build the image. Of course, you can place your Dockerfile in any other location.
    The last argument is required in Docker and it specifies a context that might
    be used; for example, if the Dockerfile contains directives to copy files from
    a relative path. In our case, we do not have such directives, and it can be, in
    general, any valid path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the image is built, we can start the `docker` container as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have passed the `DISPLAY` environment variable, mounted `/tmp/.X11-unix`,
    and specified the `/dev/video0` device in order to allow the container to use
    the desktop environment and connect to the camera, where the container is used
    in most of the chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: If the Docker container fails to connect to the *X* server of your system, you
    might need to run **`$ xhost +local:docker` **on your system in order to allow
    the connection.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we are up and running with the composed Docker image, let's examine
    how to support GPU acceleration with Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Working with a GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The environment that we create with Docker has limited access to the devices
    of your machine. In particular, you have seen that we have specified the camera
    device when running a Docker container and have mounted `/tmp/.X11-unix` in order
    to allow the Docker container to connect to the running desktop environment.
  prefs: []
  type: TYPE_NORMAL
- en: When we have custom devices such as GPUs, the integration process becomes more
    complicated, because the Docker container needs appropriate ways to talk to the
    device. Fortunately, for **NVIDIA GPUs**, this problem is solved with the help
    of **NVIDIA Container Toolkit **([https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker)).
  prefs: []
  type: TYPE_NORMAL
- en: Following installation of the toolkit, you can build and run GPU-accelerated
    Docker containers. Nvidia provides a base image so that you can build your image
    on top of it without bothering about appropriate access to the GPU. The requirement
    is that you should have an appropriate Nvidia driver installed on your system
    with an Nvidia GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we mainly use GPUs for accelerating TensorFlow. TensorFlow itself
    provides an image that can be used to run TensorFlow with GPU acceleration. Therefore,
    to have a GPU-accelerated container, we can simply pick the Docker image of TensorFlow
    and make all other installations on top of it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This declaration will pick TensorFlow version `2.1.0` with GPU acceleration
    and Python 3 support. Note that this version of the TensorFlow image uses **Python
    3.6**. Nevertheless, you can use the remaining part of the Dockerfile for the
    CPU that we described in [Appendix A](a4f1f102-9f62-4644-bcde-f478cd28621a.xhtml), *Profiling
    and Accelerating Your Apps*, and you will be up and running with a container that
    can run the code in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done with creating the image, the only modification that you have
    to do when starting a container is to pass one additional argument: `--runtime=nvidia`.
  prefs: []
  type: TYPE_NORMAL
