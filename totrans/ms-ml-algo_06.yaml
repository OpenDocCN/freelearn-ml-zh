- en: Hebbian Learning and Self-Organizing Maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to introduce the concept of Hebbian learning, based
    on the methods defined by the psychologist Donald Hebb. These theories immediately
    showed how a very simple biological law is able to describe the behavior of multiple
    neurons in achieving complex goals and was a pioneering strategy that linked the
    research activities in the fields of artificial intelligence and computational
    neurosciences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we are going to discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Hebb rule for a single neuron, which is a simple but biologically plausible
    behavioral law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some variants that have been introduced to overcome a few stability problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final result achieved by a Hebbian neuron, which consists of computing the
    first principal component of the input dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two neural network models (Sanger's network and Rubner-Tavan's network) that
    can extract a generic number of principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of **Self-Organizing Maps** (**SOMs**) with a focus on the Kohonen
    Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hebb's rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hebb''s rule** has been proposed as a conjecture in 1949 by the Canadian
    psychologist Donald Hebb to describe the synaptic plasticity of natural neurons.
    A few years after its publication, this rule was confirmed by neurophysiological
    studies, and many research studies have shown its validity in many application,
    of Artificial Intelligence. Before introducing the rule, it''s useful to describe
    the generic Hebbian neuron, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4985e161-7f26-46bd-b3bf-7fc4bb204dab.png)'
  prefs: []
  type: TYPE_IMG
- en: Generic Hebbian neuron with a vectorial input
  prefs: []
  type: TYPE_NORMAL
- en: 'The neuron is a simple computational unit that receives an input vector *x*,
    from the pre-synaptic units (other neurons or perceptive systems) and outputs
    a single scalar value, *y*. The internal structure of the neuron is represented
    by a weight vector, *w,* that models the strength of each synapse. For a single
    multi-dimensional input, the output is obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40a04e23-dfd5-421c-a65d-fefd03aa791a.png)'
  prefs: []
  type: TYPE_IMG
- en: In this model, we are assuming that each input signal is encoded in the corresponding
    component of the vector, *x*; therefore, *x[i]* is processed by the synaptic weight *w[i,]*
    and so on. In the original version of Hebb's theory, the input vectors represent
    neural firing rates, which are always non-negative. This means that the synaptic
    weights can only be strengthened (the neuroscientific term for this phenomenon
    is **long-term potentiation** (**LTP**)). However, for our purposes, we assume
    that *x* is a real-valued vector, as is *w*. This condition allows modeling more
    artificial scenarios without a loss of generality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same operation performed on a single vector holds when it''s necessary
    to process many input samples organized in a matrix. If we have *N* m-dimensional
    input vectors, the formula becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17fb5f3f-1296-4d5a-9250-2054a211817c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic form of Hebb''s rule in a discrete form can be expressed (for a single
    input) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b07422b-adc1-44d4-b098-d10f7539e46e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weight correction is hence a vector that has the same orientation of *x*
    and magnitude equal to *|x|* multiplied by a positive parameter, *η*, which is
    called the learning rate and the corresponding output, *y* (which can have either
    a positive or a negative sign). The sense of *Δw* is determined by the sign of
    *y*; therefore, under the assumption that *x* and *y* are real values, two different
    scenarios arise from this rule:'
  prefs: []
  type: TYPE_NORMAL
- en: If *x[i]* > 0 (< 0) and *y* > 0 (< 0), *w[i]* is strengthened
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *x[i]* > 0 (< 0) and *y* < 0 (> 0), *w[i]* is weakened
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s easy to understand this behavior considering two-dimensional vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/172d792a-ded9-46e4-8f33-ef7367a00529.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, if the initial angle *α* between *w* and *x* is less than 90°, *w*
    will have the same orientation of *x* and viceversa if *α* is greater than 90°.
    In the following diagram, there''s a schematic representation of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba5db290-9b0f-434f-87fd-9837830b0738.png)'
  prefs: []
  type: TYPE_IMG
- en: Vectorial analysis of Hebb's rule
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s possible to simulate this behavior using a very simple Python snippet.
    Let''s start with a scenario where *α* is less than 90° and 50 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the final angle, *α,* is close to zero and *w* has the same orientation
    and sense of *x*. We can now repeat the experiment with *α* greater than 90° (we
    change only the value of *w* because the procedure is the same):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the final angle, *α,* is about 180° and, of course, *w* has the
    opposite sense with respect to *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scientist S. Löwel expressed this concept with the famous sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '"*Neurons that fire together wire together*"'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can re-express this concept (adapting it to a machine learning scenario)
    by saying that the main assumption of this approach is based on the idea that
    when pre- and post-synaptic units are coherent (their signals have the same sign),
    the connection between neurons becomes stronger and stronger. On the other side,
    if they are discordant, the corresponding synaptic weight is decreased. For the
    sake of precision, if *x* is a spiking rate, it should be represented as a real
    function *x(t)* as well as *y(t)*. According to the original Hebbian theory, the
    discrete equation must be replaced by a differential equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7eb6471-fb14-428f-bfa3-bf53dff22bc1.png)'
  prefs: []
  type: TYPE_IMG
- en: If *x(t)* and *y(t)* have the same fire rate, the synaptic weight is strengthened
    proportionally to the product of both rates. If instead there's a relatively long
    delay between the pre-synaptic activity *x(t)* and the post-synaptic one *y(t)*,
    the corresponding weight is weakened. This is a more biologically plausible explanation
    of the relation *fire together → wire together*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, even if the theory has a strong neurophysiological basis, some modifications
    are necessary. In fact, it''s easy to understand that the resulting system is
    always unstable. If two inputs are repeatedly applied (both real values and firing
    rates), the norm of the vector, w, grows indefinitely and this isn''t a plausible
    assumption for a biological system. In fact, if we consider a discrete iteration
    step, we have the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/118b6d4e-b823-409d-878f-23b62659bfd5.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous output, *y[k,]* is always multiplied by a factor greater than *1*
    (except in the case of null input), therefore it grows without a bound. As *y
    = w · x*, this condition implies that the magnitude of *w* increases (or remains
    constant if the magnitude of *x* is null) at each iteration (a more rigorous proof
    can be easily obtained considering the original differential equation).
  prefs: []
  type: TYPE_NORMAL
- en: Such a situation is not only biologically unacceptable, but it's also necessary
    to properly manage it in machine learning problems in order to avoid a numerical
    overflow after a few iterations. In the next paragraph, we're going to discuss
    some common methods to overcome this issue. For now, we can continue our analysis
    without introducing a correction factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now consider a dataset, *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaf67da3-266b-44a7-8ecd-796b0f33f4b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can apply the rule iteratively to all elements, but it''s easier (and more
    useful) to average the weight modifications over the input samples (the index
    now refers to the whole specific vector, not to the single components):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/750fa24e-dfdb-40fd-8a83-5db4126319a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous formula, *C* is the input correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc8af2b6-bb65-4941-83b5-a2d0f51b8b90.png)'
  prefs: []
  type: TYPE_IMG
- en: For our purposes, however, it's useful to consider a slightly different Hebbian
    rule based on a threshold *θ* for the input vector (there's also a biological
    reason that justifies this choice, but it's beyond the scope of this book; the
    reader who is interested can find it in *Theoretical Neuroscience*,*Dayan P.*,
    *Abbott F. L.*, *The MIT Press*).
  prefs: []
  type: TYPE_NORMAL
- en: It's easy to understand that in the original theory where *x(t)* and *y(t)*
    are firing rates, this modification allows a phenomenon opposite to LTP and called
    **long-term depression** (**LTD**). In fact, when *x(t) < **θ* and *y(t)* is positive,
    the product *(x(t) - θ)y(t)* is negative and the synaptic weight is weakened.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we set *θ = 〈x〉 ≈ E[X]*, we can derive an expression very similar to the
    previous one, but based on the input covariance matrix (unbiased through the Bessel''s
    correction):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55550e5b-8d33-4cb3-9a9b-d8ccb94e8a17.png)'
  prefs: []
  type: TYPE_IMG
- en: For obvious reasons, this variant of the original Hebb's rule is called the **covariance
    rule**.
  prefs: []
  type: TYPE_NORMAL
- en: It's also possible to use the **Maximum Likelihood Estimation** (**MLE**) (or
    biased) covariance matrix (dividing by *N*), but it's important to check which
    version is adopted by the mathematical package that is employed. When using NumPy,
    it's possible to decide the version using the `np.cov()` function and setting
    the `bias=True/False` parameter (the default value is `False`). However, when
    *N >> 1*, the difference between versions decreases and can often be discarded.
    In this book, we'll use the unbiased version. The reader who wants to see further
    details about the Bessel's correction can read *Applied Statistics*,*Warner R.*,
    *SAGE Publications*.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the covariance rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The covariance matrix *Σ* is real and symmetric. If we apply the eigendecomposition,
    we get (for our purposes it''s more useful to keep *V^(-1)* instead of the simplified
    version *V^T*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c15dfbc-e259-44ff-9b0e-76284886dfbc.png)'
  prefs: []
  type: TYPE_IMG
- en: '*V* is an orthogonal matrix (thanks to the fact that *Σ* is symmetric) containing
    the eigenvectors of *Σ* (as columns), while *Ω* is a diagonal matrix containing
    the eigenvalues. Let''s suppose we sort both eigenvalues (*λ[1]*, *λ[2]*, ...,
    *λ[m]*) and the corresponding eigenvectors (*v[1]*, *v[2]*, ..., *v[m]*) so that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87a4b53a-2784-44dd-9477-31a10ae287ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, let''s suppose that *λ*[*1* ]is dominant over all the other eigenvalues
    (it''s enough that *λ[1] > λ[i]* with *i ≠ 1*). As the eigenvectors are orthogonal,
    they constitute a basis and it''s possible to express the vector w, with a linear
    combination of the eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e779900-d7e2-45b3-9f33-78ee8de613b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The vector *u* contains the coordinates in the new basis. Let''s now consider
    the modification to the covariance rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f0dcb94-6c16-46d8-8a28-6e07331f23d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we apply the rule iteratively, we get a matrix polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8998d91-ae5b-442e-b5ce-f460f2343956.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Exploiting the Binomial theorem and considering that *Σ**⁰=I*, we can get a
    general expression for *w^((k))* as a function of *w^((0))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/153067b7-ec73-4ae7-8012-40e7920f5964.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now rewrite the previous formula using the change of basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b09938a-c336-43f8-8f18-c8fd5440a388.png)'
  prefs: []
  type: TYPE_IMG
- en: The vector *u^((0))* contains the coordinates of *w^((0))* in the new basis;
    hence, *w^((k))* is expressed as a polynomial where the generic term is proportional
    to *VΩ^iu^((0))*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now consider the diagonal matrix *Ω^k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecf21a99-c3ee-404f-b495-1a2ac596daef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last step derives from the hypothesis that *λ*[*1* ]is greater than any
    other eigenvalue and when *k → ∞*, all *λ[i≠1]^k<< λ[1]^k*. Of course, if *λ[i][≠1]
    > 1, λ[i≠1]*^(*k* )will grow as well as *λ[1]^(k )*however, the contribution of
    the *secondary* eigenvalues to *w^((k))* becomes significantly weaker when *k → ∞*.
    Just to understand the validity of this approximation, let''s consider the following
    situation where *λ[1]* is slightly larger that *λ[2]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/937e3fe5-ece3-4b89-8507-2c80a0e1aeba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The result shows a very important property: not only is the approximation correct,
    but as we''re going to show, if an eigenvalue *λ[i]* is larger than all the other
    ones, the covariance rule will always converge to the corresponding eigenvector
    *v[i]*. No other stable fixed points exist!'
  prefs: []
  type: TYPE_NORMAL
- en: This hypothesis is no more valid if *λ[1] = λ[2] = ... = λ[n]*. In this case,
    the total variance is explained equally by the direction of each eigenvector (a
    condition that implies a symmetry which isn't very common in real-life scenarios).
    This situation can also happen when working with finite-precision arithmetic,
    but in general, if the difference between the largest eigenvalue and the second
    one is less than the maximum achievable precision (for example, 32-bit floating
    point), it's plausible to accept the equality.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we assume that the dataset is not whitened, because our goal (also
    in the next paragraphs) is to reduce the original dimensionality considering only
    a subset of components with the highest total variability (the decorrelation,
    like in **Principal Component Analysis** (**PCA**), must be an outcome of the
    algorithm, not a precondition). On the other side, zero-centering the dataset
    could be useful, even if not really necessary for this kind of algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we rewrite the expression for *w[k]* considering this approximation, we
    obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/573a41b5-7c5b-4984-9ed6-cafbe8c9e831.png)'
  prefs: []
  type: TYPE_IMG
- en: As *a[1]v + a[2]v + ... + a[k]v ∝ v*, this result shows that, when *k → ∞*, *w[k]*
    will become proportional to the first eigenvector of the covariance matrix *Σ*
    (if *u[1]^((0))* is not null) and its magnitude, without normalization, will grow
    indefinitely. The spurious effect due to the other eigenvalues becomes negligible
    (above all, if *w* is divided by its norm, so that the length is always *||w||
    = 1*) after a limited number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before drawing our conclusions, an important condition must be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/885c829c-9e7c-416e-9cf5-e0ec02e1586c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In fact, if *w(0)* were orthogonal to *v1*, we would get (the eigenvectors
    are orthogonal to each other):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/231720a5-186b-414f-8052-d8ca438252f3.png)'
  prefs: []
  type: TYPE_IMG
- en: This important result shows how a Hebbian neuron working with the covariance
    rule is able to perform a PCA limited to the first component without the need
    for eigendecomposing *Σ*. In fact, the vector *w* (we're not considering the problem
    of the increasing magnitude, which can be easily managed) will rapidly converge
    to the orientation where the input dataset *X* has the highest variance. In [Chapter
    5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and Applications,*
    we discussed the details of PCA; in the next paragraph, we're going to discuss
    a couple of methods to find the first N principal components using a variant of
    the Hebb's rule.
  prefs: []
  type: TYPE_NORMAL
- en: Example of covariance rule application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before moving on, let''s simulate this behavior with a simple Python example.
    We first generate `1000` values sampled from a bivariate Gaussian distribution
    (the variance is voluntarily asymmetric) and then we apply the covariance rule
    to find the first principal component (*w*^(*(0)* )has been chosen so not to be
    orthogonal to *v[1]*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm is straightforward, but there are a couple of elements that we
    need to comment on. The first one is the normalization of vector *w* at the end
    of each iteration. This is one of the techniques needed to avoid the uncontrolled
    growth of *w*. The second *tricky* element is the final multiplication, *w • 50*.
    As we are multiplying by a positive scalar, the direction of *w* is not impacted,
    but it's easier to show the vector in the complete plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e48fbb26-9d2b-42eb-920b-060cd073f118.png)'
  prefs: []
  type: TYPE_IMG
- en: Application of the covariance rule. w[∞] becomes proportional to the first principal
    component
  prefs: []
  type: TYPE_NORMAL
- en: After a limited number of iterations, *w[∞]* has the same orientation of the
    principal eigenvector which is, in this case, parallel to the *x* axes. The sense
    depends on the initial value *w[0]*; however, in a PCA, this isn't an important
    element.
  prefs: []
  type: TYPE_NORMAL
- en: Weight vector stabilization and Oja's rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to stabilize the weight vector is normalizing it after each
    update. In this way, its length will be always kept equal to one. In fact, in
    this kind of neural networks we are not interested in the magnitude, but only
    in the direction (that remains unchanged after the normalization). However, there
    are two main reasons that discourage this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: It's non-local. To normalize vector *w*, we need to know all its values and
    this isn't biologically plausible. A real synaptic weight model should be self-limiting,
    without the need to have access to external pieces of information that cannot
    be available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The normalization must be performed after having applied the correction and
    hence needs a double iterative step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In many machine learning contexts, these conditions are not limiting and they
    can be freely adopted, but when it''s necessary to work with neuroscientific models,
    it''s better to look for other solutions. In a discrete form, we need to determine
    a correction term for the standard Hebb''s rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffdb9cc5-e9d4-4945-b336-68dd948cea74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *f* function can work both as a local and non-local normalizer. An example
    of the first type is **Oja''s rule**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3bda4b6-7a4b-4f1a-9062-c7f2e2ab741c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *α* parameter is a positive number that controls the strength of the normalization.
    A non-rigorous proof of the stability of this rule can be obtained considering
    the condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37414372-7223-4fdb-b431-147780383d76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second expression implies that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011c8c-f57b-4ec0-bba1-b835ba71c37a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, when *t → ∞*, the magnitude of the weight correction becomes close
    to zero and the length of the weight vector *w* will approach a finite limit value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9115aeb-7623-4d4e-aa59-f3fb96f7756a.png)'
  prefs: []
  type: TYPE_IMG
- en: Sanger's network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **Sanger''s network** is a neural network model for online *Principal Component*
    extraction proposed by T. D. Sanger in *Optimal Unsupervised Learning in a Single-Layer
    Linear Feedforward Neural Network*, *Sanger T. D.*, *Neural Networks*, *1989/2*.
    The author started with the standard version of Hebb''s rule and modified it to
    be able to extract a variable number of principal components (*v[1], v[2], ...,
    v[m]*) in descending order (*λ[1] > λ[2] > ... > λ[m]*). The resulting approach,
    which is a natural extension of Oja''s rule, has been called the **Generalized
    Hebbian Rule** (**GHA**) (or Learning). The structure of the network is represented
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/922bd024-d3e8-440d-92f9-b7211b43f136.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The network is fed with samples extracted from an n-dimensional dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5438be6b-e7b4-4d77-a244-27166015c419.png)'
  prefs: []
  type: TYPE_IMG
- en: The *m* output neurons are connected to the input through a weight matrix, *W
    = {w[ij]},* where the first index refers to the input components (pre-synaptic
    units) and the second one to the neuron. The output of the network can be easily
    computed with a scalar product; however, in this case, we are not interested in
    it, because just like for the covariance (and Oja's) rules, the principal components
    are extracted through the weight updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem that arose after the formulation of Oja''s rule was about the extraction
    of multiple components. In fact, if we applied the original rule to the previous
    network, all weight vectors (the rows of *w*) would converge to the first principal
    component. The main idea (based on the **Gram-Schmidt** orthonormalization method)
    to overcome this limitation is based on the observation that once we have extracted
    the first component *w[1]*, the second one *w[2]* can be forced to be orthogonal
    to *w[1]*, the third one *w[3]* can be forced to be orthogonal to *w[1]* and *w[2]*,
    and so on. Consider the following representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17d5ae7b-0982-409f-800d-a40557680cc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Orthogonalization of two weight vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we are assuming that *w[1]* is stable and *w[2][0]* is another
    weight vector that is converging to *w[1]*. The projection of w[20] onto *w[1]*
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8f20651-fe3c-4e2a-aca2-69f43e36661e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous formula, we can omit the norm if we don''t need to normalize
    (in the network, this process is done after a complete weight update). The orthogonal component
    of *w[20]* is simply obtained with a difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11870193-f3f7-464f-acbb-66481e7b485f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying this method to the original Oja''s rule, we obtain a new expression
    for the weight update (called Sanger''s rule):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9062ac15-b758-4542-a3d7-d8c33994de1b.png)'
  prefs: []
  type: TYPE_IMG
- en: The rule is referred to a single input vector *x*, hence *x[j]* is the *j^(th)*
    component of *x*. The first term is the classic Hebb's rule, which forces weight
    *w* to become parallel to the first principal component, while the second one
    acts in a way similar to the Gram-Schmidt orthogonalization, by subtracting a
    term proportional to the projection of *w* onto all the weights connected to the
    previous post-synaptic units and considering, at the same time, the normalization
    constraint provided by Oja's rule (which is proportional to the square of the
    output).
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, expanding the last term, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a681a15a-0bea-46f0-86b6-d82361d03a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: The term subtracted to each component *w[ij]* is proportional to all the components
    where the index *j* is fixed and the first index is equal to *1, 2, ..., i*. This
    procedure doesn't produce an immediate orthogonalization but requires several
    iterations to converge. The proof is non-trivial, involving convex optimization
    and dynamic systems methods, but, it can be found in the aforementioned paper.
    Sanger showed that the algorithm converges always to the sorted first *n* principal
    components (from the largest eigenvalue to the smallest one) if the `learning_rate` *η(t)*
    decreases monotonically and converges to zero when *t → ∞*. Even if necessary
    for the formal proof, this condition can be relaxed (a stable *η < 1* is normally
    sufficient). In our implementation, matrix *W* is normalized after each iteration,
    so that, at the end of the process, *W^T* (the weights are in the rows) is orthonormal
    and constitutes a basis for the eigenvector subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'In matrix form, the rule becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b86aee4-3330-452a-a4e0-ced608cb83b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Tril(•) is a matrix function that transforms its argument into a lower-triangular
    matrix and the term *yy^T* is equal to *Wxx^TW*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm for a Sanger''s network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize *W^((0))* with random values. If the input dimensionality is *n*
    and *m* principal components must be extracted, the shape will be *(m × n)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a `learning_rate` *η* (for example, `0.01`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a `threshold` *Thr* (for example, `0.001`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter *T = 0.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *||W^((t)) - W^((t-1))||[F] > Thr*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *ΔW = 0* (same shape of *W*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *x* in *X*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *T = T + 1*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *y = W^((t))x*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute and accumulate *ΔW += η(yx^T - Tril(yy^T)W^((t))*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update *W^((t+1)) = W^((t)) + (η / T)ΔW*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *W^((t+1)) = W^((t+1))** / ||W^((t+1))||^((rows))* (the norm must be computed
    row-wise)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm can also be iterated a fixed number of times (like in our example),
    or the two stopping approaches can be used together.
  prefs: []
  type: TYPE_NORMAL
- en: Example of Sanger's network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this Python example, we consider a bidimensional zero-centered dataset
    `X` with 500 samples (we are using the function defined in the first chapter).
    After the initialization of `X`, we also compute the eigendecomposition, to be
    able to double-check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The eigenvalues are in reverse order; therefore, we expect to have a final
    *W* with the rows swapped. The initial condition (with the weights multiplied
    by 15) is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b8617f9-5a6f-47cc-a63e-4a13b0ae3871.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dataset with *W* initial condition, we can implement the algorithm. For simplicity,
    we preferred a fixed number of iterations (`5000`) with a `learning_rate` of *η=0.01*.
    The reader can modify the snippet to stop when the weight matrix becomes stable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to check is the final state of *W* (we transposed the matrix
    to be able to compare the columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, *W* has converged to the eigenvectors of the input correlation
    matrix (the sign `*–*` which is associated with the sense of *w—*is not important
    because we care only about the orientation). The second eigenvalue is the highest,
    so the columns are swapped. Replotting the diagram, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae5146f7-2565-4816-becc-d82eb2f1ee16.png)'
  prefs: []
  type: TYPE_IMG
- en: Final condition, w has converged to the two principal components
  prefs: []
  type: TYPE_NORMAL
- en: The two components are perfectly orthogonal (the final orientations can change
    according to the initial conditions or the random state) and *w[0]* points in
    the direction of the first principal component, while *w[1]* points in the direction
    of the second component. Considering this nice property, it's not necessary to
    check the magnitude of the eigenvalues; therefore, this algorithm can operate
    without eigendecomposing the input covariance matrix. Even if a formal proof is
    needed to explain this behavior, it's possible to understand it intuitively. Every
    single neuron converges to the first principal component given a full eigenvector
    subspace. This property is always maintained, but after the orthogonalization,
    the subspace is implicitly reduced by a dimension. The second neuron will always
    converge to the first component, which now corresponds to the global second component,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: One of the advantages of this algorithm (and also of the next one) is that a
    standard PCA is normally a bulk process (even if there are batch algorithms),
    while a Sanger's network is an online algorithm that is trained incrementally.
    In general, the time performance of a Sanger's network is worse than the direct
    approach because of the iterations (some optimizations can be achieved using more
    vectorization or GPU support). On the other side, a Sanger's network is memory-saving
    when the number of components is less than the input dimensionality (for example,
    the covariance matrix for *n=1000* has *10⁶* elements, if *m = 100*, the weight
    matrix has *10⁴* elements).
  prefs: []
  type: TYPE_NORMAL
- en: Rubner-Tavan's network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and
    Applications,* we said that any algorithm that decorrelates the input covariance
    matrix is performing a PCA without dimensionality reduction. Starting from this
    approach, Rubner, and Tavan (in the paper *A Self-Organizing Network for Principal-Components
    Analysis*,*Rubner J.*, *Tavan P.*, *Europhysics. Letters*, *10(7)*, *1989*) proposed
    a neural model whose goal is decorrelating the output components to force the
    consequent decorrelation of the output covariance matrix (in lower-dimensional
    subspace). Assuming a zero-centered dataset and *E[y] = 0*, the output covariance
    matrix for *m* principal components is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d1f7b9f-24d5-4b1a-8917-70fc3898e4c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, it''s possible to achieve an approximate decorrelation, forcing the
    terms *y[i]y[j]* with *i ≠ j* to become close to zero. The main difference with
    a standard approach (such as whitening or vanilla PCA) is that this procedure
    is local, while all the standard methods operate globally, directly with the covariance
    matrix. The neural model proposed by the authors is shown in the following diagram
    (the original model was proposed for binary units, but it works quite well also
    for linear ones):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3075a27-0124-44c9-9c04-7508c476e208.png)'
  prefs: []
  type: TYPE_IMG
- en: Rubner-Tavan network. The connections v[jk] are based on the anti-Hebbian rule
  prefs: []
  type: TYPE_NORMAL
- en: 'The network has *m* output units and the last *m-1* neurons have a summing
    node that receives the weighted output of the previous units (hierarchical lateral
    connections). The dynamic is simple: the first output isn''t modified. The second
    one is forced to become decorrelated with the first one. The third one is forced
    to become decorrelated with both the first and the second one and so on. This
    procedure must be iterated a number of times because the inputs are presented
    one by one and the cumulative term that appears in the correlation/covariance
    matrix (it''s always easier to zero-center the dataset and work with the correlation
    matrix) must be implicitly split into its addends. It''s not difficult to understand
    that the convergence to the only stable fixed point (which has been proven to
    exist by the authors) needs some iterations to correct the wrong output estimations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the network is made up of two contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9a5f767-e0a4-46cc-9e88-3973029b93c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The notation *y/x^((i))* indicates the *i^(th)* element of *y/x*. The first
    term produces a partial output based only on the input, while the second one uses
    hierarchical lateral connections to correct the values and enforce the decorrelation.
    The internal weights *w[ij]* are updated using the standard version of Oja''s
    rule (this is mainly responsible for the convergence of each weight vector to
    the first principal component):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5cd6989-755d-4167-a085-1fbf5cbfc5f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead, the external weights *v[jk]* are updated using an anti-Hebbian rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c3b2608-4927-40c2-bf13-43fb8094fd6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous formula can be split into two parts: the first term *-ηy[j]y[k]*
    acts in the opposite direction of a standard version of Hebb''s rule (that''s
    why it''s called anti-Hebbian) and forces the decorrelation. The second one *-ηy[j]y[k]v[jk]*
    acts as a regularizer and it''s analogous to Oja''s rule. The term *-ηy[j]y[k]*
    works as a feedback signal for the Oja''s rule that readapts the updates according
    to the new magnitude of the actual output. In fact, after modifying the lateral
    connections, the outputs are also forced to change and this modification impacts
    on the update of *w[ij]*. When all the outputs are decorrelated, the vectors *w[i]*
    are implicitly obliged to be orthogonal. It''s possible to imagine an analogy
    with the Gram-Schmidt orthogonalization, even if in this case the relation between
    the extraction of different components and the decorrelation is more complex.
    Like for Sanger''s network, this model extracts the first *m* principal components
    in descending order (the reason is the same that has been intuitively explained),
    but for a complete (non-trivial) mathematical proof, please refer to the aforementioned
    paper.'
  prefs: []
  type: TYPE_NORMAL
- en: If input dimensionality is *n* and the number of components is equal to *m*, it's
    possible to use a lower-triangular matrix *V (m × m)* with all diagonal elements
    set to *0* and a standard matrix for *W (n × m)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of *W* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c226c098-cf73-4aa3-a6a5-33ea8a83fcf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, *w[i]* is a column-vector that must converge to the corresponding
    eigenvector. The structure of *V* is instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9e7ed01-388d-4a44-8937-86da61b19c04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this notation, the output becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/891105c6-b4ac-406d-b433-f0e1d4c2223a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the output is based on recurrent lateral connections, its value must be
    stabilized by iterating the previous formula for a fixed number times or until
    the norm between two consecutive values becomes smaller than a predefined threshold.
    In our example, we use a fixed number of iterations equal to five. The update
    rules cannot be written directly in matrix notation, but it''s possible to use
    the vectors *w*[*i* ](columns) and *v*[*j* ](rows):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d3a2fdb-8a2d-45fb-95f0-68f1f59c729e.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, *y^((i))* means the *i^(th)* component of *y*. The two matrices
    must be populated with a loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete Rubner-Tavan''s network algorithm is (the dimensionality of *x*
    is *n*, the number of components is denoted with *m*):'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize  *W^((0))* randomly. The shape is *(n × m)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize  *V^((0))* randomly. The shape is *(m × m)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V^((0))= Tril(V^((0)))*. *Tril(•)* transforms the input argument in a lower-triangular
    matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set all diagonal components of *V^((0))* equal to *0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `learning_rate` *η* (for example, `0.001`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a `threshold` *Thr* (for example, `0.0001`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a cycle counter *T=0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of iterations `max_iterations` (for example, 1000).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set a number of `stabilization_cycles` (for example, `5`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *||W^((t)) - W^((t-1))||[F] > Thr* and T < `max_iterations`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *T = T + 1*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *x* in *X*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *y[prev]* to zero. The shape is (*m, 1*).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to `stabilization_cycles`:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*y = W^Tx + Vy[prev]*.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*y[prev] = y*.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the updates for *W* and *V*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two empty matrices *ΔW (n × m)* and *ΔV (m × m)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'for *t=1* to *m*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Δw[t] = ηy^((t))(x - y^((t))w[t])*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Δv[t] = -ηy^((t))(y + y^((t))v[t])*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update *W* and *V*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*W^((t+1)) = W^((t)) + ΔW*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*V^((t+1)) = V^((t)) + ΔV*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V = Tril(V)* and set all the diagonal elements to *0*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *W^((t+1)) = W^((t+1))** / ||W^((t+1))||^((columns))* (The norm must be
    computed column-wise)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we have adopted both a threshold and a maximum number of iterations
    because this algorithms normally converges very quickly. Moreover, I suggest the
    reader always checks the shapes of vectors and matrices when performing dot products.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, as well as in all the other ones, the NumPy random seed is
    set equal to `1000` (`np.random.seed(1000)`). Using different values (or repeating
    more times the experiments without resetting the seed) can lead to slightly different
    results (which are always coherent).
  prefs: []
  type: TYPE_NORMAL
- en: Example of Rubner-Tavan's network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our Python example, we are going to use the same dataset already created
    for the Sanger''s network (which is expected to be available in the variable `Xs`).
    Therefore, we can start setting up all the constants and variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it''s possible to implement the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The final `W` and the output covariance matrix are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the algorithm has successfully converged to the eigenvectors (in
    descending order) and the output covariance matrix is almost completely decorrelated
    (the sign of the non-diagonal elements can be either positive or negative). Rubner-Tavan's
    networks are generally faster than Sanger's network, thanks to the feedback signal
    created by the anti-Hebbian rule; however, it's important to choose the right
    value for the learning rate. A possible strategy is to implement a temporal decay
    (as done in Sanger's network) starting with a value not greater than `0.0001`.
    However, it's important to reduce *η* when *n* increases (for example, *η = 0.0001
    / n*), because the normalization strength of Oja's rule on the lateral connections
    *v[jk]* is often not enough to avoid over and underflows when *n >> 1*. I don't
    suggest any extra normalization on *V* (which must be carefully analyzed considering
    that *V* is singular) because it can slow down the process and reduce the final
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Self-organizing maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Self-organizing maps (SOMs)** have been proposed by Willshaw and Von Der
    Malsburg (*Willshaw D. J., Von Der Malsburg C., How patterned neural connections
    can be set up by self-organization, Proceedings of the Royal Society of London,
    B/194, N. 1117*) to model different neurobiological phenomena observed in animals.
    In particular, they discovered that some areas of the brain develop structures
    with different areas, each of them with a high sensitivity for a specific input
    pattern. The process behind such a behavior is quite different from what we have
    discussed up until now, because it''s based on competition among neural units
    based on a principle called **winner-takes-all**. During the training period,
    all the units are excited with the same signal, but only one will produce the
    highest response. This unit is automatically candidate to become the receptive
    basin for that specific pattern. The particular model we are going to present
    has been introduced by **Kohonen** (in the paper *Kohonen T., Self-organized formation
    of topologically correct feature maps, Biological Cybernetics, 43/1*) and it''s
    named after him.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea is to implement a gradual winner-takes-all paradigm, to avoid
    the premature convergence of a neuron (as a definitive winner) and increment the
    level of plasticity of the network. This concept is expressed graphically in the
    following graph (where we are considering a linear sequence of neurons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57a73e57-00b6-4470-b100-4cfb3fa1ac95.png)'
  prefs: []
  type: TYPE_IMG
- en: Mexican-hat dynamic implemented by a Kohonen network
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the same pattern is presented to all the neurons. At the beginning
    of the training process (**t=0**), a positive response is observed in **x[i-2]**
    to **x[i+2]** with a peak in **x[i]**. The potential winner is obviously **x[i]**,
    but all these units are potentiated according to their distance from **x[i]**.
    In other words, the network (which is trained sequentially) is still receptive
    to change if other patterns produce a stronger activation. If instead **x[i]**
    keeps on being the winner, the radius is slightly reduced, until the only potentiated
    unit will be **x[i]**. Considering the shape of this function, this dynamic is
    often called *Mexican Hat*. With this approach, the network remains plastic until
    all the patterns have been repeatedly presented. If, for example, another pattern
    elicits a stronger response in **x[i]**, it's important that its activation is
    still not too high, to allow a fast reconfiguration of the network. At the same
    time, the new winner will probably be a neighbor of **x[i]**, which has received
    a partial potentiation and can easily take the place of **x[i]**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Kohonen SOM** (also known as Kohonen network or simply Kohonen map) is
    normally represented as a bidimensional map (for example, a square matrix *m ×
    m*, or any other rectangular shape), but 3D surfaces, such as spheres or toruses
    are also possible (the only necessary condition is the existence of a suitable
    metric). In our case, we always refer to a square matrix where each cell is a
    receptive neuron characterized by a synaptic weight *w* with the dimensionality
    of the input patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dedd26ae-e2c3-4165-b46c-c3b3daa5b1e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'During both training and working phases, the winning unit is determined according
    to a similarity measure between a sample and each weight vector. The most common
    metric is the Euclidean; hence, if we consider a bidimensional map *W* with a
    shape *(k × p)* so that *W ∈ ℜ^(k × p × n)*, the winning unit (in terms of its
    coordinates) is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5c7c5c2-7139-45ad-a402-832a0386cf87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As explained before, it''s important to avoid the premature convergence because
    the complete final configuration could be quite different from the initial one.
    Therefore, the training process is normally subdivided into two different stages.
    During the first one, whose duration is normally about 10-20% of the total number
    of iterations (let''s call this value *t[max]*), the correction is applied to
    the winning unit and its neighbors (computed by adopting a decaying radius). Instead,
    during the second one, the radius is set to 1.0 and the correction is applied
    only to the winning unit. In this way, it''s possible to analyze a larger number
    of possible configurations, automatically selecting the one associated with the
    least error. The neighborhood can have different shapes; it can be square (in
    closed 3D maps, the boundaries don''t exist anymore), or, more easily, it''s possible
    to employ a radial basis function based on an exponentially decaying distance-weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7966bd27-723f-4813-87ac-1097856d8226.png)'
  prefs: []
  type: TYPE_IMG
- en: The relative weight of each neuron is determined by the *σ(t)*. *σ[0 ]*function is
    the initial radius and *τ* is a time-constant that must be considered as a hyperparameter
    which determines the slope of the decaying weight. Suitable values are 5-10% of
    the total number of iterations. Adopting a radial basis function, it's not necessary
    to compute an actual neighborhood because the multiplication factor *n(i, j)*
    becomes close to zero outside of the boundaries. A drawback is related to the
    computational cost, which is higher than a square neighborhood (as the function
    must be computed for the whole map); however, it's possible to speed up the process
    by precomputing all the squared distances (the numerator) and exploiting the vectorization
    features offered by packages such as NumPy (a single exponential is computed every
    time).
  prefs: []
  type: TYPE_NORMAL
- en: 'The update rule is very simple and it''s based on the idea to move the winning
    unit synaptic weights closer to the pattern, *x[i,]* (repeated for the whole dataset,
    *X*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76b3c98b-fae1-47ca-a47f-68a63228bea4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *η(t)* function is the learning rate, which can be fixed, but it''s preferable
    to start with a higher value, *η[0 ]*and let it decay to a target final value,
    *η[∞]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59cd23a7-45d4-465e-a887-f2d7efc46a0a.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, the initial changes force the weights to align with the input patterns,
    while all the subsequent updates allow slight modifications to improve the overall
    accuracy. Therefore, each update is proportional to the learning rate, the neighborhood
    weighted distance, and the difference between each pattern and the synaptic vector.
    Theoretically, if *Δw[ij]* is equal to 0.0 for the winning unit, it means that
    a neuron has become the attractor of a specific input pattern, and its neighbors
    will be receptive to noisy/altered versions. The most interesting aspect is that
    the complete final map will contain the attractors for all patterns which are
    organized to maximize the similarity between adjacent units. In this way, when
    a new pattern is presented, the area of neurons that maps the most similar shapes
    will show a higher response. For example, if the patterns are made up of handwritten
    digits, attractors for the digit 1 and for digit 7 will be closer than the attractor,
    for example, for digit 8\. A malformed 1 (which could be interpreted as 7) will
    elicit a response that is between the first two attractors, allowing us to assign
    a relative probability based on the distance. As we're going to see in the example,
    this feature yields to a smooth transition between different variants of the same
    pattern class avoiding rigid boundaries that oblige a binary decision (like in
    a K-means clustering or in a hard classifier).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete Kohonen SOM algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initialize *W^((0))*. The shape is *(k × p × n)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize `nb_iterations`, the total number of iterations, and *t[max]* (for
    example, `nb_iterations` = 1000 and t[max ]= 150).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *τ* (for example, *τ* = 100).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *η[0]* and *η[∞]* (for example, *η*[*0* ]= 1.0 and *η*[*∞* ]= 0.05).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For `t = 0` to `nb_iterations`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t < t[max]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *η(t)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *σ(t)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Otherwise:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *η(t) = η[∞]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *σ(t) = σ[∞]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *x[i]* in *X*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the winning unit *u^** (let's assume that the coordinates are *i*, *j*)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *n(i, j)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the weight correction *Δw[ij]^((t))* to all synaptic weights *W^((t))*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Renormalize *W^((t)) = W^((t))** / ||W^((t))||^((columns))* (the norm must be
    computed column-wise)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of SOM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now implement an SOM using the Olivetti faces dataset. As the process
    can be very long, in this example we limit the number of input patterns to 100
    (with a 5 × 5 matrix). The reader can try with the whole dataset and a larger
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is loading the data, normalizing it so that all values are bounded
    between 0.0 and 1.0, and setting the constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can initialize the weight matrix using a normal distribution
    with a small standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to define the functions to determine the winning unit based on
    the least distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s also useful to define the functions *η(t)* and *σ(t)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As explained before, instead of computing the radial basis function for each
    unit, it''s preferable to use a precomputed distance matrix (in this case, 5 ×
    5 × 5 × 5) containing all the possible distances between couples of units. In
    this way, NumPy allows a faster calculation thanks to its vectorization features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `distance_matrix` function returns the value of the radial basis function
    for the whole map given the center point (the winning unit) `xt, yt` and the current
    value of *σ* `sigmat`. Now, it''s possible to start the training process (in order
    to avoid correlations, it''s preferable to shuffle the input sequence at the beginning
    of each iteration):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have set *η*[*∞* ]= `0.2` but I invite the reader to try different
    values and evaluate the final result. After training for `5000` epochs, we got
    the following weight matrix (each weight is plotted as a bidimensional array):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2b13744-1295-477f-a34a-6f447eb97f70.png)'
  prefs: []
  type: TYPE_IMG
- en: As it's possible to see, the weights have converged to faces with slightly different
    features. In particular, looking at the shapes of the faces and the expressions,
    it's easy to notice the transition between different attractors (some faces are
    smiling, while others are more serious; some have glasses, mustaches, and beards,
    and so on). It's also important to consider that the matrix is larger than the
    minimum capacity (there are ten different individuals in the dataset). This allows
    mapping more patterns that cannot be easily attracted by the right neuron. For
    example, an individual can have pictures with and without a beard and this can
    lead to confusion. If the matrix is too small, it's possible to observe an instability
    in the convergence process, while if it's too large, it's easy to see redundancies.
    The right choice depends on each different dataset and on the internal variance
    and there's no way to define a standard criterion. A good starting point is picking
    a matrix whose capacity is between 2.0 and 3.0 times larger than the number of
    desired attractors and then increasing or reducing its size until the accuracy
    reaches a maximum. The last element to consider is the labeling phase. At the
    end of the training process, we have no knowledge about the weight distribution
    in terms of winning neurons, so it's necessary to process the dataset and annotate
    the winning unit for each pattern. In this way, it's possible to submit new patterns
    to get the most likely label. This process has not been shown, but it's straightforward
    and the reader can easily implement it for every different scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed Hebb's rule, showing how it can drive the
    computation of the first principal component of the input dataset. We have also
    seen that this rule is unstable because it leads to the infinite growth of the
    synaptic weights and how it's possible to solve this problem using normalization
    or Oja's rule.
  prefs: []
  type: TYPE_NORMAL
- en: We have introduced two different neural networks based on Hebbian learning (Sanger's
    and Rubner-Tavan's networks), whose internal dynamics are slightly different,
    which are able to extract the first *n* principal components in the right order
    (starting from the largest eigenvalue) without eigendecomposing the input covariance
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have introduced the concept of SOM and presented a model called
    a Kohonen network, which is able to map the input patterns onto a surface where
    some attractors (one per class) are placed through a competitive learning process.
    Such a model is able to recognize new patterns (belonging to the same distribution)
    by eliciting a strong response in the attractor, that is most similar to the pattern.
    In this way, after a labeling process, the model can be employed as a soft classifier
    that can easily manage noisy or altered patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to discuss some important clustering algorithms,
    focusing on the difference (already discussed in the previous chapters) between
    hard and soft clustering and discussing the main techniques employed to evaluate
    the performance of an algorithm.
  prefs: []
  type: TYPE_NORMAL
