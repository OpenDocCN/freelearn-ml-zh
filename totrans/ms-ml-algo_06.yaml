- en: Hebbian Learning and Self-Organizing Maps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赫布学习和自组织映射
- en: In this chapter, we're going to introduce the concept of Hebbian learning, based
    on the methods defined by the psychologist Donald Hebb. These theories immediately
    showed how a very simple biological law is able to describe the behavior of multiple
    neurons in achieving complex goals and was a pioneering strategy that linked the
    research activities in the fields of artificial intelligence and computational
    neurosciences.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍基于心理学家唐纳德·赫布（Donald Hebb）定义的方法的赫布学习（Hebbian learning）概念。这些理论立即展示了一个非常简单的生物法则如何能够描述多个神经元在实现复杂目标时的行为，并且是连接人工智能和计算神经科学领域研究活动的开创性策略。
- en: 'In particular, we are going to discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们将要讨论以下主题：
- en: The Hebb rule for a single neuron, which is a simple but biologically plausible
    behavioral law
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个神经元的赫布法则，这是一个简单但生物上合理的行性行为法则
- en: Some variants that have been introduced to overcome a few stability problems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些引入的变体，用以克服一些稳定性问题
- en: The final result achieved by a Hebbian neuron, which consists of computing the
    first principal component of the input dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赫布神经元最终得到的结果，它包括计算输入数据集的第一个主成分
- en: Two neural network models (Sanger's network and Rubner-Tavan's network) that
    can extract a generic number of principal components
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种神经网络模型（Sanger网络和Rubner-Tavan网络），可以提取一定数量的主成分
- en: The concept of **Self-Organizing Maps** (**SOMs**) with a focus on the Kohonen
    Networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自组织映射**（**SOMs**）的概念，重点关注Kohonen网络'
- en: Hebb's rule
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赫布法则
- en: '**Hebb''s rule** has been proposed as a conjecture in 1949 by the Canadian
    psychologist Donald Hebb to describe the synaptic plasticity of natural neurons.
    A few years after its publication, this rule was confirmed by neurophysiological
    studies, and many research studies have shown its validity in many application,
    of Artificial Intelligence. Before introducing the rule, it''s useful to describe
    the generic Hebbian neuron, as shown in the following diagram:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**赫布法则**是由加拿大心理学家唐纳德·赫布（Donald Hebb）于1949年提出的一个猜想，用以描述自然神经元的突触可塑性。在其发表几年后，这一法则通过神经生理学研究得到了证实，许多研究已经表明它在人工智能领域的许多应用中是有效的。在介绍这一法则之前，描述一下以下图中所示的通用赫布神经元是有用的：'
- en: '![](img/4985e161-7f26-46bd-b3bf-7fc4bb204dab.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4985e161-7f26-46bd-b3bf-7fc4bb204dab.png)'
- en: Generic Hebbian neuron with a vectorial input
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具有向量输入的通用赫布神经元
- en: 'The neuron is a simple computational unit that receives an input vector *x*,
    from the pre-synaptic units (other neurons or perceptive systems) and outputs
    a single scalar value, *y*. The internal structure of the neuron is represented
    by a weight vector, *w,* that models the strength of each synapse. For a single
    multi-dimensional input, the output is obtained as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是一个简单的计算单元，它接收来自突触前单元（其他神经元或感知系统）的输入向量*x*，并输出一个单一的标量值*y*。神经元的内部结构由一个权重向量*w*表示，它模拟了每个突触的强度。对于单个多维输入，输出如下：
- en: '![](img/40a04e23-dfd5-421c-a65d-fefd03aa791a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/40a04e23-dfd5-421c-a65d-fefd03aa791a.png)'
- en: In this model, we are assuming that each input signal is encoded in the corresponding
    component of the vector, *x*; therefore, *x[i]* is processed by the synaptic weight *w[i,]*
    and so on. In the original version of Hebb's theory, the input vectors represent
    neural firing rates, which are always non-negative. This means that the synaptic
    weights can only be strengthened (the neuroscientific term for this phenomenon
    is **long-term potentiation** (**LTP**)). However, for our purposes, we assume
    that *x* is a real-valued vector, as is *w*. This condition allows modeling more
    artificial scenarios without a loss of generality.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们假设每个输入信号都编码在向量*x*的相应分量中；因此，*x[i]*由突触权重*w[i,]*处理，依此类推。在赫布理论的原始版本中，输入向量代表神经元的放电率，总是非负的。这意味着突触权重只能加强（神经科学中这种现象的术语是**长期增强**（**LTP**））。然而，为了我们的目的，我们假设*x*是一个实值向量，就像*w*一样。这个条件允许在不失去一般性的情况下模拟更多的人工场景。
- en: 'The same operation performed on a single vector holds when it''s necessary
    to process many input samples organized in a matrix. If we have *N* m-dimensional
    input vectors, the formula becomes as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要处理组织在矩阵中的多个输入样本时，对单个向量的相同操作仍然适用。如果我们有*N*个m维输入向量，公式如下：
- en: '![](img/17fb5f3f-1296-4d5a-9250-2054a211817c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17fb5f3f-1296-4d5a-9250-2054a211817c.png)'
- en: 'The basic form of Hebb''s rule in a discrete form can be expressed (for a single
    input) as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 赫布规则的基本形式在离散形式中可以表示如下（对于单个输入）：
- en: '![](img/6b07422b-adc1-44d4-b098-d10f7539e46e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6b07422b-adc1-44d4-b098-d10f7539e46e.png)'
- en: 'The weight correction is hence a vector that has the same orientation of *x*
    and magnitude equal to *|x|* multiplied by a positive parameter, *η*, which is
    called the learning rate and the corresponding output, *y* (which can have either
    a positive or a negative sign). The sense of *Δw* is determined by the sign of
    *y*; therefore, under the assumption that *x* and *y* are real values, two different
    scenarios arise from this rule:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 权重校正因此是一个与*x*具有相同方向且大小等于*|x|*乘以一个正参数*η*的向量，这个参数称为学习率，相应的输出*y*（可以是正或负）。*Δw*的方向由*y*的符号决定；因此，在假设*x*和*y*是实值的情况下，从这个规则中产生了两种不同的场景：
- en: If *x[i]* > 0 (< 0) and *y* > 0 (< 0), *w[i]* is strengthened
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*x[i]* > 0 (< 0)且*y* > 0 (< 0)，*w[i]*会加强
- en: If *x[i]* > 0 (< 0) and *y* < 0 (> 0), *w[i]* is weakened
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*x[i]* > 0 (< 0)且*y* < 0 (> 0)，*w[i]*会减弱
- en: 'It''s easy to understand this behavior considering two-dimensional vectors:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到二维向量，这个行为很容易理解：
- en: '![](img/172d792a-ded9-46e4-8f33-ef7367a00529.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/172d792a-ded9-46e4-8f33-ef7367a00529.png)'
- en: 'Therefore, if the initial angle *α* between *w* and *x* is less than 90°, *w*
    will have the same orientation of *x* and viceversa if *α* is greater than 90°.
    In the following diagram, there''s a schematic representation of this process:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果*w*和*x*之间的初始角度*α*小于90°，*w*将具有与*x*相同的方向，反之亦然，如果*α*大于90°。在下图中，有这个过程的示意图：
- en: '![](img/ba5db290-9b0f-434f-87fd-9837830b0738.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba5db290-9b0f-434f-87fd-9837830b0738.png)'
- en: Vectorial analysis of Hebb's rule
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 赫布规则的矢量分析
- en: 'It''s possible to simulate this behavior using a very simple Python snippet.
    Let''s start with a scenario where *α* is less than 90° and 50 iterations:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个非常简单的Python代码片段可以模拟这种行为。让我们从一个场景开始，其中*α*小于90°并且进行50次迭代：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As expected, the final angle, *α,* is close to zero and *w* has the same orientation
    and sense of *x*. We can now repeat the experiment with *α* greater than 90° (we
    change only the value of *w* because the procedure is the same):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，最终的角*α*接近零，*w*具有与*x*相同的方向和方向。现在我们可以用*α*大于90°重复这个实验（我们只改变*w*的值，因为过程是相同的）：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, the final angle, *α,* is about 180° and, of course, *w* has the
    opposite sense with respect to *x*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最终的角，*α*，大约是180°，当然，*w*相对于*x*具有相反的方向。
- en: 'The scientist S. Löwel expressed this concept with the famous sentence:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 科学家S. Löwel用著名的句子表达了这一概念：
- en: '"*Neurons that fire together wire together*"'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “*放电同步的神经元会连接在一起*”
- en: 'We can re-express this concept (adapting it to a machine learning scenario)
    by saying that the main assumption of this approach is based on the idea that
    when pre- and post-synaptic units are coherent (their signals have the same sign),
    the connection between neurons becomes stronger and stronger. On the other side,
    if they are discordant, the corresponding synaptic weight is decreased. For the
    sake of precision, if *x* is a spiking rate, it should be represented as a real
    function *x(t)* as well as *y(t)*. According to the original Hebbian theory, the
    discrete equation must be replaced by a differential equation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将其（适应到机器学习场景）重新表达这个概念，即这个方法的主要假设基于这样一个想法：当突触前和突触后单元协调一致（它们的信号具有相同的符号）时，神经元之间的连接会变得越来越强。另一方面，如果它们不一致，相应的突触权重会降低。为了精确起见，如果*x*是放电率，它应该表示为实函数*x(t)*以及*y(t)*。根据原始的赫布理论，离散方程必须被微分方程所取代：
- en: '![](img/c7eb6471-fb14-428f-bfa3-bf53dff22bc1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c7eb6471-fb14-428f-bfa3-bf53dff22bc1.png)'
- en: If *x(t)* and *y(t)* have the same fire rate, the synaptic weight is strengthened
    proportionally to the product of both rates. If instead there's a relatively long
    delay between the pre-synaptic activity *x(t)* and the post-synaptic one *y(t)*,
    the corresponding weight is weakened. This is a more biologically plausible explanation
    of the relation *fire together → wire together*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*x(t)*和*y(t)*具有相同的放电率，突触权重将按比例加强，与两个速率的乘积成正比。如果突触前活动*x(t)*和突触后活动*y(t)*之间存在相对较长的延迟，相应的权重会减弱。这是对“一起放电→一起连接”关系的更符合生物学的解释。
- en: 'However, even if the theory has a strong neurophysiological basis, some modifications
    are necessary. In fact, it''s easy to understand that the resulting system is
    always unstable. If two inputs are repeatedly applied (both real values and firing
    rates), the norm of the vector, w, grows indefinitely and this isn''t a plausible
    assumption for a biological system. In fact, if we consider a discrete iteration
    step, we have the following equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使理论有很强的神经生理学基础，也需要进行一些修改。事实上，很容易理解，所得到的系统总是不稳定的。如果反复应用两个输入（包括实值和放电率），向量
    w 的范数无限增长，这对生物系统来说不是一个合理的假设。事实上，如果我们考虑一个离散迭代步长，我们就有以下方程：
- en: '![](img/118b6d4e-b823-409d-878f-23b62659bfd5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/118b6d4e-b823-409d-878f-23b62659bfd5.png)'
- en: The previous output, *y[k,]* is always multiplied by a factor greater than *1*
    (except in the case of null input), therefore it grows without a bound. As *y
    = w · x*, this condition implies that the magnitude of *w* increases (or remains
    constant if the magnitude of *x* is null) at each iteration (a more rigorous proof
    can be easily obtained considering the original differential equation).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的输出 *y[k,]* 总是乘以一个大于 *1* 的因子（除了零输入的情况），因此它无界增长。由于 *y = w · x*，这个条件意味着在每个迭代中
    *w* 的大小增加（如果 *x* 的大小为零，则保持不变）（一个更严格的证明可以通过考虑原始微分方程轻松获得）。
- en: Such a situation is not only biologically unacceptable, but it's also necessary
    to properly manage it in machine learning problems in order to avoid a numerical
    overflow after a few iterations. In the next paragraph, we're going to discuss
    some common methods to overcome this issue. For now, we can continue our analysis
    without introducing a correction factor.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况不仅在生物学上不可接受，而且在机器学习问题中，为了防止迭代几次后出现数值溢出，也必须对其进行适当的管理。在下一段中，我们将讨论一些克服这一问题的常用方法。目前，我们可以继续分析，而不引入校正因子。
- en: 'Let''s now consider a dataset, *X*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在考虑一个数据集，*X*：
- en: '![](img/aaf67da3-266b-44a7-8ecd-796b0f33f4b8.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aaf67da3-266b-44a7-8ecd-796b0f33f4b8.png)'
- en: 'We can apply the rule iteratively to all elements, but it''s easier (and more
    useful) to average the weight modifications over the input samples (the index
    now refers to the whole specific vector, not to the single components):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将该规则迭代应用于所有元素，但平均输入样本（现在索引指的是整个特定向量，而不是单个分量）的权重修改更容易（也更有用）：
- en: '![](img/750fa24e-dfdb-40fd-8a83-5db4126319a6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/750fa24e-dfdb-40fd-8a83-5db4126319a6.png)'
- en: 'In the previous formula, *C* is the input correlation matrix:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*C* 是输入相关矩阵：
- en: '![](img/fc8af2b6-bb65-4941-83b5-a2d0f51b8b90.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc8af2b6-bb65-4941-83b5-a2d0f51b8b90.png)'
- en: For our purposes, however, it's useful to consider a slightly different Hebbian
    rule based on a threshold *θ* for the input vector (there's also a biological
    reason that justifies this choice, but it's beyond the scope of this book; the
    reader who is interested can find it in *Theoretical Neuroscience*,*Dayan P.*,
    *Abbott F. L.*, *The MIT Press*).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于我们来说，考虑一个基于输入向量阈值 *θ* 的略有不同的赫布规则是有用的（也有一个生物学理由可以证明这一选择，但超出了本书的范围；感兴趣的读者可以在
    *Theoretical Neuroscience*，*Dayan P.*，*Abbott F. L.*，*The MIT Press* 中找到它）。
- en: It's easy to understand that in the original theory where *x(t)* and *y(t)*
    are firing rates, this modification allows a phenomenon opposite to LTP and called
    **long-term depression** (**LTD**). In fact, when *x(t) < **θ* and *y(t)* is positive,
    the product *(x(t) - θ)y(t)* is negative and the synaptic weight is weakened.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易理解，在原始理论中，*x(t)* 和 *y(t)* 是放电率，这种修改允许一个与LTP相反的现象，称为**长期抑制**（**LTD**）。事实上，当
    *x(t) < **θ* 且 *y(t)* 为正时，乘积 *(x(t) - **θ**)y(t)* 为负，突触权重减弱。
- en: 'If we set *θ = 〈x〉 ≈ E[X]*, we can derive an expression very similar to the
    previous one, but based on the input covariance matrix (unbiased through the Bessel''s
    correction):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设 *θ = 〈x〉 ≈ E[X]*，我们可以推导出一个与之前非常相似的表达式，但基于输入协方差矩阵（通过贝塞尔校正无偏）：
- en: '![](img/55550e5b-8d33-4cb3-9a9b-d8ccb94e8a17.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/55550e5b-8d33-4cb3-9a9b-d8ccb94e8a17.png)'
- en: For obvious reasons, this variant of the original Hebb's rule is called the **covariance
    rule**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于明显的原因，这种原始赫布规则的变体被称为**协方差规则**。
- en: It's also possible to use the **Maximum Likelihood Estimation** (**MLE**) (or
    biased) covariance matrix (dividing by *N*), but it's important to check which
    version is adopted by the mathematical package that is employed. When using NumPy,
    it's possible to decide the version using the `np.cov()` function and setting
    the `bias=True/False` parameter (the default value is `False`). However, when
    *N >> 1*, the difference between versions decreases and can often be discarded.
    In this book, we'll use the unbiased version. The reader who wants to see further
    details about the Bessel's correction can read *Applied Statistics*,*Warner R.*,
    *SAGE Publications*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用**最大似然估计**（**MLE**）（或带偏置）的协方差矩阵（除以 *N*），但重要的是要检查所使用的数学包采用了哪个版本。当使用 NumPy
    时，可以通过设置 `np.cov()` 函数的 `bias=True/False` 参数来决定版本（默认值为 `False`）。然而，当 *N >> 1*
    时，版本之间的差异减小，通常可以忽略不计。在这本书中，我们将使用无偏版本。想要了解贝塞尔校正更多细节的读者可以阅读 *《应用统计学》*，*Warner R.*，*SAGE
    Publications*。
- en: Analysis of the covariance rule
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协方差规则分析
- en: 'The covariance matrix *Σ* is real and symmetric. If we apply the eigendecomposition,
    we get (for our purposes it''s more useful to keep *V^(-1)* instead of the simplified
    version *V^T*):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵 *Σ* 是实对称的。如果我们应用特征分解，我们得到（对我们来说，保留 *V^(-1)* 而不是简化的版本 *V^T* 更有用）：
- en: '![](img/1c15dfbc-e259-44ff-9b0e-76284886dfbc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c15dfbc-e259-44ff-9b0e-76284886dfbc.png)'
- en: '*V* is an orthogonal matrix (thanks to the fact that *Σ* is symmetric) containing
    the eigenvectors of *Σ* (as columns), while *Ω* is a diagonal matrix containing
    the eigenvalues. Let''s suppose we sort both eigenvalues (*λ[1]*, *λ[2]*, ...,
    *λ[m]*) and the corresponding eigenvectors (*v[1]*, *v[2]*, ..., *v[m]*) so that:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*V* 是一个正交矩阵（由于 *Σ* 是对称的），包含 *Σ* 的特征向量（作为列），而 *Ω* 是一个对角矩阵，包含特征值。假设我们将特征值（*λ[1]*，*λ[2]*，...，*λ[m]*)
    和相应的特征向量（*v[1]*，*v[2]*，...，*v[m]*) 排序，使得：'
- en: '![](img/87a4b53a-2784-44dd-9477-31a10ae287ea.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87a4b53a-2784-44dd-9477-31a10ae287ea.png)'
- en: 'Moreover, let''s suppose that *λ*[*1* ]is dominant over all the other eigenvalues
    (it''s enough that *λ[1] > λ[i]* with *i ≠ 1*). As the eigenvectors are orthogonal,
    they constitute a basis and it''s possible to express the vector w, with a linear
    combination of the eigenvectors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设 *λ*[*1* ]在所有其他特征值中占主导地位（只要 *λ[1] > λ[i]*，其中 *i ≠ 1* 就足够了）。由于特征向量是正交的，它们构成一个基，可以表示向量
    w 为特征向量的线性组合：
- en: '![](img/5e779900-d7e2-45b3-9f33-78ee8de613b1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5e779900-d7e2-45b3-9f33-78ee8de613b1.png)'
- en: 'The vector *u* contains the coordinates in the new basis. Let''s now consider
    the modification to the covariance rule:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 *u* 包含了在新基中的坐标。现在让我们考虑对协方差规则的修改：
- en: '![](img/1f0dcb94-6c16-46d8-8a28-6e07331f23d4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f0dcb94-6c16-46d8-8a28-6e07331f23d4.png)'
- en: 'If we apply the rule iteratively, we get a matrix polynomial:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们迭代地应用这个规则，我们得到一个矩阵多项式：
- en: '![](img/d8998d91-ae5b-442e-b5ce-f460f2343956.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d8998d91-ae5b-442e-b5ce-f460f2343956.png)'
- en: 'Exploiting the Binomial theorem and considering that *Σ**⁰=I*, we can get a
    general expression for *w^((k))* as a function of *w^((0))*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 利用二项式定理并考虑 *Σ**⁰=I*，我们可以得到 *w^((k))* 作为 *w^((0))* 的函数的一般表达式：
- en: '![](img/153067b7-ec73-4ae7-8012-40e7920f5964.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/153067b7-ec73-4ae7-8012-40e7920f5964.png)'
- en: 'Let''s now rewrite the previous formula using the change of basis:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用基变换重写前面的公式：
- en: '![](img/5b09938a-c336-43f8-8f18-c8fd5440a388.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5b09938a-c336-43f8-8f18-c8fd5440a388.png)'
- en: The vector *u^((0))* contains the coordinates of *w^((0))* in the new basis;
    hence, *w^((k))* is expressed as a polynomial where the generic term is proportional
    to *VΩ^iu^((0))*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 *u^((0))* 包含了 *w^((0))* 在新基中的坐标；因此，*w^((k))* 可以表示为一个多项式，其中通项与 *VΩ^iu^((0))*
    成正比。
- en: 'Let''s now consider the diagonal matrix *Ω^k*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑对角矩阵 *Ω^k*：
- en: '![](img/ecf21a99-c3ee-404f-b495-1a2ac596daef.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ecf21a99-c3ee-404f-b495-1a2ac596daef.png)'
- en: 'The last step derives from the hypothesis that *λ*[*1* ]is greater than any
    other eigenvalue and when *k → ∞*, all *λ[i≠1]^k<< λ[1]^k*. Of course, if *λ[i][≠1]
    > 1, λ[i≠1]*^(*k* )will grow as well as *λ[1]^(k )*however, the contribution of
    the *secondary* eigenvalues to *w^((k))* becomes significantly weaker when *k → ∞*.
    Just to understand the validity of this approximation, let''s consider the following
    situation where *λ[1]* is slightly larger that *λ[2]*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步来源于假设 *λ*[*1*] 大于任何其他特征值，并且当 *k → ∞* 时，所有 *λ[i≠1]^k<< λ[1]^k*。当然，如果 *λ[i][≠1]
    > 1，λ[i≠1]*^(*k* )也会增长，但是当 *k → ∞* 时，*λ[1]^(k )*对 *w^((k))* 的贡献变得显著减弱。为了理解这个近似的有效性，让我们考虑以下情况，其中
    *λ[1]* 略大于 *λ[2]*：
- en: '![](img/937e3fe5-ece3-4b89-8507-2c80a0e1aeba.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/937e3fe5-ece3-4b89-8507-2c80a0e1aeba.png)'
- en: 'The result shows a very important property: not only is the approximation correct,
    but as we''re going to show, if an eigenvalue *λ[i]* is larger than all the other
    ones, the covariance rule will always converge to the corresponding eigenvector
    *v[i]*. No other stable fixed points exist!'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了一个非常重要的性质：不仅近似是正确的，而且正如我们将要展示的，如果一个特征值 *λ[i]* 大于所有其他特征值，协方差规则将始终收敛到相应的特征向量
    *v[i]*。不存在其他稳定的固定点！
- en: This hypothesis is no more valid if *λ[1] = λ[2] = ... = λ[n]*. In this case,
    the total variance is explained equally by the direction of each eigenvector (a
    condition that implies a symmetry which isn't very common in real-life scenarios).
    This situation can also happen when working with finite-precision arithmetic,
    but in general, if the difference between the largest eigenvalue and the second
    one is less than the maximum achievable precision (for example, 32-bit floating
    point), it's plausible to accept the equality.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *λ[1] = λ[2] = ... = λ[n]* 时，这个假设就不再有效。在这种情况下，总方差由每个特征向量的方向等量解释（这是一个在现实场景中不常见的对称性条件）。这种情况也可能在处理有限精度算术时发生，但一般来说，如果最大特征值与第二个特征值之间的差异小于可达到的最大精度（例如，32位浮点数），接受相等性是合理的。
- en: Of course, we assume that the dataset is not whitened, because our goal (also
    in the next paragraphs) is to reduce the original dimensionality considering only
    a subset of components with the highest total variability (the decorrelation,
    like in **Principal Component Analysis** (**PCA**), must be an outcome of the
    algorithm, not a precondition). On the other side, zero-centering the dataset
    could be useful, even if not really necessary for this kind of algorithm.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们假设数据集没有被白化，因为我们的目标（也在下一段中）是只考虑具有最高总变异性的子集成分来减少原始维度（与主成分分析（PCA）一样，去相关必须是算法的结果，而不是先决条件）。另一方面，对数据集进行零中心化可能是有用的，尽管对于这类算法来说并非真正必要。
- en: 'If we rewrite the expression for *w[k]* considering this approximation, we
    obtain the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑这个近似重新写 *w[k]* 的表达式，我们得到以下结果：
- en: '![](img/573a41b5-7c5b-4984-9ed6-cafbe8c9e831.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/573a41b5-7c5b-4984-9ed6-cafbe8c9e831.png)'
- en: As *a[1]v + a[2]v + ... + a[k]v ∝ v*, this result shows that, when *k → ∞*, *w[k]*
    will become proportional to the first eigenvector of the covariance matrix *Σ*
    (if *u[1]^((0))* is not null) and its magnitude, without normalization, will grow
    indefinitely. The spurious effect due to the other eigenvalues becomes negligible
    (above all, if *w* is divided by its norm, so that the length is always *||w||
    = 1*) after a limited number of iterations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *a[1]v + a[2]v + ... + a[k]v ∝ v*，这个结果表明，当 *k → ∞* 时，*w[k]* 将与协方差矩阵 *Σ* 的第一个特征向量成正比（如果
    *u[1]^((0))* 不为零），其大小在没有归一化的情况下将无限增长。由于其他特征值引起的虚假效应在有限次迭代后变得可以忽略（尤其是如果 *w* 除以其范数，使得长度始终为
    *||w|| = 1*）。
- en: 'However, before drawing our conclusions, an important condition must be added:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在得出结论之前，必须添加一个重要条件：
- en: '![](img/885c829c-9e7c-416e-9cf5-e0ec02e1586c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/885c829c-9e7c-416e-9cf5-e0ec02e1586c.png)'
- en: 'In fact, if *w(0)* were orthogonal to *v1*, we would get (the eigenvectors
    are orthogonal to each other):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果 *w(0)* 与 *v1* 正交，我们将得到（特征向量彼此正交）：
- en: '![](img/231720a5-186b-414f-8052-d8ca438252f3.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/231720a5-186b-414f-8052-d8ca438252f3.png)'
- en: This important result shows how a Hebbian neuron working with the covariance
    rule is able to perform a PCA limited to the first component without the need
    for eigendecomposing *Σ*. In fact, the vector *w* (we're not considering the problem
    of the increasing magnitude, which can be easily managed) will rapidly converge
    to the orientation where the input dataset *X* has the highest variance. In [Chapter
    5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and Applications,*
    we discussed the details of PCA; in the next paragraph, we're going to discuss
    a couple of methods to find the first N principal components using a variant of
    the Hebb's rule.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个重要结果展示了如何使用协方差规则工作的赫布神经元能够执行仅限于第一个成分的PCA，而无需对 *Σ* 进行特征分解。实际上，向量 *w*（我们不考虑大小增加的问题，这可以很容易地处理）将迅速收敛到输入数据集
    *X* 变异性最高的方向。在[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)“EM算法及其应用”中，我们讨论了PCA的细节；在下一段中，我们将讨论使用赫布规则的变体来找到前N个主成分的几种方法。
- en: Example of covariance rule application
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协方差规则应用示例
- en: 'Before moving on, let''s simulate this behavior with a simple Python example.
    We first generate `1000` values sampled from a bivariate Gaussian distribution
    (the variance is voluntarily asymmetric) and then we apply the covariance rule
    to find the first principal component (*w*^(*(0)* )has been chosen so not to be
    orthogonal to *v[1]*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们用一个简单的Python示例来模拟这种行为。我们首先生成`1000`个从双变量高斯分布中采样的值（方差是有意不对称的），然后应用协方差规则来找到第一个主成分（*w*^(*(0)* ）已被选择，以便不与*v[1]*正交）：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The algorithm is straightforward, but there are a couple of elements that we
    need to comment on. The first one is the normalization of vector *w* at the end
    of each iteration. This is one of the techniques needed to avoid the uncontrolled
    growth of *w*. The second *tricky* element is the final multiplication, *w • 50*.
    As we are multiplying by a positive scalar, the direction of *w* is not impacted,
    but it's easier to show the vector in the complete plot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 算法很简单，但有几个元素我们需要评论。第一个是每次迭代结束时向量 *w* 的归一化。这是避免 *w* 无控制增长所需的技术之一。第二个“棘手”的元素是最后的乘法，*w
    • 50*。因为我们乘以一个正标量，所以 *w* 的方向不受影响，但在完整的图中更容易展示向量。
- en: 'The result is shown in the following diagram:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在以下图中展示：
- en: '![](img/e48fbb26-9d2b-42eb-920b-060cd073f118.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e48fbb26-9d2b-42eb-920b-060cd073f118.png)'
- en: Application of the covariance rule. w[∞] becomes proportional to the first principal
    component
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差规则的应用。w[∞]成为第一个主成分的比例
- en: After a limited number of iterations, *w[∞]* has the same orientation of the
    principal eigenvector which is, in this case, parallel to the *x* axes. The sense
    depends on the initial value *w[0]*; however, in a PCA, this isn't an important
    element.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 经过有限次数的迭代后，*w[∞]* 具有与主特征向量相同的方向，在这种情况下，与 *x* 轴平行。方向取决于初始值 *w[0]*；然而，在主成分分析（PCA）中，这不是一个重要的元素。
- en: Weight vector stabilization and Oja's rule
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重向量稳定化和Oja规则
- en: 'The easiest way to stabilize the weight vector is normalizing it after each
    update. In this way, its length will be always kept equal to one. In fact, in
    this kind of neural networks we are not interested in the magnitude, but only
    in the direction (that remains unchanged after the normalization). However, there
    are two main reasons that discourage this approach:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定权重向量的最简单方法是每次更新后对其进行归一化。这样，其长度将始终保持等于一。实际上，在这种类型的神经网络中，我们感兴趣的并不是大小，而是方向（归一化后保持不变）。然而，有两个主要原因使得这种方法不太可行：
- en: It's non-local. To normalize vector *w*, we need to know all its values and
    this isn't biologically plausible. A real synaptic weight model should be self-limiting,
    without the need to have access to external pieces of information that cannot
    be available.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是非局部的。为了归一化向量 *w*，我们需要知道其所有值，这在生物学上是不可能的。一个真实的突触权重模型应该是自我限制的，不需要访问外部信息，而这些信息可能无法获得。
- en: The normalization must be performed after having applied the correction and
    hence needs a double iterative step.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化必须在应用校正之后进行，因此需要双重迭代步骤。
- en: 'In many machine learning contexts, these conditions are not limiting and they
    can be freely adopted, but when it''s necessary to work with neuroscientific models,
    it''s better to look for other solutions. In a discrete form, we need to determine
    a correction term for the standard Hebb''s rule:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多机器学习场景中，这些条件并不是限制性的，它们可以被自由采用，但当需要与神经科学模型一起工作时，最好寻找其他解决方案。在离散形式中，我们需要为标准的赫布规则确定一个校正项：
- en: '![](img/ffdb9cc5-e9d4-4945-b336-68dd948cea74.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ffdb9cc5-e9d4-4945-b336-68dd948cea74.png)'
- en: 'The *f* function can work both as a local and non-local normalizer. An example
    of the first type is **Oja''s rule**:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* 函数可以作为局部和非局部归一化器。第一种类型的一个例子是**Oja规则**：'
- en: '![](img/e3bda4b6-7a4b-4f1a-9062-c7f2e2ab741c.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e3bda4b6-7a4b-4f1a-9062-c7f2e2ab741c.png)'
- en: 'The *α* parameter is a positive number that controls the strength of the normalization.
    A non-rigorous proof of the stability of this rule can be obtained considering
    the condition:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*α* 参数是一个控制归一化强度的正数。可以通过考虑以下条件获得该规则稳定性的非严格证明：'
- en: '![](img/37414372-7223-4fdb-b431-147780383d76.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/37414372-7223-4fdb-b431-147780383d76.png)'
- en: 'The second expression implies that:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个表达式意味着：
- en: '![](img/00011c8c-f57b-4ec0-bba1-b835ba71c37a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00011c8c-f57b-4ec0-bba1-b835ba71c37a.png)'
- en: 'Therefore, when *t → ∞*, the magnitude of the weight correction becomes close
    to zero and the length of the weight vector *w* will approach a finite limit value:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当*t → ∞*时，权重校正的幅度接近于零，权重向量*w*的长度将趋近于一个有限极限值：
- en: '![](img/e9115aeb-7623-4d4e-aa59-f3fb96f7756a.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e9115aeb-7623-4d4e-aa59-f3fb96f7756a.png)'
- en: Sanger's network
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sanger网络
- en: 'A **Sanger''s network** is a neural network model for online *Principal Component*
    extraction proposed by T. D. Sanger in *Optimal Unsupervised Learning in a Single-Layer
    Linear Feedforward Neural Network*, *Sanger T. D.*, *Neural Networks*, *1989/2*.
    The author started with the standard version of Hebb''s rule and modified it to
    be able to extract a variable number of principal components (*v[1], v[2], ...,
    v[m]*) in descending order (*λ[1] > λ[2] > ... > λ[m]*). The resulting approach,
    which is a natural extension of Oja''s rule, has been called the **Generalized
    Hebbian Rule** (**GHA**) (or Learning). The structure of the network is represented
    in the following diagram:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sanger网络**是由T. D. Sanger在《单层线性前馈神经网络中的最优无监督学习》一文中提出的在线**主成分**提取的神经网络模型，发表于*Neural
    Networks*，1989年第2期。作者从Hebb规则的标准版本开始，修改它以能够按降序提取可变数量的主成分（v[1]，v[2]，...，v[m]）（λ[1]
    > λ[2] > ... > λ[m]）。这种结果方法，作为Oja规则的自然扩展，被称为**广义Hebbian规则**（**GHA**）（或学习）。网络的结构在以下图中表示：'
- en: '![](img/922bd024-d3e8-440d-92f9-b7211b43f136.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/922bd024-d3e8-440d-92f9-b7211b43f136.png)'
- en: 'The network is fed with samples extracted from an n-dimensional dataset:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用从n维数据集中提取的样本进行喂养：
- en: '![](img/5438be6b-e7b4-4d77-a244-27166015c419.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5438be6b-e7b4-4d77-a244-27166015c419.png)'
- en: The *m* output neurons are connected to the input through a weight matrix, *W
    = {w[ij]},* where the first index refers to the input components (pre-synaptic
    units) and the second one to the neuron. The output of the network can be easily
    computed with a scalar product; however, in this case, we are not interested in
    it, because just like for the covariance (and Oja's) rules, the principal components
    are extracted through the weight updates.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*m*个输出神经元通过权重矩阵*W = {w[ij]}*与输入连接，其中第一个索引指的是输入分量（突触单元），第二个索引指的是神经元。网络的输出可以通过标量积轻松计算；然而，在这种情况下，我们对此不感兴趣，因为就像协方差（和Oja）规则一样，主成分是通过权重更新提取的。'
- en: 'The problem that arose after the formulation of Oja''s rule was about the extraction
    of multiple components. In fact, if we applied the original rule to the previous
    network, all weight vectors (the rows of *w*) would converge to the first principal
    component. The main idea (based on the **Gram-Schmidt** orthonormalization method)
    to overcome this limitation is based on the observation that once we have extracted
    the first component *w[1]*, the second one *w[2]* can be forced to be orthogonal
    to *w[1]*, the third one *w[3]* can be forced to be orthogonal to *w[1]* and *w[2]*,
    and so on. Consider the following representation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Oja规则提出后出现的问题涉及多个成分的提取。实际上，如果我们将原始规则应用于前面的网络，所有权重向量（*w*的行）都会收敛到第一个主成分。克服这种限制的主要思想（基于**Gram-Schmidt**正交化方法）基于以下观察：一旦我们提取了第一个成分*w[1]*，第二个成分*w[2]*就可以被强制与*w[1]*正交，第三个成分*w[3]*可以被强制与*w[1]*和*w[2]*正交，依此类推。考虑以下表示：
- en: '![](img/17d5ae7b-0982-409f-800d-a40557680cc4.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17d5ae7b-0982-409f-800d-a40557680cc4.png)'
- en: Orthogonalization of two weight vectors
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 两个权重向量的正交化
- en: 'In this case, we are assuming that *w[1]* is stable and *w[2][0]* is another
    weight vector that is converging to *w[1]*. The projection of w[20] onto *w[1]*
    is as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们假设*w[1]*是稳定的，*w[2][0]*是另一个权重向量，它正在收敛到*w[1]*。w[20]在*w[1]*上的投影如下：
- en: '![](img/a8f20651-fe3c-4e2a-aca2-69f43e36661e.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a8f20651-fe3c-4e2a-aca2-69f43e36661e.png)'
- en: 'In the previous formula, we can omit the norm if we don''t need to normalize
    (in the network, this process is done after a complete weight update). The orthogonal component
    of *w[20]* is simply obtained with a difference:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，如果我们不需要归一化，可以省略范数（在网络中，这个过程是在完成完整的权重更新后进行的）。*w[20]*的正交分量可以通过差分简单地获得：
- en: '![](img/11870193-f3f7-464f-acbb-66481e7b485f.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/11870193-f3f7-464f-acbb-66481e7b485f.png)'
- en: 'Applying this method to the original Oja''s rule, we obtain a new expression
    for the weight update (called Sanger''s rule):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将此方法应用于原始的Oja规则，我们得到权重更新的新表达式（称为Sanger规则）：
- en: '![](img/9062ac15-b758-4542-a3d7-d8c33994de1b.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9062ac15-b758-4542-a3d7-d8c33994de1b.png)'
- en: The rule is referred to a single input vector *x*, hence *x[j]* is the *j^(th)*
    component of *x*. The first term is the classic Hebb's rule, which forces weight
    *w* to become parallel to the first principal component, while the second one
    acts in a way similar to the Gram-Schmidt orthogonalization, by subtracting a
    term proportional to the projection of *w* onto all the weights connected to the
    previous post-synaptic units and considering, at the same time, the normalization
    constraint provided by Oja's rule (which is proportional to the square of the
    output).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则指的是单个输入向量 *x*，因此 *x[j]* 是 *x* 的 *j^(th)* 个分量。第一项是经典的赫布规则，它迫使权重 *w* 与第一个主成分平行，而第二项则以一种类似于格拉姆-施密特正交化的方式起作用，通过减去与先前突触后单元连接的所有权重在
    *w* 上的投影成比例的项，同时考虑由奥贾规则提供的归一化约束（该约束与输出的平方成正比）。
- en: 'In fact, expanding the last term, we get the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，展开最后一个项，我们得到以下结果：
- en: '![](img/a681a15a-0bea-46f0-86b6-d82361d03a2f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a681a15a-0bea-46f0-86b6-d82361d03a2f.png)'
- en: The term subtracted to each component *w[ij]* is proportional to all the components
    where the index *j* is fixed and the first index is equal to *1, 2, ..., i*. This
    procedure doesn't produce an immediate orthogonalization but requires several
    iterations to converge. The proof is non-trivial, involving convex optimization
    and dynamic systems methods, but, it can be found in the aforementioned paper.
    Sanger showed that the algorithm converges always to the sorted first *n* principal
    components (from the largest eigenvalue to the smallest one) if the `learning_rate` *η(t)*
    decreases monotonically and converges to zero when *t → ∞*. Even if necessary
    for the formal proof, this condition can be relaxed (a stable *η < 1* is normally
    sufficient). In our implementation, matrix *W* is normalized after each iteration,
    so that, at the end of the process, *W^T* (the weights are in the rows) is orthonormal
    and constitutes a basis for the eigenvector subspace.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从每个分量 *w[ij]* 中减去的项与所有分量成比例，其中索引 *j* 固定，第一个索引等于 *1, 2, ..., i*。此过程不会立即产生正交化，但需要多次迭代才能收敛。证明是非平凡的，涉及凸优化和动态系统方法，但可以在上述论文中找到。Sanger表明，如果
    `学习率` *η(t)* 单调递减并随着 *t → ∞* 趋向于零，则算法始终收敛到排序后的前 *n* 个主成分（从最大的特征值到最小的特征值）。即使对于形式证明是必要的，这个条件也可以放宽（通常稳定的
    *η < 1* 足够）。在我们的实现中，矩阵 *W* 在每次迭代后都会进行归一化，因此，在过程结束时，*W^T*（权重在行中）是正交归一的，并构成特征向量子空间的一个基。
- en: 'In matrix form, the rule becomes as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，该规则如下：
- en: '![](img/3b86aee4-3330-452a-a4e0-ced608cb83b2.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3b86aee4-3330-452a-a4e0-ced608cb83b2.png)'
- en: Tril(•) is a matrix function that transforms its argument into a lower-triangular
    matrix and the term *yy^T* is equal to *Wxx^TW*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Tril(•) 是一个矩阵函数，将它的参数转换为一个下三角矩阵，而项 *yy^T* 等于 *Wxx^TW*。
- en: 'The algorithm for a Sanger''s network is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Sanger网络的算法如下：
- en: Initialize *W^((0))* with random values. If the input dimensionality is *n*
    and *m* principal components must be extracted, the shape will be *(m × n)*.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化 *W^((0))*。如果输入维度是 *n* 且必须提取 *m* 个主成分，则形状将为 *(m × n)*。
- en: Set a `learning_rate` *η* (for example, `0.01`).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个 `学习率` *η*（例如，`0.01`）。
- en: Set a `threshold` *Thr* (for example, `0.001`).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个 `阈值` *Thr*（例如，`0.001`）。
- en: Set a counter *T = 0.*
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个计数器 *T = 0*。
- en: 'While *||W^((t)) - W^((t-1))||[F] > Thr*:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *||W^((t)) - W^((t-1))||[F] > Thr* 时：
- en: Set *ΔW = 0* (same shape of *W*)
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *ΔW = 0*（与 *W* 相同的形状）
- en: 'For each *x* in *X*:'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *X* 中的每个 *x*：
- en: Set *T = T + 1*
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *T = T + 1*
- en: Compute *y = W^((t))x*
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *y = W^((t))x*
- en: Compute and accumulate *ΔW += η(yx^T - Tril(yy^T)W^((t))*
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并累积 *ΔW += η(yx^T - Tril(yy^T)W^((t))*
- en: Update *W^((t+1)) = W^((t)) + (η / T)ΔW*
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 *W^((t+1)) = W^((t)) + (η / T)ΔW*
- en: Set *W^((t+1)) = W^((t+1))** / ||W^((t+1))||^((rows))* (the norm must be computed
    row-wise)
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *W^((t+1)) = W^((t+1))** / ||W^((t+1))||^((rows))*（必须按行计算范数）
- en: The algorithm can also be iterated a fixed number of times (like in our example),
    or the two stopping approaches can be used together.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法也可以迭代固定次数（如我们的示例所示），或者可以将两种停止方法一起使用。
- en: Example of Sanger's network
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sanger网络的示例
- en: 'For this Python example, we consider a bidimensional zero-centered dataset
    `X` with 500 samples (we are using the function defined in the first chapter).
    After the initialization of `X`, we also compute the eigendecomposition, to be
    able to double-check the result:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Python示例中，我们考虑一个二维零中心的样本集 `X`，包含500个样本（我们使用第一章中定义的函数）。在初始化 `X` 之后，我们还计算特征分解，以便能够双重检查结果：
- en: '[PRE3]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The eigenvalues are in reverse order; therefore, we expect to have a final
    *W* with the rows swapped. The initial condition (with the weights multiplied
    by 15) is shown in the following diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值是逆序的；因此，我们预计最终的 *W* 将有行被交换。以下图表显示了初始条件（权重乘以 15）：
- en: '![](img/8b8617f9-5a6f-47cc-a63e-4a13b0ae3871.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b8617f9-5a6f-47cc-a63e-4a13b0ae3871.png)'
- en: 'Dataset with *W* initial condition, we can implement the algorithm. For simplicity,
    we preferred a fixed number of iterations (`5000`) with a `learning_rate` of *η=0.01*.
    The reader can modify the snippet to stop when the weight matrix becomes stable:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 具有初始条件 *W* 的数据集，我们可以实现该算法。为了简化，我们更喜欢使用固定的迭代次数（`5000`）和 `learning_rate` 为 *η=0.01*。读者可以修改代码片段，以便在权重矩阵变得稳定时停止：
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first thing to check is the final state of *W* (we transposed the matrix
    to be able to compare the columns):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要检查的是 *W* 的最终状态（我们转置了矩阵以便比较列）：
- en: '[PRE5]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As expected, *W* has converged to the eigenvectors of the input correlation
    matrix (the sign `*–*` which is associated with the sense of *w—*is not important
    because we care only about the orientation). The second eigenvalue is the highest,
    so the columns are swapped. Replotting the diagram, we get the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，*W* 已经收敛到输入相关矩阵的特征向量（与 *w—* 相关的符号 `*–*` 并不重要，因为我们只关心方向）。第二个特征值是最高的，因此列被交换。重新绘制图表，我们得到以下结果：
- en: '![](img/ae5146f7-2565-4816-becc-d82eb2f1ee16.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae5146f7-2565-4816-becc-d82eb2f1ee16.png)'
- en: Final condition, w has converged to the two principal components
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最终状态，w 已经收敛到两个主成分
- en: The two components are perfectly orthogonal (the final orientations can change
    according to the initial conditions or the random state) and *w[0]* points in
    the direction of the first principal component, while *w[1]* points in the direction
    of the second component. Considering this nice property, it's not necessary to
    check the magnitude of the eigenvalues; therefore, this algorithm can operate
    without eigendecomposing the input covariance matrix. Even if a formal proof is
    needed to explain this behavior, it's possible to understand it intuitively. Every
    single neuron converges to the first principal component given a full eigenvector
    subspace. This property is always maintained, but after the orthogonalization,
    the subspace is implicitly reduced by a dimension. The second neuron will always
    converge to the first component, which now corresponds to the global second component,
    and so on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个成分是完美正交的（最终的取向可以根据初始条件或随机状态而改变），*w[0]* 指向第一个主成分的方向，而 *w[1]* 指向第二个成分的方向。考虑到这个良好的性质，我们不需要检查特征值的幅度；因此，这个算法可以在不分解输入协方差矩阵的情况下运行。即使需要正式的证明来解释这种行为，也可以直观地理解它。每个神经元都会收敛到给定的完整特征向量子空间中的第一个主成分。这个性质始终得到保持，但在正交化之后，子空间隐式地减少了一个维度。第二个神经元将始终收敛到第一个成分，现在它对应于全局第二个成分，依此类推。
- en: One of the advantages of this algorithm (and also of the next one) is that a
    standard PCA is normally a bulk process (even if there are batch algorithms),
    while a Sanger's network is an online algorithm that is trained incrementally.
    In general, the time performance of a Sanger's network is worse than the direct
    approach because of the iterations (some optimizations can be achieved using more
    vectorization or GPU support). On the other side, a Sanger's network is memory-saving
    when the number of components is less than the input dimensionality (for example,
    the covariance matrix for *n=1000* has *10⁶* elements, if *m = 100*, the weight
    matrix has *10⁴* elements).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法（以及下一个算法）的一个优点是，标准的 PCA 通常是一个批量过程（即使有批量算法），而 Sanger 的网络是一个在线算法，可以逐步训练。一般来说，Sanger
    的网络的时间性能比直接方法差，因为迭代（可以通过更多的矢量化或 GPU 支持实现一些优化）。另一方面，当成分的数量小于输入维度时，Sanger 的网络可以节省内存（例如，对于
    *n=1000* 的协方差矩阵有 *10⁶* 个元素，如果 *m = 100*，则权重矩阵有 *10⁴* 个元素）。
- en: Rubner-Tavan's network
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rubner-Tavan 的网络
- en: 'In [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM Algorithm and
    Applications,* we said that any algorithm that decorrelates the input covariance
    matrix is performing a PCA without dimensionality reduction. Starting from this
    approach, Rubner, and Tavan (in the paper *A Self-Organizing Network for Principal-Components
    Analysis*,*Rubner J.*, *Tavan P.*, *Europhysics. Letters*, *10(7)*, *1989*) proposed
    a neural model whose goal is decorrelating the output components to force the
    consequent decorrelation of the output covariance matrix (in lower-dimensional
    subspace). Assuming a zero-centered dataset and *E[y] = 0*, the output covariance
    matrix for *m* principal components is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，“EM算法及其应用”中，我们提到任何去相关输入协方差矩阵的算法都在执行PCA而不进行降维。从这个方法出发，Rubner和Tavan（在论文“A
    Self-Organizing Network for Principal-Components Analysis”中，Rubner J.，Tavan P.，Europhysics.
    Letters，10(7)，1989）提出了一种神经网络模型，其目标是去相关输出分量，以强制输出协方差矩阵（在低维子空间）的后续去相关。假设数据集零中心化且*E[y]
    = 0*，*m*个主成分的输出协方差矩阵如下：
- en: '![](img/2d1f7b9f-24d5-4b1a-8917-70fc3898e4c9.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d1f7b9f-24d5-4b1a-8917-70fc3898e4c9.png)'
- en: 'Hence, it''s possible to achieve an approximate decorrelation, forcing the
    terms *y[i]y[j]* with *i ≠ j* to become close to zero. The main difference with
    a standard approach (such as whitening or vanilla PCA) is that this procedure
    is local, while all the standard methods operate globally, directly with the covariance
    matrix. The neural model proposed by the authors is shown in the following diagram
    (the original model was proposed for binary units, but it works quite well also
    for linear ones):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以实现近似的去相关，迫使*i ≠ j*的*y[i]y[j]*项接近于零。与标准方法（如白化或vanilla PCA）的主要区别在于，此过程是局部的，而所有标准方法都是全局的，直接与协方差矩阵操作。作者提出的神经网络模型如下所示（原始模型是为二进制单元提出的，但它对线性单元也相当有效）：
- en: '![](img/e3075a27-0124-44c9-9c04-7508c476e208.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3075a27-0124-44c9-9c04-7508c476e208.png)'
- en: Rubner-Tavan network. The connections v[jk] are based on the anti-Hebbian rule
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Rubner-Tavan网络。连接v[jk]基于反Hebbian规则
- en: 'The network has *m* output units and the last *m-1* neurons have a summing
    node that receives the weighted output of the previous units (hierarchical lateral
    connections). The dynamic is simple: the first output isn''t modified. The second
    one is forced to become decorrelated with the first one. The third one is forced
    to become decorrelated with both the first and the second one and so on. This
    procedure must be iterated a number of times because the inputs are presented
    one by one and the cumulative term that appears in the correlation/covariance
    matrix (it''s always easier to zero-center the dataset and work with the correlation
    matrix) must be implicitly split into its addends. It''s not difficult to understand
    that the convergence to the only stable fixed point (which has been proven to
    exist by the authors) needs some iterations to correct the wrong output estimations.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络有*m*个输出单元，最后*m-1*个神经元有一个求和节点，该节点接收前一个单元的加权输出（分层横向连接）。其动态过程很简单：第一个输出不被修改。第二个输出被迫与第一个输出去相关。第三个输出被迫与第一个和第二个输出都去相关，依此类推。由于输入是一个接一个地呈现的，并且出现在相关/协方差矩阵中的累积项（总是更容易将数据集零中心化并使用相关矩阵）必须隐式地分成其加数，因此必须迭代多次。不难理解，收敛到唯一的稳定固定点（作者已经证明其存在）需要一些迭代来纠正错误的输出估计。
- en: 'The output of the network is made up of two contributions:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出由两个贡献组成：
- en: '![](img/a9a5f767-e0a4-46cc-9e88-3973029b93c1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9a5f767-e0a4-46cc-9e88-3973029b93c1.png)'
- en: 'The notation *y/x^((i))* indicates the *i^(th)* element of *y/x*. The first
    term produces a partial output based only on the input, while the second one uses
    hierarchical lateral connections to correct the values and enforce the decorrelation.
    The internal weights *w[ij]* are updated using the standard version of Oja''s
    rule (this is mainly responsible for the convergence of each weight vector to
    the first principal component):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 符号*y/x^((i))*表示*y/x*的第*i*个元素。第一个项仅基于输入产生部分输出，而第二个项使用分层横向连接来纠正值并强制去相关。内部权重*w[ij]*使用Oja规则的标准版本进行更新（这主要是每个权重向量收敛到第一个主成分的原因）：
- en: '![](img/e5cd6989-755d-4167-a085-1fbf5cbfc5f9.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5cd6989-755d-4167-a085-1fbf5cbfc5f9.png)'
- en: 'Instead, the external weights *v[jk]* are updated using an anti-Hebbian rule:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，外部权重*v[jk]*使用反Hebbian规则进行更新：
- en: '![](img/5c3b2608-4927-40c2-bf13-43fb8094fd6d.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c3b2608-4927-40c2-bf13-43fb8094fd6d.png)'
- en: 'The previous formula can be split into two parts: the first term *-ηy[j]y[k]*
    acts in the opposite direction of a standard version of Hebb''s rule (that''s
    why it''s called anti-Hebbian) and forces the decorrelation. The second one *-ηy[j]y[k]v[jk]*
    acts as a regularizer and it''s analogous to Oja''s rule. The term *-ηy[j]y[k]*
    works as a feedback signal for the Oja''s rule that readapts the updates according
    to the new magnitude of the actual output. In fact, after modifying the lateral
    connections, the outputs are also forced to change and this modification impacts
    on the update of *w[ij]*. When all the outputs are decorrelated, the vectors *w[i]*
    are implicitly obliged to be orthogonal. It''s possible to imagine an analogy
    with the Gram-Schmidt orthogonalization, even if in this case the relation between
    the extraction of different components and the decorrelation is more complex.
    Like for Sanger''s network, this model extracts the first *m* principal components
    in descending order (the reason is the same that has been intuitively explained),
    but for a complete (non-trivial) mathematical proof, please refer to the aforementioned
    paper.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个公式可以分为两部分：第一项 *-ηy[j]y[k]* 与标准版的赫布规则（这就是为什么它被称为反赫布）方向相反，并强制去相关。第二项 *-ηy[j]y[k]v[jk]*
    起到正则化作用，类似于奥贾规则。项 *-ηy[j]y[k]* 作为奥贾规则的反馈信号，根据实际输出的新幅度重新调整更新。实际上，在修改侧向连接后，输出也被迫改变，这种修改会影响
    *w[ij]* 的更新。当所有输出都去相关后，向量 *w[i]* 隐含地必须正交。可以想象一个与格拉姆-施密特正交化的类比，即使在这种情况下，不同分量的提取与去相关的关联更为复杂。就像桑格网络一样，该模型按降序提取前
    *m* 个主成分（原因与之前直观解释的相同），但对于一个完整的（非平凡）数学证明，请参阅上述论文。
- en: If input dimensionality is *n* and the number of components is equal to *m*, it's
    possible to use a lower-triangular matrix *V (m × m)* with all diagonal elements
    set to *0* and a standard matrix for *W (n × m)*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入维度为 *n* 且分量数等于 *m*，则可以使用所有对角元素设置为 *0* 的下三角矩阵 *V (m × m)* 和标准矩阵 *W (n × m)*。
- en: 'The structure of *W* is as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*W* 的结构如下：'
- en: '![](img/c226c098-cf73-4aa3-a6a5-33ea8a83fcf0.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c226c098-cf73-4aa3-a6a5-33ea8a83fcf0.png)'
- en: 'Therefore, *w[i]* is a column-vector that must converge to the corresponding
    eigenvector. The structure of *V* is instead:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此， *w[i]* 是一个必须收敛到相应特征向量的列向量。*V* 的结构如下：
- en: '![](img/f9e7ed01-388d-4a44-8937-86da61b19c04.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f9e7ed01-388d-4a44-8937-86da61b19c04.png)'
- en: 'Using this notation, the output becomes as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种表示法，输出结果如下：
- en: '![](img/891105c6-b4ac-406d-b433-f0e1d4c2223a.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/891105c6-b4ac-406d-b433-f0e1d4c2223a.png)'
- en: 'As the output is based on recurrent lateral connections, its value must be
    stabilized by iterating the previous formula for a fixed number times or until
    the norm between two consecutive values becomes smaller than a predefined threshold.
    In our example, we use a fixed number of iterations equal to five. The update
    rules cannot be written directly in matrix notation, but it''s possible to use
    the vectors *w*[*i* ](columns) and *v*[*j* ](rows):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输出基于循环侧向连接，其值必须通过迭代前一个公式固定次数或直到连续两个值之间的范数小于预定义的阈值来稳定。在我们的例子中，我们使用固定次数等于五的迭代。更新规则不能直接用矩阵表示法写出，但可以使用向量
    *w*[*i* ](列) 和 *v*[*j* ](行)：
- en: '![](img/0d3a2fdb-8a2d-45fb-95f0-68f1f59c729e.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0d3a2fdb-8a2d-45fb-95f0-68f1f59c729e.png)'
- en: In this case, *y^((i))* means the *i^(th)* component of *y*. The two matrices
    must be populated with a loop.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下， *y^((i))* 表示 *y* 的 *i^(th)* 个分量。这两个矩阵必须通过循环来填充。
- en: 'The complete Rubner-Tavan''s network algorithm is (the dimensionality of *x*
    is *n*, the number of components is denoted with *m*):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 Rubner-Tavan 网络算法如下（*x* 的维度为 *n*，分量数用 *m* 表示）：
- en: Initialize  *W^((0))* randomly. The shape is *(n × m)*.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化  *W^((0))*。其形状为 *(n × m)*。
- en: Initialize  *V^((0))* randomly. The shape is *(m × m)*.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化 *V^((0))*。其形状为 *(m × m)*。
- en: Set *V^((0))= Tril(V^((0)))*. *Tril(•)* transforms the input argument in a lower-triangular
    matrix.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *V^((0))= Tril(V^((0)))*。*Tril(•)* 将输入参数转换为下三角矩阵。
- en: Set all diagonal components of *V^((0))* equal to *0*.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *V^((0))* 的所有对角元素设置为 *0*。
- en: Set the `learning_rate` *η* (for example, `0.001`).
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `学习率` *η*（例如，`0.001`）。
- en: Set a `threshold` *Thr* (for example, `0.0001`).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个 `阈值` *Thr*（例如，`0.0001`）。
- en: Set a cycle counter *T=0*.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置周期计数器 *T=0*。
- en: Set a maximum number of iterations `max_iterations` (for example, 1000).
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置最大迭代次数 `max_iterations`（例如，1000）。
- en: 'Set a number of `stabilization_cycles` (for example, `5`):'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个 `稳定周期数` *stabilization_cycles*（例如，`5`）：
- en: 'While *||W^((t)) - W^((t-1))||[F] > Thr* and T < `max_iterations`:'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *||W^((t)) - W^((t-1))||[F] > Thr* 且 T < `max_iterations` 时：
- en: Set *T = T + 1*.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *T = T + 1* 设置。
- en: 'For each *x* in *X*:'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *X* 中的每个 *x*：
- en: Set *y[prev]* to zero. The shape is (*m, 1*).
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *y[prev]* 设置为零。形状为 (*m, 1*)。
- en: 'For *i=1* to `stabilization_cycles`:'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 `stabilization_cycles`：
- en: '*y = W^Tx + Vy[prev]*.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*y = W^Tx + Vy[prev]*。'
- en: '*y[prev] = y*.'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*y[prev] = y*。'
- en: 'Compute the updates for *W* and *V*:'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *W* 和 *V* 的更新：
- en: Create two empty matrices *ΔW (n × m)* and *ΔV (m × m)*
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个空矩阵 *ΔW (n × m)* 和 *ΔV (m × m)*
- en: 'for *t=1* to *m*:'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=1* 到 *m*：
- en: '*Δw[t] = ηy^((t))(x - y^((t))w[t])*'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Δw[t] = ηy^((t))(x - y^((t))w[t])*。'
- en: '*Δv[t] = -ηy^((t))(y + y^((t))v[t])*'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Δv[t] = -ηy^((t))(y + y^((t))v[t])*。'
- en: 'Update *W* and *V*:'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 *W* 和 *V*：
- en: '*W^((t+1)) = W^((t)) + ΔW*'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*W^((t+1)) = W^((t)) + ΔW*。'
- en: '*V^((t+1)) = V^((t)) + ΔV*'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*V^((t+1)) = V^((t)) + ΔV*。'
- en: Set *V = Tril(V)* and set all the diagonal elements to *0*
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *V = Tril(V)* 并将所有对角线元素设置为 *0*。
- en: Set *W^((t+1)) = W^((t+1))** / ||W^((t+1))||^((columns))* (The norm must be
    computed column-wise)
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *W^((t+1)) = W^((t+1))** / ||W^((t+1))||^((columns))*（规范必须按列计算）
- en: In this case, we have adopted both a threshold and a maximum number of iterations
    because this algorithms normally converges very quickly. Moreover, I suggest the
    reader always checks the shapes of vectors and matrices when performing dot products.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们采用了阈值和最大迭代次数，因为该算法通常收敛得非常快。此外，我建议读者在执行点积时始终检查向量和矩阵的形状。
- en: In this example, as well as in all the other ones, the NumPy random seed is
    set equal to `1000` (`np.random.seed(1000)`). Using different values (or repeating
    more times the experiments without resetting the seed) can lead to slightly different
    results (which are always coherent).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，以及所有其他例子中，NumPy 随机种子被设置为 `1000` (`np.random.seed(1000)`)。使用不同的值（或重复更多次实验而不重置种子）可能会导致略微不同的结果（这些结果总是连贯的）。
- en: Example of Rubner-Tavan's network
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rubner-Tavan 网络的示例
- en: 'For our Python example, we are going to use the same dataset already created
    for the Sanger''s network (which is expected to be available in the variable `Xs`).
    Therefore, we can start setting up all the constants and variables:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 Python 示例，我们将使用为 Sanger 的网络已创建的相同数据集（预计将在变量 `Xs` 中可用）。因此，我们可以开始设置所有常数和变量：
- en: '[PRE6]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, it''s possible to implement the training loop:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以实施训练循环：
- en: '[PRE7]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The final `W` and the output covariance matrix are as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的 `W` 和输出协方差矩阵如下：
- en: '[PRE8]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As expected, the algorithm has successfully converged to the eigenvectors (in
    descending order) and the output covariance matrix is almost completely decorrelated
    (the sign of the non-diagonal elements can be either positive or negative). Rubner-Tavan's
    networks are generally faster than Sanger's network, thanks to the feedback signal
    created by the anti-Hebbian rule; however, it's important to choose the right
    value for the learning rate. A possible strategy is to implement a temporal decay
    (as done in Sanger's network) starting with a value not greater than `0.0001`.
    However, it's important to reduce *η* when *n* increases (for example, *η = 0.0001
    / n*), because the normalization strength of Oja's rule on the lateral connections
    *v[jk]* is often not enough to avoid over and underflows when *n >> 1*. I don't
    suggest any extra normalization on *V* (which must be carefully analyzed considering
    that *V* is singular) because it can slow down the process and reduce the final
    accuracy.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，该算法已成功收敛到特征向量（按降序排列），并且输出协方差矩阵几乎完全去相关（非对角元素的符号可以是正也可以是负）。Rubner-Tavan
    的网络通常比 Sanger 的网络更快，这得益于由反 Hebbian 规则产生的反馈信号；然而，选择正确的学习率值非常重要。一种可能的策略是从不超过 `0.0001`
    的值开始实现时间衰减（如 Sanger 的网络中所做的那样）。然而，当 *n* 增加时（例如，*η = 0.0001 / n*），重要的是要减少 *η*，因为
    Oja 规则在侧向连接 *v[jk]* 上的归一化强度通常不足以避免当 *n >> 1* 时的溢出和下溢。我不建议对 *V*（必须仔细分析，因为 *V* 是奇异的）进行任何额外的归一化，因为这可能会减慢过程并降低最终精度。
- en: Self-organizing maps
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自组织映射
- en: '**Self-organizing maps (SOMs)** have been proposed by Willshaw and Von Der
    Malsburg (*Willshaw D. J., Von Der Malsburg C., How patterned neural connections
    can be set up by self-organization, Proceedings of the Royal Society of London,
    B/194, N. 1117*) to model different neurobiological phenomena observed in animals.
    In particular, they discovered that some areas of the brain develop structures
    with different areas, each of them with a high sensitivity for a specific input
    pattern. The process behind such a behavior is quite different from what we have
    discussed up until now, because it''s based on competition among neural units
    based on a principle called **winner-takes-all**. During the training period,
    all the units are excited with the same signal, but only one will produce the
    highest response. This unit is automatically candidate to become the receptive
    basin for that specific pattern. The particular model we are going to present
    has been introduced by **Kohonen** (in the paper *Kohonen T., Self-organized formation
    of topologically correct feature maps, Biological Cybernetics, 43/1*) and it''s
    named after him.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**自组织映射（SOMs）**是由威尔肖和冯·德·马尔堡（Willshaw D. J., Von Der Malsburg C., How patterned
    neural connections can be set up by self-organization, Proceedings of the Royal
    Society of London, B/194, N. 1117*）提出的，用于模拟动物中观察到的不同神经生物学现象。特别是，他们发现大脑的一些区域发展出具有不同区域的结构，每个区域对特定输入模式都有高度敏感性。这种行为的背后过程与我们迄今为止讨论的内容截然不同，因为它基于被称为**胜者全得**原则的神经单元之间的竞争。在训练期间，所有单元都受到相同信号的刺激，但只有一个会产生最高的响应。这个单元将自动成为该特定模式的感受野。我们将要介绍的特定模型是由**科霍恩**（在论文*Kohonen
    T., Self-organized formation of topologically correct feature maps, Biological
    Cybernetics, 43/1*）提出的，并以他的名字命名。'
- en: 'The main idea is to implement a gradual winner-takes-all paradigm, to avoid
    the premature convergence of a neuron (as a definitive winner) and increment the
    level of plasticity of the network. This concept is expressed graphically in the
    following graph (where we are considering a linear sequence of neurons):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思想是实现一个逐渐的胜者全得范式，以避免神经元（作为最终胜者）过早收敛，并增加网络的塑性水平。这个概念在以下图表中得到了图形化表达（我们考虑的是神经元的一个线性序列）：
- en: '![](img/57a73e57-00b6-4470-b100-4cfb3fa1ac95.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/57a73e57-00b6-4470-b100-4cfb3fa1ac95.png)'
- en: Mexican-hat dynamic implemented by a Kohonen network
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 科霍恩网络实现的墨西哥帽动态
- en: In this case, the same pattern is presented to all the neurons. At the beginning
    of the training process (**t=0**), a positive response is observed in **x[i-2]**
    to **x[i+2]** with a peak in **x[i]**. The potential winner is obviously **x[i]**,
    but all these units are potentiated according to their distance from **x[i]**.
    In other words, the network (which is trained sequentially) is still receptive
    to change if other patterns produce a stronger activation. If instead **x[i]**
    keeps on being the winner, the radius is slightly reduced, until the only potentiated
    unit will be **x[i]**. Considering the shape of this function, this dynamic is
    often called *Mexican Hat*. With this approach, the network remains plastic until
    all the patterns have been repeatedly presented. If, for example, another pattern
    elicits a stronger response in **x[i]**, it's important that its activation is
    still not too high, to allow a fast reconfiguration of the network. At the same
    time, the new winner will probably be a neighbor of **x[i]**, which has received
    a partial potentiation and can easily take the place of **x[i]**.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，相同的模式被呈现给所有神经元。在训练过程的开始（**t=0**），观察到**x[i-2]**到**x[i+2]**之间的正响应，在**x[i]**处达到峰值。显然，潜在的胜者是**x[i]**，但所有这些单元都会根据它们与**x[i]**的距离被增强。换句话说，这个网络（按顺序训练）仍然对变化保持敏感，如果其他模式产生更强的激活。如果相反**x[i]**继续是胜者，半径会略微减小，直到只有增强的单元将是**x[i]**。考虑到这个函数的形状，这种动态通常被称为*墨西哥帽*。通过这种方法，网络保持可塑性，直到所有模式都被反复呈现。例如，如果另一个模式在**x[i]**处引起更强的响应，那么它的激活仍然不要太高，以便网络能够快速重新配置。同时，新的胜者很可能是**x[i]**的邻居，它已经接受了部分增强，可以轻易地取代**x[i]**。
- en: 'A **Kohonen SOM** (also known as Kohonen network or simply Kohonen map) is
    normally represented as a bidimensional map (for example, a square matrix *m ×
    m*, or any other rectangular shape), but 3D surfaces, such as spheres or toruses
    are also possible (the only necessary condition is the existence of a suitable
    metric). In our case, we always refer to a square matrix where each cell is a
    receptive neuron characterized by a synaptic weight *w* with the dimensionality
    of the input patterns:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dedd26ae-e2c3-4165-b46c-c3b3daa5b1e4.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'During both training and working phases, the winning unit is determined according
    to a similarity measure between a sample and each weight vector. The most common
    metric is the Euclidean; hence, if we consider a bidimensional map *W* with a
    shape *(k × p)* so that *W ∈ ℜ^(k × p × n)*, the winning unit (in terms of its
    coordinates) is computed as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5c7c5c2-7139-45ad-a402-832a0386cf87.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'As explained before, it''s important to avoid the premature convergence because
    the complete final configuration could be quite different from the initial one.
    Therefore, the training process is normally subdivided into two different stages.
    During the first one, whose duration is normally about 10-20% of the total number
    of iterations (let''s call this value *t[max]*), the correction is applied to
    the winning unit and its neighbors (computed by adopting a decaying radius). Instead,
    during the second one, the radius is set to 1.0 and the correction is applied
    only to the winning unit. In this way, it''s possible to analyze a larger number
    of possible configurations, automatically selecting the one associated with the
    least error. The neighborhood can have different shapes; it can be square (in
    closed 3D maps, the boundaries don''t exist anymore), or, more easily, it''s possible
    to employ a radial basis function based on an exponentially decaying distance-weight:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7966bd27-723f-4813-87ac-1097856d8226.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: The relative weight of each neuron is determined by the *σ(t)*. *σ[0 ]*function is
    the initial radius and *τ* is a time-constant that must be considered as a hyperparameter
    which determines the slope of the decaying weight. Suitable values are 5-10% of
    the total number of iterations. Adopting a radial basis function, it's not necessary
    to compute an actual neighborhood because the multiplication factor *n(i, j)*
    becomes close to zero outside of the boundaries. A drawback is related to the
    computational cost, which is higher than a square neighborhood (as the function
    must be computed for the whole map); however, it's possible to speed up the process
    by precomputing all the squared distances (the numerator) and exploiting the vectorization
    features offered by packages such as NumPy (a single exponential is computed every
    time).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The update rule is very simple and it''s based on the idea to move the winning
    unit synaptic weights closer to the pattern, *x[i,]* (repeated for the whole dataset,
    *X*):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76b3c98b-fae1-47ca-a47f-68a63228bea4.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'The *η(t)* function is the learning rate, which can be fixed, but it''s preferable
    to start with a higher value, *η[0 ]*and let it decay to a target final value,
    *η[∞]*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59cd23a7-45d4-465e-a887-f2d7efc46a0a.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: In this way, the initial changes force the weights to align with the input patterns,
    while all the subsequent updates allow slight modifications to improve the overall
    accuracy. Therefore, each update is proportional to the learning rate, the neighborhood
    weighted distance, and the difference between each pattern and the synaptic vector.
    Theoretically, if *Δw[ij]* is equal to 0.0 for the winning unit, it means that
    a neuron has become the attractor of a specific input pattern, and its neighbors
    will be receptive to noisy/altered versions. The most interesting aspect is that
    the complete final map will contain the attractors for all patterns which are
    organized to maximize the similarity between adjacent units. In this way, when
    a new pattern is presented, the area of neurons that maps the most similar shapes
    will show a higher response. For example, if the patterns are made up of handwritten
    digits, attractors for the digit 1 and for digit 7 will be closer than the attractor,
    for example, for digit 8\. A malformed 1 (which could be interpreted as 7) will
    elicit a response that is between the first two attractors, allowing us to assign
    a relative probability based on the distance. As we're going to see in the example,
    this feature yields to a smooth transition between different variants of the same
    pattern class avoiding rigid boundaries that oblige a binary decision (like in
    a K-means clustering or in a hard classifier).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete Kohonen SOM algorithm is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initialize *W^((0))*. The shape is *(k × p × n)*.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize `nb_iterations`, the total number of iterations, and *t[max]* (for
    example, `nb_iterations` = 1000 and t[max ]= 150).
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *τ* (for example, *τ* = 100).
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *η[0]* and *η[∞]* (for example, *η*[*0* ]= 1.0 and *η*[*∞* ]= 0.05).
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For `t = 0` to `nb_iterations`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *t < t[max]*:'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *η(t)*
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *σ(t)*
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Otherwise:'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *η(t) = η[∞]*
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *σ(t) = σ[∞]*
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each *x[i]* in *X*:'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the winning unit *u^** (let's assume that the coordinates are *i*, *j*)
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *n(i, j)*
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the weight correction *Δw[ij]^((t))* to all synaptic weights *W^((t))*
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Renormalize *W^((t)) = W^((t))** / ||W^((t))||^((columns))* (the norm must be
    computed column-wise)
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of SOM
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now implement an SOM using the Olivetti faces dataset. As the process
    can be very long, in this example we limit the number of input patterns to 100
    (with a 5 × 5 matrix). The reader can try with the whole dataset and a larger
    map.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is loading the data, normalizing it so that all values are bounded
    between 0.0 and 1.0, and setting the constants:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point, we can initialize the weight matrix using a normal distribution
    with a small standard deviation:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we need to define the functions to determine the winning unit based on
    the least distance:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义函数来确定基于最小距离的获胜单元：
- en: '[PRE11]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It''s also useful to define the functions *η(t)* and *σ(t)*:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 定义函数 *η(t)* 和 *σ(t)* 也是有用的：
- en: '[PRE12]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As explained before, instead of computing the radial basis function for each
    unit, it''s preferable to use a precomputed distance matrix (in this case, 5 ×
    5 × 5 × 5) containing all the possible distances between couples of units. In
    this way, NumPy allows a faster calculation thanks to its vectorization features:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，与其为每个单元计算径向基函数，不如使用预先计算的包含所有可能距离的单元对的距离矩阵（在这种情况下，5 × 5 × 5 × 5）。这样，NumPy
    通过其向量化功能允许更快地计算：
- en: '[PRE13]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `distance_matrix` function returns the value of the radial basis function
    for the whole map given the center point (the winning unit) `xt, yt` and the current
    value of *σ* `sigmat`. Now, it''s possible to start the training process (in order
    to avoid correlations, it''s preferable to shuffle the input sequence at the beginning
    of each iteration):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`distance_matrix` 函数返回整个映射中径向基函数的值，给定中心点（获胜单元） `xt, yt` 和 *σ* 的当前值 `sigmat`。现在，可以开始训练过程（为了避免相关性，最好在每个迭代的开始时对输入序列进行洗牌）：'
- en: '[PRE14]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In this case, we have set *η*[*∞* ]= `0.2` but I invite the reader to try different
    values and evaluate the final result. After training for `5000` epochs, we got
    the following weight matrix (each weight is plotted as a bidimensional array):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将 *η*[*∞*] 设置为 `0.2`，但我邀请读者尝试不同的值并评估最终结果。经过 `5000` 个周期的训练后，我们得到了以下权重矩阵（每个权重都绘制为二维数组）：
- en: '![](img/c2b13744-1295-477f-a34a-6f447eb97f70.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2b13744-1295-477f-a34a-6f447eb97f70.png)'
- en: As it's possible to see, the weights have converged to faces with slightly different
    features. In particular, looking at the shapes of the faces and the expressions,
    it's easy to notice the transition between different attractors (some faces are
    smiling, while others are more serious; some have glasses, mustaches, and beards,
    and so on). It's also important to consider that the matrix is larger than the
    minimum capacity (there are ten different individuals in the dataset). This allows
    mapping more patterns that cannot be easily attracted by the right neuron. For
    example, an individual can have pictures with and without a beard and this can
    lead to confusion. If the matrix is too small, it's possible to observe an instability
    in the convergence process, while if it's too large, it's easy to see redundancies.
    The right choice depends on each different dataset and on the internal variance
    and there's no way to define a standard criterion. A good starting point is picking
    a matrix whose capacity is between 2.0 and 3.0 times larger than the number of
    desired attractors and then increasing or reducing its size until the accuracy
    reaches a maximum. The last element to consider is the labeling phase. At the
    end of the training process, we have no knowledge about the weight distribution
    in terms of winning neurons, so it's necessary to process the dataset and annotate
    the winning unit for each pattern. In this way, it's possible to submit new patterns
    to get the most likely label. This process has not been shown, but it's straightforward
    and the reader can easily implement it for every different scenario.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，权重已经收敛到具有略微不同特征的脸上。特别是，观察脸的形状和表情，很容易注意到不同吸引子之间的过渡（有些脸在笑，而有些则更严肃；有些人戴眼镜，有胡须和胡须，等等）。还重要的是要考虑矩阵的容量大于最小容量（数据集中有十个不同的个体）。这允许映射更多不能轻易被正确神经元吸引的图案。例如，一个人可以有带胡须和不带胡须的照片，这可能会导致混淆。如果矩阵太小，可能会观察到收敛过程中的不稳定性，而如果太大，则很容易看到冗余。正确的选择取决于每个不同的数据集以及内部方差，而且无法定义一个标准标准。一个好的起点是选择一个容量是所需吸引子数量的2.0到3.0倍以上的矩阵，然后增加或减少其大小，直到准确性达到最大。最后要考虑的是标记阶段。在训练过程的最后，我们对获胜神经元的权重分布一无所知，因此有必要处理数据集并为每个模式标记获胜单元。这样，就可以提交新的模式以获得最可能的标签。这个过程尚未展示，但它很简单，读者可以轻松地为每种不同场景实现它。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed Hebb's rule, showing how it can drive the
    computation of the first principal component of the input dataset. We have also
    seen that this rule is unstable because it leads to the infinite growth of the
    synaptic weights and how it's possible to solve this problem using normalization
    or Oja's rule.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了赫布规则，展示了它是如何驱动输入数据集的第一主成分的计算的。我们还看到，这个规则是不稳定的，因为它会导致突触权重的无限增长，以及如何通过归一化或奥贾规则来解决这个问题。
- en: We have introduced two different neural networks based on Hebbian learning (Sanger's
    and Rubner-Tavan's networks), whose internal dynamics are slightly different,
    which are able to extract the first *n* principal components in the right order
    (starting from the largest eigenvalue) without eigendecomposing the input covariance
    matrix.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了两种基于赫布学习（桑格的和鲁布纳-塔万网络）的不同神经网络，它们的内部动力学略有不同，能够按正确顺序（从最大的特征值开始）提取前*n*个主成分，而无需对输入协方差矩阵进行特征值分解。
- en: Finally, we have introduced the concept of SOM and presented a model called
    a Kohonen network, which is able to map the input patterns onto a surface where
    some attractors (one per class) are placed through a competitive learning process.
    Such a model is able to recognize new patterns (belonging to the same distribution)
    by eliciting a strong response in the attractor, that is most similar to the pattern.
    In this way, after a labeling process, the model can be employed as a soft classifier
    that can easily manage noisy or altered patterns.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了SOM的概念，并介绍了一个称为 kohonen 网络的模型，该模型能够通过竞争学习过程将输入模式映射到一个表面上，在该表面上放置了一些吸引子（每个类别一个）。这样的模型能够通过在吸引子中引发与模式最相似的强烈反应来识别新的模式（属于同一分布）。通过这种方式，在标记过程之后，该模型可以作为软分类器使用，可以轻松地处理噪声或改变的模式。
- en: In the next chapter, we're going to discuss some important clustering algorithms,
    focusing on the difference (already discussed in the previous chapters) between
    hard and soft clustering and discussing the main techniques employed to evaluate
    the performance of an algorithm.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论一些重要的聚类算法，重点关注硬聚类和软聚类之间的差异（已在上一章中讨论），并讨论评估算法性能的主要技术。
