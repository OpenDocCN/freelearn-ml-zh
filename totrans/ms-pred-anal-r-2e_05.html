<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;5.&#xA0;Neural Networks" id="1AT9A1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Neural Networks</h1></div></div></div><p class="calibre8">So far, we've looked at two of the most well-known methods used for predictive modeling. Linear regression is probably the most typical starting point for problems where the goal is to predict a numerical quantity. The model is based on a linear combination of input features. Logistic regression uses a nonlinear transformation of this linear feature combination in order to restrict the range of the output in the interval [0,1]. In so doing, it predicts the probability that the output belongs to one of two classes. Thus, it is a very well-known technique for classification.</p><p class="calibre8">Both methods share the disadvantage that they are not robust when dealing with many input features. In addition, logistic regression is typically used for binary classification problems. In this chapter, we will introduce the concept of <span class="strong"><strong class="calibre2">neural networks</strong></span>, a nonlinear approach <a id="id384" class="calibre1"/>to solving both regression and classification problems. They are significantly more robust when dealing with a higher dimensional input feature space, and for classification, they possess a natural way to handle more than two output classes.</p><p class="calibre8">Neural networks are a biologically inspired model, the origins of which date back to the 1940s. Interest in neural networks has fluctuated greatly over the years as the first models proved to be quite limited compared to the expectations at the time. Additionally, training a large <a id="id385" class="calibre1"/>neural network requires substantial computational resources. Recently, there has been a huge surge in interest in neural networks as distributed on-demand computing resources are now widespread and an important area of machine learning, known as <span class="strong"><strong class="calibre2">deep learning</strong></span>, is already showing great promise. For this reason, it is a great time to be learning about this type of model.</p></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Neural Networks" id="1AT9A1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="The biological neuron"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec38" class="calibre1"/>The biological neuron</h1></div></div></div><p class="calibre8">Neural <a id="id386" class="calibre1"/>network models draw their analogy from the organization of <a id="id387" class="calibre1"/>neurons in the human brain, and for this reason they are also often referred to as <span class="strong"><strong class="calibre2">artificial neural networks</strong></span> (<span class="strong"><strong class="calibre2">ANNs</strong></span>) to distinguish them from their biological counterparts. The key parallel is that a single biological neuron acts as a simple computational unit, but when a large number of these are combined together, the result is an extremely powerful and massively distributed processing machine capable of complex learning, known more commonly as the human brain. To get an idea of how neurons are connected in the brain, the following image shows a <a id="id388" class="calibre1"/>simplified picture of a human neural cell:</p><div class="mediaobject"><img src="../images/00079.jpeg" alt="The biological neuron" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In a nutshell, we can think of a human neuron as a computational unit that takes in a series <a id="id389" class="calibre1"/>of parallel electrical signal inputs known as <span class="strong"><strong class="calibre2">synaptic neurotransmitters</strong></span> coming in from the <span class="strong"><strong class="calibre2">dendrites</strong></span>. The dendrites transmit signal chemicals to <a id="id390" class="calibre1"/>the <span class="strong"><strong class="calibre2">soma</strong></span> or body of the neuron in response to the received <a id="id391" class="calibre1"/>synaptic neurotransmitters. This conversion of an external input signal to a local signal can be thought of as a process in which the dendrites apply a <span class="strong"><strong class="calibre2">weight</strong></span> (which can be negative or positive depending on whether the chemicals produced are <span class="strong"><strong class="calibre2">inhibitors</strong></span> or <span class="strong"><strong class="calibre2">activators,</strong></span> respectively) to their inputs.</p><p class="calibre8">The soma <a id="id392" class="calibre1"/>of the neuron, which houses the <span class="strong"><strong class="calibre2">nucleus</strong></span> or central processor, mixes <a id="id393" class="calibre1"/>these input signals in a process that can be thought of as summing <a id="id394" class="calibre1"/>up all the signals. Consequently, the original dendrite inputs are <a id="id395" class="calibre1"/>basically transformed into a single linear weighted sum. This sum is sent to the <span class="strong"><strong class="calibre2">axon</strong></span> of the neuron, which is the transmitter of the neuron. The weighted sum of electrical <a id="id396" class="calibre1"/>inputs creates an electric potential in the neuron, and this potential is processed in the axon by means of an <span class="strong"><strong class="calibre2">activation function</strong></span>, which determines whether the neuron will fire.</p><p class="calibre8">Typically, the activation function is modeled as a switch that requires a minimum electrical <a id="id397" class="calibre1"/>potential, known as the <span class="strong"><strong class="calibre2">bias</strong></span>, to be reached before it is turned on. Thus, the activation function <a id="id398" class="calibre1"/>essentially determines whether the neuron will output an electrical signal or not, and if so, the signal is transported through the axon <a id="id399" class="calibre1"/>and propagated to other neurons through the <span class="strong"><strong class="calibre2">axon terminals</strong></span>. These, in turn, connect to the dendrites of neighboring neurons and the electrical signal output becomes an input to subsequent neural processing.</p><p class="calibre8">This description is, of course, a simplification of what happens in our neurons, but the goal here is to explain what aspects of the biological process have been used to inspire the computational model of a neural network.</p></div></div>
<div class="book" title="The artificial neuron" id="1BRPS1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec39" class="calibre1"/>The artificial neuron</h1></div></div></div><p class="calibre8">Using our <a id="id400" class="calibre1"/>biological analogy, we can construct a model of a computational neuron, and this <a id="id401" class="calibre1"/>model is known as the <span class="strong"><strong class="calibre2">McCulloch-Pitts model </strong></span>of a neuron:</p><div class="mediaobject"><img src="../images/00080.jpeg" alt="The artificial neuron" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note18" class="calibre1"/>Note</h3><p class="calibre8">
<span class="strong"><em class="calibre9">Warren McCulloch</em></span> and <span class="strong"><em class="calibre9">Walter Pitts</em></span> proposed this model of a neural network as a computing machine in a paper titled <span class="strong"><em class="calibre9">A logical calculus of the ideas immanent in nervous activity</em></span>, published by the <span class="strong"><em class="calibre9">Bulletin of Mathematical Biophysics</em></span> in 1943.</p></div><p class="calibre8">This computational neuron is the simplest example of a neural network. We can construct the output function, <span class="strong"><em class="calibre9">y</em></span>, of our neural network directly from following our diagram:</p><div class="mediaobject"><img src="../images/00081.jpeg" alt="The artificial neuron" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The function <code class="email">g()</code> in our neural network is the activation function. Here, the specific activation <a id="id402" class="calibre1"/>function that is chosen is the <span class="strong"><strong class="calibre2">step function</strong></span>:</p><div class="mediaobject"><img src="../images/00082.jpeg" alt="The artificial neuron" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">When the linear weighted sum of inputs exceeds zero, the step function outputs 1, and when it does not, the function outputs -1. It is customary to create a dummy input feature <span class="strong"><em class="calibre9">x<sub class="calibre14">0</sub></em></span> which is always taken to be 1, in order to merge the bias or threshold <span class="strong"><em class="calibre9">w<sub class="calibre14">0</sub></em></span> into the main sum as follows:</p><div class="mediaobject"><img src="../images/00083.jpeg" alt="The artificial neuron" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Using our <a id="id403" class="calibre1"/>experience with logistic regression, it should be very easy to conclude that we could construct a simple classifier using this setup for the binary classification problem. The only difference is that in logistic regression, we would choose the logistic function as the activation function. In fact, in 1957, <span class="strong"><em class="calibre9">Frank Rosenblatt</em></span> proposed a supervised learning algorithm for training the <span class="strong"><em class="calibre9">McCulloch-Pitts</em></span> model of neurons to perform <a id="id404" class="calibre1"/>binary classification, and this algorithm along with the learning model produced is known as the <span class="strong"><strong class="calibre2">Rosenblatt perceptron</strong></span>.</p><p class="calibre8">We've thus far presented linear and logistic regression as models that can solve supervised learning problems and showed the criteria that are used to train them without actually going into the optimization details of the training algorithms involved. This was done intentionally to allow us to focus our attention on understanding the models themselves, and how to apply them in R.</p><p class="calibre8">Now that we have built up some experience with classification and regression, this chapter is going to be different, in that we will look at some of the details of how predictive models are trained, as this too is an important process that adds to our overall understanding of a model. In addition, neural networks differ substantially from previous models we have seen so far, in that training a neural network is often more time consuming and involves adjusting a number of parameters, many of which arise from the optimization procedure itself. Thus, it helps to understand the role these parameters play during training and how <a id="id405" class="calibre1"/>they can affect the final model.</p><p class="calibre8">Before we present a training algorithm for the perceptron, we'll first have to learn one of the most fundamental techniques used in solving optimization problems.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Stochastic gradient descent"><div class="book" id="1CQAE2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec40" class="calibre1"/>Stochastic gradient descent</h1></div></div></div><p class="calibre8">In the <a id="id406" class="calibre1"/>models we've seen so far, such as linear regression, we've talked about a criterion or objective function that the model must minimize while it <a id="id407" class="calibre1"/>is being trained. This criterion is also sometimes known as the <span class="strong"><strong class="calibre2">cost function</strong></span>. For example, the least squares cost function for a model can be expressed as:</p><div class="mediaobject"><img src="../images/00084.jpeg" alt="Stochastic gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We've added a constant term of ½ in front of this for reasons that will become apparent shortly. We know from basic differentiation that when we are minimizing a function, multiplying the function by a constant factor does not alter the value of the minimum value of the function. In linear regression, just as with our perceptron model, our model's predicted <span class="strong"><img src="../images/00085.jpeg" alt="Stochastic gradient descent" class="calibre26"/></span>are just the sum of a linear weighted combination of the input features. If we assume that our data is fixed and that the weights are variable and must be chosen so as to minimize our criterion, we can treat the cost function as being a function of the weights:</p><div class="mediaobject"><img src="../images/00086.jpeg" alt="Stochastic gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We have used the letter <span class="strong"><em class="calibre9">w</em></span> to represent the model weights here for the more general case, though in linear regression we've seen that it is customary to use the Greek letter <span class="strong"><em class="calibre9">β</em></span> instead. As our model variables are the weights, we can consider that our function is a function of a weight vector <span class="strong"><img src="../images/00087.jpeg" alt="Stochastic gradient descent" class="calibre26"/></span>. To find the minimum of this function, we just need to take the partial derivative of our cost function with respect to this weight vector. For a specific weight <span class="strong"><em class="calibre9">w<sub class="calibre14">k</sub></em></span>, this partial derivative is given by:</p><div class="mediaobject"><img src="../images/00088.jpeg" alt="Stochastic gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that <a id="id408" class="calibre1"/>the coefficient of one half has usefully cancelled out the <span class="strong"><em class="calibre9">2</em></span> from the derivative. We now have three different subscripts, so it is a good idea to take a step back and try to understand this equation. The innermost sum is still computing, which is the model's predicted output. Let's replace this in the equation to simplify things a bit:</p><div class="mediaobject"><img src="../images/00089.jpeg" alt="Stochastic gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now we should be in a better position to understand this equation. It says that the partial derivative of the cost function that we are trying to minimize for a specific weight, <span class="strong"><em class="calibre9">w<sub class="calibre14">k</sub></em></span>, in our model is just the difference between the predicted output of the model and the actual labeled output, multiplied by <span class="strong"><em class="calibre9">x<sub class="calibre14">ik</sub></em></span> (for the <span class="strong"><em class="calibre9">i<sup class="calibre15">th</sup></em></span> observation, the value of the input feature that corresponds to our weight <span class="strong"><em class="calibre9">w<sub class="calibre14">k</sub></em></span>), and averaged over all the <span class="strong"><em class="calibre9">n</em></span> observations in our dataset.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip13" class="calibre1"/>Tip</h3><p class="calibre8">If you are not familiar with partial differentiation, but are familiar with differentiation, you already know everything you need to in order to understand this equation. We use partial differentiation to explicitly identify the variable that we will be differentiating with respect to an equation that has more than one variable. When we do this, we treat all other variables as constants and the differentiation is carried out normally.</p></div><p class="calibre8">To find the optimal weights, we need to solve this equation for every weight in our weight vector. Note that through the predicted output term, all the weights in the model appear in the partial derivative of every individual weight. Put differently, this produces a complete system of linear equations that is often very large, so solving this directly is often prohibitively expensive, computationally speaking.</p><p class="calibre8">Instead, many model implementations use iterative optimization procedures that are designed to <a id="id409" class="calibre1"/>gradually approach the correct solution. One such method is <span class="strong"><strong class="calibre2">gradient descent</strong></span>. For a <a id="id410" class="calibre1"/>particular value of the weight vector, gradient descent finds the direction in which the gradient of the cost function is steepest, and adjusts the weights in that direction by a small amount, which is determined <a id="id411" class="calibre1"/>by a parameter known as the <span class="strong"><strong class="calibre2">learning rate</strong></span>. Thus, the updated equation is:</p><div class="mediaobject"><img src="../images/00090.jpeg" alt="Stochastic gradient descent" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the previous equation, the learning rate is denoted by the Greek letter <span class="strong"><em class="calibre9">η</em></span>. Setting the learning rate to an appropriate value is a very important aspect of optimizing with gradient descent. If we choose a value that is too small, the algorithm will update the weights by a very small amount each time, and thus it will take too long to finish. If we use a value that is too large, we may cause the weights to change too drastically, oscillating between values, and so again the learning algorithm will either take too long to converge or oscillate continuously.</p><p class="calibre8">There are various sophisticated methods to estimate an appropriate learning rate, the details of which we won't discuss here. Instead, we'll try to find an appropriate learning rate through trial and error, and this often works just fine in practice. One way to keep track of whether our chosen learning rate is decent is to plot the cost function we are trying to minimize versus time (represented by the number of iterations made through the dataset). We should be seeing a decreasing (or at least non-increasing) change in the cost function over time if we have chosen a good value for the learning rate.</p><p class="calibre8">A variant of the gradient descent method is <span class="strong"><strong class="calibre2">stochastic gradient descent</strong></span>, which does a similar computation, but takes the observations one at a time instead of all together. The key idea is that, on average, the gradient of the cost function computed for a particular observation will equal that of the gradient computed across all observations. This is, of course, an approximation, but it does mean that we can process individual observations one at a time, which is very useful, especially if we want to perform online learning. </p><p class="calibre8">Stochastic gradient descent updates a particular weight, <span class="strong"><em class="calibre9">w<sub class="calibre14">k</sub></em></span>, when processing the <span class="strong"><em class="calibre9">i<sup class="calibre15">th</sup></em></span> observation in the dataset according to the following equation:</p><div class="mediaobject"><img src="../images/00091.jpeg" alt="Stochastic gradient descent" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note19" class="calibre1"/>Note</h3><p class="calibre8">An excellent resource for some of the tricks that are useful when training a model with stochastic gradient descent is a book chapter by <span class="strong"><em class="calibre9">Leo Bottou</em></span>, titled <span class="strong"><em class="calibre9">Stochastic Gradient Descent Tricks</em></span>. A version of this can be found online at <a class="calibre1" href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">http://research.microsoft.com/pubs/192769/tricks-2012.pdf</a>.</p></div></div>

<div class="book" title="Stochastic gradient descent">
<div class="book" title="Gradient descent and local minima"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec50" class="calibre1"/>Gradient descent and local minima</h2></div></div></div><p class="calibre8">Gradient <a id="id412" class="calibre1"/>descent methods rely on the idea <a id="id413" class="calibre1"/>that the cost function that is being <a id="id414" class="calibre1"/>minimized is a <span class="strong"><strong class="calibre2">convex function</strong></span>. We'll skip the mathematical details of this and just say that a convex function is a function that has, at most, a single global minimum. Let's look at an example of a non-convex cost function in terms of a single weight <span class="strong"><em class="calibre9">w</em></span>:</p><div class="mediaobject"><img src="../images/00092.jpeg" alt="Gradient descent and local minima" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The global <a id="id415" class="calibre1"/>minimum of this function is the first trough on the left for a value of <span class="strong"><em class="calibre9">w</em></span>, close to 4.5. If our initial guess for the weight <span class="strong"><em class="calibre9">w</em></span> is 1, the gradient of the cost function points towards the global minimum, and we will <a id="id416" class="calibre1"/>progressively approach it until we reach it. If our initial guess of the weight is 12, then the gradient of the cost function will point downwards towards the trough near the value 10.5. Once we reach the second trough, the gradient of the cost function will be 0 and consequently, we will not be able to make any progress towards our global minimum because we have landed in a local minimum.</p><p class="calibre8">Detecting and avoiding local minima can be very tricky, especially if there are many of them. One way to do this is to repeat the optimization with different starting points and then pick the weights that produce the lowest value of the cost function across the different times the optimization is run. This procedure works well if the number of local minima is small and they are not too close together. Thankfully, the squared error cost function that <a id="id417" class="calibre1"/>we saw in the previous section is a convex function and so gradient descent methods are guaranteed to find the global <a id="id418" class="calibre1"/>minimum, but it is good to be aware that there are other examples of cost functions that we will encounter that are non-convex.</p></div></div>

<div class="book" title="Stochastic gradient descent">
<div class="book" title="The perceptron algorithm"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec51" class="calibre1"/>The perceptron algorithm</h2></div></div></div><p class="calibre8">Without <a id="id419" class="calibre1"/>further ado, we'll present our first training algorithm for classification with neural networks. This is a variation of the <a id="id420" class="calibre1"/>perceptron learning algorithm and is known as the <span class="strong"><strong class="calibre2">pocket perceptron algorithm</strong></span>.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Inputs:</strong></span>
</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">x</code>: A two-dimensional matrix, where the rows are the observations and the columns are the input features.</li><li class="listitem"><code class="email">y</code>: A vector with the class label (-1 or 1) for all the observations in <span class="strong"><em class="calibre9">x</em></span>.</li><li class="listitem"><code class="email">learning_rate</code>: A number that controls the learning rate of the algorithm.</li><li class="listitem"><code class="email">max_iterations</code>: The maximum number of cycles through our data that our algorithm is allowed to perform while learning.</li></ul></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Outputs:</strong></span>
</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">w</code>: The learned weights of the perceptron.</li><li class="listitem"><code class="email">converged</code>: Whether the algorithm converged (true or false).</li><li class="listitem"><code class="email">iterations</code>: The actual number of iterations through the data performed during learning.</li></ul></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Method:</strong></span>
</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Randomly initialize the weights <span class="strong"><em class="calibre9">w</em></span>.</li><li class="listitem" value="2">Select an observation in <span class="strong"><em class="calibre9">x</em></span>, and call it <span class="strong"><em class="calibre9">xi</em></span>.</li><li class="listitem" value="3">
Compute the predicted class,<span class="strong"><img src="../images/00093.jpeg" alt="The perceptron algorithm" class="calibre26"/></span>, using the current values of the weights <span class="strong"><em class="calibre9">w</em></span> and the equation for the output of the perceptron.
</li><li class="listitem" value="4">
If the predicted class,<span class="strong"><img src="../images/00093.jpeg" alt="The perceptron algorithm" class="calibre26"/></span> is not the same as the actual class, <code class="email">yi</code>, then update the weights vector using stochastic gradient descent.
</li><li class="listitem" value="5">Repeat steps 2–4 for all the observations in our dataset and count the number of errors made.</li><li class="listitem" value="6">If the number of errors is zero, we have converged and the algorithm terminates.</li><li class="listitem" value="7">If the number of errors made in the current iteration was less than the lowest numbers of errors ever made, store the weights vector as the best weights vector seen so far.</li><li class="listitem" value="8">If we have reached the maximum number of iterations, stop and return the value of the best weights vector. Otherwise, begin a new iteration over the dataset at step 2.</li></ol><div class="calibre13"/></div><p class="calibre8">We'll see <a id="id421" class="calibre1"/>the R code for this directly and discuss the steps in detail:</p><div class="informalexample"><pre class="programlisting">step_function &lt;- function(x) {
   if (x &lt; 0) -1 else 1
}

pocket_perceptron &lt;- function(x, y, learning_rate, max_iterations) {
  nObs = nrow(x)
  nFeatures = ncol(x)
  w = rnorm(nFeatures + 1, 0, 2) # Random weight initialization
  current_iteration = 0
  has_converged = F
  best_weights = w
  # Start by assuming you get all the examples wrong
  best_error = nObs 
  while ((has_converged == F) &amp;
         (current_iteration &lt; max_iterations)) {
    # Assume we are done unless we misclassify an observation
    has_converged = T 
    # Keep track of misclassified observations
    current_error = 0
    for (i in 1:nObs) {
      xi = c(1, x[i,]) # Append 1 for the dummy input feature x0
      yi = y[i]
      y_predicted = step_function(sum(w * xi))
      if (yi != y_predicted) {
        current_error = current_error + 1
        # We have at least one misclassified example
        has_converged = F 
        w = w - learning_rate * sign(y_predicted - yi) * xi
      }
    }
    if (current_error &lt; best_error) {
      best_error = current_error
      best_weights = w
    }
    current_iteration = current_iteration+1
  }
  model &lt;- list("weights" = best_weights, 
"converged" = has_converged, 
"iterations" = current_iteration)
  model
}</pre></div><p class="calibre8">The first function we define is the step function, which we know will produce either the value <code class="email">-1</code> or the value <code class="email">1</code> corresponding to the two classes in our dataset. We then define our main function, which we call <code class="email">pocket_perceptron()</code>. The job of this function is to learn the weights for our perceptron so that our model classifies our training data correctly.</p><p class="calibre8">Note that <a id="id422" class="calibre1"/>we have not introduced any regularization in our algorithm to keep things simple, and so we will likely end up with a model that will overfit our data, as we are shooting for 100 percent training accuracy. Proceeding with our algorithm description, we begin our function by initializing the weights vector to small randomly generated numbers. In practice, it is a good idea to make sure that weights are not set to <code class="email">0</code> and are not symmetric, and this method is a good way to avoid this.</p><p class="calibre8">We will also set our starting best guess of the weights to be our initial vector and our starting best error rate to be the total number of observations, which is the worst possible error rate on a dataset.</p><p class="calibre8">The main <code class="email">while</code> loop of the function controls the number of iterations over which our algorithm will run. We will only begin a new iteration when we have not converged and when we have not hit our maximum number of iterations. Inside the <code class="email">while</code> loop, we use a <code class="email">for</code> loop to iterate over the observations in our dataset and classify these using the current version of our weight vector.</p><p class="calibre8">Every time we make a mistake in classification, we update our error rate, note that we have not converged in this iteration, and update our weight vector according to the stochastic gradient descent update rule for least squares that we saw in the previous section. Although the cost function for the perceptron is not differentiable because of the step function used to threshold the output, it turns out that we can, in fact, still use the same update rule for the weights.</p><p class="calibre8">At the <a id="id423" class="calibre1"/>end of a complete iteration through our dataset, also known as an <span class="strong"><strong class="calibre2">epoch</strong></span>, we check whether we need to update our best weights vector and update the number of iterations. We update our best weights vector only if the performance in the current iteration on the training data was the best performance we have seen thus far across all completed iterations. When the algorithm terminates, we return the best weights we found, whether or not we converged, and the total number of completed iterations.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note20" class="calibre1"/>Note</h3><p class="calibre8">The definitive textbook on neural networks, and one that explains perceptron learning in more detail, including proof of why the algorithm works, is <span class="strong"><em class="calibre9">Neural Networks and Learning Machines 3rd Edition</em></span>, <span class="strong"><em class="calibre9">Simon Haykin</em></span>, <span class="strong"><em class="calibre9">Prentice Hall.</em></span>
</p></div><p class="calibre8">We can put our model to the test by generating some artificial data. We'll do this by sampling values from two uniform distributions in order to create two input features: <span class="strong"><em class="calibre9">x<sub class="calibre14">1</sub></em></span> and <span class="strong"><em class="calibre9">x<sub class="calibre14">2</sub></em></span>. We'll then separate these data points into two different classes according to a linear decision boundary that we've chosen randomly:</p><div class="mediaobject"><img src="../images/00094.jpeg" alt="The perceptron algorithm" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Once we <a id="id424" class="calibre1"/>have the data and the computed class labels, we can run our perceptron algorithm on it. The following code generates the test data and builds our model:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(4910341)
&gt; x1 &lt;- runif(200, 0, 10)
&gt; set.seed(2125151)
&gt; x2 &lt;- runif(200, 0, 10)
&gt; x &lt;- cbind(x1, x2)
&gt; y &lt;- sign(-0.89 + 2.07 * x[,1] - 3.09 * x[,2])
&gt; pmodel &lt;- pocket_perceptron(x, y, 0.1, 1000)
&gt; pmodel
$weights
                 x1        x2 
-1.738271  4.253327 -6.360326 

$converged
[1] TRUE

$iterations
[1] 32</pre></div><p class="calibre8">We can see that after 32 iterations, our perceptron algorithm has converged. If we divide our weights vector by <code class="email">2</code> (this does not alter our decision boundary), we can see more clearly that we have a decision boundary that is very close to the one that was used when classifying the data:</p><div class="informalexample"><pre class="programlisting">&gt; pmodel$weights / 2
                   x1         x2 
-0.8741571  2.1420697 -3.2122627</pre></div><p class="calibre8">The following plot shows that the model's decision boundary is virtually indistinguishable from the population line. For our artificially generated dataset, this is because the two classes are so close together. If the classes were further apart, we would more likely see a noticeable difference between the population decision boundary and the model's decision boundary. </p><p class="calibre8">This is because the space of possible lines (or planes when we are dealing with more <a id="id425" class="calibre1"/>than two features) that can separate the data would be larger.</p><div class="mediaobject"><img src="../images/00095.jpeg" alt="The perceptron algorithm" class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div class="book" title="Stochastic gradient descent">
<div class="book" title="Linear separation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec52" class="calibre1"/>Linear separation</h2></div></div></div><p class="calibre8">The data <a id="id426" class="calibre1"/>that we generated had a particular property that ensured that the perceptron algorithm would converge--it was <span class="strong"><strong class="calibre2">linearly separable</strong></span>. When two classes are linearly separable in terms of a set of features, it means <a id="id427" class="calibre1"/>that it is possible to find a linear combination of these features as a decision boundary that will allow us to classify the two classes with 100 percent accuracy.</p><p class="calibre8">If we <a id="id428" class="calibre1"/>consider plotting the data points belonging to the two classes in the <span class="strong"><em class="calibre9">p</em></span>-dimensional feature space, then linear separation means that there is a plane (or line for two dimensions, as we saw in our example) that can be drawn to separate the two classes. There is a theorem, known as the <span class="strong"><strong class="calibre2">perceptron convergence theorem</strong></span>, which states that for linearly separable classes, the perceptron learning algorithm will always converge to a solution that correctly classifies all the data given enough time.</p></div></div>

<div class="book" title="Stochastic gradient descent">
<div class="book" title="The logistic neuron"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec53" class="calibre1"/>The logistic neuron</h2></div></div></div><p class="calibre8">The perceptron <a id="id429" class="calibre1"/>is also known as a <span class="strong"><strong class="calibre2">binary threshold neuron</strong></span>. We can <a id="id430" class="calibre1"/>create different types of neurons by changing the activation function. For example, if we remove the threshold function <a id="id431" class="calibre1"/>completely, we end up with a <span class="strong"><strong class="calibre2">linear neuron</strong></span>, which essentially performs the same task as linear regression. By changing the activation function to a <a id="id432" class="calibre1"/>logistic function, we can create a <span class="strong"><strong class="calibre2">logistic neuron</strong></span>.</p><p class="calibre8">A logistic neuron performs the same task as logistic regression, by taking a linear combination of inputs and applying the logistic function to predict a value in the interval [0,1]. Stochastic gradient descent can be applied in order to learn the weights of linear neurons as well as logistic neurons. Hence, it can also be applied to learn the weights for logistic and linear regression. The general form of the stochastic gradient descent weight update rule is:</p><div class="mediaobject"><img src="../images/00096.jpeg" alt="The logistic neuron" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, the derivative is computing the gradient of the cost function at the particular observation. We saw the simple form for linear regression and the linear neuron in the previous section. If we perform differentiation on the cost function for logistic regression, we will discover that the update rule for stochastic gradient descent for the logistic neuron appears to be exactly the same as with the linear neuron:</p><div class="mediaobject"><img src="../images/00097.jpeg" alt="The logistic neuron" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The subtle difference here is that the form of <span class="strong"><img src="../images/00093.jpeg" alt="The logistic neuron" class="calibre26"/></span> is completely different as it now includes the weights inside the logistic function, whereas this was not the case in linear regression. Logistic <a id="id433" class="calibre1"/>neurons are very important because they are the most common type of neuron used when building networks of many neurons <a id="id434" class="calibre1"/>connected together. As we'll see in the next section, we generally build neural networks in layers. The layer containing the neurons that produce our <a id="id435" class="calibre1"/>outputs is known as the <span class="strong"><strong class="calibre2">output layer</strong></span>. The <span class="strong"><strong class="calibre2">input layer</strong></span> is comprised of our data features that are the inputs to network.</p><p class="calibre8">Layers in <a id="id436" class="calibre1"/>between the input and output layers are known as <span class="strong"><strong class="calibre2">hidden layers</strong></span>. Logistic neurons are the most common hidden layer neuron. Additionally, we use logistic neurons as output layer neurons when our task is classification, and linear neurons when our task is regression.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Multilayer perceptron networks" id="1DOR01-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec41" class="calibre1"/>Multilayer perceptron networks</h1></div></div></div><p class="calibre8">Multilayer <a id="id437" class="calibre1"/>neural networks are models that chain many neurons in order to create a neural architecture. Individually, neurons are very basic units, but when organized together, we can create a model significantly more powerful than the individual neurons.</p><p class="calibre8">As touched <a id="id438" class="calibre1"/>upon in the previous section, we build neural networks in layers and we distinguish between different kinds of neural networks primarily on the basis of the connections that exist between these layers and the types of neurons used. The following diagram shows the general structure of a <span class="strong"><strong class="calibre2">multilayer perceptron</strong></span> (<span class="strong"><strong class="calibre2">MLP</strong></span>) neural network, shown here for two hidden layers:</p><div class="mediaobject"><img src="../images/00098.jpeg" alt="Multilayer perceptron networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first <a id="id439" class="calibre1"/>characteristic of the MLP network is that the information flows in a single direction from input layer to output layer. Thus, it is known <a id="id440" class="calibre1"/>as a <span class="strong"><strong class="calibre2">feedforward neural network</strong></span>. This is in contrast to other neural network types, in which there are cycles that allow information to flow <a id="id441" class="calibre1"/>back to earlier neurons in the network as a feedback signal. These networks are known as <span class="strong"><strong class="calibre2">feedback neural networks</strong></span> or <span class="strong"><strong class="calibre2">recurrent neural</strong></span> networks. Recurrent neural networks are generally very difficult to train and often do not scale well with the number of inputs. Nonetheless, they do find a number of applications, in particular with problems involving a time component such as forecasting and signal processing.</p><p class="calibre8">Returning to the MLP architecture shown in the diagram, we note that the first group of neurons on the left are known as the input neurons and form the input layer. We always have as many input neurons as there are input features. The input neurons are said to produce the values of our input features as outputs. For this reason, we often don't refer to them as input neurons, but rather as input sources or input nodes. At the far right of the diagram, we have the output layer with the output neurons. We usually have as many output neurons as outputs that we are modeling. Thus, our neural network can naturally learn to predict more than one thing at a time. One exception to this rule is that when we are modeling a multiclass classification problem, we usually have one binary output neuron for every class. In this case, all the output neurons are a dummy encoding of a single multiclass factor output.</p><p class="calibre8">Between the input and output layers, we have the hidden layers. Neurons are organized into layers depending on how many neurons are between them and an input neuron. For example, neurons in the first hidden layer are directly connected to at least one neuron in the input layer, whereas neurons in the second hidden layer are directly connected to one or more neurons in the first hidden layer. Our diagram is an example of a 4-4 architecture, which means that there are two hidden layers with four neurons each. Even though they are not neurons themselves, the diagram explicitly shows the bias units for all the neurons. We saw in our equation for the output of a single neuron that we can treat the bias unit as a dummy input feature with a value of 1 that has a weight on it that corresponds to the bias or threshold.</p><p class="calibre8">Not all <a id="id442" class="calibre1"/>the neurons in the architecture are assumed to have the same activation function. In general, we pick the activation function for the neurons in the hidden layers separately from that of the output layer. The activation function for the output layer we've already seen is chosen based on what type of output we would like, which in turn depends on whether we are performing regression or classification.</p><p class="calibre8">The activation function for the hidden layer neurons is generally nonlinear, because chaining together linear neurons can be algebraically simplified to a single linear neuron with different weights and so this does not add any power to the network. The most common activation function is the logistic function, but others such as the hyperbolic tangent function are also used.</p><p class="calibre8">The output of the neural network can be calculated by successively computing the outputs of the neurons of each layer. The output of the units of the first hidden layer can be computed using the equations for the output of a neuron that we have seen thus far. These outputs become inputs to the neurons of the second hidden layer and thus, are effectively the new features with respect to that layer.</p><p class="calibre8">One of the strengths of neural networks is this power to learn new features through the learning <a id="id443" class="calibre1"/>of weights in the hidden layers. This process repeats for every layer in the neural network until the final layer, where we obtain the output of the neural network as a whole. This process of propagating the signals from the input to the output layer is known as <span class="strong"><strong class="calibre2">forward propagation</strong></span>.</p></div>

<div class="book" title="Multilayer perceptron networks" id="1DOR01-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Training multilayer perceptron networks"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec54" class="calibre1"/>Training multilayer perceptron networks</h2></div></div></div><p class="calibre8">Multilayer <a id="id444" class="calibre1"/>perceptron networks are more complicated to train than a single perceptron. The famous algorithm used to train them--which has <a id="id445" class="calibre1"/>been around since the 1980s--is known as the <span class="strong"><strong class="calibre2">backpropagation algorithm</strong></span>. We'll give a sketch of how this algorithm works here, but the reader interested in neural networks is strongly encouraged to read up on this algorithm in more depth.</p><p class="calibre8">There are two very important insights to understand this algorithm. The first is that for every observation, it proceeds in two steps. The forward propagation step begins at the input layer and ends at the output layer, and computes the predicted output of the network for this observation. This is relatively straightforward to do using the equation for the output of each neuron, which is just the application of its activation function on the linear weighted sum of its inputs.</p><p class="calibre8">The backward propagation step is designed to modify the weights of the network when the predicted output does not match the desired output. This step begins at the output layer, computing the error on the output nodes and the necessary updates to the weights of the output neurons. Then, it moves backwards through the network, updating the weights of each hidden layer in reverse until it reaches the first hidden layer, which is processed last. Thus, there is a forward pass through the network, followed by a backward pass.</p><p class="calibre8">The second <a id="id446" class="calibre1"/>important insight to understand is that updating the weights of the neurons in the hidden layer is substantially trickier than updating the weights in the output layer. To see this, consider that when we want to update the weights of neurons in the output layer, we know precisely what the desired output for that neuron should be for a given input. </p><p class="calibre8">This is because the desired outputs of the output neurons are the outputs of the network itself, which are available to us in our training data. By contrast, at first glance, we don't actually know what the right output of a neuron in a hidden layer should be for a particular input. Additionally, this output is distributed to all the neurons of the next layer in the network and hence impacts all of their outputs as well.</p><p class="calibre8">The key insight here is that we propagate the error made in the output neurons back to the neurons in the hidden layers. We do this by finding the gradient of the cost function to adjust the weights of the neurons in the direction of the greatest error reduction and apply the chain rule of differentiation to express this gradient in terms of the output of the individual neuron we are interested in. This process results in a general formula for updating the weights <a id="id447" class="calibre1"/>of any neuron in the network, known as the <span class="strong"><strong class="calibre2">delta update rule</strong></span>:</p><div class="mediaobject"><img src="../images/00099.jpeg" alt="Training multilayer perceptron networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Let's understand this equation by assuming that we are currently processing the weights for all the neurons in layer l. This equation tells us how to update the weight between the <span class="strong"><em class="calibre9">j<sup class="calibre15">th</sup></em></span> neuron in layer l and the <span class="strong"><em class="calibre9">i<sup class="calibre15">th</sup></em></span> neuron in the layer before it (layer l-1). The (<span class="strong"><em class="calibre9">n</em></span>) superscripts all denote the fact that we are currently updating the weight as a result of processing the <span class="strong"><em class="calibre9">n<sup class="calibre15">th</sup></em></span> observation in our dataset. We will drop these from now on, and assume they are implied.</p><p class="calibre8">In a nutshell, the delta rule tells us that to obtain the new value of the neuron weight; we must add a product of three terms to the old value. The first of these terms is the learning rate <span class="strong"><em class="calibre9">η</em></span>. The second is known as the local gradient, <span class="strong"><em class="calibre9">δ<sub class="calibre14">j</sub></em></span>, and is the product of the error, <span class="strong"><em class="calibre9">e<sub class="calibre14">j</sub></em></span>, of neuron <span class="strong"><em class="calibre9">j</em></span> and the gradient of its activation function, <span class="strong"><em class="calibre9">g()</em></span>:</p><div class="mediaobject"><img src="../images/00100.jpeg" alt="Training multilayer perceptron networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, we denote the output of neuron <span class="strong"><em class="calibre9">j</em></span> before applying its activation function by <span class="strong"><em class="calibre9">z<sub class="calibre14">j</sub></em></span>, so that the following relation holds:</p><div class="mediaobject"><img src="../images/00101.jpeg" alt="Training multilayer perceptron networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">It turns <a id="id448" class="calibre1"/>out that the local gradient is also the gradient of the cost function of the network computed with respect to <span class="strong"><em class="calibre9">z<sub class="calibre14">j</sub></em></span>. Finally, the third term in the delta update rule is the input to neuron <span class="strong"><em class="calibre9">j</em></span> from neuron <span class="strong"><em class="calibre9">i</em></span>, which is just the output of neuron <span class="strong"><em class="calibre9">i</em></span>, <span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span>. The only term that differs between output layer neurons and hidden layer neurons is the local gradient term. We'll see an illustrative example for neural networks that perform classification using logistic neurons throughout. When neuron <span class="strong"><em class="calibre9">j</em></span> is an output neuron, the local gradient is given by:</p><div class="mediaobject"><img src="../images/00102.jpeg" alt="Training multilayer perceptron networks" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first term in brackets is just the known error of the output neuron, this being the difference between the target output, <span class="strong"><em class="calibre9">t<sub class="calibre14">j</sub></em></span>, and the actual output, <span class="strong"><em class="calibre9">y<sub class="calibre14">j</sub></em></span>. The other two terms arise from the differentiation of the logistic activation function. When neuron <span class="strong"><em class="calibre9">j</em></span> is a hidden layer neuron, the gradient of the logistic activation function is the same, but the error term is computed as the weighted sum of the local gradients of the <span class="strong"><em class="calibre9">k</em></span> neurons in the next layer that receive input from neuron <span class="strong"><em class="calibre9">j</em></span>:</p><div class="mediaobject"><img src="../images/00103.jpeg" alt="Training multilayer perceptron networks" class="calibre10"/></div><p class="calibre11"> </p></div></div>
<div class="book" title="The back propagation algorithm" id="1ENBI1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec42" class="calibre1"/>The back propagation algorithm</h1></div></div></div><p class="calibre8">The backward <a id="id449" class="calibre1"/>propagation of errors, or simply backpropagation, is another somewhat common method for training artificial neural networks and it is used in combination with an optimization method (such as gradient descent, which is described later in this chapter).</p><p class="calibre8">The goal of backpropagation is to <span class="strong"><em class="calibre9">optimize the weights</em></span> so that the neural network model can learn how to correctly map arbitrary inputs to outputs. In other words, when using back propagation, the initial system output is continually compared to the desired output, and the system is adjusted until the difference between the two is minimized.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting the energy efficiency of buildings"><div class="book" id="1FLS42-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec43" class="calibre1"/>Predicting the energy efficiency of buildings</h1></div></div></div><p class="calibre8">In this <a id="id450" class="calibre1"/>section, we will investigate how neural networks can be used to solve a real-world regression problem. Once again, we turn to <a id="id451" class="calibre1"/>the UCI Machine Learning Repository for our dataset. We've chosen to try out the<span class="strong"><em class="calibre9"> energy efficiency dataset</em></span> available at <a class="calibre1" href="http://archive.ics.uci.edu/ml/datasets/Energy+efficiency">http://archive.ics.uci.edu/ml/datasets/Energy+efficiency</a>. The prediction task is to use various building characteristics, such as surface area and roof area, in order to predict the energy efficiency of a building, which is expressed in the form of two different metrics--heating load and cooling load.</p><p class="calibre8">This is a good example for us to try out as we can demonstrate how neural networks can be used to predict two different outputs with a single network. The full attribute description of the dataset is given in the following table:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">relCompactness</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Relative compactness</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">surfArea</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Surface area</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">wallArea</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Wall area</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">roofArea</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Roof area</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">height</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Overall height</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">orientation</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Building orientation (factor)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">glazArea</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Glazing area</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">glazAreaDist</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Glazing area distribution (factor)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">heatLoad</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Heating load (first output)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">coolLoad</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Cooling load (second output)</p>
</td></tr></tbody></table></div><p class="calibre8">The data was generated using a simulator called <span class="strong"><em class="calibre9">Ecotect</em></span>. Each observation in the dataset corresponds to a simulated building. All the buildings have the same volume, but other attributes that impact their energy efficiency, such as their glazing area, are modified.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note21" class="calibre1"/>Note</h3><p class="calibre8">This dataset is described in the paper <span class="strong"><em class="calibre9">Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools</em></span>, <span class="strong"><em class="calibre9">Athanasios Tsanas</em></span> and <span class="strong"><em class="calibre9">Angeliki Xifara</em></span>, published in <span class="strong"><em class="calibre9">Energy and Buildings</em></span>, Vol. 49, in 2012.</p></div><p class="calibre8">The data <a id="id452" class="calibre1"/>on the website comes in Microsoft Excel format. To load this into R, we can use the R package <code class="email">xlsx</code>, which can read and understand Microsoft Excel files:</p><div class="informalexample"><pre class="programlisting">&gt; library(xlsx)
&gt; eneff &lt;- read.xlsx2("ENB2012_data.xlsx", sheetIndex = 1, 
                      colClasses = rep("numeric", 10))
&gt; names(eneff) &lt;- c("relCompactness", "surfArea", "wallArea", "roofArea", "height", "orientation", "glazArea", "glazAreaDist", "heatLoad", "coolLoad")
&gt; eneff &lt;- eneff[complete.cases(eneff),]</pre></div><p class="calibre8">The import adds a number of empty observations at the end of the data frame, so the last line removes these. Now, by referring to the paper in which the dataset was presented, we discover that two of our attributes are actually factors. In order for our neural network to work with these, we will need to convert them into dummy variables. To do this, we will use the <code class="email">dummyVars()</code> function from the <code class="email">caret</code> package:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; eneff$orientation &lt;- factor(eneff$orientation)
&gt; eneff$glazAreaDist &lt;- factor(eneff$glazAreaDist)
&gt; dummies &lt;- dummyVars(heatLoad + coolLoad ~ ., data = eneff)
&gt; eneff_data &lt;- cbind(as.data.frame(predict(dummies, newdata =  
                                            eneff)), eneff[,9:10])
&gt; dim(eneff_data)
[1] 768  18</pre></div><p class="calibre8">The <code class="email">dummyVars()</code> function takes in a formula and a data frame. From these, it identifies the input features and performs dummy encoding on those that are factors in order to produce new binary columns. There are as many columns created for a factor as there are levels in that factor. Just as with the <code class="email">preProcess()</code> function that we've been using, we actually obtain the columns themselves after using the <code class="email">predict()</code> function. Next, we'll do an 80-20 split between the training and test data:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(474576)
&gt; eneff_sampling_vector &lt;- createDataPartition(eneff_data$heatLoad, p 
  = 0.80, list = FALSE)
&gt; eneff_train &lt;- eneff_data[eneff_sampling_vector, 1:16]
&gt; eneff_train_outputs &lt;- eneff_data[eneff_sampling_vector, 17:18]
&gt; eneff_test &lt;- eneff_data[-eneff_sampling_vector, 1:16]
&gt; eneff_test_outputs &lt;- eneff_data[-eneff_sampling_vector, 17:18]</pre></div><p class="calibre8">One of <a id="id453" class="calibre1"/>the most important preprocessing steps to perform when training neural networks is to scale input features and outputs. One good reason to perform input scaling is in order to avoid <span class="strong"><strong class="calibre2">saturation,</strong></span> which occurs <a id="id454" class="calibre1"/>when the optimization procedure reaches a point where the gradient of the error function is very small in absolute value. This is usually the result of very large or very small inputs to the nonlinear neuron activation functions. Saturation causes the optimization procedure to terminate, thinking we have converged.</p><p class="calibre8">Depending on the particular neural network implementation, for regression tasks it may also make sense to scale the outputs as some implementations of linear neurons are designed to produce an output in the interval [-1,1]. Scaling can also help convergence. Consequently, we will use <code class="email">caret</code> to scale all our data dimensions to the unit interval, noting that this has no effect on the binary columns produced earlier:</p><div class="informalexample"><pre class="programlisting">&gt; eneff_pp &lt;- preProcess(eneff_train, method = c("range"))
&gt; eneff_train_pp &lt;- predict(eneff_pp, eneff_train)
&gt; eneff_test_pp &lt;- predict(eneff_pp, eneff_test)

&gt; eneff_train_out_pp &lt;- preProcess(eneff_train_outputs, method = 
                        c("range"))
&gt; eneff_train_outputs_pp &lt;- 
  predict(eneff_train_out_pp, eneff_train_outputs)
&gt; eneff_test_outputs_pp &lt;- 
  predict(eneff_train_out_pp, eneff_test_outputs)</pre></div><p class="calibre8">Several different packages implement neural networks in R, each with their various merits and strengths. For this reason, it helps to be familiar with more than one package and in this chapter we will investigate three of these, the first being <code class="email">neuralnet</code>:</p><div class="informalexample"><pre class="programlisting">&gt; library("neuralnet")
&gt; n &lt;- names(eneff_data)
&gt; f &lt;- as.formula(paste("heatLoad + coolLoad ~", paste(n[!n %in% 
                  c("heatLoad", "coolLoad")], collapse = " + ")))
&gt; eneff_model &lt;- neuralnet(f, 
  data = cbind(eneff_train_pp, eneff_train_outputs_pp), hidden = 10)
&gt; eneff_model
Call: neuralnet(formula = f, data = cbind(eneff_train_pp, eneff_train_outputs_pp),     hidden = 10)

1 repetition was calculated.

         Error Reached Threshold Steps
1 0.3339635783    0.009307995429  9998</pre></div><p class="calibre8">The <code class="email">neuralnet()</code> function trains a neural network based on the information provided in its arguments. The first argument we provide is a formula and the format is similar to the formulae <a id="id455" class="calibre1"/>that we've seen with the <code class="email">lm()</code> and <code class="email">glm()</code> functions in previous chapters. One interesting difference here is that we have specified two outputs, <code class="email">heatLoad</code> and <code class="email">coolLoad</code>. Another difference is that currently we are unable to use the dot (<code class="email">.</code>) notation to imply that all the remaining columns in our data frame can be used as features, so we need to specify them explicitly.</p><p class="calibre8">Note that with the formula, we have effectively defined the input and output layers of the neural network and so what remains to be specified is the structure of the hidden layers. This is specified with the <code class="email">hidden</code> parameter, which either takes in a scalar for a single layer, or a vector of scalars that specify the number of hidden units in each layer, starting from the layer just after the input layer, and ending with the layer just before the output layer.</p><p class="calibre8">In the example we saw earlier, we've used a single layer with 10 nodes. We can actually visualize our neural network as the package provides us with the ability to plot the model directly (the numbered circles are dummy bias neurons):</p><div class="mediaobject"><img src="../images/00104.jpeg" alt="Predicting the energy efficiency of buildings" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The call to <code class="email">neuralnet()</code> also allows us to specify what type of activation function we would like to use for our neurons through the parameter string <code class="email">act.fct</code>. By default, this is set to <a id="id456" class="calibre1"/>the logistic activation function and so we have not changed this. Another very important parameter is <code class="email">linear.output</code>, which can be either <code class="email">TRUE</code> or <code class="email">FALSE</code>. This specifies whether we should apply the activation function to the neurons in the output layer. The default value of <code class="email">TRUE</code> that we used means that we do not apply the activation function and so we can observe a linear output. For regression type problems, this is what is appropriate. This is because; if we were to apply a logistic activation function our output would be bounded in the interval [0,1]. Finally, we can specify a differentiable error function through the <code class="email">err.fct</code> parameter to use as part of our optimization strategy. As we are doing regression, we use the default value of <code class="email">sse</code>, which corresponds to the sum of squared error.</p><p class="calibre8">As there is a random component in neural network training, namely the initialization of the weights, we may want to specify that we should retrain the same model a number of times in order for us to pick the best possible model that we get (using criteria such as the SSE to rank these). This can be done by specifying an integer value for the <code class="email">rep</code> parameter. Let's rewrite our original call to explicitly show the default values we are using:</p><div class="informalexample"><pre class="programlisting">&gt; eneff_model &lt;- neuralnet(f, 
  data = cbind(eneff_train_pp, eneff_train_outputs_pp), hidden = 10, act.fct = "logistic", linear.output = TRUE, err.fct = "sse", rep = 1)</pre></div><p class="calibre8">The model's output provides us with some information about the performance of the neural network and what is shown depends on its configuration. As we have specified the SSE as <a id="id457" class="calibre1"/>our error metric, the error shown is the SSE that was obtained. The threshold figure is just the value of the partial derivative of the error function when the model stopped training. Essentially, instead of terminating when the gradient is 0 exactly, we specify a very small value below which the error gradient needs to fall before the algorithm terminates. The default value for this is 0.01 and it can be changed by supplying a number for the threshold parameter in the <code class="email">neuralnet()</code> function. Reducing this value will generally result in longer training times. The model output also shows us the number of training steps that were performed. Finally, if we had used the <code class="email">rep</code> parameter to repeat this process multiple times, we would see a row for each model trained. Our output shows us that we trained only one model.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip14" class="calibre1"/>Tip</h3><p class="calibre8">As the neural network contains a random component in the form of the initialization of weight vectors, reproducing our code will likely not give the exact same results. If, when running the examples, R outputs a message that the model has not converged, try running the code again.</p></div></div>

<div class="book" title="Predicting the energy efficiency of buildings">
<div class="book" title="Evaluating multilayer perceptrons for regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec55" class="calibre1"/>Evaluating multilayer perceptrons for regression</h2></div></div></div><p class="calibre8">The package <code class="email">neuralnet</code> provides us with a neat way to use our model to perform predictions <a id="id458" class="calibre1"/>through the <code class="email">compute()</code> function. Essentially, it provides us with not only the predicted output for a data frame of observations, but also shows us the output values of all the neurons in the model's architecture. To evaluate the performance of the model, we are interested in the outputs of the neural network on our test set:</p><div class="informalexample"><pre class="programlisting">&gt; test_predictions &lt;- compute(eneff_model, eneff_test_pp)</pre></div><p class="calibre8">We can access the predicted outputs of the neural network using the <code class="email">net.result</code> attribute of the <code class="email">test_predictions</code> object as follows:</p><div class="informalexample"><pre class="programlisting">&gt; head(test_predictions$net.result)
            [,1]          [,2]
7  0.38996108769 0.39770348145
8  0.38508402576 0.46726904682
14 0.29555228848 0.24157156896
21 0.49912349400 0.51244876337
23 0.50036257800 0.47436990729
29 0.01133684342 0.01815294595</pre></div><p class="calibre8">As this is a regression problem, we would like to be able to use the MSE in order to evaluate the performance of our model on both target outputs. In order to do that, we need to transform our predicted outputs back onto their original scale for a fair assessment to be made. The scaling constants we used on our data are stored in the <code class="email">ranges</code> attribute of the <code class="email">eneff_train_out_pp</code> object:</p><div class="informalexample"><pre class="programlisting">&gt; eneff_train_out_pp$ranges
     heatLoad coolLoad
[1,]     6.01    10.90
[2,]    42.96    48.03</pre></div><p class="calibre8">The first <a id="id459" class="calibre1"/>row contains the minimum values of the original data, and the second row contains the maximum values. We'll now write a function that will take in a scaled vector and another vector that contains the original minimum and maximum values, and will return the original unscaled vector:</p><div class="informalexample"><pre class="programlisting">reverse_range_scale &lt;- function(v, ranges) {
     return( (ranges[2] - ranges[1]) * v + ranges[1] )
 }</pre></div><p class="calibre8">Next, we'll use this to obtain the unscaled predicted outputs for our test set:</p><div class="informalexample"><pre class="programlisting">&gt; output_ranges &lt;- eneff_train_out_pp$ranges
&gt; test_predictions_unscaled &lt;- sapply(1:2, function(x) 
  reverse_range_scale(test_predictions[,x], output_ranges[,x]))</pre></div><p class="calibre8">We can also define a simple function to compute the MSE and use it to check the performance on our two tasks:</p><div class="informalexample"><pre class="programlisting">mse &lt;- function(y_p, y) {
  return(mean((y - y_p) ^ 2))
}

&gt; mse(test_predictions_unscaled[,1], eneff_test_outputs[,1])
[1] 0.2940468477
&gt; mse(test_predictions_unscaled[,2], eneff_test_outputs[,2])
[1] 1.440127075</pre></div><p class="calibre8">These values are very low, indicating that we have very good prediction accuracy. We can also investigate correlation, which is scale independent, and we could have used it on the unscaled outputs as well:</p><div class="informalexample"><pre class="programlisting">&gt; cor(test_predictions_unscaled[,1], eneff_test_outputs[,1])
[1] 0.9986655316
&gt; cor(test_predictions_unscaled[,2], eneff_test_outputs[,2])
[1] 0.9926735348</pre></div><p class="calibre8">These <a id="id460" class="calibre1"/>values are extremely high, indicating that we have near-perfect performance, something very rare to see with real-world data. If the accuracy were not this high, we would experiment by making the architecture more complicated. We could, for example, build a model with an additional layer by setting <code class="email">hidden=c(10,5)</code> so that we would have an additional layer of five neurons before the output layer.</p></div></div>
<div class="book" title="Predicting glass type revisited"><div class="book" id="1GKCM2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec44" class="calibre1"/>Predicting glass type revisited</h1></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 3. Linear Regression" href="part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7">Chapter 3</a>, <span class="strong"><em class="calibre9">Linear Regression</em></span>, we analyzed the glass identification dataset, whose task is to <a id="id461" class="calibre1"/>identify the type of glass comprising a glass fragment found at a crime scene. The output of this dataset is a factor with several class levels corresponding to different types of glass. Our previous approach was to build a one-versus-all model using multinomial logistic regression. The results were not very promising, and one of the main points of concern was a poor model fit on the training data.</p><p class="calibre8">In this section, we will revisit this dataset and see whether a neural network model can do better. At the same time, we will demonstrate how neural networks can handle classification problems as well:</p><div class="informalexample"><pre class="programlisting">&gt; glass &lt;- read.csv("glass.data", header = FALSE)
&gt; names(glass) &lt;- c("id", "RI", "Na", "Mg", "Al", "Si", "K", "Ca", 
"Ba", "Fe", "Type")
&gt; glass$id &lt;- NULL</pre></div><p class="calibre8">Our output is a multiclass factor and so we will want to dummy-encode this into binary columns. With the <code class="email">neuralnet</code> package, we would normally need to do this manually as a preprocessing step before we can build our model.</p><p class="calibre8">In this section, we will look at a second package that contains functions for building neural networks, <code class="email">nnet</code>. This is actually the same package that we used for multinomial logistic regression. One of the benefits of this package is that for multiclass classification, the <code class="email">nnet()</code> function that trains the neural network will automatically detect outputs that are factors and perform the dummy encoding for us. With that in mind, we will prepare a training and test set:</p><div class="informalexample"><pre class="programlisting">&gt; glass$Type &lt;- factor(glass$Type)
&gt; set.seed(4365677)
&gt; glass_sampling_vector &lt;- createDataPartition(glass$Type, p = 
                           0.80, list = FALSE)
&gt; glass_train &lt;- glass[glass_sampling_vector,]
&gt; glass_test &lt;- glass[-glass_sampling_vector,]</pre></div><p class="calibre8">Next, just as with our previous dataset, we will normalize our input data:</p><div class="informalexample"><pre class="programlisting">&gt; glass_pp &lt;- preProcess(glass_train[1:9], method = c("range"))
&gt; glass_train &lt;- cbind(predict(glass_pp, glass_train[1:9]), Type = glass_train$Type)
&gt; glass_test  &lt;- cbind(predict(glass_pp, glass_test[1:9]), Type = glass_test$Type)</pre></div><p class="calibre8">We are <a id="id462" class="calibre1"/>now ready to train our model. Whereas the <code class="email">neuralnet</code> package is able to model multiple hidden layers, the <code class="email">nnet</code> package is designed to model neural networks with a single hidden layer. As a result, we still specify a formula as before, but this time, instead of a <code class="email">hidden</code> parameter that can be either a scalar or a vector of integers, we specify a <code class="email">size</code> parameter that is an integer representing the number of nodes in the single hidden layer of our model.</p><p class="calibre8">Also, the default neural network model in the <code class="email">nnet</code> package is for classification, as the output layer uses a logistic activation function. It is really important when working with different packages for training the same type of model, such as multilayer perceptrons, to check the default values for the various model parameters, as these will be different from package to package. One other difference between the two packages that we will mention here is that <code class="email">nnet</code> currently does not offer any plotting capabilities. Without further ado, we will now train our model:</p><div class="informalexample"><pre class="programlisting">&gt; glass_model &lt;- nnet(Type ~ ., data = glass_train, size = 10)
# weights:  166
initial  value 343.685179 
iter  10 value 265.604188
iter  20 value 220.518320
iter  30 value 194.637078
iter  40 value 192.980203
iter  50 value 192.569751
iter  60 value 192.445198
iter  70 value 192.421655
iter  80 value 192.415382
iter  90 value 192.415166
iter 100 value 192.414794
final  value 192.414794 
stopped after 100 iterations</pre></div><p class="calibre8">From the output, we can see that the model has not converged, stopping after the default value of 100 iterations. To converge, we can either rerun this code a number of times or we can increase the number of allowed iterations to 1,000 using the <code class="email">maxit</code> parameter:</p><div class="informalexample"><pre class="programlisting">&gt; glass_model &lt;- nnet(Type ~ ., data = glass_train, size = 10, maxit = 
                      1000)</pre></div><p class="calibre8">Let's first investigate the accuracy of our model on the training data in order to assess the quality of fit. To compute predictions, we use the <code class="email">predict()</code> function and specify the type parameter to be <code class="email">class</code>. This lets the <code class="email">predict()</code> function know that we want the class with highest probability to be selected. If we want to see the probabilities of each class, we can specify the value <code class="email">response</code> for the <code class="email">type</code> parameter. Finally, remember that we must pass in a data frame without the outputs to the <code class="email">predict()</code> function, and thus the need to subset the training data frame:</p><div class="informalexample"><pre class="programlisting">&gt; train_predictions &lt;- predict(glass_model, glass_train[,1:9], 
                               type = "class")
&gt; mean(train_predictions == glass_train$Type)
[1] 0.7183908046</pre></div><p class="calibre8">Our first <a id="id463" class="calibre1"/>attempt shows us that we are getting the same quality of fit as with our multinomial logistic regression model. To improve upon this, we'll increase the complexity of the model by adding more neurons in our hidden layer. We will also increase our <code class="email">maxit</code> parameter to <code class="email">10,000</code> as the model is more complex and might need more iterations to converge:</p><div class="informalexample"><pre class="programlisting">&gt; glass_model2 &lt;- nnet(Type ~ ., data = glass_train, size = 50, maxit = 
                       10000)
&gt; train_predictions2 &lt;- predict(glass_model2, glass_train[,1:9], 
                                type = "class")
&gt; mean(train_predictions2 == glass_train$Type)
[1] 1</pre></div><p class="calibre8">As we can see, we have now achieved 100 percent training accuracy. Now that we have a decent model fit, we can investigate our performance on the test set:</p><div class="informalexample"><pre class="programlisting">&gt; test_predictions2 &lt;- predict(glass_model2, glass_test[,1:9], 
                               type = "class")
&gt; mean(test_predictions2 == glass_test$Type)
[1] 0.6</pre></div><p class="calibre8">Even though our model fits the training data perfectly, we see that the accuracy on the test set is only 60 percent. Even factoring in that the dataset is very small, this discrepancy is a classic signal that our model is overfitting on the training data. When we looked at linear and logistic regression, we saw that there are shrinkage methods, such as the lasso, which are designed to combat overfitting by restricting the size of the coefficients in the model.</p><p class="calibre8">An <a id="id464" class="calibre1"/>analogous technique known as <span class="strong"><strong class="calibre2">weight decay</strong></span> exists for neural networks. With this approach, the product of a decay constant and the sum <a id="id465" class="calibre1"/>of the squares of all the network weights is added to the cost function. This limits any weights from taking overly large values and thus performs regularization on the network. Whereas there is currently no option for regularization with <code class="email">neuralnet()</code>, <code class="email">nnet()</code> uses the <code class="email">decay</code> parameter:</p><div class="informalexample"><pre class="programlisting">&gt; glass_model3 &lt;- nnet(Type~., data = glass_train, size = 10, maxit = 
                       10000, decay = 0.01)
&gt; train_predictions3 &lt;- predict(glass_model3, glass_train[,1:9], 
                                type = "class")
&gt; mean(train_predictions3 == glass_train$Type)
[1] 0.9367816092
&gt; test_predictions3 &lt;- predict(glass_model3, glass_test[,1:9],  
                               type = "class")
&gt; mean(test_predictions3 == glass_test$Type)
[1] 0.775</pre></div><p class="calibre8">With this model, the fit on our training data is still very high, and substantially higher than we achieved with multinomial logistic regression. On the test set, the performance is still worse than on the training set, but much better than we had before.</p><p class="calibre8">We won't spend any more time on the glass identification data. Instead, we will reflect on a few lessons learned before moving on. The first of these is that achieving good performance with a neural network, and sometimes even just reaching convergence, might be tricky. Training the model involves a random initialization of network weights and the final result is often quite sensitive to these starting conditions. We can convince ourselves of this fact by training the different model configurations we have seen so far a number of times and noticing that certain configurations on some runs might not converge, and the performance on our training and test set does tend to differ from one run to the next.</p><p class="calibre8">Another insight is that training a neural network involves tuning a diverse range of parameters, from the number and arrangement of hidden neurons to the value of the <code class="email">decay</code> parameter. Others that we did not experiment with include the choice of nonlinear activation function to use with the hidden layer neurons, the criteria for convergence, and the particular <a id="id466" class="calibre1"/>cost function we use to fit our model. For example, instead of using least squares, we could use a criterion known as <span class="strong"><strong class="calibre2">entropy</strong></span>.</p><p class="calibre8">Before settling on a final choice of model, therefore, it pays to try out as many different combinations of these as possible. A good place to experiment with different parameter combinations is the <code class="email">train()</code> function of the <code class="email">caret</code> package. It provides a unified interface for both neural network packages we have seen and, in conjunction with <code class="email">expand.grid()</code>, allows the simultaneous training and evaluation of several different neural network configurations. We'll provide just a vignette here, and the interested reader can use this <a id="id467" class="calibre1"/>to continue their investigation further:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; nnet_grid &lt;- expand.grid(.decay = c(0.1, 0.01, 0.001, 0.0001), 
                           .size = c(50, 100, 150, 200, 250))
&gt; nnetfit &lt;- train(Type ~ ., data = glass_train, method = "nnet", 
  maxit = 10000, tuneGrid = nnet_grid, trace = F, MaxNWts = 10000)</pre></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting handwritten digits"><div class="book" id="1HIT82-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec45" class="calibre1"/>Predicting handwritten digits</h1></div></div></div><p class="calibre8">Our final <a id="id468" class="calibre1"/>application for neural networks will be the handwritten digit prediction task. In this task, the goal is to build a model that will be presented with an image of a numerical digit (0–9) and the model must predict which digit is <a id="id469" class="calibre1"/>being shown. We will use the <span class="strong"><em class="calibre9">MNIST</em></span> database of handwritten digits from <a class="calibre1" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p><p class="calibre8">From this page, we have downloaded and unzipped the two training files, <code class="email">train-images-idx3-ubyte.gz</code> and <code class="email">train-images-idx3-ubyte.gz</code>. The former contains the data from the images and the latter contains the corresponding digit labels. The advantage of using this website is that the data has already been preprocessed by centering each digit in the image and scaling the digits to a uniform size. To load the data, we've used information from the website about the IDX format to write two functions:</p><div class="informalexample"><pre class="programlisting">read_idx_image_data &lt;- function(image_file_path) {
  con &lt;- file(image_file_path, "rb")
  magic_number &lt;- readBin(con, what = "integer", n = 1, size = 4, 
                          endian = "big")
  n_images &lt;- readBin(con, what = "integer", n = 1, size = 4, 
                      endian="big")
  n_rows &lt;- readBin(con, what = "integer", n = 1, size = 4, 
                    endian = "big")
  n_cols &lt;- readBin(con, what = "integer", n = 1, size = 4, 
                    endian = "big")
  n_pixels &lt;- n_images * n_rows * n_cols
  pixels &lt;- readBin(con, what = "integer", n = n_pixels, size = 1, 
                    signed = F)
  image_data &lt;- matrix(pixels, nrow = n_images, ncol = n_rows * 
                       n_cols, byrow = T)
  close(con)
  return(image_data)
}

read_idx_label_data &lt;- function(label_file_path) {
  con &lt;- file(label_file_path, "rb")
  magic_number &lt;- readBin(con, what = "integer", n = 1, size = 4, 
                          endian = "big")
  n_labels &lt;- readBin(con, what = "integer", n = 1, size = 4, 
                      endian = "big")
  label_data &lt;- readBin(con, what = "integer", n = n_labels, size = 1, 
                        signed = F)
  close(con)
  return(label_data)
}</pre></div><p class="calibre8">We can then load our two data files by issuing the following two commands:</p><div class="informalexample"><pre class="programlisting">&gt; mnist_train &lt;- read_idx_image_data("train-images-idx3-ubyte")
&gt; mnist_train_labels &lt;- read_idx_label_data("train-labels-idx1-
                                            ubyte")
&gt; str(mnist_train)
 int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
&gt; str(mnist_train_labels)
 int [1:60000] 5 0 4 1 9 2 1 3 1 4 ...</pre></div><p class="calibre8">Each image <a id="id470" class="calibre1"/>is represented by a 28-pixel by 28-pixel matrix of grayscale values in the range 0 to 255, where 0 is white and 255 is black. Thus, our observations each have 282 = 784 feature values. Each image is stored as a vector by rasterizing the matrix from right to left and top to bottom. There are 60,000 images in the training data, and our <code class="email">mnist_train</code> object stores these as a matrix of 60,000 rows by 78 columns so that each row corresponds to a single image. To get an idea of what our data looks like, we can visualize the first seven images:</p><div class="mediaobject"><img src="../images/00105.jpeg" alt="Predicting handwritten digits" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To analyze this dataset, we will introduce our third and final R package for training neural network <a id="id471" class="calibre1"/>models, <code class="email">RSNNS</code>. This package is actually an R wrapper around the <span class="strong"><strong class="calibre2">Stuttgart Neural Network Simulator</strong></span> (<span class="strong"><strong class="calibre2">SNNS</strong></span>), a popular software package containing standard implementations of neural networks in C created at the University of Stuttgart.</p><p class="calibre8">The package authors have added a convenient interface for the many functions in the original software. One of the benefits of using this package is that it provides several of its own functions for data processing, such as splitting the data into a training and test set. Another is that it implements many different types of neural networks, not just MLPs. We will begin by normalizing our data to the unit interval by dividing by <code class="email">255</code> and then indicating that our output is a factor with each level corresponding to a digit:</p><div class="informalexample"><pre class="programlisting">&gt; mnist_input &lt;- mnist_train / 255
&gt; mnist_output &lt;- as.factor(mnist_train_labels)</pre></div><p class="calibre8">Although the MNIST website already contains separate files with test data, we have chosen to split the training data file as the models already take quite a while to run. The reader is encouraged to repeat the analysis that follows with the supplied test files as well. To prepare the data for splitting, we will randomly shuffle our images in the training data:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(252)
&gt; mnist_index &lt;- sample(1:nrow(mnist_input), nrow(mnist_input))
&gt; mnist_data &lt;- mnist_input[mnist_index, 1:ncol(mnist_input)]
&gt; mnist_out_shuffled &lt;- mnist_output[mnist_index]</pre></div><p class="calibre8">Next, we must dummy-encode our output factor as this is not done automatically for us. The <code class="email">decodeClassLabels()</code> function from the <code class="email">RSNNS</code> package is a convenient way to do this. Additionally, we will split our shuffled data into an 80–20 training and test set split using <code class="email">splitForTrainingAndTest()</code>. This will store the features and labels for the training and test sets separately, which will be useful for us shortly.</p><p class="calibre8">Finally, we <a id="id472" class="calibre1"/>can also normalize our data using the <code class="email">normTrainingAndTestSet()</code> function. To specify unit interval normalization, we must set the <code class="email">type</code> parameter to <code class="email">0_1</code>:</p><div class="informalexample"><pre class="programlisting">&gt; library("RSNNS")
&gt; mnist_out &lt;- decodeClassLabels(mnist_out_shuffled)
&gt; mnist_split &lt;- splitForTrainingAndTest(mnist_data, mnist_out, 
                                         ratio = 0.2)
&gt; mnist_norm &lt;- normTrainingAndTestSet(mnist_split, type = "0_1")</pre></div><p class="calibre8">For comparison, we will train two MLP networks using the <code class="email">mlp()</code> function. By default, this is configured for classification and uses the logistic function as the activation function for hidden layer neurons. The first model will have a single hidden layer with 100 neurons; the second model will use 300.</p><p class="calibre8">The first argument to the <code class="email">mlp()</code> function is the matrix of input features and the second is the vector of labels. The <code class="email">size</code> parameter plays the same role as the <code class="email">hidden</code> parameter in the <code class="email">neuralnet</code> package. That is to say, we can specify a single integer for a single hidden layer, or a vector of integers specifying the number of hidden neurons per layer when we want more than one hidden layer.</p><p class="calibre8">Next, we can use the <code class="email">inputsTest</code> and <code class="email">targetsTest</code> parameters to specify the features and labels of our test set beforehand, so that we can be ready to observe the performance on our test set in one call. The models we will train will take several hours to run. If we want to know how long each model took to run, we can save the current time using <code class="email">proc.time()</code> before training a model and comparing it against the time when the model completes. Putting all this together, here is how we trained our two MLP models:</p><div class="informalexample"><pre class="programlisting">&gt; start_time &lt;- proc.time()
&gt; mnist_mlp &lt;- mlp(mnist_norm$inputsTrain, mnist_norm$targetsTrain, size = 100, inputsTest = mnist_norm$inputsTest, targetsTest = mnist_norm$targetsTest)
&gt; proc.time() - start_time
    user   system  elapsed 
 2923.936    5.470 2927.415

&gt; start_time &lt;- proc.time()
&gt; mnist_mlp2 &lt;- mlp(mnist_norm$inputsTrain, mnist_norm$targetsTrain, size = 300, inputsTest = mnist_norm$inputsTest, targetsTest = mnist_norm$targetsTest)
&gt; proc.time() - start_time
     user   system  elapsed 
 7141.687    7.488 7144.433</pre></div><p class="calibre8">As we <a id="id473" class="calibre1"/>can see, the models take quite a long time to run (the values are in seconds). For reference, these were trained on a 2.5 GHz Intel Core i7 Apple MacBook Pro with 16 GB of memory. The model predictions on our test set are saved in the <code class="email">fittedTestValues</code> attribute (and for our training set, they are stored in the <code class="email">fitted.values</code> attribute). We will focus on test set accuracy. First, we must decode the dummy-encoded network outputs by selecting the binary column with the maximum value. We must also do this for the target outputs. Note that the first column corresponds to the digit <code class="email">0</code>:</p><div class="informalexample"><pre class="programlisting">&gt; mnist_class_test &lt;- (0:9)[apply(mnist_norm$targetsTest, 1, which.max)]
&gt; mlp_class_test &lt;- (0:9)[apply(mnist_mlp$fittedTestValues, 1, which.max)]
&gt; mlp2_class_test &lt;- (0:9)[apply(mnist_mlp2$fittedTestValues, 1, which.max)]</pre></div><p class="calibre8">Now we can check the accuracy of our two models, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; mean(mnist_class_test == mlp_class_test)
[1] 0.974
&gt; mean(mnist_class_test == mlp2_class_test)
[1] 0.981</pre></div><p class="calibre8">The accuracy is very high for both models, with the second model slightly outperforming the first. We can use the <code class="email">confusionMatrix()</code> function to see the errors made in detail:</p><div class="informalexample"><pre class="programlisting">&gt; confusionMatrix(mnist_class_test, mlp2_class_test)
       predictions
targets    0    1    2    3    4    5    6    7    8    9
      0 1226    0    0    1    1    0    1    1    3    1
      1    0 1330    5    3    0    0    0    3    0    1
      2    3    0 1135    3    2    1    1    5    3    0
      3    0    0    6 1173    0   11    1    5    6    1
      4    0    5    0    0 1143    1    5    5    0   10
      5    2    2    1   12    2 1077    7    3    5    4
      6    3    0    2    1    1    3 1187    0    1    0
      7    0    0    7    1    3    1    0 1227    1    4
      8    5    4    3    5    1    4    4    0 1110    5
      9    1    0    0    6    8    5    0   11    6 1164</pre></div><p class="calibre8">As expected, we see quite a bit of symmetry in this matrix because certain pairs of digits are often <a id="id474" class="calibre1"/>harder to distinguish than others. For example, the most common pair of digits that the model confuses is the pair (3,5). The test data available on the website contains some examples of digits that are harder to distinguish from others.</p><p class="calibre8">By default, the <code class="email">mlp()</code> function allows for a maximum of 100 iterations, via its <code class="email">maxint</code> parameter. Often, we don't know the number of iterations we should run for a particular model; a good way to determine this is to plot the training and testing error rates versus iteration number. With the RSNNS package, we can do this with the <code class="email">plotIterativeError()</code> function.</p><p class="calibre8">The following graphs show that for our two models, both errors plateau after 30 iterations:</p><div class="mediaobject"><img src="../images/00106.jpeg" alt="Predicting handwritten digits" class="calibre10"/></div><p class="calibre11"> </p></div>

<div class="book" title="Predicting handwritten digits">
<div class="book" title="Receiver operating characteristic curves"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec56" class="calibre1"/>Receiver operating characteristic curves</h2></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 3. Linear Regression" href="part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7">Chapter 3</a>, <span class="strong"><em class="calibre9">Logistic Regression</em></span>, we studied the precision-recall graph as an example of an important graph showing the trade-off between two important performance metrics of a binary <a id="id475" class="calibre1"/>classifier--precision and recall. In this chapter, we will present another related and commonly used graph to <a id="id476" class="calibre1"/>show binary classification performance, the <span class="strong"><strong class="calibre2">receiver operating characteristic</strong></span> (<span class="strong"><strong class="calibre2">ROC</strong></span>) curve.</p><p class="calibre8">This curve <a id="id477" class="calibre1"/>is a plot of the true positive rate on the <span class="strong"><em class="calibre9">y</em></span> axis and the false positive rate on the <span class="strong"><em class="calibre9">x</em></span> axis. The true positive rate, as we know, is just the recall or, equivalently, the sensitivity of a binary classifier. The false positive rate is just 1 minus the specificity. A random binary classifier will have a true positive rate equal to the false positive rate and thus, on the ROC curve, the line <span class="strong"><em class="calibre9">y = x</em></span> is the line showing the performance of a random classifier. Any curve lying above this line will perform better than a random classifier.</p><p class="calibre8">A perfect classifier will exhibit a curve from the origin to the point (0,1), which corresponds to a 100 percent true positive rate and a 0 percent false positive rate. We often talk about the <span class="strong"><strong class="calibre2">ROC Area Under the Curve</strong></span> (<span class="strong"><strong class="calibre2">ROC AUC</strong></span>) as a performance metric. The area under the random classifier is just 0.5 as we are computing the area under the line <span class="strong"><em class="calibre9">y = x</em></span> on a unit square. By convention, the area under a perfect classifier is 1 as the curve passes through the point (0,1). In practice, we obtain values between these two. For our MNIST digit classifier, we have a multiclass problem, but we can use the <code class="email">plotROC()</code> function of the RSNNS package to study the performance of our classifier on individual digits. </p><p class="calibre8">The following <a id="id478" class="calibre1"/>plot shows the ROC curve for digit 1, which is almost perfect:</p><div class="mediaobject"><img src="../images/00107.jpeg" alt="Receiver operating characteristic curves" class="calibre10"/></div><p class="calibre11"> </p></div></div>
<div class="book" title="Radial basis function networks" id="1IHDQ1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec46" class="calibre1"/>Radial basis function networks</h1></div></div></div><p class="calibre8">A radial <a id="id479" class="calibre1"/>basis function network-based upon the concept of function approximation - is a kind of artificial neural network that uses <span class="strong"><em class="calibre9">radial basis functions</em></span> to define a node's output (given a set of inputs). The output of the network consists of a<span class="strong"><em class="calibre9"> linear combination</em></span> of radial basis functions of the inputs and neuron parameters.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Radial basis function</strong></span> (<span class="strong"><strong class="calibre2">RBF</strong></span>) networks (also referred to as RBFNN for Radial Basis Function Neural Networks) will <a id="id480" class="calibre1"/>have three separate layers: an <span class="strong"><strong class="calibre2">input</strong></span> layer, a <span class="strong"><strong class="calibre2">hidden</strong></span> layer, and a linear <span class="strong"><strong class="calibre2">output</strong></span> layer. The input layer will be a set of <a id="id481" class="calibre1"/>several nodes that transfer transition the input values to the second (or hidden) layer where activation patterns are applied. These patterns will be selected <a id="id482" class="calibre1"/>radial basis functions that best fit the application or objective. This transformation occurs in a non-linear fashion. The third layer (or output layer) provides the <a id="id483" class="calibre1"/>response of the network to the activation or RFB functions applied to the inputs. In an RFB network, the transformation from the hidden layer to the output layer is nonlinear.</p><p class="calibre8">A radial <a id="id484" class="calibre1"/>basis function network is a neural network usually approached by viewing the design as a curve-fitting (guesstimate) problem in a high dimensional space. Learning is equivalent to finding a multidimensional function that provides a best fit to the training data, with the criterion for best fit being measured in some statistical sense.</p><p class="calibre8">Generally, RBF networks seem to have the advantages of a more easily understood design, generalization ability, and a record of good tolerance to "noise" within the data.</p><p class="calibre8">The properties of RBF networks make it a very good choice for designing control systems that are required to be very flexible in that they must continually evaluate the various <span class="strong"><em class="calibre9">paths to completion</em></span> and determine the most efficient. The study most famous in using RBF networks is in solving the traveling salesman problem (finding the shortest closed path between a group of cities).</p></div>
<div class="book" title="Summary" id="1JFUC1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec47" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we saw neural networks as a nonlinear method capable of solving both regression and classification problems. Motivated by the biological analogy to human neurons, we first introduced the simplest neural network, the perceptron. This is able to solve binary classification problems only when the two classes are linearly separable, something that we very rarely rely upon in practice.</p><p class="calibre8">By changing the function that transforms the linear weighted combination of inputs, namely the activation function, we discovered how to create different types of individual neurons. A linear activation function creates a neuron that performs linear regression, whereas the logistic activation function creates a neuron that performs logistic regression. By organizing and connecting neurons into layers, we can create multilayer neural networks that are powerful models for solving nonlinear problems.</p><p class="calibre8">The idea behind having hidden layers of neurons is that each hidden layer learns a new set of features from its inputs. As the most common type of multilayer neural network, we introduced the multilayer perceptron and saw that it can naturally learn multiple outputs with the same network. In addition, we experimented on real-world datasets for both regression and classification tasks, including a multiclass classification problem that we saw is also handled naturally. R has a number of packages for implementing neural networks, including <code class="email">neuralnet</code>, <code class="email">nnet</code>, and <code class="email">RSNNS</code>, and we experimented with each of these in turn. Each has its respective advantages and disadvantages and there isn't a clear winner for every circumstance.</p><p class="calibre8">An important benefit of working with neural networks is that they can be very powerful in solving highly complex nonlinear problems of regression and classification alike without making any significant assumptions about the relationships between the input features. On the other hand, neural networks can often be quite tricky to train. Scaling input features is important. It is also important to be aware of the various parameters affecting the convergence of the model, such as the learning rate and the error gradient tolerance. Another crucial decision to make is the number and distribution of hidden layer neurons. As the complexity of the network, the number of input features, or the size of the training data increases, the training time often becomes quite long compared to other supervised learning methods.</p><p class="calibre8">We also saw in our regression example that because of the flexibility and power of neural networks, they can be prone to overfitting the data, thus overestimating the model's accuracy. Regularization approaches, such as weight decay, exist to mitigate this problem to a certain extent. Finally, one clear disadvantage that deserves mention is that the neural weights have no direct interpretation, unlike regression coefficients, and even though the neural network topology may learn features, these are difficult to explain or interpret.</p><p class="calibre8">Our next chapter continues our foray into the world of supervised learning and presents support vector machines, our third nonlinear modeling tool, which is primarily used for dealing with classification problems.</p></div></body></html>