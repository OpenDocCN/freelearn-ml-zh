<html><head></head><body>
<div class="book" title="Chapter&#xA0;4.&#xA0;Random Forests" id="VF2I1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Random Forests</h1></div></div></div><p class="calibre7">The previous chapter introduced bagging as an ensembling technique based on homogeneous base learners, with the decision tree serving as a base learner. A slight shortcoming of the bagging method is that the bootstrap trees are correlated. Consequently, although the variance of predictions is reduced, the bias will persist. Breiman proposed randomly sampling the covariates and independent variables at each split, and this method then went on to help in decorrelating the bootstrap trees.</p><p class="calibre7">In the first section of this chapter, the random forest algorithm is introduced and illustrated. The notion of variable importance is crucial to decision trees and all of their variants, and a section is devoted to clearly illustrating this concept. Do the random forests perform better than bagging? An answer will be provided in the following section.</p><p class="calibre7">Breiman laid out the importance of proximity plots in the context of random forests, and we will delve into this soon enough. An algorithm as complex as this will have a lot of nitty-gritty details, and some of these will be illustrated through programs and real data. Missing data is almost omnipresent and we will undertake the task of imputing missing values using random forests. Although a random forest is primarily a supervised learning technique, it can also be used for clustering observations regarding the data, and this topic will be the concluding section.</p><p class="calibre7">The core topics of this chapter are the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The Random Forest algorithm</li><li class="listitem">Variable importance for decision trees and random forests</li><li class="listitem">Comparing random forests with bagging</li><li class="listitem">Use of proximity plots</li><li class="listitem">Random forest details, nitty-gritty, and nuances</li><li class="listitem">Handling missing data by using random forests</li><li class="listitem">Clustering with random forests</li></ul></div></div>

<div class="book" title="Chapter&#xA0;4.&#xA0;Random Forests" id="VF2I1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec31" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will be using the following libraries in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">kernlab</code></li><li class="listitem"><code class="literal">randomForest</code></li><li class="listitem"><code class="literal">randomForestExplainer</code></li><li class="listitem"><code class="literal">rpart</code></li></ul></div></div></div>
<div class="book" title="Random Forests"><div class="book" id="10DJ42-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec32" class="calibre1"/>Random Forests</h1></div></div></div><p class="calibre7">
<a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, generalized the decision tree using the bootstrap principle. Before we embark on a journey with<a id="id168" class="calibre1"/> random forests, we will quickly review the history of decision trees and highlight some of their advantages and drawbacks. The invention of decision trees followed through a culmination of papers, and the current form of the trees can be found in detail in Breiman, et al. (1984). Breiman's method is popularly known as <span class="strong"><strong class="calibre8">C</strong></span>lassification <span class="strong"><strong class="calibre8">a</strong></span>nd <span class="strong"><strong class="calibre8">R</strong></span>egression <span class="strong"><strong class="calibre8">T</strong></span>rees, aka <span class="strong"><strong class="calibre8">CART</strong></span>. Around the late 1970s and early 1980s, Quinlan invented an algorithm <a id="id169" class="calibre1"/>called C4.5 independently of Breiman. For more information, see Quinlan (1984). To a large extent, the current form of decision trees, bagging, and random forests is owed to Breiman. A somewhat similar approach is also available in an algorithm popularly known by the abbreviation CHAID, which stands for <span class="strong"><strong class="calibre8">Ch</strong></span>i-square <span class="strong"><strong class="calibre8">A</strong></span>utomatic <span class="strong"><strong class="calibre8">I</strong></span>nteraction <span class="strong"><strong class="calibre8">D</strong></span>etector. An in-depth look at CART <a id="id170" class="calibre1"/>can be found in Hastie, et al. (2009), and a statistical perspective can be found in Berk (2016). An excellent set of short notes can also be found in Seni and Elder (2010). Without any particular direction, we highlight some advantages and drawbacks of CART:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Trees automatically <a id="id171" class="calibre1"/>address the problem of variable selection since at each split, they look for the variable that gives the best split in the regressand, and thus a tree eliminates variables that are not useful.</li><li class="listitem">Trees do not require <a id="id172" class="calibre1"/>data processing. This means that we don't have to consider transformation, rescaling, and/or weight-of-evidence preprocessing.</li><li class="listitem">Trees are computationally scalable and the time complexity is manageable.</li><li class="listitem">Trees give a metric called variable importance that is based on the contribution of the variable to error reduction across all the splits of the trees.</li><li class="listitem">Trees efficiently handle missing values and if an observation has a missing value, the tree will continue to use the available values of the observation. Handling missing data is often enabled by the notion of a surrogate split.</li><li class="listitem">Trees have fewer parameters to manage, as seen in the previous chapter.</li><li class="listitem">Trees have a simple top-down interpretation.</li><li class="listitem">Trees with great depth tend to be almost unbiased.</li><li class="listitem">The interaction effect is easily identified among the variables.</li><li class="listitem">Its drawback is that the fitted model is not continuous and it will have sharp edges. Essentially, trees are piecewise constant regression models.</li><li class="listitem">Trees can't approximate low interaction target functions.</li><li class="listitem">The greedy search approach to trees results in high variance.</li></ul></div><p class="calibre7">The first extension of the trees <a id="id173" class="calibre1"/>was seen in the bagging algorithm discussed in the previous chapter. Suppose we have N observations. For each bootstrap sample, we draw N observations with replacement. How many observations are likely to be common between two bootstrap samples? Let's write a simple program to find it first, using the simple <code class="literal">sample</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; N &lt;- seq(1e3,1e4,1e3)
&gt; N
 [1]  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000
&gt; B &lt;- 1e3
&gt; Common_Prob &lt;- NULL
&gt;index&lt;- 1
&gt;for(i in N){
+   temp_prob &lt;- NULL
+   for(j in 1:B){
+     s1 &lt;- sample(i,size=i,replace=TRUE)
+     s2 &lt;- sample(i,size=i,replace=TRUE)
+     temp_prob &lt;- c(temp_prob,length(intersect(s1,s2))/i)
+   }
+   Common_Prob[index] &lt;- mean(temp_prob)
+   index&lt;- index + 1
+ }
&gt; Common_Prob
 [1] 0.4011 0.4002 0.3996 0.3982 0.3998 0.3996 0.3994 0.3997 0.3996 0.3995</pre></div><p class="calibre7">This program needs explanation. The number of <span class="strong"><em class="calibre9">N</em></span> observations varies from 1000 to 10000 with an increment of 1000, and we run <span class="strong"><em class="calibre9">B = 1e3 = 1000</em></span> bootstrap iterations. Now, for a fixed size of <span class="strong"><em class="calibre9">N</em></span>, we draw two samples with replacement of size <span class="strong"><em class="calibre9">N</em></span>, see how many observations are common between them, and divide it by <span class="strong"><em class="calibre9">N</em></span>. The average of the <span class="strong"><em class="calibre9">B = 1000</em></span> samples is the probability of finding a common observation between two samples. Equivalently, it gives the common observation percentage between two samples.</p><p class="calibre7">The bootstrap probability clearly shows that about 40% of observations will be common between any two trees. Consequently, the trees will be correlated.</p><p class="calibre7">In Chapter 15, Hastie, et al. (2009) points out that the bagging trees are IID trees and hence the expectation of any one tree is the same as the expectation of any other tree. Consequently, the bias of the bagged trees is the same as that of the individual trees. Thus, variance reduction is the only improvement provided by bagging. Suppose that we have B independent and identically distributed IID random variables with a variance of <span class="strong"><img src="../images/00179.jpeg" alt="Random Forests" class="calibre15"/></span>. The sample average has a variance of <span class="strong"><img src="../images/00180.jpeg" alt="Random Forests" class="calibre15"/></span>. However, if we know that the variables are only identically distributed and that there is a positive pairwise correlation of <span class="strong"><img src="../images/00181.jpeg" alt="Random Forests" class="calibre15"/></span>, then the variance of the sample average is as follows:</p><div class="mediaobject"><img src="../images/00182.jpeg" alt="Random Forests" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Note that as the number of B samples increase, the second term vanishes and the first term remains. Thus, we see that the correlatedness of the bagged trees restricts the benefits of averaging. This motivated Breiman to innovate in a way that subsequent trees will not be correlated.</p><p class="calibre7">Breiman's solution is that before <a id="id174" class="calibre1"/>each split, select <span class="strong"><em class="calibre9">m &lt; p</em></span> number of input variables at random for splitting. This lays the foundation of random forests, where we shake the data to improve the performance. Note that merely <span class="strong"><em class="calibre9">shaking</em></span> does not guarantee improvement. This trick helps when we have highly nonlinear estimators. The formal random forest algorithm, following Hastie, et al. (2009) and Berk (2016), is given as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Draw a random sample of size N with replacement from the data.</li><li class="listitem" value="2">Draw a random sample without replacement of the predictors.</li><li class="listitem" value="3">Construct the first recursive partition of the data in the usual way.</li><li class="listitem" value="4">Repeat step 2 for each subsequent split until the tree is as large as desired. Importantly, do not prune. Compute each terminal node proportion.</li><li class="listitem" value="5">Drop the out-of-bag (OOB) data down the tree and store the assigned class to each observation, along with each observation's predictor values.</li><li class="listitem" value="6">Repeat steps 1-5 a large number of times, say 1000.</li><li class="listitem" value="7">Using only the class assigned to each observation when that observation is OOB, count the number of times over the trees that the observation is classified in one category and the number of times over trees it is classified in the other category.</li><li class="listitem" value="8">Assign each case to a category by a majority vote over the set of trees when that case is OOB.</li></ol><div class="calibre13"/></div><p class="calibre7">From his practical experience, Breiman recommends randomly selecting a number of covariates at each split as <span class="strong"><img src="../images/00183.jpeg" alt="Random Forests" class="calibre15"/></span> with a minimum node size of 1 for a classification problem, whereas the recommendation for a regression problem is <span class="strong"><img src="../images/00184.jpeg" alt="Random Forests" class="calibre15"/></span> with a minimum node size of 5.</p><p class="calibre7">We will use the <code class="literal">randomForest</code> R package for software implementation. The German Credit data will be used for <a id="id175" class="calibre1"/>further analysis. If you remember, in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, the accuracy obtained by using the basic classification tree was 70%. We will set up the German credit data using the same settings as earlier, and we will build the random forest:</p><div class="informalexample"><pre class="programlisting">&gt;load("../Data/GC2.RData")
&gt;set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(GC2),
+ replace = TRUE,prob = c(0.7,0.3))
&gt; GC2_Train &lt;- GC2[Train_Test=="Train",]
&gt; GC2_TestX &lt;- within(GC2[Train_Test=="Test",],rm(good_bad))
&gt; GC2_TestY &lt;- GC2[Train_Test=="Test","good_bad"]
&gt; GC2_Formula &lt;- as.formula("good_bad~.")
&gt; GC2_RF &lt;- randomForest(GC2_Formula,data=GC2_Train,ntree=500)
&gt; GC2_RF_Margin &lt;- predict(GC2_RF,newdata = GC2_TestX,type="class")
&gt;sum(GC2_RF_Margin==GC2_TestY)/313
[1] 0.7795527</pre></div><p class="calibre7">The <code class="literal">randomForest</code> function applies over the <code class="literal">formula</code> and <code class="literal">data</code> as seen earlier. Here, we have specified the number of trees to be 500 with <code class="literal">ntree=500</code>.</p><p class="calibre7">If we compare this random result with the bagging result from the previous chapter, the accuracy obtained there was only <code class="literal">0.78</code>. Here, we have <code class="literal">p = 19</code> covariates, and so we will try to increase the number of covariates sampled for a split at <code class="literal">8</code>, and see how it performs:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_RF2 &lt;- randomForest(GC2_Formula,data=GC2_Train,mtry=8, 
+ ntree=500)
&gt; GC2_RF_Margin &lt;- predict(GC2_RF,newdata = GC2_TestX, type="class")
&gt; GC2_RF2_Margin &lt;- predict(GC2_RF2,newdata = GC2_TestX,type="class")
&gt; sum(GC2_RF2_Margin==GC2_TestY)/313
[1] 0.7859425</pre></div><p class="calibre7">An increase of <code class="literal">0.01</code>, or about 1%, might appear meager. However, in a banking context, this accuracy will translate into millions of dollars. We will use the usual <code class="literal">plot</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;plot(GC2_RF2)
&gt; GC2_RF2.legend &lt;- colnames(GC2_RF2$err.rate)
&gt; legend(x=300,y=0.5,legend = GC2_RF2.legend,lty=c(1,2,3), col=c(1,2,3))
&gt;head(GC2_RF2$err.rate,10)
            OOB       bad      good
 [1,] 0.3206751 0.4743590 0.2452830
 [2,] 0.3218673 0.4769231 0.2490975
 [3,] 0.3222656 0.5437500 0.2215909
 [4,] 0.3006993 0.5224719 0.2005076
 [5,] 0.3262643 0.5445026 0.2274882
 [6,] 0.3125000 0.5522388 0.2027335
 [7,] 0.3068702 0.5631068 0.1893096
 [8,] 0.2951807 0.5741627 0.1670330
 [9,] 0.2976190 0.5619048 0.1774892
[10,] 0.2955882 0.5801887 0.1666667</pre></div><p class="calibre7">The following graph is the output of the preceding code executed using the <code class="literal">plot </code>function<code class="literal">:</code>
</p><div class="mediaobject"><img src="../images/00185.jpeg" alt="Random Forests" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Error rate of the Random Forest for the German Credit data</p></div></div><p class="calibre11"> </p><p class="calibre7">We have three curves: the error rate for OOB, the error rate for the good class, and the error rate for the bad class. Note that the error rate stabilizes at around 100 trees. Using the loss matrix, it might be possible to reduce the gap between the three curves. Ideally, the three curves should be as close as possible.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Create random forests<a id="id176" class="calibre1"/> with the options of <code class="literal">split</code> criteria, <code class="literal">loss</code> matrix, <code class="literal">minsplit</code>, and different <code class="literal">mtry</code>. Examine the error rate curves and prepare a summary.</p><p class="calibre7">Visualize the random forest! Where are the trees? Apparently, we need to do a lot of exercises to extract trees out of the fitted <code class="literal">randomForest</code> object. A new function, <code class="literal">plot_RF</code>, has been defined in the <code class="literal">Utilities.R</code> file and we will display it here:</p><div class="mediaobject"><img src="../images/00186.jpeg" alt="Random Forests" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The <code class="literal">plot_RF</code> function first obtains the number of <code class="literal">$ntree </code>trees in the forest. It will then run through a <code class="literal">for</code> loop. In each iteration of the loop, it will extract information related to that tree with the <code class="literal">getTree</code> function and create a new <code class="literal">dendogram</code> object. The <code class="literal">dendogram</code> is then visualized, and is nothing but the tree. Furthermore, the <code class="literal">print</code> command is optional and can be muted out.</p><p class="calibre7">Four arbitrarily chosen trees from the forest in the PDF file are displayed in the following figure, Trees of the Random Forest:</p><div class="mediaobject"><img src="../images/00187.jpeg" alt="Random Forests" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Trees of the Random Forest</p></div></div><p class="calibre11"> </p><p class="calibre7">A quick visit is paid to the Pima<a id="id177" class="calibre1"/> Indians diabetes problem. In the accuracy table of <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, we could see that the accuracy for the decision tree was 0.7588, or 75.88%:</p><div class="informalexample"><pre class="programlisting">&gt; data("PimaIndiansDiabetes")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(PimaIndiansDiabetes),
+ replace = TRUE, prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; PimaIndiansDiabetes_Train &lt;- PimaIndiansDiabetes[Train_Test=="Train",]
&gt; PimaIndiansDiabetes_TestX &lt;- within(PimaIndiansDiabetes[Train_Test=="Test",],rm(diabetes))
&gt; PimaIndiansDiabetes_TestY &lt;- PimaIndiansDiabetes[ 
+ Train_Test=="Test","diabetes"]
&gt; PID_Formula &lt;- as.formula("diabetes~.")
&gt; PID_RF &lt;- randomForest(PID_Formula,data=PimaIndiansDiabetes_Train,coob=TRUE,
+                        ntree=500,keepX=TRUE,mtry=5,
+                        parms=list(prior=c(0.65,0.35)))
&gt; PID_RF_Margin &lt;- predict(PID_RF,newdata = PimaIndiansDiabetes_TestX, type="class")
&gt; sum(PID_RF_Margin==PimaIndiansDiabetes_TestY)/257
[1] 0.7704</pre></div><p class="calibre7">Thus, we have an improved<a id="id178" class="calibre1"/> accuracy of 0.7704 – 0.7588 = 0.0116, or about 1.2%.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Obtain the error rate plot of the Pima Indian Diabetes problem.</p></div>
<div class="book" title="Variable importance"><div class="book" id="11C3M2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec33" class="calibre1"/>Variable importance</h1></div></div></div><p class="calibre7">Statistical models, say<a id="id179" class="calibre1"/> linear regression and logistic regression, indicate which variables are significant with measures such as p-value and t-statistics. In a decision tree, a split is caused by a single variable. If the specification of the number of variables for the surrogate splits, a certain variable may appear as the split criteria more than once in the tree and some variables may never appear in the tree splits at all. During each split, we select the variable that leads to the maximum reduction in impurity, and the contribution of a variable across the tree splits would also be different. The overall improvement across each split of the tree (by the reduction in impurity for the classification tree or by the improvement in the split criterion) is referred to as the <span class="strong"><em class="calibre9">variable importance</em></span>. In the case of ensemble methods such as bagging and random forest, the variable importance is measured for each tree in the technique. While the concept of variable importance is straightforward, its computational understanding is often unclear. This is primarily because a formula or an expression is not given in mathematical form. The idea is illustrated next through simple code.</p><p class="calibre7">The <code class="literal">kyphosis</code> dataset from the <code class="literal">rpart</code> package consists of four variables, and the target variable here is named <code class="literal">Kyphosis</code>, indicating the presence of the kyphosis type of deformation following an operation. The three explanatory variables are <code class="literal">Age</code>, <code class="literal">Number</code>, and <code class="literal">Start</code>. We build a classification tree with zero surrogate variables for the split criteria with the <code class="literal">maxsurrogate=0</code> option. The choice of zero surrogates ensures that we have only one variable at a split. The tree is set up and visualized as follows:</p><div class="informalexample"><pre class="programlisting">&gt; data(kyphosis)
&gt; kc&lt;- rpart(Kyphosis~.,data=kyphosis,maxsurrogate=0)
&gt; plot(kc);text(kc)</pre></div><div class="mediaobject"><img src="../images/00188.jpeg" alt="Variable importance" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Kyphosis Classification Tree</p></div></div><p class="calibre11"> </p><p class="calibre7">In the no-surrogate<a id="id180" class="calibre1"/> tree, the first split variable is <code class="literal">Start</code>, with a terminal leaf on the right part of the split. The left side/partition further splits again with the <code class="literal">Start</code> variable, with a terminal node/leaf on the left side and a split on the later right side. In the next two split points, we use only the <code class="literal">Age</code> variable, and the <code class="literal">Number</code> variable is not used anywhere in the tree. Thus, we expect the <code class="literal">Number</code> variable to have zero importance.</p><p class="calibre7">Using <code class="literal">$variable.importance</code> on the fitted classification tree, we obtain the variable importance of the three explanatory variables:</p><div class="informalexample"><pre class="programlisting">&gt;kc$variable.importance
Start   Age 
7.783 2.961 </pre></div><p class="calibre7">As expected, the <code class="literal">Number</code> variable is not shown as having any importance. The importance of <code class="literal">Start</code> is given as <code class="literal">7.783</code> and <code class="literal">Age</code> as <code class="literal">2.961</code>. To understand how R has computed these values, run the <code class="literal">summary</code> function on the classification tree:</p><div class="informalexample"><pre class="programlisting">&gt;summary(kc)
Call:
rpart(formula = Kyphosis ~ ., data = kyphosis, maxsurrogate = 0)
  n= 81 
       CP nsplit rel error xerror   xstd
1 0.17647      0    1.0000      1 0.2156
2 0.01961      1    0.8235      1 0.2156
3 0.01000      4    0.7647      1 0.2156

Variable importance
Start   Age 
   72    28 

Node number 1: 81 observations,    complexity param=0.1765
predicted class=absent   expected loss=0.2099  P(node) =1
class counts:    64    17
probabilities: 0.790 0.210 
left son=2 (62 obs) right son=3 (19 obs)
  Primary splits:
<span class="strong"><strong class="calibre8">Start  &lt; 8.5</strong></span>to the right, <span class="strong"><strong class="calibre8">improve=6.762</strong></span>, (0 missing)
      Number &lt;5.5  to the left,  improve=2.867, (0 missing)
      Age    &lt; 39.5 to the left,  improve=2.250, (0 missing)

Node number 2: 62 observations,    complexity param=0.01961
predicted class=absent   expected loss=0.09677  P(node) =0.7654
class counts:    56     6
probabilities: 0.903 0.097 
left son=4 (29 obs) right son=5 (33 obs)
  Primary splits:
<span class="strong"><strong class="calibre8">Start  &lt; 14.5</strong></span>to the right, <span class="strong"><strong class="calibre8">improve=1.0210</strong></span>, (0 missing)
      Age    &lt; 55   to the left,  improve=0.6849, (0 missing)
      Number &lt;4.5  to the left,  improve=0.2975, (0 missing)

Node number 3: 19 observations
predicted class=present  expected loss=0.4211  P(node) =0.2346
class counts:     8    11
probabilities: 0.421 0.579 

Node number 4: 29 observations
predicted class=absent   expected loss=0  P(node) =0.358
class counts:    29     0
probabilities: 1.000 0.000 

Node number 5: 33 observations,    complexity param=0.01961
predicted class=absent   expected loss=0.1818  P(node) =0.4074
class counts:    27     6
probabilities: 0.818 0.182 
left son=10 (12 obs) right son=11 (21 obs)
  Primary splits:
<span class="strong"><strong class="calibre8">Age    &lt; 55</strong></span>to the left,  <span class="strong"><strong class="calibre8">improve=1.2470</strong></span>, (0 missing)
Start  &lt; 12.5 to the right, improve=0.2888, (0 missing)
      Number &lt;3.5  to the right, improve=0.1753, (0 missing)

Node number 10: 12 observations
predicted class=absent   expected loss=0  P(node) =0.1481
class counts:    12     0
probabilities: 1.000 0.000 

Node number 11: 21 observations,    complexity param=0.01961
predicted class=absent   expected loss=0.2857  P(node) =0.2593
class counts:    15     6
probabilities: 0.714 0.286 
left son=22 (14 obs) right son=23 (7 obs)
  Primary splits:
<span class="strong"><strong class="calibre8">Age    &lt;111</strong></span>  to the right, <span class="strong"><strong class="calibre8">improve=1.71400</strong></span>, (0 missing)
Start  &lt; 12.5 to the right, improve=0.79370, (0 missing)
      Number &lt;3.5  to the right, improve=0.07143, (0 missing)

Node number 22: 14 observations
predicted class=absent   expected loss=0.1429  P(node) =0.1728
class counts:    12     2
probabilities: 0.857 0.143 

Node number 23: 7 observations
predicted class=present  expected loss=0.4286  P(node) =0.08642
class counts:     3     4
probabilities: 0.429 0.571 </pre></div><p class="calibre7">Four lines of output have been highlighted in the summary output, and each line contains the information about the split, the<a id="id181" class="calibre1"/> best of improvement offered by each of the variables, and the variable selected at the split. Thus, for the <code class="literal">Start</code> variable, the first highlighted line shows the improvement at <code class="literal">6.762</code> and the second line shows <code class="literal">1.021</code>. By adding these, we get <code class="literal">6.762 + 1.021 = 7.783</code>, which is the same as the output given from the <code class="literal">$variable.importance</code> extractor. Similarly, the last two highlighted lines show the contribution of <code class="literal">Age</code> as <code class="literal">1.274 + 1.714 = 2.961</code>. Thus, we have clearly outlined the computation of the variable importance.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Create a new classification tree, say <code class="literal">KC2</code>, and allow a surrogate split. Using the <code class="literal">summary</code> function, verify the computations associated with the variable importance.</p><p class="calibre7">The <code class="literal">VarImpPlot</code> function from the <code class="literal">randomForest</code> package gives us a dot chart plot of the variable importance measure:</p><div class="informalexample"><pre class="programlisting">&gt;windows(height=100,width=200)
&gt;par(mfrow=c(1,2))
&gt;varImpPlot(GC2_RF,main="Variable Importance plot for \n Random 
+ Forest of German Data")
&gt;varImpPlot(PID_RF,main="Variable Importance plot for \n Random Forest of Pima Indian Diabetes")</pre></div><p class="calibre7">A visual display is given in the following figure:</p><div class="mediaobject"><img src="../images/00189.jpeg" alt="Variable importance" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Variable Importance Plots for German and Pima Indian Diabetes Random Forests</p></div></div><p class="calibre11"> </p><p class="calibre7">Thus, the five most important <a id="id182" class="calibre1"/>variables for classifying the German credit as good or bad are <code class="literal">amount</code>, <code class="literal">checking</code>, <code class="literal">age</code>, <code class="literal">duration</code>, and <code class="literal">purpose</code>. For the Pima Indian Diabetes classification, the three most important variables are <code class="literal">glucose</code>, <code class="literal">mass</code>, and <code class="literal">age</code>.</p><p class="calibre7">We will look at the notion of the proximity measure next.</p></div>
<div class="book" title="Proximity plots" id="12AK81-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec34" class="calibre1"/>Proximity plots</h1></div></div></div><p class="calibre7">According to Hastie, et al. (2009), "<span class="strong"><em class="calibre9">one of the advertised outputs of a random forest is a proximity plot"</em></span> (see page 595). But what are proximity plots? If we have <span class="strong"><em class="calibre9">n</em></span> observations in the training dataset, a<a id="id183" class="calibre1"/> proximity matrix of order <span class="strong"><img src="../images/00190.jpeg" alt="Proximity plots" class="calibre15"/></span> is created. Here, the matrix is initialized with all the values at 0. Whenever a pair of observations such as OOB occur jointly in the terminal node of a tree, the proximity count is increased by 1. The proximity matrix is visualized using the multidimensional scaling method, a concept beyond the scope of this chapter, where the proximity matrix is represented in two dimensions. The proximity plots give an indication of which points are closer to each other from the perspective of the random forest.</p><p class="calibre7">In the earlier creation of random forests, we had not specified the option of a proximity matrix. Thus, we will first create the random forest using the option of proximity as follows:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_RF3 &lt;- randomForest(GC2_Formula,data=GC2_Train,
+                         ntree=500,proximity=TRUE,cob.prox=TRUE)
&gt; GC2_RF3$proximity[1:10,1:10]
        5      6      7      8     11     12     14     15     16     17
5  1.0000 0.0000 0.0000 0.0133 0.0139 0.0159 0.0508 0.0645 0.0000 0.0000
6  0.0000 1.0000 0.0435 0.0308 0.0000 0.0000 0.0000 0.0000 0.0000 0.0417
7  0.0000 0.0435 1.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.2000
8  0.0133 0.0308 0.0000 1.0000 0.0000 0.0000 0.0000 0.0000 0.0137 0.0000
11 0.0139 0.0000 0.0000 0.0000 1.0000 0.0395 0.0000 0.2034 0.0147 0.0000
12 0.0159 0.0000 0.0000 0.0000 0.0395 1.0000 0.0000 0.0323 0.0000 0.0000
14 0.0508 0.0000 0.0000 0.0000 0.0000 0.0000 1.0000 0.0167 0.0435 0.0182
15 0.0645 0.0000 0.0000 0.0000 0.2034 0.0323 0.0167 1.0000 0.0345 0.0000
16 0.0000 0.0000 0.0000 0.0137 0.0147 0.0000 0.0435 0.0345 1.0000 0.0159
17 0.0000 0.0417 0.2000 0.0000 0.0000 0.0000 0.0182 0.0000 0.0159 1.0000
&gt;MDSplot(GC2_RF3,fac = GC2_Train$good_bad,
+         main="MDS Plot for Proximity Matrix of a RF")</pre></div><p class="calibre7">The options <code class="literal">proximity=TRUE,cob.prox=TRUE</code> are important to obtain the <code class="literal">proximity</code> matrix. We then simply make use of the <code class="literal">MDSplot</code> graphical function:</p><div class="mediaobject"><img src="../images/00191.jpeg" alt="Proximity plots" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: The multidimensional plot for the proximity matrix of an RF</p></div></div><p class="calibre11"> </p><p class="calibre7">It is easier to find which<a id="id184" class="calibre1"/> observation is closest to a given observation from the proximity data perspective, and not the Euclidean distance, using the <code class="literal">which.max</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;which.max(GC2_RF3$proximity[1,-1])
962 
657 
&gt;which.max(GC2_RF3$proximity[2,-2])
686 
458 </pre></div><p class="calibre7">Thus, the observations numbered <code class="literal">657</code> in the training dataset (and <code class="literal">962</code> in the overall dataset) are closest to the first observation. Note that the overall position is because of the name extracted from the sample function. The <code class="literal">which.max</code> function is useful for finding the maximum position in an array.</p><p class="calibre7">It turns out that most often, the graphical display using the <code class="literal">MDSplot</code> function results in a similar star-shape display. The proximity matrix also helps in carrying out cluster analysis, as will be seen in the concluding section of the chapter. Next, we will cover the parameters of a random forest in more detail.</p></div>
<div class="book" title="Random Forest nuances"><div class="book" id="1394Q2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec35" class="calibre1"/>Random Forest nuances</h1></div></div></div><p class="calibre7">The <code class="literal">GC_Random_Forest.pdf</code> file consists of the 500 trees which serve as the homogeneous learners in the random<a id="id185" class="calibre1"/> forest ensemble. It is well known that a decision tree has a nice and clear interpretation. This is because it shows how one traverses the path to a terminal node. The random selection of features at each split and the bootstrap samples lead to the setting up of the random forest. Refer to the figure <span class="strong"><em class="calibre9">Trees of the Random Forest</em></span>, which depicts trees numbered <code class="literal">78</code>, <code class="literal">176</code>, <code class="literal">395</code>, and <code class="literal">471</code>. The first split across the four trees is respectively <code class="literal">purpose</code>, <code class="literal">amount</code>, <code class="literal">property</code>, and <code class="literal">duration</code>. The second split for the first left side of these four trees is <code class="literal">employed</code>, <code class="literal">resident</code>, <code class="literal">purpose</code>, and <code class="literal">amount</code>, respectively. It is a cumbersome exercise to see which variables are meaningful over the others. We know that the earlier a variable appears, the higher its importance is. The question that then arises is, with respect to a random forest, how do we find the depth distribution of the variables? This and many other points are addressed through a powerful random forest package available as <code class="literal">randomForestExplainer</code>, and it is not an exaggeration that this section would not have been possible without this awesome package.</p><p class="calibre7">By applying the <code class="literal">min_depth_distribution</code> function on the random forest object, we get the depth distribution of the variables. Using <code class="literal">plot_min_depth_distribution</code>, we then get the plot of minimum depth distribution:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_RF_MDD &lt;- min_depth_distribution(GC2_RF)
&gt;head(GC2_RF_MDD)
tree variable minimal_depth
1    1      age             4
2    1   amount             3
3    1 checking             0
4    1    coapp             2
5    1  depends             8
6    1 duration             2
&gt;windows(height=100,width=100)
&gt; plot_min_depth_distribution(GC2_RF_MDD,k=nrow(GC2_TestX))</pre></div><p class="calibre7">The result of the preceding code block is <span class="strong"><em class="calibre9">Minimum Depth Distribution of German Random Forest</em></span>, which is as follows:</p><div class="mediaobject"><img src="../images/00192.jpeg" alt="Random Forest nuances" class="calibre10"/><div class="caption"><p class="calibre14">Figure 6: Minimum Depth Distribution of German Random Forest</p></div></div><p class="calibre11"> </p><p class="calibre7">From previous figure, it is clear that the <code class="literal">checking</code> variable appears more often as the primary split, followed by <code class="literal">savings</code>, <code class="literal">purpose</code>, <code class="literal">amount</code>, and <code class="literal">duration</code>. Consequently, we get a useful depiction through the minimum depth distribution plot. Further analyses are possible by using the <code class="literal">measure_importance</code> function, which gives us various measures of importance for the variables of the random forest:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_RF_VIM &lt;- measure_importance(GC2_RF)
[1] "Warning: your forest does not contain information on local importance so 'accuracy_decrease' measure cannot be extracted. To add it regrow the forest with the option localImp = TRUE and run this function again."</pre></div><p class="calibre7">We are warned here that the random forest has not been grown with the option of <code class="literal">localImp = TRUE</code>, which is central to obtaining the measures. Thus, we create a new random forest with this option, and then run the <code class="literal">measure_importance</code> function on it:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_RF4&lt;- randomForest(GC2_Formula,data=GC2_Train,
+                        ntree=500,localImp=TRUE)
&gt; GC2_RF4_VIM &lt;- measure_importance(GC2_RF4)</pre></div><p class="calibre7">The output has a wider format, and hence<a id="id186" class="calibre1"/> we provide it in an image format and display the result vertically in <span class="strong"><em class="calibre9">Analysis of Variable Importance Measure</em></span>. We can see that the <code class="literal">measure_importance</code> function gives a lot of information on the average minimum depth, number of nodes across the 500 trees that the variable appears as node, the average decrease in accuracy, the Gini decrease, and so on.</p><p class="calibre7">We can see from the output that if the mean minimum depth is higher, the associated p-value is also higher and hence the variable is insignificant. For example, the variables <code class="literal">coapp</code>, <code class="literal">depends</code>, <code class="literal">existcr</code>, <code class="literal">foreign</code>, and <code class="literal">telephon</code> have a higher mean minimum depth, and their p-value is also 1 in most cases. Similarly, lower values of <code class="literal">gini_decrease</code> are also associated with higher p-values, and this indicates the insignificance of the variables:</p><div class="mediaobject"><img src="../images/00193.jpeg" alt="Random Forest nuances" class="calibre10"/><div class="caption"><p class="calibre14">Figure 7: Analysis of Variable Importance Measure</p></div></div><p class="calibre11"> </p><p class="calibre7">The importance measure object <code class="literal">GC2_RF_VIM</code> can be used for further analyses. For the <code class="literal">no_of_nodes</code> measure, we can compare the various metrics from the previous variable importance measures. For instance, we <a id="id187" class="calibre1"/>would like to see how the <code class="literal">times_a_root</code> values for the variables turns out against the mean minimum depth. Similarly, we would like to analyze other measures. By applying the <code class="literal">plot_multi_way_importance</code> graphical function on this object, we get the following output:</p><div class="informalexample"><pre class="programlisting">&gt; P1 &lt;- plot_multi_way_importance(GC2_RF4_VIM, size_measure = "no_of_nodes",
+                           x_measure="mean_min_depth",
+                           y_measure = "times_a_root")
&gt; P2 &lt;- plot_multi_way_importance(GC2_RF4_VIM, size_measure = "no_of_nodes",
+                           x_measure="mean_min_depth",
+                           y_measure = "gini_decrease")
&gt; P3 &lt;- plot_multi_way_importance(GC2_RF4_VIM, size_measure = "no_of_nodes",
+                           x_measure="mean_min_depth",
+                           y_measure = "no_of_trees")
&gt; P4 &lt;- plot_multi_way_importance(GC2_RF4_VIM, size_measure = "no_of_nodes",
+                           x_measure="mean_min_depth",
+                           y_measure = "p_value")
&gt; grid.arrange(P1,P2,P3,P4, ncol=2)</pre></div><div class="mediaobject"><img src="../images/00194.jpeg" alt="Random Forest nuances" class="calibre10"/><div class="caption"><p class="calibre14">Figure 8: Multi-way Importance Plot for the German Credit Data</p></div></div><p class="calibre11"> </p><p class="calibre7">Here, the <code class="literal">times_a_root</code> values of the variables are plotted against the mean minimum depth, <code class="literal">mean_min_depth</code>, while keeping the number of nodes to their size. The non-top variables are black, while the top variables are blue. Similarly, we plot <code class="literal">gini_decrease</code>, <code class="literal">no_of_trees</code> and <code class="literal">p_value</code> against <code class="literal">mean_min_depth</code> in the preceding figure.</p><p class="calibre7">The correlation between the five measures is depicted next, using the <code class="literal">plot_importance_ggpairs</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; plot_importance_ggpairs(GC2_RF4_VIM)</pre></div><div class="mediaobject"><img src="../images/00195.jpeg" alt="Random Forest nuances" class="calibre10"/><div class="caption"><p class="calibre14">Figure 9: Relationship Between the Measures of Importance</p></div></div><p class="calibre11"> </p><p class="calibre7">Since the measures are strongly<a id="id188" class="calibre1"/> correlated, either positively or negatively, we need to have all five of these measures to understand random forests.</p><p class="calibre7">A great advantage of the tree structure is the interpretation of interaction between the variables. For instance, if the split in a parent is by one variable, and by another variable in the daughter node, we can conclude that there is interaction between these two variables. Again, the question arises for the random forests. Using the <code class="literal">important_variables</code> and <code class="literal">min_depth_interactions</code>, we can obtain the interactions among the variables of a random forest as follows:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_RF4_VIN &lt;- important_variables(GC2_RF4, k = 5, 
+                                    measures = c("mean_min_depth", "no_of_trees"))
&gt; GC2_RF4_VIN_Frame &lt;- min_depth_interactions(GC2_RF4,GC2_RF4_VIN)
&gt;head(GC2_RF4_VIN_Frame[order(GC2_RF4_VIN_Frame$occurrences, decreasing = TRUE), ])
variable root_variable mean_min_depth occurrences       interaction
7    amount      checking            1.6         442   checking:amount
2       age      checking            2.0         433      checking:age
27 duration      checking            2.1         426 checking:duration
77  purpose      checking            2.0         420  checking:purpose
32 employed      checking            2.6         417 checking:employed
8    amount      duration            2.4         408   duration:amount
   uncond_mean_min_depth
7                    2.4
2                    2.5
27                   2.3
77                   2.3
32                   3.0
8                    2.4
&gt; plot_min_depth_interactions(GC2_RF2_VIN_Frame)</pre></div><p class="calibre7">The following is the output that will be obtained:</p><div class="mediaobject"><img src="../images/00196.jpeg" alt="Random Forest nuances" class="calibre10"/><div class="caption"><p class="calibre14">Figure 10: Minimum Depth Interaction for German Random Forest</p></div></div><p class="calibre11"> </p><p class="calibre7">Thus, we can easily find the interaction <a id="id189" class="calibre1"/>variables of the random forest.</p><p class="calibre7">The <code class="literal">randomForestExplainer</code> R package is very powerful and helps us to carry out many diagnostics after obtaining the random forests. Without post diagnostics, we cannot evaluate any fitted model. Consequently, the reader is advised to carry out most of the steps learned in this section in their implementation of random forests.</p><p class="calibre7">We will compare a random forest with the bagging procedure in the next section.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Carry out the diagnostics for the random<a id="id190" class="calibre1"/> forest built for the Pima Indian Diabetes problem.</p></div>
<div class="book" title="Comparisons with bagging" id="147LC1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec36" class="calibre1"/>Comparisons with bagging</h1></div></div></div><p class="calibre7">When comparing the<a id="id191" class="calibre1"/> random forest results with the bagging counterpart for the German credit data and Pima Indian Diabetes datasets, we did not see much improvement in the accuracy over the validated partition of the data. A potential reason might be that the variability reduction achieved by bagging is at the optimum reduced variance, and that any bias improvement will not lead to an increase in the accuracy.</p><p class="calibre7">We consider a dataset to be available from the R core package <code class="literal">kernlab</code>. The dataset is spam and it has a collection of 4601 emails with labels that state whether the email is spam or non-spam. The dataset has a good collection of 57 variables derived from the email contents. The task is to build a good classifier for the spam identification problem. The dataset is quickly partitioned into training and validation partitions, as with earlier problems:</p><div class="informalexample"><pre class="programlisting">&gt; data("spam")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(spam),replace = TRUE,
+ prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; spam_Train &lt;- spam[Train_Test=="Train",]
&gt; spam_TestX &lt;- within(spam[Train_Test=="Test",],rm(type))
&gt; spam_TestY &lt;- spam[Train_Test=="Test","type"]
&gt; spam_Formula &lt;- as.formula("type~.")</pre></div><p class="calibre7">First, we will build the simple classification tree:</p><div class="informalexample"><pre class="programlisting">&gt; spam_ct &lt;- rpart(spam_Formula,data=spam_Train)
&gt; spam_ct_predict &lt;- predict(spam_ct,newdata=spam_TestX, 
+ type="class")
&gt; ct_accuracy &lt;- sum(spam_ct_predict==spam_TestY)/nrow(spam_TestX)
&gt; ct_accuracy
[1] 0.8994</pre></div><p class="calibre7">The classification tree gives a modest accuracy of about 90%. We will then apply <code class="literal">randomForest</code> and build the random forest:</p><div class="informalexample"><pre class="programlisting">&gt; spam_rf &lt;- randomForest(spam_Formula,data=spam_Train,coob=TRUE,
+                         ntree=500,keepX=TRUE,mtry=5)
&gt; spam_rf_predict &lt;- predict(spam_rf,newdata=spam_TestX, 
+ type="class")
&gt; rf_accuracy &lt;- sum(spam_rf_predict==spam_TestY)/nrow(spam_TestX)
&gt; rf_accuracy
[1] 0.9436</pre></div><p class="calibre7">Bagging can be performed with the <code class="literal">randomForest</code> function. The trick is to ask the random forest to use all the variables while <a id="id192" class="calibre1"/>setting up a split. Thus, the choice of <code class="literal">mtry=ncol(spal_TestX)</code> will select all the variables and bagging is then easily performed:</p><div class="informalexample"><pre class="programlisting">&gt; spam_bag &lt;- randomForest(spam_Formula,data=spam_Train,coob=TRUE,
+ ntree=500,keepX=TRUE,mtry=ncol(spam_TestX))
&gt; spam_bag_predict &lt;- predict(spam_bag,newdata=spam_TestX,
+ type="class")
&gt; bag_accuracy &lt;- sum(spam_bag_predict==spam_TestY)/
+ nrow(spam_TestX)
&gt; bag_accuracy
[1] 0.935
&gt; windows(height=100,width=200)
&gt; par(mfrow=c(1,2))
&gt; plot(spam_rf,main="Random Forest for Spam Classification")
&gt; plot(spam_bag,main="Bagging for Spam Classification")</pre></div><p class="calibre7">The increase in accuracy is also reflected in the accuracy plots, as shown in the following figure:</p><div class="mediaobject"><img src="../images/00197.jpeg" alt="Comparisons with bagging" class="calibre10"/><div class="caption"><p class="calibre14">Figure 11: Random Forest and Bagging Comparisons for the Spam Classification Problem</p></div></div><p class="calibre11"> </p><p class="calibre7">We will look at some niche applications of random forests in the concluding two sections.</p></div>
<div class="book" title="Missing data imputation" id="1565U1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec37" class="calibre1"/>Missing data imputation</h1></div></div></div><p class="calibre7">Missing data is<a id="id193" class="calibre1"/> a menace! It pops up out of nowhere and blocks <a id="id194" class="calibre1"/>analysis until it is properly taken care of. The statistical technique of the expectation-maximization algorithm, or simply the EM algorithm, needs a lot of information on the probability distributions, structural relationship, and in-depth details of statistical models. However, an approach using the EM algorithm is completely ruled out here. Random forests can be used to overcome the missing data problem.</p><p class="calibre7">We will use the <code class="literal">missForest</code> R package to fix the missing data problem whenever we come across it in the rest of the book. The<a id="id195" class="calibre1"/> algorithm for the <code class="literal">missForest</code> function and other details can be found at <a class="calibre1" href="https://academic.oup.com/bioinformatics/article/28/1/112/219101">https://academic.oup.com/bioinformatics/article/28/1/112/219101</a>. For any variable/column with missing data, the technique is to build a random forest for that variable and obtain the OOB prediction as the imputation error estimates. Note that the function can handle continuous as well as categorical missing values. The creators of the package have enabled the functions with parallel run capability to save time.</p><p class="calibre7">We will take a simple dataset from <a class="calibre1" href="https://openmv.net/info/travel-times">https://openmv.net/info/travel-times</a>, and there are missing values in the data. The data consists of <code class="literal">13</code> variables and <code class="literal">205</code> observations. Of the <code class="literal">13</code> variables available, only the <code class="literal">FuelEconomy</code> variable has missing values. Let's explore the dataset in the R terminal:</p><div class="informalexample"><pre class="programlisting">&gt; TT &lt;- read.csv("../Data/Travel_Times.csv")
&gt;dim(TT)
[1] 205  13
&gt;sum(is.na(TT))
[1] 19
&gt;sapply(TT,function(x) sum(is.na(x)))
          Date      StartTime      DayOfWeek        GoingTo       Distance 
             0              0              0              0              0 
      MaxSpeed       AvgSpeed AvgMovingSpeed    FuelEconomy      TotalTime 
             0              0              0             19              0 
    MovingTime     Take407All       Comments 
             0              0              0 
&gt; TT$FuelEconomy
  [1]    NA    NA    NA    NA    NA    NA    NA    NA  8.89  8.89  8.89  8.89
 [13]  8.89  8.89  8.89  8.89  9.08  9.08  9.08  9.08  9.08  9.08  9.08  9.08
 [25]  9.76  9.76  9.76  9.76  9.76  9.76  9.76  9.16  9.16  9.16    NA    NA
 [37]    NA    NA    NA    NA    NA    NA  9.30  9.30  9.30  9.30  9.30  9.30
 [49] 10.05 10.05 10.05 10.05  9.53  9.53  9.53  9.53  9.53  9.53  9.53  9.53
 [61]  9.35  9.35  9.35  9.35  9.35  9.35  9.35  9.35  8.32  8.32  8.32  8.32

[181]  8.48  8.48  8.48  8.45  8.45  8.45  8.45  8.45  8.45  8.45  8.45  8.45
[193]  8.45  8.28  8.28  8.28  7.89  7.89  7.89  7.89  7.89  7.89    NA    NA
[205]    NA</pre></div><p class="calibre7">It can be seen that<a id="id196" class="calibre1"/> there are <code class="literal">19</code> observations with missing values. The <code class="literal">sapply</code> function tells us that all <code class="literal">19</code> observations <a id="id197" class="calibre1"/>have missing values for the <code class="literal">FuelEconomy</code> variable only. The <code class="literal">missForest</code> function is now deployed in action:</p><div class="informalexample"><pre class="programlisting">&gt; TT_Missing &lt;- missForest(TT[,-c(1,2,12)],
+                          maxiter = 10,ntree=500,mtry=6)
missForest iteration 1 in progress...done!
missForest iteration 2 in progress...done!
missForest iteration 3 in progress...done!
missForest iteration 4 in progress...done!
&gt; TT_FuelEconomy &lt;- cbind(TT_Missing$ximp[,7],TT$FuelEconomy)
&gt; TT_FuelEconomy[is.na(TT$FuelEconomy),]
      [,1] [,2]
 [1,] 8.59   NA
 [2,] 8.91   NA
 [3,] 8.82   NA
 [4,] 8.63   NA
 [5,] 8.44   NA
 [6,] 8.63   NA
 [7,] 8.60   NA
 [8,] 8.50   NA
 [9,] 9.07   NA
[10,] 9.10   NA
[11,] 8.52   NA
[12,] 9.12   NA
[13,] 8.53   NA
[14,] 8.85   NA
[15,] 8.70   NA
[16,] 9.42   NA
[17,] 8.40   NA
[18,] 8.49   NA
[19,] 8.64   NA</pre></div><p class="calibre7">We have now imputed the missing values. It needs to be noted that the imputed values should make sense and should not look out of place. In <a class="calibre1" title="Chapter 9. Ensembling Regression Models" href="part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee">Chapter 9</a>, <span class="strong"><em class="calibre9">Ensembling Regression Models</em></span>, we will use the <code class="literal">missForest</code> function to impute a lot of missing values.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: How can the imputed values be validated? Use the <code class="literal">prodNA</code> function from the <code class="literal">missForest</code> package and puncture good values with missing data. Using the <code class="literal">missForest</code> function, get the imputed values and compare them with the original values.</p><p class="calibre7">The proximity matrix tells us <a id="id198" class="calibre1"/>how close the observations are<a id="id199" class="calibre1"/> from the random forest perspective. If we have information about the observations neighborhood, we can carry out a cluster analysis. As a by-product of using the proximity matrix, we can now also use random forests for unsupervised problems.</p></div>
<div class="book" title="Clustering with Random Forest" id="164MG1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec38" class="calibre1"/>Clustering with Random Forest</h1></div></div></div><p class="calibre7">Random forests can be set up without the target variable. Using this feature, we will calculate the proximity matrix<a id="id200" class="calibre1"/> and use the OOB proximity values. Since the proximity matrix gives us a measure of closeness between the observations, it can be converted into clusters using hierarchical clustering methods.</p><p class="calibre7">We begin with the setup of <code class="literal">y = NULL</code> in the <code class="literal">randomForest</code> function. The options of <code class="literal">proximity=TRUE</code> and <code class="literal">oob.prox=TRUE</code> are specified to ensure that we obtain the required proximity matrix:</p><div class="informalexample"><pre class="programlisting">&gt;data(multishapes)
&gt;par(mfrow=c(1,2))
&gt;plot(multishapes[1:2],col=multishapes[,3], 
+      main="Six Multishapes Data Display")
&gt; MS_RF &lt;- randomForest(x=multishapes[1:2],y=NULL,ntree=1000,
+ proximity=TRUE, oob.prox=TRUE,mtry = 1)</pre></div><p class="calibre7">Next, we use the <code class="literal">hclust</code> function with the option of <code class="literal">ward.D2</code> to carry out the hierarchical cluster analysis on the proximity matrix of dissimilarities. The <code class="literal">cutree</code> function divides the <code class="literal">hclust</code> object into <code class="literal">k = 6</code> number of clusters. Finally, the <code class="literal">table</code> function and the visuals give an idea of how good the clustering has been by using the random forests:</p><div class="informalexample"><pre class="programlisting">&gt; MS_hclust &lt;- hclust(as.dist(1-MS_RF$proximity),method="ward.D2")
&gt; MS_RF_clust &lt;- cutree(MS_hclust,k=6)
&gt;table(MS_RF_clust,multishapes$shape)

MS_RF_clust   1   2   3   4   5   6
          1 113   0   0   0  10   0
          2 143   0   0   0  20  50
3  57 170   0   0   3   0
4  63  55   0   0   3   0
5  24 175   0   0   2   0
          6   0   0 100 100  12   0
&gt;plot(multishapes[1:2],col=MS_RF_clust,
+      main="Clustering with Random Forest</pre></div><p class="calibre7">The following is a diagram illustrating clustering using random forests:</p><div class="mediaobject"><img src="../images/00198.jpeg" alt="Clustering with Random Forest" class="calibre10"/><div class="caption"><p class="calibre14">Figure 12: Clustering Using Random Forests</p></div></div><p class="calibre11"> </p><p class="calibre7">Although the clusters <a id="id201" class="calibre1"/>provided by the random forests do not fit the label identification problem, we will take them as a starting point. It needs to be understood that random forests can be used properly for cluster analysis.</p></div>
<div class="book" title="Summary" id="173721-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec39" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">Random forests were created as an improvement on the bagging method. As an example of the homogeneous ensemble method, we saw how the forests help in obtaining higher accuracy. Visualization and variable importance for random forests were thoroughly detailed. We also saw a lot of diagnostic methods that can be used after fitting a random forest. The method was then compared with bagging. Novel applications of random forest for missing data imputation and cluster analysis were also demonstrated.</p><p class="calibre7">In the next chapter, we will look at boosting, which is a very important ensemble.</p></div></body></html>