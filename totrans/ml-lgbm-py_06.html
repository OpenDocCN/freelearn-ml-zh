<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer064">
<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor094"/>6</h1>
<h1 id="_idParaDest-93"><a id="_idTextAnchor095"/>Solving Real-World Data Science Problems with LightGBM</h1>
<p>With the preceding chapters, we have slowly been building out a toolset for us to be able to solve machine learning problems. We’ve seen examples of examining our data, addressing data issues, and creating models. This chapter formally defines and applies the data science process to two <span class="No-Break">case studies.</span></p>
<p>The chapter gives a detailed overview of the data science life cycle and all the steps it encompasses. The concepts of problem definition, data exploration, data cleaning, modeling, and reporting are discussed in a regression and classification problem context. We also look at preparing data for modeling and building optimized LightGBM models using our learned techniques. Finally, we look deeper at utilizing a trained model as an introduction<a id="_idIndexMarker353"/> to <strong class="bold">machine learning </strong><span class="No-Break"><strong class="bold">operations</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MLOps</strong></span><span class="No-Break">).</span></p>
<p>The main topics of this chapter are <span class="No-Break">as follows:</span></p>
<ul>
<li>The data science <span class="No-Break">life cycle</span></li>
<li>Predicting wind turbine power generation <span class="No-Break">with LightGBM</span></li>
<li>Classifying individual credit scores <span class="No-Break">with LightGBM</span></li>
</ul>
<h1 id="_idParaDest-94"><a id="_idTextAnchor096"/>Technical requirements</h1>
<p>The chapter includes examples and code excerpts illustrating how to perform parameter optimization studies for LightGBM using Optuna. Complete examples and instructions for setting up a suitable environment for this chapter are available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-6"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-6</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor097"/>The data science life cycle</h1>
<p>Data science<a id="_idIndexMarker354"/> has emerged as a critical discipline, enabling organizations to derive valuable insights from their data and drive better decision-making. At the heart of data science lies the data science life cycle, a systematic, iterative process that guides data-driven problem-solving across various industries and domains. This life cycle outlines a series of steps that data scientists follow to ensure they address the right problem and deliver actionable insights that create <span class="No-Break">real-world impact.</span></p>
<p>The first stage of the data science life cycle involves defining the problem, which entails understanding the business context, articulating objectives, and formulating hypotheses. This crucial stage establishes the entire project’s foundation by establishing a clear direction and scope. Subsequent stages in the life cycle focus on data collection, preparation, and exploration, collectively involving gathering relevant data, cleaning and preprocessing it, and conducting exploratory data analysis to unveil patterns <span class="No-Break">and trends.</span></p>
<p>Once the data is analyzed, the data science life cycle progresses to model selection, training, evaluation, and tuning. These stages are central to developing accurate and reliable predictive or descriptive models by choosing the most appropriate algorithms, training them on the preprocessed data, and optimizing their performance. The goal is to build a robust model that generalizes well to unseen data and addresses the problem at <span class="No-Break">hand effectively.</span></p>
<p>Lastly, the data science life cycle emphasizes the importance of deploying the final model into a production environment, monitoring its performance, and maintaining it to ensure its ongoing relevance and accuracy. Equally important is the communication of results and insights to stakeholders, which is vital for driving informed decision-making and realizing the full potential of data science. By following the data science life cycle, organizations can systematically extract value from their data and unlock new opportunities for growth <span class="No-Break">and innovation.</span></p>
<p>With previous<a id="_idIndexMarker355"/> examples, we followed a loose recipe of steps for working with data and creating models. In the next section, we formally define and discuss the steps of the data science <span class="No-Break">life cycle.</span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor098"/>Defining the data science life cycle</h2>
<p>The <a id="_idIndexMarker356"/>following are the key steps broadly applied in the data science life cycle. The steps are also shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>, which illustrates the cyclical nature of the <span class="No-Break">life cycle.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<img alt="Figure 6.1 – Diagram depicting the data science life cycle" height="1332" src="image/B16690_06_01.jpg" width="1230"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Diagram depicting the data science life cycle</p>
<p>These are the <span class="No-Break">key steps:</span></p>
<ol>
<li><strong class="bold">Define the problem</strong>: Clearly<a id="_idIndexMarker357"/> articulate the business problem, goals, and objectives. This stage involves understanding stakeholder<a id="_idIndexMarker358"/> requirements, formulating hypotheses, and determining the project’s scope. Defining the problem also sets the stage for data collection and can determine how we’ll utilize <span class="No-Break">our models.</span></li>
<li><strong class="bold">Data collection</strong>: Gather the <a id="_idIndexMarker359"/>required data from various sources, such as databases, APIs, web scraping, or third-party data providers. Ensure the data is representative, accurate, and relevant to the problem. It is important to document where <a id="_idIndexMarker360"/>data originates and how it is moved around to establish the <strong class="bold">data lineage</strong>. Further, build a <strong class="bold">data dictionary</strong> that <a id="_idIndexMarker361"/>documents the data’s format, structure, content, and meaning. Importantly, validate any potential bias in the collection or sampling <span class="No-Break">of data.</span></li>
<li><strong class="bold">Data preparation</strong>: Clean and <a id="_idIndexMarker362"/>preprocess the data to make it suitable for analysis. This stage includes tasks such<a id="_idIndexMarker363"/> as <strong class="bold">data cleansing</strong> (e.g., handling missing values and removing duplicates), <strong class="bold">data transformation</strong> (e.g., normalization<a id="_idIndexMarker364"/> and encoding categorical variables), and <strong class="bold">feature engineering</strong> (e.g., creating new variables or aggregating existing ones). Moving and<a id="_idIndexMarker365"/> joining the data to where it can be analyzed and modeled might also <span class="No-Break">be necessary.</span></li>
<li><strong class="bold">Data exploration</strong>: Conduct <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) to gain insights into the data. This<a id="_idIndexMarker366"/> step involves visualizing data<a id="_idIndexMarker367"/> distributions, identifying trends and patterns, detecting outliers and anomalies, and checking for relationships and correlations <span class="No-Break">between features.</span></li>
<li><strong class="bold">Model selection</strong>: Choose the <a id="_idIndexMarker368"/>most appropriate data modeling techniques based on the problem type (e.g., regression, classification, or clustering) and the data characteristics. It’s important to choose multiple model algorithms to validate performance on the <span class="No-Break">data set.</span></li>
<li><strong class="bold">Model training</strong>: Train <a id="_idIndexMarker369"/>the selected models using the prepared <a id="_idIndexMarker370"/>data. This step involves splitting the data into training and validation sets, setting model parameters (hyperparameters), and fitting the models to <span class="No-Break">the data.</span></li>
<li><strong class="bold">Model evaluation</strong>: Assess the <a id="_idIndexMarker371"/>performance of the trained models using appropriate <a id="_idIndexMarker372"/>evaluation metrics (e.g., accuracy, precision, recall, F1-score, <strong class="bold">Area under the ROC Curve</strong> (<strong class="bold">AUC-ROC</strong>), or <strong class="bold">root mean square error</strong> (<strong class="bold">RMSE</strong>)) and <a id="_idIndexMarker373"/>compare them to select the best-performing model(s). Perform cross-validation or use holdout test sets to ensure an <span class="No-Break">unbiased evaluation.</span></li>
<li><strong class="bold">Model tuning</strong>: Fine-tune the<a id="_idIndexMarker374"/> selected model by optimizing hyperparameters, feature selection, or incorporating domain knowledge. This step aims to improve the model’s performance and generalization to unseen data. It might also be appropriate to tune the model for the specific problem; for instance, when recognizing faces, a higher precision is more appropriate than a <span class="No-Break">high recall.</span></li>
<li><strong class="bold">Model deployment</strong>: If the <a id="_idIndexMarker375"/>model is to be part of a more extensive software system, deploy the final model into a production environment, where it can be used to make predictions or inform decision-making. Deployment may involve integrating the model into existing systems, creating APIs, or setting up monitoring and <span class="No-Break">maintenance procedures.</span></li>
<li><strong class="bold">Model monitoring and maintenance</strong>: Continuously monitor the model’s performance and update it as <a id="_idIndexMarker376"/>necessary to ensure it remains accurate and relevant. Techniques such as detecting model and data drift should be used to ensure model performance. Model maintenance may involve retraining the model with new data, updating features, or refining the <span class="No-Break">problem definition.</span></li>
<li><strong class="bold">Communicate results</strong>: Share insights and<a id="_idIndexMarker377"/> results with stakeholders, including any recommendations or actions based on the analysis. Communicating the results may involve creating visualizations, dashboards, or reports to communicate the<a id="_idIndexMarker378"/> <span class="No-Break">findings effectively.</span></li>
</ol>
<p>We now examine two case studies to see how the data science life cycle is applied practically to real-world data. We look at a regression problem, predicting wind turbine power generation, and a classification problem, classifying individual <span class="No-Break">credit scores.</span></p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor099"/>Predicting wind turbine power generation with LightGBM</h1>
<p>Our first case<a id="_idIndexMarker379"/> study is a problem where we aim to predict the power generation of wind turbines. The dataset for the problem is available <span class="No-Break">from </span><a href="https://www.kaggle.com/datasets/mukund23/hackerearth-machine-learning-challenge"><span class="No-Break">https://www.kaggle.com/datasets/mukund23/hackerearth-machine-learning-challenge</span></a><span class="No-Break">.</span></p>
<p>We work through the problem using the steps defined in the previous section, articulating the details involved in each step alongside code snippets. The complete end-to-en<a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-6/wind-turbine-power-output.ipynb">d solution is available <span class="No-Break">at </span><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-6/wind</span></a><span class="No-Break">-turbine-power-output.ipynb</span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor100"/>Problem definition</h2>
<p>The dataset consists of power <a id="_idIndexMarker380"/>generation (in kW/h) measurements of wind turbines taken at a specific date and time. Alongside each measurement are the parameters of the wind turbine, which include physical measurements of the windmill (including windmill height, blade breadth, and length), operating measurements for the turbine (including resistance in ohms, motor torque, generator temperature, and rotor torque) and atmospheric conditions (including wind speed, temperature, <span class="No-Break">and pressure).</span></p>
<p>Given the <a id="_idIndexMarker381"/>set of parameters, we must build a regression model to predict the generated power in kW/h. Therefore, we employ regression modeling. The quality of the model is measured using the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) and the coefficient of determination (<span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>). We must also determine <a id="_idIndexMarker382"/>which factors have the most significant impact on <span class="No-Break">power generation.</span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Data collection</h2>
<p>The dataset <a id="_idIndexMarker383"/>comprises 22,800 samples collected within 11 months, from October 2018 to September 2019. The data is available as CSV files and is released as public domain data. No additional data <span class="No-Break">is collected.</span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor102"/>Data preparation</h2>
<p>We can now look at preparing<a id="_idIndexMarker384"/> the data for cleaning and exploration. The dataset consists of 18 numerical features, 2 categorical features, and the date feature, as we can see by getting the information from our <span class="No-Break">pandas DataFrame:</span></p>
<pre class="source-code">
train_df.info()
#   Column                          Non-Null Count  Dtype
---  ------                          --------------  -----
 0   tracking_id                     28200 non-null  object
 1   datetime                        28200 non-null  object
 2   wind_speed(m/s)                 27927 non-null  float64
 3   atmospheric_temperature(°C)     24750 non-null  float64
 4   shaft_temperature(°C)           28198 non-null  float64
...
 20  windmill_height(m)              27657 non-null  float64
 21  windmill_generated_power(kW/h)  27993 non-null  float64</pre>
<p>We can<a id="_idIndexMarker385"/> immediately see that the dataset has missing values, with some features having fewer than 28,200 values. We can get a better sense of the data distribution by calculating the <span class="No-Break">statistical description:</span></p>
<pre class="source-code">
train_df.describe().T.style.bar(subset=['mean'])</pre>
<p>This prints out <span class="No-Break">the following:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-6">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">count</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">mean</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">std</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">min</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">max</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">wind_speed(m/s)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">27927</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">69.04</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">76.28</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">496.21</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">601.46</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">atmospheric_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">24750</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.38</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">44.28</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">99.00</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">80.22</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">shaft_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28198</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40.09</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">27.20</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">99.00</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">169.82</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">blades_angle(°)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">27984</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">9.65</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">47.92</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">146.26</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">165.93</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">gearbox_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28199</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">41.03</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">43.66</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">244.97</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">999.00</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">engine_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28188</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">42.61</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">6.12</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">3.17</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">50.00</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">motor_torque(N-m)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28176</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">1710</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">827</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">500</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">3000.00</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">generator_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28188</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">65.03</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">19.82</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">33.89</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">100.00</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">atmospheric_pressure(Pascal)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">25493</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">53185</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">187504</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">1188624</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">1272552</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">area_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28200</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">32.74</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">7.70</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">30.00</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">55.00</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">windmill_body_temperature(°C)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">25837</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">20.80</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">54.36</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">999.00</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">323.00</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">wind_direction(°)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">23097</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">306.89</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">134.06</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.00</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">569.97</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">resistance(ohm)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28199</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">1575.6</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">483.33</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">1005.22</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">4693.48</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">rotor_torque(N-m)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">27628</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">25.85</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">32.42</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">136.73</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">236.88</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">blade_length(m)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">23107</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">2.25</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">11.28</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">99.00</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">18.21</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">blade_breadth(m)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">28200</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.40</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.06</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.20</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.50</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">windmill_height(m)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">27657</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">25.89</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">7.77</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">30.30</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">78.35</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">windmill_generated_power (kW/h)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">27993</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">6.13</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">2.70</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.96</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">20.18</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 – Statistical description of numerical features in the Wind Turbine dataset</p>
<p>Looking at<a id="_idIndexMarker386"/> the statistical description of the features in <em class="italic">Table 6.1</em>, we can see the <span class="No-Break">following irregularities:</span></p>
<ul>
<li><strong class="bold">Many features have outliers</strong>: Generally, a standard deviation larger than the mean may indicate outlying values. Examples include wind speed, atmospheric temperature, and atmospheric pressure. Similarly, a minimum or maximum far away from the mean may indicate outliers in the data. We can further verify this by visualizing the data distribution using <span class="No-Break">a histogram.</span></li>
<li><strong class="bold">Physical impossibilities</strong>: The data shows impossibilities in the data of some of the measurements: lengths (in meters) less than 0 (e.g., blade length) and temperatures outside of natural ranges (a body temperature <span class="No-Break">of -999).</span></li>
<li><strong class="bold">Repeated values</strong>: The values <strong class="source-inline">-99.0</strong> and <strong class="source-inline">-999.0</strong> repeat for a few features. It’s improbable that these values occur naturally across features. We can infer that these indicate missing or erroneous measurements in <span class="No-Break">the sample.</span></li>
</ul>
<p>We can <a id="_idIndexMarker387"/>visualize the distribution of features to investigate the outlying values. For example, for atmospheric temperature, we have <span class="No-Break">the following:</span></p>
<pre class="source-code">
sns.histplot(train_df["atmospheric_temperature(°C)"], bins=30)</pre>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="Figure 6.2 – Histogram showing atmospheric temperature in Celsius" height="530" src="image/B16690_06_02.jpg" width="1002"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Histogram showing atmospheric temperature in Celsius</p>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em> illustrates two of the issues found in the data: a high frequency of measurements with a value of precisely <strong class="source-inline">-99.0</strong>, indicating an error. A few outlying values are also far removed from <span class="No-Break">the mean.</span></p>
<p>Finally, we can check for duplicate data using <span class="No-Break">the following:</span></p>
<pre class="source-code">
train_df[train_df.duplicated()]</pre>
<p>No rows are <a id="_idIndexMarker388"/>returned, indicating no duplicates in <span class="No-Break">the dataset.</span></p>
<h3>Data cleaning</h3>
<p>We have<a id="_idIndexMarker389"/> identified multiple issues in the dataset that we need to address as part of the <em class="italic">data </em><span class="No-Break"><em class="italic">cleaning</em></span><span class="No-Break"> step:</span></p>
<ul>
<li><strong class="bold">Outliers</strong>: Many<a id="_idIndexMarker390"/> features have outlier values that skew the distribution of values for <span class="No-Break">the feature.</span></li>
<li><strong class="bold">Measurement errors</strong>: Some <a id="_idIndexMarker391"/>features have values that fall out of the bounds of physical impossibilities (lengths smaller than 0 or temperatures in <span class="No-Break">impossible ranges).</span></li>
<li><strong class="bold">Missing values</strong>: Many of the<a id="_idIndexMarker392"/> features have missing values. There are also particular values (<strong class="source-inline">-99.0 </strong>and <strong class="source-inline">-999.0</strong>) that we <span class="No-Break">consider missing.</span></li>
</ul>
<p>We first address the outliers and measurement errors, as this impacts how we handle the <span class="No-Break">missing values.</span></p>
<h4>Dealing with outliers</h4>
<p>There are two parts to <a id="_idIndexMarker393"/>dealing with outliers in a dataset: accurately identifying outliers and choosing appropriate values for replacement. Ways to identify outliers are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Visualization</strong>: As shown previously, histograms or other plots that visualize the data distribution (such as box plots or scatter plots) can <span class="No-Break">be used</span></li>
<li><strong class="bold">Domain knowledge</strong>: Like how we identify measurement errors, domain knowledge can be leveraged to decide whether values <span class="No-Break">are outlying</span></li>
<li><strong class="bold">Statistical analysis</strong>: Two popular methods for determining whether values are outlying using statistics are <a id="_idIndexMarker394"/>the <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>) and the <span class="No-Break">standard deviation.</span></li>
</ul>
<p>The IQR is the difference between the 25th and 75th quartiles. Values over 1.5 times the IQR away from the 25th or 75th quartiles are <span class="No-Break">considered outlying.</span></p>
<p>Alternatively, we can leverage the standard deviation: we calculate the mean and standard deviation of the dataset. Any values over two or three times removed from the mean are outlying. Setting bounds to twice or three times the standard deviation depends on the underlying data. Using twice the standard deviation could lead to many false positives, but it is appropriate if a lot of data is centered around the mean. Using three times the standard deviation is more conservative and only marks values very far from the mean <span class="No-Break">as outlying.</span></p>
<p>When the outlying values are detected, we must do something about them. Generally, our options are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Remove</strong>: If an outlier results from an error in data entry, measurement, or collection, it may be reasonable to remove it from the dataset. However, this should be done cautiously, as removing too many data points can lead to a loss of information and biased results. Removing instances with outliers in our dataset would result in close to 70% data loss and isn’t <span class="No-Break">an option.</span></li>
<li><strong class="bold">Impute</strong>: Similar to missing values, replace the outlier value with a more representative value, such as the mean, median, or mode of the variable, or use more sophisticated imputation methods <a id="_idIndexMarker395"/>such<a id="_idIndexMarker396"/> as <strong class="bold">k-nearest neighbors</strong> or <span class="No-Break"><strong class="bold">regression-based imputation</strong></span><span class="No-Break">.</span></li>
<li><strong class="bold">Cap or truncate</strong>: Set a threshold (either upper or lower) and cap or truncate the outlier values at that threshold. This method retains the data’s original structure while reducing the influence of <span class="No-Break">extreme values.</span></li>
</ul>
<p>For the Wind <a id="_idIndexMarker397"/>Turbine dataset, we detect the outliers using bounds set to three times the standard deviation and map the values to <strong class="source-inline">np.nan</strong>, so we may replace <span class="No-Break">them later:</span></p>
<pre class="source-code">
column_data = frame[feature]
column_data = column_data[~np.isnan(column_data)]
mean, std = np.mean(column_data), np.std(column_data)
lower_bound = mean - std * 3
upper_bound = mean + std * 3
frame.loc[((frame[feature] &lt; lower_bound) | (frame[feature] &gt; upper_bound))] = np.nan</pre>
<h4>Handling measurement errors</h4>
<p>The values<a id="_idIndexMarker398"/> detected as types of measurement errors could also be considered outliers, although not in the statistical sense of the word. However, we can handle these values slightly differently than with the <span class="No-Break">statistical outliers.</span></p>
<p>We apply our domain knowledge and some research surrounding the weather to identify appropriate ranges for these features. We then cap the erroneous values to <span class="No-Break">these ranges:</span></p>
<pre class="source-code">
frame.loc[frame["wind_speed(m/s)"] &lt; 0, "wind_speed(m/s)"] = 0
frame.loc[frame["wind_speed(m/s)"] &gt; 113, "wind_speed(m/s)"] = 113
frame.loc[frame["blade_length(m)"] &lt; 0, "blade_length(m)"] = 0
frame.loc[frame["windmill_height(m)"] &lt; 0, "windmill_height(m)"] = 0
frame.loc[frame["resistance(ohm)"] &lt; 0, "resistance(ohm)"] = 0</pre>
<p>Here, we set any negative lengths, heights, and electrical resistance to <strong class="source-inline">0</strong>. We also cap the windspeed to <strong class="source-inline">113</strong> m/s, the maximum gust speed <span class="No-Break">on record.</span></p>
<p>Finally, we can deal with the missing values in <span class="No-Break">the dataset.</span></p>
<h4>Handling missing values</h4>
<p>We have<a id="_idIndexMarker399"/> discussed working with missing values in earlier chapters. To summarize here, some of the potential approaches we can take are <span class="No-Break">as follows:</span></p>
<ul>
<li>Remove instances with <span class="No-Break">missing values</span></li>
<li>Impute the missing values using descriptive statistics (the mean, median, <span class="No-Break">or mode)</span></li>
<li>Use other machine learning algorithms, typically unsupervised techniques such as clustering, to calculate more <span class="No-Break">robust statistics</span></li>
</ul>
<p>Removing the <a id="_idIndexMarker400"/>missing values would discard a significant portion of our dataset. Here, we decide to replace missing values using descriptive statistics to retain as much data <span class="No-Break">as possible.</span></p>
<p>First, we mark the <strong class="source-inline">-99.0</strong> and <strong class="source-inline">-999.0</strong> values <span class="No-Break">as missing:</span></p>
<pre class="source-code">
df.loc[frame[f] == -99.0, f] = np.nan
df.loc[frame[f] == 99.0, f] = np.nan
df.loc[frame[f] == -999.0, f] = np.nan
df.loc[frame[f] == 999.0, f] = np.nan</pre>
<p>We then replace missing numerical values with the mean and categorical values with <span class="No-Break">the mode:</span></p>
<pre class="source-code">
if f in numerical_columns:
    frame[f].fillna(frame[f].mean(), inplace=True)
else:
    frame[f].fillna(frame[f].mode()[0], inplace=True)</pre>
<p>Usually, we would have to be careful when using the mean since the mean is affected by outliers. However, since we have already marked the outlier values as <strong class="source-inline">np.nan</strong>, they are excluded when calculating the mean. An additional caveat comes into play when replacing missing values in the test set: since the test set should be treated as unseen data, we must use the mean from the training dataset to replace missing values in the <span class="No-Break">test set.</span></p>
<p>This concludes the necessary data cleaning for our dataset. We should validate our work by rechecking for missing values and recalculating the descriptive statistics and <span class="No-Break">data histograms.</span></p>
<p>With the <a id="_idIndexMarker401"/>dataset clean, we can move to the next data preparation step: <span class="No-Break">feature engineering.</span></p>
<h3>Feature engineering</h3>
<p><strong class="bold">Feature engineering</strong> refers<a id="_idIndexMarker402"/> to the process of creating new features or modifying existing ones to <a id="_idIndexMarker403"/>improve the performance of a machine learning model. In essence, feature engineering is using domain knowledge and data understanding to create features that make machine learning algorithms work more effectively. It’s an art as much as a science, requiring creativity, intuition, and a deep understanding of <span class="No-Break">the problem.</span></p>
<p>The feature engineering process often starts with exploring the data to understand its characteristics, distributions, and relationships between variables. This exploration phase can reveal potential opportunities for feature creation, such as interaction terms, aggregate features, or temporal features; for example, if you’re working with a dataset containing customer transaction data, you might engineer features that capture the frequency of transactions, the average transaction value, or the time since the <span class="No-Break">last transaction.</span></p>
<p>There are also several standard techniques used in feature engineering. These include encoding categorical variables, normalizing numerical variables, creating polynomial features, and binning continuous variables. For instance, categorical variables are often encoded into numerical formats (such as one-hot or ordinal encoding) to be used in mathematical models. Similarly, numerical variables are often normalized (such as min-max scaling or standardization) to ensure they’re on a comparable scale and to prevent certain variables from dominating others simply because of <span class="No-Break">their scale.</span></p>
<p>However, feature engineering is not a one-size-fits-all process. The appropriate features for a model can depend heavily on the specific problem, the algorithm used, and the nature of the data. Therefore, feature engineering often requires iterative experimentation and evaluation. Despite its challenges, effective feature engineering can significantly enhance <span class="No-Break">model performance.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">As you may have noticed, feature engineering requires understanding and exploration of the data, which depends on the engineered features’ availability. This highlights the cyclical process within the data science life cycle: we iterate between data preparation <span class="No-Break">and exploration.</span></p>
<p>For example, a feature suitable for further engineering in our data is the <strong class="source-inline">datetime</strong> field. The specific date and time a measurement was taken are not informative to the model for <span class="No-Break">future predictions.</span></p>
<p>However, suppose we extract the year, month, day of the month, and hour of the day into new features. In that case, the model can capture potential relationships between the power generated and different time cycles. The <strong class="source-inline">date</strong> decomposition allows questions such as: Does the time of year, seasons, specific months, and so on influence the power generated? Or does the time of day, morning, noon, or night have <span class="No-Break">any impact?</span></p>
<p>We can decompose the date into new features <span class="No-Break">as follows:</span></p>
<pre class="source-code">
frame["date_year"] = train_df["datetime"].dt.year
frame["date_month"] = train_df["datetime"].dt.month
frame["date_day"] = train_df["datetime"].dt.day
frame["date_hour"] = train_df["datetime"].dt.hour
frame = frame.drop(columns=["tracking_id", "datetime"], axis=1)</pre>
<p>If, after modeling, we find these features to be uninformative, we can do further work with the <strong class="source-inline">time</strong> fields to assist in modeling. Future directions to explore are aggregations based on specific periods or creating a time series by ordering measurements to study trends in power <a id="_idIndexMarker404"/>generation <span class="No-Break">over time.</span></p>
<p>We now proceed to our case study’s EDA portion to visualize and better understand <span class="No-Break">our data.</span></p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor103"/>EDA</h2>
<p>We have already done some <a id="_idIndexMarker405"/>EDA to find missing values and outliers in our dataset. There is no fixed methodology for performing EDA on a dataset; some experience and creativity are required to guide <span class="No-Break">the process.</span></p>
<p>Besides <a id="_idIndexMarker406"/>gaining insight and understanding of the data, the main goal is to attempt to identify patterns and relationships within the data. Here, we start with a correlation heatmap to explore any direct correlations <span class="No-Break">between features:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="Figure 6.3 – Correlation heatmap for the Wind Turbine dataset" height="934" src="image/B16690_06_03.jpg" width="1067"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Correlation heatmap for the Wind Turbine dataset</p>
<p>The correlation heatmap in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> shows a few notable correlations between wind speed and atmospheric temperature, engine metrics such as engine temperature, generator temperature, and motor torque, and weaker correlations between our date features and <span class="No-Break">atmospheric conditions.</span></p>
<p>Notably, a<a id="_idIndexMarker407"/> very strong correlation exists between motor torque and generator temperature. Intuitively, this makes sense: if the motor produces more torque, it will produce more heat. Since torque is the causative feature, we can consider dropping the generator temperature <span class="No-Break">for modeling.</span></p>
<p>We can also see correlations between power generation and engine metrics, including electrical resistance and wind direction. We can anticipate that these features will have a significant impact on the <span class="No-Break">model’s performance.</span></p>
<p>We can also explore correlations between the categorical features and the power generated. The turbine status has seemingly little effect (in isolation) on the power generated. However, the cloud level has a significant impact. Plotting the cloud level against power generated, we have <span class="No-Break">the following:</span></p>
<pre class="source-code">
train_df.groupby("cloud_level")["windmill_generated_power(kW/h)"].mean().plot.bar()</pre>
<div>
<div class="IMG---Figure" id="_idContainer055">
<img alt="Figure 6.4 – Average power generated under the various cloud levels" height="732" src="image/B16690_06_04.jpg" width="968"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Average power generated under the various cloud levels</p>
<p>As shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em>, extremely low clouds strongly correlate with reduced power generation. In further exploration of the data, it is helpful to control for cloud level to ensure the effect of the cloud level doesn’t dominate any <span class="No-Break">emergent patterns.</span></p>
<p>Another helpful visualization to see the effect of various features is the scatterplot. Plotting each value makes it straightforward to spot features by visually identifying patterns and clusters in <span class="No-Break">the data.</span></p>
<p>Next, we provide examples of scatterplots that reveal patterns within <span class="No-Break">our data.</span></p>
<p>To investigate any effect the blade angle might have on power generation, we can create a scatterplot <span class="No-Break">as follows:</span></p>
<pre class="source-code">
sns.scatterplot(x='blades_angle(°)', y='windmill_generated_power(kW/h)', hue='cloud_level', data=train_df)</pre>
<p>In the scatterplot, we <a id="_idIndexMarker408"/>also add hue differentiation for the cloud level so we can visually verify that any effect isn’t stemming from the cloud <span class="No-Break">level alone:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<img alt="Figure 6.5 – Scatterplot of power generated (y axis) against the blade angle (x axis)" height="750" src="image/B16690_06_05.jpg" width="1035"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Scatterplot of power generated (y axis) against the blade angle (x axis)</p>
<p>The blade<a id="_idIndexMarker409"/> angle scatterplot is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em>. The scatterplot indicates that specific blade angle ranges correlate with increased power generated: [0, 10] degrees and [65, 75] degrees (both reciprocated in the other direction). Tree-based algorithms model correlations such as these <span class="No-Break">as well.</span></p>
<p>Another example that illustrates the power of our feature engineering is a scatterplot of the month against the power generated. We again control for the cloud level via a different hue for <span class="No-Break">those points:</span></p>
<pre class="source-code">
sns.scatterplot(x='date_month',y='windmill_generated_power(kW/h)',hue='cloud_level',data=train_df)</pre>
<div>
<div class="IMG---Figure" id="_idContainer057">
<img alt="Figure 6.6 – Scatterplot of power generated (y axis) by month (x axis)" height="725" src="image/B16690_06_06.jpg" width="999"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Scatterplot of power generated (y axis) by month (x axis)</p>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.6</em> shows that April to September is correlated with a significant decrease in power generated. We could conclude that the wind turbines’ location isn’t particularly windy these months, and other sources would have to supplement the lack of power generation. By decomposing our date feature, we enable our learning algorithm to exploit <span class="No-Break">this correlation.</span></p>
<p>There is no definitive end goal with EDA. For large, complex datasets, the analysis can go deeper and deeper into the data, iteratively exploring further facets and nuances almost indefinitely. However, two sanity checks that are useful in determining whether data has been explored sufficiently are <span class="No-Break">as follows:</span></p>
<ul>
<li>Do we sufficiently understand the meaning of each feature and the potential effect on the <span class="No-Break">model’s output?</span></li>
<li>Is the data well prepared for modeling? To the best of our knowledge, are the features informative, the data clean and unbiased, and formatted so that the model <a id="_idIndexMarker410"/>can be trained <span class="No-Break">on it?</span></li>
</ul>
<p>We now move on to modeling the data, leveraging the techniques of previous chapters to build a <span class="No-Break">well-optimized model.</span></p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor104"/>Modeling</h2>
<p>The first step of <a id="_idIndexMarker411"/>modeling is model selection. It’s best practice to first model the data using a straightforward algorithm to validate our data preparation and establish a baseline. If the modeling fails with a simple algorithm, it’s easier to debug what might be going wrong or isolate data instances that may be causing <span class="No-Break">the issue.</span></p>
<h3>Model selection</h3>
<p>For our wind<a id="_idIndexMarker412"/> turbine data, we use a linear regression model to establish a baseline and validate the data’s suitability <span class="No-Break">for modeling.</span></p>
<p>We also train a random forest regression model as a point of comparison against our LightGBM model. It’s also good practice, if budget allows, to train more than one model using a different learning algorithm, as specific problems may be better suited to <span class="No-Break">particular algorithms.</span></p>
<p>Finally, we train a LightGBM regressor as our <span class="No-Break">primary model.</span></p>
<h3>Model training and evaluation</h3>
<p>From <a id="_idIndexMarker413"/>the EDA, we saw that the generator temperature is redundant (due to the correlation with motor torque). As such, we exclude it from the <span class="No-Break">training data:</span></p>
<pre class="source-code">
X = train_df.drop(columns=["windmill_generated_power(kW/h)", axis=1)
y = train_df["windmill_generated_power(kW/h)"]</pre>
<p>Unlike LightGBM, neither linear regression nor scikit-learn’s random forest regressor can automatically deal with <span class="No-Break">categorical features.</span></p>
<p>We, therefore, use <strong class="source-inline">get_dummies</strong> from pandas to encode the features for training. The <strong class="source-inline">get_dummies</strong> operation performs a process <a id="_idIndexMarker414"/>known as <strong class="bold">one-hot encoding</strong>. The categorical value is decomposed into as many binary (<strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>) columns as there are unique values. The corresponding value is marked with <strong class="source-inline">1</strong> (one-hot) for each pattern, and other values are marked with <strong class="source-inline">0</strong>. For example, consider the cloud level feature: there are three categories (medium, low, and extremely low). A row in our dataset with a medium cloud level would be encoded as <strong class="source-inline">100</strong> (three separate columns). Similarly, a low cloud level is encoded as <strong class="source-inline">010</strong>, and <span class="No-Break">so on.</span></p>
<p>Performing one-hot encoding allows algorithms, such as linear regression, that only support numerical columns to model the data at the cost of increased memory usage for the <span class="No-Break">additional columns.</span></p>
<p>As stated <a id="_idIndexMarker415"/>in the problem definition, we’ll use two metrics to evaluate the models: the coefficient of determination and the MSE. Both are calculated using <span class="No-Break">five-fold cross-validation.</span></p>
<p>We can now proceed to train our linear, random forest, and <span class="No-Break">LightGBM regressors:</span></p>
<pre class="source-code">
X_dummies = pd.get_dummies(X)
linear = LinearRegression()
scores = cross_val_score(linear, X_dummies, y)
scores = cross_val_score(linear, X_dummies, y, scoring="neg_mean_squared_error")
forest = RandomForestRegressor()
X_dummies = pd.get_dummies(X)
scores = cross_val_score(forest, X_dummies, y)
scores = cross_val_score(forest, X_dummies, y, scoring="neg_mean_squared_error")
lgbm = lgb.LGBMRegressor(force_row_wise=True, verbose = -1)
scores = cross_val_score(lgbm, X, y)
scores = cross_val_score(lgbm, X_dummies, y, scoring="neg_mean_squared_error")</pre>
<p>The following table summarizes the performance of <span class="No-Break">each model:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-4">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Algorithm</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">R</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
</td>
<td class="No-Table-Style">
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic">MSE</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Linear Regression</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.558</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">2.261</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Random Forest</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.956</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.222</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">LightGBM</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.956</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.222</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.2 – Five-fold cross-validated performance metrics on the Wind Turbine dataset</p>
<p>The LightGBM and random forest regressors show almost identical performance with the <a id="_idIndexMarker416"/>same rounded <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> and MSE scores. Both algorithms significantly outperformed <span class="No-Break">linear regression.</span></p>
<p>Our model performs very well, with an absolute error of around 471 W/h. However, there’s a problem that’s easy to spot if we plot the feature importance of our <span class="No-Break">trained model.</span></p>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> shows each feature’s relative importance for our <span class="No-Break">LightGBM model.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<img alt="Figure 6.7 – Relative feature importance of each feature to our LightGBM model" height="595" src="image/B16690_06_07.jpg" width="1142"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Relative feature importance of each feature to our LightGBM model</p>
<p>As we can see from the features’ importance, three features stand out: <strong class="source-inline">blades_angle</strong>, <strong class="source-inline">motor_torque</strong>, and <strong class="source-inline">resistance</strong>. However, for two of the features, <strong class="source-inline">motor_torque</strong> and <strong class="source-inline">resistance</strong>, we could ask: Do these features lead to improved generated power, or do they result from an increase in the power generated? These<a id="_idIndexMarker417"/> features are examples<a id="_idIndexMarker418"/> of <strong class="bold">target leakage</strong>, as <span class="No-Break">explained next.</span></p>
<h4>Target leakage</h4>
<p>Target <a id="_idIndexMarker419"/>leakage, often called “leakage,” is a common pitfall in designing and training machine learning models. It occurs when the model is inadvertently given access to the target variable (or some proxy of the target variable) during the training process. As a result, the model’s performance during training may seem impressive, but it performs poorly on new, unseen data because it has effectively “cheated” <span class="No-Break">during training.</span></p>
<p>Some common examples of how leakage can occur are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Time-based leakage</strong>: Suppose<a id="_idIndexMarker420"/> you’re trying to predict stock prices for tomorrow. If you include data from tomorrow in your training set (maybe accidentally), that would cause leakage. Similarly, data only available after the fact (such as aggregate data over all stocks) is another example of <span class="No-Break">time-based leakage.</span></li>
<li><strong class="bold">Preprocessing mistakes</strong>: These happen when you perform a specific operation, such as scaling or normalizing, using statistics that include both the training and <span class="No-Break">test set.</span></li>
<li><strong class="bold">Incorrect data splits</strong>: For time-series data, using a simple random split might result in future data being present in the <span class="No-Break">training set.</span></li>
<li><strong class="bold">Contaminated validation sets</strong>: Sometimes, when creating validation or test sets, some data might overlap or be very closely related to the training data, causing optimistic and unrepresentative <span class="No-Break">validation scores.</span></li>
</ul>
<p>In our example, <strong class="source-inline">motor_torque</strong> and <strong class="source-inline">resistance</strong> are examples of time-based leakage: both metrics can only be measured after power is generated, which is what we are trying to predict. This<a id="_idIndexMarker421"/> also illustrates the importance of performing baseline training tests, as problems like these may not be easily <span class="No-Break">found beforehand.</span></p>
<p>We fix this error by removing the features from our dataset. We can then proceed with model tuning to further improve the <span class="No-Break">model’s performance.</span></p>
<h3>Model tuning</h3>
<p>We<a id="_idIndexMarker422"/> utilize Optuna to perform our parameter optimization study. We’ll leverage Optuna’s <strong class="bold">Tree-structured Parzen Estimator</strong> (<strong class="bold">TPE</strong>) sampling algorithm with Hyperband pruning for efficiency. We define our objective<a id="_idIndexMarker423"/> function with the required parameters, a pruning callback, and measure <span class="No-Break">the MSE:</span></p>
<pre class="source-code">
def objective(trial):
    boosting_type = trial.suggest_categorical("boosting_type", ["dart", "gbdt"])
    lambda_l1 = trial.suggest_float(
        'lambda_l1', 1e-8, 10.0, log=True),
...
    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, "mean_squared_error")
    model = lgb.LGBMRegressor(
...
        callbacks=[pruning_callback],
        verbose=-1)
    scores = cross_val_score(model, X, y, scoring="neg_mean_squared_error")
    return scores.mean()</pre>
<p>We then create our Optuna study with a TPE sampler, Hyperband pruning, and an optimization budget of <span class="No-Break">200 trails:</span></p>
<pre class="source-code">
sampler = optuna.samplers.TPESampler()
pruner = optuna.pruners.HyperbandPruner(
    min_resource=20, max_resource=400, reduction_factor=3)
study = optuna.create_study(
    direction='maximize', sampler=sampler,
    pruner=pruner
)
study.optimize(objective, n_trials=200, gc_after_trial=True, n_jobs=-1)</pre>
<p>Using the <a id="_idIndexMarker424"/>optimized parameters found through Optuna, we further improve the performance of the LightGBM model to an <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> of <strong class="source-inline">0.93</strong> and an MSE of <strong class="source-inline">0.21</strong> in our run. Your results may differ slightly due to the stochastic nature of <span class="No-Break">Optuna studies.</span></p>
<p>With an<a id="_idIndexMarker425"/> optimized model trained, we can proceed to the next phases of the data science process: deployment <span class="No-Break">and reporting.</span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor105"/>Model deployment</h2>
<p>We can now use our <a id="_idIndexMarker426"/>trained model to make predictions on unseen data. The coming chapters focus on various ways to deploy and monitor models as part of the <span class="No-Break">MLOps process.</span></p>
<p>However, the simplest way of using our model is to save the model and write a simple script that loads the model and makes <span class="No-Break">the prediction.</span></p>
<p>We can save our model using standard Python serialization or the LightGBM API. Here, we illustrate using standard <span class="No-Break">Python tooling:</span></p>
<pre class="source-code">
joblib.dump(model, "wind_turbine_model.pkl")</pre>
<p>A simple script to load the model and make predictions would be <span class="No-Break">as follows:</span></p>
<pre class="source-code">
def make_predictions(data):
    model = joblib.load("wind_turbine_model.pkl")
    return model.predict(data)
if __name__ == '__main__':
    make_predictions(prepare_data(pd.read_csv("wind-turbine/test.csv")))</pre>
<p>Importantly, we must repeat the data preparation for any data we want to predict to add engineered features and drop columns that <span class="No-Break">aren’t used.</span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor106"/>Communicating results</h2>
<p>The final step of the data science <a id="_idIndexMarker427"/>process is to communicate results. A data scientist typically compiles a report with salient findings and visualizations to present the results <span class="No-Break">to stakeholders.</span></p>
<p>A <a id="_idIndexMarker428"/>report would look similar to the write-up of this case study. We might present the correlations found between our features, for example, the correlation between the month and the power generated. We would also highlight problems in the data, such as the outliers and missing values, to improve future data <span class="No-Break">collection efforts.</span></p>
<p>We would further highlight features important to the model, such that wind turbines can be optimized to maximize the <span class="No-Break">power generated.</span></p>
<p>Focus on the quality of the report. Use well-designed and detailed visualizations and other supporting material instead of solely relying on text. An infographic or interactive chart can be more helpful than a detailed write-up. Check your writing for errors, and be sure to have the report proofread before sending <span class="No-Break">it out.</span></p>
<p>The content of a report should address the problem as defined by the problem statement. Any hypothesis that was tested must be answered in the report. But, the report also strongly depends on and should be tailored to your audience. For example, if your audience is business executives, include content that’s understandable to them and answers questions they could have, which would be centered around the business impact of <span class="No-Break">your </span><span class="No-Break"><a id="_idIndexMarker429"/></span><span class="No-Break">findings.</span></p>
<p>We now look at a case study for a classification problem. We show that, though each dataset is unique and has specific challenges, the overall data science process remains <span class="No-Break">the same.</span></p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor107"/>Classifying individual credit scores with LightGBM</h1>
<p>Our<a id="_idIndexMarker430"/> second case study is a problem of credit score classification for individuals. The dataset is <a href="https://www.kaggle.com/datasets/parisrohan/credit-score-classification?datasetId=2289007">available <span class="No-Break">from </span><span class="No-Break">https://www.kaggle.com/datasets/parisrohan/credit-score-classification?da</span></a><span class="No-Break">tasetId=2289007</span><span class="No-Break">.</span></p>
<p>The dataset is significantly larger than the previous problem and has unique data formatting problems. For brevity, we will not go through the solution in as much detail as with the previous problem (as much of the work is the same), but the end-to-end solution i<a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-6/credit-score-classification.ipynb">s available <span class="No-Break">at </span><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-6/credit-score-class</span></a><span class="No-Break">ification.ipynb</span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor108"/>Problem definition</h2>
<p>The <a id="_idIndexMarker431"/>dataset consists of 100,000 rows and 27 columns representing individuals’ demographic and financial information, including a credit score rating. The data includes information regarding individual income, number of loans, payment behavior, and investments. The credit score may be rated as good, standard, <span class="No-Break">or poor.</span></p>
<p>Our task is to analyze the data and build a model that accurately classifies the credit scores of unseen individuals. The quality of predictions is measured using classification accuracy and <span class="No-Break">the F1-score.</span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor109"/>Data collection</h2>
<p>The data <a id="_idIndexMarker432"/>is collected from the database of customers of a US financial institution. Individuals aged 14 to 65 form part of the dataset. There is no documented bias toward sampling specific demographics (low-income brackets, age, or racial groups), but this must be validated. No additional data <span class="No-Break">is collected.</span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor110"/>Data preparation</h2>
<p>As<a id="_idIndexMarker433"/> before, we start with simple data exploration tasks to determine the cleanliness of the data. We first check the data structure <span class="No-Break">and types:</span></p>
<pre class="source-code">
train_df.info()
#   Column                    Non-Null Count   Dtype
---  ------                    --------------   -----
 0   ID                        100000 non-null  object
 1   Customer_ID               100000 non-null  object
 2   Month                     100000 non-null  object
 3   Name                      90015 non-null   object
 4   Age                       100000 non-null  object
 5   SSN                       100000 non-null  object
 ...
 25  Payment_Behaviour         100000 non-null  object
 26  Monthly_Balance           98800 non-null   object
 27  Credit_Score              100000 non-null  object</pre>
<p>We notice that there are missing values for some features. Also, many of the features we expect to be numeric (annual income, number of loans, and others) are interpreted as objects instead of integers or floats. These features have to be coerced into <span class="No-Break">numeric features.</span></p>
<p>We also check the descriptive statistics of the features and for duplicated rows. Two features are found to have outlier values, age and the number of bank accounts, and need to <span class="No-Break">be cleaned.</span></p>
<p>Notably, the <strong class="source-inline">Type_of_Loan</strong> field has a comma-separated list, including conjunctions, of the types of loans for each individual. For example “<strong class="source-inline">student_loan, mortgage_loan, and personal_loan</strong>”. The modeling algorithm could not extract the loan types <a id="_idIndexMarker434"/>as part of a string. We have to engineer new fields to enable <span class="No-Break">effective modeling.</span></p>
<h3>Data cleaning</h3>
<p>We can now<a id="_idIndexMarker435"/> proceed with cleaning the data. In summary, the following issues have to <span class="No-Break">be addressed:</span></p>
<ul>
<li>Coercing object columns to numeric columns <span class="No-Break">where appropriate</span></li>
<li>Handling outliers in the age, number of bank accounts, and monthly <span class="No-Break">balance columns</span></li>
<li>Engineering new features for the types <span class="No-Break">of loans</span></li>
<li>Handling missing values and <span class="No-Break">duplicate rows</span></li>
</ul>
<h4>Coercing to numeric columns</h4>
<p>One <a id="_idIndexMarker436"/>of the main issues with the dataset is the mixed types found in columns that are supposed to be numeric, but due to error values, pandas interprets them as objects. For example, the <strong class="source-inline">Annual_Income</strong> column contains values such as <strong class="source-inline">100000.0_</strong>, which is then interpreted as <span class="No-Break">a string.</span></p>
<p>To clean and convert the features to numbers, we first remove character symbols using a <span class="No-Break">regular expression:</span></p>
<pre class="source-code">
frame[col] = frame[col].astype(str).str.replace(r'[^\d\.]', '', regex=True)</pre>
<p>This allows<a id="_idIndexMarker437"/> us to use pandas to coerce the column to a numeric feature, turning any errors (empty values) into <span class="No-Break"><strong class="source-inline">np.nan</strong></span><span class="No-Break"> values:</span></p>
<pre class="source-code">
frame[col] = pd.to_numeric(frame[col], errors="coerce")</pre>
<p>The <strong class="source-inline">Credit_History_Age</strong> feature requires more work from our side. The age is specified using natural language such as “12 years and 3 months.” Here, we use Python string processing to convert the years and months to a <span class="No-Break">floating-point number:</span></p>
<pre class="source-code">
def clean_credit_age(age):
    if age == 'nan':
        return np.nan
    if not "Years" in age:
        return age
    years, months = age.split(" Years and ")
    months = months.replace(" Months", "")
    return int(years) + int(months) / 12</pre>
<h4>Splitting delimiter-separated strings</h4>
<p>As<a id="_idIndexMarker438"/> mentioned previously, the <strong class="source-inline">Type_of_Loan</strong> feature is a comma-separated list of the types of loans the individual has. Although we could approach the problem in multiple ways, the most helpful technique is to parse the fields and build a Boolean array of columns indicating which loans an <span class="No-Break">individual has.</span></p>
<p>There are eight unique types of loans and a specific category if the loan type is unspecified. These are <strong class="source-inline">Auto Loan</strong>, <strong class="source-inline">Credit-Builder Loan</strong>, <strong class="source-inline">Debt Consolidation Loan</strong>, <strong class="source-inline">Home Equity Loan</strong>, <strong class="source-inline">Mortgage Loan</strong>, <strong class="source-inline">Payday Loan</strong>, <strong class="source-inline">Personal Loan</strong>, and <span class="No-Break"><strong class="source-inline">Student Loan</strong></span><span class="No-Break">.</span></p>
<p>Our encoding strategy would then process the feature as follows. We create nine new columns (one per loan type and unspecified) and set the column to true if the individual has that loan type. For example, here we have three conjoined <span class="No-Break">loan descriptions:</span></p>
<pre class="source-code">
"Home Equity Loan, and Payday Loan"
"Payday Loan, Personal Loan"
"Student Loan, Auto Loan, and Debt Consolidation Loan"</pre>
<p><em class="italic">Table 6.3</em> shows the encoding results for these examples, where a true flag is set if the individual has that type <span class="No-Break">of loan.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Auto</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Credit-Builder</span></p>
</td>
<td class="No-Table-Style">
<p>Debt <span class="No-Break">Cons.</span></p>
</td>
<td class="No-Table-Style">
<p>Home <span class="No-Break">Equity</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Mortgage</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Payday</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Personal</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Student</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Unspecified</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
<td class="No-Table-Style">
<p>T</p>
</td>
<td class="No-Table-Style">
<p>F</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.3 – Loan type columns encode customers’ types of loans</p>
<p>We can utilize<a id="_idIndexMarker439"/> pandas’ String utilities to accomplish the preceding encoding, as in <span class="No-Break">this example:</span></p>
<pre class="source-code">
frame["auto_loan"] = frame["Type_of_Loan"].str.lower().str.contains("auto loan").astype(bool)</pre>
<h4>Outliers and missing values</h4>
<p>We<a id="_idIndexMarker440"/> follow the same general strategy as before: we impute missing values using descriptive statistics and, where able, set outlier values to boundaries we define using <span class="No-Break">domain knowledge.</span></p>
<p>Duplicate rows <span class="No-Break">are dropped.</span></p>
<p>In terms of outliers, the age, number of bank accounts, and monthly balance features have outlying values (which we confirm using the mean and standard deviation and distribution plots). We set the outlying values to the upper bound for <span class="No-Break">these features:</span></p>
<pre class="source-code">
frame.loc[frame["Age"] &gt; 65, "Age"] = 65
frame.loc[frame["Num_Bank_Accounts"] &gt; 1000, "Num_Bank_Accounts"] = 1000
frame.loc[frame["Monthly_Balance"] &gt; 1e6, "Monthly_Balance"] = np.nan</pre>
<p>After <a id="_idIndexMarker441"/>our data cleaning, we can verify that all features have the correct type and that missing values have <span class="No-Break">been handled:</span></p>
<pre class="source-code">
train_df.info()
train_df.isnull().sum()
train_df[train_df.duplicated()]</pre>
<p>We can proceed with a more thorough exploratory analysis with the <span class="No-Break">clean dataset.</span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor111"/>EDA</h2>
<p>Next, we<a id="_idIndexMarker442"/> highlight some of the patterns found during <span class="No-Break">our EDA.</span></p>
<p>As described in the problem description, we need to validate whether any potential bias exists in <span class="No-Break">the data.</span></p>
<p>We start by visualizing the <span class="No-Break">customers’ ages:</span></p>
<pre class="source-code">
sns.histplot(train_df["Age"], bins=20)</pre>
<div>
<div class="IMG---Figure" id="_idContainer059">
<img alt="Figure 6.8 – Histogram showing the count of customers by age" height="825" src="image/B16690_06_08.jpg" width="990"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Histogram showing the count of customers by age</p>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em> shows<a id="_idIndexMarker443"/> that all age groups are represented, with data mainly normally distributed around <span class="No-Break">the middle-aged.</span></p>
<p>We also check monthly income. A lack of data in lower income brackets could indicate that minorities <span class="No-Break">were excluded:</span></p>
<pre class="source-code">
sns.histplot(train_df["Monthly_Inhand_Salary"], bins=30)</pre>
<div>
<div class="IMG---Figure" id="_idContainer060">
<img alt="Figure 6.9 – Histogram showing the count of customers by monthly in-hand salary" height="843" src="image/B16690_06_09.jpg" width="999"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Histogram showing the count of customers by monthly in-hand salary</p>
<p>As seen in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, monthly income follows an expected distribution and lower income brackets are<a id="_idIndexMarker444"/> <span class="No-Break">well represented.</span></p>
<p>We again visualize the correlation heatmap for the numeric features to highlight and <span class="No-Break">direct correlations:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<img alt="Figure 6.10 – Correlation heatmap for the Credit Score dataset" height="973" src="image/B16690_06_10.jpg" width="1092"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Correlation heatmap for the Credit Score dataset</p>
<p>Two strong correlations are notable: monthly balance and monthly salary, and outstanding debt and delay in due date. Further visualization of these correlations indicates that a customer’s monthly balance increases with salary and that poor credit scores are associated with lower balances <span class="No-Break">and salaries.</span></p>
<p>A similar<a id="_idIndexMarker445"/> correlation exists between outstanding debt and credit score: an increase in debt is associated with poor credit scores, and vice versa. The analysis confirms that our model should be able to capture both of <span class="No-Break">these correlations.</span></p>
<p>Finally, and importantly, we also have to check the class distribution for <span class="No-Break">the dataset:</span></p>
<pre class="source-code">
sns.histplot(train_df["Credit_Score"], bins=30)</pre>
<div>
<div class="IMG---Figure" id="_idContainer062">
<img alt="Figure 6.11 – Histogram of the class distribution for the credit scoring dataset" height="845" src="image/B16690_06_11.jpg" width="1012"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Histogram of the class distribution for the credit scoring dataset</p>
<p>The class <a id="_idIndexMarker446"/>distribution for our dataset is shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em>. As we can see in the figure, there is a significant class imbalance. The imbalance has to be addressed before we can proceed <span class="No-Break">with modeling.</span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor112"/>Modeling</h2>
<p>As before, we<a id="_idIndexMarker447"/> proceed with model <span class="No-Break">selection first.</span></p>
<h3>Model selection</h3>
<p>Due<a id="_idIndexMarker448"/> to the dataset’s size and complexity, a linear model is not expected to perform very well. As such, we use a regular decision tree as a baseline model and include a random forest <span class="No-Break">for comparison.</span></p>
<h3>Model training and evaluation</h3>
<p>We<a id="_idIndexMarker449"/> are almost ready to train our models, but one issue remains: we must address the <span class="No-Break">class imbalance.</span></p>
<h4>Handling class imbalance</h4>
<p>Class imbalance<a id="_idIndexMarker450"/> potentially biases any trained models to the majority class or classes. Although tree-based algorithms are better at dealing with imbalanced classes than most other learning algorithms, it’s still best practice to address class imbalance <span class="No-Break">before modeling.</span></p>
<p>Generally, the following strategies may be employed to deal with <span class="No-Break">imbalanced classes:</span></p>
<ul>
<li><span class="No-Break"><strong class="bold">Resampling techniques</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Oversampling</strong>: This involves increasing the number of samples in the minority class to match the<a id="_idIndexMarker451"/> majority class. One<a id="_idIndexMarker452"/> common technique is the <strong class="bold">Synthetic Minority Over-sampling Technique</strong> (<strong class="bold">SMOTE</strong>), where<a id="_idIndexMarker453"/> new samples are created <a id="_idIndexMarker454"/>based on the <span class="No-Break">existing ones.</span></li><li><strong class="bold">Undersampling</strong>: This<a id="_idIndexMarker455"/> involves reducing the <a id="_idIndexMarker456"/>number of samples in the majority class to match the minority class. One risk with this approach is the loss of potentially <span class="No-Break">valuable data.</span></li></ul></li>
<li><strong class="bold">Cost-sensitive learning</strong>: This <a id="_idIndexMarker457"/>method assigns higher costs to the misclassification of the minority class during the training process. The idea is to make the model pay more attention to the minority class. LightGBM supports this through the <strong class="source-inline">class_weight</strong> (for multi-class) and <strong class="source-inline">scale_pos_weight</strong> (for binary classes) parameters. Examples of applying these parameters are given in <a href="B16690_04.xhtml#_idTextAnchor067"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Comparing LightGBM, XGBoost, and </em><span class="No-Break"><em class="italic">Deep Learning</em></span><span class="No-Break">.</span></li>
<li><strong class="bold">Data augmentation</strong>: This <a id="_idIndexMarker458"/>involves creating new instances in the dataset by adding small perturbations to existing instances. This method is prevalent in image <span class="No-Break">classification tasks.</span></li>
<li><strong class="bold">Use of appropriate evaluation metrics</strong>: Accuracy is often misleading in class imbalance. Instead, metrics such as precision, recall, F1-score, <strong class="bold">Area under the ROC Curve</strong> (<strong class="bold">AUC-ROC</strong>), and <a id="_idIndexMarker459"/>confusion matrix can provide a more comprehensive view of <span class="No-Break">model performance.</span></li>
</ul>
<p>In our case <a id="_idIndexMarker460"/>study, we are already employing robust evaluation metrics against imbalanced classes. For this problem, we use SMOTE, an oversampling technique, to balance our classes while preserving <span class="No-Break">our data.</span></p>
<h4>SMOTE</h4>
<p>SMOTE was <a id="_idIndexMarker461"/>developed to overcome<a id="_idIndexMarker462"/> some of the shortcomings of simple oversampling of the minority class, which can lead to overfitting due to the exact duplication of instances <em class="italic">[1]</em>. Instead of simply duplicating minority samples, SMOTE creates synthetic, or “fake,” samples that are similar, but not identical, to existing samples in the <span class="No-Break">minority class.</span></p>
<p>The SMOTE algorithm proceeds as follows. New sample points, called synthetic samples, are synthesized by choosing points between samples close to minority class samples. Specifically, for each minority class sample, SMOTE calculates the k-nearest neighbors, chooses one of these neighbors, and then multiplies the difference between the feature vectors of the sample and its chosen neighbor by a random number between 0 and 1, adding this to the original sample to create a new, <span class="No-Break">synthetic sample.</span></p>
<p>By creating synthetic examples, SMOTE presents a more robust solution to the imbalance problem, encouraging the model to draw more generalizable decision boundaries. It’s important to note, however, that while SMOTE can improve the performance of models on imbalanced datasets, it’s not always the best choice. For instance, it can introduce noise if the minority samples are not sufficiently close in the feature space, leading to overlapping classes. As with all sampling techniques, it’s essential to use cross-validation or a separate validation set to carefully evaluate the impact of SMOTE on your <span class="No-Break">model’s performance.</span></p>
<p>SMOTE <a id="_idIndexMarker463"/>over-sampling is implemented<a id="_idIndexMarker464"/> in the <strong class="source-inline">imblearn</strong> Python library. We can fit and resample our data <span class="No-Break">as follows:</span></p>
<pre class="source-code">
X = train_df.drop(columns=["Credit_Score"], axis=1)
X_dummies = pd.get_dummies(X)
y = train_df["Credit_Score"]
smote = SMOTE(sampling_strategy='auto')
return smote.fit_resample(X_dummies, y)</pre>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.12</em> shows the class distribution for the dataset after resampling with SMOTE. As shown in the figure, the classes are now <span class="No-Break">perfectly balanced.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<img alt="Figure 6.12 – Histogram of the class distribution for the Credit Score dataset after resampling the data using SMOTE; classes are now balanced" height="823" src="image/B16690_06_12.jpg" width="994"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Histogram of the class distribution for the Credit Score dataset after resampling the data using SMOTE; classes are now balanced</p>
<h4>Training and evaluation</h4>
<p>We<a id="_idIndexMarker465"/> can now proceed with the modeling. The following table shows the results of our run from the decision tree classifier, random forest, and LightGBM models using <span class="No-Break">default parameters.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table004-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Algorithm</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Accuracy</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">F1 score</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Decision tree</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">59.87%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.57</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Random forest</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">69.35%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.67</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">LightGBM</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">70.00%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.68</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.4 – Five-fold cross-validated performance metrics on the Credit Score dataset</p>
<p>As <a id="_idIndexMarker466"/>shown in <em class="italic">Table 6.4</em>, the LightGBM model performs the best, slightly outperforming the random forest model accuracy. Both algorithms perform better than the decision tree baseline. The LightGBM model also trained the fastest, more than seven times faster than the random <span class="No-Break">forest model.</span></p>
<p>We now proceed with parameter optimization of the <span class="No-Break">LightGBM model.</span></p>
<h3>Model tuning</h3>
<p>Similar to<a id="_idIndexMarker467"/> the previous case study, we use Optuna for parameter optimization. We again use the TPE sampler with an optimization budget of <span class="No-Break">50 trials.</span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor113"/>Model deployment and results</h2>
<p>Our model is <a id="_idIndexMarker468"/>now prepared and ready to be deployed to a platform of our choice. Potential deployment options would include building a web API around our model, deploying with a tool such <a id="_idIndexMarker469"/>as <strong class="bold">PostgresML</strong>, or using a cloud platform such<a id="_idIndexMarker470"/> as <strong class="bold">AWS SageMaker</strong>. These and other options are discussed in detail in the <span class="No-Break">coming chapters.</span></p>
<p>We could also use the model as part of a data science report. See the previous section for details on writing a good report. Keep in mind the most important aspects of communicating data science results, <span class="No-Break">as follows:</span></p>
<ul>
<li>Always report results in an unbiased and <span class="No-Break">fair way</span></li>
<li>Keep your <a id="_idIndexMarker471"/>audience in mind and focus on the report’s details that provide them with the <span class="No-Break">most value</span></li>
</ul>
<h1 id="_idParaDest-112"><a id="_idTextAnchor114"/>Summary</h1>
<p>This chapter presented two case studies on how to apply the data science process with LightGBM. The data science life cycle and the typical constituent steps were discussed <span class="No-Break">in detail.</span></p>
<p>A case study involving wind turbine power generation was presented as an example of approaching a data problem while working through the life cycle. Feature engineering and how to handle outliers were discussed in detail. An example exploratory data analysis was performed with samples given for visualization. Model training and tuning were shown alongside a basic script for exporting and using the model as <span class="No-Break">a program.</span></p>
<p>A second case study involving multi-class credit score classification was also presented. The data science process was again followed, with particular attention given to data cleaning and class imbalance problems in <span class="No-Break">the dataset.</span></p>
<p>The next chapter discusses the AutoML framework FLAML and introduces the concept of machine <span class="No-Break">learning pipelines.</span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor115"/>References</h1>
<table class="No-Table-Style" id="table005-2">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="Bibliography"><em class="italic">[</em><span class="No-Break"><em class="italic">1]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthetic Minority Over-sampling Technique,” Journal of Artificial Intelligence Research, vol. 16, p. 321–357, </em><span class="No-Break"><em class="italic">June 2002.</em></span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div></body></html>