- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Data Acquisition, Data Quality, and Noise
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采集、数据质量和噪声
- en: Data for machine learning systems can come directly from humans and software
    systems – usually called *source systems*. Where the data comes from has implications
    regarding what it looks like, what kind of quality it has, and how to process
    it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的数据可以直接来自人类和软件系统——通常称为*源系统*。数据的来源对其外观、质量以及如何处理它都有影响。
- en: The data that originates from humans is usually noisier than data that originates
    from software systems. We, as humans, are known for small inconsistencies and
    we can also understand things inconsistently. For example, the same defect reported
    by two different people could have a very different description; the same is true
    for requirements, designs, and source code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 来自人类的数据通常比来自软件系统的数据更嘈杂。我们人类以小的不一致性而闻名，我们也可以不一致地理解事物。例如，两个人报告的同一缺陷可能有非常不同的描述；对于需求、设计和源代码也是如此。
- en: The data that originates from software systems is often more consistent and
    contains less noise or the noise in the data is more regular than the noise in
    the human-generated data. This data is generated by source systems. Therefore,
    controlling and monitoring the quality of the data that’s generated automatically
    is different – for example, software systems do not “lie” in the data, so it makes
    no sense to check the believability of the data that’s generated automatically.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 来自软件系统的数据通常更一致，包含的噪声更少，或者数据中的噪声比人类生成数据的噪声更规律。这些数据由源系统生成。因此，控制和监控自动生成数据的品质是不同的——例如，软件系统不会在数据中“撒谎”，因此检查自动生成数据的可信度是没有意义的。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The different sources of data and what we can do with them
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的不同来源以及我们可以如何利用它们
- en: How to assess the quality of data that’s used for machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估用于机器学习的数据的品质
- en: How to identify, measure, and reduce noise in data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何识别、测量和减少数据中的噪声
- en: Sources of data and what we can do with them
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据来源以及我们可以如何利用它们
- en: Machine learning software has become increasingly important in all fields today.
    Anything from telecommunication networks, self-driving vehicles, computer games,
    smart navigation systems, and facial recognition to websites, news production,
    cinematography, and experimental music creation can be done using machine learning.
    Some applications are very successful at, for example, using machine learning
    in search strings (BERT models). Some applications are not so successful, such
    as using machine learning in hiring processes. Often, this depends on the programmers,
    data scientists, or models that are used in these applications. However, in most
    cases, the success of a machine learning application is often in the data that
    is used to train it and use it. It depends on the quality of that data and the
    features that are extracted from it. For example, Amazon’s machine learning recommender
    was taken out of operation because it was biased against women. Since it was trained
    on historical recruitment data, which predominantly included male candidates,
    the system tended to recommend male candidates in future hiring.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习软件在当今所有领域都变得越来越重要。从电信网络、自动驾驶汽车、计算机游戏、智能导航系统、面部识别到网站、新闻制作、电影制作和实验音乐创作，都可以使用机器学习来完成。一些应用在例如使用机器学习进行搜索字符串（BERT模型）方面非常成功。一些应用则不太成功，例如在招聘过程中使用机器学习。通常，这取决于在这些应用中使用到的程序员、数据科学家或模型。然而，在大多数情况下，机器学习应用的成功往往取决于用于训练和使用的训练数据。它取决于数据的品质以及从中提取的特征。例如，亚马逊的机器学习推荐系统被停用，因为它对女性存在偏见。由于它是基于历史招聘数据训练的，而这些数据主要包含男性候选人，因此系统倾向于在未来的招聘中推荐男性候选人。
- en: Data that’s used in machine learning systems can originate from all kinds of
    sources. However, we can classify these sources into two main types – manual/human
    and automated software/hardware. These two types have different characteristics
    that determine how to organize these sources and how to extract features from
    the data from these sources. *Figure 4**.1* illustrates these types of data and
    provides examples of data of each type.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用于机器学习系统的数据可以来自各种来源。然而，我们可以将这些来源分为两大类——人工/人类和自动化软件/硬件。这两类具有不同的特征，决定了如何组织这些来源以及如何从这些来源的数据中提取特征。*图4.1*展示了这些数据类型并提供了每种类型数据的示例。
- en: Manually generated data is the data that comes from human input or originates
    in humans. This kind of data is often much richer in information than the data
    generated by software, but it also has much larger variability. This variability
    can come from the natural variability of us, humans. The same question in a form,
    for example, can be understood and answered differently by two different people.
    This kind of data is often more susceptible to random errors than systematic ones.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 手动生成数据是指来自人类输入或起源于人类的数据。这类数据通常比软件生成数据信息量更丰富，但变异性也更大。这种变异性可能来自我们人类的自然变异性。例如，在表格中提出的问题，两个人可能会有不同的理解和回答。这类数据通常比系统性错误更容易受到随机错误的影响。
- en: 'Automatically generated data originates from hardware or software systems,
    usually measured by sensors or measurement scripts that collect data from other
    systems, products, processes, or organizations. This kind of data is often more
    consistent and repeatable but also more susceptible to systematic errors:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成数据起源于硬件或软件系统，通常由传感器或测量脚本收集来自其他系统、产品、流程或组织的数据。这类数据通常更一致、可重复，但也更容易受到系统性错误的影响：
- en: '![Figure 4.1 – Sources of data and their classification. The green part of
    this figure is the scope of this book](img/B19548_04_1.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 数据来源及其分类。此图中的绿色部分是本书的范围](img/B19548_04_1.jpg)'
- en: Figure 4.1 – Sources of data and their classification. The green part of this
    figure is the scope of this book
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 数据来源及其分类。此图中的绿色部分是本书的范围
- en: An example of data that originates from humans, and is often used in machine
    learning, is data about software defects. A human tester often inspects a problem
    and reports it using a form. This form contains fields about the testing phase,
    components affected, impact on the customer, and more, but one part of the form
    is often a description of the problem in natural language, which is an interpretation
    of what happens when it’s authored by a human tester.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人类起源的数据示例，常用于机器学习的是关于软件缺陷的数据。人类测试员通常会检查问题并使用表格报告。这个表格包含有关测试阶段、受影响的组件、对客户的影响等内容，但表格的一部分通常是问题描述的自然语言描述，这是由人类测试员对发生情况的一种解释。
- en: Another type of data that’s generated by humans is source code. We, as programmers,
    write source code for the software in a programming language with a given syntax,
    and we use programming guidelines to keep a consistent style so that the product
    of our work – the source code – can be automatically interpreted or compiled by
    software. There are some structures in the code that we write, but it is far from
    being consistent. Even the same algorithm, when implemented by two different programmers,
    differs in the naming of variables, their types, or even how the problem is solved.
    A good example of this is the Rosetta code website, which provides the same solutions
    in different programming languages and sometimes even in the same programming
    language.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种由人类生成数据是源代码。作为程序员，我们使用给定语法的编程语言编写软件源代码，并使用编程指南来保持一致的风格，以便我们的工作成果——源代码——可以被软件自动解释或编译。我们编写的代码中存在一些结构，但远非一致。即使是相同的算法，当由两位不同的程序员实现时，也会在变量命名、类型或解决问题的方法上有所不同。一个很好的例子是Rosetta代码网站，它提供了不同编程语言中的相同解决方案，有时甚至在同一编程语言中。
- en: Requirement specifications or data input using forms have the same properties
    and characteristics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用表格进行的需求规范或数据输入具有相同的属性和特征。
- en: However, there is one origin of data that is particularly interesting – medical
    data. This is the data from patients’ records and charts, input by the medical
    specialists as part of medical procedures. This data can be electronic, but it
    reflects the specialists’ understanding of the symptoms and their interpretation
    of the medical tests and diagnoses.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一种数据来源特别有趣——医疗数据。这是来自患者记录和图表的数据，由医疗专家作为医疗程序的一部分输入。这些数据可以是电子的，但它反映了专家对症状的理解以及他们对医疗测试和诊断的解释。
- en: On the other hand, we have data that’s generated by software or hardware in
    one way or another. The data that’s generated automatically is more consistent,
    although not free from problems, and more repetitive. An example of such data
    is the data generated in telecommunication networks to transmit information from
    one telecommunication node to another. The radio signals are very stable compared
    to other types of data, and can be disturbed by external factors such as weather
    (precipitation) or obstructions such as construction cranes. The data is repeatable,
    with all variability originating from the external factors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们还有由软件或硬件以某种方式生成的数据。自动生成的数据更一致，尽管不是没有问题，并且更重复。此类数据的例子是电信网络中生成以从一电信节点传输信息到另一电信节点。与其它类型的数据相比，无线电信号非常稳定，可能会受到外部因素（如降水）或障碍物（如建筑起重机）的干扰。数据是可重复的，所有变异性都源于外部因素。
- en: Another example is the data from vehicles, which register information around
    them and store it for further processing. This data can contain signaling between
    the vehicle’s components as well as communication with other vehicles or the infrastructure.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是来自车辆的数据，这些数据记录了它们周围的信息并存储以供进一步处理。这些数据可以包含车辆组件之间的信号以及与其他车辆或基础设施的通信。
- en: Medical data, such as **electroencephalograph** (**EEG** – that is, brain waves)
    or **electrocardiogram** (**ECG** – that is, heart rate), is collected from source
    systems, which we can see as measurement instruments. So, technically, it has
    been generated by computer systems, but it comes from human patients. This origin
    in patients means that the data has the same natural variability as any other
    data collected from humans. As every patient is a bit different, and the measurement
    systems can be attached to each patient a bit differently, the data generated
    by each patient differs slightly from other patients. For example, the ECG heartbeat
    data contains basic, consistent information – the number of beats per minute (among
    other parameters). However, the raw data differs in the amplitude of the ECG signal
    (depending on the placement of the measurement electrode) or the difference between
    spikes in the curve (R and T-spikes).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗数据，例如**脑电图**（**EEG**——即脑电波）或**心电图**（**ECG**——即心率），是从源系统中收集的，我们可以将其视为测量仪器。因此，从技术上讲，这些数据是由计算机系统生成的，但它们来自人类患者。这种来自患者的起源意味着数据具有与其他从人类收集的数据相同的自然变异性。由于每个患者都有所不同，测量系统可以以略微不同的方式连接到每个患者，因此每个患者生成的数据与其他患者略有不同。例如，心电图心跳数据包含基本、一致的信息——每分钟的跳动次数（以及其他参数）。然而，原始数据在心电图信号的幅度（取决于测量电极的位置）或曲线尖峰之间的差异（R和T尖峰）上有所不同。
- en: Therefore, my first best practice in this chapter has to do with the origin
    of the data that we use for software systems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章我的第一个最佳实践与我们所使用的软件数据的来源有关。
- en: 'Best practice #25'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #25'
- en: Identify the origin of the data used in your software and create your data processing
    pipeline accordingly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 识别你软件中使用的数据的来源，并据此创建你的数据处理流程。
- en: Since all types of data require different ways of working in terms of cleaning,
    formatting, and feature extraction, we should make sure that we know how the data
    is produced and what kind of problems we can expect (and handle). Therefore, first,
    we need to identify what kind of data we need, where it comes from, and what kind
    of problems it can carry with it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有类型的数据在清洁、格式化和特征提取方面都需要不同的处理方式，我们应该确保我们知道数据是如何产生的，以及我们可以预期（并处理）哪些问题。因此，首先，我们需要确定我们需要什么类型的数据，它来自哪里，以及它可能携带哪些问题。
- en: Extracting data from software engineering tools – Gerrit and Jira
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从软件工程工具中提取数据——Gerrit和Jira
- en: To illustrate how to work with data extraction, let’s extract data from a popular
    software engineering tool for code reviews – Gerrit. This tool is used for reviewing
    and discussing fragments of code developed by individual programmers, just before
    they are integrated into the main code base of the product.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何进行数据提取，让我们从一款流行的代码审查软件工具——Gerrit中提取数据。这个工具用于审查和讨论个人程序员在代码集成到产品主代码库之前开发的代码片段。
- en: 'The following program code shows how to access the database of Gerrit – that
    is, through the JSON API – and how to extract the list of all changes for a specific
    project. This program uses the Python `pygerrit2` package ([https://pypi.org/project/pygerrit2/](https://pypi.org/project/pygerrit2/)).
    This module helps us use the JSON API as it provides Python functions instead
    of just JSON strings:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下程序代码展示了如何通过 JSON API 访问 Gerrit 的数据库——也就是说，通过 JSON API ——以及如何提取特定项目的所有变更列表。此程序使用
    Python `pygerrit2` 包 ([https://pypi.org/project/pygerrit2/](https://pypi.org/project/pygerrit2/))。此模块帮助我们使用
    JSON API，因为它提供 Python 函数而不是仅仅 JSON 字符串：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The key line in this code fragment is `rest.get("/changes/?q=status:merged&o=ALL_FILES&o=ALL_REVISIONS&o=DETAILED_LABELS&start=0",
    headers={''Content-Type'': ''application/json''})`. This line specifies the endpoint
    for the changes to be retrieved, along with the parameters. It says that we want
    to access all files and all revisions, as well as all details of the changes.
    In these details, we can find the information about all revisions (particular
    patches/commits) and we can then parse each of these revisions. It is important
    to know that the JSON API returns a maximum of 500 changes in each query, and
    therefore, the last parameter – `start=0` – can be used to access changes from
    500 upward. The output of this program is a very long list of changes in JSON,
    so I will not present it in detail in this book. Instead, I encourage you to execute
    this script and go through this file at your own pace. The script can be found
    in this book’s GitHub repository at [https://github.com/miroslawstaron/machine_learning_best_practices](https://github.com/miroslawstaron/machine_learning_best_practices),
    under [*Chapter 4*](B19548_04.xhtml#_idTextAnchor049). The name of the script
    is `gerrit_exporter.ipynb`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '此代码片段中的关键行是 `rest.get("/changes/?q=status:merged&o=ALL_FILES&o=ALL_REVISIONS&o=DETAILED_LABELS&start=0",
    headers={''Content-Type'': ''application/json''})`。这一行指定了要检索变更的端点以及参数。它表示我们想要访问所有文件和所有修订，以及所有变更的详细信息。在这些详细信息中，我们可以找到有关所有修订（特定的补丁/提交）的信息，然后我们可以解析这些修订中的每一个。重要的是要知道
    JSON API 在每个查询中返回的最大变更数是 500，因此最后一个参数——`start=0`——可以用来访问从 500 开始的变更。此程序的输出是一个非常长的变更列表，以
    JSON 格式，因此我不会在本书中详细展示。相反，我鼓励您执行此脚本，并根据自己的节奏浏览此文件。脚本可以在本书的 GitHub 仓库中找到，网址为 [https://github.com/miroslawstaron/machine_learning_best_practices](https://github.com/miroslawstaron/machine_learning_best_practices)，在
    [*第 4 章*](B19548_04.xhtml#_idTextAnchor049) 下。脚本的名称是 `gerrit_exporter.ipynb`。'
- en: Now, extracting just the list of changes is not very useful for analyses as
    it only provides the information that has been collected automatically – for example,
    which revisions exist, and who created these revisions. It does not contain information
    about which files and lines are commented on, or what the comments are – in other
    words, the particularly useful information. Therefore, we need to interact with
    Gerrit a bit more, as presented in *Figure 4**.2*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，仅提取变更列表对于分析来说并不很有用，因为它只提供了自动收集的信息——例如，哪些修订存在，以及谁创建了这些修订。它不包含有关哪些文件和行被注释的信息，或者注释是什么——换句话说，特别有用的信息。因此，我们需要与
    Gerrit 进行更多交互，如 *图 4**.2* 所示。
- en: 'The program flow presented in *Figure 4**.2* illustrates the complexity of
    the relationships in a code comment database such as Gerrit. Therefore, the program
    to access the database and export this data is a bit too long for this book. It
    can be accessed in the same repository as the previous one, under the name `gerrit_exporter_loop.ipynb`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 4**.2* 中展示的程序流程说明了代码注释数据库（如 Gerrit）中关系的复杂性。因此，访问数据库并导出这些数据的程序对于本书来说有点太长了。它可以在与之前相同的仓库中找到，名称为
    `gerrit_exporter_loop.ipynb`：
- en: '![Figure 4.2 – Interactions with Gerrit to extract commented files, commented
    lines, and the content of the comments](img/B19548_04_2.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 与 Gerrit 交互以提取注释文件、注释行和注释内容](img/B19548_04_2.jpg)'
- en: Figure 4.2 – Interactions with Gerrit to extract commented files, commented
    lines, and the content of the comments
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 与 Gerrit 交互以提取注释文件、注释行和注释内容
- en: This kind of data can be used to train machine learning models to review the
    code or even identify which lines of code need to be reviewed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这类数据可以用来训练机器学习模型以审查代码，甚至可以识别哪些代码行需要被审查。
- en: When working with Gerrit, I have found the following best practice to be useful.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Gerrit 的时候，我发现以下最佳实践非常有用。
- en: 'Best practice # 26'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 # 26'
- en: Extract as much data as you need and store it locally to reduce the disturbances
    for the software engineers who use the tool for their work.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提取您所需的所有数据并将其存储在本地，以减少使用该工具进行工作的软件工程师的干扰。
- en: Although it is possible to extract the changes one by one, it is better to extract
    the whole set of changes once and keep a local copy of it. In this way, we reduce
    the strain on the servers that others use for their daily work. We must remember
    that data extraction is a secondary task for these source systems, while their
    primary task is to support software engineers in their work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以逐个提取更改，但最好一次性提取整个更改集并保留其本地副本。这样，我们减轻了其他人日常工作中使用的服务器压力。我们必须记住，数据提取是这些源系统的次要任务，而它们的主要任务是支持软件工程师的工作。
- en: Another good source of data for software systems that support software engineering
    tasks is JIRA, an issue and task management system. JIRA is used to document epics,
    user stories, software defects, and tasks and has become one of the most popular
    tools for this kind of activity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个为支持软件工程任务的软件系统提供数据的好来源是JIRA，一个问题和任务管理系统。JIRA用于记录史诗、用户故事、软件缺陷和任务，并已成为此类活动最受欢迎的工具之一。
- en: Therefore, we can extract a lot of useful information about the processes from
    JIRA as a source system. Then, we can use this information to develop machine
    learning models to assess and improve tasks and requirements (in the form of user
    stories) and design tools that can help us identify overlapping user stories or
    group them into more coherent epics. Such software can be used to improve the
    quality of these tasks or provide better estimations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以从JIRA作为源系统提取大量有关流程的有用信息。然后，我们可以使用这些信息来开发机器学习模型，评估和改进任务和需求（以用户故事的形式），并设计帮助我们识别重叠的用户故事或将它们分组为更连贯的史诗的工具。这种软件可以用来提高这些任务的质量或提供更好的估计。
- en: 'The following code fragment illustrates how to make a connection to a JIRA
    instance and then how to extract all issues for a particular project:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段说明了如何连接到JIRA实例，然后如何提取特定项目的所有问题：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this fragment, for illustration purposes, I’m using my own JIRA database
    and my own project (`MLBPB`). This code requires the `atlassian-python-api` module
    to be imported. This module provides an API to connect to and interact with a
    JIRA database using Python, in a similar way as the API of Gerrit. Therefore,
    the same best practice as for Gerrit applies to JIRA.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，为了说明目的，我正在使用自己的JIRA数据库和自己的项目（`MLBPB`）。此代码需要导入`atlassian-python-api`模块。此模块提供了一个API，用于使用Python连接到并交互JIRA数据库，类似于Gerrit的API。因此，适用于Gerrit的最佳实践也适用于JIRA。
- en: Extracting data from product databases – GitHub and Git
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从产品数据库中提取数据——GitHub和Git
- en: JIRA and Gerrit are, to some extent, additional tools to the main product development
    tools. However, every software development organization uses a source code repository
    to store the main asset – the source code of the company’s software product. Today,
    the tools that are used the most are Git version control and its close relative,
    GitHub. Source code repositories can be a very useful source of data for machine
    learning systems – we can extract the source code of the product and analyze it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: JIRA和Gerrit在一定程度上是主要产品开发工具的补充工具。然而，每个软件开发组织都使用源代码仓库来存储主要资产——公司软件产品的源代码。如今，使用最频繁的工具是Git版本控制和它的近亲GitHub。源代码仓库可以成为机器学习系统非常有用的数据来源——我们可以提取产品的源代码并对其进行分析。
- en: GitHub is a great source of data for machine learning if we use it responsibly.
    Please remember that the source code provided as open source, by the community,
    is not for profiting off. We need to follow the licenses and we need to acknowledge
    the contributions that were made by the authors, contributors, and maintainers
    of the open source community. Regardless of the license, we are always able to
    analyze our own code or the code that belongs to our company.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们负责任地使用GitHub，它对机器学习来说是一个很好的数据来源。请记住，由社区提供的开源源代码不是为了盈利。我们需要遵守许可证，并承认开源社区作者、贡献者和维护者所做的贡献。无论许可证如何，我们总是能够分析我们自己的代码或我们公司的代码。
- en: 'Once we can access the source code of our product or the product that we want
    to analyze, the following code fragment helps us connect to the GitHub server
    and access a repository:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们可以访问我们产品或我们想要分析的产品源代码，以下代码片段帮助我们连接到GitHub服务器并访问存储库：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To support secure access to the code, GitHub uses access tokens rather than
    passwords when connecting to it. We can also use SSL and CLI interfaces, but for
    the sake of simplicity, we’ll use the HTTPS protocol with a token. The `g = Github(token,
    per_page=100)` line uses the token to instantiate the PyGitHub library’s main
    class. The token is individual and needs to be generated either per repository
    or per user.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持对代码的安全访问，GitHub在连接到它时使用访问令牌而不是密码。我们还可以使用SSL和CLI接口，但为了简单起见，我们将使用带有令牌的HTTPS协议。`g
    = Github(token, per_page=100)`这一行使用令牌来实例化PyGitHub库的主类。令牌是唯一的，需要为每个仓库或每个用户单独生成。
- en: 'The connection to the repository is done by the next line, `repo = g.get_repo("miroslawstaron/machine_learning_best_practices")`,
    which, in this example, connects to the repository associated with this book.
    Finally, the last line in the code fragment obtains the number of commits in the
    repository. Once obtained, we can print it and start analyzing it, as shown in
    the following code fragment:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码通过`repo = g.get_repo("miroslawstaron/machine_learning_best_practices")`建立与仓库的连接，在这个例子中，它连接到与这本书相关的仓库。最后，代码片段中的最后一行获取仓库中的提交数量。一旦获取，我们可以打印它并开始分析，如下面的代码片段所示：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The last line of this code fragment prints out the commit message of the latest
    commit. It is worth noting that the latest commit is always first in the list
    of commits. Once we know the commit, we can also access the list of files that
    are included in that commit. This is illustrated by the following code fragment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段的最后一行打印出最新提交的提交信息。值得注意的是，最新提交总是位于提交列表的第一位。一旦我们知道提交，我们也可以访问包含在该提交中的文件列表。以下代码片段展示了这一点：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Printing the list of the files in the commits is good, but it’s not very useful.
    Something more useful is to access these files and analyze them. The following
    code fragment shows how to access two files from the latest two commits. First,
    we access the files, after which we download their content and store it in two
    different variables – `linesOne` and `linesTwo`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 打印提交中文件的列表是好的，但并不十分有用。更有用的事情是访问这些文件并分析它们。以下代码片段展示了如何从最近的两个提交中访问两个文件。首先，我们访问文件，然后下载它们的内容并将它们存储在两个不同的变量中——`linesOne`和`linesTwo`：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we can analyze the two files for one of the most important tasks –
    to get the diff between the two files. We can use the `difflib` Python library
    for this task, as shown here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以分析这两个文件，执行最重要的任务之一——获取两个文件之间的差异。我们可以使用`difflib` Python库来完成这项任务，如下所示：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code fragment allows us to identify differences between files
    and print them in a way similar to how GitHub presents the differences.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段允许我们识别文件之间的差异，并以类似于GitHub展示差异的方式打印它们。
- en: My next best practice is related to the use of public repositories.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我接下来的最佳实践与公共仓库的使用有关。
- en: 'Best practice # 27'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 # 27'
- en: When accessing data from public repositories, please check the licenses and
    ensure you acknowledge the contribution of the community that created the analyzed
    code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当从公共仓库访问数据时，请检查许可证并确保你承认创建了分析代码的社区的贡献。
- en: As I mentioned previously, open source programs are here for everyone to use,
    including to analyze and learn from them. However, the community behind this source
    code has spent countless hours creating and perfecting it. Therefore, we should
    use these repositories responsibly. If we use the repositories to create our own
    products, including machine learning software products, we need to acknowledge
    the community’s contribution and, if we use the software under a copyleft license,
    give back our work to the community.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，开源程序是为了让每个人使用，包括分析和从中学习。然而，这个源代码背后的社区已经投入了无数小时来创建和完善它。因此，我们应该负责任地使用这些仓库。如果我们使用仓库来创建自己的产品，包括机器学习软件产品，我们需要承认社区的贡献，如果我们使用的是copyleft许可证下的软件，我们需要将我们的工作回馈给社区。
- en: Data quality
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量
- en: When designing and developing machine learning systems, we consider the data
    quality on a relatively low level. We look for missing values, outliers, or similar.
    They are important because they can cause problems when training machine learning
    models. Nevertheless, they are nearly enough from a software engineering perspective.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计和开发机器学习系统时，我们从相对较低的水平考虑数据质量。我们寻找缺失值、异常值或类似情况。它们很重要，因为它们可能在训练机器学习模型时引起问题。尽管如此，从软件工程的角度来看，它们几乎足够了。
- en: When engineering reliable software systems, we need to know more about the data
    we use than whether it contains (or not) missing values. We need to know whether
    we can trust the data (whether it is believable), whether the data is representative,
    or whether it is up to date. So, we need a quality model for our data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建可靠的软件系统时，我们需要了解我们使用的不仅仅是数据是否包含（或不包含）缺失值。我们需要知道我们是否可以信任数据（是否可信），数据是否具有代表性，或者是否是最新的。因此，我们需要为我们的数据建立一个质量模型。
- en: There are several quality models for data in software engineering, and the one
    I often use, and recommend, is the AIMQ model – a methodology for assessing information
    quality.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，数据有几种质量模型，我经常使用并推荐的模型是AIMQ模型——一种评估信息质量的方法。
- en: 'The quality dimensions of the AIMQ model are as follows (cited from Lee, Y.W.,
    et al., *AIMQ: a methodology for information quality assessment*. Information
    & management, 2002\. 40(2): p. 133-146):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: AIMQ模型的质量维度如下（摘自Lee, Y.W.等，*AIMQ：信息质量评估方法*。信息与管理，2002年，40(2)：p. 133-146）：
- en: '**Accessibility**: The information is easily retrievable and accessible to
    our system'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：信息易于检索，并且可以轻松访问我们的系统'
- en: '**Appropriate amount**: The information is of sufficient volume for our needs
    and for our applications'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适当数量**：信息对于我们的需求和我们的应用来说是足够的'
- en: '**Believability**: The information is trustworthy and can be believed'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可信度**：信息是可信的，可以信赖'
- en: '**Completeness**: The information includes all the necessary values for our
    system'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：信息包括我们系统所需的所有必要值'
- en: '**Concise representation**: The information is formatted compactly and appropriately
    for our application'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁表示**：信息以紧凑和适当的方式格式化，适用于我们的应用'
- en: '**Consistent representation**: The information is consistently presented in
    the same format, including its representation over time'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致表示**：信息以一致的方式呈现，包括其随时间的变化表示'
- en: '**Ease of operation**: The information is easy to manipulate to meet our needs'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：信息易于操作以满足我们的需求'
- en: '**Free of errors**: The information is correct, accurate, and reliable for
    the application that we’re creating'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无错误**：信息对于我们所创建的应用来说是正确的、准确的和可靠的'
- en: '**Interpretability**: It is easy to interpret what the information means'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：很容易理解信息的含义'
- en: '**Objectivity**: The information was objectively collected and is based on
    facts'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客观性**：信息是客观收集的，并基于事实'
- en: '**Relevance**: The information is useful, relevant, appropriate, and applicable
    to our system'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：信息对我们系统是有用的、相关的、适当的，并且适用于我们的系统'
- en: '**Reputation**: The information has a good reputation for quality and comes
    from good sources'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声誉**：信息在质量上享有良好声誉，且来源于可靠来源'
- en: '**Security**: The information is protected from unauthorized access and sufficiently
    restricted'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：信息受到未经授权的访问保护，并且受到充分的限制'
- en: '**Timeliness**: The information is sufficiently current for our work'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时效性**：信息对于我们的工作来说是足够最新的'
- en: '**Understandability**: The information is easy to understand and comprehend'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可理解性**：信息易于理解和理解'
- en: Some of these dimensions are certainly universal for all kinds of applications.
    For example, the *free from error* dimension is relevant for all systems and all
    machine learning models. At the same time, *relevance* must be evaluated in the
    context of the application and software that we design. The dimensions that are
    closer to the raw data can be assessed automatically more easily than the dimensions
    related to applications. For application-related dimensions, we often need to
    make an expert assessment or conduct manual analyses or investigations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些维度对于所有类型的应用都是通用的。例如，“无错误”维度对所有系统和所有机器学习模型都相关。同时，“相关性”必须在我们所设计和设计的应用和软件的上下文中进行评估。接近原始数据的维度比与应用相关的维度更容易自动评估。对于与应用相关的维度，我们通常需要进行专家评估或进行手动分析或调查。
- en: Take believability, for example. To assess whether our source data is trustworthy
    and can be used for this application, we need to understand where the data comes
    from, who/what created that data, and under which premises. This cannot be automated
    as it requires human, expert, judgment.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以可信度为例。为了评估我们的源数据是否可信以及是否可用于此应用，我们需要了解数据来自哪里，谁/什么创造了这些数据，以及基于哪些前提。这不能自动化，因为它需要人类、专家的判断。
- en: 'Therefore, we can organize these dimensions at different abstraction levels
    – raw data or source systems, data for training and inference, machine learning
    models and algorithms, and the entire software product. It can be useful to show
    which of these dimensions are closer to the raw data, which ones are closer to
    the algorithms, and which ones are closer to the products. *Figure 4**.3* shows
    this organization:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将这些维度组织在不同的抽象层次上 – 原始数据或源系统、用于训练和推理的数据、机器学习模型和算法，以及整个软件产品。显示这些维度中哪些更接近原始数据，哪些更接近算法，哪些更接近产品可能是有用的。*图4*.3展示了这种组织：
- en: '![Figure 4.3 – Data quality attributes organized according to their logical
    relevance](img/B19548_04_3.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 根据逻辑相关性组织的数据质量属性](img/B19548_04_3.jpg)'
- en: Figure 4.3 – Data quality attributes organized according to their logical relevance
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 根据逻辑相关性组织的数据质量属性
- en: '*Figure 4**.3* indicates that there is a level of abstraction in how we check
    information quality, which is entirely correct. The lowest abstraction levels,
    or the first checks, are intended to quantify the basic quality dimensions. These
    checks do not have to be very complex either. The entire quality measurement and
    monitoring system can be quite simplistic, yet very powerful. *Figure 4**.4* presents
    a conceptual design of such a system. This system consists of three parts – the
    machine learning pipeline, the log files, and the information quality measurement
    and monitoring:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4*.3表明我们在检查信息质量方面存在一个抽象层次，这是完全正确的。最低的抽象层次，或者说第一次检查，旨在量化基本的质量维度。这些检查也不必非常复杂。整个质量测量和监控系统可以非常简单，但非常强大。*图4*.4展示了这样一个系统的概念设计。该系统由三个部分组成
    – 机器学习管道、日志文件和信息质量测量与监控：'
- en: '![Figure 4.4 – Information quality measurement system](img/B19548_04_4.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 信息质量测量系统](img/B19548_04_4.jpg)'
- en: Figure 4.4 – Information quality measurement system
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 信息质量测量系统
- en: First, the machine learning pipeline contains probes, or measurement instruments,
    that collect information about problems related to information quality. For instance,
    these instruments can collect information if there are any problems with accessing
    the data, which can indicate problems with the *accessibility* quality dimension.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，机器学习管道包含探测器，或测量仪器，用于收集与信息质量相关的问题信息。例如，这些仪器可以收集有关访问数据是否存在问题的信息，这可以表明*可访问性*质量维度存在问题。
- en: 'The following code fragment shows how this can be realized in practice. It
    configures a rudimentary log file that collects the error information from the
    machine learning pipeline:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在实践中实现这一点。它配置了一个基本的日志文件，用于收集机器学习管道的错误信息：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code fragment creates log files, provides them with a unique name that
    the entire machine learning pipeline uses, and specifies the format of the error
    messages. Then, in the machine learning pipeline itself, the log files are propagated
    with messages. The following code fragment shows an example of how the data export
    tool presented previously is instrumented to propagate this information:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建日志文件，为它们提供整个机器学习管道使用的唯一名称，并指定错误消息的格式。然后，在机器学习管道本身中，日志文件通过消息进行传播。以下代码片段展示了之前展示的数据导出工具是如何被配置为传播这些信息的：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 2023-01-15 17:11:45,618;Gerrit data export pipeline;INFO;Configuration started
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 2023-01-15 17:11:45,618;Gerrit数据导出管道;INFO;配置开始
- en: 2023-01-15 17:11:45,951;Gerrit data export pipeline;INFO;Configuration ended
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2023-01-15 17:11:45,951;Gerrit数据导出管道;INFO;配置结束
- en: 2023-01-15 17:11:46,052;Gerrit data export pipeline;INFO;Downloading fresh data
    to ./gerrit_reviews.csv
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 2023-01-15 17:11:46,052;Gerrit数据导出管道;INFO;将新鲜数据下载到./gerrit_reviews.csv
- en: '2023-01-15 17:11:46,055;pygerrit2;DEBUG;Error parsing netrc: netrc missing
    or no credentials found in netrc'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 2023-01-15 17:11:46,055;pygerrit2;DEBUG;解析netrc错误：netrc文件缺失或未在netrc中找到凭据
- en: 2023-01-15 17:11:46,057;Gerrit data export pipeline;INFO;Geting the data about
    changes from 0 to 500
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2023-01-15 17:11:46,057;Gerrit数据导出管道;INFO;获取从0到500的更改数据
- en: '2023-01-15 17:11:46,060;urllib3.connectionpool;DEBUG;Starting new HTTPS connection
    (1): gerrit.onap.org:443'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2023-01-15 17:11:46,060;urllib3.connectionpool;DEBUG;开始新的HTTPS连接（1）：gerrit.onap.org:443
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'try:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'try:'
- en: logFile = open("./information_quality_gerrit.log", "r")
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: logFile = open("./information_quality_gerrit.log", "r")
- en: 'for logMessage in logFile:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'for logMessage in logFile:'
- en: '# splitting the log information - again, this is linked to how we'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '# 再次分割日志信息 - 这与我们的方法有关'
- en: '# structured the log message in the measurement system'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '# 在测量系统中结构化日志消息'
- en: logItem = logMessage.split(';')
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: logItem = logMessage.split(';')
- en: logLevel = logItem[2]
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: logLevel = logItem[2]
- en: logSource = logItem[1]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: logSource = logItem[1]
- en: logTime = logItem[0]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: logTime = logItem[0]
- en: logProblem = logItem[3]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: logProblem = logItem[3]
- en: '# this part is about extracting the relevant information'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '# 这部分是关于提取相关信息'
- en: '# if this is a problem at all:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '# 如果这确实是一个问题：'
- en: 'if (logLevel == ''ERROR''):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (logLevel == ''ERROR''):'
- en: '# if this is a problem with the library'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '# 如果这是库的问题'
- en: 'if (''LIBRARIES'' in logProblem):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (''LIBRARIES'' in logProblem):'
- en: iq_problems_configuration_libraries += 1
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: iq_problems_configuration_libraries += 1
- en: 'if (''ENTITY_ACCESS'' in logProblem):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'if (''ENTITY_ACCESS'' in logProblem):'
- en: iq_problems_entity_access += 1
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: iq_problems_entity_access += 1
- en: 'except Exception as e:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 'except Exception as e:'
- en: iq_general_error = 1
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: iq_general_error = 1
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'def getIndicatorColor(ind_value):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 'def getIndicatorColor(ind_value):'
- en: 'if ind_value > 0:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'if ind_value > 0:'
- en: return 'red'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: return 'red'
- en: 'else:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: return 'green'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: return 'green'
- en: columns = ('Information quality check', 'Value')
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列 = ('信息质量检查', '值')
- en: rows = ['Entity access check', 'Libraries']
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 行 = ['实体访问检查', '库']
- en: 'cell_text = [[f''Entity access: {iq_problems_entity_access}''],'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'cell_text = [[f''实体访问: {iq_problems_entity_access}''],'
- en: '[f''Libraries: {iq_problems_configuration_libraries}'']]'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[f''库: {iq_problems_configuration_libraries}'']]'
- en: colors = [[getIndicatorColor(iq_problems_entity_access)],
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色 = [[getIndicatorColor(iq_problems_entity_access)],
- en: '[getIndicatorColor(iq_problems_configuration_libraries)]]'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[getIndicatorColor(iq_problems_configuration_libraries)]]'
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: fig, ax = plt.subplots()
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: fig, ax = plt.subplots()
- en: ax.axis('tight')
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ax.axis('tight')
- en: ax.axis('off')
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ax.axis('off')
- en: the_table = ax.table(cellText=cell_text,
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: the_table = ax.table(cellText=cell_text,
- en: cellColours=colors,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: cellColours=colors,
- en: colLabels=columns,
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: colLabels=columns,
- en: loc='left')
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: loc='left')
- en: plt.show()
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: now, let's check if there are any duplicate entries
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在，让我们检查是否有任何重复条目
- en: get the number of all data points
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取所有数据点的数量
- en: allDataPoints = len(dfData.Summary)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: allDataPoints = len(dfData.Summary)
- en: and get the number of unique data points
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取唯一数据点的数量
- en: uniqueDataPoints = len(dfData.Summary.unique())
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: uniqueDataPoints = len(dfData.Summary.unique())
- en: check if the number of unique and all data points is the same
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查唯一数据点和所有数据点的数量是否相同
- en: 'if allDataPoints != uniqueDataPoints:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'if allDataPoints != uniqueDataPoints:'
- en: print(f'There are {allDataPoints - uniqueDataPoints} duplicate entries, which
    can potentially be noisy')
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'有 {allDataPoints - uniqueDataPoints} 个重复条目，这可能会产生噪声')
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: then, we find duplicate data points
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 然后，我们找到重复数据点的索引
- en: first we group the datapoints
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 首先，我们分组数据点
- en: dfGrouped = dfData.groupby(by=dfData.Summary).count()
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: dfGrouped = dfData.groupby(by=dfData.Summary).count()
- en: then we find the index of the ones that are not unique
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 然后，我们找到那些不唯一的索引
- en: lstDuplicated = dfGrouped[dfGrouped.Time > 1].index.to_list()
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: lstDuplicated = dfGrouped[dfGrouped.Time > 1].index.to_list()
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: for each of these data points, we check if these data points
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于这些数据点中的每一个，我们检查这些数据点
- en: are classified to different labels adn remove only the ones that have different
    labels
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 被分类到不同的标签，并仅移除具有不同标签的标签
- en: 'for onePoint in lstDuplicated:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'for onePoint in lstDuplicated:'
- en: '# find all instances of this datapoint'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '# 找到这个数据点的所有实例'
- en: dfPoint = dfData[dfData.Summary == onePoint]
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: dfPoint = dfData[dfData.Summary == onePoint]
- en: '# now check if these data points have a different score'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '# 现在检查这些数据点是否有不同的分数'
- en: numLabels = len(dfPoint.Score.unique())
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: numLabels = len(dfPoint.Score.unique())
- en: '# if the number of labels is more than 1, then'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '# 如果标签数量超过1，那么'
- en: '# this means that we have noise in the dataset'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '# 这意味着数据集中有噪声'
- en: '# and we should remove this point'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '# 我们应该移除这个点'
- en: 'if numLabels > 1:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'if numLabels > 1:'
- en: dfData.drop(dfData[dfData.Summary == onePoint].index, inplace=True)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: dfData.drop(dfData[dfData.Summary == onePoint].index, inplace=True)
- en: '# let''s also print the data point that we remove'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '# 让我们也打印出我们移除的数据点'
- en: 'print(f''point: {onePoint}, number of labels: {len(dfPoint.Score.unique())}'')'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'print(f''点: {onePoint}, 标签数量: {len(dfPoint.Score.unique())}'')'
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
