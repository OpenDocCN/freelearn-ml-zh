- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Acquisition, Data Quality, and Noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data for machine learning systems can come directly from humans and software
    systems – usually called *source systems*. Where the data comes from has implications
    regarding what it looks like, what kind of quality it has, and how to process
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The data that originates from humans is usually noisier than data that originates
    from software systems. We, as humans, are known for small inconsistencies and
    we can also understand things inconsistently. For example, the same defect reported
    by two different people could have a very different description; the same is true
    for requirements, designs, and source code.
  prefs: []
  type: TYPE_NORMAL
- en: The data that originates from software systems is often more consistent and
    contains less noise or the noise in the data is more regular than the noise in
    the human-generated data. This data is generated by source systems. Therefore,
    controlling and monitoring the quality of the data that’s generated automatically
    is different – for example, software systems do not “lie” in the data, so it makes
    no sense to check the believability of the data that’s generated automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The different sources of data and what we can do with them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to assess the quality of data that’s used for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to identify, measure, and reduce noise in data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of data and what we can do with them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning software has become increasingly important in all fields today.
    Anything from telecommunication networks, self-driving vehicles, computer games,
    smart navigation systems, and facial recognition to websites, news production,
    cinematography, and experimental music creation can be done using machine learning.
    Some applications are very successful at, for example, using machine learning
    in search strings (BERT models). Some applications are not so successful, such
    as using machine learning in hiring processes. Often, this depends on the programmers,
    data scientists, or models that are used in these applications. However, in most
    cases, the success of a machine learning application is often in the data that
    is used to train it and use it. It depends on the quality of that data and the
    features that are extracted from it. For example, Amazon’s machine learning recommender
    was taken out of operation because it was biased against women. Since it was trained
    on historical recruitment data, which predominantly included male candidates,
    the system tended to recommend male candidates in future hiring.
  prefs: []
  type: TYPE_NORMAL
- en: Data that’s used in machine learning systems can originate from all kinds of
    sources. However, we can classify these sources into two main types – manual/human
    and automated software/hardware. These two types have different characteristics
    that determine how to organize these sources and how to extract features from
    the data from these sources. *Figure 4**.1* illustrates these types of data and
    provides examples of data of each type.
  prefs: []
  type: TYPE_NORMAL
- en: Manually generated data is the data that comes from human input or originates
    in humans. This kind of data is often much richer in information than the data
    generated by software, but it also has much larger variability. This variability
    can come from the natural variability of us, humans. The same question in a form,
    for example, can be understood and answered differently by two different people.
    This kind of data is often more susceptible to random errors than systematic ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatically generated data originates from hardware or software systems,
    usually measured by sensors or measurement scripts that collect data from other
    systems, products, processes, or organizations. This kind of data is often more
    consistent and repeatable but also more susceptible to systematic errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Sources of data and their classification. The green part of
    this figure is the scope of this book](img/B19548_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Sources of data and their classification. The green part of this
    figure is the scope of this book
  prefs: []
  type: TYPE_NORMAL
- en: An example of data that originates from humans, and is often used in machine
    learning, is data about software defects. A human tester often inspects a problem
    and reports it using a form. This form contains fields about the testing phase,
    components affected, impact on the customer, and more, but one part of the form
    is often a description of the problem in natural language, which is an interpretation
    of what happens when it’s authored by a human tester.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of data that’s generated by humans is source code. We, as programmers,
    write source code for the software in a programming language with a given syntax,
    and we use programming guidelines to keep a consistent style so that the product
    of our work – the source code – can be automatically interpreted or compiled by
    software. There are some structures in the code that we write, but it is far from
    being consistent. Even the same algorithm, when implemented by two different programmers,
    differs in the naming of variables, their types, or even how the problem is solved.
    A good example of this is the Rosetta code website, which provides the same solutions
    in different programming languages and sometimes even in the same programming
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement specifications or data input using forms have the same properties
    and characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is one origin of data that is particularly interesting – medical
    data. This is the data from patients’ records and charts, input by the medical
    specialists as part of medical procedures. This data can be electronic, but it
    reflects the specialists’ understanding of the symptoms and their interpretation
    of the medical tests and diagnoses.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we have data that’s generated by software or hardware in
    one way or another. The data that’s generated automatically is more consistent,
    although not free from problems, and more repetitive. An example of such data
    is the data generated in telecommunication networks to transmit information from
    one telecommunication node to another. The radio signals are very stable compared
    to other types of data, and can be disturbed by external factors such as weather
    (precipitation) or obstructions such as construction cranes. The data is repeatable,
    with all variability originating from the external factors.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the data from vehicles, which register information around
    them and store it for further processing. This data can contain signaling between
    the vehicle’s components as well as communication with other vehicles or the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Medical data, such as **electroencephalograph** (**EEG** – that is, brain waves)
    or **electrocardiogram** (**ECG** – that is, heart rate), is collected from source
    systems, which we can see as measurement instruments. So, technically, it has
    been generated by computer systems, but it comes from human patients. This origin
    in patients means that the data has the same natural variability as any other
    data collected from humans. As every patient is a bit different, and the measurement
    systems can be attached to each patient a bit differently, the data generated
    by each patient differs slightly from other patients. For example, the ECG heartbeat
    data contains basic, consistent information – the number of beats per minute (among
    other parameters). However, the raw data differs in the amplitude of the ECG signal
    (depending on the placement of the measurement electrode) or the difference between
    spikes in the curve (R and T-spikes).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, my first best practice in this chapter has to do with the origin
    of the data that we use for software systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #25'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the origin of the data used in your software and create your data processing
    pipeline accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Since all types of data require different ways of working in terms of cleaning,
    formatting, and feature extraction, we should make sure that we know how the data
    is produced and what kind of problems we can expect (and handle). Therefore, first,
    we need to identify what kind of data we need, where it comes from, and what kind
    of problems it can carry with it.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from software engineering tools – Gerrit and Jira
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate how to work with data extraction, let’s extract data from a popular
    software engineering tool for code reviews – Gerrit. This tool is used for reviewing
    and discussing fragments of code developed by individual programmers, just before
    they are integrated into the main code base of the product.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following program code shows how to access the database of Gerrit – that
    is, through the JSON API – and how to extract the list of all changes for a specific
    project. This program uses the Python `pygerrit2` package ([https://pypi.org/project/pygerrit2/](https://pypi.org/project/pygerrit2/)).
    This module helps us use the JSON API as it provides Python functions instead
    of just JSON strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The key line in this code fragment is `rest.get("/changes/?q=status:merged&o=ALL_FILES&o=ALL_REVISIONS&o=DETAILED_LABELS&start=0",
    headers={''Content-Type'': ''application/json''})`. This line specifies the endpoint
    for the changes to be retrieved, along with the parameters. It says that we want
    to access all files and all revisions, as well as all details of the changes.
    In these details, we can find the information about all revisions (particular
    patches/commits) and we can then parse each of these revisions. It is important
    to know that the JSON API returns a maximum of 500 changes in each query, and
    therefore, the last parameter – `start=0` – can be used to access changes from
    500 upward. The output of this program is a very long list of changes in JSON,
    so I will not present it in detail in this book. Instead, I encourage you to execute
    this script and go through this file at your own pace. The script can be found
    in this book’s GitHub repository at [https://github.com/miroslawstaron/machine_learning_best_practices](https://github.com/miroslawstaron/machine_learning_best_practices),
    under [*Chapter 4*](B19548_04.xhtml#_idTextAnchor049). The name of the script
    is `gerrit_exporter.ipynb`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, extracting just the list of changes is not very useful for analyses as
    it only provides the information that has been collected automatically – for example,
    which revisions exist, and who created these revisions. It does not contain information
    about which files and lines are commented on, or what the comments are – in other
    words, the particularly useful information. Therefore, we need to interact with
    Gerrit a bit more, as presented in *Figure 4**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program flow presented in *Figure 4**.2* illustrates the complexity of
    the relationships in a code comment database such as Gerrit. Therefore, the program
    to access the database and export this data is a bit too long for this book. It
    can be accessed in the same repository as the previous one, under the name `gerrit_exporter_loop.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Interactions with Gerrit to extract commented files, commented
    lines, and the content of the comments](img/B19548_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Interactions with Gerrit to extract commented files, commented
    lines, and the content of the comments
  prefs: []
  type: TYPE_NORMAL
- en: This kind of data can be used to train machine learning models to review the
    code or even identify which lines of code need to be reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: When working with Gerrit, I have found the following best practice to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice # 26'
  prefs: []
  type: TYPE_NORMAL
- en: Extract as much data as you need and store it locally to reduce the disturbances
    for the software engineers who use the tool for their work.
  prefs: []
  type: TYPE_NORMAL
- en: Although it is possible to extract the changes one by one, it is better to extract
    the whole set of changes once and keep a local copy of it. In this way, we reduce
    the strain on the servers that others use for their daily work. We must remember
    that data extraction is a secondary task for these source systems, while their
    primary task is to support software engineers in their work.
  prefs: []
  type: TYPE_NORMAL
- en: Another good source of data for software systems that support software engineering
    tasks is JIRA, an issue and task management system. JIRA is used to document epics,
    user stories, software defects, and tasks and has become one of the most popular
    tools for this kind of activity.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can extract a lot of useful information about the processes from
    JIRA as a source system. Then, we can use this information to develop machine
    learning models to assess and improve tasks and requirements (in the form of user
    stories) and design tools that can help us identify overlapping user stories or
    group them into more coherent epics. Such software can be used to improve the
    quality of these tasks or provide better estimations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code fragment illustrates how to make a connection to a JIRA
    instance and then how to extract all issues for a particular project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this fragment, for illustration purposes, I’m using my own JIRA database
    and my own project (`MLBPB`). This code requires the `atlassian-python-api` module
    to be imported. This module provides an API to connect to and interact with a
    JIRA database using Python, in a similar way as the API of Gerrit. Therefore,
    the same best practice as for Gerrit applies to JIRA.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from product databases – GitHub and Git
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: JIRA and Gerrit are, to some extent, additional tools to the main product development
    tools. However, every software development organization uses a source code repository
    to store the main asset – the source code of the company’s software product. Today,
    the tools that are used the most are Git version control and its close relative,
    GitHub. Source code repositories can be a very useful source of data for machine
    learning systems – we can extract the source code of the product and analyze it.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub is a great source of data for machine learning if we use it responsibly.
    Please remember that the source code provided as open source, by the community,
    is not for profiting off. We need to follow the licenses and we need to acknowledge
    the contributions that were made by the authors, contributors, and maintainers
    of the open source community. Regardless of the license, we are always able to
    analyze our own code or the code that belongs to our company.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we can access the source code of our product or the product that we want
    to analyze, the following code fragment helps us connect to the GitHub server
    and access a repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To support secure access to the code, GitHub uses access tokens rather than
    passwords when connecting to it. We can also use SSL and CLI interfaces, but for
    the sake of simplicity, we’ll use the HTTPS protocol with a token. The `g = Github(token,
    per_page=100)` line uses the token to instantiate the PyGitHub library’s main
    class. The token is individual and needs to be generated either per repository
    or per user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The connection to the repository is done by the next line, `repo = g.get_repo("miroslawstaron/machine_learning_best_practices")`,
    which, in this example, connects to the repository associated with this book.
    Finally, the last line in the code fragment obtains the number of commits in the
    repository. Once obtained, we can print it and start analyzing it, as shown in
    the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of this code fragment prints out the commit message of the latest
    commit. It is worth noting that the latest commit is always first in the list
    of commits. Once we know the commit, we can also access the list of files that
    are included in that commit. This is illustrated by the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing the list of the files in the commits is good, but it’s not very useful.
    Something more useful is to access these files and analyze them. The following
    code fragment shows how to access two files from the latest two commits. First,
    we access the files, after which we download their content and store it in two
    different variables – `linesOne` and `linesTwo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can analyze the two files for one of the most important tasks –
    to get the diff between the two files. We can use the `difflib` Python library
    for this task, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code fragment allows us to identify differences between files
    and print them in a way similar to how GitHub presents the differences.
  prefs: []
  type: TYPE_NORMAL
- en: My next best practice is related to the use of public repositories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice # 27'
  prefs: []
  type: TYPE_NORMAL
- en: When accessing data from public repositories, please check the licenses and
    ensure you acknowledge the contribution of the community that created the analyzed
    code.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned previously, open source programs are here for everyone to use,
    including to analyze and learn from them. However, the community behind this source
    code has spent countless hours creating and perfecting it. Therefore, we should
    use these repositories responsibly. If we use the repositories to create our own
    products, including machine learning software products, we need to acknowledge
    the community’s contribution and, if we use the software under a copyleft license,
    give back our work to the community.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing and developing machine learning systems, we consider the data
    quality on a relatively low level. We look for missing values, outliers, or similar.
    They are important because they can cause problems when training machine learning
    models. Nevertheless, they are nearly enough from a software engineering perspective.
  prefs: []
  type: TYPE_NORMAL
- en: When engineering reliable software systems, we need to know more about the data
    we use than whether it contains (or not) missing values. We need to know whether
    we can trust the data (whether it is believable), whether the data is representative,
    or whether it is up to date. So, we need a quality model for our data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several quality models for data in software engineering, and the one
    I often use, and recommend, is the AIMQ model – a methodology for assessing information
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quality dimensions of the AIMQ model are as follows (cited from Lee, Y.W.,
    et al., *AIMQ: a methodology for information quality assessment*. Information
    & management, 2002\. 40(2): p. 133-146):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accessibility**: The information is easily retrievable and accessible to
    our system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appropriate amount**: The information is of sufficient volume for our needs
    and for our applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Believability**: The information is trustworthy and can be believed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness**: The information includes all the necessary values for our
    system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concise representation**: The information is formatted compactly and appropriately
    for our application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistent representation**: The information is consistently presented in
    the same format, including its representation over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ease of operation**: The information is easy to manipulate to meet our needs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Free of errors**: The information is correct, accurate, and reliable for
    the application that we’re creating'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability**: It is easy to interpret what the information means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Objectivity**: The information was objectively collected and is based on
    facts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance**: The information is useful, relevant, appropriate, and applicable
    to our system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reputation**: The information has a good reputation for quality and comes
    from good sources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: The information is protected from unauthorized access and sufficiently
    restricted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timeliness**: The information is sufficiently current for our work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understandability**: The information is easy to understand and comprehend'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these dimensions are certainly universal for all kinds of applications.
    For example, the *free from error* dimension is relevant for all systems and all
    machine learning models. At the same time, *relevance* must be evaluated in the
    context of the application and software that we design. The dimensions that are
    closer to the raw data can be assessed automatically more easily than the dimensions
    related to applications. For application-related dimensions, we often need to
    make an expert assessment or conduct manual analyses or investigations.
  prefs: []
  type: TYPE_NORMAL
- en: Take believability, for example. To assess whether our source data is trustworthy
    and can be used for this application, we need to understand where the data comes
    from, who/what created that data, and under which premises. This cannot be automated
    as it requires human, expert, judgment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can organize these dimensions at different abstraction levels
    – raw data or source systems, data for training and inference, machine learning
    models and algorithms, and the entire software product. It can be useful to show
    which of these dimensions are closer to the raw data, which ones are closer to
    the algorithms, and which ones are closer to the products. *Figure 4**.3* shows
    this organization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Data quality attributes organized according to their logical
    relevance](img/B19548_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Data quality attributes organized according to their logical relevance
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.3* indicates that there is a level of abstraction in how we check
    information quality, which is entirely correct. The lowest abstraction levels,
    or the first checks, are intended to quantify the basic quality dimensions. These
    checks do not have to be very complex either. The entire quality measurement and
    monitoring system can be quite simplistic, yet very powerful. *Figure 4**.4* presents
    a conceptual design of such a system. This system consists of three parts – the
    machine learning pipeline, the log files, and the information quality measurement
    and monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Information quality measurement system](img/B19548_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Information quality measurement system
  prefs: []
  type: TYPE_NORMAL
- en: First, the machine learning pipeline contains probes, or measurement instruments,
    that collect information about problems related to information quality. For instance,
    these instruments can collect information if there are any problems with accessing
    the data, which can indicate problems with the *accessibility* quality dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code fragment shows how this can be realized in practice. It
    configures a rudimentary log file that collects the error information from the
    machine learning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment creates log files, provides them with a unique name that
    the entire machine learning pipeline uses, and specifies the format of the error
    messages. Then, in the machine learning pipeline itself, the log files are propagated
    with messages. The following code fragment shows an example of how the data export
    tool presented previously is instrumented to propagate this information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 2023-01-15 17:11:45,618;Gerrit data export pipeline;INFO;Configuration started
  prefs: []
  type: TYPE_NORMAL
- en: 2023-01-15 17:11:45,951;Gerrit data export pipeline;INFO;Configuration ended
  prefs: []
  type: TYPE_NORMAL
- en: 2023-01-15 17:11:46,052;Gerrit data export pipeline;INFO;Downloading fresh data
    to ./gerrit_reviews.csv
  prefs: []
  type: TYPE_NORMAL
- en: '2023-01-15 17:11:46,055;pygerrit2;DEBUG;Error parsing netrc: netrc missing
    or no credentials found in netrc'
  prefs: []
  type: TYPE_NORMAL
- en: 2023-01-15 17:11:46,057;Gerrit data export pipeline;INFO;Geting the data about
    changes from 0 to 500
  prefs: []
  type: TYPE_NORMAL
- en: '2023-01-15 17:11:46,060;urllib3.connectionpool;DEBUG;Starting new HTTPS connection
    (1): gerrit.onap.org:443'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: logFile = open("./information_quality_gerrit.log", "r")
  prefs: []
  type: TYPE_NORMAL
- en: 'for logMessage in logFile:'
  prefs: []
  type: TYPE_NORMAL
- en: '# splitting the log information - again, this is linked to how we'
  prefs: []
  type: TYPE_NORMAL
- en: '# structured the log message in the measurement system'
  prefs: []
  type: TYPE_NORMAL
- en: logItem = logMessage.split(';')
  prefs: []
  type: TYPE_NORMAL
- en: logLevel = logItem[2]
  prefs: []
  type: TYPE_NORMAL
- en: logSource = logItem[1]
  prefs: []
  type: TYPE_NORMAL
- en: logTime = logItem[0]
  prefs: []
  type: TYPE_NORMAL
- en: logProblem = logItem[3]
  prefs: []
  type: TYPE_NORMAL
- en: '# this part is about extracting the relevant information'
  prefs: []
  type: TYPE_NORMAL
- en: '# if this is a problem at all:'
  prefs: []
  type: TYPE_NORMAL
- en: 'if (logLevel == ''ERROR''):'
  prefs: []
  type: TYPE_NORMAL
- en: '# if this is a problem with the library'
  prefs: []
  type: TYPE_NORMAL
- en: 'if (''LIBRARIES'' in logProblem):'
  prefs: []
  type: TYPE_NORMAL
- en: iq_problems_configuration_libraries += 1
  prefs: []
  type: TYPE_NORMAL
- en: 'if (''ENTITY_ACCESS'' in logProblem):'
  prefs: []
  type: TYPE_NORMAL
- en: iq_problems_entity_access += 1
  prefs: []
  type: TYPE_NORMAL
- en: 'except Exception as e:'
  prefs: []
  type: TYPE_NORMAL
- en: iq_general_error = 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'def getIndicatorColor(ind_value):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if ind_value > 0:'
  prefs: []
  type: TYPE_NORMAL
- en: return 'red'
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: return 'green'
  prefs: []
  type: TYPE_NORMAL
- en: columns = ('Information quality check', 'Value')
  prefs: []
  type: TYPE_NORMAL
- en: rows = ['Entity access check', 'Libraries']
  prefs: []
  type: TYPE_NORMAL
- en: 'cell_text = [[f''Entity access: {iq_problems_entity_access}''],'
  prefs: []
  type: TYPE_NORMAL
- en: '[f''Libraries: {iq_problems_configuration_libraries}'']]'
  prefs: []
  type: TYPE_NORMAL
- en: colors = [[getIndicatorColor(iq_problems_entity_access)],
  prefs: []
  type: TYPE_NORMAL
- en: '[getIndicatorColor(iq_problems_configuration_libraries)]]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: fig, ax = plt.subplots()
  prefs: []
  type: TYPE_NORMAL
- en: ax.axis('tight')
  prefs: []
  type: TYPE_NORMAL
- en: ax.axis('off')
  prefs: []
  type: TYPE_NORMAL
- en: the_table = ax.table(cellText=cell_text,
  prefs: []
  type: TYPE_NORMAL
- en: cellColours=colors,
  prefs: []
  type: TYPE_NORMAL
- en: colLabels=columns,
  prefs: []
  type: TYPE_NORMAL
- en: loc='left')
  prefs: []
  type: TYPE_NORMAL
- en: plt.show()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: now, let's check if there are any duplicate entries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: get the number of all data points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: allDataPoints = len(dfData.Summary)
  prefs: []
  type: TYPE_NORMAL
- en: and get the number of unique data points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: uniqueDataPoints = len(dfData.Summary.unique())
  prefs: []
  type: TYPE_NORMAL
- en: check if the number of unique and all data points is the same
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'if allDataPoints != uniqueDataPoints:'
  prefs: []
  type: TYPE_NORMAL
- en: print(f'There are {allDataPoints - uniqueDataPoints} duplicate entries, which
    can potentially be noisy')
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: then, we find duplicate data points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: first we group the datapoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: dfGrouped = dfData.groupby(by=dfData.Summary).count()
  prefs: []
  type: TYPE_NORMAL
- en: then we find the index of the ones that are not unique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lstDuplicated = dfGrouped[dfGrouped.Time > 1].index.to_list()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: for each of these data points, we check if these data points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: are classified to different labels adn remove only the ones that have different
    labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'for onePoint in lstDuplicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '# find all instances of this datapoint'
  prefs: []
  type: TYPE_NORMAL
- en: dfPoint = dfData[dfData.Summary == onePoint]
  prefs: []
  type: TYPE_NORMAL
- en: '# now check if these data points have a different score'
  prefs: []
  type: TYPE_NORMAL
- en: numLabels = len(dfPoint.Score.unique())
  prefs: []
  type: TYPE_NORMAL
- en: '# if the number of labels is more than 1, then'
  prefs: []
  type: TYPE_NORMAL
- en: '# this means that we have noise in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '# and we should remove this point'
  prefs: []
  type: TYPE_NORMAL
- en: 'if numLabels > 1:'
  prefs: []
  type: TYPE_NORMAL
- en: dfData.drop(dfData[dfData.Summary == onePoint].index, inplace=True)
  prefs: []
  type: TYPE_NORMAL
- en: '# let''s also print the data point that we remove'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f''point: {onePoint}, number of labels: {len(dfPoint.Score.unique())}'')'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
