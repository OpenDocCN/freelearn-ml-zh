- en: Java Libraries and Platforms for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing machine learning algorithms by yourself is probably the best way
    to learn machine learning, but you can progress much faster if you step on the
    shoulders of the giants and leverage one of the existing open source libraries.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter reviews various libraries and platforms for machine learning in
    Java. The goal is to understand what each library brings to the table and what
    kind of problems it is able to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The requirement of Java for implementing a machine learning application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weka, a general purpose machine learning platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Java machine learning library, a collection of machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout, a scalable machine learning platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark, a distributed machine learning library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning4j, a deep learning library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MALLET, a text mining library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll also discuss how to design the complete machine learning application stack
    for both single-machine and big data apps by using these libraries with other
    components.
  prefs: []
  type: TYPE_NORMAL
- en: The need for Java
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: New machine learning algorithms are often first scripted at university labs,
    gluing together several languages such as shell scripting, Python, R, MATLAB,
    Scala, or C++ to provide a new concept and theoretically analyze its properties.
    An algorithm might take a long path of refactoring before it lands in a library
    with standardized input or output and interfaces. While Python, R, and MATLAB
    are quite popular, they are mainly used for scripting, research, and experimenting.
    Java, on the other hand, is the de facto enterprise language, which could be attributed
    to static typing, robust IDE support, good maintainability, as well as decent
    threading model and high performance concurrent data structure libraries. Moreover,
    there are already many Java libraries available for machine learning, which makes
    it really convenient to apply them in existing Java applications and leverage
    powerful machine learning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are over 70 Java-based open source machine learning projects listed on
    the [MLOSS.org](https://mloss.org/software/) website, and probably many more unlisted
    projects live at university servers, GitHub, or Bitbucket. In this section, we
    will review the major libraries and platforms, the kind of problems they can solve,
    the algorithms they support, and the kind of data they can work with.
  prefs: []
  type: TYPE_NORMAL
- en: Weka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Waikato Environment for Knowledge Analysis** (**WEKA**) is a machine learning
    library that was developed at the University of Waikato, New Zealand, and is probably
    the most well-known Java library. It is a general purpose library that is able
    to solve a wide variety of machine learning tasks, such as classification, regression,
    and clustering. It features a rich graphical user interface, command-line interface,
    and Java API. You can check out Weka at [http://www.cs.waikato.ac.nz/ml/weka/](http://www.cs.waikato.ac.nz/ml/weka/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this book, Weka contains 267 algorithms in total: data
    preprocessing (82), attribute selection (33), classification and regression (133),
    clustering (12), and association rules mining (7). Graphical interfaces are well
    suited for exploring your data, while the Java API allows you to develop new machine
    learning schemes and use the algorithms in your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Weka is distributed under the **GNU General Public License** (**GNU GPL**),
    which means that you can copy, distribute, and modify it as long as you track
    changes in source files and keep it under GNU GPL. You can even distribute it
    commercially, but you must disclose the source code or obtain a commercial license.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to several supported file formats, Weka features its own default
    data format, ARFF, to describe data by attribute-data pairs. It consists of two
    parts. The first part contains a header, which specifies all of the attributes
    and their types, for instance, nominal, numeric, date, and string. The second
    part contains the data, where each line corresponds to an instance. The last attribute
    in the header is implicitly considered the target variable and missing data is
    marked with a question mark. For example, returning to the example from [Chapter
    1](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml), *Applied Machine Learning Quick
    Start*, the `Bob` instance written in an ARFF file format would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The file consists of three sections. The first section starts with the `@RELATION
    <String>` keyword, specifying the dataset name. The next section starts with the
    `@ATTRIBUTE` keyword, followed by the attribute name and type. The available types
    are `STRING`, `NUMERIC`, `DATE`, and a set of categorical values. The last attribute
    is implicitly assumed to be the target variable that we want to predict. The last
    section starts with the `@DATA` keyword, followed by one instance per line. Instance
    values are separated by commas and must follow the same order as attributes in
    the second section.
  prefs: []
  type: TYPE_NORMAL
- en: More Weka examples will be demonstrated in [Chapter 3](e0c71e12-6bd7-4f63-b71d-78bb5a87b801.xhtml),
    *Basic Algorithms – Classification, Regression, and Clustering*, and [Chapter
    4](6ac8d4de-1e7f-4f60-9cf0-93ab2fe55e4d.xhtml), *Customer Relationship Prediction
    with Ensembles*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about Weka, pick up a quick-start book—*Weka How-to,* by *Kaluza, Packt
    Publishing* to start coding, or look into *Data Mining: Practical Machine Learning
    Tools and Techniques with Java Implementations* by *Witten and Frank*, *Morgan
    Kaufmann Publishers* for theoretical background and in-depth explanations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weka''s Java API is organized into the following top-level packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`weka.associations`: These are data structures and algorithms for association
    rules learning, including **Apriori**, **predictive Apriori**, **FilteredAssociator**, **FP-Growth**,
    **Generalized Sequential Patterns** (**GSP**), **hotSpot**, and **Tertius**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers`: These are supervised learning algorithms, evaluators, and
    data structures. The package is further split into the following components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.bayes`: This implements Bayesian methods, including Naive
    Bayes, Bayes net, Bayesian logistic regression, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.evaluation`: These are supervised evaluation algorithms for
    nominal and numerical prediction, such as evaluation statistics, confusion matrix,
    ROC curve, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.functions`: These are regression algorithms, including linear
    regression, isotonic regression, Gaussian processes, **Support Vector Machines **(**SVMs**),
    multilayer perceptron, voted perceptron, and others.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.lazy`: These are instance-based algorithms such as k-nearest
    neighbors, K*, and lazy Bayesian rules.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.meta`: These are supervised learning meta-algorithms, including
    AdaBoost, bagging, additive regression, random committee, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.mi`: These are multiple-instance learning algorithms, such
    as citation k-nearest neighbors, diverse density, AdaBoost, and others.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.rules`: These are decision tables and decision rules based
    on the separate-and-conquer approach, RIPPER, PART, PRISM, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.trees`: These are various decision trees algorithms, including
    ID3, C4.5, M5, functional tree, logistic tree, random forest, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.clusterers`: These are clustering algorithms, including k-means, CLOPE,
    Cobweb, DBSCAN hierarchical clustering, and FarthestFirst.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.core`: These are various utility classes such as the attribute class,
    statistics class, and instance class.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.datagenerators`: These are data generators for classification, regression,
    and clustering algorithms.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.estimators`: These are various data distribution estimators for discrete/nominal
    domains, conditional probability estimations, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.experiment`: These are a set of classes supporting necessary configuration,
    datasets, model setups, and statistics to run experiments.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.filters`: These are attribute-based and instance-based selection algorithms
    for both supervised and unsupervised data preprocessing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.gui`: These are graphical interface implementing explorer, experimenter,
    and knowledge flow applications. The Weka Explorer allows you to investigate datasets,
    algorithms, as well as their parameters, and visualize datasets with scatter plots
    and other visualizations. The Weka Experimenter is used to design batches of experiments,
    but it can only be used for classification and regression problems.The Weka KnowledgeFlow
    implements a visual drag-and-drop user interface to build data flows and, for
    example, load data, apply filter, build classifier, and evaluate it.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Java machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Java Machine Learning Library** (**Java-ML**) is a collection of machine
    learning algorithms with a common interface for algorithms of the same type. It
    only features the Java API, and so it is primarily aimed at software engineers
    and programmers. Java-ML contains algorithms for data preprocessing, feature selection,
    classification, and clustering. In addition, it features several Weka bridges
    to access Weka's algorithms directly through the Java-ML API. It can be downloaded
    from [http://java-ml.sourceforge.net](http://java-ml.sourceforge.net/).
  prefs: []
  type: TYPE_NORMAL
- en: Java-ML is also a general-purpose machine learning library. Compared to Weka,
    it offers more consistent interfaces and implementations of recent algorithms
    that are not present in other packages, such as an extensive set of state-of-the-art
    similarity measures and feature-selection techniques, for example, **dynamic time
    warping** (**DTW**), random forest attribute evaluation, and so on. Java-ML is
    also available under the GNU GPL license.
  prefs: []
  type: TYPE_NORMAL
- en: Java-ML supports all types of files as long as they contain one data sample
    per line and the features are separated by a symbol such as a comma, semicolon,
    or tab.
  prefs: []
  type: TYPE_NORMAL
- en: 'The library is organized around the following top-level packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`net.sf.javaml.classification`: These are classification algorithms, including
    Naive Bayes, random forests, bagging, self-organizing maps, k-nearest neighbors,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.clustering`: These are clustering algorithms such as k-means,
    self-organizing maps, spatial clustering, Cobweb, ABC, and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.core`: These are classes representing instances and datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.distance`: These are algorithms that measure instance distance
    and similarity, for example, Chebyshev distance, cosine distance/similarity, Euclidean
    distance, Jaccard distance/similarity, Mahalanobis distance, Manhattan distance,
    Minkowski distance, Pearson correlation coefficient, Spearman''s footrule distance,
    DTW, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.featureselection`: These are algorithms for feature evaluation,
    scoring, selection, and ranking, for instance, gain ratio, ReliefF, Kullback-Leibler
    divergence, symmetrical uncertainty, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.filter`: These are methods for manipulating instances by filtering,
    removing attributes, setting classes or attribute values, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.matrix`: This implements in-memory or file-based arrays'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.sampling`: This implements sampling algorithms to select a subset
    of datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.tools`: These are utility methods on dataset, instance manipulation,
    serialization, Weka API interface, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.sf.javaml.utils`: These are utility methods for algorithms, for example,
    statistics, math methods, contingency tables, and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Apache Mahout project aims to build a scalable machine learning library.
    It is built atop scalable, distributed architectures, such as Hadoop, using the
    MapReduce paradigm, which is an approach for processing and generating large datasets
    with a parallel, distributed algorithm using a cluster of servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mahout features a console interface and the Java API as scalable algorithms
    for clustering, classification, and collaborative filtering. It is able to solve
    three business problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Item recommendation**: Recommending items such as **People who liked this
    movie also liked**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Sorting of text documents into groups of topically-related
    documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: Learning which topic to assign to an unlabelled document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahout is distributed under a commercially friendly Apache license, which means
    that you can use it as long as you keep the Apache license included and display
    it in your program's copyright notice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mahout features the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`org.apache.mahout.cf.taste`: These are collaborative filtering algorithms
    based on user-based and item-based collaborative filtering and matrix factorization
    with ALS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.classifier`: These are in-memory and distributed implementations,
    including logistic regression, Naive Bayes, random forest, **hidden Markov models**
    (**HMM**), and multilayer perceptron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.clustering`: These are clustering algorithms such as canopy
    clustering, k-means, fuzzy k-means, streaming k-means, and spectral clustering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.common`: These are utility methods for algorithms, including
    distances, MapReduce operations, iterators, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.driver`: This implements a general-purpose driver to run
    main methods of other classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.ep`: This is the evolutionary optimization using the recorded-step
    mutation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.math`: These are various math utility methods and implementations
    in Hadoop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.mahout.vectorizer`: These are classes for data presentation, manipulation,
    and MapReduce jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark, or simply Spark, is a platform for large-scale data processing
    builds atop Hadoop, but, in contrast to Mahout, it is not tied to the MapReduce
    paradigm. Instead, it uses in-memory caches to extract a working set of data,
    process it, and repeat the query. This is reported to be up to ten times as fast
    as a Mahout implementation that works directly with data stored in the disk. It
    can be grabbed from [https://spark.apache.org](https://spark.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: There are many modules built atop Spark, for instance, GraphX for graph processing,
    Spark Streaming for processing real-time data streams, and MLlib for machine learning
    library featuring classification, regression, collaborative filtering, clustering,
    dimensionality reduction, and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark''s MLlib can use a Hadoop-based data source, for example, **Hadoop Distributed
    File System** (**HDFS**) or HBase, as well as local files. The supported data
    types include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local vectors** are stored on a single machine. Dense vectors are presented
    as an array of double-typed values, for example, (2.0, 0.0, 1.0, 0.0), while sparse
    vector is presented by the size of the vector, an array of indices, and an array
    of values, for example, [4, (0, 2), (2.0, 1.0)].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labelled point** is used for supervised learning algorithms and consists
    of a local vector labelled with double-typed class values. The label can be a
    class index, binary outcome, or a list of multiple class indices (multiclass classification).
    For example, a labelled dense vector is presented as [1.0, (2.0, 0.0, 1.0, 0.0)].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local matrices** store a dense matrix on a single machine. It is defined
    by matrix dimensions and a single double-array arranged in a column-major order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed matrices** operate on data stored in Spark''s **Resilient Distributed
    Dataset** (**RDD**), which represents a collection of elements that can be operated
    on in parallel. There are three presentations: row matrix, where each row is a
    local vector that can be stored on a single machine, row indices are meaningless;
    indexed row matrix, which is similar to row matrix, but the row indices are meaningful,
    that is, rows can be identified and joins can be executed; and coordinate matrix,
    which is used when a row cannot be stored on a single machine and the matrix is
    very sparse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark''s MLlib API library provides interfaces for various learning algorithms
    and utilities, as outlined in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.classification`: These are binary and multiclass classification
    algorithms, including linear SVMs, logistic regression, decision trees, and Naive
    Bayes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.clustering`: These are k-means clustering algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.linalg`: These are data presentations, including dense
    vectors, sparse vectors, and matrices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.optimization`: These are the various optimization algorithms
    that are used as low-level primitives in MLlib, including gradient descent, **stochastic
    gradient descent** (**SGD**), update schemes for distributed SGD, and the limited-memory
    **Broyden–Fletcher–Goldfarb–Shanno** (**BFGS**) algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.recommendation`: These are model-based collaborative
    filtering techniques implemented with alternating least squares matrix factorization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.regression`: These are regression learning algorithms,
    such as linear least squares, decision trees, Lasso, and Ridge regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.stat`: These are statistical functions for samples
    in sparse or dense vector format to compute the mean, variance, minimum, maximum,
    counts, and nonzero counts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.tree`: This implements classification and regression
    decision tree-learning algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.apache.spark.mllib.util`: These are a collection of methods used for loading,
    saving, preprocessing, generating, and validating the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning4j
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deeplearning4j, or DL4J, is a deep learning library written in Java. It features
    a distributed as well as a single-machine deep learning framework that includes
    and supports various neural network structures such as feedforward neural networks,
    RBM, convolutional neural nets, deep belief networks, autoencoders, and others.
    DL4J can solve distinct problems, such as identifying faces, voices, spam, or
    e-commerce fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deeplearning4j is also distributed under the Apache 2.0 license and can be
    downloaded from [http://deeplearning4j.org](http://deeplearning4j.org/). The library
    is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.base`: These are loading classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.berkeley`: These are math utility methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.clustering`: This is the implementation of k-means clustering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.datasets`: This is dataset manipulation, including import,
    creation, iterating, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.distributions`: These are utility methods for distributions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.eval`: These are evaluation classes, including the confusion
    matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.exceptions`: This implements the exception handlers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.models`: These are supervised learning algorithms, including
    deep belief networks, stacked autoencoders, stacked denoising autoencoders, and
    RBM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.nn`: These are the implementations of components and algorithms
    based on neural networks, such as neural networks, multi-layer networks, convolutional
    multi-layer networks, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.optimize`: These are neural net optimization algorithms,
    including back propagation, multi-layer optimization, output layer optimization,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.plot`: These are various methods for rendering data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.rng`: This is a random data generator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.deeplearning4j.util`: These are helper and utility methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MALLET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Machine Learning for Language Toolkit** (**MALLET**) is a large library
    of natural language processing algorithms and utilities. It can be used in a variety
    of tasks such as document classification, document clustering, information extraction,
    and topic modelling. It features a command-line interface as well as a Java API
    for several algorithms such as Naive Bayes, HMM, Latent Dirichlet topic models,
    logistic regression, and conditional random fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'MALLET is available under the Common Public License 1.0, which means that you
    can even use it in commercial applications. It can be downloaded from [http://mallet.cs.umass.edu](http://mallet.cs.umass.edu/).
    A MALLET instance is represented by name, label, data, and source. However, there
    are two methods to import data into the MALLET format, as shown in the following
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instance per file**: Each file or document corresponds to an instance and
    MALLET accepts the directory name for the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instance per line**: Each line corresponds to an instance, where the following
    format is assumed—the `instance_name` label token. Data will be a feature vector,
    consisting of distinct words that appear as tokens and their occurrence count.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The library is comprised of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cc.mallet.classify`: These are algorithms for training and classifying instances,
    including AdaBoost, bagging, C4.5, as well as other decision tree models, multivariate
    logistic regression, Naive Bayes, and Winnow2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.cluster`: These are unsupervised clustering algorithms, including
    greedy agglomerative, hill climbing, k-best, and k-means clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.extract`: This implements tokenizers, document extractors, document
    viewers, cleaners, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.fst`: This implements sequence models, including conditional random
    fields, HMM, maximum entropy Markov models, and corresponding algorithms and evaluators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.grmm`: This implements graphical models and factor graphs such as
    inference algorithms, learning, and testing, for example, loopy belief propagation,
    Gibbs sampling, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.optimize`: These are optimization algorithms for finding the maximum
    of a function, such as gradient ascent, limited-memory BFGS, stochastic meta ascent,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.pipe`: These are methods as pipelines to process data into MALLET
    instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.topics`: These are topics modelling algorithms, such as Latent Dirichlet
    allocation, four-level pachinko allocation, hierarchical PAM, DMRT, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.types`: This implements fundamental data types such as dataset,
    feature vector, instance, and label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cc.mallet.util`: These are miscellaneous utility functions such as command-line
    processing, search, math, test, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Encog Machine Learning Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Encog is a machine learning framework in Java/C# that was developed by Jeff
    Heaton, a data scientist. It supports normalizing and processing data and a variety
    of advanced algorithm such as SVM, Neural Networks, Bayesian Networks, Hidden
    Markov Models, Genetic Programming, and Genetic Algorithms. It has been actively
    developed since 2008\. It supports multi-threading, which boosts performance on
    multi-core systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be found at [https://www.heatonresearch.com/encog/](https://www.heatonresearch.com/encog/).
    MLMethod is the base interface, which includes all of the methods for the models.
    The following are some of the interfaces and classes that it includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MLRegression`: This interface defines regression algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLClassification`: This interface defines classification algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLClustering`: This interface defines clustering algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLData`: This class represents a vector used in a model, either for input
    or output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLDataPair`: The functionality of this class is similar to that of `MLData`,
    but can be used for both input and output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLDataSet`: This represents the list of `MLDataPair` instances for trainers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FreeformNeuron`: This class is used as a neuron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FreeformConnection`: This shows the weighted connection between neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FreeformContextNeuron`: This represents a context neuron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InputSummation`: This value specifies how the inputs are summed to form a
    single neuron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BasicActiveSummation`: This is the simple sum of all input neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BasicFreeConnection`: This is the basic weighted connection between neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BasicFreeformLayer`: This interface provides a layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ELKI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELKI creates an environment for developing KDD applications supported by index
    structures, with an emphasis on unsupervised learning. It provides various implementations
    for cluster analysis and outlier detection. It provides index structures such
    as R*-tree for performance boosting and scalability. It is widely used in research
    areas by students and faculties up until now and has been gaining attention from
    other parties recently.
  prefs: []
  type: TYPE_NORMAL
- en: 'ELKI uses the AGPLv3 license, and can be found at [https://elki-project.github.io/](https://elki-project.github.io/).
    It is comprised of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`de.lmu.ifi.dbs.elki.algorithm`: Contains various algorithms such as clustering,
    classification, itemset mining, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`de.lmu.ifi.dbs.elki.outlier`: Defines an outlier-based algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`de.lmu.ifi.dbs.elki.statistics`: Defines a statistical analysis algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`de.lmu.ifi.dbs.elki.database`: This is the ELKI database layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`de.lmu.ifi.dbs.elki.index`: This is for index structure implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`de.lmu.ifi.dbs.elki.data`: Defines various data types and database object
    types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MOA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Massive Online Analysis** (**MOA**) contains a vast collection of various
    machine learning algorithms that includes algorithms for classification, regression,
    clustering, outlier detection, concept drift detection and recommender system,
    and tools for evaluation. All algorithms are designed for large-scale machine
    learning, with the concept of drift and deals with big streams of real-time data.
    It also works and integrates well with Weka.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is available as a GNU license and can be downloaded from [https://moa.cms.waikato.ac.nz/](https://moa.cms.waikato.ac.nz/).
    The following are its main packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`moa.classifiers`: Contains the algorithms for classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`moa.clusters`: Contains the algorithms for clustering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`moa.streams`: Contains the classes related to working with streams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`moa.evaluation`: Used for evaluating'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table summarizes all of the presented libraries. The table is,
    by no means, exhaustive—there are many more libraries that cover specific problem
    domains. This review should serve as an overview of the big names in the Java
    machine learning world:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Libraries** | **Problem domains** | **License** | **Architecture** | **Algorithms**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Weka | General purpose | GNU GPL | Single machine | Decision trees, Naive
    Bayes, neural network, random forest, AdaBoost, hierarchical clustering, and so
    on |'
  prefs: []
  type: TYPE_TB
- en: '| Java-ML | General purpose | GNU GPL | Single machine | K-means clustering,
    self-organizing maps, Markov chain clustering, Cobweb, random forest, decision
    trees, bagging, distance measures, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| Mahout | Classification, recommendation and clustering | Apache 2.0 License
    | Distributed single machine | Logistic regression, Naive Bayes, random forest,
    HMM, multilayer perceptron, k-means clustering, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| Spark | General purpose | Apache 2.0 License | Distributed | SVM, logistic
    regression, decision trees, Naive Bayes, k-means clustering, linear least squares,
    Lasso, ridge regression, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| DL4J | Deep learning | Apache 2.0 License | Distributed single machine |
    RBM, deep belief networks, deep autoencoders, recursive neural tensor networks,
    convolutional neural network, and stacked denoising autoencoders |'
  prefs: []
  type: TYPE_TB
- en: '| MALLET | Text mining | Common Public License 1.0 | Single machine | Naive
    Bayes, decision trees, maximum entropy, HMM, and conditional random fields |'
  prefs: []
  type: TYPE_TB
- en: '| Encog | Machine Learning Framework | Apache 2.0 License | Cross Platform
    | SVM, Neural Network, Bayesian Networks, HMMs, Genetic Programming, and Genetic
    Algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| ELKI | Data Mining | AGPL | Distributed single machine | Cluster Detection,
    Anomaly Detection, Evaluation, Index |'
  prefs: []
  type: TYPE_TB
- en: '| MOA | Machine Learning | GNU GPL | Distributed single machine | Classification,
    Regression, Clustering, Outlier Detection, Recommender System, Frequent Pattern
    Mining |'
  prefs: []
  type: TYPE_TB
- en: Building a machine learning application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning applications, especially those focused on classification,
    usually follow the same high-level workflow that''s shown in the following diagram.
    The workflow is comprised of two phases—training the classifier and the classification
    of new instances. Both phases share common steps, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2371275-8390-457f-bc94-2b1fb45251b9.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we use a set of training data, select a representative subset as the
    training set, preprocess the missing data, and extract its features. A selected
    supervised learning algorithm is used to train a model, which is deployed in the
    second phase. The second phase puts a new data instance through the same preprocessing
    and feature extraction procedure and applies the learned model to obtain the instance
    label. If you are able to collect new labelled data, periodically rerun the learning
    phase to retrain the model and replace the old one with the retrained one in the
    classification phase.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional machine learning architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured data, such as transactional, customers, analytical, and market data,
    usually resides within a local relational database. Given a query language, such
    as SQL, we can query the data used for processing, as shown in the workflow in
    the preceding diagram. Usually, all the data can be stored in memory and further
    processed with a machine learning library such as Weka, Java-ML, or MALLET.
  prefs: []
  type: TYPE_NORMAL
- en: A common practice in the architecture design is to create data pipelines, where
    different steps in the workflow are split. For instance, in order to create a
    client data record, we might have to scrap the data from different data sources.
    The record can be then saved in an intermediate database for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how the high-level aspects of big data architecture differ, let's
    first clarify when data is considered big.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Big data existed long before the phrase was invented. For instance, banks and
    stock exchanges have been processing billions of transactions daily for years
    and airline companies have worldwide real-time infrastructures for operational
    management of passenger booking, and so on. So, what is big data really? Doug
    Laney (2001) suggested that big data is defined by three Vs: volume, velocity,
    and variety. Therefore, to answer the question of whether your data is big, we
    can translate this into the following three sub-questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: Can you store your data in memory?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: Can you process new incoming data with a single machine?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: Is your data from a single source?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you answered all of these questions with yes, then your data is probably
    not big, and you have just simplified your application architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your answer to all of these questions was no, then your data is big! However,
    if you have mixed answers, then it''s complicated. Some may argue that one V is
    important; others may say that the other Vs are more important. From a machine
    learning point of view, there is a fundamental difference in algorithm implementation
    in order process the data in memory or from distributed storage. Therefore, a
    rule of thumb is: if you cannot store your data in memory, then you should look
    into a big data machine learning library.'
  prefs: []
  type: TYPE_NORMAL
- en: The exact answer depends on the problem that you are trying to solve. If you're
    starting a new project, I suggest that you start off with a single-machine library
    and prototype your algorithm, possibly with a subset of your data if the entire
    data does not fit into the memory. Once you've established good initial results,
    consider moving to something more heavy duty such as Mahout or Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Big data application architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data, such as documents, web blogs, social networks, sensor data, and others,
    are stored in a NoSQL database, such as MongoDB, or a distributed filesystem,
    such as HDFS. In case we deal with structured data, we can deploy database capabilities
    using systems such as Cassandra or HBase, which are built atop Hadoop. Data processing
    follows the MapReduce paradigm, which breaks data processing problems into smaller
    sub problems and distributes tasks across processing nodes. Machine learning models
    are finally trained with machine learning libraries such as Mahout and Spark.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB is a NoSQL database, which stores documents in a JSON-like format. You
    can read more about it at [https://www.mongodb.org](https://www.mongodb.org/).
    Hadoop is a framework for the distributed processing of large datasets across
    a cluster of computers. It includes its own filesystem format, HDFS, job scheduling
    framework, YARD, and implements the MapReduce approach for parallel data processing.
    We can learn more about Hadoop at [http://hadoop.apache.org/](http://hadoop.apache.org/).
    Cassandra is a distributed database management system that was built to provide
    fault-tolerant, scalable, and decentralized storage. More information is available
    at [http://cassandra.apache.org/](http://cassandra.apache.org/). HBase is another
    database that focuses on random read/write access for distributed storage. More
    information is available at [https://hbase.apache.org/](https://hbase.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting a machine learning library has an important impact on your application
    architecture. The key is to consider your project requirements. What kind of data
    do you have? What kind of problem are you trying to solve? Is your data big? Do
    you need distributed storage? What kind of algorithm are you planning to use?
    Once you figure out what you need to solve your problem, pick a library that best
    fits your needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover how to complete basic machine learning tasks
    such as classification, regression, and clustering by using some of the presented
    libraries.
  prefs: []
  type: TYPE_NORMAL
