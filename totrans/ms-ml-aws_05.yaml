- en: Predicting House Value with Regression Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归算法预测房屋价值
- en: This chapter will introduce the basics of regression algorithms and apply them
    to predict the price of houses given a number of features. We'll also introduce
    how to use logistic regression for classification problems. Examples in SageMaker
    Notebooks for scikit-learn,  Apache Spark, and SageMaker's linear learner will
    be provided.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍回归算法的基础知识，并将它们应用于根据多个特征预测房屋价格。我们还将介绍如何使用逻辑回归解决分类问题。将提供SageMaker Notebooks中scikit-learn、Apache
    Spark和SageMaker的线性学习器的示例。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Predicting the price of houses
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测房屋价格
- en: Understanding linear regression
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线性回归
- en: Evaluating regression models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Implementing linear regression through scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过scikit-learn实现线性回归
- en: Implementing linear regression through Apache Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Apache Spark实现线性回归
- en: Implementing linear regression through SageMaker's linear learner
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过SageMaker的线性学习器实现线性回归
- en: Understanding logistic regression
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解逻辑回归
- en: Pros and cons of linear models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型的优缺点
- en: Predicting the price of houses
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测房屋价格
- en: 'In this chapter, we will consider the problem of trying to predict the value
    of houses in Boston''s suburbs based on a number of variables, such as the number
    of rooms and house age. The details of the dataset can be found here: [https://www.kaggle.com/c/boston-housing/](https://www.kaggle.com/c/boston-housing/).
    This problem is different to the one we considered in the last chapter, as the
    variable we''re trying to predict (price in dollars) is continuous. Models that
    are able to predict continuous quantities are called **regressors**, or **regression
    algorithms**. There are many such algorithms, but in this chapter, we will focus
    on the simplest (but very popular) kind, linear regressors.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将考虑尝试根据多个变量（如房间数量和房屋年龄）预测波士顿郊区的房屋价值的问题。数据集的详细信息可以在此处找到：[https://www.kaggle.com/c/boston-housing/](https://www.kaggle.com/c/boston-housing/)。与上一章考虑的问题不同，因为我们试图预测的变量（美元价格）是连续的。能够预测连续量的模型被称为**回归器**或**回归算法**。有许多这样的算法，但本章我们将专注于最简单（但非常流行）的一种，即线性回归器。
- en: Understanding linear regression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线性回归
- en: Regression algorithms are an important algorithm in a data scientist's toolkit
    as they can be used for various non-binary prediction tasks. The linear regression
    algorithm models the relationship between a dependent variable that we are trying
    to predict with a vector of independent variables. The vector of variables is
    also called the regressor in the context of regression algorithms. Linear regression
    assumes that there is a linear relationship between the vector of independent
    variables and the dependent variable that we are trying to predict. Hence, linear
    regression models learn the unknown variables and constants of a  linear function
    using the training data, such that the linear function best fits the training
    data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回归算法是数据科学家工具箱中的重要算法，因为它们可以用于各种非二值预测任务。线性回归算法模型了我们试图预测的因变量与独立变量向量之间的关系。在回归算法的上下文中，变量向量也被称为回归器。线性回归假设独立变量向量与我们试图预测的因变量之间存在线性关系。因此，线性回归模型使用训练数据学习线性函数的未知变量和常数，使得线性函数最佳地拟合训练数据。
- en: Linear regression can be applied in cases where the goal is to predict or forecast
    the dependent variable based on the regressor variables. We will use an example
    to explain how linear regression trains based on data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可以应用于目标是根据回归变量预测或预测因变量的情况。我们将通过一个示例来解释线性回归如何根据数据训练。
- en: 'The following table shows a sample dataset where the goal is to predict the
    price of a house based on three variables:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了一个样本数据集，目标是根据三个变量预测房屋价格：
- en: '| **Floor Size** | **Number of Bedrooms** | **Number of Bathrooms** | **House
    Price** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **楼层面积** | **卧室数量** | **浴室数量** | **房屋价格** |'
- en: '| 2500 | 4 | 2 | 6,00,000 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 2500 | 4 | 2 | 600,000 |'
- en: '| 2800 | 4 | 2 | 6,50,000 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 2800 | 4 | 2 | 650,000 |'
- en: '| 2700 | 4 | 3 | 6,50,000 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 2700 | 4 | 3 | 650,000 |'
- en: '| 4500 | 6 | 4 | 8,00,000 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 4500 | 6 | 4 | 800,000 |'
- en: '| 3500 | 4 | 2 | 7,50,000 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 3500 | 4 | 2 | 750,000 |'
- en: '| 3000 | 5 | 4 | 7,60,000 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 3000 | 5 | 4 | 760,000 |'
- en: '| 2000 | 3 | 2 | 5,00,000 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 3 | 2 | 500,000 |'
- en: '| 4100 | 4 | 3 | 8,10,000 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 4100 | 4 | 3 | 810,000 |'
- en: In this dataset, the variables `Floor Size`, `Number of Bedrooms`, and `Number
    of Bathrooms` are assumed as independent in linear regression. Our goal is to
    predict the `House Price` value based on the variables.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，变量 `Floor Size`、`Number of Bedrooms` 和 `Number of Bathrooms` 在线性回归中被假定为相互独立。我们的目标是根据这些变量预测
    `House Price` 的值。
- en: 'Let''s simplify this problem. Let''s only consider the `Floor Size` variable
    to predict the house price. Creating linear regression from only one variable
    or regressor is referred to as a **simple linear regression**. If we create a
    scatterplot from the two columns, we can observe that there is a relationship
    between these two variables:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简化这个问题。让我们只考虑 `Floor Size` 变量来预测房价。仅从一个变量或回归器创建线性回归被称为**简单线性回归**。如果我们从两个列创建散点图，我们可以观察到这两个变量之间存在关系：
- en: '![](img/1cb29401-fe91-43e0-a51a-402e0429c16c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1cb29401-fe91-43e0-a51a-402e0429c16c.png)'
- en: Although there is not an exact linear relationship between the two variables,
    we can create an approximate line that represents the trend. The aim of the modeling
    algorithm is to minimize the error in creating this approximate line.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两个变量之间没有精确的线性关系，但我们可以创建一条近似线来表示趋势。建模算法的目的是最小化创建这条近似线时的误差。
- en: 'As we know, a straight line can be represented by the following equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，一条直线可以用以下方程表示：
- en: '![](img/390d4767-5122-40f8-a3c8-476d8a0a0cad.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/390d4767-5122-40f8-a3c8-476d8a0a0cad.png)'
- en: 'Hence, the approximately linear relationship in the preceding diagram can also
    be represented using the same formula, and the task of the linear regression model
    is to learn the value of ![](img/61c5c848-e1f1-4789-8178-51a9ddf7f18b.png) and ![](img/e922631d-d30b-4eb4-ab0b-26fe5423638e.png). Moreover,
    since we know that the relationship between the predicted variable and the regressors
    is not strictly linear, we can add a random error variable to the equation that
    models the noise in the dataset. The following formula represents how the simple
    linear regression model is represented:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面图表中近似线性关系也可以用相同的公式表示，线性回归模型的任务是学习 ![](img/61c5c848-e1f1-4789-8178-51a9ddf7f18b.png)
    和 ![](img/e922631d-d30b-4eb4-ab0b-26fe5423638e.png) 的值。此外，由于我们知道预测变量和回归器之间的关系不是严格线性的，我们可以在表示数据集中噪声的方程中添加一个随机误差变量。以下公式表示简单线性回归模型是如何表示的：
- en: '![](img/74c7bb58-49d6-4576-b25d-85441bc46446.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74c7bb58-49d6-4576-b25d-85441bc46446.png)'
- en: 'Now, let''s consider the dataset with multiple regressors. Instead of just
    representing the linear relationship between one variable ![](img/29cd6f71-77bd-4a79-86e3-e33d692957a3.png) and ![](img/72700c1f-4a44-4d77-951b-289c1b39b237.png),
    we will represent a set of regressors as ![](img/5d2d71b2-4d09-4ef3-aa4b-4e81e2f823f6.png). We
    will assume that a linear relationship between the dependent variable ![](img/0512f8a2-dfe9-4c7d-9962-3bb9fd40740c.png) and
    the regressors ![](img/74710d03-6a1e-4ce6-b62d-4a67656f1032.png) is linear.  Thus,
    a linear regression model with multiple regressors is represented by the following
    formula:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑具有多个回归器的数据集。我们不仅表示一个变量 ![](img/29cd6f71-77bd-4a79-86e3-e33d692957a3.png)
    和 ![](img/72700c1f-4a44-4d77-951b-289c1b39b237.png) 之间的线性关系，我们还将表示一组回归器为 ![](img/5d2d71b2-4d09-4ef3-aa4b-4e81e2f823f6.png)。我们将假设因变量
    ![](img/0512f8a2-dfe9-4c7d-9962-3bb9fd40740c.png) 和回归器 ![](img/74710d03-6a1e-4ce6-b62d-4a67656f1032.png)
    之间存在线性关系。因此，具有多个回归器的线性回归模型可以用以下公式表示：
- en: '![](img/4b62e289-abd2-47cd-bf76-aa49a39f1167.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b62e289-abd2-47cd-bf76-aa49a39f1167.png)'
- en: Linear regression, as we've already discussed, assumes that there is a linear
    relationship between the regressors and the dependent variable that we are trying
    to predict. This is an important assumption that may not hold true in all datasets.
    Hence, for a data scientist, using linear regression may look attractive due to
    its fast training time. However, if the dataset variables do not have a linear
    relationship with the dependent variable, it may lead to significant errors. In
    such cases, data scientists may also try algorithms such as Bernoulli regression,
    Poisson regression, or multinomial regression to improve prediction precision.
    We will also discuss logistic regression later in this chapter, which is used
    when the dependent variable is binary.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所讨论的，线性回归假设回归变量和我们要预测的因变量之间存在线性关系。这是一个重要的假设，可能并不适用于所有数据集。因此，对于数据科学家来说，由于线性回归的训练时间快，使用线性回归可能看起来很有吸引力。然而，如果数据集变量与因变量之间没有线性关系，可能会导致显著的误差。在这种情况下，数据科学家也可能尝试伯努利回归、泊松回归或多项式回归等算法来提高预测精度。我们将在本章后面讨论逻辑回归，它用于因变量为二进制的情况。
- en: During the training phase, linear regression can use various techniques for
    parameter estimation to learn the values of ![](img/66b07061-2818-4fee-a27a-9611da35a28a.png), ![](img/10ceb4e2-3cf4-4a4c-9220-a0d395cd4fc4.png),
    and ![](img/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png). We will not go into the
    details of these techniques in this book. However, we recommend that you try using
    these parameter estimation techniques in the examples that follow and observe
    their effect on the training time of the algorithm and the accuracy of prediction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，线性回归可以使用各种参数估计技术来学习 ![](img/66b07061-2818-4fee-a27a-9611da35a28a.png)、 ![](img/10ceb4e2-3cf4-4a4c-9220-a0d395cd4fc4.png)、 和
    ![](img/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png) 的值。然而，我们不会在本书中详细介绍这些技术。但是，我们建议你在接下来的示例中尝试使用这些参数估计技术，并观察它们对算法训练时间和预测准确性的影响。
- en: To fit a linear model to the data, we first need to be able to determine how
    well a linear model fits the data. There are various models being developed for
    parameter estimation in linear regression. Parameter estimation is the process
    of estimating the values of ![](img/adbf97e1-95a4-4ce9-96f8-cbd27236a730.png), ![](img/5acf773a-ee58-407c-8123-8e582436f4d4.png), and ![](img/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png).
    In the following sections, will briefly explain these two estimation techniques.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将线性模型拟合到数据中，我们首先需要能够确定线性模型拟合数据的好坏。线性回归中的参数估计正在开发出各种模型。参数估计是估计 ![](img/adbf97e1-95a4-4ce9-96f8-cbd27236a730.png)、 ![](img/5acf773a-ee58-407c-8123-8e582436f4d4.png)、 和 ![](img/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png) 的值的过程。在接下来的章节中，我们将简要介绍这两种估计技术。
- en: Linear least squares estimation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性最小二乘估计
- en: '**Linear least squares** (**LLS**) is an estimation approach that''s used to
    estimate parameters based on the given data. The optimization problem of the LLS
    estimation can be explained as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性最小二乘**（**LLS**）是一种基于给定数据估计参数的估计方法。LLS估计的优化问题可以解释如下：'
- en: '![](img/361d26cb-e963-4ee3-827e-341a7e63a582.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/361d26cb-e963-4ee3-827e-341a7e63a582.png)'
- en: LLS is a set of formulations that are used to get solutions to the statistical
    problem of linear regression by estimating the values of ![](img/5449879e-b6a5-4394-bfe4-8086ff5b67d7.png) and ![](img/17003f09-11c2-4ff3-85d4-c07326491423.png).
    LLS is an optimization methodology for getting solutions for linear regression.
    It uses the observed values of *x* and *y* to estimate the values of ![](img/903e175a-5148-410d-8744-acfac1205906.png) and ![](img/b07ede01-7c14-4fd5-a73f-ef2ff3d729b1.png). We
    encourage you to explore LLS solutions to understand how it estimates the linear
    regression parameters. However, as the focus of this book is to introduce you
    to these concepts and help you apply them in AWS, we won't go into detail about
    this methodology.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLS是一组公式，用于通过估计 ![](img/5449879e-b6a5-4394-bfe4-8086ff5b67d7.png) 和 ![](img/17003f09-11c2-4ff3-85d4-c07326491423.png) 的值来解决线性回归的统计问题。LLS是获取线性回归解决方案的优化方法。它使用观察到的*x*和*y*的值来估计 ![](img/903e175a-5148-410d-8744-acfac1205906.png) 和 ![](img/b07ede01-7c14-4fd5-a73f-ef2ff3d729b1.png) 的值。我们鼓励你探索LLS解决方案，以了解它是如何估计线性回归参数的。然而，由于本书的重点是介绍这些概念并帮助你将它们应用于AWS，我们不会详细介绍这种方法。
- en: Maximum likelihood estimation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然估计
- en: '**Maximum likelihood estimation** (**MLE**) is a popular model that''s used
    for estimating the parameters of linear regression. MLE is a probabilistic model
    that can predict what values of the parameters have the maximum likelihood to
    recreate the observed dataset. This is represented by the following formula:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大似然估计**（**MLE**）是一种流行的模型，用于估计线性回归的参数。MLE是一个概率模型，可以预测哪些参数值具有最大的可能性来重新创建观察到的数据集。这可以通过以下公式表示：'
- en: '![](img/c5e14e2a-58d9-4694-b775-dc55ae8e725f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5e14e2a-58d9-4694-b775-dc55ae8e725f.png)'
- en: For linear regression, our assumption is that the dependent variable has a linear
    relationship with the model. MLE assumes that the dependent variable values have
    a normal distribution. The idea is to predict the parameters for each observed
    value of *X* so that it models the value of *y*. We also estimate the error for
    each observed value that models how different the linear predicted value of *y*
    is from the actual value.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归，我们的假设是因变量与模型之间存在线性关系。最大似然估计（MLE）假设因变量的值服从正态分布。其思路是预测每个观察到的 *X* 值的参数，以便它能够模拟
    *y* 的值。我们还估计每个观察到的值的误差，以模拟线性预测的 *y* 值与实际值之间的差异。
- en: Gradient descent
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: The **gradient descent algorithm** is also popular for estimating parameters
    for linear regression. The gradient descent algorithm is used to minimize a function.
    Based on what we are predicting, we start with a set of initial values for the
    parameters and iteratively move toward the parameters to minimize the error in
    the function. The function to iteratively make steps in minimizing error is called
    **gradient**. The idea is to descend the gradient toward the lowest point in the
    gradient plane. Different types of gradient descent algorithms include **batch
    gradient descent**, which looks at all observed examples in each example, and
    **stochastic gradient descent**, where we iterate with only one observation at
    a time. For this reason, batch gradient descent is more accurate than stochastic
    gradient descent, but is much slower and hence not suitable for larger datasets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降算法**也常用于估计线性回归的参数。梯度下降算法用于最小化一个函数。根据我们的预测，我们从参数的初始值集合开始，迭代地移动到参数，以最小化函数中的误差。用于迭代地减小误差的函数称为**梯度**。其思路是沿着梯度平面中的最低点下降梯度。不同的梯度下降算法包括**批量梯度下降**，它查看每个例子中的所有观察到的例子，以及**随机梯度下降**，其中我们一次迭代一个观察值。因此，批量梯度下降比随机梯度下降更准确，但速度要慢得多，因此不适合大型数据集。'
- en: There is a vast amount of research being done on regression algorithms as it
    is very well suited for predicting continuous variables. We encourage you to learn
    more about linear regression libraries and try different variants that are provided
    in the library to calculate the efficiency and effectiveness of the test datasets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回归算法非常适合预测连续变量，因此关于回归算法的研究正在进行得非常广泛。我们鼓励您了解更多关于线性回归库的信息，并尝试库中提供的不同变体，以计算测试数据集的效率和效果。
- en: Evaluating regression models
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估回归模型
- en: Unlike the Naive Bayes classification model, the regression model provides a
    numerical output as a prediction. This output can be used for binary classification
    by predicting the value for both the events and using the maximum value. However,
    in examples such as predicting a house value based on regressors, we cannot use
    evaluation metrics that rely on just predicting whether we got the answer correct
    or incorrect. When we are predicting a numerical value, the evaluation metrics
    should also quantify the value of error in prediction. For example, if the house
    value is 600,000 and model A predicts it as 700,000 and model B predicts it as
    1,000,000, metrics such as precision and recall will count both these predictions
    as false positives. However, for regression models, we need evaluation metrics
    that can tell us that model A was closer to the actual value than model B. Therefore,
    in this section, we will present three metrics that are used for such numerical
    predictions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯分类模型不同，回归模型提供数值输出作为预测。这个输出可以通过预测两个事件并使用最大值来进行二分类。然而，在例如基于回归器预测房屋价值等例子中，我们不能使用仅依赖于预测是否正确或错误的评估指标。当我们预测数值时，评估指标还应量化预测中的误差值。例如，如果房屋价值为
    600,000，模型 A 预测为 700,000，模型 B 预测为 1,000,000，则精确度和召回率等指标会将这两个预测都计为假阳性。然而，对于回归模型，我们需要能够告诉我们模型
    A 比模型 B 更接近实际值的评估指标。因此，在本节中，我们将介绍用于此类数值预测的三个指标。
- en: Mean absolute error
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: '**Mean absolute error** (**MAE**) is the mean of the absolute values of the
    error. It can be represented with the following formula:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是误差绝对值的平均值。它可以表示为以下公式：'
- en: '![](img/3bec4000-89bd-4e6b-a10a-813b45e5612b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3bec4000-89bd-4e6b-a10a-813b45e5612b.png)'
- en: MAE provides an average error between two vectors. In our case, MAE is the difference
    between the actual value of ![](img/5d6873fe-60a7-4217-84a2-18731963b018.png)
    and the predicted value ![](img/013d9a96-4fc4-4978-b441-fa57474b633d.png). MAE
    is used by a lot of researchers since it gives a clear interpretation of the errors
    in the model's prediction.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 提供了两个向量之间的平均误差。在我们的例子中，MAE 是实际值 ![图片](img/5d6873fe-60a7-4217-84a2-18731963b018.png)
    和预测值 ![图片](img/013d9a96-4fc4-4978-b441-fa57474b633d.png) 之间的差异。由于它为模型预测中的误差提供了清晰的解释，MAE
    被许多研究人员使用。
- en: Mean squared error
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: '**Mean squared error** (**MSE**) is the mean of squares of the error values
    and is represented by the following formula:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均平方误差**（**MSE**）是误差值平方的平均值，表示为以下公式：'
- en: '![](img/d46f802d-b3e3-40e2-ba85-944de7e64c52.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d46f802d-b3e3-40e2-ba85-944de7e64c52.png)'
- en: MSE is useful in cases where the errors are very small. MSE incorporates both
    how far the predicted values are from the truth and also the variance in the predicted
    values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 在误差非常小的情况下很有用。MSE 结合了预测值与真实值之间的距离以及预测值中的方差。
- en: Root mean squared error
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根均方误差
- en: '**Root mean squared error** (**RMSE**) is the square root of the mean squared
    errors and is represented by the following formula:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**根均方误差**（**RMSE**）是均方误差的平方根，表示为以下公式：'
- en: '![](img/dd3c8b98-9636-47f9-8c90-3c38d0b29d1c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dd3c8b98-9636-47f9-8c90-3c38d0b29d1c.png)'
- en: RMSE, similar to MSE, captures the variance in predictions. However, in RMSE,
    since we take the square root of the squared error values, the error can be comparable
    to MSE, and also keep the advantages of MSE.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 与 MSE 类似，捕捉预测中的方差。然而，在 RMSE 中，由于我们取平方误差值的平方根，误差可以与 MSE 相比较，同时也保留了 MSE 的优点。
- en: R-squared
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R²
- en: 'Another popular metric that''s used in regression problems is the R-squared
    score, or coefficient of determination. This score measures the proportion of
    the variance in the dependent variable that is predictable from the independent
    variables:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在回归问题中常用的流行指标是 R² 分数，或确定系数。这个分数衡量了从独立变量可预测的因变量方差的比例：
- en: '![](img/d15c5172-eac3-4a23-b250-b7e8793556d7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d15c5172-eac3-4a23-b250-b7e8793556d7.png)'
- en: Here, ![](img/b783ac00-5d51-4d02-991d-d8798c4930f4.png) represents the vector
    of actual values, while ![](img/ad5e1d74-9a9e-49c0-9446-1be879d533f9.png) and
    ![](img/59ae26e6-30e8-49e1-8f2d-a387f03b4e89.png) represents the vector of predicted
    values. The mean actual value is ![](img/2be42d23-e4da-4220-86f9-0720ba123610.png).
    The denominator of the quotient measures how actual values typically differ from
    the mean, while the numerator measures how actual values differ from predicted
    values. Note that differences are squared, similar to MSE, and so large differences
    are penalized heavily.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/b783ac00-5d51-4d02-991d-d8798c4930f4.png) 代表实际值的向量，而 ![](img/ad5e1d74-9a9e-49c0-9446-1be879d533f9.png)
    和 ![](img/59ae26e6-30e8-49e1-8f2d-a387f03b4e89.png) 代表预测值的向量。实际值的平均值是 ![](img/2be42d23-e4da-4220-86f9-0720ba123610.png)。商的分子衡量实际值通常与平均值的不同程度，而分母衡量实际值与预测值的不同程度。请注意，差异是平方的，类似于均方误差（MSE），因此大的差异会受到严重惩罚。
- en: In a perfect regressor, the numerator is 0, so the best possible value for *R²*
    is 1.0\. However, we can see arbitrarily large negative values when the prediction
    errors are significant.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在完美的回归器中，分子为0，因此*R²*的最佳可能值为1.0。然而，当预测误差显著时，我们可以看到任意大的负值。
- en: All four types of evaluation metrics are implemented in machine learning packages
    and are demonstrated in the following code examples.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习包中实现了所有四种类型的评估指标，并在以下代码示例中进行了演示。
- en: Implementing linear regression through scikit-learn
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过scikit-learn实现线性回归
- en: Like we did in the previous chapter, we will show you how you can quickly use
    `scikit-learn` to train a linear model straight from a SageMaker notebook instance.
    First, you must create the notebook instance (choosing `conda_python3` as the
    kernel).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章中所做的那样，我们将向您展示如何快速使用`scikit-learn`从SageMaker笔记本实例中直接训练线性模型。首先，您必须创建笔记本实例（选择`conda_python3`作为内核）。
- en: 'We will start by loading the training data into a `pandas` dataframe:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先将训练数据加载到一个`pandas`数据框中：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code displays the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示以下输出：
- en: '![](img/2b9aefdc-61cc-4527-bd9d-2f68436c2cb6.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b9aefdc-61cc-4527-bd9d-2f68436c2cb6.png)'
- en: The last column `(medv)` stands for median value and represents the variable
    that we're trying to predict (dependent variable) based on the values from the
    remaining columns (independent variables).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一列`(medv)`代表中值，表示我们试图根据剩余列（自变量）的值来预测的变量（因变量）。
- en: 'As usual, we will split the dataset for training and testing:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常一样，我们将数据集分为训练集和测试集：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we have these datasets, we will proceed to construct a linear regressor:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了这些数据集，我们将继续构建一个线性回归器：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We start by constructing an estimator (in this case, linear regression) and
    fit the model by providing the matrix of training values, `(training_df[training_features])`,
    and the labels, `(raining_df['medv'])`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建一个估计器（在这种情况下，线性回归），并通过提供训练值的矩阵`(training_df[training_features])`和标签`(raining_df['medv'])`来拟合模型。
- en: 'After fitting the model, we can use it to get predictions for every row in
    our testing dataset. We do this by appending a new column to our existing testing
    dataframe:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在拟合模型后，我们可以用它来预测测试数据集中的每一行。我们通过向现有的测试数据框中添加一个新列来完成此操作：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code displays the following output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示以下输出：
- en: '![](img/ea8ce6ac-3ed3-4055-b45d-3b8a0c45d1e7.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea8ce6ac-3ed3-4055-b45d-3b8a0c45d1e7.png)'
- en: 'It''s always useful to check our predictions graphically. One way to do this
    is by plotting the predicted versus actual values as a scatterplot:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总是检查我们的预测结果非常有用。一种实现方式是将预测值与实际值作为散点图进行绘制：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code displays the following output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示以下输出：
- en: '![](img/92701c81-74e6-44c5-bf9c-037824d9288f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92701c81-74e6-44c5-bf9c-037824d9288f.png)'
- en: Note how the values are located mostly on the diagonal. This is a good sign,
    as a perfect regressor would yield all data points exactly on the diagonal (every
    predicted value would be exactly the same as the actual value).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意值大多位于对角线上。这是一个好兆头，因为完美的回归器会将所有数据点精确地放在对角线上（每个预测值都会与实际值完全相同）。
- en: In addition to this graphical verification, we obtain an evaluation metric that
    tells us how good our model is at predicting the values. In this example, we use
    R-squared evaluation metrics, as explained in the previous section, which is available
    in scikit-learn.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了这种图形验证之外，我们还获得了一个评估指标，它告诉我们我们的模型在预测值方面有多好。在这个例子中，我们使用R平方评估指标，如前所述，它在scikit-learn中可用。
- en: 'Let''s look at the following code block:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下代码块：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A value near 0.7 is a decent value. If you want to get a sense of what a good
    R2 correlation is, we recommend you play this game: [http://guessthecorrelation.com/](http://guessthecorrelation.com/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接近0.7的值是一个不错的值。如果您想了解什么是好的R2相关系数，我们建议您玩这个游戏：[http://guessthecorrelation.com/](http://guessthecorrelation.com/).
- en: Our linear model will create a predicted price by multiplying the value of each
    feature by a coefficient and adding up all these values, plus an independent term,
    or intercept.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的线性模型将通过将每个特征的值乘以一个系数并将所有这些值加起来，再加上一个独立项或截距来创建一个预测价格。
- en: 'We can find the values of these coefficients and intercept by accessing the
    data members in the model instance variable:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过访问模型实例变量中的数据成员来找到这些系数和截距的值：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It is usually very convenient to examine the coefficients of the different variables
    as they can be indicative of the relative importance of the features in terms
    of their independent predictive ability.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，检查不同变量的系数非常方便，因为它们可以表明特征在独立预测能力方面的相对重要性。
- en: By default, most linear regression algorithms such as `scikit-learn` or Spark
    will automatically do some degree of preprocessing (for example, it will scale
    the variables to prevent features with large values to introduce bias). Additionally,
    these algorithms support regularization parameters and provide you with options
    to choose the optimizer that's used to efficiently search for the coefficients
    that maximize the R2 score (or minimize some loss function).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，大多数线性回归算法，如`scikit-learn`或Spark，将自动进行一定程度的预处理（例如，它将变量缩放以防止具有大值的特征引入偏差）。此外，这些算法支持正则化参数，并提供选项供您选择用于高效搜索最大化R2分数（或最小化某些损失函数）的优化器。
- en: Implementing linear regression through Apache Spark
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Apache Spark实现线性回归
- en: You are likely interested in training regression models that can take huge datasets
    as input, beyond what you can do in `scikit-learn`. Apache Spark is a good candidate
    for this scenario. As we mentioned in the previous chapter, Apache Spark can easily
    run training algorithms on a cluster of machines using **Elastic MapReduce** (**EMR**)
    on AWS. We will explain how to set up EMR clusters in the next chapter. In this
    section, we'll explain how you can use the Spark ML library to train linear regression
    algorithms.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能对训练可以接受大量数据集作为输入的回归模型感兴趣，这超出了`scikit-learn`所能做到的。Apache Spark是这种场景下的一个很好的选择。正如我们在上一章中提到的，Apache
    Spark可以很容易地在AWS上使用**弹性映射减少**（**EMR**）在机器集群上运行训练算法。我们将在下一章中解释如何设置EMR集群。在本节中，我们将解释您如何使用Spark
    ML库来训练线性回归算法。
- en: 'The first step is to create a dataframe from our training data:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是从我们的训练数据创建一个dataframe：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following image shows the first few rows of the dataset:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了数据集的前几行：
- en: '![](img/78d39e07-ec5d-43e1-a0a8-ba2db1f2e68a.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/78d39e07-ec5d-43e1-a0a8-ba2db1f2e68a.png)'
- en: 'Typically, Apache Spark requires the input dataset to have a single column
    with a vector of numbers representing all the training features. In [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*,we used the `CountVectorizer` to create such a
    column. In this chapter, since the vector values are already available in our
    dataset, we just need to construct such a column using a `VectorAssembler` transformer:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，Apache Spark需要输入数据集有一个单列，该列是一个向量，代表所有训练特征。在[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)中，*使用朴素贝叶斯分类Twitter帖子*，我们使用了`CountVectorizer`来创建这样的列。在本章中，由于向量值已经存在于我们的数据集中，我们只需要使用`VectorAssembler`转换器构建这样一个列：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following screenshot shows the first few rows of the df_with_features_vector
    dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了df_with_features_vector数据集的前几行：
- en: '![](img/9f1ce736-58b2-4102-9548-b7caba902035.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9f1ce736-58b2-4102-9548-b7caba902035.png)'
- en: Note how the vector assembler created a new column called features, which assembles
    all the features that are used for training as vectors.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意向量组装器创建了一个名为features的新列，它将用于训练的所有特征组装成向量。
- en: 'As usual, we will split our dataframe into testing and training:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同往常，我们将我们的dataframe分为测试集和训练集：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now instantiate our regressor and fit a model:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以实例化我们的回归器并拟合一个模型：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'By using this model, we find predictions for each value in the test dataset:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用此模型，我们对测试数据集中的每个值进行预测：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the above `show()` command is:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上述`show()`命令的输出是：
- en: '![](img/b11e61a1-744c-4ce8-9428-1c7b61141f8d.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b11e61a1-744c-4ce8-9428-1c7b61141f8d.png)'
- en: 'We can easily find the `R2` value by using a `RegressionEvaluator`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `RegressionEvaluator` 容易地找到 `R2` 值：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, we get an `R2` of `0.688`, which is a similar result to that of
    `scikit-learn`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们得到 `R2` 为 `0.688`，这与 `scikit-learn` 的结果相似。
- en: Implementing linear regression through SageMaker's linear Learner
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 SageMaker 的线性学习器实现线性回归
- en: 'Another alternative within AWS for training regression models is to use SageMaker''s
    API to build linear models. In the previous chapter, we explained the basics of
    this service when we considered how to use BlazingText for our text classification
    problem. Similarly, we will use Linear Learners in this section and go through
    the same process, which basically entails three steps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 中用于训练回归模型的另一种选择是使用 SageMaker 的 API 来构建线性模型。在前一章中，当我们考虑如何使用 BlazingText 解决我们的文本分类问题时，我们解释了该服务的基本原理。同样，在本节中，我们将使用线性学习器，并遵循相同的过程，这基本上包括三个步骤：
- en: Stage the training and testing data in S3
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练和测试数据存档到 S3
- en: Invoke the API to train the model
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 API 来训练模型
- en: Use the model to obtain predictions
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型获取预测
- en: 'Unlike what we did in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Bayes*, instead of deploying an endpoint
    (that is, a web service) to obtain predictions, we will use a batch transformer,
    which is a service that''s capable of obtaining bulk predictions given a model
    and some data in S3\. Let''s take a look at the following steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第 2 章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，*使用朴素贝叶斯分类 Twitter 流*中所做的不一样，不是部署一个端点（即，一个网络服务）来获取预测，我们将使用批处理转换器，这是一种能够根据模型和
    S3 中的某些数据获取大量预测的服务。让我们看看以下步骤：
- en: 'Assuming that we have prepared the training and testing datasets in a similar
    way to the previous sections, we will create a SageMaker session and upload our
    training and testing data to S3:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们已经以与前面章节类似的方式准备好了训练和测试数据集，我们将创建一个 SageMaker 会话并将我们的训练和测试数据上传到 S3：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once the data is in S3, we can proceed to instantiate the estimator:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据存入 S3，我们可以继续实例化估计器：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we need to set the hyperparameters. Linear Learner in SageMaker takes
    a large set of options, which can be found here [https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html).
    In [Chapter 14](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml), *Optimizing SageMaker
    and Spark Machine Learning Models*, we will dive into how to find suitable values
    for these parameters:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置超参数。SageMaker 中的线性学习器接受大量选项，可以在以下链接中找到：[https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)。在[第
    14 章](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml)，*优化 SageMaker 和 Spark 机器学习模型*中，我们将深入了解如何找到这些参数的合适值：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Once we fit the model, we can instantiate a transformer, which is capable of
    computing predictions for our test dataset in `S3:`
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们拟合了模型，我们可以实例化一个转换器，它能够计算测试数据集在 `S3` 中的预测：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will create a file in s3 called `testing-housing.csv.out` with the following
    format:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 S3 中创建一个名为 `testing-housing.csv.out` 的文件，其格式如下：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can download this file and build a pandas dataframe with the predictions:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以下载此文件并使用预测构建一个 pandas 数据框：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following screenshot shows the first few predictions:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前几个预测：
- en: '![](img/4f6ec0ab-6f36-4f1e-aa98-e61662863166.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f6ec0ab-6f36-4f1e-aa98-e61662863166.png)'
- en: 'Given that these scores follow the exact order found in the testing dataset,
    we can then proceed to put together the actual and predicted columns by merging
    the data series:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这些分数与测试数据集中的顺序完全一致，我们可以通过合并数据系列来组合实际和预测列：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code displays the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示以下输出：
- en: '![](img/34686cb3-6d30-4969-a882-3893f92bc134.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34686cb3-6d30-4969-a882-3893f92bc134.png)'
- en: 'With this data frame, we can calculate the R2 score:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个数据框，我们可以计算 R2 分数：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The result was `0.796`, which is in line with the previous estimates, with a
    slight improvement.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 结果为 `0.796`，与之前的估计一致，略有改进。
- en: Understanding logistic regression
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解逻辑回归
- en: Logistic regression is a widely used statistical model that can be used to model
    a binary dependent variable. In linear regression, we assumed that the dependent
    variable is a numerical value that we were trying to predict. Consider a case
    where the binary variable has values of true and false. In logistic regression,
    instead of calculating the value of numerical output using the formula we used
    in the *Linear regression* section, we estimate the log odds of a binary event
    labeled True using the same formulation. The function that converts log odds to
    the probability of the event labeled 1 occurring is called the **logistic function**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一个广泛使用的统计模型，可以用来模拟二元因变量。在线性回归中，我们假设因变量是我们试图预测的数值。考虑一个二元变量具有真和假值的情况。在逻辑回归中，我们不是使用我们在*线性回归*部分使用的公式来计算数值输出，而是使用相同的公式估计标记为True的二元事件的log
    odds。将log odds转换为事件标记为1发生的概率的函数称为**逻辑函数**。
- en: 'The unit of measurement for log-odds scale is called **logit**. Log-odds are
    calculated using the following formula:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然尺度测量的单位称为**logit**。对数似然是通过以下公式计算的：
- en: '![](img/7f307bd6-134d-4d57-a9f1-b08bbc8da42b.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7f307bd6-134d-4d57-a9f1-b08bbc8da42b.png)'
- en: Thus, using the same methodology as linear regression, logistic regression is
    used for binary dependent variables by calculating the odds of the True event
    occurring. The main difference between linear regression and logistic regression
    is that linear regression is used to predict the values of the dependent variable,
    while logistic regression is used to predict the probability of the value of the
    dependent variable. Hence, as we emphasize in most of this book, data scientists
    should look at what they want to predict and choose the algorithms accordingly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用与线性回归相同的方法，逻辑回归通过计算真实事件发生的概率来用于二元因变量。线性回归与逻辑回归之间的主要区别在于，线性回归用于预测因变量的值，而逻辑回归用于预测因变量值的概率。因此，正如我们在本书的大部分内容中所强调的，数据科学家应该看看他们想要预测什么，并相应地选择算法。
- en: The logistic regression algorithm is implemented in most popular machine learning
    packages, and we will provide an example of how to use it in Spark in the following
    section.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法在大多数流行的机器学习包中都有实现，我们将在下一节提供一个如何在Spark中使用它的示例。
- en: Logistic regression in Spark
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的逻辑回归
- en: 'The `chapter3/train_logistic` notebook shows how we can instantiate a `LogisticRegression`
    Spark Trainer instead of `NaiveBayes` for the Twitter dataset we dealt with in
    [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes* and obtain a model just as good as the one we constructed:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`chapter3/train_logistic`笔记本展示了我们如何实例化一个`LogisticRegression` Spark Trainer，而不是使用`NaiveBayes`来处理我们在[第2章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)中处理的Twitter数据集，即*使用朴素贝叶斯分类Twitter帖子*，并获得一个与我们所构建的模型一样好的模型：'
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Pros and cons of linear models
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型的优缺点
- en: Regression models are very popular in machine learning and are widely applied
    in many areas. Linear regression's main advantage is its simplicity to represent
    the dataset as a simple linear model. Hence, the training time for linear regression
    is fast. Similarly, the model can be inspected by data scientists to understand
    which variable is contributing to the decisions of the overall model. Linear regression
    is recommended in cases where the problem statement is simple and fewer variables
    are used for predictions. As the complexity of the dataset increases, linear regression
    may generate significant errors if the data has a lot of noise in it.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型在机器学习中非常受欢迎，并在许多领域得到广泛应用。线性回归的主要优势是它简单地将数据集表示为简单的线性模型。因此，线性回归的训练时间很快。同样，模型可以被数据科学家检查，以了解哪个变量对整体模型的决策有贡献。在问题陈述简单且用于预测的变量较少的情况下，建议使用线性回归。随着数据集复杂性的增加，如果数据中有很多噪声，线性回归可能会产生显著的错误。
- en: Linear regression makes a bold assumption that the dependent variable has a
    linear relationship with the regressors. If this does not hold true, then the
    linear regression algorithm may not be able to fit the data well. There are variants
    such as quadratic regressions that can solve this issue. However, this leads to
    complexity in the model and hence significantly increases training time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归大胆地假设因变量与回归变量之间存在线性关系。如果这个假设不成立，那么线性回归算法可能无法很好地拟合数据。有一些变体，如二次回归，可以解决这个问题。然而，这会导致模型复杂化，从而显著增加训练时间。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started with the basics of regression algorithms and applied
    them to predict the price of houses. We then learned how to evaluate regression
    models, were introduced to linear regression through various libraries such as `scikit-learn`, 
    Apache Spark and SageMaker's linear learner, and, finally, we saw how to use logistic
    regression for classification problems, and the pros and cons of linear models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了回归算法的基础，并将其应用于预测房价。然后，我们学习了如何评估回归模型，通过如`scikit-learn`、Apache Spark和SageMaker的线性学习器等库介绍了线性回归，最后，我们看到了如何使用逻辑回归来解决分类问题，以及线性模型的优缺点。
- en: In the next chapter, we will predict user behavior with tree-based methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用基于树的算法来预测用户行为。
