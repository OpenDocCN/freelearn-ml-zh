- en: Predicting House Value with Regression Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce the basics of regression algorithms and apply them
    to predict the price of houses given a number of features. We'll also introduce
    how to use logistic regression for classification problems. Examples in SageMaker
    Notebooks for scikit-learn,  Apache Spark, and SageMaker's linear learner will
    be provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the price of houses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing linear regression through scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing linear regression through Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing linear regression through SageMaker's linear learner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros and cons of linear models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the price of houses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will consider the problem of trying to predict the value
    of houses in Boston''s suburbs based on a number of variables, such as the number
    of rooms and house age. The details of the dataset can be found here: [https://www.kaggle.com/c/boston-housing/](https://www.kaggle.com/c/boston-housing/).
    This problem is different to the one we considered in the last chapter, as the
    variable we''re trying to predict (price in dollars) is continuous. Models that
    are able to predict continuous quantities are called **regressors**, or **regression
    algorithms**. There are many such algorithms, but in this chapter, we will focus
    on the simplest (but very popular) kind, linear regressors.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression algorithms are an important algorithm in a data scientist's toolkit
    as they can be used for various non-binary prediction tasks. The linear regression
    algorithm models the relationship between a dependent variable that we are trying
    to predict with a vector of independent variables. The vector of variables is
    also called the regressor in the context of regression algorithms. Linear regression
    assumes that there is a linear relationship between the vector of independent
    variables and the dependent variable that we are trying to predict. Hence, linear
    regression models learn the unknown variables and constants of a  linear function
    using the training data, such that the linear function best fits the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression can be applied in cases where the goal is to predict or forecast
    the dependent variable based on the regressor variables. We will use an example
    to explain how linear regression trains based on data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a sample dataset where the goal is to predict the
    price of a house based on three variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Floor Size** | **Number of Bedrooms** | **Number of Bathrooms** | **House
    Price** |'
  prefs: []
  type: TYPE_TB
- en: '| 2500 | 4 | 2 | 6,00,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2800 | 4 | 2 | 6,50,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2700 | 4 | 3 | 6,50,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4500 | 6 | 4 | 8,00,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 3500 | 4 | 2 | 7,50,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 3000 | 5 | 4 | 7,60,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 3 | 2 | 5,00,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4100 | 4 | 3 | 8,10,000 |'
  prefs: []
  type: TYPE_TB
- en: In this dataset, the variables `Floor Size`, `Number of Bedrooms`, and `Number
    of Bathrooms` are assumed as independent in linear regression. Our goal is to
    predict the `House Price` value based on the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s simplify this problem. Let''s only consider the `Floor Size` variable
    to predict the house price. Creating linear regression from only one variable
    or regressor is referred to as a **simple linear regression**. If we create a
    scatterplot from the two columns, we can observe that there is a relationship
    between these two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cb29401-fe91-43e0-a51a-402e0429c16c.png)'
  prefs: []
  type: TYPE_IMG
- en: Although there is not an exact linear relationship between the two variables,
    we can create an approximate line that represents the trend. The aim of the modeling
    algorithm is to minimize the error in creating this approximate line.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, a straight line can be represented by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/390d4767-5122-40f8-a3c8-476d8a0a0cad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the approximately linear relationship in the preceding diagram can also
    be represented using the same formula, and the task of the linear regression model
    is to learn the value of ![](img/61c5c848-e1f1-4789-8178-51a9ddf7f18b.png) and ![](img/e922631d-d30b-4eb4-ab0b-26fe5423638e.png). Moreover,
    since we know that the relationship between the predicted variable and the regressors
    is not strictly linear, we can add a random error variable to the equation that
    models the noise in the dataset. The following formula represents how the simple
    linear regression model is represented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74c7bb58-49d6-4576-b25d-85441bc46446.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s consider the dataset with multiple regressors. Instead of just
    representing the linear relationship between one variable ![](img/29cd6f71-77bd-4a79-86e3-e33d692957a3.png) and ![](img/72700c1f-4a44-4d77-951b-289c1b39b237.png),
    we will represent a set of regressors as ![](img/5d2d71b2-4d09-4ef3-aa4b-4e81e2f823f6.png). We
    will assume that a linear relationship between the dependent variable ![](img/0512f8a2-dfe9-4c7d-9962-3bb9fd40740c.png) and
    the regressors ![](img/74710d03-6a1e-4ce6-b62d-4a67656f1032.png) is linear.  Thus,
    a linear regression model with multiple regressors is represented by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b62e289-abd2-47cd-bf76-aa49a39f1167.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear regression, as we've already discussed, assumes that there is a linear
    relationship between the regressors and the dependent variable that we are trying
    to predict. This is an important assumption that may not hold true in all datasets.
    Hence, for a data scientist, using linear regression may look attractive due to
    its fast training time. However, if the dataset variables do not have a linear
    relationship with the dependent variable, it may lead to significant errors. In
    such cases, data scientists may also try algorithms such as Bernoulli regression,
    Poisson regression, or multinomial regression to improve prediction precision.
    We will also discuss logistic regression later in this chapter, which is used
    when the dependent variable is binary.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, linear regression can use various techniques for
    parameter estimation to learn the values of ![](img/66b07061-2818-4fee-a27a-9611da35a28a.png), ![](img/10ceb4e2-3cf4-4a4c-9220-a0d395cd4fc4.png),
    and ![](img/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png). We will not go into the
    details of these techniques in this book. However, we recommend that you try using
    these parameter estimation techniques in the examples that follow and observe
    their effect on the training time of the algorithm and the accuracy of prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To fit a linear model to the data, we first need to be able to determine how
    well a linear model fits the data. There are various models being developed for
    parameter estimation in linear regression. Parameter estimation is the process
    of estimating the values of ![](img/adbf97e1-95a4-4ce9-96f8-cbd27236a730.png), ![](img/5acf773a-ee58-407c-8123-8e582436f4d4.png), and ![](img/bfc6f47b-e0d2-4465-bb78-dc3572f269d3.png).
    In the following sections, will briefly explain these two estimation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Linear least squares estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear least squares** (**LLS**) is an estimation approach that''s used to
    estimate parameters based on the given data. The optimization problem of the LLS
    estimation can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/361d26cb-e963-4ee3-827e-341a7e63a582.png)'
  prefs: []
  type: TYPE_IMG
- en: LLS is a set of formulations that are used to get solutions to the statistical
    problem of linear regression by estimating the values of ![](img/5449879e-b6a5-4394-bfe4-8086ff5b67d7.png) and ![](img/17003f09-11c2-4ff3-85d4-c07326491423.png).
    LLS is an optimization methodology for getting solutions for linear regression.
    It uses the observed values of *x* and *y* to estimate the values of ![](img/903e175a-5148-410d-8744-acfac1205906.png) and ![](img/b07ede01-7c14-4fd5-a73f-ef2ff3d729b1.png). We
    encourage you to explore LLS solutions to understand how it estimates the linear
    regression parameters. However, as the focus of this book is to introduce you
    to these concepts and help you apply them in AWS, we won't go into detail about
    this methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Maximum likelihood estimation** (**MLE**) is a popular model that''s used
    for estimating the parameters of linear regression. MLE is a probabilistic model
    that can predict what values of the parameters have the maximum likelihood to
    recreate the observed dataset. This is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5e14e2a-58d9-4694-b775-dc55ae8e725f.png)'
  prefs: []
  type: TYPE_IMG
- en: For linear regression, our assumption is that the dependent variable has a linear
    relationship with the model. MLE assumes that the dependent variable values have
    a normal distribution. The idea is to predict the parameters for each observed
    value of *X* so that it models the value of *y*. We also estimate the error for
    each observed value that models how different the linear predicted value of *y*
    is from the actual value.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **gradient descent algorithm** is also popular for estimating parameters
    for linear regression. The gradient descent algorithm is used to minimize a function.
    Based on what we are predicting, we start with a set of initial values for the
    parameters and iteratively move toward the parameters to minimize the error in
    the function. The function to iteratively make steps in minimizing error is called
    **gradient**. The idea is to descend the gradient toward the lowest point in the
    gradient plane. Different types of gradient descent algorithms include **batch
    gradient descent**, which looks at all observed examples in each example, and
    **stochastic gradient descent**, where we iterate with only one observation at
    a time. For this reason, batch gradient descent is more accurate than stochastic
    gradient descent, but is much slower and hence not suitable for larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: There is a vast amount of research being done on regression algorithms as it
    is very well suited for predicting continuous variables. We encourage you to learn
    more about linear regression libraries and try different variants that are provided
    in the library to calculate the efficiency and effectiveness of the test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the Naive Bayes classification model, the regression model provides a
    numerical output as a prediction. This output can be used for binary classification
    by predicting the value for both the events and using the maximum value. However,
    in examples such as predicting a house value based on regressors, we cannot use
    evaluation metrics that rely on just predicting whether we got the answer correct
    or incorrect. When we are predicting a numerical value, the evaluation metrics
    should also quantify the value of error in prediction. For example, if the house
    value is 600,000 and model A predicts it as 700,000 and model B predicts it as
    1,000,000, metrics such as precision and recall will count both these predictions
    as false positives. However, for regression models, we need evaluation metrics
    that can tell us that model A was closer to the actual value than model B. Therefore,
    in this section, we will present three metrics that are used for such numerical
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) is the mean of the absolute values of the
    error. It can be represented with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bec4000-89bd-4e6b-a10a-813b45e5612b.png)'
  prefs: []
  type: TYPE_IMG
- en: MAE provides an average error between two vectors. In our case, MAE is the difference
    between the actual value of ![](img/5d6873fe-60a7-4217-84a2-18731963b018.png)
    and the predicted value ![](img/013d9a96-4fc4-4978-b441-fa57474b633d.png). MAE
    is used by a lot of researchers since it gives a clear interpretation of the errors
    in the model's prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) is the mean of squares of the error values
    and is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d46f802d-b3e3-40e2-ba85-944de7e64c52.png)'
  prefs: []
  type: TYPE_IMG
- en: MSE is useful in cases where the errors are very small. MSE incorporates both
    how far the predicted values are from the truth and also the variance in the predicted
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Root mean squared error** (**RMSE**) is the square root of the mean squared
    errors and is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd3c8b98-9636-47f9-8c90-3c38d0b29d1c.png)'
  prefs: []
  type: TYPE_IMG
- en: RMSE, similar to MSE, captures the variance in predictions. However, in RMSE,
    since we take the square root of the squared error values, the error can be comparable
    to MSE, and also keep the advantages of MSE.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another popular metric that''s used in regression problems is the R-squared
    score, or coefficient of determination. This score measures the proportion of
    the variance in the dependent variable that is predictable from the independent
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d15c5172-eac3-4a23-b250-b7e8793556d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/b783ac00-5d51-4d02-991d-d8798c4930f4.png) represents the vector
    of actual values, while ![](img/ad5e1d74-9a9e-49c0-9446-1be879d533f9.png) and
    ![](img/59ae26e6-30e8-49e1-8f2d-a387f03b4e89.png) represents the vector of predicted
    values. The mean actual value is ![](img/2be42d23-e4da-4220-86f9-0720ba123610.png).
    The denominator of the quotient measures how actual values typically differ from
    the mean, while the numerator measures how actual values differ from predicted
    values. Note that differences are squared, similar to MSE, and so large differences
    are penalized heavily.
  prefs: []
  type: TYPE_NORMAL
- en: In a perfect regressor, the numerator is 0, so the best possible value for *R²*
    is 1.0\. However, we can see arbitrarily large negative values when the prediction
    errors are significant.
  prefs: []
  type: TYPE_NORMAL
- en: All four types of evaluation metrics are implemented in machine learning packages
    and are demonstrated in the following code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression through scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like we did in the previous chapter, we will show you how you can quickly use
    `scikit-learn` to train a linear model straight from a SageMaker notebook instance.
    First, you must create the notebook instance (choosing `conda_python3` as the
    kernel).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the training data into a `pandas` dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b9aefdc-61cc-4527-bd9d-2f68436c2cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: The last column `(medv)` stands for median value and represents the variable
    that we're trying to predict (dependent variable) based on the values from the
    remaining columns (independent variables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As usual, we will split the dataset for training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have these datasets, we will proceed to construct a linear regressor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We start by constructing an estimator (in this case, linear regression) and
    fit the model by providing the matrix of training values, `(training_df[training_features])`,
    and the labels, `(raining_df['medv'])`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After fitting the model, we can use it to get predictions for every row in
    our testing dataset. We do this by appending a new column to our existing testing
    dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea8ce6ac-3ed3-4055-b45d-3b8a0c45d1e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s always useful to check our predictions graphically. One way to do this
    is by plotting the predicted versus actual values as a scatterplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92701c81-74e6-44c5-bf9c-037824d9288f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note how the values are located mostly on the diagonal. This is a good sign,
    as a perfect regressor would yield all data points exactly on the diagonal (every
    predicted value would be exactly the same as the actual value).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this graphical verification, we obtain an evaluation metric that
    tells us how good our model is at predicting the values. In this example, we use
    R-squared evaluation metrics, as explained in the previous section, which is available
    in scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A value near 0.7 is a decent value. If you want to get a sense of what a good
    R2 correlation is, we recommend you play this game: [http://guessthecorrelation.com/](http://guessthecorrelation.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Our linear model will create a predicted price by multiplying the value of each
    feature by a coefficient and adding up all these values, plus an independent term,
    or intercept.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find the values of these coefficients and intercept by accessing the
    data members in the model instance variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It is usually very convenient to examine the coefficients of the different variables
    as they can be indicative of the relative importance of the features in terms
    of their independent predictive ability.
  prefs: []
  type: TYPE_NORMAL
- en: By default, most linear regression algorithms such as `scikit-learn` or Spark
    will automatically do some degree of preprocessing (for example, it will scale
    the variables to prevent features with large values to introduce bias). Additionally,
    these algorithms support regularization parameters and provide you with options
    to choose the optimizer that's used to efficiently search for the coefficients
    that maximize the R2 score (or minimize some loss function).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression through Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are likely interested in training regression models that can take huge datasets
    as input, beyond what you can do in `scikit-learn`. Apache Spark is a good candidate
    for this scenario. As we mentioned in the previous chapter, Apache Spark can easily
    run training algorithms on a cluster of machines using **Elastic MapReduce** (**EMR**)
    on AWS. We will explain how to set up EMR clusters in the next chapter. In this
    section, we'll explain how you can use the Spark ML library to train linear regression
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a dataframe from our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the first few rows of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78d39e07-ec5d-43e1-a0a8-ba2db1f2e68a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Typically, Apache Spark requires the input dataset to have a single column
    with a vector of numbers representing all the training features. In [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying
    Twitter Feeds with Naive Bayes*,we used the `CountVectorizer` to create such a
    column. In this chapter, since the vector values are already available in our
    dataset, we just need to construct such a column using a `VectorAssembler` transformer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few rows of the df_with_features_vector
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f1ce736-58b2-4102-9548-b7caba902035.png)'
  prefs: []
  type: TYPE_IMG
- en: Note how the vector assembler created a new column called features, which assembles
    all the features that are used for training as vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we will split our dataframe into testing and training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now instantiate our regressor and fit a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'By using this model, we find predictions for each value in the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the above `show()` command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b11e61a1-744c-4ce8-9428-1c7b61141f8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can easily find the `R2` value by using a `RegressionEvaluator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we get an `R2` of `0.688`, which is a similar result to that of
    `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression through SageMaker's linear Learner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another alternative within AWS for training regression models is to use SageMaker''s
    API to build linear models. In the previous chapter, we explained the basics of
    this service when we considered how to use BlazingText for our text classification
    problem. Similarly, we will use Linear Learners in this section and go through
    the same process, which basically entails three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Stage the training and testing data in S3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invoke the API to train the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to obtain predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unlike what we did in [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml),
    *Classifying Twitter Feeds with Naive Bayes*, instead of deploying an endpoint
    (that is, a web service) to obtain predictions, we will use a batch transformer,
    which is a service that''s capable of obtaining bulk predictions given a model
    and some data in S3\. Let''s take a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that we have prepared the training and testing datasets in a similar
    way to the previous sections, we will create a SageMaker session and upload our
    training and testing data to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is in S3, we can proceed to instantiate the estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to set the hyperparameters. Linear Learner in SageMaker takes
    a large set of options, which can be found here [https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html).
    In [Chapter 14](7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml), *Optimizing SageMaker
    and Spark Machine Learning Models*, we will dive into how to find suitable values
    for these parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Once we fit the model, we can instantiate a transformer, which is capable of
    computing predictions for our test dataset in `S3:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a file in s3 called `testing-housing.csv.out` with the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can download this file and build a pandas dataframe with the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the first few predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f6ec0ab-6f36-4f1e-aa98-e61662863166.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given that these scores follow the exact order found in the testing dataset,
    we can then proceed to put together the actual and predicted columns by merging
    the data series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34686cb3-6d30-4969-a882-3893f92bc134.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With this data frame, we can calculate the R2 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The result was `0.796`, which is in line with the previous estimates, with a
    slight improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a widely used statistical model that can be used to model
    a binary dependent variable. In linear regression, we assumed that the dependent
    variable is a numerical value that we were trying to predict. Consider a case
    where the binary variable has values of true and false. In logistic regression,
    instead of calculating the value of numerical output using the formula we used
    in the *Linear regression* section, we estimate the log odds of a binary event
    labeled True using the same formulation. The function that converts log odds to
    the probability of the event labeled 1 occurring is called the **logistic function**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unit of measurement for log-odds scale is called **logit**. Log-odds are
    calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f307bd6-134d-4d57-a9f1-b08bbc8da42b.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, using the same methodology as linear regression, logistic regression is
    used for binary dependent variables by calculating the odds of the True event
    occurring. The main difference between linear regression and logistic regression
    is that linear regression is used to predict the values of the dependent variable,
    while logistic regression is used to predict the probability of the value of the
    dependent variable. Hence, as we emphasize in most of this book, data scientists
    should look at what they want to predict and choose the algorithms accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression algorithm is implemented in most popular machine learning
    packages, and we will provide an example of how to use it in Spark in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `chapter3/train_logistic` notebook shows how we can instantiate a `LogisticRegression`
    Spark Trainer instead of `NaiveBayes` for the Twitter dataset we dealt with in
    [Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes* and obtain a model just as good as the one we constructed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Pros and cons of linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression models are very popular in machine learning and are widely applied
    in many areas. Linear regression's main advantage is its simplicity to represent
    the dataset as a simple linear model. Hence, the training time for linear regression
    is fast. Similarly, the model can be inspected by data scientists to understand
    which variable is contributing to the decisions of the overall model. Linear regression
    is recommended in cases where the problem statement is simple and fewer variables
    are used for predictions. As the complexity of the dataset increases, linear regression
    may generate significant errors if the data has a lot of noise in it.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression makes a bold assumption that the dependent variable has a
    linear relationship with the regressors. If this does not hold true, then the
    linear regression algorithm may not be able to fit the data well. There are variants
    such as quadratic regressions that can solve this issue. However, this leads to
    complexity in the model and hence significantly increases training time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with the basics of regression algorithms and applied
    them to predict the price of houses. We then learned how to evaluate regression
    models, were introduced to linear regression through various libraries such as `scikit-learn`, 
    Apache Spark and SageMaker's linear learner, and, finally, we saw how to use logistic
    regression for classification problems, and the pros and cons of linear models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will predict user behavior with tree-based methods.
  prefs: []
  type: TYPE_NORMAL
