["```py\n > library(MASS)\n\n> library(caretEnsemble)\n\n > library(caTools)\n\n > pima <- rbind(Pima.tr, Pima.te)\n\n > set.seed(502)\n\n > split <- createDataPartition(y = pima$type, p = 0.75, list = F)\n\n > train <- pima[split, ]\n\n > test <- pima[-split, ]\n\n```", "```py\n > table(train$type)\n\n No Yes \n 267 133\n\n```", "```py\n> control <- trainControl(method = \"cv\",\n number = 5,\n savePredictions = \"final\",\n classProbs = T,\n index=createResample(train$type, 5),\n sampling = \"up\",\n summaryFunction = twoClassSummary)\n\n```", "```py\n > set.seed(2) \n > models <- caretList(\n type ~ ., data = train,\n trControl = control,\n metric = \"ROC\",\n methodList = c(\"rpart\", \"earth\", \"knn\") )\n\n```", "```py\n > models\n ...\n Resampling results across tuning parameters:\n\n cp       ROC      Sens      Spec \n 0.03007519 0.7882347 0.8190343 0.6781714\n 0.04010025 0.7814718 0.7935024 0.6888857\n 0.36090226 0.7360166 0.8646440 0.6073893\n\n```", "```py\n > modelCor(resamples(models))\n rpart     earth       knn\n rpart 1.0000000 0.9589931 0.7191618\n earth 0.9589931 1.0000000 0.8834022\n knn   0.7191618 0.8834022 1.0000000\n\n```", "```py\n > model_preds <- lapply(models, predict, newdata=test, type=\"prob\") \n > model_preds <- lapply(model_preds, function(x) x[,\"Yes\"]) \n > model_preds <- data.frame(model_preds)\n\n```", "```py\n > stack <- caretStack(models, method = \"glm\",\n metric = \"ROC\",\n trControl = trainControl(\n method = \"boot\",\n number = 5,\n savePredictions = \"final\",\n classProbs = TRUE,\n summaryFunction = twoClassSummary\n ))\n\n```", "```py\n > summary(stack)\n\n Call:\n NULL\n\n Deviance Residuals: \n Min      1Q  Median     3Q    Max \n -2.1029 -0.6268 -0.3584 0.5926 2.3714 \n\n Coefficients:\n Estimate  Std. Error  z value   Pr(>|z|) \n (Intercept)  2.2212      0.2120   10.476   < 2e-16 ***\n rpart       -0.8529      0.3947   -2.161   0.03071 * \n earth       -3.0984      0.4250   -7.290   3.1e-13 ***\n knn         -1.2626      0.3524   -3.583   0.00034 ***\n\n```", "```py\n > prob <- 1-predict(stack, newdata = test, type = \"prob\")\n\n > model_preds$ensemble <- prob\n\n > colAUC(model_preds, test$type)\n rpart     earth       knn  ensemble\n No vs. Yes 0.7413481 0.7892562 0.7652376 0.8001033\n\n```", "```py\n > library(mlr) \n > library(ggplot2) \n > library(HDclassif) \n > library(DMwR) \n > library(reshape2)\n\n    > library(corrplot) \n > data(wine) \n > table(wine$class)\n\n 1  2  3 \n 59 71 48\n\n```", "```py\n > wine$class <- as.factor(wine$class)\n\n > set.seed(11)\n\n > df <- SMOTE(class ~ ., wine, perc.over = 300, perc.under = 300)\n\n > table(df$class)\n\n 1   2   3 \n 195 237 192\n\n```", "```py\n > wine.scale <- data.frame(scale(wine[, 2:5])) \n > wine.scale$class <- wine$class \n > wine.melt <- melt(wine.scale, id.var=\"class\") \n > ggplot(data = wine.melt, aes( x = class, y = value)) +\n geom_boxplot() +\n facet_wrap( ~ variable, ncol = 2)\n\n```", "```py\n > outHigh <- function(x) {\n x[x > quantile(x, 0.99)] <- quantile(x, 0.75)\n x\n }\n\n > outLow <- function(x) {\n x[x < quantile(x, 0.01)] <- quantile(x, 0.25)\n x\n }\n\n```", "```py\n    > wine.trunc <- data.frame(lapply(wine[, -1], outHigh))\n\n > wine.trunc <- data.frame(lapply(wine.trunc, outLow))\n\n > wine.trunc$class <- wine$class\n\n```", "```py\n > boxplot(wine.trunc$V3 ~ wine.trunc$class)\n\n```", "```py\n > c <- cor(wine.trunc[, -14])\n\n > corrplot.mixed(c, upper = \"ellipse\")\n\n```", "```py\n > library(caret) #if not already loaded \n > set.seed(502) \n > split <- createDataPartition(y = df$class, p = 0.7, list = F) \n > train <- df[split, ] \n > test <- df[-split, ]\n\n    > wine.task <- makeClassifTask(id = \"wine\", data = train, target = \n      \"class\") \n\n```", "```py\n > str(getTaskData(wine.task))\n 'data.frame': 438 obs. of 14 variables:\n $ class: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 2 1 2 2 1 2 1 1 2 ...\n $ V1 : num 13.6 11.8 14.4 11.8 13.1 ...\n\n```", "```py\n > rdesc <- makeResampleDesc(\"Subsample\", iters = 3)\n\n```", "```py\n > param <- makeParamSet(\n        makeDiscreteParam(\"ntree\", values = c(750, 1000, 1250, 1500, \n         1750, 2000))\n    )\n\n```", "```py\n > ctrl <- makeTuneControlGrid()\n\n```", "```py\n > tuning <- tuneParams(\"classif.randomForest\", task = wine.task,\n resampling = rdesc, par.set = param,\n control = ctrl)\n\n    > tuning$x\n    $ntree\n    [1] 1250\n\n    > tuning$y\n    mmce.test.mean \n    0.01141553 \n\n```", "```py\n > rf <- setHyperPars(makeLearner(\"classif.randomForest\",\n predict.type = \"prob\"), par.vals = tuning$x)\n\n```", "```py\n > fitRF <- train(rf, wine.task)\n\n```", "```py\n > fitRF$learner.model\n              OOB estimate of error rate: 0%\n    Confusion matrix:\n       1   2  3  class.error\n    1 72   0   0           0\n    2  0  97   0           0\n    3  0   0 101           0 \n\n```", "```py\n > predRF <- predict(fitRF, newdata = test)\n\n > getConfMatrix(predRF)\n predicted\n true     1  2  3  -SUM-\n 1     58  0  0      0\n 2      0 71  0      0\n 3      0  0 57      0\n -SUM-  0  0  0      0\n\n > performance(predRF, measures = list(mmce, acc))\n mmce acc \n 0   1\n\n```", "```py\n > ovr <- makeMulticlassWrapper(\"classif.penalized.ridge\", \n      mcw.method = \"onevsrest\")\n\n```", "```py\n > bag.ovr = makeBaggingWrapper(ovr, bw.iters = 10, #default of 10\n bw.replace = TRUE, #default\n bw.size = 0.7,\n bw.feats = 1)\n\n```", "```py\n > set.seed(317)\n > fitOVR <- mlr::train(bag.ovr, wine.task)\n    > predOVR <- predict(fitOVR, newdata = test) \n\n```", "```py\n > head(data.frame(predOVR))\n truth response\n 60     2        2\n 78     2        2\n 79     2        2\n 49     1        1\n 19     1        1\n 69     2        2\n\n > getConfMatrix(predOVR)\n predicted\n true     1  2  3  -SUM-\n 1     58  0  0      0\n 2      0 71  0      0\n 3      0  0 57      0\n -SUM-  0  0  0      0\n\n```", "```py\n > pima.task <- makeClassifTask(id = \"pima\", data = train, target = \n      \"type\")\n\n```", "```py\n > pima.smote <- smote(pima.task, rate = 2, nn = 3)\n\n    > str(getTaskData(pima.smote))\n    'data.frame': 533 obs. of 8 variables: \n\n```", "```py\n > base <- c(\"classif.randomForest\", \"classif.qda\", classif.glmnet\") \n > learns <- lapply(base, makeLearner) \n > learns <- lapply(learns, setPredictType, \"prob\")\n\n```", "```py\n > sl <- makeStackedLearner(base.learners = learns,\n super.learner = \"classif.logreg\",\n predict.type = \"prob\",\n method = \"stack.cv\")\n\n```", "```py\n > slFit <- mlr::train(sl, pima.smote)\n\n > predFit <- predict(slFit, newdata = test)\n\n > getConfMatrix(predFit)\n predicted\n true        No Yes -SUM-\n No        70  18    18\n Yes       15  29    15\n -SUM-     15  18    33\n\n > performance(predFit, measures = list(mmce, acc, auc))\n mmce    acc         auc\n 0.25   0.75   0.7874483\n\n```"]