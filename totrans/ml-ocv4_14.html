<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Selecting the Right Model with Hyperparameter Tuning</h1>
                </header>
            
            <article>
                
<p>Now that we have explored a wide variety of machine learning algorithms, I am sure you have realized that most of them come with a great number of settings to choose from. These settings or tuning knobs, the so-called <strong>hyperparameters</strong>, help us to control the behavior of the algorithm when we try to maximize performance.</p>
<p>For example, we might want to choose the depth or split criterion in a decision tree or tune the number of neurons in a neural network. Finding the values of important parameters of a model is a tricky task but necessary for almost all models and datasets.</p>
<p>In this chapter, we will dive deeper into <strong>model evaluation</strong> and <strong>hyperparameter tuning</strong>. Assume that we have two different ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can refer to the code for this chapter from the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11</a>.</p>
<p>Here is a summary of the software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li><span>You will need </span>Python version 3.6 (any Python version 3.x will be fine).</li>
<li><span>You will need </span>Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any operating system—macOS, Windows, and Linux-based OSes along with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided along with this book.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating a model</h1>
                </header>
            
            <article>
                
<p>Model evaluation strategies come in many different forms and shapes. In the following sections, we will, therefore, highlight three of the most commonly used techniques to compare models against each other:</p>
<ul>
<li>k-fold cross-validation</li>
<li>Bootstrapping</li>
<li>McNemar's test</li>
</ul>
<p>In principle, model evaluation is simple: after training a model on some data, we can estimate its effectiveness by comparing model predictions to some ground truth values. We learned early on that we should split the data into training and test sets, and we tried to follow this instruction whenever possible. But why exactly did we do that again?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating a model the wrong way</h1>
                </header>
            
            <article>
                
<p>The reason we never evaluate a model on the training set is that, in principle, any dataset can be learned if we throw a strong enough model at it.</p>
<p>A quick demonstration of this can be given with the help of the Iris dataset, which we talked about extensively in <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>. There, the goal was to classify species of iris flowers based on their physical dimensions. We can load the Iris dataset using scikit-learn:</p>
<pre>In [1]: from sklearn.datasets import load_iris<br/>...     iris = load_iris()</pre>
<p>An innocent approach to this problem would be to store all data points in the matrix, <kbd>X</kbd>, and all class labels in the vector, <kbd>y</kbd>:</p>
<pre>In [2]: import numpy as np<br/>...     X = iris.data.astype(np.float32)<br/>...     y = iris.target</pre>
<p>Next, we choose a model and its hyperparameters. For example, let's use the algorithm from <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>, which provides only a single hyperparameter: the number of neighbors, <em>k</em>. With <em>k=1</em>, we get a very simple model that classifies the label of an unknown point as belonging to the same class as its closest neighbor.</p>
<p>In the following steps, you will learn how to build a <strong>k-nearest neighbour</strong> (<strong>k-NN</strong>) <span>and compute its accuracy:</span></p>
<ol>
<li>In OpenCV, kNN instantiates as follows:</li>
</ol>
<pre style="padding-left: 60px">In [3]: import cv2<br/>...     knn = cv2.ml.KNearest_create()<br/>...     knn.setDefaultK(1)</pre>
<ol start="2">
<li>Then, we train the model and use it to predict labels for the data that we already know:</li>
</ol>
<pre style="padding-left: 60px">In [4]: knn.train(X, cv2.ml.ROW_SAMPLE, y)<br/>...     _, y_hat = knn.predict(X)</pre>
<ol start="3">
<li>Finally, we compute the fraction of the correctly labeled points:</li>
</ol>
<pre style="padding-left: 60px">In [5]: from sklearn.metrics import accuracy_score<br/>...     accuracy_score(y, y_hat)<br/>Out[5]: 1.0</pre>
<p>As we can see, the accuracy score is <kbd>1.0</kbd>, it indicates that 100% of points were correctly labeled by our model.</p>
<div class="packt_infobox">If a model gets 100% accuracy on the training set, we say the model memorized the data.</div>
<p>But is the expected accuracy truly being measured? Have we come up with a model that we expect to be correct 100% of the time?</p>
<p>As you may have gathered, the answer is no. This example shows that even a simple algorithm is capable of memorizing a real-world dataset. Imagine how easy this task would have been for a deep neural network! Usually, the more parameters a model has, the more powerful it is. We will come back to this shortly.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating a model in the right way</h1>
                </header>
            
            <article>
                
<p>A better sense of a model's performance can be found using what's known as a test set, but you already knew this. When presented with data held out from the training procedure, we can check whether a model has learned some dependencies in the data that hold across the board or whether it just memorized the training set.</p>
<p>We can split the data into training and test sets using the familiar <kbd>train_test_split</kbd> from scikit-learn's <kbd>model_selection</kbd> module:</p>
<pre>In [6]: from sklearn.model_selection import train_test_split</pre>
<p>But how do we choose the right train-test ratio? Is there even such a thing as a right ratio? Or is this considered another hyperparameter of the model?</p>
<p>There are two competing concerns here:</p>
<ul>
<li>If our ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Selecting the best model</h1>
                </header>
            
            <article>
                
<p>When a model is under-performing, it is often not clear how to make it better. Throughout this book, I have declared a rule of thumbs, for example, how to select the number of layers in a neural network. Even worse, the answer is often counter-intuitive! For example, adding another layer to the network might make the results worse, and adding more training data might not change performance at all.</p>
<p>You can see why these issues are some of the most important aspects of machine learning. At the end of the day, the ability to determine what steps will or will not improve our model is what separates the successful machine learning practitioner from all others.</p>
<p>Let's have a look at a specific example. Remember <a href="5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml" target="_blank">Chapter 5</a>, <em>Using Decision Trees to Make a Medical Diagnosis</em>, where we used decision trees in a regression task? We were fitting two different trees to a sin function—one with depth 2 and one with depth 5. As a reminder, the regression result looked like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1011 image-border" src="Images/e4b66301-dbb4-4153-b592-ef5624085515.png" style="width:47.25em;height:29.50em;" width="893" height="558"/></p>
<p>It should be clear that neither of these fits are particularly good. However, the two decision trees fail in two different ways!</p>
<p>The decision tree with depth 2 (thick line in the preceding screenshot) attempts to fit four straight lines through the data. Because the data is intrinsically more complicated than a few straight lines, this model fails. We could train it as much as we wanted, on as many training samples as we could generate—it would never be able to describe this dataset well. Such a model is said to underfit the data. <span>In other words, the model does not have enough complexity to account for all the features in the data. Hence, the model has</span> a high bias.</p>
<p>The other decision tree (thin line, depth 5) makes a different mistake. This model has enough flexibility to nearly perfectly account for the fine structures in the data. However, at some points, the model seems to follow the particular pattern of the noise; we added to the sin function rather than the sin function itself. You can see that on the right-hand side of the graph, where the blue curve (thin line) would jitter a lot. Such a model is said to overfit the data. In other words, the model is so complex that it ends up accounting for random errors in the data. Hence, the model has a high variance.</p>
<p>Long story short—here's the secret sauce: fundamentally, selecting the right model comes down to finding a sweet spot in the trade-off between bias and variance.</p>
<div class="packt_tip">The amount of flexibility a model has (also known as the <strong>model complexity</strong>) is mostly dictated by its hyperparameters. That is why it is so important to tune them!</div>
<p>Let's return to the kNN algorithm and the Iris dataset. If we repeated the procedure of fitting the model to the Iris data for all possible values of <em>k</em> and calculated both training and test scores, we would expect the result to look something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1012 image-border" src="Images/67d69189-ef9f-41e5-baca-de06a01932b6.png" style="width:24.42em;height:18.67em;" width="792" height="608"/></p>
<p>The preceding image shows model score as a function of model complexity. If there is one thing I would want you to remember from this chapter, it would be this diagram. Let's unpack it.</p>
<p>The diagram describes the model score (either training or test scores) as a function of model complexity. As mentioned in the preceding diagram, the model complexity of a neural network roughly grows with the number of neurons in the network. In the case of kNN, the opposite logic applies—the larger the value for <em>k</em>, the smoother the decision boundary, and hence, the lower the complexity. In other words, kNN with <em>k=1</em> would be all of the way to the right in the preceding diagram, where the training score is perfect. No wonder we got 100% accuracy on the training set!</p>
<p>From the preceding diagram, we can gather that there are three regimes in the model complexity landscape:</p>
<ul>
<li>Very low model complexity (a high-bias model) underfits the training data. In this regime, the model achieves only low scores on both the training and test set, no matter how long we trained it for.</li>
<li>A model with very high complexity (or a high-variance) overfits the training data, which indicates that the model can predict on the training data very well but fails on the unseen data. In this regime, the model has started to learn intricacies or peculiarities that only appear in the training data. Since these peculiarities do not apply to unseen data, the training score gets lower and lower.</li>
<li>For some intermediate value, the test score is maximal. It is this intermediate regime, where the test score is maximal, that we are trying to find. This is the sweet spot in the trade-off between bias and variance!</li>
</ul>
<p>This means that we can find the best algorithm for the task at hand by mapping out the model complexity landscape. Specifically, we can use the following indicators to know which regime we are currently in:</p>
<ul>
<li>If both training and test scores are below our expectations, we are probably in the leftmost regime in the preceding diagram, where the model is underfitting the data. In this case, a good idea might be to increase the model complexity and try again.</li>
<li>If the training score is much higher than the test score, we are probably in the rightmost regime in the preceding diagram, where the model is overfitting the data. In this case, a good idea might be to decrease the model complexity and try again.</li>
</ul>
<p>Although this procedure works in general, there are more sophisticated strategies for model evaluation that proved to be more thorough than a simple train-test split, which we will talk about in the following sections.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding cross-validation</h1>
                </header>
            
            <article>
                
<p>Cross-validation is a method of evaluating the generalization performance of a model that is generally more stable and thorough than splitting the dataset into training and test sets.</p>
<p>The most commonly used version of cross-validation is <strong>k-fold cross-validation</strong>, where <em>k</em> is a number specified by the user (usually five or ten). Here, the dataset is partitioned into <em>k</em> parts of more or less equal size, called <strong>folds</strong>. For a dataset that contains <em>N</em> data points, each fold should hence have approximatel<span>y</span> <em>N / k</em> <span>samples. Then, a series of models are trained on the data, using</span> <em>k - 1</em> <span>folds for training and one remaining fold for testing. The procedure is repeated for</span> <em>k</em> <span>iterations, each time choosing a different fold for ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Manually implementing cross-validation in OpenCV</h1>
                </header>
            
            <article>
                
<p>The easiest way to perform cross-validation in OpenCV is to do the data splits by hand.</p>
<p>For example, to implement two-fold cross-validation, we would perform the following procedure:</p>
<ol>
<li>Load the dataset:</li>
</ol>
<pre>      In [1]: from sklearn.datasets import load_iris<br/>      ...     import numpy as np<br/>      ...     iris = load_iris()<br/>      ...     X = iris.data.astype(np.float32)<br/>      ...     y = iris.target</pre>
<ol start="2">
<li>Split the data into two equally sized parts:</li>
</ol>
<pre>      In [2]: from sklearn.model_selection import model_selection<br/>      ...     X_fold1, X_fold2, y_fold1, y_fold2 = train_test_split(<br/>      ...         X, y, random_state=37, train_size=0.5<br/>      ...     )</pre>
<ol start="3">
<li>Instantiate the classifier:</li>
</ol>
<pre>      In [3]: import cv2<br/>      ...     knn = cv2.ml.KNearest_create()<br/>      ...     knn.setDefaultK(1)</pre>
<ol start="4">
<li>Train the classifier on the first fold, then predict the labels of the second fold:</li>
</ol>
<pre>      In [4]: knn.train(X_fold1, cv2.ml.ROW_SAMPLE, y_fold1)<br/>      ...     _, y_hat_fold2 = knn.predict(X_fold2)</pre>
<ol start="5">
<li>Train the classifier on the second fold, then predict the labels of the first fold:</li>
</ol>
<pre>      In [5]: knn.train(X_fold2, cv2.ml.ROW_SAMPLE, y_fold2)<br/>      ...     _, y_hat_fold1 = knn.predict(X_fold1)</pre>
<ol start="6">
<li>Compute accuracy scores for both folds:</li>
</ol>
<pre>      In [6]: from sklearn.metrics import accuracy_score<br/>      ...     accuracy_score(y_fold1, y_hat_fold1)<br/>      Out[6]: 0.92000000000000004<br/>      In [7]: accuracy_score(y_fold2, y_hat_fold2)<br/>      Out[7]: 0.88</pre>
<p>This procedure will yield two accuracy scores, one for the first fold (92% accuracy) and one for the second fold (88% accuracy). On average, our classifier hence achieved 90% accuracy on unseen data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using scikit-learn for k-fold cross-validation</h1>
                </header>
            
            <article>
                
<p>In scikit-learn, cross-validation can be performed in three steps:</p>
<ol>
<li>Load the dataset. Since we already did this earlier, we don't have to do it again.</li>
<li>Instantiate the classifier:</li>
</ol>
<pre>      In [8]: from sklearn.neighbors import KNeighborsClassifier      ...     model = KNeighborsClassifier(n_neighbors=1)</pre>
<ol start="3">
<li>Perform cross-validation with the <kbd>cross_val_score</kbd> function. This function takes as input a model, the full dataset (<kbd>X</kbd>), the target labels (<kbd>y</kbd>), and an integer value for the number of folds (<kbd>cv</kbd>). It is not necessary to split the data by hand—the function will do that automatically depending on the number of folds. After the cross-validation is completed, the function returns the test scores:</li>
</ol>
<pre> In [9]: from sklearn.model_selection ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing leave-one-out cross-validation</h1>
                </header>
            
            <article>
                
<p>Another popular way to implement cross-validation is to choose the number of folds equal to the number of data points in the dataset. In other words, if there are <em>N</em> data points, we set <em>k=N</em>. This means that we will end up having to do <em>N</em> iterations of cross-validation, but in every iteration, the training set will consist of only a single data point. The advantage of this procedure is that we get to use all-but-one data points for training. <span>Hence, this procedure is also known as</span> <span><strong>leave-one-out</strong> cross-validation</span><span>.</span></p>
<p>In scikit-learn, this functionality is provided by the <kbd>LeaveOneOut</kbd> method from the <kbd>model_selection</kbd> module:</p>
<pre>In [11]: from sklearn.model_selection import LeaveOneOut</pre>
<p>This object can be passed directly to the <kbd>cross_val_score</kbd> function in the following way:</p>
<pre>In [12]: scores = cross_val_score(model, X, y, cv=LeaveOneOut())</pre>
<p>Because every test set now contains a single data point, we would expect the scorer to return 150 values—one for each data point in the dataset. Each of these points we get could <span><span>be</span></span> either right or wrong. Hence, we expect <kbd>scores</kbd> to be a list of ones (<kbd>1</kbd>) and zeros (<kbd>0</kbd>), which corresponds to correct and incorrect classifications, respectively:</p>
<pre>In [13]: scores<br/>Out[13]: array([ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,<br/>                 1., 1., 1., 1., 1., 1., 1.])</pre>
<p>If we want to know the average performance of the classifier, we would still compute the mean and standard deviation of the scores:</p>
<pre>In [14]: scores.mean(), scores.std()<br/>Out[14]: (0.95999999999999996, 0.19595917942265423)</pre>
<p>We can see this scoring scheme returns very similar results to five-fold cross-validation.</p>
<div class="packt_tip">You can learn more about other useful cross-validation procedures at <a href="http://scikit-learn.org/stable/modules/cross_validation.html">http://scikit-learn.org/stable/modules/cross_validation.html</a>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Estimating robustness using bootstrapping</h1>
                </header>
            
            <article>
                
<p>An alternative procedure to k-fold cross-validation is <strong>bootstrapping</strong>.</p>
<p>Instead of splitting the data into folds, bootstrapping builds a training set by drawing samples randomly from the dataset. Typically, a bootstrap is formed by drawing samples with replacement. Imagine putting all of the data points into a bag and then drawing randomly from the bag. After drawing a sample, we would put it back in the bag. This allows for some samples to show up multiple times in the training set, which is something cross-validation does not allow.</p>
<p>The classifier is then tested on all samples that are not part of the bootstrap (the so-called <strong>out-of-bag</strong> examples), and the procedure is repeated a large number of times ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Manually implementing bootstrapping in OpenCV</h1>
                </header>
            
            <article>
                
<p>Bootstrapping can be implemented with the following procedure:</p>
<ol>
<li>Load the dataset. Since we already did this earlier, we don't have to do it again.</li>
<li>Instantiate the classifier:</li>
</ol>
<pre>      In [15]: knn = cv2.ml.KNearest_create()<br/>      ...      knn.setDefaultK(1)</pre>
<ol start="3">
<li>From our dataset with <em>N</em> samples, randomly choose <em>N</em> samples with replacement to form a bootstrap. This can be done most easily with the <kbd>choice</kbd> function from NumPy's <kbd>random</kbd> module. We tell the function to draw <kbd>len(X)</kbd> samples in the <kbd>[0, len(X)-1]</kbd> <span>range </span>with replacement (<kbd>replace=True</kbd>). The function then returns a list of indices, from which we form our bootstrap:</li>
</ol>
<pre>      In [16]: idx_boot = np.random.choice(len(X), size=len(X),<br/>      ...                                  replace=True)<br/>      ...      X_boot = X[idx_boot, :]<br/>      ...      y_boot = y[idx_boot]</pre>
<ol start="4">
<li>Put all samples that do not show in the bootstrap in the out-of-bag set:</li>
</ol>
<pre>      In [17]: idx_oob = np.array([x not in idx_boot<br/>      ...      for x in np.arange(len(X))],dtype=np.bool)<br/>      ...      X_oob = X[idx_oob, :]<br/>      ...      y_oob = y[idx_oob]</pre>
<ol start="5">
<li>Train the classifier on the bootstrap samples:</li>
</ol>
<pre>      In [18]: knn.train(X_train, cv2.ml.ROW_SAMPLE, y_boot)<br/>      Out[18]: True</pre>
<ol start="6">
<li>Test the classifier on the out-of-bag samples:</li>
</ol>
<pre>      In [19]: _, y_hat = knn.predict(X_oob)<br/>      ...      accuracy_score(y_oob, y_hat)<br/>      Out[19]: 0.9285714285714286</pre>
<ol start="7">
<li>Repeat <em>steps 3-6</em> for a specific number of iterations.</li>
<li>Iteration of the bootstrap. Repeat these steps up to 10,000 times to get 10,000 accuracy scores, then average the scores to get an idea of the classifier's mean performance.</li>
</ol>
<p>For our convenience, we can build a function from <em>steps 3</em>-<em>6</em> so that it is easy to run the procedure for some <kbd>n_iter</kbd> number of times. We also pass a model (our kNN classifier, <kbd>model</kbd>), the feature matrix (<kbd>X</kbd>), and the vector with all class labels (<kbd>y</kbd>):</p>
<pre>In [20]: def yield_bootstrap(model, X, y, n_iter=10000):<br/>...          for _ in range(n_iter):</pre>
<p>The steps within the <kbd>for</kbd> loop are essentially <em>steps 3</em>-<em>6</em> from the code mentioned earlier. This involved training the classifier on the bootstrap and testing it on the out-of-bag examples:</p>
<pre>...              # train the classifier on bootstrap<br/>...              idx_boot = np.random.choice(len(X), size=len(X),<br/>...                                          replace=True)<br/>...              X_boot = X[idx_boot, :]<br/>...              y_boot = y[idx_boot]<br/>...              knn.train(X_boot, cv2.ml.ROW_SAMPLE, y_boot)<br/>... <br/>...              # test classifier on out-of-bag examples<br/>...              idx_oob = np.array([x not in idx_boot<br/>...                                  for x in np.arange(len(X))],<br/>...                                 dtype=np.bool)<br/>...              X_oob = X[idx_oob, :]<br/>...              y_oob = y[idx_oob]<br/>...              _, y_hat = knn.predict(X_oob)</pre>
<p>Then, we need to return the accuracy score. You might expect a <kbd>return</kbd> statement here. However, a more elegant way is to use the <kbd>yield</kbd> statement, which turns the function automatically into a generator. This means we don't have to initialize an empty list (<kbd>acc = []</kbd>) and then append the new accuracy score at each iteration (<kbd>acc.append(accuracy_score(...))</kbd>). The bookkeeping is done automatically:</p>
<pre>...              yield accuracy_score(y_oob, y_hat)</pre>
<p>To make sure we all get the same result, let's fix the seed of the random number generator:</p>
<pre>In [21]: np.random.seed(42)</pre>
<p>Now, let's run the procedure for <kbd>n_iter=10</kbd> times by converting the function output into a list:</p>
<pre>In [22]: list(yield_bootstrap(knn, X, y, n_iter=10))<br/>Out[22]: [0.98333333333333328,<br/>          0.93650793650793651,<br/>          0.92452830188679247,<br/>          0.92307692307692313,<br/>          0.94545454545454544,<br/>          0.94736842105263153,<br/>          0.98148148148148151,<br/>          0.96078431372549022,<br/>          0.93220338983050843,<br/>          0.96610169491525422]</pre>
<p>As you can see, for this small sample we get accuracy scores anywhere between 92% and 98%. To get a more reliable estimate of the model's performance, we repeat the procedure 1,000 times and calculate both mean and standard deviation of the resulting scores:</p>
<pre>In [23]: acc = list(yield_bootstrap(knn, X, y, n_iter=1000))<br/>...      np.mean(acc), np.std(acc)<br/>Out[23]: (0.95524155136419198, 0.022040380995646654)</pre>
<p>You are always welcome to increase the number of repetitions. But once <kbd>n_iter</kbd> is large enough, the procedure should be robust to the randomness of the sampling procedure. In this case, we do not expect to see any more changes to the distribution of score values as we keep increasing <kbd>n_iter</kbd> to, for example, 10,000 iterations:</p>
<pre>In [24]: acc = list(yield_bootstrap(knn, X, y, n_iter=10000))<br/>...      np.mean(acc), np.std(acc)<br/>Out[24]: (0.95501528733009422, 0.021778543317079499)</pre>
<p>Typically, the scores obtained with bootstrapping would be used in a <em>statistical test</em> to assess the <em>significance</em> of our result. Let's have a look at how that is done.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Assessing the significance of our results</h1>
                </header>
            
            <article>
                
<p>Assume for a moment that we implemented the cross-validation procedure for two versions of our kNN classifier. The resulting test scores are—92.34% for Model A and 92.73% for Model B. How do we know which model is better?</p>
<p>Following our logic introduced here, we might argue for Model B because it has a better test score. But what if the two models are not significantly different? These could have two underlying causes, which are both a consequence of the randomness of our testing procedure:</p>
<ul>
<li>For all we know, Model B just got lucky. Perhaps we chose a really low k for our cross-validation procedure. Perhaps Model B ended up with a beneficial train-test split so that the model had no problem classifying ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing Student's t-test</h1>
                </header>
            
            <article>
                
<p>One of the most famous statistical tests is <strong>Student's t-test</strong>. You might have heard of it before: it allows us to determine whether two sets of data are significantly different from one another. This was a really important test for William Sealy Gosset, the inventor of the test, who worked at the Guinness brewery and wanted to know whether two batches of stout differed in quality.</p>
<div class="packt_tip packt_infobox">Note that "Student" here is capitalized. Although Gosset wasn't allowed to publish his test due to company policy, he did so anyway under his pen name, Student.</div>
<p>In practice, the t-test allows us to determine whether two data samples come from underlying distributions with the same mean or <em>expected value</em>.</p>
<p>For our purposes, this means that we can use the t-test to determine whether the test scores of two independent classifiers have the same mean value. We start by hypothesizing that the two sets of test scores are identical. We call this the <em>null hypothesis</em> because this is the hypothesis we want to nullify, that is, we are looking for evidence to <em>reject</em> the hypothesis because we want to ensure that one classifier is significantly better than the other.</p>
<p>We accept or reject a null hypothesis based on a parameter known as the <strong>p-value</strong> that the t-test returns. The p-value takes on values between <kbd>0</kbd> and <kbd>1</kbd>. A p-value of <kbd>0.05</kbd> would mean that the null hypothesis is right only 5 out of 100 times. A small p-value hence indicates strong evidence that the hypothesis can be safely rejected. It is customary to use <em>p=0.05</em> as a cut-off value below which we reject the null hypothesis.</p>
<p>If this is all too confusing, think of it this way: when we run a t-test to compare classifier test scores, we are looking to obtain a small p-value because that means that the two classifiers give significantly different results.</p>
<p>We can implement Student's t-test with SciPy's <kbd>ttest_ind</kbd> function from the <kbd>stats</kbd> module:</p>
<pre>In [25]: from scipy.stats import ttest_ind</pre>
<p>Let's start with a simple example. Assume we ran five-fold cross-validation on two classifiers and obtained the following scores:</p>
<pre>In [26]: scores_a = [1, 1, 1, 1, 1]<br/>...      scores_b = [0, 0, 0, 0, 0]</pre>
<p>This means that Model A achieved 100% accuracy in all five folds, whereas Model B got 0% accuracy. In this case, it is clear that the two results are significantly different. If we run the t-test on this data, we should hence find a really small p-value:</p>
<pre>In [27]: ttest_ind(scores_a, scores_b)<br/>Out[27]: Ttest_indResult(statistic=inf, pvalue=0.0)</pre>
<p>And we do! We actually get the smallest possible p-value, <em>p=0.0</em>.</p>
<p>On the other hand, what if the two classifiers got exactly the same numbers, except during different folds? In this case, we would expect the two classifiers to be equivalent, which is indicated by a really large p-value:</p>
<pre>In [28]: scores_a = [0.9, 0.9, 0.9, 0.8, 0.8]<br/>...      scores_b = [0.8, 0.8, 0.9, 0.9, 0.9]<br/>...      ttest_ind(scores_a, scores_b)<br/>Out[28]: Ttest_indResult(statistic=0.0, pvalue=1.0)</pre>
<p>Analogous to the aforementioned, we get the largest possible p-value, <em>p=1.0</em>.</p>
<p>To see what happens in a more realistic example, let's return to our kNN classifier from an earlier example. Using the test scores obtained from the ten-fold cross-validation procedure, we can compare two different kNN classifiers with the following procedure:</p>
<ol>
<li>Obtain a set of test scores for Model A. We choose Model A to be the kNN classifier from earlier (<em>k=1</em>):</li>
</ol>
<pre>      In [29]: k1 = KNeighborsClassifier(n_neighbors=1)<br/>      ...      scores_k1 = cross_val_score(k1, X, y, cv=10)<br/>      ...      np.mean(scores_k1), np.std(scores_k1)<br/>      Out[29]: (0.95999999999999996, 0.053333333333333323)</pre>
<ol start="2">
<li>Obtain a set of test scores for Model B. Let's choose Model B to be a kNN classifier with <em>k=3</em>:</li>
</ol>
<pre>      In [30]: k3 = KNeighborsClassifier(n_neighbors=3)<br/>      ...      scores_k3 = cross_val_score(k3, X, y, cv=10)<br/>      ...      np.mean(scores_k3), np.std(scores_k3)<br/>      Out[30]: (0.96666666666666656, 0.044721359549995787)</pre>
<ol start="3">
<li>Apply the t-test to both sets of scores:</li>
</ol>
<pre>      In [31]: ttest_ind(scores_k1, scores_k3)<br/>      Out[31]: Ttest_indResult(statistic=-0.2873478855663425,<br/>               pvalue=0.77712784875052965)</pre>
<p>As you can see, this is a good example of two classifiers giving different cross-validation scores (96.0% and 96.7%) that turn out to be not significantly different! Because we get a large p-value (<em>p=0.777</em>), we expect the two classifiers to be equivalent 77 out of 100 times.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing McNemar's test</h1>
                </header>
            
            <article>
                
<p>A more advanced statistical technique is <strong>McNemar's test</strong>. This test can be used on paired data to determine whether there are any differences between the two samples. As in the case of the t-test, we can use McNemar's test to determine whether two models give significantly different classification results.</p>
<p>McNemar's test operates on pairs of data points. This means that we need to know, for both classifiers, how they classified each data point. Based on the number of data points that the first classifier got right but the second got wrong and vice versa, we can determine whether the two classifiers are equivalent.</p>
<p>Let's assume the preceding Model A and Model B were applied to the same five data points. Whereas Model ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters with grid search</h1>
                </header>
            
            <article>
                
<p>The most commonly used tool for hyperparameter tuning is <em>grid search</em>, which is basically a fancy term for saying we will try all possible parameter combinations with a <kbd>for</kbd> loop.</p>
<p>Let's have a look at how that is done in practice.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing a simple grid search</h1>
                </header>
            
            <article>
                
<p>Returning to our kNN classifier, we find that we have only one hyperparameter to tune: <em>k</em>. Typically, you would have a much larger number of open parameters to mess with, but the kNN algorithm is simple enough for us to manually implement a grid search.</p>
<p>Before we get started, we need to split the dataset as we have done before into training and test sets:</p>
<ol>
<li>Here we choose a 75-25 split:</li>
</ol>
<pre style="padding-left: 60px">In [1]: from sklearn.datasets import load_iris...     import numpy as np...     iris = load_iris()...     X = iris.data.astype(np.float32)...     y = iris.targetIn [2]: X_train, X_test, y_train, y_test = train_test_split(...          X, y, random_state=37...      )</pre>
<ol start="2">
<li>Then, the goal is to loop over all possible values of <em>k</em>. As we do this, we want to keep ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the value of a validation set</h1>
                </header>
            
            <article>
                
<p>Following our best practice of splitting the data into training and test sets, we might be tempted to tell people that we have found a model that performs with 97.4% accuracy on the dataset. However, our result might not necessarily generalize to new data. The argument is the same as earlier on in this book when we warranted the train-test split that we need an independent dataset for evaluation.</p>
<p>However, when we implemented a grid search in the last section, we used the test set to evaluate the outcome of the grid search and update the hyperparameter, <em>k</em>. This means we can no longer use the test set to evaluate the final data! Any model choices made based on the test set accuracy would leak information from the test set into the model.</p>
<p>One way to resolve this data is to split the data again and introduce what is known as a <strong>validation set</strong>. The validation set is different from the training and test set and is used exclusively for selecting the best parameters of the model. It is a good practice to do all exploratory analysis and model selection on this validation set and keep a separate test set, which is only used for the final evaluation.</p>
<p>In other words, we should end up splitting the data into three different sets:</p>
<ul>
<li>A <strong>training set</strong>, which is used to build the model</li>
<li>A <strong>validation set</strong>, which is used to select the parameters of the model</li>
<li>A <strong>test set</strong>, which is used to evaluate the performance of the final model</li>
</ul>
<p>Such a three-way split is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1015 image-border" src="Images/55d53de3-6073-4095-a210-4d7bf70204d1.png" style="width:30.00em;height:9.67em;" width="1863" height="595"/></p>
<p>The preceding diagram shows an example of how to split a dataset into training, validation, and test sets. In practice, the three-way split is achieved in two steps:</p>
<ol>
<li>Split the data into two chunks: one that contains training and validation sets and another that contains the test set:</li>
</ol>
<pre>      In [6]: X_trainval, X_test, y_trainval, y_test =<br/>      ...        train_test_split(X, y, random_state=37)<br/>      In [7]: X_trainval.shape<br/>      Out[7]: (112, 4)</pre>
<ol start="2">
<li>Split <kbd>X_trainval</kbd> again into proper training and validation sets:</li>
</ol>
<pre>      In [8]: X_train, X_valid, y_train, y_valid = train_test_split(<br/>      ...         X_trainval, y_trainval, random_state=37<br/>      ...     )<br/>      In [9]: X_train.shape<br/>      Out[9]: (84, 4)</pre>
<p>Then, we repeat the manual grid search from the preceding code, but this time, we will use the validation set to find the best <em>k</em> (see code highlights):</p>
<pre>In [10]: best_acc = 0.0<br/>...      best_k = 0<br/>...      for k in range(1, 20):<br/>...          knn = cv2.ml.KNearest_create()<br/>...          knn.setDefaultK(k)<br/>...          knn.train(X_train, cv2.ml.ROW_SAMPLE, y_train)<br/>...          _, y_valid_hat = knn.predict(X_valid)<br/>...          acc = accuracy_score(y_valid, y_valid_hat)<br/>...          if acc &gt;= best_acc:<br/>...              best_acc = acc<br/>...              best_k = k<br/>...      best_acc, best_k<br/>Out[10]: (1.0, 7)</pre>
<p>We now find that a 100% validation score (<kbd>best_acc</kbd>) can be achieved with <em>k=7</em> (<kbd>best_k</kbd>)! However, recall that this score might be overly optimistic. To find out how well the model really performs, we need to test it on held-out data from the test set.</p>
<p>To arrive at our final model, we can use the value for <em>k</em> we found during grid search and re-train the model on both the training and validation data. This way, we used as much data as possible to build the model while still honoring the train-test split principle.</p>
<p>This means we should retrain the model on <kbd>X_trainval</kbd>, which contains both the training and validation sets and score it on the test set:</p>
<pre>In [25]: knn = cv2.ml.KNearest_create()<br/>...      knn.setDefaultK(best_k)<br/>...      knn.train(X_trainval, cv2.ml.ROW_SAMPLE, y_trainval)<br/>...      _, y_test_hat = knn.predict(X_test)<br/>...      accuracy_score(y_test, y_test_hat), best_k<br/>Out[25]: (0.94736842105263153, 7)</pre>
<p>With this procedure, we find a formidable score of 94.7% accuracy on the test set. Because we honored the train-test split principle, we can now be sure that this is the performance we can expect from the classifier when applied to novel data. It is not as high as the 100% accuracy reported during validation, but it is still a very good score!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Combining grid search with cross-validation</h1>
                </header>
            
            <article>
                
<p>One potential danger of the grid search we just implemented is that the outcome might be relatively sensitive to how exactly we split the data. After all, we might have accidentally chosen a split that put most of the easy-to-classify data points in the test set, resulting in an overly optimistic score. Although we would be happy at first, as soon as we tried the model on some new held-out data, we would find that the actual performance of the classifier is much lower than expected.</p>
<p>Instead, we can combine grid search with cross-validation. This way, the data is split multiple times into training and validation sets, and cross-validation is performed at every step of the grid search to evaluate ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Combining grid search with nested cross-validation</h1>
                </header>
            
            <article>
                
<p>Although grid search with cross-validation makes for a much more robust model selection procedure, you might have noticed that we performed the split into training and validation sets still only once. As a result, our results might still depend too much on the exact training-validation split of the data.</p>
<p>Instead of splitting the data into training and validation sets once, we can go a step further and use multiple splits for cross-validation. This will result in what is known as <strong>nested cross-validation</strong>, and the process is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1017 image-border" src="Images/5e469a29-e08e-478e-8c5e-fea9a6a789f7.png" style="width:125.00em;height:61.92em;" width="1500" height="743"/></p>
<p>In nested cross-validation, there is an outer loop over the grid search box that repeatedly splits the data into training and validation sets. For each of these splits, a grid search is run, which will report back a set of best parameter values. Then, for each outer split, we get a test score using the best settings.</p>
<div class="packt_infobox">Running a grid search over many parameters and on large datasets can be computationally intensive. A particular parameter setting on a particular cross-validation split can be done completely independently from the other parameter settings and models. Hence, parallelization over multiple CPU cores or a cluster is very important for grid search and cross-validation.</div>
<p>Now that we know how to find the best parameters of a model, let's take a closer look at the different evaluation metrics that we can use to score a model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scoring models using different evaluation metrics</h1>
                </header>
            
            <article>
                
<p>So far, we have evaluated classification performance using accuracy (the fraction of correctly classified samples) and regression performance using R<sup>2</sup>. However, these are only two of the many possible ways to summarize how well a supervised model performs on a given dataset. In practice, these evaluation metrics might not be appropriate for our application, and it is important to choose the right metric when selecting between models and adjusting parameters.</p>
<p>When selecting a metric, we should always have the end goal of the machine learning application in mind. In practice, we are usually interested not just in making accurate predictions but also in using these predictions as part of a larger ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing the right classification metric</h1>
                </header>
            
            <article>
                
<p>We talked about several essential scoring functions in <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>. Among the most fundamental metrics for classification were the following:</p>
<ul>
<li><strong>Accuracy</strong>: This counts the number of data points in the test set that have been predicted correctly and returns that number as a fraction of the test set size (<kbd>sklearn.metrics.accuracy_score</kbd>). This is the most basic scoring function for classifiers, and we have made extensive use of it throughout this book.</li>
<li><strong>Precision</strong>: This describes the ability of a classifier not to label a positive sample as a negative (<kbd>sklearn.metrics.precision_score</kbd>).</li>
<li><strong>Recall</strong> <strong>(or sensitivity)</strong>: This describes the ability of a classifier to retrieve all of the positive samples (<kbd>sklearn.metrics.recall_score</kbd>).</li>
</ul>
<p>Although precision and recall are important measures, looking at only one of them will not give us a good idea of the big picture. One way to summarize the two measures is known as the <strong>f-score</strong> or <strong>f-measure</strong> (<kbd>sklearn.metrics.f1_score</kbd>), which computes the harmonic mean of precision and recall as <em>2(precision x recall) / (precision + recall).</em></p>
<p>Sometimes we need to do more than maximize accuracy. For example, if we are using machine learning in a commercial application, then the decision-making should be driven by the business goals. One of these goals might be to guarantee at least 90% recall. The challenge then becomes to develop a model that still has reasonable accuracy while satisfying all secondary requirements. Setting goals like this is often called <strong>setting the operating point</strong>.</p>
<p><span>However, it is often not clear what the operating point should be when developing a new system. To understand the problem better, it is important to investigate all possible trade-offs of precision and recall them at once. </span><span>This is possible</span> using <span>a tool called the</span> <strong>precision-recall curve</strong> <span>(</span><kbd>sklearn.metrics.precision_recall_curve</kbd><span>).</span></p>
<div class="mce-root packt_infobox">Another commonly used tool to analyze the behavior of classifiers is the <strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>) curve.
<div><span>The ROC curve considers all possible thresholds for a given classifier similar to the precision-recall curve, but it shows the </span><em>false positive rate</em><span> against the </span><em><span>true</span> positive rate</em> <span>instead of reporting precision and recall.</span></div>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing the right regression metric</h1>
                </header>
            
            <article>
                
<p>Evaluation for regression can be done in similar detail as we did for classification. In <a href="323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml" target="_blank">Chapter 3</a>, <em>First Steps in Supervised Learning</em>, we also talked about some fundamental metrics for regression:</p>
<ul>
<li><strong>Mean squared error</strong>: The most commonly used error metric for regression problems is to measure the squared error between the predicted and true target value for every data point in the training set, averaged across all data points (<kbd>sklearn.metrics.mean_squared_error</kbd>).</li>
<li><strong>Explained variance</strong>: A more sophisticated metric is to measure to what degree a model can explain the variation or dispersion of the test data (<kbd>sklearn.metrics.explained_variance_score</kbd>). Often, the amount of explained variance is measured using ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chaining algorithms together to form a pipeline</h1>
                </header>
            
            <article>
                
<p>Most machine learning problems we have discussed so far consist of at least a preprocessing step and a classification step. The more complicated the problem, the longer this <em>processing chain</em> might get. One convenient way to glue multiple processing steps together and even use them in grid search is by using the <kbd>Pipeline</kbd> class from scikit-learn.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing pipelines in scikit-learn</h1>
                </header>
            
            <article>
                
<p>The <kbd>Pipeline</kbd> class itself has a <kbd>fit</kbd>, a <kbd>predict</kbd>, and a <kbd>score</kbd> method, which all behave just like any other estimator in scikit-learn. The most common use case of the <kbd>Pipeline</kbd> class is to chain different preprocessing steps together with a supervised model like a classifier.</p>
<p>Let's return to the breast cancer dataset from <a href="5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml" target="_blank">Chapter 5</a>, <em>Using Decision Trees to Make a Medical Diagnosis</em>. Using scikit-learn, we import the dataset and split it into training and test sets:</p>
<pre>In [1]: from sklearn.datasets import load_breast_cancer...     import numpy as np...     cancer = load_breast_cancer()...     X = cancer.data.astype(np.float32)...     y = cancer.targetIn [2]: X_train, X_test, y_train, y_test = train_test_split(... X, y, random_state=37 ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using pipelines in grid searches</h1>
                </header>
            
            <article>
                
<p>Using a pipeline in a grid search works the same way as using any other estimator.</p>
<p>We define a parameter grid to search over and construct <kbd>GridSearchCV</kbd> from the pipeline and the parameter grid. When specifying the parameter grid, there is, however, a slight change. We need to specify for each parameter which step of the pipeline it belongs to. Both parameters that we want to adjust, <kbd>C</kbd> and <kbd>gamma</kbd>, are parameters of <kbd>SVC</kbd>. In the preceding section, we gave this step the name <kbd>"svm"</kbd>. The syntax to define a parameter grid for a pipeline is to specify for each parameter the step name, followed by <kbd>__</kbd> (a double underscore), followed by the parameter name.</p>
<p>Hence, we would construct the parameter grid as follows:</p>
<pre>In [8]: param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],<br/>...                   'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}</pre>
<p>With this parameter grid, we can use <kbd>GridSearchCV</kbd> as usual:</p>
<pre>In [9]: grid = GridSearchCV(pipe, param_grid=param_grid, cv=10)<br/>...     grid.fit(X_train, y_train);</pre>
<p>The best score in the grid is stored in <kbd>best_score_</kbd>:</p>
<pre>In [10]: grid.best_score_<br/>Out[10]: 0.97652582159624413</pre>
<p>Similarly, the best parameters are stored in <kbd>best_params_</kbd>:</p>
<pre>In [11]: grid.best_params_<br/>Out[11]: {'svm__C': 1, 'svm__gamma': 1}</pre>
<p>But recall that the cross-validation score might be overly optimistic. To know the true performance of the classifier, we need to score it on the test set:</p>
<pre>In [12]: grid.score(X_test, y_test)<br/>Out[12]: 0.965034965034965</pre>
<p>In contrast to the grid search we did before, now, for each split in the cross-validation, <kbd>MinMaxScaler</kbd> is refit with only the training splits, and no information is leaked from the test split into the parameter search.</p>
<p>This makes it easy to build a pipeline to chain together a whole variety of steps! You can mix and match estimators in the pipeline at will, you just need to make sure that every step in the pipeline provides a <kbd>transform</kbd> method (except for the last step). This allows an estimator in the pipeline to produce a new representation of the data, which, in turn, can be used as input to the next step.</p>
<div class="packt_tip">The <kbd>Pipeline</kbd> class is not restricted to preprocessing and classification but can, in fact, join any number of estimators together. For example, we could build a pipeline containing feature extraction, feature selection, scaling, and classification, for a total of four steps. Similarly, the last step could be regression or clustering instead of classification.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we tried to complement our existing machine learning skills by discussing best practices in model selection and hyperparameter tuning. You learned how to tweak the hyperparameters of a model using grid search and cross-validation in both OpenCV and scikit-learn. We also talked about a wide variety of evaluation metrics and how to chain algorithms into a pipeline. Now, you are almost ready to start working on some real-world problems on your own.</p>
<p>In the next chapter, you will be introduced to an exciting and a new topic, that is, OpenVINO toolkit, which was one of the key releases in OpenCV 4.0.</p>


            </article>

            
        </section>
    </div>



  </body></html>