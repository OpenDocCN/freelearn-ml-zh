<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03" class="calibre1"/>Chapter 3. Recognizing Facial Expressions with Machine Learning</h1></div></div></div><p class="calibre8">Automatic facial expression recognition has attracted much attention since the early nineties, especially in human-computer interaction. As computers start becoming a part of our life, they need to become more and more intelligent. Expression recognition systems will enhance this intelligent interaction between the human and the computer.</p><p class="calibre8">Although humans can recognize facial expressions easily, a reliable expression recognition system is still a challenge. In this chapter, we will introduce a basic implementation of facial expression using various algorithms from the OpenCV library, including feature extraction and classification using the ml module.</p><p class="calibre8">In this chapter, we will be going through the following topics in brief:</p><div><ul class="itemizedlist"><li class="listitem">A simple architecture to recognize human facial expressions</li><li class="listitem">Feature extraction algorithms in the OpenCV library</li><li class="listitem">The learning and testing stage, with various machine learning algorithms</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch03lvl1sec23" class="calibre1"/>Introducing facial expression recognition</h1></div></div></div><p class="calibre8">Automatic<a id="id278" class="calibre1"/> facial expression recognition is an interesting and challenging problem and has several important applications in many areas such as human-computer interaction, human behavior understanding, and data-driven animation. Unlike face recognition, facial expression recognition needs to discriminate between the same expression in different individuals. The problem becomes more difficult as a person may show the same expression in different ways.</p><p class="calibre8">The current existing approaches for measuring facial expressions can be categorized into two types: static image and image sequence. In the static image approach, the system analyzes the facial expression in each image frame separately. In the image sequence approach, the system tries to capture the temporal pattern of the motion and changes seen on the face in the sequence of image frames. Recently, attention has been shifted toward the image sequence approach. However, this approach is more difficult and requires more computation <a id="id279" class="calibre1"/>than the static approach. In this chapter, we will follow the static image approach and compare several algorithms using the OpenCV 3 library.</p><p class="calibre8">The<a id="id280" class="calibre1"/> problem of automatic facial expression recognition includes three sub-problems:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Finding the face region in the image</strong>: The precise position of the face is very important for facial analysis. In this problem, we want to find the face region in the image. This problem can be viewed as a detection problem. In our implementation, we will use the cascade classifier in OpenCV's objdetect module to detect the faces. However, the cascade classifier is prone to alignment error. Therefore, we apply the flandmark library to extract the facial landmarks from the face region and use these landmarks to extract the precise face region.<div><h3 class="title2"><a id="note41" class="calibre1"/>Note</h3><p class="calibre8">Flandmark is an open source C library implementing a facial landmark detector. You can get more information about flandmark in the following sections. Basically, you can use whatever library you want to extract the landmarks. In our implementation, we will use this library to reduce complexity while integrating the library into our project.</p></div></li><li class="listitem"><strong class="calibre9">Extracting features from the face region</strong>: Given the face region, the system will extract facial expression information as a feature vector. The feature vector encodes the relevant information from the input data. In our implementation, the feature vector is obtained by using the combination of the feature detector from the feature2d module and the kmeans algorithm from the core module.</li><li class="listitem"><strong class="calibre9">Classifying the features into emotion categories</strong>: This is a classification problem. The system uses classification algorithms to map the extracted feature from the previous step to an emotion category (such as happy, neutral, or sad). This is the main subject of the chapter. We will evaluate machine learning algorithms from the ml module, including neural networks, the support vector machine, and K-Nearest-Neighbor.</li></ul></div><p class="calibre8">In the following sections, we will show you a complete process for implementing a facial expression system. In the next section, you will find several approaches to improve system performance to suit your needs.</p></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec18" class="calibre1"/>Facial expression dataset</h2></div></div></div><p class="calibre8">In order to simplify the chapter, we will use a dataset to demonstrate the process instead of a live <a id="id281" class="calibre1"/>camera. We will use a standard dataset, <strong class="calibre9">Japanese Female Facial Expression</strong> (<strong class="calibre9">JAFFE</strong>).There are 214 images of 10 people in the dataset. Each <a id="id282" class="calibre1"/>person has three images of each expression. The dataset includes seven expressions (happy, sad, angry, disgust, fear, surprise, and neutral) as shown in the following figure:</p><div><h3 class="title2"><a id="note42" class="calibre1"/>Note</h3><p class="calibre8">You <a id="id283" class="calibre1"/>need to download the dataset from the following link: <a class="calibre1" href="http://www.kasrl.org/jaffe.html">http://www.kasrl.org/jaffe.html</a></p></div><div><img src="img/00032.jpeg" alt="Facial expression dataset" class="calibre11"/><div><p class="calibre28">Sample image from the JAFFE dataset.</p></div></div><p class="calibre12"> </p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec24" class="calibre1"/>Finding the face region in the image</h1></div></div></div><p class="calibre8">In this <a id="id284" class="calibre1"/>section, we will show you a basic approach to detect faces in an image. We will use the cascade classifier in OpenCV to detect the face location. This approach may have alignment errors. In order to obtain a precise location, we will also provide another advanced approach to find the face region using facial landmarks. In our implementation, we will only use the face region. However, many researchers use facial landmarks to extract facial components, such as eyes and mouths, and operate on these components separately.</p><div><h3 class="title2"><a id="note43" class="calibre1"/>Note</h3><p class="calibre8">If you want to find out more, you should check the <em class="calibre10">Facial landmarks</em> section in this chapter.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec19" class="calibre1"/>Extracting the face region using a face detection algorithm</h2></div></div></div><p class="calibre8">In<a id="id285" class="calibre1"/> our implementation, we will <a id="id286" class="calibre1"/>use the Haar Feature-based cascade classifier in the objdetect module. In OpenCV, you can also extract the the face region with LBP-based cascade. LBP-based cascade is faster than Haar-based cascade. With the pre-trained model, the performance of LBP-based is lower than Haar-based cascade. However, it is possible to train an LBP-based cascade to attain the same performance as the Haar-based cascade.</p><div><h3 class="title2"><a id="note44" class="calibre1"/>Note</h3><p class="calibre8">If you want to understand object detection in detail, you should check <a class="calibre1" title="Chapter 5. Generic Object Detection for Industrial Applications" href="part0043_split_000.html#190862-940925703e144daa867f510896bffb69">Chapter 5</a>, <em class="calibre10">Generic Object Detection for Industrial Applications</em>.</p></div><p class="calibre8">The <a id="id287" class="calibre1"/>code for detecting faces is very<a id="id288" class="calibre1"/> simple. First, you need to load the pre-trained cascade classifier for faces into your OpenCV installation folder:</p><div><pre class="programlisting">CascadeClassifier face_cascade;
face_cascade.load("haarcascade_frontalface_default.xml");</pre></div><p class="calibre8">Then, load the input image in color mode, convert the image to grayscale, and apply histogram equalization to enhance the contrast:</p><div><pre class="programlisting">Mat img, img_gray;
img = imread(imgPath[i], CV_LOAD_IMAGE_COLOR);
cvtColor(img, img_gray, CV_RGB2GRAY);
equalizeHist(img_gray, img_gray);</pre></div><p class="calibre8">Now, we can find faces in the image. The <code class="email">detectMultiScale</code> function stores all the detected faces in the vector as Rect(x, y, w, h):</p><div><pre class="programlisting">vector&lt;Rect&gt; faces;
face_cascade.detectMultiScale( img_gray, faces, 1.1, 3 );</pre></div><p class="calibre8">In this code, the third parameter 1.1 is the scale factor, which specifies how much the image size will be resized at each scale. The following figure shows the scale pyramid using the scale factor. In our case, the scale factor is <code class="email">1.1</code>. This means that the image size is reduced by 10%. The lower this factor is, the better chance we have of finding the faces. The scaling process starts with the original image and ends when the image resolution reaches the model dimension in the X or Y direction. However, the computation cost is high if we have too many scales. Therefore, if you want to reduce the number of scales, increase the scale factor to <code class="email">1.2</code> (20%), <code class="email">1.3</code> (30%) ,or more. If you want to increase the number of scales, reduce the scale factor to <code class="email">1.05</code> (5%) or more. The fourth parameter <code class="email">3</code> is the minimum number of neighbors that each candidate position should have to become a face position.</p><div><img src="img/00033.jpeg" alt="Extracting the face region using a face detection algorithm" class="calibre11"/><div><p class="calibre28">Pyramid of image scales</p></div></div><p class="calibre12"> </p><p class="calibre8">The<a id="id289" class="calibre1"/> following <a id="id290" class="calibre1"/>figure is the result of face detection if we set the number of neighbors to zero:</p><div><img src="img/00034.jpeg" alt="Extracting the face region using a face detection algorithm" class="calibre11"/><div><p class="calibre28">All the candidates for face regions</p></div></div><p class="calibre12"> </p><p class="calibre8">Finally, the <a id="id291" class="calibre1"/>position of the<a id="id292" class="calibre1"/> face region can be obtained as follows:</p><div><pre class="programlisting">int bbox[4] = { faces[i].x, faces[i].y, faces[i].x + faces[i].width, faces[i].y + faces[i].height };</pre></div><p class="calibre8">Each element of the faces vector is a <code class="email">Rect</code> object. Therefore, we can get the position of the top-left corner with <code class="email">faces[i].x</code> and <code class="email">faces[i].y</code>. The position of the bottom-right corner is <code class="email">faces[i].x + faces[i].width</code> and <code class="email">faces[i].y + faces[i].height</code>. This information will be used as the initial position for the facial landmarks process, as described in the following section.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec20" class="calibre1"/>Extracting facial landmarks from the face region</h2></div></div></div><p class="calibre8">One <a id="id293" class="calibre1"/>disadvantage of the face detector is <a id="id294" class="calibre1"/>that the results may have misalignment. The misalignment may happen in scaling or translation. Therefore, the extracted face regions in all images will not align with each other. This misalignment can lead to poor recognition performance, especially with DENSE features. With the help of facial landmarks, we can align all the extracted faces so that each facial component is in the same area over the datasets.</p><p class="calibre8">Many researchers make use of facial landmarks for classification with other emotion recognition approaches.</p><p class="calibre8">We will use the flandmark library to find the location of the eyes, nose and mouth. Then, we will use these facial landmarks to extract the precise facial bounding box.</p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec01" class="calibre1"/>Introducing the flandmark library</h3></div></div></div><p class="calibre8">Flandmark is<a id="id295" class="calibre1"/> an open source C library implementing a facial landmark detector.</p><div><h3 class="title2"><a id="note45" class="calibre1"/>Note</h3><p class="calibre8">You<a id="id296" class="calibre1"/> can access the  flandmark library main page at: <a class="calibre1" href="http://cmp.felk.cvut.cz/~uricamic/flandmark/">http://cmp.felk.cvut.cz/~uricamic/flandmark/</a>.</p></div><p class="calibre8">Given a face image, the goal of the flandmark library is to estimate an S shape that represents the location of the facial component. A facial shape in an S is an array of (x, y) positions shown as: S = [x<sub class="calibre29">0</sub>y<sub class="calibre29">0</sub>x<sub class="calibre29">1</sub>y<sub class="calibre29">1</sub>....x<sub class="calibre29">n</sub>y<sub class="calibre29">n</sub>].</p><p class="calibre8">The pre-trained model in flandmark contains eight points, as shown in the following figure:</p><div><img src="img/00035.jpeg" alt="Introducing the flandmark library" class="calibre11"/><div><p class="calibre28">The 8 landmarks model and the corresponding index for each landmark.</p></div></div><p class="calibre12"> </p><p class="calibre8">In our implementation, we use flandmark because it is easy to integrate it into an OpenCV project. Besides, the flandmark library is really robust in many scenarios, even when the person is wearing glasses. In the following figure, we show the result of using the flandmark library<a id="id297" class="calibre1"/> on an image where the person is wearing dark glasses. The red dots indicate the facial landmarks.</p><div><img src="img/00036.jpeg" alt="Introducing the flandmark library" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">In the next section, we will show you the steps to download and use flandmark in our project.</p></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec02" class="calibre1"/>Downloading and compiling the flandmark library</h3></div></div></div><p class="calibre8">Flandmark is<a id="id298" class="calibre1"/> implemented in C and can be integrated into<a id="id299" class="calibre1"/> our project easily. However, we need to modify some headers in the library source to use it with OpenCV 3. The following are the steps to download and compile the library:</p><div><ol class="orderedlist"><li class="listitem" value="1">Go to <a id="id300" class="calibre1"/>the main page of the flandmark library and follow the GitHub link: <a class="calibre1" href="http://github.com/uricamic/flandmark">http://github.com/uricamic/flandmark</a></li><li class="listitem" value="2">Clone the library to your local machine with the following command:<div><pre class="programlisting">
<strong class="calibre9">git clone http://github.com/uricamic/flandmark</strong>
</pre></div></li><li class="listitem" value="3">Copy the <code class="email">libflandmark</code> folder to your project folder.</li><li class="listitem" value="4">Copy <code class="email">flandmark_model.dat,which is</code> in the data folder, to your project folder.</li><li class="listitem" value="5">Edit the <code class="email">liblbp.h</code> file in <code class="email">libflandmark</code> and change:<div><pre class="programlisting">#include "msvc-compat.h"</pre></div><p class="calibre26">to</p><div><pre class="programlisting">#include &lt;stdint.h&gt;</pre></div></li><li class="listitem" value="6">Edit<a id="id301" class="calibre1"/> the <code class="email">flandmark_detector.h</code> file<a id="id302" class="calibre1"/> in <code class="email">libflandmark</code> and change:<div><pre class="programlisting">#include "msvc-compat.h"
#include &lt;cv.h&gt; 
#include &lt;cvaux.h&gt;</pre></div><p class="calibre26">to</p><div><pre class="programlisting">#include &lt;stdint.h&gt;
#include "opencv2/opencv.hpp"
#include "opencv2/objdetect/objdetect.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp"
#include &lt;iostream&gt;
#include &lt;stdio.h&gt;
using namespace std;
using namespace cv;</pre></div></li><li class="listitem" value="7">Edit <code class="email">CMakeLists.txt</code> in your project folder to add the flandmark library:<div><pre class="programlisting">add_subdirectory(libflandmark)
include_directories("${PROJECT_SOURCE_DIR}/libflandmark")</pre></div></li><li class="listitem" value="8">Link the executable file with the flandmark static library.</li><li class="listitem" value="9">Add the flandmark header to your source code:<div><pre class="programlisting">#include "flandmark_detector.h"</pre></div></li></ol><div></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec03" class="calibre1"/>Detecting facial landmarks with flandmark</h3></div></div></div><p class="calibre8">Once<a id="id303" class="calibre1"/> you have finished the above steps, the process to extract facial components is very straightforward.</p><p class="calibre8">First, we create a <code class="email">FLANDMARK_Model</code> variable to load the pre-trained model:</p><div><pre class="programlisting">FLANDMARK_Model * model = flandmark_init("flandmark_model.dat");</pre></div><p class="calibre8">Then, we save the number of landmarks into the <code class="email">num_of_landmark</code> variable and create an array to store the output result:</p><div><pre class="programlisting">int num_of_landmark = model-&gt;data.options.M;
double *points = new double[2 * num_of_landmark];</pre></div><p class="calibre8">Finally, for each face region, we create an integer array to store the face location and use the <code class="email">flandmark_detect</code> function to obtain the final result in the <code class="email">points</code> array:</p><div><pre class="programlisting">int bbox[4] = { faces[i].x, faces[i].y, faces[i].x + faces[i].width, faces[i].y + faces[i].height };
flandmark_detect(new IplImage(img_gray), bbox, model, points);</pre></div><p class="calibre8">The <a id="id304" class="calibre1"/>first parameter in the <code class="email">flandmark_detect</code> function is <code class="email">IplImage</code> so we need to pass our gray image into the <code class="email">IplImage</code> constructor.</p></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec04" class="calibre1"/>Visualizing the landmarks in an image</h3></div></div></div><p class="calibre8">This<a id="id305" class="calibre1"/> step is optional. You don't need to implement the code in this section. However, we recommend that you try and understand the results. The following code draws a circle on the image at the location of the landmarks:</p><div><pre class="programlisting">for(int j = 0 ; j &lt; num_of_landmark; j++){
  Point landmark = Point((int)points[2 * j], (int)points[2* j + 1]);
  circle(img, landmark, 4, Scalar(255, 255, 255), -1);
}</pre></div><p class="calibre8">The following figure shows multiple examples of the results using the above code:</p><div><img src="img/00037.jpeg" alt="Visualizing the landmarks in an image" class="calibre11"/><div><p class="calibre28">Some examples of flandmark results on JAFFE images</p></div></div><p class="calibre12"> </p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec21" class="calibre1"/>Extracting the face region</h2></div></div></div><p class="calibre8">We now <a id="id306" class="calibre1"/>have the location of the eyes, nose, and mouth. It is very easy to extract the face region.</p><p class="calibre8">First, we compute the center of the left eye as the middle of point 2 and point 6:</p><div><pre class="programlisting">Point centerLeft = Point( (int) (points[2 * 6] + points[2 * 2]) / 2, (int) (points[2 * 6 + 1] + points[2 * 2 + 1]) / 2 );</pre></div><p class="calibre8">Second, the width of the eye region is the difference between x coordinates of point 2 and point 6:</p><div><pre class="programlisting">int widthLeft = abs(points[2 * 6] - points[2 * 2]);</pre></div><p class="calibre8">Then, we find the center and the width of the right eye:</p><div><pre class="programlisting">Point centerRight = Point( (int) (points[2 * 1] + points[2 * 5]) / 2, (int) (points[2 * 1 + 1] + points[2 * 5 + 1]) / 2 );
int widthRight = abs(points[2 * 1] - points[2 * 5]);</pre></div><p class="calibre8">We can<a id="id307" class="calibre1"/> assume that the width of the face is a bit larger than the distance between the eyes, and the height of the face is larger than the width of the face, so we can get the eyebrows. We can obtain a good face position with the following code:</p><div><pre class="programlisting">int widthFace = (centerLeft.x + widthLeft) - (centerRight.x - widthRight);
int heightFace = widthFace * 1.2;</pre></div><p class="calibre8">Finally, the face region can be extracted with the following code:</p><div><pre class="programlisting">Mat face = img(Rect( centerRight.x - widthFace/4  , centerRight.y - heightFace/4, widthFace, heightFace ));</pre></div><p class="calibre8">The following figure shows some extracted images from our implementation:</p><div><img src="img/00038.jpeg" alt="Extracting the face region" class="calibre11"/><div><p class="calibre28">Some examples of extracted face regions from JAFFE images</p></div></div><p class="calibre12"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec22" class="calibre1"/>Software usage guide</h2></div></div></div><p class="calibre8">We have <a id="id308" class="calibre1"/>implemented the software to extract facial components from the JAFFE dataset. You can use the code as follows:</p><div><ol class="orderedlist"><li class="listitem" value="1">Download the source code. Open the terminal and change directory to the source code folder.</li><li class="listitem" value="2">Build the software with <code class="email">cmake</code> using the following command:<div><pre class="programlisting">
<strong class="calibre9">mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make</strong>
</pre></div></li><li class="listitem" value="3">You can use the facial_components tool, as follows:<div><pre class="programlisting">
<strong class="calibre9">./facial_components -src &lt;input_folder&gt; -dest &lt;out_folder&gt;</strong>
</pre></div></li></ol><div></div><div><h3 class="title2"><a id="note46" class="calibre1"/>Note</h3><p class="calibre8">The <a id="id309" class="calibre1"/>software for this chapter based on OpenCV 3 can be found at: <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/</a></p></div><p class="calibre8">In order<a id="id310" class="calibre1"/> to simplify the process, we save the image paths in a <code class="email">.yaml</code> file, <code class="email">list.yml</code>. The structure of this <code class="email">.yaml</code> file is simple. First, we save the number of images in the <code class="email">num_of_image</code> variable. After that, we save the paths of all the images, as shown in the following screenshot:</p><div><img src="img/00039.jpeg" alt="Software usage guide" class="calibre11"/><div><p class="calibre28">An image of the list.yml file</p></div></div><p class="calibre12"> </p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec25" class="calibre1"/>Feature extraction</h1></div></div></div><p class="calibre8">Given a<a id="id311" class="calibre1"/> dataset of face regions, we can use feature extraction to obtain the feature vector, which gives us the most important information from the expression. The following figure shows the process that we use in our implementation to extract features vectors:</p><div><img src="img/00040.jpeg" alt="Feature extraction" class="calibre11"/><div><p class="calibre28">The feature extraction process</p></div></div><p class="calibre12"> </p><p class="calibre8">In order to<a id="id312" class="calibre1"/> understand this chapter, you need to understand that the feature representation of the expression image is the distribution of image features over k clusters (k = 1000 in our implementation). We have implemented a few common types of features that are supported in OpenCV, such as SIFT, SURF, and some advanced features, such as DENSE-SIFT, KAZE, DAISY. Since these image features are computed at image key points such as corners, except for DENSE cases, the number of image features can vary between images. However, we want to have a fixed feature size for every image to perform classification, since we will apply machine learning classification techniques later. It is important that the feature size of the images is the same so that we can compare them to obtain the final result. Therefore, we apply a clustering technique (kmeans in our case) to separate the image feature space into a k cluster. The final feature representation for each image is the histogram of the image features over k bins. Moreover, in order to reduce the dimension of the final feature, we apply principle component analysis as a last step.</p><p class="calibre8">In the <a id="id313" class="calibre1"/>following sections, we will explain the process step by step. At the end of this section, we will show you how to use our implementation to obtain the final feature representation of the dataset.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec23" class="calibre1"/>Extracting image features from facial component regions</h2></div></div></div><p class="calibre8">At this<a id="id314" class="calibre1"/> point, we will assume that you have the face region for each image in the dataset. The next step is to extract the image features from these face regions. OpenCV provides good implementations of many well-known key point detection and feature description algorithms.</p><div><h3 class="title2"><a id="note47" class="calibre1"/>Note</h3><p class="calibre8">Detailed explanations for each algorithm are out of the scope of this chapter.</p></div><p class="calibre8">In this section, we will show you how to use some of these algorithms in our implementation.</p><p class="calibre8">We will use a function that takes current regions, a feature type, and returns a matrix with image features as rows:</p><div><pre class="programlisting">Mat extractFeature(Mat face, string feature_name);</pre></div><p class="calibre8">In this <code class="email">extractFeature</code> function, we will extract image features from each Mat and return the descriptors. The implementation of <code class="email">extractFeature</code> is simple, and shown here:</p><div><pre class="programlisting">Mat extractFeature(Mat img, string feature_name){
    Mat descriptors;
    if(feature_name.compare("brisk") == 0){
        descriptors = extractBrisk(img);
    } else if(feature_name.compare("kaze") == 0){
        descriptors = extractKaze(img);
    } else if(feature_name.compare("sift") == 0){
        descriptors = extractSift(img);
    } else if(feature_name.compare("dense-sift") == 0){
        descriptors = extractDenseSift(img);
    } else if(feature_name.compare("daisy") == 0){
        descriptors = extractDaisy(img);
    }
    return descriptors;
}</pre></div><p class="calibre8">In the above code, we call the corresponding function for each feature. For simplicity, we only use one feature each time. In this chapter, we will discuss two types of features:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Contributed features</strong>: SIFT, DAISY, and DENSE SIFT. In OpenCV 3, the implementation <a id="id315" class="calibre1"/>of SIFT and SURF have been moved to the opencv_contrib module.<div><h3 class="title2"><a id="note48" class="calibre1"/>Note</h3><p class="calibre8">These features are patented and you should pay for them if you want to use them in commercial applications.</p></div><p class="calibre26">In this chapter, we will use SIFT features and the SIFT variant, DENSE SIFT.</p><div><h3 class="title2"><a id="note49" class="calibre1"/>Note</h3><p class="calibre8">If you want to use the opencv_contrib module, we suggest that you go to the <em class="calibre10">Further reading</em> section and take a look at the <em class="calibre10">Compiling the opencv_contrib module</em> section.</p></div></li><li class="listitem"><strong class="calibre9">Advanced features</strong>: BRISK and KAZE. These <a id="id316" class="calibre1"/>features are a good alternative<a id="id317" class="calibre1"/> to SIFT and SURF in both performance and computation time. DAISY and KAZE are only available in OpenCV 3. DAISY is in opencv_contrib. KAZE is in the main OpenCV repository.</li></ul></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec05" class="calibre1"/>Contributed features</h3></div></div></div><p class="calibre8">Let's take a<a id="id318" class="calibre1"/> look at SIFT features first.</p><p class="calibre8">In order to use SIFT features in OpenCV 3, you need to compile the opencv_contrib module with OpenCV.</p><div><h3 class="title2"><a id="note50" class="calibre1"/>Note</h3><p class="calibre8">We will assume that you have followed the instructions in the <em class="calibre10">Further reading</em> section.</p></div><p class="calibre8">The code to extract SIFT features is very simple:</p><div><pre class="programlisting">Mat extractSift(Mat img){
    Mat descriptors;
    vector&lt;KeyPoint&gt; keypoints;
    
    Ptr&lt;Feature2D&gt; sift = xfeatures2d::SIFT::create();
    sift-&gt;detect(img, keypoints, Mat());
    sift-&gt;compute(img, keypoints, descriptors);

    return descriptors;
}</pre></div><p class="calibre8">First, we create the <code class="email">Feature2D</code> variable with <code class="email">xfeatures2d::SIFT::create()</code> and use the <code class="email">detect</code> function to obtain key points. The first parameter for the detection function is the image<a id="id319" class="calibre1"/> that we want to process. The second parameter is a vector to store detected key points. The third parameter is a mask specifying where to look for key points. We want to find key points in every position of the images so we just pass an empty Mat here.</p><p class="calibre8">Finally, we use the <code class="email">compute</code> function to extract features descriptors at these key points. The computed descriptors are stored in the descriptors variable.</p><p class="calibre8">Next, let's take a look at the SURF features.</p><p class="calibre8">The code to obtain SURF features is more or less the same as that for SIFT features. We only change the namespace from SIFT to SURF:</p><div><pre class="programlisting">Mat extractSurf(Mat img){
    Mat descriptors;
    vector&lt;KeyPoint&gt; keypoints;
    
    Ptr&lt;Feature2D&gt; surf = xfeatures2d::SURF::create();
    surf-&gt;detect(img, keypoints, Mat());
    surf-&gt;compute(img, keypoints, descriptors);

    return descriptors;
}</pre></div><p class="calibre8">Let's now move on to DAISY.</p><p class="calibre8">DAISY is an improved version of the rotation-invariant BRISK descriptor and the LATCH binary descriptor that is comparable to the heavier and slower SURF. DAISY is only available in OpenCV 3 in the opencv_contrib module. The code to implement DAISY features is fairly similar to the Sift function. However, the DAISY class doesn't have a <code class="email">detect</code> function so we will use SURF to detect key points and use DAISY to extract descriptors:</p><div><pre class="programlisting">Mat extractDaisy(Mat img){
    Mat descriptors;
    vector&lt;KeyPoint&gt; keypoints;

    Ptr&lt;FeatureDetector&gt; surf = xfeatures2d::SURF::create();
    surf-&gt;detect(img, keypoints, Mat());
    Ptr&lt;DescriptorExtractor&gt; daisy = xfeatures2d::DAISY::create();
    daisy-&gt;compute(img, keypoints, descriptors);

    return descriptors;
}</pre></div><p class="calibre8">It is now time to take a look at dense SIFT features.</p><p class="calibre8">Dense collects features at every location and scale in an image. There are plenty of applications where dense features are used. However, in OpenCV 3, the interface for extracting<a id="id320" class="calibre1"/> dense features has been removed. In this section, we show a simple approach to extracting dense features using the function in the OpenCV 2.4 source code to extract the vector of key points.</p><p class="calibre8">The function to extract dense Sift is similar to the Sift function:</p><div><pre class="programlisting">Mat extractDenseSift(Mat img){
    Mat descriptors;
    vector&lt;KeyPoint&gt; keypoints;

    Ptr&lt;Feature2D&gt; sift = xfeatures2d::SIFT::create();
<strong class="calibre9">    createDenseKeyPoints(keypoints, img);</strong>
    sift-&gt;compute(img, keypoints, descriptors);

    return descriptors;
}</pre></div><p class="calibre8">Instead of using the <code class="email">detect</code> function, we can use the <code class="email">createDenseKeyPoints</code> function to obtain key points. After that, we pass this dense key points vector to compute the function. The code for <code class="email">createDenseKeyPoints</code> is obtained from the OpenCV 2.4 source code. You can find this code at <code class="email">modules/features2d/src/detectors.cpp</code> in the OpenCV 2.4 repository:</p><div><pre class="programlisting">void createDenseFeature(vector&lt;KeyPoint&gt; &amp;keypoints, Mat image, float initFeatureScale=1.f, int featureScaleLevels=1,
                                    float featureScaleMul=0.1f,
                                    int initXyStep=6, int initImgBound=0,
                                    bool varyXyStepWithScale=true,
                                    bool varyImgBoundWithScale=false){
    float curScale = static_cast&lt;float&gt;(initFeatureScale);
    int curStep = initXyStep;
    int curBound = initImgBound;
    for( int curLevel = 0; curLevel &lt; featureScaleLevels; curLevel++ )
    {
        for( int x = curBound; x &lt; image.cols - curBound; x += curStep )
        {
            for( int y = curBound; y &lt; image.rows - curBound; y += curStep )
            {
                keypoints.push_back( KeyPoint(static_cast&lt;float&gt;(x), static_cast&lt;float&gt;(y), curScale) );
            }
        }
        curScale = static_cast&lt;float&gt;(curScale * featureScaleMul);
        if( varyXyStepWithScale ) curStep = static_cast&lt;int&gt;( curStep * featureScaleMul + 0.5f );
        if( varyImgBoundWithScale ) curBound = static_cast&lt;int&gt;( curBound * featureScaleMul + 0.5f );
    }
}</pre></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec06" class="calibre1"/>Advanced features</h3></div></div></div><p class="calibre8">OpenCV 3 <a id="id321" class="calibre1"/>comes bundled with many new and advanced features. In our implementation, we will only use the BRISK and KAZE features. However, there are many other features in OpenCV.</p><p class="calibre8">Let us familiarize ourselves with the BRISK features.</p><p class="calibre8">BRISK is a new feature and a good alternative to SURF. It has been added to OpenCV since the 2.4.2 version. BRISK is under a BSD license so you don't have to worry about the patent problem, as with SIFT or SURF.</p><div><pre class="programlisting">Mat extractBrisk(Mat img){
    Mat descriptors;
    vector&lt;KeyPoint&gt; keypoints;

    Ptr&lt;DescriptorExtractor&gt; brisk = BRISK::create();
    brisk-&gt;detect(img, keypoints, Mat());
    brisk-&gt;compute(img, keypoints, descriptors);

    return descriptors;
}</pre></div><div><h3 class="title2"><a id="note51" class="calibre1"/>Note</h3><p class="calibre8">There is <a id="id322" class="calibre1"/>an interesting article about all this, <em class="calibre10">A battle of three descriptors: SURF, FREAK and BRISK,</em>  available at <a class="calibre1" href="http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/">http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/</a>.</p></div><p class="calibre8">Let's now move on and have a look at the KAZE features.</p><p class="calibre8">KAZE is a new feature in OpenCV 3. It produces the best results in many scenarios, especially with image matching problems, and it is comparable to SIFT. KAZE is in the OpenCV repository so you don't need opencv_contrib to use it. Apart from the high performance, one reason to use KAZE is that it is open source and you can use it freely in any commercial <a id="id323" class="calibre1"/>applications. The code to use this feature is very straightforward:</p><div><pre class="programlisting">Mat extractKaze(Mat img){
    Mat descriptors;
    vector&lt;KeyPoint&gt; keypoints;

    Ptr&lt;DescriptorExtractor&gt; kaze = KAZE::create();
    kaze-&gt;detect(img, keypoints, Mat());
    kaze-&gt;compute(img, keypoints, descriptors);

    return descriptors;
}</pre></div><div><h3 class="title2"><a id="note52" class="calibre1"/>Note</h3><p class="calibre8">The <a id="id324" class="calibre1"/>image matching comparison between KAZE, SIFT, and SURF is available at the author repository: <a class="calibre1" href="https://github.com/pablofdezalc/kaze">https://github.com/pablofdezalc/kaze</a></p></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec07" class="calibre1"/>Visualizing key points for each feature type</h3></div></div></div><p class="calibre8">In the <a id="id325" class="calibre1"/>following figure, we visualize the position of key points for each feature type. We draw a circle at each key point; the radius of the circle specifies the scale of the image where the key point is extracted. You can see that the key points and the corresponding descriptors differ between these features. Therefore, the performance of the system will vary, based on the quality of the feature.</p><div><h3 class="title2"><a id="note53" class="calibre1"/>Note</h3><p class="calibre8">We recommend that you refer to the <em class="calibre10">Evaluation</em> section for more details.</p></div><div><img src="img/00041.jpeg" alt="Visualizing key points for each feature type" class="calibre11"/><div><p class="calibre28">The feature extraction process</p></div></div><p class="calibre12"> </p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec24" class="calibre1"/>Computing the distribution of feature representation over k clusters</h2></div></div></div><p class="calibre8">If you <a id="id326" class="calibre1"/>have followed the previous pseudo-code, you should now have a vector of descriptors. You can see that the size of descriptors varies between images. Since we want a fixed size of feature representation for each image, we will compute the distribution of feature representation over k clusters. In our implementation, we will use the kmeans clustering algorithm in the core module.</p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec08" class="calibre1"/>Clustering image features space into k clusters</h3></div></div></div><p class="calibre8">First, we <a id="id327" class="calibre1"/>assume that the descriptors<a id="id328" class="calibre1"/> of all the images are added to a vector, called <code class="email">features_vector</code>. Then, we need to create a Mat <code class="email">rawFeatureData</code> that will contain all of the image features as a row. In this case, <code class="email">num_of_feature</code> is the total number of features in every image and <code class="email">image_feature_size</code> is the size of each image feature. We choose the number of clusters based on experiment. We start with 100 and increase the number for a few iterations. It depends on the type of features and data, so you should try to change this variable to suit your situation. One downside of a large number of clusters is that the cost for computation with kmeans will be high. Moreover, if the number of clusters is too large, the feature vector will be too sparse and it may not be good for classification.</p><div><pre class="programlisting">Mat rawFeatureData = Mat::zeros(num_of_feature, image_feature_size, CV_32FC1);</pre></div><p class="calibre8">We need to copy the data from the vector of descriptors (<code class="email">features_vector</code> in the code) to <code class="email">imageFeatureData</code>:</p><div><pre class="programlisting">int cur_idx = 0;
for(int i = 0 ; i &lt; features_vector.size(); i++){
    features_vector[i].copyTo(rawFeatureData.rowRange(cur_idx, cur_idx + features_vector[i].rows));
    cur_idx += features_vector[i].rows;
}</pre></div><p class="calibre8">Finally, we use the <code class="email">kmeans</code> function to perform clustering on the data, as follows:</p><div><pre class="programlisting">Mat labels, centers;
kmeans(rawFeatureData, k, labels, TermCriteria( TermCriteria::EPS+TermCriteria::COUNT, 100, 1.0), 3, KMEANS_PP_CENTERS, centers);</pre></div><p class="calibre8">Let's discuss the parameters of the <code class="email">kmeans</code> function:</p><div><pre class="programlisting">double kmeans(InputArray data, int K, InputOutputArray bestLabels, TermCriteria criteria, int attempts, int flags, OutputArray centers=noArray())</pre></div><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">InputArray data</strong>: It contains all the samples as a row.</li><li class="listitem"><strong class="calibre9">int K</strong>: The number of clusters to split the samples ( k = 1000 in our implementation).</li><li class="listitem"><strong class="calibre9">InputOutputArray bestLabels</strong>: Integer array that contains the cluster indices<a id="id329" class="calibre1"/> for each sample.</li><li class="listitem"><strong class="calibre9">TermCriteria criteria</strong>: The algorithm<a id="id330" class="calibre1"/> termination criteria. This contains three parameters (<code class="email">type</code>, <code class="email">maxCount</code>, <code class="email">epsilon</code>).</li><li class="listitem"><strong class="calibre9">Type</strong>: Type of termination criteria. There are three types:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre9">COUNT</strong>: Stop the algorithm after a number of iterations (<code class="email">maxCount</code>).</li><li class="listitem"><strong class="calibre9">EPS</strong>: Stop the algorithm if the specified accuracy (epsilon) is reached.</li><li class="listitem"><strong class="calibre9">EPS+COUNT</strong>: Stop the algorithm if the COUNT and EPS conditions are fulfilled.</li></ul></div></li><li class="listitem"><strong class="calibre9">maxCount</strong>: It is the maximum number of iterations.</li><li class="listitem"><strong class="calibre9">epsilon</strong>: It is the required accuracy needed to stop the algorithm.</li><li class="listitem"><strong class="calibre9">int attemtps</strong>: It is the number of times the algorithm is executed with different initial centroids. The algorithm returns the labels that have the best compactness.</li><li class="listitem"><strong class="calibre9">int flags</strong>: This flag specifies how initial centroids are random. There are three types of flags. Normally, <code class="email">KMEANS_RANDOM_CENTERS</code> and <code class="email">KMEANS_PP_CENTERS</code> are used. If you want to provide your own initial labels, you should use <code class="email">KMEANS_USE_INITIAL_LABELS</code>. In this case, the algorithm will use your initial labels on the first attempt. For further attempts, <code class="email">KMEANS_*_CENTERS</code> flags are applied.</li><li class="listitem"><strong class="calibre9">OutputArray centers</strong>: It contains all cluster centroids, one row per each centroid.</li><li class="listitem"><strong class="calibre9">double compactness</strong>: It is the returned value of the function. This is the sum of the squared distance between each sample to the corresponding centroid.</li></ul></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec09" class="calibre1"/>Computing a final feature for each image</h3></div></div></div><p class="calibre8">We <a id="id331" class="calibre1"/>now have labels for every image feature in the dataset. The next step is to compute a fixed size feature for each image. With this in mind, we iterate through each image and create a feature vector of k elements, where k is the number of clusters.</p><p class="calibre8">Then, we iterate through the image features in the current image and increase the ith element of the feature vector where i is the label of the image features.</p><p class="calibre8">Imagine that we are trying to make a histogram representation of the features based on the k centroids. This method looks like a bag of words approach. For example, image X has 100<a id="id332" class="calibre1"/> features and image Y has 10 features. We cannot compare them because they do not have the same size. However, if we make a histogram of 1,000 dimensions for each of them, they are then the same size and we can compare them easily.</p></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec10" class="calibre1"/>Dimensionality reduction</h3></div></div></div><p class="calibre8">In this <a id="id333" class="calibre1"/>section, we will use <strong class="calibre9">Principle Component Analysis</strong> (<strong class="calibre9">PCA</strong>) to reduce the dimension of the feature space. In the previous <a id="id334" class="calibre1"/>step, we have 1,000 dimensional feature vectors for each image. In our dataset, we only have 213 samples. Hence, the further classifiers tend to overfit the training data in high dimensional space. Therefore, we want to use PCA to obtain the most important dimension, which has the largest variance.</p><p class="calibre8">Next, we will show you how to use PCA in our system.</p><p class="calibre8">First, we assume that you can store all the features in a Mat named <code class="email">featureDataOverBins</code>. The number of rows of this Mat should equal to the number of images in the dataset and the number of columns of this Mat should be 1,000. Each row in <code class="email">featureDataOverBins</code> is a feature of an image.</p><p class="calibre8">Second, we create a PCA variable:</p><div><pre class="programlisting">PCA pca(featureDataOverBins, cv::Mat(), CV_PCA_DATA_AS_ROW, 0.90);</pre></div><p class="calibre8">The first parameter is the data that contains all the features. We don't have a pre-computed mean vector so the second parameter should be an empty Mat. The third parameter indicates that the feature vectors are stored as matrix rows. The final parameter specifies the percentage of variance that PCA should retain.</p><p class="calibre8">Finally, we need to project all the features from 1,000 dimensional feature spaces to a lower space. After the projection, we can save these features for further processes.</p><div><pre class="programlisting">for(int i = 0 ; i &lt; num_of_image; i++){
    Mat feature = pca.project(featureDataOverBins.row(i));
    // save the feature in FileStorage
}</pre></div><p class="calibre8">The number of dimensions of the new features can be obtained by:</p><div><pre class="programlisting">int feature_size = pca.eigenvectors.rows;</pre></div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec25" class="calibre1"/>Software usage guide</h2></div></div></div><p class="calibre8">We have<a id="id335" class="calibre1"/> implemented the previous process to extract the fixed size feature for the dataset. Using the software is quite easy:</p><div><ol class="orderedlist"><li class="listitem" value="1">Download the source code. Open the terminal and change directory to the source code folder.</li><li class="listitem" value="2">Build the software with <code class="email">cmake</code> using the following command:<div><pre class="programlisting">
<strong class="calibre9">mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make</strong>
</pre></div></li><li class="listitem" value="3">You can use the <code class="email">feature_extraction</code> tool as follows:<div><pre class="programlisting">
<strong class="calibre9">./feature_extraction  -feature &lt;feature_name&gt; -src &lt;input_folder&gt; -dest &lt;output_folder&gt;</strong>
</pre></div></li></ol><div></div><p class="calibre8">The <code class="email">feature_extraction</code> tool creates a YAML file in the output folder which contains the features and labels of every image in the dataset. The available parameters are:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">feature_name</code>: This can be sift, surf, opponent-sift, or opponent-surf. This is the name of the feature type which is used in the feature extraction process.</li><li class="listitem"><code class="email">input_folder</code>: This has the absolute path to the location of facial components.</li><li class="listitem"><code class="email">output_folder</code>: This has the absolute path to the folder where you want to keep the output file.</li></ul></div><p class="calibre8">The structure of the output file is fairly simple.</p><p class="calibre8">We store the size of the feature, cluster centers, the number of images, the number of train and test images, the number of labels, and the corresponding label names. We also store PCA means, eigenvectors, and eigenvalues. The following figure shows a part of the YAML file:</p><div><img src="img/00042.jpeg" alt="Software usage guide" class="calibre11"/><div><p class="calibre28">A part of the features.yml file</p></div></div><p class="calibre12"> </p><p class="calibre8">For each<a id="id336" class="calibre1"/> image, we store three variables, as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">image_feature_&lt;idx&gt;</code>: It is a Mat that contains features of image idx</li><li class="listitem"><code class="email">image_label_&lt;idx&gt;</code>: It is a label of the image idx</li><li class="listitem"><code class="email">image_is_train_&lt;idx&gt;</code>: It is a Boolean specifying whether the image is used for training or not.</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec26" class="calibre1"/>Classification</h1></div></div></div><p class="calibre8">Once <a id="id337" class="calibre1"/>you have extracted the features for all the samples in the dataset, it is time to start the classification process. The target of this classification process is to learn how to make accurate predictions automatically based on the training examples. There are many approaches to this problem. In this section, we will talk about machine learning algorithms in OpenCV, including neural networks, support vector machines, and k-nearest neighbors.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec26" class="calibre1"/>Classification process</h2></div></div></div><p class="calibre8">Classification is<a id="id338" class="calibre1"/> considered supervised learning. In a classification problem, a correctly labelled training set is necessary. A model is produced during the training stage which makes predictions and is corrected when predictions are wrong. Then, the model is used for predicting in other applications. The model needs to be trained every time you have more training data. The following figure shows an overview of the classification process:</p><div><img src="img/00043.jpeg" alt="Classification process" class="calibre11"/><div><p class="calibre28">Overview of the classification process</p></div></div><p class="calibre12"> </p><p class="calibre8">The choice of learning algorithm to use is a critical step. There are a lot of solutions to the classification problem. In this section, we list some of the popular machine learning algorithms in OpenCV. The performance of each algorithm can vary between classification problems. You should make some evaluations and select the one that is the most appropriate for your problem to get the best results. It is essential as feature selection may affect the performance of the learning algorithm. Therefore, we also need to evaluate each learning algorithm with each different feature selection.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec27" class="calibre1"/>Splitting the dataset into a training set and testing set</h2></div></div></div><p class="calibre8">It is <a id="id339" class="calibre1"/>important that the dataset is separated into two parts, the training set and the testing set. We will use the training set for the learning stage and the testing set for the testing stage. In the testing stage, we want to test how the trained model predicts unseen samples. In other words, we want to test the <em class="calibre10">generalization capability</em> of the trained model. Therefore, it is important that the test samples are different from the trained samples. In our implementation, we will simply split the dataset into two parts. However, it is better if you use k-fold cross validation as mentioned in the <em class="calibre10">Further reading</em> section.</p><p class="calibre8">There is<a id="id340" class="calibre1"/> no accurate way to split the dataset into two parts. Common ratios are 80:20 and 70:30. Both the training set and the testing set should be selected randomly. If they have the same data, the evaluation is misleading. Basically, even if you achieve 99 percent accuracy on your testing set, the model can't work in the real world, where the data is different from the training data.</p><p class="calibre8">In our implementation of feature extraction, we have already randomly split the dataset and saved the selection in the YAML file.</p><div><h3 class="title2"><a id="note54" class="calibre1"/>Note</h3><p class="calibre8">The k-fold cross validation is explained in more detail at the end of the <em class="calibre10">Further reading</em> section.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec28" class="calibre1"/>Support vector machines</h2></div></div></div><p class="calibre8"><strong class="calibre9">A Support Vector Machine</strong> (<strong class="calibre9">SVM</strong>) is a supervised learning technique applicable to both classification<a id="id341" class="calibre1"/> and regression. Given labelled training data, the goal of SVM is to produce an optimal hyper plane which predicts the target value of a test sample with only test sample attributes. In other words, SVM generates a function to map between input and output based on labelled training data.</p><p class="calibre8">For example, let's assume that we want to find a line to separate two sets of 2D points. The following figure shows that there are several solutions to the problem:</p><div><img src="img/00044.jpeg" alt="Support vector machines" class="calibre11"/><div><p class="calibre28">A lot of hyper planes can solve a problem</p></div></div><p class="calibre12"> </p><p class="calibre8">The goal of SVM is to find a hyper plane that maximizes the distances to the training samples. The<a id="id342" class="calibre1"/> distances are calculated to only support those vectors that are closest to the hyper plane. The following figure shows an optimal hyper plane to separate two sets of 2D points:</p><div><img src="img/00045.jpeg" alt="Support vector machines" class="calibre11"/><div><p class="calibre28">An optimal hyper plane that maximizes the distances to the training samples. R is the maximal margin</p></div></div><p class="calibre12"> </p><p class="calibre8">In the following sections, we will show you how to use SVM to train and test facial expression data.</p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec11" class="calibre1"/>Training stage</h3></div></div></div><p class="calibre8">One <a id="id343" class="calibre1"/>of the most difficult parts about training an SVM is parameters selection. It is not possible to explain everything without some deep understanding of how SVM works. Luckily, OpenCV implements a <code class="email">trainAuto</code> method for automatic parameter estimation. If you have enough knowledge of SVM, you should try to use your own parameters. In this section, we will introduce the <code class="email">trainAuto</code> method to give you an overview of SVM.</p><p class="calibre8">SVM is inherently a technique for building an optimal hyper plane in binary (2-class) classification. In our facial expression problem, we want to classify seven expressions. One-versus-all and one-versus-one are two common approaches that we can follow to use SVM in this problem. One-versus-all trains one SVM for each class. There are seven SVMs in our case. For class i, every sample with the label i is considered as positive and the rest of the samples are negative. This approach is prone to error when the dataset samples are<a id="id344" class="calibre1"/> imbalanced between classes. The one-versus-one approach trains an SVM for each different pairs of classes. The number of SVMs in total is <em class="calibre10">N*(N-1)/2</em> SVMs. This means 21 SVMs in our case.</p><p class="calibre8">In OpenCV, you don't have to follow these approaches. OpenCV supports the training of one multiclass SVM. However, you should follow the above methods for better results. We will still use one multiclass SVM. The training and testing process will be simpler.</p><p class="calibre8">Next, we will demonstrate our implementation to solve the facial expression problem.</p><p class="calibre8">First, we create an instance of SVM:</p><div><pre class="programlisting">Ptr&lt;ml::SVM&gt; svm = ml::SVM::create();</pre></div><p class="calibre8">If you want to change parameters, you can call the <code class="email">set</code> function in the <code class="email">svm</code> variable, as shown:</p><div><pre class="programlisting">svm-&gt;setType(SVM::C_SVC);
svm-&gt;setKernel(SVM::RBF);</pre></div><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Type</strong>: It is the type of SVM formulation. There are five possible values: <code class="email">C_SVC</code>, <code class="email">NU_SVC</code>, <code class="email">ONE_CLASS</code>, <code class="email">EPS_SVR</code>, and <code class="email">NU_SVR</code>. However, in our multiclass classification, only <code class="email">C_SVC</code> and <code class="email">NU_SVC</code> are suitable. The difference between these two lies in the mathematical optimization problem. For now, we can just use <code class="email">C_SVC</code>.</li><li class="listitem"><strong class="calibre9">Kernel</strong>: It is the type of SVM kernel. There are four possible values: <code class="email">LINEAR</code>, <code class="email">POLY</code>, <code class="email">RBF</code>, and <code class="email">SIGMOID</code>. The kernel is a function to map the training data to a higher dimensional space that makes data linearly separable. This is also known as <em class="calibre10">Kernel Trick</em>. Therefore, we can use SVM in non-linear cases with the support of the kernel. In our case, we choose the most commonly-used kernel, RBF. You can switch between these kernels and choose the best.</li></ul></div><p class="calibre8">You can also set other parameters such as TermCriteria, Degree, Gamma. We are just using the default parameters.</p><p class="calibre8">Second, we create a variable of <code class="email">ml::TrainData</code> to store all the training set data:</p><div><pre class="programlisting">Ptr&lt;ml::TrainData&gt; trainData = ml::TrainData::create(train_features, ml::SampleTypes::ROW_SAMPLE, labels);</pre></div><div><ul class="itemizedlist"><li class="listitem"><code class="email">train_features</code>: It is a Mat that contains each features vector as a row. The number of rows of <code class="email">train_features</code> is the number of training samples, and the number of columns is the size of one features vector.</li><li class="listitem"><code class="email">SampleTypes::ROW_SAMPLE</code>: It specifies that each features vector is in a row. If your features vectors are in columns, you should use COL_SAMPLE.</li><li class="listitem"><code class="email">train_labels</code>: It is a Mat that contains labels for each training feature. In SVM, <code class="email">train_labels</code> will be a Nx1 matrix, N is the number of training samples. The value of each row is the truth label of the corresponding sample. At <a id="id345" class="calibre1"/>the time of writing, the type of <code class="email">train_labels</code> should be <code class="email">CV_32S</code>. Otherwise, you may encounter an error. The following code is what we use to create the <code class="email">train_labels</code> variable:<div><pre class="programlisting">Mat train_labels = Mat::zeros( labels.rows, 1, CV_32S);
for(int i = 0 ; i &lt; labels.rows; i ++){
    train_labels.at&lt;unsigned int&gt;(i, 0) = labels.at&lt;int&gt;(i, 0);
}</pre></div></li></ul></div><p class="calibre8">Finally, we pass <code class="email">trainData</code> to the<code class="email"> trainAuto</code> function so that OpenCV can select the best parameters automatically. The interface of the <code class="email">trainAuto</code> function contains many other parameters. In order to keep things simple, we will use the default parameters:</p><div><pre class="programlisting">svm-&gt;trainAuto(trainData);</pre></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec12" class="calibre1"/>Testing stage</h3></div></div></div><p class="calibre8">After <a id="id346" class="calibre1"/>we've trained the SVM, we can pass a test sample to the predict function of the <code class="email">svm</code> model and receive a label prediction, as follows:</p><div><pre class="programlisting">float predict = svm-&gt;predict(sample);</pre></div><p class="calibre8">In this case, the sample is a feature vector just like the feature vector in the training features. The response is the label of the sample.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec29" class="calibre1"/>Multi-layer perceptron</h2></div></div></div><p class="calibre8">OpenCV <a id="id347" class="calibre1"/>implements the most common type of artificial neural network, the multi-layer perceptron (MLP). A typical MLP consists of an input layer, an output layer, and one or more hidden layers. It is known as a supervised learning method because it needs a desired output to train. With enough data, MLP, given enough hidden layers, can approximate any function to any desired accuracy.</p><p class="calibre8">An MLP with a single hidden layer can be represented as it is in the following figure:</p><div><img src="img/00046.jpeg" alt="Multi-layer perceptron" class="calibre11"/><div><p class="calibre28">A single hidden layer perceptron</p></div></div><p class="calibre12"> </p><p class="calibre8">A detailed <a id="id348" class="calibre1"/>explanation and proof of how the MLP learns are out of the scope of this chapter. The idea is that the output of each neuron is a function of neurons from previous layers.</p><p class="calibre8">In the above single hidden layer MLP, we use the following notation:</p><p class="calibre8">Input layer: x<sub class="calibre29">1</sub> x<sub class="calibre29">2</sub></p><p class="calibre8">Hidden layer: h<sub class="calibre29">1</sub> h<sub class="calibre29">2</sub> h<sub class="calibre29">3</sub></p><p class="calibre8">Output layer: y</p><p class="calibre8">Each connection between each neuron has a weight. The weight shown in the above figure is between neuron i (that is i = 3) in the current layer and neuron j (that is j = 2) in the previous layer is w<sub class="calibre29">ij</sub>. Each neuron has a bias value 1 with a weight, w<sub class="calibre29">i,bias</sub>.</p><p class="calibre8">The output at neuron i is the result of an activation function <em class="calibre10">f</em>:</p><div><img src="img/00047.jpeg" alt="Multi-layer perceptron" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">There are<a id="id349" class="calibre1"/> many types of activation functions. In OpenCV, there are three types of activation functions: Identity, Sigmoid, and Gaussian. However, the Gaussian function is not completely supported at the time of writing and the Identity function is not commonly used. We recommend that you use the default activation, Sigmoid.</p><p class="calibre8">In the following sections, we will show you how to train and test a multi-layer perceptron.</p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec13" class="calibre1"/>Training stage</h3></div></div></div><p class="calibre8">In<a id="id350" class="calibre1"/> the training stage, we first define the network and then train the network.</p><div><div><div><div><h4 class="title3"><a id="ch03lvl4sec01" class="calibre1"/>Define the network</h4></div></div></div><p class="calibre8">We will<a id="id351" class="calibre1"/> use a simple four layer neural network in our facial expression problem. The network has one input layer, two hidden layers, and one output layer.</p><p class="calibre8">First, we need to create a matrix to hold the layers definition. This matrix has four rows and one column:</p><div><pre class="programlisting">Mat layers = Mat(3, 1, CV_32S);</pre></div><p class="calibre8">Then, we assign the number of neurons for each layer, as follows:</p><div><pre class="programlisting">layers.row(0) = Scalar(feature_size);
layers.row(1) = Scalar(20);
layers.row(2) = Scalar(num_of_labels);</pre></div><p class="calibre8">In this network, the number of neurons for the input layer has to be equal to the number of elements of each feature vector, and number of neurons for the output layer is the number of facial expression labels (<code class="email">feature_size</code> equals <code class="email">train_features.cols</code> where <code class="email">train_features</code> is the Mat that contains all features and <code class="email">num_of_labels</code> equals 7 in our implementation).</p><p class="calibre8">The above parameters in our implementation are not optimal. You can try different values for different numbers of hidden layers and numbers of neurons. Remember that the number of hidden neurons should not be larger than the number of training samples. It is very difficult to choose the number of neurons in a hidden layer and the number of layers in your network. If you do some research, you can find several rules of thumb and diagnostic techniques. The best way to choose these parameters is experimentation. Basically, the more layers and hidden neurons there are, the more capacity you have in the network. However, more capacity may lead to overfitting. One of the most important rules is that the number of examples in the training set should be larger than the number of weights in <a id="id352" class="calibre1"/>the network. Based on our experience, you should start with one hidden layer with a small number of neurons and calculate the generalization error and training error. Then, you should modify the number of neurons and repeat the process.</p><div><h3 class="title2"><a id="note55" class="calibre1"/>Note</h3><p class="calibre8">Remember to make a graph to visualize the error when you change parameters. Keep in mind that the number of neurons is usually between the input layer size and the output layer size. After a few iterations, you can decide whether to add an additional layer or not.</p></div><p class="calibre8">However, in this case, we don't have much data. This makes the network hard to train. We may not add neurons and layers to improve the performance.</p></div><div><div><div><div><h4 class="title3"><a id="ch03lvl4sec02" class="calibre1"/>Train the network</h4></div></div></div><p class="calibre8">First, we<a id="id353" class="calibre1"/> create a network variable, ANN_MLP, and add the layers definition to the network:</p><div><pre class="programlisting">Ptr&lt;ml::ANN_MLP&gt; mlp = ml::ANN_MLP::create();
mlp-&gt;setLayerSizes(layers);</pre></div><p class="calibre8">Then, we need to prepare some parameters for training algorithms. There are two algorithms for training MLP: the back-propagation algorithm and the RPROP algorithm. RPROP is the default algorithm for training. There are many parameters for RPROP so we will use the back-propagation algorithm for simplicity.</p><p class="calibre8">Below is our code for setting parameters for the back-propagation algorithm:</p><div><pre class="programlisting">mlp-&gt;setTrainMethod(ml::ANN_MLP::BACKPROP);
mlp-&gt;setActivationFunction(ml::ANN_MLP::SIGMOID_SYM, 0, 0);
mlp-&gt;setTermCriteria(TermCriteria(TermCriteria::EPS+TermCriteria::COUNT, 100000, 0.00001f));</pre></div><p class="calibre8">We set the <code class="email">TrainMethod</code> to <code class="email">BACKPROP</code> to use the back-propagation algorithm. Select Sigmoid as the activation function There are three types of activation in OpenCV: <code class="email">IDENTITY</code>, <code class="email">GAUSSIAN</code>, and <code class="email">SIGMOID</code>. You can go to the overview of this section for more details.</p><p class="calibre8">The final parameter is <code class="email">TermCriteria</code>. This is the algorithm termination criteria. You can see an explanation of this parameter in the kmeans algorithm in the previous section.</p><p class="calibre8">Next, we create a <code class="email">TrainData</code> variable to store all the training sets. The interface is the same as in the SVM section.</p><div><pre class="programlisting">Ptr&lt;ml::TrainData&gt; trainData = ml::TrainData::create(train_features, ml::SampleTypes::ROW_SAMPLE, train_labels);</pre></div><p class="calibre8"><code class="email">train_features</code> is the Mat which stores all training samples as in the SVM section. However, <code class="email">train_labels</code> is different:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">train_features</code>: This is a Mat that contains each features vector as a row as we<a id="id354" class="calibre1"/> did in the SVM. The number of rows of <code class="email">train_features</code> is the number of training samples and the number of columns is the size of one features vector.</li><li class="listitem"><code class="email">train_labels</code>: This is a Mat that contains labels for each training feature. Instead of the Nx1 matrix in SVM, <code class="email">train_labels</code> in MLP should be a NxM matrix, N is the number of training samples and M is the number of labels. If the feature at row i is classified as label j, the position (i, j) of <code class="email">train_labels</code> will be 1. Otherwise, the value will be zero. The code to create the <code class="email">train_labels</code> variable is as follows:<div><pre class="programlisting">Mat train_labels = Mat::zeros( labels.rows, num_of_label, CV_32FC1);
for(int i = 0 ; i &lt; labels.rows; i ++){
    int idx = labels.at&lt;int&gt;(i, 0);
    train_labels.at&lt;float&gt;(i, idx) = 1.0f;
}</pre></div></li></ul></div><p class="calibre8">Finally, we train the network with the following code:</p><div><pre class="programlisting">mlp-&gt;train(trainData);</pre></div><p class="calibre8">The training process takes a few minutes to complete. If you have a lot of training data, it may take a few hours.</p></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec14" class="calibre1"/>Testing stage</h3></div></div></div><p class="calibre8">Once we<a id="id355" class="calibre1"/> have trained our MLP, the testing stage is very simple.</p><p class="calibre8">First, we create a Mat to store the response of the network. The response is an array, whose length is the number of labels.</p><div><pre class="programlisting">Mat response(1, num_of_labels, CV_32FC1);</pre></div><p class="calibre8">Then, we assume that we have a Mat, called sample, which contains a feature vector. In our facial expression case, its size should be 1x1000.</p><p class="calibre8">We can call the <code class="email">predict</code> function of the <code class="email">mlp</code> model to obtain the response, as follows:</p><div><pre class="programlisting">mlp-&gt;predict(sample, response);</pre></div><p class="calibre8">The predicted label of the input sample is the index of the maximum value in the response array. You can find the label by simply iterating through the array. The disadvantage of this type of response is that you have to apply a <code class="email">softmax</code> function if you want a probability for each response. In other neural network frameworks, there is usually <a id="id356" class="calibre1"/>a softmax layer for this reason. However, the advantage of this type of response is that the magnitude of each response is retained.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch03lvl2sec30" class="calibre1"/>K-Nearest Neighbors (KNN)</h2></div></div></div><p class="calibre8"><strong class="calibre9">K-Nearest Neighbors</strong> (<strong class="calibre9">KNN</strong>) is a very simple algorithm for machine learning but works very well <a id="id357" class="calibre1"/>in many practical problems. The idea of KNN is to classify an unknown example with the most common class among k-nearest known examples. KNN is also known as a non-parametric lazy learning algorithm. It means that KNN doesn't make any assumptions about the data distribution. The training process is very fast since it only caches all training examples. However, the testing process requires a lot of computation. The following figure demonstrates how KNN works in a 2D points case. The green dot is an unknown sample. KNN will find k-nearest known samples in space, (k = 5 in this example). There are three samples of red labels and two samples of blue labels. Therefore, the label for the prediction is red.</p><div><img src="img/00048.jpeg" alt="K-Nearest Neighbors (KNN)" class="calibre11"/><div><p class="calibre28">An explanation of how KNN predicts labels for unknown samples</p></div></div><p class="calibre12"> </p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec15" class="calibre1"/>Training stage</h3></div></div></div><p class="calibre8">The implementation of KNN algorithms is very simple. We only need three lines of code to train<a id="id358" class="calibre1"/> a KNN model:</p><div><pre class="programlisting">Ptr&lt;ml::KNearest&gt; knn = ml::KNearest::create();
Ptr&lt;ml::TrainData&gt; trainData = ml::TrainData::create(train_features, ml::SampleTypes::ROW_SAMPLE, labels);
knn-&gt;train(trainData);</pre></div><p class="calibre8">The preceding code is the same as with SVM:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">train_features</code>: This is a Mat that contains each features vector as a row. The number of rows in <code class="email">train_features</code> is the number of training samples and the number of columns is the size of one features vector.</li><li class="listitem"><code class="email">train_labels</code>: This is a Mat that contains labels for each training feature. In KNN, <code class="email">train_labels</code> is a Nx1 matrix, N is the number of training samples. The value of each row is the truth label of the corresponding sample. The type of this Mat should be <code class="email">CV_32S</code>.</li></ul></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec16" class="calibre1"/>The testing stage</h3></div></div></div><p class="calibre8">The<a id="id359" class="calibre1"/> testing stage is very straightforward. We can just pass a feature vector to the <code class="email">findNearest</code> method of the <code class="email">knn</code> model and obtain the label:</p><div><pre class="programlisting">Mat predictedLabels;
knn-&gt;findNearest(sample, K, predictedLabels);</pre></div><p class="calibre8">The second parameter is the most important parameter. It is the number of maximum neighbors that may be used for classification. In theory, if there are an infinite number of samples available, a larger K always means a better classification. However, in our facial expression problem, we only have 213 samples in total and about 170 samples in the training set. Therefore, if we use a large K, KNN may end up looking for samples that are not neighbors. In our implementation, K equals 2.</p><p class="calibre8">The predicted labels are stored in the <code class="email">predictedLabels</code> variable and can be obtained as follows:</p><div><pre class="programlisting">float prediction = bestLabels.at&lt;float&gt;(0,0);</pre></div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_6"><a id="ch03lvl2sec31" class="calibre1"/>Normal Bayes classifier</h2></div></div></div><p class="calibre8">The Normal Bayes classifier is one of the simplest classifiers in OpenCV. The Normal Bayes classifier assumes that features vectors from each class are normally distributed, although not necessarily independently. This classifier is an effective classifier that can handle multiple classes. In the training step, the classifier estimates the mean and co-variance of the distribution for each class. In the testing step, the classifier computes the probability of the features to each class. In practice, we then test to see if the maximum probability is over a threshold. If it is, the label of the sample will be the<a id="id360" class="calibre1"/> class that has the maximum probability. Otherwise, we say that we can't recognize the sample.</p><p class="calibre8">OpenCV has already implemented this classifier in the ml module. In this section, we will show you the code to use the Normal Bayes classifier in our facial expression problem.</p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec17" class="calibre1"/>Training stage</h3></div></div></div><p class="calibre8">The code<a id="id361" class="calibre1"/> to implement the Normal Bayes classifier is the same as with SVM and KNN. We only need to call the <code class="email">create</code> function to obtain the classifier and start the training process. All the other parameters are the same as with SVM and KNN.</p><div><pre class="programlisting">Ptr&lt;ml::NormalBayesClassifier&gt; bayes = ml::NormalBayesClassifier::create();
Ptr&lt;ml::TrainData&gt; trainData = ml::TrainData::create(train_features, ml::SampleTypes::ROW_SAMPLE, labels);
bayes-&gt;train(trainData);</pre></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec18" class="calibre1"/>Testing stage</h3></div></div></div><p class="calibre8">The <a id="id362" class="calibre1"/>code to test a sample with the Normal Bayes classifier is a little different from previous methods:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, we need to create two Mats to store the output class and probability:<div><pre class="programlisting">Mat output, outputProb;</pre></div></li><li class="listitem" value="2">Then, we call the <code class="email">predictProb</code> function of the model:<div><pre class="programlisting">bayes-&gt;predictProb(sample, output, outputProb);</pre></div></li><li class="listitem" value="3">The computed probability is stored in <code class="email">outputProb</code> and the corresponding label can be retrieved as:<div><pre class="programlisting">unsigned int label = output.at&lt;unsigned int&gt;(0, 0);</pre></div></li></ol><div></div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_7"><a id="ch03lvl2sec32" class="calibre1"/>Software usage guide</h2></div></div></div><p class="calibre8">We<a id="id363" class="calibre1"/> have implemented the above process to perform classification with a training set. Using the software is quite easy:</p><div><ol class="orderedlist"><li class="listitem" value="1">Download the source code. Open the terminal and change directory to the source code folder.</li><li class="listitem" value="2">Build the software with <code class="email">cmake</code> using the follow command:<div><pre class="programlisting">
<strong class="calibre9">mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make</strong>
</pre></div></li><li class="listitem" value="3">You can use the <code class="email">train</code> tool as follows:<div><pre class="programlisting">
<strong class="calibre9">./train -algo &lt;algorithm_name&gt; -src &lt;input_features&gt; -dest &lt;output_folder&gt;</strong>
</pre></div></li></ol><div></div><p class="calibre8">The <code class="email">train</code> tool <a id="id364" class="calibre1"/>performs the training process and outputs the accuracy on the console. The learned model will be saved to the output folder for further use as <code class="email">model.yml</code>. Furthermore, kmeans centers and pca information from features extraction are also saved in <code class="email">features_extraction.yml</code>. The available parameters are:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">algorithm_name</code>: This can be <code class="email">mlp</code>, <code class="email">svm</code>, <code class="email">knn</code>, <code class="email">bayes</code>. This is the name of the learning algorithm.</li><li class="listitem"><code class="email">input_features</code>: This is the absolute path to the location of the YAML features file from the <code class="email">prepare_dataset</code> tool.</li><li class="listitem"><code class="email">output_folder</code>: This is the absolute path to the folder where you want to keep the output model.</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec27" class="calibre1"/>Evaluation</h1></div></div></div><p class="calibre8">In this section, we<a id="id365" class="calibre1"/> will show the performance of our facial expression recognition system. In our test, we will keep the parameters of each learning algorithm the same and only change the feature extraction. We will evaluate the feature extraction with the number of clusters equaling 200, 500, 1,000, 1,500, 2,000, and 3,000.</p><p class="calibre8">The following table shows the accuracy of the system with the number of clusters equaling 200, 500, 1,000, 1,500, 2,000, and 3,000.</p><p class="calibre8">Table 1: The accuracy (%) of the system with 1,000 clusters</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">K = 1000</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">MLP</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">SVM</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">KNN</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Normal Bayes</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">93.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">88.6364</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SURF</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">61.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">BRISK</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">61.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">KAZE</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">50</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">61.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DAISY</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DENSE-SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">20.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">45.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">40.9091</p>
</td></tr></tbody></table></div><p class="calibre8">Table 2: The accuracy (%) of the system with 500 clusters</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">K = 500</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">MLP</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">SVM</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">KNN</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Normal Bayes</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">56.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">70.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SURF</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">54.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">63.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">BRISK</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">36.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">52.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">52.2727</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">KAZE</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">56.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">63.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DAISY</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">54.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">63.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DENSE-SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">27.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">38.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td></tr></tbody></table></div><p class="calibre8">Table 3: The <a id="id366" class="calibre1"/>accuracy (%) of the system with 200 clusters</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">K = 200</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">MLP</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">SVM</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">KNN</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Normal Bayes</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">50</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SURF</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">54.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">52.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">63.6364</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">BRISK</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">29.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">50</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">54.5455</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">KAZE</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">50</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DAISY</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">45.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">70.4545</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DENSE-SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">29.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">40.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">31.8182</p>
</td></tr></tbody></table></div><p class="calibre8">Table 4: The accuracy (%) of the system with 1,500 clusters</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">K = 1500</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">MLP</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">SVM</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">KNN</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Normal Bayes</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">45.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">84.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SURF</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">88.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">86.3636</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">BRISK</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">54.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">56.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">KAZE</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">45.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DAISY</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">61.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">88.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DENSE-SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">34.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">38.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">38.6364</p>
</td></tr></tbody></table></div><p class="calibre8">Table 5: The accuracy (%) of the system with 2,000 clusters</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">K = 2000</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">MLP</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">SVM</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">KNN</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Normal Bayes</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">63.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">88.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">88.6364</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SURF</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">84.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">BRISK</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">61.3636</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">KAZE</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DAISY</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">84.0909</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DENSE-SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">38.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">45.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">36.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td></tr></tbody></table></div><p class="calibre8">Table 6: The <a id="id367" class="calibre1"/>accuracy (%) of the system with 3,000 clusters</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">K = 3000</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">MLP</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">SVM</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">KNN</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Normal Bayes</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">52.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">88.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">86.3636</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">SURF</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">77.2727</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">BRISK</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">52.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">65.9091</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.1818</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">59.0909</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">KAZE</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">61.3636</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">81.8182</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">70.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">84.0909</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DAISY</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">72.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">79.5455</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">70.4545</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">68.1818</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">DENSE-SIFT</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">27.2727</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">47.7273</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">38.6364</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">45.4545</p>
</td></tr></tbody></table></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec33" class="calibre1"/>Evaluation with different learning algorithms</h2></div></div></div><p class="calibre8">We can<a id="id368" class="calibre1"/> create graphs with the above results to compare the performance between features and learning algorithms in the following figure. We can see that SVM and Normal Bayes have better results than the others in most cases. The best result is 93.1818% for SVM and SIFT in 1,000 clusters. MLP has the lowest result in almost every case. One reason is that MLP requires lots of data to prevent over fitting. We only have around 160 training images. However, the feature size for each sample is between 100 and 150. Even with two hidden neurons, the number of weights is larger than the number of samples. KNN seems to work better than MLP but can't beat SVM and Normal Bayes.</p><div><img src="img/00049.jpeg" alt="Evaluation with different learning algorithms" class="calibre11"/><div><p class="calibre28">Relationship between the performance of features and machine algorithms under different numbers of clusters</p></div></div><p class="calibre12"> </p><div><img src="img/00050.jpeg" alt="Evaluation with different learning algorithms" class="calibre11"/><div><p class="calibre28">Effect of the number of centroids on the performance of features according to different machine algorithms</p></div></div><p class="calibre12"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec34" class="calibre1"/>Evaluation with different features</h2></div></div></div><p class="calibre8">In the <a id="id369" class="calibre1"/>figure, <em class="calibre10">Relationship between the performance of features and machine algorithms under different numbers of clusters</em>, we have evaluated six features. SIFT gives the best results in most cases. DAISY is comparable to SIFT. In some cases, KAZE also gives good results. DENSE-SIFT is not a good choice for our facial expression problem since the results are poor. Moreover, the computation cost for DENSE features is really high. In conclusion, SIFT is still the most stable choice. However, SIFT is under patent. You may want to look at DAISY or KAZE. We <a id="id370" class="calibre1"/>recommend you do the evaluation on your data and choose the most suitable feature.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec35" class="calibre1"/>Evaluation with a different number of clusters</h2></div></div></div><p class="calibre8">In<a id="id371" class="calibre1"/> the figure, <em class="calibre10">Effect of the number of centroids on the performance of features according to different machine algorithms</em>, we made a graph to visualize the effects of the number of clusters on performance. As you can see, the number of clusters differs between features. In SIFT, KAZE, and BRISK, the best number of clusters is 1,000. However, in SURF, DAISY, and DENSE-SIFT, 1,500 is a better choice. Basically, we don't want the number of clusters to be too large. The computation cost in kmeans increases with a larger number of clusters, especially in DENSE-SIFT.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec28" class="calibre1"/>System overview</h1></div></div></div><p class="calibre8">In this<a id="id372" class="calibre1"/> section, we will explain the process to apply the trained model in your application. Given a face image, we detect and process each face separately. Then, we find landmarks and extract the face region. The image features are extracted and passed to kmeans to obtain a 1,000-dimensional feature vector. PCA is applied to reduce the dimension of this feature vector. The learned machine learning model is used to predict the expression of the input face.</p><p class="calibre8">The following figure shows the complete process to predict the facial expression of a face in an image:</p><div><img src="img/00051.jpeg" alt="System overview" class="calibre11"/><div><p class="calibre28">The process to predict a facial expression in a new image</p></div></div><p class="calibre12"> </p></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec29" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">We have introduced a basic system for facial expression. If you are really interested in this topic, you may want to read this section for more guidance on how to improve the performance of the system. In this section, we will introduce you to compiling the <code class="email">opencv_contrib</code> module, the Kaggle facial expression dataset, and the k-cross validation approach. We will also give you some suggestions on how to get better feature extraction.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec36" class="calibre1"/>Compiling the opencv_contrib module</h2></div></div></div><p class="calibre8">In this <a id="id373" class="calibre1"/>section, we will introduce the process for compiling <code class="email">opencv_contrib</code> in Linux-based systems. If you use Windows, you can use the Cmake GUI with the same options.</p><p class="calibre8">First, clone the <code class="email">opencv</code> repository to your local machine:</p><div><pre class="programlisting">
<strong class="calibre9">git clone https://github.com/Itseez/opencv.git --depth=1</strong>
</pre></div><p class="calibre8">Second, clone the <code class="email">opencv_contrib</code> repository to your local machine:</p><div><pre class="programlisting">
<strong class="calibre9">git clone https://github.com/Itseez/opencv_contrib --depth=1</strong>
</pre></div><p class="calibre8">Change directory to the <code class="email">opencv</code> folder and make a build directory:</p><div><pre class="programlisting">
<strong class="calibre9">cd opencv</strong>
<strong class="calibre9">mkdir build</strong>
<strong class="calibre9">cd build</strong>
</pre></div><p class="calibre8">Build OpenCV from source with opencv_contrib support. You should change <code class="email">OPENCV_EXTRA_MODULES_PATH</code> to the location of <code class="email">opencv_contrib</code> on your machine:</p><div><pre class="programlisting">
<strong class="calibre9">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/ ..</strong>
<strong class="calibre9">make -j4</strong>
<strong class="calibre9">make install</strong>
</pre></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec37" class="calibre1"/>Kaggle facial expression dataset</h2></div></div></div><p class="calibre8">Kaggle is <a id="id374" class="calibre1"/>a great community of data scientists. There are many competitions <a id="id375" class="calibre1"/>hosted by Kaggle. In 2013, there was a facial expression recognition challenge.</p><div><h3 class="title2"><a id="note56" class="calibre1"/>Note</h3><p class="calibre8">At the<a id="id376" class="calibre1"/> moment, you can go to the following link to access the full dataset:</p><p class="calibre8"><a class="calibre1" href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/</a></p></div><p class="calibre8">The dataset consists of 48x48 pixel grayscale images of faces. There are 28,709 training samples, 3,589 public test images and 3,589 images for final test. The dataset contains seven expressions (Anger, Disgust, Fear, Happiness, Sadness, Surprise and Neutral). The winner achieved<a id="id377" class="calibre1"/> a score of 69.769 %. This dataset is huge so we think that <a id="id378" class="calibre1"/>our basic system may not work out of the box. We believe that you should try to improve the performance of the system if you want to use this dataset.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec38" class="calibre1"/>Facial landmarks</h2></div></div></div><p class="calibre8">In our<a id="id379" class="calibre1"/> facial expression system, we use face detection as a pre-processing step to extract the face region. However, face detection is prone to misalignment, hence, feature extraction may not be reliable. In recent years, one of the most common approaches has been the usage of facial landmarks. In this kind of method, the facial landmarks are detected and used to align the face region. Many researchers use facial landmarks to extract the facial components such as the eyes, mouth, and so on, and do feature extractions separately.</p><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec19" class="calibre1"/>What are facial landmarks?</h3></div></div></div><p class="calibre8">Facial<a id="id380" class="calibre1"/> landmarks are predefined locations of facial components. The figure <a id="id381" class="calibre1"/>below shows an example of a 68 points system from the iBUG group (<a class="calibre1" href="http://ibug.doc.ic.ac.uk/resources/facial-point-annotations">http://ibug.doc.ic.ac.uk/resources/facial-point-annotations</a>)</p><div><img src="img/00052.jpeg" alt="What are facial landmarks?" class="calibre11"/><div><p class="calibre28">An example of a 68 landmarks points system from the iBUG group</p></div></div><p class="calibre12"> </p></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec20" class="calibre1"/>How do you detect facial landmarks?</h3></div></div></div><p class="calibre8">There <a id="id382" class="calibre1"/>are several ways to detect facial landmarks in a face region. We will give you a few solutions so that you can start your project easily</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Active Shape Model</strong>: This <a id="id383" class="calibre1"/>is one of the most common approaches to this problem. You may find the following library useful:<p class="calibre26">Stasm: <a class="calibre1" href="https://github.com/cxcxcxcx/asmlib-opencv">https://github.com/cxcxcxcx/asmlib-opencv</a></p></li><li class="listitem"><strong class="calibre9">Face Alignment by Explicit Regression by Cao et al</strong>: This is one of the latest <a id="id384" class="calibre1"/>works on facial landmarks. This system is very efficient and highly accurate. You can find an open source implementation at the following hyperlink: <a class="calibre1" href="https://github.com/soundsilence/FaceAlignment">https://github.com/soundsilence/FaceAlignment</a></li></ul></div></div><div><div><div><div><h3 class="title2"><a id="ch03lvl3sec21" class="calibre1"/>How do you use facial landmarks?</h3></div></div></div><p class="calibre8">You can <a id="id385" class="calibre1"/>use facial landmarks in many ways. We will give you some guides:</p><div><ul class="itemizedlist"><li class="listitem">You can use facial landmarks to align the face region to a common standard and extract the features vectors as in our basic facial expression system.</li><li class="listitem">You can extract features vectors in different facial components such as eyes and mouths separately and combine everything in one feature vector for classification.</li><li class="listitem">You can use the location of facial landmarks as a feature vector and ignore the texture in the image.</li><li class="listitem">You can build classification models for each facial component and combine the prediction in a weighted manner.</li></ul></div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec39" class="calibre1"/>Improving feature extraction</h2></div></div></div><p class="calibre8">Feature<a id="id386" class="calibre1"/> extraction is one of the most important parts of facial expression. It is better to choose the right feature for your problem. In our implementation, we have only used a few features in OpenCV. We recommend that you try every possible feature in OpenCV. Here is the list of supported features in Open CV: BRIEF, BRISK, FREAK, ORB, SIFT, SURF, KAZE, AKAZE, FAST, MSER, and STAR.</p><p class="calibre8">There are other great features in the community that might be suitable for your problem, such as LBP, Gabor, HOG, and so on.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch03lvl2sec40" class="calibre1"/>K-fold cross validation</h2></div></div></div><p class="calibre8">K-fold cross <a id="id387" class="calibre1"/>validation is a common technique for estimating the performance of a classifier. Given a training set, we will divide it into k partitions. For each fold i of k experiments, we will train the classifier using all the samples that do not belong to fold i and use the samples in fold i to test the classifier.</p><p class="calibre8">The advantage of k-fold cross validation is that all the examples in the dataset are eventually used for training and validation.</p><p class="calibre8">It is important to divide the original dataset into the training set and the testing set. Then, the training set will be used for k-fold cross validation and the testing set will be used for the final test.</p><p class="calibre8">Cross validation combines the prediction error of each experiment and derives a more accurate estimate of the model. It is very useful, especially in cases where we don't have much data for training. Despite a high computation time, using a complex feature is a great idea if you want to improve the overall performance of the system.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec30" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter showed the complete process of a facial expression system in OpenCV 3. We went through each step of the system and gave you a lot of alternative solutions for each step. This chapter also made an evaluation of the results based on features and learning algorithms.</p><p class="calibre8">Finally, this chapter gave you a few hints for further improvement including a great facial expression challenge, a facial landmarks approach, some features suggestions, and k-fold cross validation.</p></div></body></html>