- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Being Successful with Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的成功之道
- en: An all-too-common problem in the field of machine learning occurs when students,
    with fresh excitement from learning the methods, struggle to apply what they’ve
    learned to real-world projects. Much as the beauty of a forest trail feels sinister
    in the darkness of night, code and methods that initially seemed straightforward
    feel daunting in the absence of a step-by-step roadmap. Without such a guide,
    the learning curve feels so much steeper and pitfalls appear deeper.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，一个过于常见的问题发生在学生从学习方法中获得新鲜兴奋之后，却难以将所学应用于现实世界项目。就像森林小径在夜幕下的黑暗中显得邪恶一样，最初看似简单的代码和方法在没有一步一步的路线图的情况下显得令人畏惧。没有这样的指南，学习曲线显得更加陡峭，陷阱看起来更深。
- en: 'It is discouraging to think about the countless students that have been turned
    away from machine learning, due to the chasm between machine learning in theory
    and practice. Having worked in the field for over a decade, and having trained,
    interviewed, hired, and supervised numerous new practitioners, I have seen the
    challenges of this catch-22 firsthand. It is seemingly a paradox: gaining real-world
    experience in machine learning seems impossible without first having gained experience
    in machine learning!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 想到由于机器学习理论与实践之间的鸿沟，无数学生被拒之门外，这令人沮丧。在机器学习领域工作超过十年，并培训、面试、雇佣和监督了许多新从业者后，我亲眼目睹了这种“鸡生蛋，蛋生鸡”的挑战。这似乎是一个悖论：在没有首先获得机器学习经验的情况下，获得机器学习的实际经验似乎是不可能的！
- en: 'The purpose of this chapter, as well as those that follow, is to serve as a
    bridge between the simple teaching examples in prior chapters and the unyielding
    complexity of the real world. In this chapter, you will learn:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的，以及随后的章节，是作为先前章节中简单教学示例和现实世界不可抗拒的复杂性之间的桥梁。在本章中，你将学习：
- en: The factors that contribute to the success and failure of machine learning models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 影响机器学习模型成功与失败的因素
- en: Strategies for designing projects that are likely to perform well
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计可能表现良好的项目的策略
- en: How to perform data exploration to spot potential issues early
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何进行数据探索以提前发现潜在问题
- en: Why data science and competition are relevant to machine learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么数据科学和竞赛与机器学习相关
- en: Whether your definition of success involves finding a job in the field, building
    better machine learning models, or simply deepening your knowledge of the field’s
    tools and techniques, you will find something to learn in the coming pages. You
    may even find yourself with a newfound desire to join many others in online machine
    learning competitions, which stretch your skills and put your knowledge to the
    test.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的成功定义是在该领域找到工作、构建更好的机器学习模型，还是仅仅深化你对该领域工具和技术的了解，你将在接下来的页面中找到一些可以学习的东西。你甚至可能会发现自己突然有了加入许多在线机器学习竞赛的新愿望，这些竞赛可以拓展你的技能并检验你的知识。
- en: What makes a successful machine learning practitioner?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么使机器学习从业者成功？
- en: To be clear, the challenges of real-world machine learning are not due to the
    addition of more advanced or complex methods; after all, the first nine chapters
    of this book covered practical, real-world problems as diverse and challenging
    as identifying cancer cells, filtering spam messages, and predicting risky bank
    loans. Instead, the challenges of real-world machine learning have much to do
    with aspects of the field that are difficult to convey in a scripted setting,
    like a textbook or lecture. Machine learning is as much art as it is science,
    and just as it would be challenging to learn to paint, dance, or speak a foreign
    language without real-world practice, it is equally difficult to apply machine
    learning methods to new, uncharted domains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，现实世界机器学习的挑战并非源于更多高级或复杂方法的添加；毕竟，本书的前九章涵盖了各种各样、具有挑战性的实际问题，如识别癌细胞、过滤垃圾邮件和预测风险银行贷款。相反，现实世界机器学习的挑战与该领域难以在脚本设置中传达的方面有很大关系，如教科书或讲座。机器学习既是艺术也是科学，就像在没有现实世界实践的情况下学习绘画、跳舞或说一门外语一样具有挑战性，同样难以将机器学习方法应用于新的、未知的领域。
- en: Like pioneers exploring distant lands, you will encounter never-before-seen
    challenges requiring soft skills, including persistence and creativity. You will
    encounter large, messy, and complex datasets requiring in-depth exploration and
    documentation; graphs and visualizations are the field’s equivalent to the pioneers’
    charts and maps. Your analytical and programming skills will be tested, and when
    you fail, as tends to happen early and often, you will need to iterate and improve
    upon your mistakes. Creating reproducible experiments using the scientific method
    will become the breadcrumbs that will prevent you from walking in circles. To
    become the machine learning equivalent of a rugged individualist involves being
    both nimble and adaptable, yet also having an insatiable curiosity like a dog
    with a bone—that is, once it bites down, it doesn’t let go.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就像探索遥远土地的先驱者一样，你将遇到前所未有的挑战，这些挑战需要软技能，包括坚持和创造力。你将遇到大量、混乱和复杂的数据集，需要深入探索和记录；图表和可视化是这个领域的先驱者图表和地图的等价物。你的分析和编程技能将受到考验，当你失败时，正如常常发生的那样，尤其是在早期，你需要迭代并改进你的错误。使用科学方法创建可重复的实验将成为防止你绕圈子走的面包屑。要成为机器学习领域的坚韧个体主义者，需要既灵活又适应性强，同时还要像狗咬住骨头一样有着无法满足的好奇心——也就是说，一旦咬住，就不会放手。
- en: Although the idea of the rugged individualist is a fantastic metaphor for the
    solo elements of machine learning, the work is very much also a team sport. Exploring
    the tundra of Antarctica or climbing the peaks of Mount Everest is a grueling
    effort, and so are most real-world machine learning projects. It would be unwise
    or risky to go at such tasks alone—perhaps even dangerous, if the stakes are high.
    However, even within a team, failure can occur due to poor planning or poor communication.
    The handoff between the data scientists that develop the models and the data engineers
    that provide them with the data is particularly treacherous. If a team makes it
    this far, an even more challenging handoff occurs later when the model must be
    implemented into business practices and IT systems. For this reason, among many
    others, the majority of machine learning models are never deployed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然坚韧个体主义者的概念是机器学习独立元素的绝佳隐喻，但这项工作在很大程度上也是一种团队运动。探索南极洲的苔原或攀登珠穆朗玛峰是一项艰巨的努力，大多数现实世界的机器学习项目也是如此。独自承担这样的任务是不明智的或危险的，如果风险很高，甚至可能非常危险。然而，即使在团队中，由于计划不周或沟通不畅，也可能发生失败。数据科学家开发模型和数据工程师提供数据之间的交接尤其危险。如果一个团队走到了这一步，那么在模型必须实施到业务实践和IT系统中的时候，还会发生一个更具挑战性的交接。因此，在许多其他原因中，大多数机器学习模型从未被部署。
- en: If it seems like real-world machine learning requires superhuman-like skillsets,
    this may not be far from the truth if a perusal of recent online job postings
    is any indication. One very specific job posting asks for experience building
    recommender systems and designing image recognition tools, as well as familiarity
    with graph representation learning and natural language processing. Another asks
    for experience building “performant inference pipelines on very large datasets.”
    Many want experience with deep neural networks, yet some are more general, requiring
    a “solid understanding of machine learning fundamentals” and the “ability to analyze
    a wide variety of data both structured and unstructured.” The diverse sets of
    skills can be somewhat intimidating to early-career practitioners and lead them
    to wonder where to begin.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感觉现实世界的机器学习需要超人的技能集，那么如果查看最近的在线招聘信息，这可能并不离真相太远。一个非常具体的职位要求有构建推荐系统和设计图像识别工具的经验，以及熟悉图表示学习和自然语言处理。另一个要求有在非常大数据集上构建“高性能推理管道”的经验。许多人希望有深度神经网络的经验，而有些人则更通用，需要“对机器学习基础有扎实的理解”以及“能够分析各种结构化和非结构化数据的能力”。这些多样的技能组合可能会让初入职场的从业者感到有些害怕，并使他们想知道从哪里开始。
- en: '*Figure 11.1* lists some of the so-called “hard” technical skills commonly
    used in the field as well as some of the beneficial “soft” traits. By the end
    of this book, you will have been exposed to most of the tools and skills on the
    left side of the figure, and by completing the exercises, you will develop the
    characteristics on the right. Keep in mind that finding a person who has more
    than a superficial handle on every one of these skills is exceptionally rare.
    These are the fabled “unicorns” of the field, although even they would likely
    admit that there is still much to learn. It is always possible to go deeper and
    learn more about a particular topic. Given limited time and energy, most people
    must compromise on whether to go broader across many areas or deeper into just
    a few.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.1*列出了该领域常用的所谓“硬”技术技能以及一些有益的“软”特质。到本书结束时，你将接触到图中左侧的大部分工具和技能，通过完成练习，你将培养出右侧的特性。请记住，找到一个对这些技能中的每一个都有深入了解的人是非常罕见的。这些是传说中的“独角兽”。尽管如此，他们可能会承认还有很多东西要学。总是有可能更深入地学习特定主题。鉴于有限的时间和精力，大多数人必须在广泛涉猎多个领域或深入少数几个领域之间做出妥协。'
- en: '![Diagram  Description automatically generated](img/B17290_11_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17290_11_01.png)'
- en: 'Figure 11.1: Real-world machine learning requires numerous technical skills
    (left) and soft skills (right)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：现实世界的机器学习需要众多技术技能（左侧）和软技能（右侧）
- en: One of the best ways to hone machine learning skills, and especially soft skills,
    is through competition. As depicted in *Figure 11.1*, competition improves machine
    learning results in more ways than one; it is a key component of the individual
    drive to innovate and improve one’s own performance, yet it also fosters strong
    teamwork toward meeting a common goal. For these reasons, competition has long
    been a part of machine learning training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提高机器学习技能，尤其是软技能的最好方法之一是通过竞赛。如图11.1所示，竞赛以多种方式提高了机器学习的结果；它是个人创新和提升自身表现动力的关键组成部分，同时它也促进了团队为实现共同目标而进行的强大合作。出于这些原因，竞赛长期以来一直是机器学习培训的一部分。
- en: For instance, academic computer scientists have competed for over 25 years in
    an exercise called the **Knowledge Discovery and Data Mining** (**KDD**) Cup ([https://www.kdd.org/kdd-cup](https://www.kdd.org/kdd-cup)),
    which chooses winners based on their performance on machine learning tasks that
    change annually. Similar competitions exist for specialized topics in image, text,
    and audio data, as well as many others.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，学术计算机科学家已经参加了超过25年的名为**知识发现与数据挖掘**(**KDD**)杯的竞赛([https://www.kdd.org/kdd-cup](https://www.kdd.org/kdd-cup))，该竞赛根据参赛者在每年变化的机器学习任务中的表现来选择获胜者。类似的竞赛存在于图像、文本和音频数据等专门主题，以及许多其他领域。
- en: In one of the earliest widely known examples of machine learning competitions
    in the for-profit space, in 2006, the Netflix video streaming service began offering
    a $1M prize to make its movie recommendations 10 percent more accurate. The publicity
    around this event spurred a wave of additional corporate-sponsored challenges,
    including those listed on Kaggle ([https://www.kaggle.com](https://www.kaggle.com)),
    a website that hosts competitions offering cash rewards for advancing the state
    of the art on tough machine learning tasks across varied domains. Kaggle soon
    became so popular that it inspired a generation of machine learning practitioners,
    some of whom used their victories as a springboard for future work in consulting
    and tech companies. In *Chapter 12*, *Advanced Data Preparation*, you will learn
    from the experience of some of these Kaggle champions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在盈利空间中最早广泛知名的机器学习竞赛之一，2006年，Netflix视频流媒体服务开始提供100万美元的奖金，以使其电影推荐准确率提高10%。这一事件带来的宣传促使了一系列由企业赞助的挑战活动，包括在Kaggle([https://www.kaggle.com](https://www.kaggle.com))上列出的那些，该网站举办竞赛，为在各个领域推进复杂机器学习任务的先进状态提供现金奖励。Kaggle很快变得非常受欢迎，激发了一代机器学习实践者的灵感，其中一些人利用他们的胜利作为未来在咨询和科技公司工作的跳板。在*第12章*，*高级数据准备*中，你将学习一些这些Kaggle冠军的经验。
- en: Not everybody enjoys head-to-head competition, but you can still compete against
    yourself or imagine your company in competition against other businesses. Some
    practitioners are content with challenging themselves to beat their own “high
    score” on a model performance statistic. Others are motivated by “survival of
    the fittest” and the thought that a business market rewards companies that outperform
    others—some of which eventually go extinct. In any case, the goal of competition
    is not to boost one’s ego, but rather to motivate innovation and continuous quality
    improvement, ensuring your skills stay up to date.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个人都喜欢面对面的竞争，但你仍然可以与自己竞争，或者想象你的公司在与其他企业竞争。一些从业者满足于挑战自己，在模型性能统计上超越自己的“高分”。其他人则受到“适者生存”和这样一个想法的激励，即商业市场奖励那些超越他人的公司——其中一些最终会灭绝。无论如何，竞争的目标不是提升个人的自尊，而是激励创新和持续的质量改进，确保你的技能保持最新。
- en: The need to continually learn, and to continually apply your learning, may be
    the most important characteristics of a machine learning devotee. As described
    in *Chapter 1*, *Introducing Machine Learning*, the field evolved in an environment
    in which the volume and complexity of data grew alongside the computing power
    and statistical methods necessary to make sense of it. This evolution shows no
    signs of slowing down. Data, tools, and methods will change, but there will always
    be a need for people to apply them. Therefore, approach each project as an opportunity
    to learn something new. The iterative and sometimes addictive process of building
    better models is an apt place to begin this journey.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习和不断应用所学知识可能是机器学习爱好者最重要的特征。正如在*第一章*，*介绍机器学习*中所描述的，该领域是在数据量、复杂性以及处理这些数据所需的计算能力和统计方法共同增长的环境中演化的。这种演变并没有放缓的迹象。数据、工具和方法会发生变化，但总会有需要人们去应用它们的需求。因此，将每个项目视为学习新知识的机会。构建更好模型的迭代过程，有时甚至具有上瘾性，是开始这段旅程的一个合适起点。
- en: What makes a successful machine learning model?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么因素使机器学习模型成功？
- en: Until now, we have taken a largely *quantitative* perspective of what it means
    to be a successful machine learning model. Supervised learners were initially
    said to perform well if the accuracy was high.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要从*定量*的角度来理解一个成功的机器学习模型意味着什么。监督学习者在最初被认为如果准确率高就会表现良好。
- en: In *Chapter 10*, *Evaluating Model Performance*, we expanded this definition
    to include other, more sophisticated performance measures, such as the Matthews
    correlation coefficient and the area under the ROC curve, to account for the fact
    that accuracy is misleading for unbalanced datasets and to consider performance
    trade-offs for potential use cases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第十章*，*评估模型性能*中，我们将这个定义扩展到包括其他更复杂的性能度量，如马修斯相关系数和ROC曲线下的面积，以解释准确性对于不平衡数据集来说是误导性的，并考虑潜在用例的性能权衡。
- en: So far, we have relegated *qualitative* measures of model performance to the
    realm of unsupervised learning, although there are certainly non-quantifiable
    considerations in the area of predictive modeling as well. Consider, for example,
    a credit scoring model that is so computationally expensive that it cannot be
    implemented in a real-time application, or so algorithmically complex that an
    explanation for its decisions cannot be provided to the applicants. With this
    in mind, we may favor a simpler, less accurate model, if the alternative is no
    model at all. After all, even a simple predictive model is usually better than
    nothing—the word “usually” being a key qualifier, given what we will examine shortly
    about models failing dramatically in the real world!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将模型性能的*定性*度量限制在无监督学习的领域，尽管在预测建模领域也存在一些无法量化的考虑因素。例如，考虑一个计算成本如此之高以至于无法在实时应用中实施，或者算法如此复杂以至于无法向申请人提供其决策解释的信用评分模型。考虑到这一点，如果选择没有模型，我们可能会倾向于选择一个更简单、不那么精确的模型。毕竟，即使是简单的预测模型通常也比没有好——这里的“通常”是一个关键限定词，考虑到我们很快将要探讨的模型在现实世界中失败得非常严重的情况！
- en: 'There may be business costs, resource constraints, or human resource factors
    not easily incorporated into the model itself that impact the success of a modeling
    project. To illustrate this fact, imagine that you create a customer churn forecasting
    algorithm that can identify with a high degree of accuracy the most likely customers
    to stop purchasing a product. Unfortunately, upon deploying the model, you receive
    complaints from the sales representatives, who have the role of attempting to
    retain these customers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在难以融入模型本身的业务成本、资源限制或人力资源因素，这些因素会影响建模项目的成功。为了说明这一点，想象你创建了一个客户流失预测算法，该算法可以以高精度识别最有可能停止购买产品的客户。不幸的是，在部署模型后，你收到了销售代表的投诉，他们试图保留这些客户：
- en: “I already know these people will churn. You’re telling me nothing new.”
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我已经知道这些人会流失。你告诉我什么新东西？”
- en: “That customer stopped buying 2 months ago. They already churned.”
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “那个客户两个月前就停止购买了。他们已经流失了。”
- en: “We actually want these people to churn, as they are low-value customers.”
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们实际上希望这些人流失，因为他们是低价值客户。”
- en: “I’ve already spoken with that customer, and there’s nothing we can do.”
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我已经和那个客户谈过了，我们无能为力。”
- en: “Your model’s predictions make no sense. I don’t trust it.”
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你的模型预测没有意义。我不信任它。”
- en: “What makes you think this customer will churn? They seem happy to me.”
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你怎么知道这个客户会流失？他们看起来对我很满意。”
- en: “We’ll lose money trying to retain that customer.”
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我们试图保留那个客户会亏损。”
- en: “The predictions don’t seem to be as good as before. What happened?”
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “预测似乎没有以前好。发生了什么？”
- en: These types of comments are typical in real-world machine learning, and represent
    common barriers to a project’s overall success, even for a model that, by conventional,
    statistical performance metrics, was deemed accurate or effective. The trouble
    with these barriers is that they are not easily navigated without deep knowledge
    of the business in which the model will be used. On the other hand, these types
    of issues tend to follow similar patterns, which can be foreseen with practice.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这类评论在现实世界的机器学习中很常见，代表了项目整体成功中的常见障碍，即使按照传统的统计性能指标，该模型被认为准确或有效。这些障碍的问题在于，没有深入了解模型将用于其中的业务，它们就不容易被克服。另一方面，这类问题往往遵循相似的模式，通过实践可以预见。
- en: 'The following table categorizes the problems into four groups, along with typical
    symptoms and potential workarounds:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格将问题分为四组，以及典型的症状和可能的解决方案：
- en: '| **Pitfall** | **Symptoms** | **Potential solutions** |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **陷阱** | **症状** | **可能的解决方案** |'
- en: '| Predicting the obvious |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 预测显而易见的事情 |'
- en: A simpler model (or a well-known rule-of-thumb) performs almost as well
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个更简单的模型（或一个众所周知的经验法则）几乎表现得一样好
- en: The model’s performance statistics seem “too good to be true”
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能统计数据看起来“太好了，以至于不真实”
- en: The model performs well on training and test sets but makes no impact when deployed
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在训练集和测试集上表现良好，但在部署时没有产生影响
- en: The outcomes seem inevitable; having the predictions provides no actionable
    way to intervene
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果似乎不可避免；拥有预测结果并没有提供可采取的行动来干预
- en: '|'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Reformulate the problem to be more challenging to the learning algorithm
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新表述问题，使其对学习算法更具挑战性
- en: Look out for rote memorization, circular logic, or target leakage (having a
    predictor that is essentially a proxy for the target)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意避免死记硬背、循环逻辑或目标泄露（拥有一个本质上是对目标的代理的预测器）
- en: Recode the target variable, or limit access to certain predictors overly correlated
    with the target, such that the model finds new connections rather than the obvious
    ones
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新编码目标变量，或限制对与目标高度相关的某些预测器的访问，这样模型就能找到新的联系而不是显而易见的联系
- en: Examine the mid-range of predicted probabilities, or filter out the most obvious
    or inevitable predictions
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查预测概率的中值，或过滤掉最明显或不可避免预测
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Conducting unfair evaluations |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 进行不公平的评估 |'
- en: The model performs much worse in the real world than during testing
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在实际世界中的表现远不如在测试中
- en: Determining the “best” model used a lot of iteration or tuning
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定使用的“最佳”模型需要大量的迭代或调整
- en: The correct or incorrect predictions are seemingly predictable; the model may
    do better or worse than expected on certain segments of data
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确或错误的预测似乎是可以预测的；模型在某些数据段上的表现可能比预期好或差
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Ensure an appropriate evaluation dataset is used
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保使用适当的评估数据集
- en: Use cross-validation correctly and understand its limitations
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确使用交叉验证并了解其局限性
- en: Beware of common forms of internally correlated data and understand how to construct
    fair test sets in these cases
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警惕内部相关数据的常见形式，并了解在这些情况下如何构建公平的测试集
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Being inconsiderate of real-world impacts |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 忽视现实世界的影响 |'
- en: The results are interesting but not very impactful
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果很有趣，但影响不大
- en: An unclear business case to implement the model
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施模型的不明确业务案例
- en: Important subsets of examples are neglected by the model
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型忽略了重要的示例子集
- en: Overreliance on simple, quantitative performance measures
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度依赖简单的定量性能指标
- en: Ignoring predicted probabilities and treating all predictions with equal weight
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略预测概率，对所有预测给予同等权重
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Forecast the impact of the project under various plausible scenarios using simulations
    and experiments
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模拟和实验预测项目在各种可能情景下的影响
- en: Put filters on the output that reflect real-world constraints
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输出上设置反映现实世界约束的过滤器
- en: Create ROC curves and other cost-aware performance metrics
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建ROC曲线和其他成本感知的性能指标
- en: Focus on the high-impact outcomes, not the “low-hanging fruit”
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注高影响的结果，而不是“低垂的果实”
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Lack of trust |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 缺乏信任 |'
- en: Stakeholders refuse to act upon the data and rely on intuition instead
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利益相关者拒绝根据数据采取行动，而依赖直觉
- en: A preference to do things the “old” way
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于用“老”方法做事
- en: Little interest in working through predictions systematically
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对系统地处理预测缺乏兴趣
- en: Stakeholders cherry-picking results they agree/disagree with
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利益相关者挑选他们同意/不同意的结果
- en: Repeatedly asking to justify the predictions
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反复要求证明预测的合理性
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Identify “champions” that will help promote the project
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定将有助于推广项目的“冠军”
- en: Include stakeholders in the modeling process (especially data preparation) and
    iterate on their feedback
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将利益相关者纳入建模过程（特别是数据准备）并迭代他们的反馈
- en: Craft an “elevator pitch” and “road show” slide deck, to preemptively address
    FAQs
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定“电梯演讲”和“路演”幻灯片，以预先解决常见问题
- en: Document cases where the model made an impact and tell these stories repeatedly
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录模型产生影响的案例，并反复讲述这些故事
- en: Output predictions in an actionable form, such as a “stoplight” approach
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以可操作的形式输出预测，例如“交通灯”方法
- en: Use model explainability tools
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型可解释性工具
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: It is likely that many pages could be spent describing a career’s worth of experience
    with each of these three categories of pitfalls, but unfortunately, this would
    be no substitute for learning for oneself the hard way. That being said, there
    are some broad pointers that may help you steer clear of some of the bumps in
    the road.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能需要花费许多页面来描述每个这些陷阱类别的职业生涯经验，但遗憾的是，这并不能替代自己艰难学习的过程。话虽如此，有一些广泛的指导方针可能有助于你避开道路上的某些颠簸。
- en: Avoiding obvious predictions
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免明显的预测
- en: When it comes to *predicting the obvious*, it may not be at all obvious how
    or why this happens in the first place! The short answer to this question is that
    is often easier than one might think to accidentally construct a model that “cheats”
    by finding a way to simplify the problem, or “short-circuit” the problem without
    doing the deeper work necessary to truly understand the problem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到*预测显而易见的事情*时，可能根本不明显的是它最初是如何或为什么发生的！这个问题的简短答案是，往往比人们想象的更容易意外构建一个“作弊”的模型，通过找到一种简化问题的方法，或者“短路”问题，而不做真正理解问题的必要深入工作。
- en: This is especially true for projects that track features and outcomes over time,
    such as forecasting an event that will happen in the future. These types of projects
    typically begin with **time series data**, which repeatedly measures the same
    attributes for examples over time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于跟踪特征和结果随时间变化的项目尤其正确，例如预测未来将发生的事件。这类项目通常从**时间序列数据**开始，这些数据反复测量随时间变化的示例的相同属性。
- en: We will consider time series data from a data preparation perspective in *Chapter
    12*, *Advanced Data Preparation*, but for now, it suffices to say that data with
    a time dimension must be carefully coded in order to avoid using values from the
    future to predict the past. This problem falls under the broader category of **leakage**,
    or more specifically, **target leakage**, which describes a situation in which
    the learning algorithm knows something about the target to be predicted, which
    it would not have available in the real-world deployment environment. When there
    is a clear out-of-sequence issue, such as using current credit scores to predict
    past loan defaults, target leakage is quite obvious in hindsight. It still occurs
    surprisingly often, especially when analysts simply dump all potential predictors
    into a model without considering what they mean. Other times, target leakage is
    very, very subtle and more difficult to detect, only to be revealed upon deeper
    analysis when a stakeholder rejects the results as “too good to be true.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第12章“高级数据准备”中从数据准备的角度考虑时间序列数据，但就目前而言，只需说具有时间维度的数据必须仔细编码，以避免使用未来的值来预测过去。这个问题属于更广泛的**泄露**类别，或者更具体地说，是**目标泄露**，它描述了学习算法对要预测的目标有所了解的情况，而在现实世界的部署环境中，它不会拥有这种了解。当存在明显的顺序问题时，例如使用当前的信用评分来预测过去的贷款违约，目标泄露在事后看来非常明显。它仍然令人惊讶地经常发生，尤其是在分析师简单地将所有潜在的预测变量放入模型中，而没有考虑它们的意义时。在其他时候，目标泄露非常微妙且难以检测，只有在利益相关者拒绝结果为“太好了，以至于不可能是真的”时，才会通过更深入的分析揭示出来。
- en: One of the most subtle forms of leakage occurs when the target variable is defined
    in such a way that it creates tautological or definitional predictions. The relationship
    between the target and the predictors need not always be fully deterministic in
    this case, but merely unduly correlated, or linked together in some unclear way.
    For example, suppose we use a business definition that defines this month’s churn
    status using a 3-month rolling average in sales. Using last month and 2 months
    ago as predictors for the current month’s churn then gets the algorithm two-thirds
    of the way to the correct answer—customers with low sales in those months are
    very likely to churn, by definition. This type of mistake is easy to make when
    using complex survey data, in which the individual survey responses are used as
    predictors and a score computed from the set of survey responses is used as the
    target to be predicted. As a general rule, to avoid target leakage, it is best
    to use a target that is generated by a completely independent process, and is
    collected at a later time than the data used to make the prediction.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量以这种方式定义，从而产生同义反复或定义性预测时，目标变量与预测变量之间的关系在这种情况下不必总是完全确定性的，但可能是过度相关的，或者以某种不清楚的方式相互关联。例如，假设我们使用一个业务定义，该定义使用销售中的3个月滚动平均来定义这个月的流失状态。然后，使用上个月和两个月前的数据作为当前月份流失的预测变量，算法就能得到正确答案的三分之二——那些月份销售量低的客户，按照定义，很可能流失。当使用复杂的调查数据时，这种错误很容易发生，其中个人调查响应被用作预测变量，从调查响应集中计算出的分数被用作要预测的目标。一般来说，为了避免目标泄露，最好使用由完全独立的过程生成的目标，并且收集时间晚于用于预测的数据。
- en: Beware of using the future to predict the past! For a model that will be deployed
    in the real world, this is almost always a clear sign that target leakage is present.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 小心使用未来预测过去！对于一个将在现实世界中部署的模型，这几乎总是目标泄露存在的明显迹象。
- en: Leakage can also occur when the target variable is linked in a hidden manner
    to predictors by a business practice. For instance, imagine a scenario in which
    a manufacturer attempts to boost customer acquisition rates by building a model
    to predict which customers are most likely to purchase their brand’s car.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量以某种隐藏方式通过业务实践与预测变量相关联时，也可能发生泄露。例如，想象一个场景，一个制造商试图通过建立一个模型来预测哪些客户最有可能购买他们品牌的汽车，以此来提高客户获取率。
- en: It seems reasonable to create a predictor for whether the person has opened
    or clicked on marketing emails, but if the marketing emails were sent only to
    prior customers, then the model is likely to make the common-sense forecast that
    loyal customers tend to stay loyal customers, and the sales agents are unlikely
    to be impressed. Due to the strong connection between the target and the predictor,
    the model essentially ignores the group of people that have never purchased from
    the brand before, which is the group most likely to impact the company’s profits!
    Excluding this predictor from the model, or building the model only based on first-time
    car buyers, would focus the algorithm on the most impactful predictions, or the
    ones that are less inevitable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个预测人员是否打开或点击营销电子邮件的预测器似乎是合理的，但如果营销电子邮件只发送给以前的客户，那么模型很可能会做出常识性的预测，即忠诚的客户往往会保持忠诚，销售人员不太可能印象深刻。由于目标和预测器之间的强烈联系，模型基本上忽略了之前从未购买过该品牌的人群，这是最有可能影响公司利润的人群！从模型中排除这个预测器，或者仅基于首次购车者构建模型，将使算法专注于最具影响力的预测，或者那些不太不可避免的预测。
- en: 'Another factor leading to obvious predictions is related to **autocorrelation**,
    which describes the inertia-like phenomenon in which measurements that are close
    together in time tend to also be close together in value. Based on this observation,
    one can conclude that the best predictor of something today is often the value
    of that same thing yesterday. This is almost universally true: today’s energy
    use, spending, calorie intake, happiness, and virtually anything else one can
    imagine are all closely linked to the state of these things the day prior. Stated
    differently, autocorrelation implies that the long-term variation across people
    or other units of analysis, such as households, businesses, stock values, and
    so on, tends to be greater than the variation within those same units over small
    periods of time.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 导致明显预测的另一个因素与**自相关**有关，它描述了类似惯性的现象，即时间上接近的测量值往往在数值上也接近。基于这一观察，可以得出结论，今天某物的最佳预测通常是昨天该物的价值。这几乎是普遍适用的：今天的能源消耗、支出、卡路里摄入、幸福感以及几乎所有可以想象的东西都与前一天这些事物的状态紧密相关。换句话说，自相关意味着在人们或其他分析单位（如家庭、企业、股票价值等）之间的长期变化往往大于这些单位在较短时间内内的变化。
- en: Machine learning algorithms can quickly identify instances of autocorrelation
    and will happily produce a model that uses yesterday’s values to predict today.
    It will even have high accuracy on the test set, which leads many inexperienced
    analysts to ignore the underlying issue. Specifically, this is merely an overly
    complex way to do list-sorting! If a business wanted to predict the customers
    most likely to spend large amounts tomorrow, they could simply query the database
    for the top-spending customers as of today and sort the list in a simple spreadsheet
    application. In fact, this sorting approach has worked quite well for many years
    under the name **recency, frequency, monetary value** (**RFM**) analysis, which
    was introduced in *Chapter 6*, *Forecasting Numeric Data – Regression Methods*,
    in contrast to a machine learning based approach. The RFM approach basically says
    that the customer who purchases more recently and frequently and spends more money
    is more likely to continue these trends. Unfortunately, this does little to forecast
    which new customers are most likely to become top spenders in the future.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以快速识别自相关实例，并且乐于构建一个使用昨天数据来预测今天情况的模型。它甚至会在测试集上拥有很高的准确率，这导致许多缺乏经验的分析师忽略了潜在的问题。具体来说，这仅仅是一种过度复杂的列表排序方法！如果一家企业想要预测明天最有可能大量消费的客户，他们可以直接查询数据库中今天花费最高的客户，并在简单的电子表格应用程序中对列表进行排序。实际上，这种排序方法在名为**最近性、频率、货币价值**（**RFM**）分析下已经行之有效多年，该分析在*第6章*，*预测数值数据
    – 回归方法*中介绍，与基于机器学习的方法形成对比。RFM方法基本上表明，购买频率更高、购买金额更大的客户更有可能继续这些趋势。不幸的是，这几乎对预测哪些新客户最有可能在未来成为顶级消费者毫无帮助。
- en: Forcing the learning algorithm to tackle this more actionable question involves
    redefining the target variable around the “action.” The target needs to be very
    specific and indicate the exact circumstances in which the model will make an
    impact. In the previous example, rather than modeling total spending, it would
    be more actionable to model the spending increase or decrease over time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 强迫学习算法处理这个更具操作性的问题，需要围绕“行动”重新定义目标变量。目标需要非常具体，并指明模型将产生影响的精确情况。在先前的例子中，与其建模总支出，不如建模随时间变化的支出增加或减少。
- en: This is known as the **delta**, and forecasting the sales delta will allow sales
    agents to intervene before the predicted increase or decrease occurs. Alternatively,
    it is possible to model the intervention’s impact itself; for example, rather
    than predicting the customers most likely to churn, it is better to model the
    customers most likely to respond positively to the churn intervention. Of course,
    this requires historic data that records the attributes of previous anti-churn
    interventions. Often, this type of data is lacking in most businesses.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**delta**，预测销售delta将允许销售代表在预测的增加或减少发生之前进行干预。或者，也可以建模干预本身的影响；例如，与其预测最有可能流失的客户，不如建模最有可能对流失干预措施做出积极反应的客户。当然，这需要记录先前反流失干预措施属性的历史数据。通常，这种类型的数据在大多数企业中是缺乏的。
- en: Conducting fair evaluations
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行公平的评价
- en: It is not uncommon for machine learning projects that performed well on paper
    to perform worse in the real world. This is sometimes related to the problem of
    *conducting unfair evaluations*, and whether it is due to honest oversight or
    intentional deception, this mistake should not exist. Given the results of case
    studies in previous chapters, we know not to assume that training performance
    is an unbiased estimate of future performance. Therefore, we’ve always constructed
    a holdout test set to simulate future unseen data and provide this fair estimate.
    In *Chapter 10*, *Evaluating Model Performance*, we learned that to compare and
    choose between multiple candidate models, we should use a validation dataset so
    that the test set can be “kept in a vault” and remain an unbiased estimate of
    future performance. The underlying issue is that by cherry-picking a model that
    performs best on the test set, one essentially overfits to the test set, and the
    performance is inflated, just as it is when overfitting to the training set. Violating
    the “vault” rule will, unsurprisingly, lead to unexpectedly poor performance models
    in the real world.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，在纸上表现良好但在现实世界中表现不佳的情况并不少见。这有时与*进行不公平的评价*的问题有关，无论是因为疏忽还是故意欺骗，这种错误都不应该存在。鉴于前几章案例研究的成果，我们知道不应假设训练性能是未来性能的无偏估计。因此，我们始终构建一个保留测试集来模拟未来的未见数据，并提供这个公平的估计。在第10章“评估模型性能”中，我们了解到为了比较和选择多个候选模型，我们应该使用验证数据集，这样测试集就可以“存放在保险库中”，并保持对未来性能的无偏估计。根本问题是，通过选择在测试集上表现最佳的模型，本质上是对测试集进行了过拟合，性能被夸大，就像对训练集进行过拟合时一样。违反“保险库”规则，不出所料，会导致现实世界中性能模型出人意料地差。
- en: Perhaps more surprisingly, just as it is possible to overfit to training and
    testing, it is also possible to overfit to the validation set. This is especially
    true when numerous models are built iteratively, or the model is “tuned” extensively
    to identify the optimal parameter values, as will be discussed in *Chapter 14*,
    *Building Better Learners*. The problem is that by repeatedly using the same data,
    some information about the data inevitably “leaks” out to the learning algorithm,
    and it can eventually become overfitted to the validation set.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 也许更令人惊讶的是，正如可能对训练和测试数据进行过拟合一样，也可能对验证集进行过拟合。这尤其适用于在迭代过程中构建了众多模型，或者模型被广泛“调整”以识别最佳参数值的情况，正如将在第14章“构建更好的学习者”中讨论的那样。问题是，通过反复使用相同的数据，关于数据的一些信息不可避免地“泄露”到学习算法中，最终可能导致对验证集的过拟合。
- en: It may help to visualize the procedures of model building, model selection,
    and model evaluation as a sequence of steps, as shown in the following figure.
    During the training step, the algorithm identifies the optimal fit for the data;
    in doing so, it optimizes internal values, known as **parameters**, which are
    the basis of the model abstraction. In some cases, like regression models, neural
    networks, and support vector machines, the parameters are easily visible to the
    end user as coefficients, weights, or support vectors. In other cases, such as
    k-NN, decision trees, and rule learners, the parameters are more conceptual; think
    of the parameters as the internal choices the algorithm made to fit the data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型构建、模型选择和模型评估的过程可视化为一系列步骤可能会有所帮助，如下面的图所示。在训练步骤中，算法确定数据的最佳拟合；在此过程中，它优化了内部值，称为**参数**，这是模型抽象的基础。在某些情况下，例如回归模型、神经网络和支持向量机，参数对最终用户来说是可见的，如系数、权重或支持向量。在其他情况下，例如k-NN、决策树和规则学习器，参数则更具有概念性；将参数视为算法为了拟合数据而做出的内部选择。
- en: In any case, as the model has chosen a single “best” set of parameters to fit
    the training data, any performance estimate is likely to be optimistic, and it
    is likely to perform at least slightly more poorly upon generalization to the
    test set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，由于模型已经选择了一组“最佳”参数来拟合训练数据，任何性能估计都可能过于乐观，并且它在推广到测试集时可能会至少稍微表现得更差。
- en: '![Diagram, logo, company name  Description automatically generated](img/B17290_11_02.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图表、标志、公司名称  自动生成的描述](img/B17290_11_02.png)'
- en: 'Figure 11.2: Because a “best” model is chosen in training and validation, the
    performance estimates tend to be optimistic'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：由于在训练和验证过程中选择了“最佳”模型，性能估计往往过于乐观
- en: The validation dataset is used to select between various types of models, test
    multiple iterations of a single type of model, or both of these simultaneously.
    This can be understood as the process of identifying the optimal **hyperparameters**
    for the learner, or any other parameters that are set outside the learner and
    not estimated by the algorithm itself. Having read the previous chapters, you
    will already be familiar with several hyperparameters, such as the value of *k*
    for the k-NN algorithm, the cost parameter *C* and the kernel for the SVM algorithm,
    and the learning rate and the number of hidden nodes for a neural network, among
    many others. Defined broadly, the notion of a hyperparameter does not only refer
    to choices that directly influence a specific algorithm but also the overall choice
    of algorithm itself, as well as how the algorithm can be combined with others,
    as you will learn later in *Chapter 14*, *Building Better Learners*. For reasons
    that will soon become clear, it may be helpful to consider a “hyperparameter”
    to be any decision that is made outside the learning process and may impact the
    model’s fit.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据集用于在多种类型的模型之间进行选择，测试单一类型模型的多个迭代，或者同时进行这两者。这可以理解为识别学习器最优**超参数**的过程，或者任何设置在学习器外部且不由算法本身估计的参数。在阅读了前面的章节后，你将已经熟悉了几个超参数，例如k-NN算法中*k*的值、SVM算法的成本参数*C*和核函数，以及神经网络的学习率和隐藏节点的数量，等等。广义而言，超参数的概念不仅指直接影响特定算法的选择，还包括算法的整体选择，以及算法如何与其他算法结合，正如你将在第14章“构建更好的学习器”中学习到的那样。由于即将成为明显的原因，将“超参数”视为任何在学习过程之外做出的决策，并且可能影响模型拟合，可能是有帮助的。
- en: Now, suppose you have a validation dataset, and you systematically evaluate
    numerous approaches on this data. You may test a variety of algorithms, like neural
    networks versus decision trees and SVMs, and then test various iterations of each
    of these models using different hyperparameter values. Upon choosing the “best”
    performer out of all of these dozens or hundreds of possibilities, based upon
    the validation set performance, this model’s performance is likely to regress
    when applied to the testing dataset given the potential overfitting to the validation
    set, just as it did between training and validation. We are left wondering whether
    we truly selected the true best model and how robust the performance will be.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你有一个验证数据集，并且你在这个数据集上系统地评估了多种方法。你可能测试各种算法，比如神经网络与决策树和SVMs，然后使用不同的超参数值测试这些模型的各个迭代版本。在所有这些数十或数百种可能性中选择“最佳”表现者，基于验证集的性能，当应用于测试数据集时，这个模型的性能很可能会因为对验证集的潜在过拟合而下降，就像它在训练和验证之间所做的那样。我们留下的问题是，我们是否真正选择了最佳模型，以及性能的稳健性如何。
- en: The 10-fold CV approach, which was introduced in *Chapter 10*, *Evaluating Model
    Performance*, seems at first as if it may solve both problems. Indeed, the practice
    of computing the average and standard deviation of performance across the 10 folds
    does provide a measure of the robustness of model performance as the underlying
    training data changes. This provides a sense of how well a model will generalize
    to future, unseen data. Consequently, a common practice is to run 10-fold CV repeatedly
    on the same dataset to compare various hyperparameters and choose the winner.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章“评估模型性能”中介绍的10折交叉验证方法，乍一看似乎可以解决这两个问题。确实，计算10个折叠的平均值和标准差的做法确实提供了模型性能稳健性的度量，因为底层训练数据发生变化。这提供了关于模型将如何泛化到未来未见数据的感觉。因此，一个常见的做法是在同一数据集上重复运行10折交叉验证，以比较各种超参数并选择胜者。
- en: However, as *Figure 11.3* illustrates, 10-fold CV in its standard form (left)
    does not provide a validation dataset, and thus can only estimate the generalization
    error that arises from the model’s internal search for optimal parameters. For
    example, suppose we compare 10-fold CV performance statistics to help make a decision
    about whether a neural network performs better than a decision tree, or we use
    10-fold CV to determine which of 25 potential values of *C* is best for an SVM
    approach. In both of these cases, notice that we are again cherry-picking the
    winner, which has the consequence of inflating our performance estimate. Ultimately,
    if cross-validation is used with extensive hyperparameter turning, we may be overly
    optimistic about the model’s true future performance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如图11.3所示，标准形式的10折交叉验证（左侧）并不提供验证数据集，因此只能估计模型内部搜索最优参数时产生的泛化误差。例如，假设我们比较10折交叉验证的性能统计信息，以帮助决定神经网络是否比决策树表现更好，或者我们使用10折交叉验证来确定对于SVM方法来说，25个潜在值中的哪一个*C*是最好的。在这两种情况下，请注意我们再次在赢家中进行挑选，这会导致我们的性能估计被夸大。最终，如果使用交叉验证进行广泛的超参数调整，我们可能会对模型的真实未来性能过于乐观。
- en: '![Diagram  Description automatically generated](img/B17290_11_03.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_11_03.png)'
- en: 'Figure 11.3: Nested 10-fold CV adds an “inner loop” of 10-fold CV for learning
    optimal hyperparameters on the validation set'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：嵌套10折交叉验证为在验证集上学习最优超参数添加了一个“内部循环”的10折交叉验证
- en: It is better to think of cross-validation as not merely estimating the ability
    of the model to fit the training data (learning the best parameters) but also
    the entire pipeline of decisions made in the course of choosing the final model
    (learning the best hyperparameters). Suppose we could write an R function that
    automates these decisions; it takes several candidate approaches and chooses the
    best performer among them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最好将交叉验证视为不仅仅是估计模型拟合训练数据的能力（学习最佳参数），还包括在选择最终模型过程中做出的所有决策的整个流程（学习最佳超参数）。假设我们可以编写一个R函数来自动化这些决策；它接受几个候选方法，并从中选择表现最好的一个。
- en: For lack of a more formal term, let’s call this an “assessor” function. In this
    case, we might consider modifying our standard 10-fold CV approach by dividing
    each of the 10 folds into training and validation sets, rather than training and
    test sets. Each model would be evaluated by the assessor for its performance on
    the validation dataset, and the winning model would be chosen by the assessor
    as the one with the best average performance across the 10 folds. At this point,
    we are still left trying to figure out how well the performance will generalize
    to new, unseen datasets in the future.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏一个更正式的术语，让我们称这个为“评估”函数。在这种情况下，我们可能会考虑通过将每个10折中的每个折分为训练集和验证集，而不是训练集和测试集，来修改我们的标准10折交叉验证方法。每个模型将由评估函数评估其在验证数据集上的性能，获胜的模型将由评估函数选择，作为在10个折上平均性能最佳的模型。此时，我们仍然试图弄清楚性能在未来对新、未见过的数据集的泛化程度如何。
- en: As depicted in *Figure 11.3*, the purpose of **nested cross-validation** (typically,
    nested 10-fold CV) is to use cross-validation as an inner loop in which parameters
    are learned on each of the inner folds, and the best hyperparameters are determined
    by an assessor function at each of the folds of an outer loop (also typically
    10-fold CV). Within each of the inner loop folds, any number of models can be
    evaluated by the assessor function on the validation dataset. Perhaps we are evaluating
    three different types of models, such as decision trees, k-NN, and SVMs, or we
    may be evaluating 25 different learning rates for a single neural network. In
    the end, whether we are assessing 3 or 25 models, only the single best one is
    nominated by the assessor function to be sent for evaluation in the outer loop,
    where the 10 best inner loop models’ performance values are averaged on the corresponding
    fold’s test set. This means that nested cross-validation does not measure the
    performance of a single model but the entire process of selecting models and learning
    the best parameters and hyperparameters.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.3*所示，**嵌套交叉验证**（通常是嵌套的10折交叉验证）的目的是将交叉验证作为一个内部循环，在其中每个内部折上学习参数，最佳超参数由外部循环（通常也是10折交叉验证）的每个折上的评估函数确定。在内部循环的每个折中，评估函数可以在验证数据集上评估任意数量的模型。也许我们正在评估三种不同类型的模型，如决策树、k-NN和SVMs，或者我们可能正在评估单个神经网络的25个不同的学习率。最后，无论我们评估的是3个还是25个模型，评估函数只提名一个最佳模型进入外部循环进行评估，在外部循环中，10个最佳内部循环模型的性能值在相应的测试集上平均。这意味着嵌套交叉验证不仅测量单个模型的性能，还测量选择模型、学习最佳参数和超参数的整个过程。
- en: As noted in *Chapter 10*, *Evaluating Model Performance*, given the complexity
    of nested cross-validation—both in implementation and interpretation—standard
    10-fold CV is often good enough for most real-world applications of machine learning.
    On one hand, as the amount of information leak occurring in the validation process
    is relatively minor, the larger the dataset, the smaller the impact will be on
    the analysis. On the other hand, if the performance difference between two models
    is small, it is possible that the magnitude of the overfitting is enough to cause
    the wrong choice to be made. This is especially true as the amount of tuning,
    iteration, and hyperparameterization is increased.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如同在第10章“评估模型性能”中所述，鉴于嵌套交叉验证的复杂性——无论是实现还是解释——标准的10折交叉验证对于大多数机器学习的实际应用通常已经足够。一方面，由于验证过程中发生的信息泄露相对较小，数据集越大，对分析的影响就越小。另一方面，如果两个模型之间的性能差异很小，那么过拟合的程度可能足以导致错误的选择。这一点在调整、迭代和超参数化的数量增加时尤其正确。
- en: Overall, whether nested cross-validation is necessary or overkill for a fair
    evaluation depends much on how the results will be used. For an industry benchmark
    or an academic publication, the more complex nested technique may be justified.
    For lower-stakes work, it may be wiser to choose the simpler standard 10-fold
    cross-validation, and use the time saved to consider how the model will be deployed.
    As will become clear in the next section, it is not uncommon for a model that
    performs well in theory to fail in the real world. Thus, simpler approaches that
    can get to failure faster will allow you more time to course-correct.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，嵌套交叉验证是否必要或过度，很大程度上取决于结果将如何被使用。对于一个行业基准或学术出版物，更复杂的嵌套技术可能是合理的。对于风险较低的工作，选择更简单的标准10折交叉验证可能更明智，并用节省下来的时间来考虑模型如何部署。正如下一节将清楚说明的，一个在理论上表现良好的模型在现实世界中失败并不罕见。因此，能够更快地达到失败点的简单方法将为你提供更多时间进行纠正。
- en: Considering real-world impacts
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑现实世界的影响
- en: 'The adrenaline rush of creating a machine learning project that, by all objective
    measures, appears like it will perform well at its intended task often leads to
    another common pitfall: *being inconsiderate of real-world impacts*. Real-world
    machine learning projects are generally not exercises performed for fun; they
    are typically costly, time-consuming endeavors. The stakeholders that commission
    a machine learning project generally expect a **return on investment** (**ROI**),
    not just for the time and money needed to produce the model but also for the resources
    that will be required to act upon the result. A model that doesn’t work at all
    is a one-time sunk cost, but a model that provides poor recommendations or squanders
    or misdirects the company’s future time and resources is one that throws good
    money after bad. Escalating commitment to a good idea that simply didn’t work
    as intended is the root of the **sunk cost fallacy**, in which one believes that
    a failing project can be saved with more investment. A machine learning project
    used this way is worse than nothing at all.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个机器学习项目，从所有客观指标来看，似乎将在其预期任务中表现良好，这种肾上腺素激增往往会导致另一个常见的陷阱：*不考虑现实世界的影响*。现实世界的机器学习项目通常不是为了娱乐而进行的练习；它们通常是成本高昂、耗时的工作。委托机器学习项目的利益相关者通常期望**投资回报率**（**ROI**），而不仅仅是生产模型所需的时间和金钱，还包括将用于采取行动的资源。一个完全不工作的模型是一次性的沉没成本，但一个提供糟糕建议或浪费或误导公司未来时间和资源的模型，则是将好钱扔到坏钱里。对一个好主意不断投入，而它根本就没有按预期工作，这是**沉没成本谬误**的根源，其中一个人相信可以通过更多的投资来挽救一个失败的项目。以这种方式使用的机器学习项目比什么都没有还要糟糕。
- en: In addition to wasting the resources of the implementation team, it is also
    possible that a project can cause unexpected harm. This is true even if the machine
    learning algorithm is doing what is rational in light of its training. To provide
    one humorous personal example of this possibility, see the following image, which
    shows dozens of mail pieces I received from the same credit card company over
    the period of just a few months. Making matters worse, this isn’t even all of
    them, as I didn’t start collecting them until I realized what was happening! There
    were even some periods when I received a letter every day.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了浪费实施团队的资源外，一个项目还可能造成意外的伤害。即使机器学习算法在其训练的基础上做的是合理的事情，这也是正确的。为了提供一个幽默的个人例子，请看以下图片，它展示了我在短短几个月内从同一家信用卡公司收到的数十封信件。更糟糕的是，这甚至不是全部，因为我直到意识到发生了什么才开始收集它们！甚至有些时候我每天都会收到一封信。
- en: Knowing that banks are generally not inclined to waste money on postage without
    good reason, my suspicion is that a customer acquisition machine learning model
    determined that I would be a valuable customer to acquire, even if it meant spending
    a lot to do so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然知道银行通常不会在没有充分理由的情况下浪费邮资，我的怀疑是，一个客户获取机器学习模型判定我将成为一个值得获取的宝贵客户，即使这意味着花费大量资金。
- en: '![A picture containing text  Description automatically generated](img/B17290_11_04.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片 自动生成描述](img/B17290_11_04.png)'
- en: 'Figure 11.4: A solution that seems reasonable to an algorithm may have negative
    real-world impacts, such as “spamming” end users with solicitations'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：一个算法看起来合理的解决方案可能对现实世界产生负面影响，例如“垃圾邮件”式地向最终用户发送请求
- en: Although I should have been flattered that an algorithm considered me valuable
    enough to mail relentlessly, it may have ultimately tarnished my impression of
    the credit card company. While I cannot be certain that this was not caused by
    some sort of glitch, while it was happening, I couldn’t help but recall what I
    had heard during a presentation from Rayid Ghani, the Chief Data Scientist of
    the 2012 Barack Obama presidential campaign. Specifically, the campaign had run
    thousands of experiments on their email solicitations and, in doing so, discovered
    that the more emails they sent, the more money they made. This occurred with effectively
    no limit.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我应该为算法认为我足够有价值而感到荣幸，不断给我发送邮件，但最终可能损害了我对信用卡公司的印象。虽然我不能确定这不是由某种故障引起的，但在发生时，我不禁回想起我在雷伊德·加尼（Rayid
    Ghani）的演讲中听到的话。具体来说，该竞选活动对其电子邮件征集进行了数千次实验，并在此过程中发现，他们发送的电子邮件越多，赚的钱就越多。这种情况几乎没有上限。
- en: At no point did the number of people unsubscribing from their mailing list outweigh
    the additional donations they received by being constantly atop of email inboxes.
    Perhaps a finding like this explains my flood of paper mail from the credit card
    company; it certainly explains the huge increase in email marketing that consumers
    now see in their inboxes. Only time will tell what the long-term consequences
    of this approach will be.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时候，从其邮件列表中取消订阅的人数都没有超过他们通过不断占据电子邮件收件箱而收到的额外捐款。也许这样的发现可以解释我收到信用卡公司的大量纸质邮件；这当然可以解释消费者现在在他们的收件箱中看到的电子邮件营销的巨大增加。只有时间才能告诉我们这种方法的长期后果是什么。
- en: For more interesting findings discovered during the Obama campaign’s analysis
    of email behavior data, visit [https://www.wired.com/2013/06/dont-dismiss-email-a-case-study-from-the-obama-campaign/](https://www.wired.com/2013/06/dont-dismiss-email-a-case-study-from-the-obama-campaign/)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于奥巴马竞选活动分析电子邮件行为数据时发现的有趣发现，请访问[https://www.wired.com/2013/06/dont-dismiss-email-a-case-study-from-the-obama-campaign/](https://www.wired.com/2013/06/dont-dismiss-email-a-case-study-from-the-obama-campaign/)
- en: 'Rather than leaving it to chance, the single best way to be considerate of
    real-world impacts is to design the machine learning project as a close approximation
    of the ultimate deployment scenario. Simulations, experiments, and small **proof-of-concept**
    (**POC**) trial runs are especially helpful tools to this end. Before the project
    is even considered for deployment, the machine learning practitioner should be
    able to answer typical stakeholder questions such as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是将其留给运气，对现实世界影响表示关心的最佳方式是将机器学习项目设计成与最终部署场景的紧密近似。模拟、实验以及小规模的**概念验证**（**POC**）试验运行是这一目标的特别有用的工具。在项目甚至被考虑部署之前，机器学习实践者应该能够回答典型的利益相关者问题，例如：
- en: How many dollars, lives, widgets, and so on, can be saved using this model?
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个模型可以节省多少美元、生命、小工具等等？
- en: How many “misses” will happen for every successful prediction?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一次成功的预测，会有多少“失误”发生？
- en: Is the ROI dependent on high-risk, high-reward events, or does it accumulate
    smaller, slow-and-steady wins?
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投资回报率（ROI）是否依赖于高风险、高回报的事件，还是它积累的是较小的、缓慢而稳定的胜利？
- en: Is the model’s performance markedly better on certain types of examples, or
    does it perform evenly well across the full set?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能在某些类型的示例上是否明显更好，或者它在整个集合中表现均匀？
- en: Does the model systematically favor or ignore categories of interest, such as
    protected age, race, or ethnic groups, geographic regions, or customer segments?
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否系统地偏向或忽略感兴趣的类别，例如受保护的年龄、种族或民族群体、地理区域或客户细分？
- en: What are the potential unintended consequences? Can the algorithm cause harm,
    or be exploited by bad actors to do so?
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会有哪些意想不到的后果？算法是否可能造成伤害，或者被不良行为者利用来造成伤害？
- en: Cleverly designed projects that include a simulated deployment and an estimated
    ROI calculation will help provide data to answer these questions. In these projects,
    it is important to not simply calculate accuracy but also take a step further,
    and compute a number that describes how this accuracy translates into a real-world
    impact once the model has been deployed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 设计巧妙的项目，包括模拟部署和估计投资回报率（ROI）的计算，将有助于提供数据来回答这些问题。在这些项目中，重要的是不仅要计算准确性，还要更进一步，计算一个数字，描述一旦模型部署后这种准确性如何转化为现实世界的影响。
- en: Making a fair comparison requires enough business knowledge to construct or
    identify an appropriate **control group**—the existing status quo or situation
    that serves as a benchmark or baseline frame of reference.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 进行公平的比较需要足够的商业知识来构建或识别一个合适的**对照组**——现有的现状或作为基准或基线参照框架的情况。
- en: For a cancer identification model, for example, one might estimate the number
    of lives saved using the model’s predictions, and then compare this to a baseline
    comprising the lives saved using traditional human diagnosis. In cases where the
    intervention of a machine learning model has no obvious frame of reference, the
    baseline may be a scenario using no model at all, one in which the outcome is
    essentially decided at random, or one decided using a “dumb” model that always
    picks the majority class or predicts the mean value. You may also use a simple
    model like the OneR rule learning algorithm discussed in *Chapter 5*, *Divide
    and Conquer – Classification Using Decision Trees and Rules*, to simulate a simple
    rule-of-thumb heuristic.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个癌症识别模型，例如，人们可能会估计使用该模型的预测结果所挽救的生命数量，然后将其与使用传统人工诊断所挽救的生命数量进行比较。在机器学习模型的干预没有明显参照框架的情况下，基线可能是一个完全不使用模型的情况，其结果基本上是随机决定的，或者使用一个“愚蠢”的模型，该模型总是选择多数类或预测平均值。你也可以使用像第5章中讨论的OneR规则学习算法这样的简单模型来模拟一个简单的经验法则。
- en: Naturally, the real world is extremely complex and constantly changing, and
    thus it is important to not only estimate the impact of a model but also examine
    how robust its impact is under various constraints. Some of these constraints
    may be ethical, such as the need to ensure that it performs evenly well across
    subgroups of interest. In a business environment, these important subgroups may
    consist of categories like age, race, ethnicity, gender, and economic status;
    in medical contexts, these may be based additionally on health characteristics
    like body mass index and whether the subject smokes. It is up to the analyst to
    determine what real-world contexts are most important to evaluate for performance.
    Once these have been decided, the analyst can make predictions on each of the
    subgroups and compare the model’s performance across the groups, checking for
    bias reflected in systematically over-performing or underperforming groups.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，现实世界极其复杂且不断变化，因此，不仅要估计模型的影响，还要检查其在各种约束下的影响稳健性。这些约束中的一些可能是伦理方面的，例如确保它在感兴趣的子群体中表现均匀。在商业环境中，这些重要的子群体可能包括年龄、种族、民族、性别和经济状况等类别；在医疗环境中，这些可能还包括基于健康特征，如体重指数和受试者是否吸烟。分析员需要确定哪些现实世界情境对于评估性能最为重要。一旦这些情境确定，分析员就可以对每个子群体进行预测，并比较模型在不同群体中的性能，检查是否存在系统性过度表现或表现不佳的群体所反映的偏差。
- en: Aside from variation across subgroups, a model’s impact may also vary in the
    real world due to changes over time in the data used for training or evaluation.
    In practice, 10-fold CV and even the more sophisticated nested cross-validation
    variant oversimplify many aspects of how machine learning models are built and
    deployed in the real world. They do not reflect many potential external factors,
    beyond mere variations in the training data, which may influence a model’s future
    performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了子群体之间的差异外，由于用于训练或评估的数据随时间变化，模型在现实世界中的影响也可能发生变化。在实践中，10折交叉验证甚至更复杂的嵌套交叉验证变体过于简化了机器学习模型在现实世界中构建和部署的许多方面。它们没有反映许多潜在的外部因素，而不仅仅是训练数据的变异，这些因素可能会影响模型未来的性能。
- en: To help understand these other factors, *Figure 11.5* provides a simplified
    view of how models are generally built and evaluated in most real-world environments.
    It imagines a data stream composed of the entities for which the model is to make
    its predictions; you might imagine this as a single-file line of potential cancer
    patients, customers, loan applicants, and so on. To forecast their future outcomes,
    we generally take a snapshot of this data stream today, at a single point in historical
    time, or record observations for a period of time, and then divide this data into
    separate sets for training, validation, and testing—possibly using 10-fold CV
    or similar methods. The performance estimates from models built and evaluated
    on this snapshot are assumed to be reasonable estimates of future performance,
    but we cannot be certain until the future eventually happens.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解这些其他因素，*图11.5* 提供了在大多数现实世界环境中模型通常是如何构建和评估的简化视图。它想象了一个由模型需要预测其预测的实体组成的数据流；你可以想象这是一行潜在癌症患者、客户、贷款申请人等等的单文件。为了预测他们的未来结果，我们通常会在今天的历史时间点对这一数据流进行快照，或者记录一段时间内的观察结果，然后将这些数据分成用于训练、验证和测试的单独集合——可能使用10折交叉验证或类似方法。从这个快照上构建和评估的模型性能估计被认为是未来性能的合理估计，但我们不能确定，除非未来最终发生。
- en: Of course, in an ideal scenario, we would simply build our model on today’s
    data (or past data) and then wait some time for the “future” to occur and conduct
    the evaluation, but this is very rare in practice. Consider yourself fortunate
    if your business had the foresight to gather sufficient historical data or has
    the patience to wait while it is gathered; in many cases, business moves too quickly
    and resources are too scarce for this to be true.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在一个理想场景中，我们只需在今天的数据（或过去的数据）上构建我们的模型，然后等待一段时间“未来”发生并执行评估，但在实践中这非常罕见。如果你所在的企业有远见卓识，能够收集足够的历史数据，或者有耐心等待数据收集，那么你就很幸运了；在许多情况下，商业发展得太快，资源又太稀缺，以至于这种情况不成立。
- en: '![](img/B17290_11_05.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_11_05.png)'
- en: 'Figure 11.5: The data used to train and evaluate a model is often from a snapshot
    in time much earlier than the time of deployment'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：用于训练和评估模型的通常是早于部署时间的数据快照
- en: Even when a model can be evaluated on truly never-before-seen future data, the
    relentlessness of time and the data stream often cause problems after deployment
    that are difficult to foresee ahead of time. Specifically, as the real world is
    constantly changing, deployed machine learning projects tend to be subject to
    **model decay**, a term that describes the common phenomenon in which their performance
    deteriorates over time after implementation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型可以在真正从未见过的未来数据上进行评估，时间的无情和数据流的持续往往会在部署后造成难以预见的问题。具体来说，由于现实世界不断变化，部署的机器学习项目往往会受到**模型退化**的影响，这是一个描述在实施后随着时间的推移其性能逐渐下降的常见现象的术语。
- en: Sometimes, this is due to systematic changes in the input data over time, known
    as **data drift**, which can happen due to cyclical patterns like purchasing behavior
    or disease spread, which vary by season, as well as changes in the meaning or
    magnitude of the data itself. Data drift occurs after changing the scale on which
    something was measured, as in switching from a scale from 1 to 5 to 1 to 10, as
    well as inflating the values overall, which occurs with currency inflation. It
    can also occur subtly even without changes in the attribute values themselves
    if, for instance, a survey question’s wording changed.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，这可能是由于输入数据随时间发生的系统性变化，称为**数据漂移**，这可能是由于购买行为或疾病传播等周期性模式，这些模式随季节变化，以及数据本身的意义或幅度的变化。数据漂移发生在改变测量某物的尺度之后，例如从1到5的尺度切换到1到10，以及整体膨胀，如货币通货膨胀。即使属性值本身没有变化，也可能发生微妙的变化，例如，如果调查问题的措辞发生了变化。
- en: Perhaps the survey used a value of three to indicate “neither agree nor disagree”
    but was later translated into another language as “no opinion.” This drift in
    meaning over time may contribute to degraded performance over time, but it can
    be somewhat mitigated with strict maintenance of codes and definitions—easier
    said than done!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 也许调查使用了三个值来表示“既不同意也不反对”，但后来被翻译成另一种语言为“没有意见”。这种随时间推移的意义漂移可能会随着时间的推移导致性能下降，但可以通过严格维护代码和定义来在一定程度上缓解——说起来容易做起来难！
- en: Another contributor to model decay is **model drift**, which occurs when the
    relationship between the target and the predictors changes over time, even if
    the meaning of the underlying data remains constant. This generally reflects a
    change external to the model, or an external force that would have been difficult
    to foresee. For example, there may be broad changes to the economy or customer
    behavior and preferences, evolutionary changes in how a disease behaves, or other
    such factors that fundamentally disrupt the patterns the learning algorithm discovered
    during training. Fortunately, model drift can be remedied via more frequent training—the
    model simply learns the new patterns—but this leads to confusion and further complexity,
    as it is not clear how frequently or infrequently one should retrain the model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 模型退化另一个贡献者是**模型漂移**，这发生在目标变量与预测变量之间的关系随时间变化时，即使底层数据的含义保持不变。这通常反映了模型外部的变化，或者是一个难以预见的外部力量。例如，经济或客户行为和偏好的广泛变化，疾病行为的进化变化，或其他会从根本上破坏学习算法在训练期间发现的模式的因素。幸运的是，模型漂移可以通过更频繁的训练来纠正——模型只是学习新的模式——但这会导致困惑和进一步的复杂性，因为它并不清楚应该多频繁或多久重新训练模型。
- en: Although one might think that frequent or nearly-real-time training is always
    best, this substantially increases the complexity of deployment and can lead to
    increased variability in the model’s predictions over time, and thus can contribute
    to a lack of trust in the model’s output—a problem described in the next section.
    Perhaps the best approach is to experiment and identify a schedule to refresh
    models that works best for the specific use case, such as annually, seasonally,
    or upon suspicion of data drift. Then, closely monitor the results and refine
    as needed. As usual, there is no such thing as a free lunch!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人们可能会认为频繁或近乎实时的训练总是最好的，但这实际上大大增加了部署的复杂性，并且可能导致模型预测随时间增加的变异性，从而可能导致对模型输出的不信任——这是下一节中描述的问题。或许最好的方法是进行实验，确定一个最适合特定用例的模型刷新计划，比如每年、季节性或是在怀疑数据漂移时。然后，密切监控结果并根据需要调整。像往常一样，没有免费的午餐！
- en: Building trust in the model
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立对模型的信任
- en: The final category of pitfall contributing to failed data science projects has
    little to do with the technical details of model implementation or the performance
    of the model itself; instead, it stems from a fundamental *lack of trust* in the
    project from key stakeholders. This pitfall is especially burnout-inducing because
    it occurs so late in the workflow. It is disheartening to invest countless hours
    in a project only to see it fail to gain traction with the stakeholders who asked
    for it, or the end users that would benefit most from its implementation. As frustrating
    as this sounds, it is even more troubling to hear the raw statistics about the
    problem’s magnitude; a quick web search reveals a variety of estimates—none of
    them good—of the proportion of machine learning projects that make it into production.
    Some firms estimate the number of failed machine learning projects at over 60
    percent, while other estimates are as high as a staggering 85 percent.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 导致数据科学项目失败的陷阱的最后一类与模型实现的细节或模型本身的性能关系不大；相反，它源于关键利益相关者对项目的基本**不信任**。这个陷阱特别容易导致燃尽，因为它发生在工作流程的后期。在投入无数小时的项目中，却看到它未能获得请求它的利益相关者或最有可能从其实施中受益的最终用户的认可，这是令人沮丧的。尽管听起来很令人沮丧，但听到关于问题严重性的原始统计数据更是令人不安；快速的网络搜索揭示了关于机器学习项目进入生产的比例的各种估计——没有一个是好的。一些公司估计失败的机器学习项目数量超过60%，而其他估计高达惊人的85%。
- en: How can it be possible that only around 15 percent of projects are ever launched
    successfully? Even using the more optimistic estimate, less than half will be
    implemented! Can this possibly be true? Unfortunately, if this seems to be an
    impossibly low number, you are unlikely to have worked in the field of machine
    learning for very long, or are one of the lucky few to work for a company that
    has found a solution to this epidemic of failed projects. Instead, most practitioners
    follow a similar pattern in which their efforts continually fall flat in an organization,
    and thus they look for another workplace where they can make a bigger impact.
    As most organizations suffer from the same issue, the cycle of discontent inevitably
    begins again soon.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 怎么可能只有大约15%的项目能够成功启动？即使使用更乐观的估计，也少于一半的项目会被实施！这可能是真的吗？不幸的是，如果这个数字看起来不可思议地低，你很可能在机器学习领域工作的时间不长，或者你是那些幸运的少数之一，在一家已经找到解决这个失败项目流行病的方法的公司工作。相反，大多数从业者遵循类似的模式，他们的努力在组织中不断受挫，因此他们寻找另一个可以产生更大影响的工作场所。由于大多数组织都存在同样的问题，不满的循环不可避免地很快就会再次开始。
- en: The reasons for this failure to launch are myriad, and it is tempting to place
    the blame solely on the stakeholders who often have unrealistic expectations of
    the upside of machine learning or the costs and resources needed to follow a project
    through to completion. Perhaps they bought into the hype surrounding artificial
    intelligence and expected it to be plug-and-play with minimal investment. Under-resourced
    information technology teams are, after all, not a recent phenomenon. This being
    said, there is much that machine learning practitioners can do to proactively
    build the stakeholders’ trust in the project and make it much more likely to take
    root. A key part of building trust in modeling projects is recognizing that machine
    learning is both an art and a science. With experience comes the understanding
    that while soft skills are not necessarily fundamental to performing the work,
    they are virtually essential to its ultimate success.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 导致这次失败启动的原因多种多样，很容易将责任归咎于那些常常对机器学习的潜在收益抱有不切实际的期望、对完成项目所需的成本和资源缺乏了解的利益相关者。也许他们被围绕人工智能的炒作所吸引，并期望它能够以最小的投资实现即插即用。毕竟，资源不足的信息技术团队并非最近才出现。然而，机器学习从业者可以采取许多措施来积极建立利益相关者对项目的信任，并大大增加项目成功扎根的可能性。在建立对建模项目的信任方面，一个关键部分是认识到机器学习既是艺术也是科学。随着经验的积累，人们会理解虽然软技能并非执行工作的根本，但它们对于工作的最终成功几乎是必不可少的。
- en: There is a lot that machine learning practitioners can learn from the study
    of magicians or illusionists, particularly with respect to showmanship. Of course,
    that is not to say in any way that the work itself should be phony—you don’t want
    to be a snake oil salesman—rather, consider the fact that to the end user, machine
    learning and artificial intelligence are a black box that might as well be magic.
    If it works as intended, it undoubtedly stirs a magical feeling. Being heavily
    invested in building the tool, the practitioner may not even realize this.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从业者可以从对魔术师或幻术师的研究中学习到很多，特别是在表演技巧方面。当然，这并不是说他们的工作本身应该是虚假的——你不想成为一个卖假药的人——相反，考虑这样一个事实：对于最终用户来说，机器学习和人工智能可能就像魔法一样是一个黑盒子。如果它按预期工作，无疑会激发一种神奇的感觉。由于对构建工具投入了大量精力，从业者可能甚至没有意识到这一点。
- en: As noted by David Copperfield, the most commercially successful magician in
    world history, “*magicians lose the opportunity to experience a sense of wonder*.”
    Savvy practitioners will take advantage of the magical allure of machine learning
    and identify early-adopter “champions” who will promote the work and help it take
    root in the organization. These stakeholders will know more about business operations,
    and including such stakeholders in the process of building the model, particularly
    during the phases of gathering data and translating the model into action, will
    provide an end user perspective and help ensure that the project will eventually
    be useful rather than merely interesting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如世界历史上最成功的商业魔术师大卫·科波菲尔所说，“*魔术师失去了体验惊奇感的机会*。”精明的从业者将利用机器学习的神奇吸引力，并确定早期采用“倡导者”，他们将推广这项工作并帮助它在组织中扎根。这些利益相关者将更多地了解业务运营，在构建模型的过程中包括这样的利益相关者，尤其是在收集数据和将模型转化为行动的阶段，将提供最终用户视角，并有助于确保项目最终是有用的而不是仅仅有趣的。
- en: Most successful magicians take their show on the road to audiences all over
    the world. In machine learning, this is beneficial not only to sell the project
    to ever-growing audiences of stakeholders but also to gather feedback on what
    will work in practice and what will not. During this road-show period, it is wise
    to craft an elevator pitch, or a short two-or three-sentence description of the
    project that could be given in a brief elevator ride. Honing this pitch by repeatedly
    practicing it in one-on-one settings with potential end users will not only improve
    your ability to deliver the pitch but also assist with identifying additional
    champions of the project and gathering success stories. The successes can later
    be peppered into future conversations to build even greater buzz around the project.
    Of course, be sure to use the appropriate level of detail for the audience. Paraphrasing
    famous sleight-of-hand magician Jerry Andrus, it’s our job to dazzle and “fool”
    the audience, but not to make them feel like fools.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数成功的魔术师都会将他们的表演带到世界各地的观众面前。在机器学习中，这不仅有利于向不断增长的利益相关者受众推销项目，而且还有助于收集关于哪些做法可行、哪些不可行的反馈。在这个巡回演出期间，制作一个电梯演讲稿，或者一个简短的两到三句话的项目描述，这在简短的电梯之旅中可以提供，是明智的。通过在一对一的场合反复练习，与潜在最终用户进行练习，不仅可以提高你提供演讲稿的能力，还可以帮助识别项目的其他支持者，并收集成功故事。这些成功故事可以后来穿插到未来的对话中，以在项目周围营造更大的轰动。当然，确保为受众提供适当的细节水平。借用著名的手法魔术师杰里·安德鲁斯的名言，我们的工作是让受众眼花缭乱，让他们“上当”，但不是让他们觉得自己是傻瓜。
- en: Questions, criticisms, and even outright negativity can be helpful; these can
    lead to improvements in the project or can be added to an FAQ document or a slide
    deck, which can be presented to larger audiences. During the first few large audience
    presentations, incorporating success stories from audience members that are known
    to be in attendance, or even planting a champion in the audience to ask predefined
    questions, can contribute to trust in the project, much like a magician often
    plants an associate in the audience to “volunteer” for the illusion. Over time,
    you will begin to recognize similar questions and criticisms and preemptively
    address them or, even better, provide just enough information to allow the audience
    members to discover the answer for themselves. The trick of leaving some of the
    project faults in plain sight, and revealing them to the audience seemingly by
    accident, can be especially effective on known detractors. This is especially
    powerful because, according to Teller, of the long-running Las Vegas performing
    duo Penn and Teller, “*when a magician lets you notice something on your own,
    his lie becomes impenetrable*.”
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 问题、批评，甚至直接的负面评论都可能是有益的；这些可以促进项目的改进，也可以添加到FAQ文档或演示文稿中，以便向更广泛的受众展示。在最初的几次面向大型受众的演示中，引入已知出席的观众的成功故事，或者甚至在观众中培养一个支持者来提出预定义的问题，可以增强项目在受众中的信任度，就像魔术师经常在观众中培养一个“志愿者”来“自愿”参与魔术一样。随着时间的推移，你将开始识别类似的问题和批评，并提前解决它们，或者更好的是，提供足够的信息，让受众自己发现答案。将项目的一些缺陷公之于众，并似乎意外地向观众展示，对于已知的批评者来说，这种方法可能特别有效。这尤其强大，因为根据长期在拉斯维加斯表演的搭档彭斯和泰勒的说法，“*当魔术师让你自己注意到某件事时，他的谎言就变得无法穿透*。”
- en: Sometimes, rather than glitz and showmanship, the path to building trust is
    to use statistical tools and methods to provide a more intuitive means of acting
    upon the model’s predictions. For example, suppose a model has been built to predict
    whether someone will develop lung cancer. Since lung cancer is relatively rare
    in the overall population (roughly 1 in 16 Americans will develop it in their
    lifetime), many models will tend toward low predicted probabilities of cancer,
    even among those most likely to develop cancer. For a person who is eight times
    more likely than the average person to develop lung cancer, it is still only a
    coin flip overall on whether they will develop it in their lifetime, and thus
    a reasonable prediction for this person may still be “no cancer.” Given this person’s
    high relative risk, early intervention may be impactful, but a prediction that
    simply states “no cancer” is of no use to distinguish this person from the others
    with much lower risk; likewise, a predicted probability of 49.999 percent is of
    little use without knowledge that the baseline cancer probability is 6.25 percent.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，建立信任的道路不是通过华丽和表演，而是使用统计工具和方法提供一种更直观的方式来采取基于模型的预测的行动。例如，假设已经建立了一个模型来预测某人是否会患上肺癌。由于肺癌在总体人群中相对罕见（大约每16个美国人中就有1个会在其一生中患上肺癌），许多模型将倾向于预测癌症的低概率，即使在最有可能患上肺癌的人群中也是如此。对于一个比普通人有8倍可能性患上肺癌的人来说，他们是否会在一生中患上肺癌仍然是一个硬币的两面，因此对于这个人来说，合理的预测可能仍然是“无癌症”。鉴于这个人的高相对风险，早期干预可能是有影响力的，但仅仅声明“无癌症”的预测对于区分这个人与风险较低的其他人没有帮助；同样，如果没有了解基线癌症概率为6.25%，那么49.999%的预测概率几乎没有用处。
- en: As many machine learning projects focus on rare events, translating the raw
    predicted probabilities into categories of relative risk can help drive trust
    and action. For the lung cancer model described previously, this may mean creating
    a red, yellow, and green “stoplight” system in which red indicates the highest
    possible risk, yellow indicates a moderate risk, and green indicates a low risk
    of developing the disease. These red, yellow, and green labels can then be presented
    directly in a report or dashboard and understood intuitively by the agents that
    need to act upon this information.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多机器学习项目专注于罕见事件，将原始预测概率转换为相对风险类别可以帮助建立信任并驱动行动。对于之前描述的肺癌模型，这可能意味着创建一个红灯、黄灯和绿灯的“交通灯”系统，其中红灯表示最高风险，黄灯表示中等风险，绿灯表示低风险。这些红、黄、绿标签可以直接在报告或仪表板上展示，并能够被需要采取行动的代理人员直观理解。
- en: For models intended for other use cases, other formats may be more appropriate
    to drive action. One business might use a primary school A, B, C, D, or E letter
    grade system for loan applicants, another might transform raw predicted churn
    probabilities in to a percentile-based system that ranks customers on their likelihood
    to churn, and some may even warrant more complex presentations that include not
    only a prediction but also a confidence rating. Part of the benefit of the road
    show is that, hopefully, while dazzling audiences with the predictive power of
    the model, you have also gained a sense of what output format will drive them
    to use it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旨在其他用例的模型，其他格式可能更合适以驱动行动。一家企业可能会使用小学的A、B、C、D或E等级制度来评估贷款申请人，另一家企业可能会将原始预测的流失概率转换为基于百分比的系统，按客户流失的可能性对客户进行排名，而一些模型可能甚至需要更复杂的展示方式，不仅包括预测，还包括置信度评级。路演的一部分好处是，在用模型的预测能力眩惑观众的同时，你也许已经获得了关于哪种输出格式能驱动他们使用它的感觉。
- en: Even if end user adoption is high, or perhaps *especially* if end user adoption
    is high, there will be predictions that are either counterintuitive or appear
    to be nonsensical. Occasionally, end users are merely curious about why a specific
    prediction was made. These raise questions that can only be answered by a dive
    under the hood of the model itself, which is virtually impossible for black-box
    models like neural networks and challenging even for simpler approaches like regression
    and decision trees, especially when these models have been applied to large and
    complex datasets. These differences get at the notion of **model interpretability**,
    which is the ability for a human to understand how a model works. If a model is
    simple and transparent, it will be readily understood, and stakeholders will tend
    to trust it.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Closely related to interpretability, which describes generally *how* a model
    makes predictions, it is also important to understand *why* a specific prediction
    was made. The growing field of **model explainability** involves the development
    of methods that can be used to probe models, to develop a simplified or intuitive
    understanding of the factors the prediction was based on. Model explainability
    may be currently one of the fastest-evolving subsections of machine learning and
    artificial intelligence due to the rapid adoption of deep learning models, which
    are notoriously impossible to interpret, yet are used in fields like finance and
    medicine, which have strict requirements for model explainability. Model explainability
    tools allow powerful but uninterpretable models to be used even for applications
    where it is crucial to justify the decisions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Model explainability is a rapidly advancing field of study, and new methods
    and best practices are being discovered regularly. One promising technique, called
    **Shapley Additive Explanations** (**SHAP**), uses principles from game theory
    to allocate credit for a prediction to individual features that are most responsible
    for the predicted value. This is a more challenging task than it may seem at first
    because, for complex models, a given feature might not have a simple, linear impact
    on the outcome. Instead, the feature’s impact may also depend on the value of
    the other features, which themselves may have different impacts depending on how
    they are combined. Because it is computationally expensive to compute all possible
    permutations, most SHAP implementations use heuristics that simplify this computation,
    and the average impact of each feature is measured across all possibilities.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: SHAP implementations for R are available in the `shapr` and `shapper` packages,
    but the most active development work is in Python in the `shap` package. The documentation
    for this package is an outstanding resource to learn the fundamentals of SHAP,
    even if you plan to work in R. It can be found on the web at [https://shap.readthedocs.io](https://shap.readthedocs.io)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Because machine learning, at its core, is about turning data into action, explainability
    tools help build trust in a model, which leads to increased adoption and greater
    impact. For instance, a model that identifies a hospital patient to have a high
    risk of mortality will cause more harm than good unless we know what factors are
    causing the risk to be elevated. Without an explanation, a patient will be in
    fear for no reason. On the other hand, knowing that the risk is due to preventable
    factors will lead to interventions that save lives. Making these connections between
    the model and the real world is up to the practitioner. Thus, much in the same
    way that explainability techniques can lead to more effective models, your own
    practices and storytelling skills can contribute to the project’s success, as
    you will see in the sections that follow.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Putting the “science” in data science
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the time since the first edition of *Machine Learning with R* was published,
    a new phrase has become somewhat ubiquitous within the field of machine learning.
    That buzzword, of course, is **data science**—a term that has been defined by
    many but is generally agreed to describe a field of work or study encapsulating
    aspects of statistics, data preparation and visualization, subject-matter expertise,
    as well as machine learning.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: It is debatable whether data science is synonymous with what used to be called
    data mining, but it is safe to assume that there is a lot of overlap between the
    two. A reasonable outsider might observe that data science is simply a more formalized
    version of data mining. The methods and techniques in data mining were often learned
    informally on the job or passed between practitioners at industry events. This
    is in stark contrast to the field of data science, which offers countless opportunities
    to earn formal credentials and experience via online training courses and in-person
    degree programs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in *Figure 11.6*, a search of Google Trends data suggests that the
    term truly began to grow in popularity just as the first edition of this book
    was published. While I would love to take credit for popularizing the phrase,
    unfortunately, I cannot do so, as it barely appeared in the first edition at all!
    Of only two appearances of the phrase in the text, the most notable appearance
    was literally on the book’s final page, where I wrote briefly about the “burgeoning”
    data science community. If only I had known how true this would be! At least I
    was in good company; even the Wikipedia page for data science was just in its
    infancy in 2012 when the earliest pages of this book’s first edition were being
    written.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with low confidence](img/B17290_11_06.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: A Google Trends search shows the rapid rise of “data science”
    in the past decade'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Since then, my role as a machine learning practitioner in the workforce, like
    the roles of many others around the world at the time, was transformed from the
    title of data analyst in to data scientist. Yet, despite the rapid change in title
    and perception, it seems that the work itself changed very little. Applied machine
    learning was essentially just data mining, historically, and today’s data scientists
    are expected to use statistics and machine learning, as well as a strong hacker
    or tinkerer’s work ethic, to find useful insights in data—just like the data miners
    of yore. What made this new field of data science different from what we previously
    did?
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Numerous blogs and news publications attempted to answer this question on the
    road to understanding the hype and why data science suddenly became one of the
    “hottest new career fields of the 21^(st) century,” as it was so often called.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Early in this trend, a common theme was to use a Venn diagram to illustrate
    the requisite skills. As shown in the following Bing image search, this depiction
    of data science was and continues to be pervasive, and the Venn diagram visualization
    has practically reached meme-like status. There are subtle variations, but most
    share the same overall structure: data science is found at the intersection of
    computer science, statistics, and domain expertise.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, chart  Description automatically generated](img/B17290_11_07.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: The data science Venn diagram has reached meme-like status; most
    place data science at the intersection of programming, statistics, and domain
    expertise'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with this type of reductionist conception is that while it captures
    the broad strokes of data science, it misses the soul and key distinction: a scientific
    mindset. One can have the requisite skills of statistics, programming, and domain
    knowledge, but if the work is treated without scientific rigor, then it is no
    different than the data mining of years prior. To be clear, this is not to say
    that data mining as it was performed before was useless even then, but it is certainly
    doubtful that it was deemed scientific by anybody who knew how the work was performed
    in the trenches.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we put the “science” into data science? Answering this question involves
    realizing why the science matters now when it may not have previously. In particular,
    the operationalization of data science occurred as organizations of all sizes
    and across many domains began rapidly staffing and resourcing business intelligence
    teams to find insights in the so-called “treasure trove” of big data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: These growing layers of complexity necessitated more sophisticated tools and
    processes to coordinate efforts and link findings across parts of an organization.
    When one or two people were doing data mining in the dark recesses of a business,
    it was not essential to be very scientific, but as the teams and tools grew, a
    more methodical approach was warranted to avoid having the effort devolve into
    chaos.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Knowing that data science at its core is a team sport, it is also important
    to incorporate elements of the scientific method into solo machine learning projects.
    Small machine learning projects can quickly grow in complexity, even for relatively
    simple tasks, via the use of trial and error and iteration, which are essential
    to the scientific method. *Figure 11.8* is intended to illustrate some of the
    dead ends and tangents that one explores during a rigorous machine learning project.
    Hypotheses are generated and examined during data exploration, only some of which
    prove to be fruitful. These insights inform feature engineering, which itself
    may have multiple false starts. Several models are tested; some fail, while others
    can be used to springboard more sophisticated models. Ultimately, the most promising
    models are evaluated, and then tuned for better performance in concert with additional
    feature engineering before deployment. From start to finish, the entire sequence
    can take days, weeks, or even months and years for complex, real-world projects.
    Recognizing the fits and starts of this process as natural steps in the scientific
    method helps communicate to stakeholders that progress isn’t always a straight
    line forward, proportional to time invested.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17290_11_08.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: Machine learning projects rarely proceed in a straight line from
    start to finish'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, it is important for you, the data science practitioner, to recognize
    that your work will not proceed in a linear fashion. Unlike machine learning tutorials
    in books and on the web, the messy complexity of real-world projects requires
    more careful attention to avoid getting lost in code or reinventing the wheel.
    No longer is it sufficient to create a single R code file, which is executed line
    by line by hand. Instead, we assemble our code and output in a single, well-organized
    place, in a form that we hope will also serve as an artifact of our investigation
    for future readers or our future, forgetful selves. Thankfully, R and RStudio
    make this work seamless.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Using R Notebooks and R Markdown
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upon completion of a large machine learning project, after letting out a sigh
    of relief, you may find yourself looking back and wondering where the time went.
    Dwelling on this question for too long may lead to insecurity, as inevitably,
    you may start to ask whether you might have avoided some of the more obvious mistakes,
    or perhaps made different design choices. “If only!” you may find yourself saying
    repeatedly. How is it possible that a project seemingly so simple in hindsight
    consumed so much time and effort?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: This question stems from a newfound perspective atop the summit of a difficult
    data analysis project, with a clear view of the outcome. Recall the meandering
    pathway of a typical machine learning project depicted in *Figure 11.8* in the
    previous section. At the project’s completion, we tend to forget the numerous
    time-consuming dead ends and false starts and simplify the journey as a straight
    line from start to finish, rather than recalling the convoluted pathway it actually
    took.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Early in a data science career, people tend to assume that there is a way to
    avoid these detours and jump straight to the conclusion without “wasting” so much
    time chasing pointless leads. There isn’t. This work is not in vain but is an
    essential part of the machine learning process. The work is not wasted at all;
    as you become smarter about the data, the machine likewise will become smarter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Later in a data science career, you may recognize that these initial exploratory
    ventures are a necessary step in all projects. However, you may have a suspicion
    that the work is not as impactful as it could be. While data exploration may inform
    a single analysis, it doesn’t seem to make a lasting impression, and the same
    mistakes are often repeated. Part of this may relate to the fact that it is much
    easier to recall what worked than what didn’t work. Successes stick in one’s mind
    while failures are forgotten and, in many cases, literally deleted from the R
    code file. In this way, the exploratory work doesn’t seem to accumulate in the
    same way that other experiences do. Failures, hypotheses ruled out, and paths
    not taken are, therefore, not easily remembered and not easily transferred to
    others to build historical knowledge about a project’s roots. This is to the detriment
    of your future self, or others who may take over your code upon your retirement.
    Rather than deleting everything except the final, clean solution, it would be
    better to have a way to present the full investigation—dead ends, mistakes, and
    all.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The RStudio development environment provides a solution to this problem in the
    form of **R notebooks**, which are a special type of R code file that combines
    both R code and explanatory free-form text. These notebooks can easily be compiled
    into HTML, PDF, or Microsoft Word formats, or even slideshows and books with a
    bit more effort. The resulting output document embeds code within a report’s text,
    or text within code, depending on your perspective.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: This provides an artifact that can be used to document the entire machine learning
    process from start to finish, yet it doesn’t feel tedious due to the fact that
    the code can still be run interactively line by line or block by block during
    development. By spending a little extra time to add explanatory or contextual
    documentation to the R code file, the result is a report that can be shared with
    others or reviewed by your future self to refresh your memory.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'R notebooks are simply plain text files, much like a standard R code file,
    but saved with the `.Rmd` file extension. These notebooks allow code to be executed
    interactively within the notebook, and the output will be displayed inline with
    the surrounding text. In Rstudio, a new file can be created by using the **File**
    menu, selecting **New File**, and choosing the **R Notebook** option. This will
    create a new R notebook using the default template, as shown in the following
    image:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_11_09.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: An R notebook file open in RStudio allows code and output to be
    integrated within a report'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The top of the file between the `---` dashes includes metadata about the notebook,
    such as the title and the intended output format. The “gear” icon to the right
    of the **Preview** button in Rstudio provides settings to switch between the default
    HTML notebook format and PDF or Microsoft Word document if you do not want to
    edit this setting manually. These settings govern the output format when the R
    notebook is compiled upon completion of the project.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Directly below the header metadata, we find a key distinction between an R notebook
    and traditional R code files. In particular, this section is not R code but rather
    **R Markdown**, which is a simple specification for formatting reports within
    plain text files. Because R and RStudio were not designed to be word processors,
    the styles are not controlled via a graphical user interface but rather by simple
    formatting codes, such as `*italics*` and `**bold**`, which are translated into
    *italics* and **bold**, respectively, in the final output file.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook template provides examples of several of the basic formatting
    options but does not begin to describe the complete set of R Markdown formatting
    capabilities. Other formats, such as headers, lists, and even embedded mathematical
    equations, are also possible. Much more information, including a one-page cheat
    sheet, is available at the R Markdown website: [https://rmarkdown.rstudio.com](https://rmarkdown.rstudio.com)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Because the R notebook format defaults to R Markdown, any R code must be embedded
    into the file using special indicators, denoting where the code begins and ends.
    The indicators are three backtick characters followed by the code language, surrounded
    by curly brackets. For example, a section of R code would begin using the [PRE0]{r}
    [PRE1]` [PRE2]` statement. Alternatively, these sections can be added to a notebook
    using the graphical user interface **Insert** button, just above the editor window
    and to the right of the **Preview** and “gear” buttons. The **Insert** button
    provides a drop-down selection of the programming languages available to use in
    the notebook, but keep in mind that these other languages may not be able to take
    advantage of the objects in the R environment—at least not without some additional
    steps.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Executing a code block, either by clicking the **Run** (green triangle) button
    within the chunk or by pressing your environment’s key combination, displays the
    command’s output inline with the R Markdown text. Options to manage the output
    format for each code block can be found using the “gear” icon at the top-right
    of the block. Here, one can govern whether the code or results are hidden from
    the final document, and whether or not the code should be executed at all. These
    features may be useful to suppress extraneous output from the report or prevent
    long-running code from being run unnecessarily.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Preview** button at the top of the notebook file generates a
    preview version of the final output report, using whatever R code output has been
    run interactively. For HTML notebooks, this file will open in a simple viewer,
    as shown in the screenshot that follows, or it can be opened in a web browser.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Because the file only uses output as it is generated in real time, the preview
    file is regenerated automatically by RStudio every time the notebook is saved.
    Leaving it open in the viewer window will allow you to see approximately how the
    final report will look at the end of the project.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_11_10.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: The preview file for an HTML notebook embeds the output within
    the text documentation'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The drop-down menu button to the right of the **Preview** button provides a
    means to compile or “knit” the document to its final output format. This runs
    the complete set of R code blocks from start to finish and uses the `knitr` package
    to bring together the code and text into a single report. Knitting to HTML is
    usually straightforward, but knitting to PDF or Microsoft Word may require installing
    additional packages.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The resulting files are inclusive of code, text, and images and without dependences,
    so they can be easily shared via email. This is true even of the final HTML notebook
    format, which is saved with the file extension `.nb.html` and offers some simple
    interactivity when viewed in a web browser. This format also embeds the original
    `.Rmd` R Markdown file so that a recipient can open the file and recreate the
    analysis if needed.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Performing advanced data exploration
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Falling squarely on the “art” side of data science, data exploration is a topic
    rarely given much coverage in academic textbooks. Tutorials may provide lip service
    to the practice, showing learners how to create graphs and visualizations, but
    rarely explaining how these are useful or why they may be necessary. Even this
    book is guilty of this; although the first few chapters performed simple data
    exploration, these exploratory analyses very rarely expanded beyond the five-number
    summary statistics described in *Chapter 2*, *Managing and Understanding Data*.
    Based on the limited coverage of this topic, one might gain the impression that
    it is not very important in practice, but this couldn’t be further from the truth;
    in fact, data exploration is a key component of real-world data science, and it
    is especially important for large, complex, and unfamiliar datasets.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Even though we have already performed simple exploratory analyses, we have not
    yet formally defined what it means to do so. The pioneering mathematician and
    statistician John W. Tukey, whose 1977 book on the subject brought the term into
    widespread awareness, noted that **exploratory data analysis** (**EDA**) involves
    allowing a dataset to suggest hypotheses and reveal useful insights rather than
    simply answer predetermined questions. It is often aided by graphs and charts,
    which in Tukey’s view, force us “*to notice what we never expected to see*.” One
    might imagine Tukey’s perspective as the notion that presenting the data in clear
    yet surprising ways, and listening carefully to what these analyses tell us, is
    not a ritual performed to merely understand the data itself but to better understand
    how we might ask questions about the data. In short, a rigorous precursory exploratory
    analysis is likely to lead to a more accurate main analysis.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering his wide contributions to the field, John Tukey might reasonably
    be considered the grandfather of EDA. You are already familiar with one of his
    most famous inventions: the box-and-whiskers plot. He also literally wrote the
    original textbook on EDA techniques, *Exploratory Data Analysis, Tukey, JW, Addison-Wesley;
    1977*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: As the goal of machine learning is not merely to answer predetermined questions,
    the form of exploratory data analysis that should be performed in concert with
    a machine learning project is very much in line with Tukey’s line of thinking.
    Advanced data exploration, conducted well, allows data to suggest insights that
    can be exploited to improve the performance of the machine learning task. Given
    the goal of improving machine learning models, it is best when data exploration
    is performed systematically and iteratively, but this is no easy task without
    prior experience. Without direction, one can explore countless dead ends. To counter
    this, the next few sections provide some ideas on how and where to begin this
    journey.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a data exploration roadmap
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we are to follow Tukey’s conception of data exploration, we are to believe
    that data exploration is less like an interrogation and more like a conversation,
    or perhaps even a one-sided listening session in which the data shares its nuggets
    of wisdom. Unfortunately, when this impression is combined with the superficial
    manner in which exploratory data analysis is depicted in many contexts, many new
    data explorers are left in a state of so-called “analysis paralysis” and unable
    to determine where to begin. It is as if the data scientist has been led into
    a dark room, told to conduct a séance, and wait for the data’s illuminating response.
    It’s no wonder this is a bit intimidating!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: No exploratory analysis is exactly like another, and each data scientist should
    develop the confidence to perform the work in their own way. However, while building
    your own experience and your own data exploration roadmap, you may find it helpful
    to learn by example. With that in mind, this section provides advice that may
    be of assistance in guiding exploratory analyses in general. It is not able to
    cover the exhaustive set of approaches, nor is it intended to imply a single best
    approach for data exploration. Again, the best approach is a systematic, iterative,
    and perhaps even intimate conversation with the dataset, and just as there is
    no textbook to completely prepare you for a human conversation, there is no single
    formula to converse with data.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: That being said, although every verbal conversation may be unique, they tend
    to begin similarly with a greeting and an exchange of names and pleasantries.
    Likewise, your data exploration roadmap may also begin with you simply becoming
    familiar with the data. Obtain a data dictionary, or create one in a text file
    or spreadsheet, which describes each of the features available for use. You may
    also record additional metadata such as the number of rows, the data source, when
    and where it was collected, and whether there are any known problems with the
    data. Such details may prompt questions during the analysis, or they may help
    enlighten when unexpected results are encountered.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: You may find it fruitful to print a paper copy of the data dictionary and work
    methodically row by row, exploring each feature one at a time. Although this work
    can certainly be performed in an electronic document, with large datasets having
    hundreds of predictors or more, the task somehow feels less daunting when it is
    performed with pen, paper, and highlighter pens—not to mention the satisfying
    feeling of making check marks and notes and crossing items off lists! The following
    figure illustrates the result of one such real-world data exploration process;
    rows indicate the available features, which have been annotated with stars, highlighting,
    and notes, indicating the perceived importance of each potential predictor, as
    well as any potential concerns found during the exploration.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_11_11.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: When performing data exploration, it can be helpful to print
    a data dictionary (or list of available attributes) and write notes directly on
    the paper by hand'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Working systematically down the list of features, you may first scout for any
    potential pitfalls. An attribute that at first appears to be incredibly useful
    may ultimately prove to be useless, due to newly discovered flaws or issues. For
    each variable, you may consider whether any of the following potential issues
    are present:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Missing or unexpected values
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers or extreme or unusual values
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric features with high skew
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric features with multiple modes
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numeric features with very high or very low variance
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical features with very many levels (known as high **cardinality**)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical features with levels that have very few observations (known as “sparse”
    data)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features strongly or weakly associated with the target or each other
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that even if these potential issues are encountered, they do not
    always indicate a problem. Many of them can be worked around, and in fact, we
    will explore (or have already explored) solutions for all of them in this book.
    For now, by focusing only on the exploration and not on the workarounds, it is
    likely that you are already familiar with analysis methods that may help to identify
    cases of each of the listed issues.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Simple one-way tables or visualizations like histograms can provide a look into
    individual features and identify problematic values, but more complex visuals
    may be necessary to investigate the data more deeply. It is important not to merely
    perform univariate analyses, which consider features in isolation, but also consider
    each feature’s relationship to the others and the target. This will require bivariate
    analysis such as cross tables or visualizations, like stacked bar charts, heat
    maps, and scatterplots. Given that the number of potential bivariate analyses
    is large for large numbers of predictors, R’s sophisticated visualization capabilities,
    described later in this chapter, make data exploration less tedious than it otherwise
    might be.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The power of data exploration is not simply to probe the data for anything of
    negative value but also to identify aspects of positive value. By working one
    by one down the list, you should ask whether each potential feature could provide
    any useful information about the outcome. Conversely, you might ask whether the
    feature is completely useless, or whether it might provide even a tiny bit of
    assistance toward the model’s goals. This is where human intelligence and subject-matter
    expertise are helpful for having an insightful conversation with the data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: As truly useless data is extremely rare, you may turn this into a kind of game
    in which you act as a detective, trying to discover the hidden information encoded
    in the supposedly “useless” attribute. As the saying goes, “One man’s trash is
    another man’s treasure.” Data scientists that are very good at turning trash into
    treasure will have a strong edge over the competition, as they will develop models
    that use more and better data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration may differ for so-called “big” datasets, with many millions
    of rows or features so numerous and confusing that manual exploration is infeasible.
    Rather than manually exploring these datasets, a typical practice is to write
    programs to systematically determine which features are useful or, otherwise,
    reduce the complexity of the data. We will examine some of these methods in later
    chapters.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Encountering outliers: a real-world pitfall'
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as how the process of data exploration, which once seemed quaint, became
    much more intricate in light of real-world complexity, many of the seemingly simple
    concepts of data exploration are actually much more nuanced in reality than they
    may have at first appeared. We will experience this many times firsthand when
    working through more complex real-world examples throughout the remaining chapters
    of this book; however, the nature of outliers may be the epitome of this phenomenon.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Thus far, we’ve taken our definition of outliers for granted; in *Chapter 2*,
    *Managing and Understanding Data*, we simply said that an outlier is “atypically
    high or low relative to the majority of data.” We observed such outliers quite
    easily on a box-and-whiskers plot, denoted by circles that were 1.5 times above
    or below the **interquartile range** (**IQR**) beyond the median. In fact, these
    are not merely outliers but, specifically, **Tukey outliers**, named after—if
    you haven’t already guessed—John W. Tukey, our previously noted forebearer of
    exploratory data analysis. This outlier definition is by no means wrong, but it
    may be slightly narrow. It is likely safe to assume that Tukey himself would agree
    that his own definition is but one of many ways to conceive of an “outlier.”
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a slightly broadened definition of the term and define an **outlier**
    as a value that is unusual compared to others in a dataset; it is not necessarily
    high or low, but simply “unusual.” Although this may seem only slightly different,
    technically speaking, from the prior definition, the word “unusual” has been precisely
    chosen to convey a very specific meaning. In particular, the word “unusual” does
    not imply a particular way to fix the data, whereas terms like “high” and “low”
    suggest that a data point is wrong in a specific way. You generally cannot easily
    correct “unusual” to “usual” without first having a firm grasp of what “usual”
    means. Unusual things are simply odd or curious; we should investigate them further.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: With this mindset, study the following hypothetical dataset comprising images
    of road signs taken from a simple Bing image search. Which of these are outliers?
    Most stop signs are red, so it would seem that the yellow (middle) and blue (bottom
    left) stop signs, as well as the “stop ahead” signs, are clearly outliers, but
    there are some other oddities too. There’s a stop sign with a hand, some with
    additional text, and many with slight variations to the sign’s font and border.
    Moreover, what about stop signs on a plain white background versus a natural landscape?
    Or, perhaps if you are from another country, literally all of these would be unusual,
    and therefore, all would be considered outliers. If you are from Hawaii, where
    the picture of the blue stop sign was apparently taken, then even a blue stop
    sign may be completely within the ordinary!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17290_11_12.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12: Which of the images in this hypothetical stop sign dataset are
    outliers?'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The takeaway from this exercise, of course, is that an outlier is almost always
    a matter of perspective, and thus, detecting and fixing outliers becomes much
    more complicated. On one hand, it does become a bit easier to discern if an outlier
    is obviously the product of a data error, as in a “mistake” that was made when
    recording a value. For example, suppose a data entry error recorded someone’s
    wealth as 1 trillion dollars rather than 1 billion. The extremeness of this value
    even relative to other wealthy people makes the value easy to detect, and the
    fact that it is obviously wrong makes for an easy fix: simply input the correct
    value. On the other hand, outliers that are “real,” such as Elon Musk who, at
    the time of writing, is worth nearly $200 billion, are much less straightforward
    to handle. This distinction between “real” and “mistake” outliers is intended
    to illustrate the idea of whether or not an outlier is explainable. It is often
    but surely not always best to try to model the explainable outliers; this makes
    the model more robust. On the other hand, modeling the “mistake” outliers, which
    are essentially random variations, will usually just add noise and make for a
    weaker model.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The most important question to consider when encountering outliers during data
    exploration is whether including the outlier in the training data will ultimately
    improve or detract from the learning algorithm’s ability to perform the desired
    task. This speaks to the **generalizability** of the model, or its ability to
    perform well on data that it has not seen before. While doing a thorough job of
    data exploration, keep in mind the deployment scenario and whether the model will
    need to be robust against similar outliers in the future. For example, if the
    prior stop sign images were being used to train an autonomous vehicle driving
    algorithm, then one might remove outliers that are not expected to be encountered
    on public roadways. Yet, a real-world self-driving vehicle would be expected to
    encounter signs defaced with graffiti, concealed by darkness, or obscured by plants
    and weather conditions, so one might also argue that this dataset has *too few*
    outliers!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'As has been and will continue to be the theme for real-world machine learning,
    there is no single one-size-fits-all approach to handling this problem. Deleting
    outliers is likely the most common strategy, and is often taught in introductory
    statistics courses, but it is perhaps one of the worst. It is certainly easy,
    but this ease comes with a dark side: deleting outliers may discard very important
    details about the learning task. The practice precludes the data scientist from
    engaging in a deeper conversation with the dataset about whether the information
    is useful or useless.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches require more effort but may be more likely to improve the model’s
    generalizability. In the case of events that present as outliers due to their
    rarity, it may be possible to collect more data on these rare events. Alternatively,
    it may be possible to group outliers into a single, more frequent category through
    binning or bucketing rare values, or capping values at a maximum level. Ideally,
    these groups will be based on an intuitive sense of how the learning algorithm
    will use the data, but in the absence of subject-matter expertise, it is often
    sufficient to group them into a top decile or create groups of values that have
    a similar impact on the target variable.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the question of what it means to be an outlier, we have already
    observed that context is key. Something that appears unusual in one context, such
    as a blue stop sign, may be ordinary in another context. Likewise, something that
    is completely ordinary in one context may be highly irregular in another. In short,
    not only can a reasonable value falsely appear to be an outlier but actual outliers
    can also be hidden in plain sight. Truly grasping this fact is central to rigorous
    data exploration. For example, consider a dataset with a typical population distribution.
    We’d expect to see a fair number of elderly women and a fair number of pregnant
    women, but observing a pregnant elderly woman would be highly irregular! Exploratory
    data analysis, performed well, helps identify these types of anomalies and ultimately
    leads to better-performing models.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Example – using ggplot2 for visual data exploration
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As noted previously, data exploration is at its best when aided by graphs and
    charts, which according to John Tukey—himself a pioneer of innovative data visualization
    techniques—help us “to notice what we never expected to see.” We’ve explored a
    variety of datasets in previous chapters, yet until now, we have only used R’s
    built-in graphing capabilities to create simple visualizations, like boxplots,
    histograms, and scatterplots.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: For a deeper, more thorough job of data exploration, we’ll need to build more
    complex visuals, and although we could do so using base R, a better option is
    available. That option comes in the form of the `ggplot2` package, which provides
    a “grammar of graphics” that describes how the elements of a plot relate to each
    other and the visualization itself. The package has been widely used for over
    a decade and is highly popular. It can create professional, publication-ready
    images, and its output can be seen in many academic journals and on many common
    websites. Even if you didn’t know it at the time, you are likely to have seen
    its output before.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Entire books have been dedicated to the `ggplot2` package and the “grammar of
    graphics.” This section covers only the essentials necessary to get started using
    the package. For many free resources on this topic, visit the website at [https://ggplot2.tidyverse.org](https://ggplot2.tidyverse.org),
    where you can even download a single-page cheat sheet with the most used commands.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: It would fill an entire book to demonstrate the capabilities of the `ggplot2`
    package, but the fundamentals can be illustrated with several basic recipes. To
    this end, we’ll use it to explore a dataset over 100 years in the making. The
    dataset describes the passengers of the Titanic ship, which sunk in the year 1912\.
    The machine learning application is used to predict which of the 1,309 passengers
    were tragically killed in the disaster, and although a predictive model is of
    little use today, the dataset is well suited to practicing data exploration, due
    to having many hidden patterns, which visualizations can help reveal.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: The Titanic dataset is a widely popular teaching dataset and is available from
    numerous online sources. The original file and documentation are available via
    the Vanderbilt University Department of Biostatistics, located on the web at [https://hbiostat.org/data/](https://hbiostat.org/data/).
    This book uses a variant of the Titanic dataset that was created to introduce
    learners to the Kaggle competition format. To read more or join the competition,
    visit [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin by loading the Titanic model training dataset and examining its
    features:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output shows that the dataset includes 12 features for 891 of the Titanic’s
    1,309 passengers; the remaining 418 passengers can be found in the `titanic_test.csv`
    file, representing a roughly 70/30 split for training and testing. The binary
    target feature `Survived` indicates whether the passenger survived the shipwreck,
    with `1` indicating survival and `0` indicating the less fortunate outcome. Note
    that in the test set, `Survived` is left blank to simulate unseen future data
    for prediction.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In the spirit of building a data exploration roadmap, we can start thinking
    about each feature’s potential value for prediction. The `Pclass` column indicates
    the passenger class, as in first-, second-, or third-class ticket status. This,
    as well as the `Sex` and `Age` attributes, seem like potentially useful predictors
    of survival. We’ll use the `ggplot2` package to explore these potential relationships
    in more depth. If you haven’t already installed this package, do so using the
    `install.packages("ggplot2")` before proceeding.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Every ggplot2 visualization is composed of layers, which place graphics upon
    a blank canvas. Executing the `ggplot()` function alone creates an empty gray
    plot area with no data points:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To create something more interesting than a blank gray coordinate system, we’ll
    need to add additional layers to the plot object stored in the `p` object. Additional
    layers are specified by a **geom** function, which determines the type of layer
    to be added. Each of the many `geom` functions requires a `mapping` parameter,
    which invokes the package’s aesthetic function, `aes()`, to link the dataset’s
    features to their visual depiction. This series of steps can be somewhat confusing,
    so the best way to learn is by example.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by creating a simple boxplot of the `Age` feature. You’ll recall
    that in *Chapter 2*, *Managing and Understanding Data*, we used R’s built-in `boxplot()`
    feature to construct such visualizations as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To accomplish the same in the ggplot2 environment, we simply add `geom_boxplot()`
    to the blank coordinate system, with the `aes()` aesthetic mapping function indicating
    that we would like the `Age` feature to be mapped to the `y` coordinate as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting figures are largely similar, with only a few stylistic differences
    in how the data is presented. Even the use of Tukey outliers is the same across
    both plots:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B17290_11_13.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.13: R’s built-in boxplot function (left) compared to the ggplot2
    version of the same (right). Both depict the distribution of Titanic passenger
    ages.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it may seem pointless to use the more complicated `ggplot()` visualization
    when the simpler function will suffice, the strength of the framework is its ability
    to visualize bivariate relationships with only small changes to the code. For
    example, suppose we’d like to examine how age is related to survival status. We
    can do so using a simple modification of our previous code. Note that the `Age`
    has been mapped to the `x` dimension in order to create a horizontal boxplot rather
    than the vertical boxplot used previously. Supplying a factor-converted `Survived`
    as the `y` dimension creates a boxplot for each of the two levels of the factor.
    Using this plot, it appears that survivors tended to be a bit younger than non-survivors:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Chart, box and whisker chart  Description automatically generated](img/B17290_11_14.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.14: Side-by-side boxplots help compare the age distribution of Titanic’s
    survivors and non-survivors'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes a slightly different visualization can better tell a story. With
    this in mind, recall that in *Chapter 2*, *Managing and Understanding Data*, we
    also used R’s `hist()` function to examine the distribution of numeric features.
    We’ll begin by replicating this in ggplot to compare the two side by side. The
    built-in function is quite simple:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `ggplot` version uses the `geom_histogram()`:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting figures are largely the same, aside from stylistic differences
    and the defaults regarding the number of bins:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B17290_11_15.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.15: R’s built-in histogram (left) compared to the ggplot2 version
    of the same (right) examining the distribution of Titanic passenger ages'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Again, where the `ggplot2` framework shines is the ability to make a few small
    tweaks and reveal interesting relationships in the data. Here, let’s examine three
    variants of the same comparison of age and survival.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can construct overlapping histograms by adding a `fill` parameter
    to the `aes()` function. This colorizes the bars according to the levels of the
    factor provided. We’ll also use the `ggtitle()` function to add an informative
    title to the figure:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Second, rather than having overlapping histograms, we can create a grid of
    side-by-side plots using the `facet_grid()` function. This function takes `rows`
    and `cols` parameters to define the cells in the grid. In our case, to create
    side-by-side plots, we need to define the columns for survivors and non-survivors
    using the `Survived` variable. This must be wrapped by the `vars()` function to
    denote that it’s a feature from the accompanying dataset:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Third, rather than using the histogram `geom`, we can use `geom_density()`
    to create a density plot. This type of visualization is like a histogram but uses
    a smoothed curve instead of individual bars to depict the proportion of records
    at each value of the `x` dimension. We’ll set the color of the line based on the
    levels of `Survived` and fill the area beneath the curve with the same color.
    Because the areas overlap, the `alpha` parameter allows us to control the level
    of transparency so that both may be seen at the same time. Note that this is a
    parameter of the `geom` function and not the `aes()` function. The complete command
    is as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting three figures visualize the same data in different ways:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, histogram  Description automatically generated](img/B17290_11_16.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.16: Small changes in the ggplot() function call can create vastly
    different outputs'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: These three visualizations demonstrate the fact that different visualizations
    of the same data can help tell different stories. For example, the top figure
    with overlapping histograms seems to highlight the fact that a relatively small
    proportion of people survived. In contrast, the bottom figure clearly depicts
    the spike in survival for travelers below 10 years of age; this provides evidence
    of a “women and children first” policy for the lifeboats—at least with respect
    to children.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine a few more plots to see if we can uncover more details of the
    Titanic’s evacuation policies. We’ll begin by confirming the assumed differences
    in survival by gender. For this, we’ll create a simple bar chart using the `geom_bar()`
    layer. By default, this simply counts the number of occurrences of the supplied
    dimension. The following command creates a bar chart, illustrating the fact that
    there were nearly twice as many males as females on board the Titanic:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Chart, bar chart  Description automatically generated](img/B17290_11_17.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.17: A simple bar chart of a single feature helps to put count data
    into perspective'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'A more interesting visualization would be to compare the rate of survival by
    gender. To do this, we must not only supply the `Survived` outcome as the `y`
    parameter to the `aes()` function but also tell the `geom_bar()` function to compute
    a summary statistic of the data—in particular, using the `mean` function—using
    the `stat` and `fun` parameters as shown:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The resulting figures confirm the assumption of a “women and children first”
    lifeboat policy. Although there were almost twice as many men on board, women
    were three times more likely to survive:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart  Description automatically generated](img/B17290_11_18.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.18: More complex bar charts can illustrate disparities in survival
    rates by gender'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'To once again demonstrate ggplot’s ability to create a large variety of visualizations
    and tell different stories about the data with relatively small changes to the
    code, we’ll examine the passenger class (`Pclass`) feature in a few different
    ways. First, we’ll create a simple bar chart that depicts the survival rate using
    the `stat` and `fun` parameters, just as we did for survival rate by gender:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The resulting figure depicts a substantial decline in survival likelihood for
    second- and third-class passengers:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart  Description automatically generated](img/B17290_11_19.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.19: A bar chart shows a clear disparity in survival outcomes for
    Titanic’s lower passenger classes'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Color can be an effective tool to communicate additional dimensions. Using
    the `fill` parameter, we’ll create a simple bar chart of passenger counts with
    bars that are filled with color according to survival status, which is converted
    to a factor:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result highlights the fact that the overwhelming number of deceased came
    from the third-class section of the ship:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart  Description automatically generated](img/B17290_11_20.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.20: A bar chart emphasizing the number of third-class passengers
    that died on the Titanic'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll modify this plot using a `position` parameter that informs `ggplot()`
    how to arrange the colorized bars. In this case, we’ll set `position = "fill"`,
    which creates a stacked bar chart that fills the vertical space—essentially giving
    each color in the stack a relative proportion out of 100 percent:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resulting figure emphasizes the decreased odds of survival for the lower
    classes:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart  Description automatically generated](img/B17290_11_21.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.21: A bar chart contrasting the survival rates by passenger class'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we’ll attempt to visualize the relationship between three dimensions:
    passenger class, gender, and survival. The `Pclass` and Survived features define
    the `x` and `y` dimensions, leaving `Sex` to define the bar colors via the `fill`
    parameter. Setting the `position = "dodge"` tells `ggplot()` to place the colored
    bars side-by-side rather than stacked, while the `stat` and `fun` parameters compute
    the survival rate. The full command is as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This figure reveals the fact that nearly all first- and second-class female
    passengers survived, whereas men of all classes were more likely to perish:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, bar chart  Description automatically generated](img/B17290_11_22.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.22: A bar chart illustrating the low survival rate for males, regardless
    of passenger class'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Examining more facets of the Titanic data is an exercise best left to the reader.
    After all, data exploration may be best thought of as a personal conversation
    between the data and the data scientist. Similarly, as mentioned before, it is
    beyond the scope of this book to cover every aspect of the `ggplot2` package.
    Still, this section should have demonstrated ways in which data visualization
    can help identify connections between features, which is useful for developing
    a rich understanding of the data. Diving deeper into the capabilities of the `ggplot()`
    function, perhaps by exploring a dataset of personal interest to you, will do
    much to improve your model building and storytelling skills—both of which are
    important elements of being successful with machine learning.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The *R Graphics Cookbook* by Winston Chang ([https://r-graphics.org](https://r-graphics.org))
    is available online for free and provides a wealth of recipes, covering virtually
    every type of `ggplot2` visualization.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the fundamentals of what it means to be a successful
    machine learning practitioner and the skills necessary to build successful machine
    learning models. These require not only a broad set of requisite knowledge and
    experience but also a thorough understanding of the learning algorithms, the training
    dataset, the real-world deployment scenario, and the myriad ways that the work
    can go wrong—either by accident or by design.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The data science buzzword suggests a relationship between the data, the machine,
    and the people who guide the learning process. This is a team effort, and the
    growing emphasis on data science as a distinct outgrowth from the field of data
    mining that came before it, with numerous degree programs and online certifications,
    reflects its operationalization as a field of study concerned with not just statistics,
    data, and computer algorithms but also the technologic and bureaucratic infrastructure
    that enables applied machine learning to be successful.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Applied machine learning and data science ask their practitioners to be compelling
    explorers and storytellers. The audacious use of data must be carefully balanced
    with what truly can be learned from the data, and what may reasonably be done
    with what is learned. This is certainly both an art and a science, and therefore,
    few can master the field in its entirety. Instead, striving to constantly improve,
    iterate, and compete will lead to an improvement of self that inevitably leads
    to models that better perform at their intended real-world applications. This
    contributes to the so-called “virtuous cycle” of artificial intelligence, in which
    a flywheel-like effect rapidly increases the productivity of organizations employing
    data science methods.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Just as this chapter revisited familiar topics and revealed the newfound complexity
    in the real-world practice of machine learning, the next chapter revisits data
    preparation to consider solutions to common problems found in large and messy
    datasets. We’ll work our way back into the trenches by learning about a completely
    new way to program in R, which is not only more capable of handling such challenges
    but also, once the initial learning curve has been passed, perhaps even more fun
    and intuitive to use.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
