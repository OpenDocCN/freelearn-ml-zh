- en: Chapter 9. Object Tracking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章. 对象跟踪
- en: In this chapter, we are going to learn about tracking an object in a live video.
    We will discuss the different characteristics that can be used to track an object.
    We will also learn about the different methods and techniques for object tracking.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在实时视频中跟踪对象。我们将讨论可用于跟踪对象的不同特征。我们还将了解对象跟踪的不同方法和技巧。
- en: 'By the end of this chapter, you will know:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解：
- en: How to use frame differencing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用帧差分
- en: How to use colorspaces to track colored objects
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用色彩空间跟踪彩色对象
- en: How to build an interactive object tracker
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个交互式对象跟踪器
- en: How to build a feature tracker
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建特征跟踪器
- en: How to build a video surveillance system
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建视频监控系统
- en: Frame differencing
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 帧差分
- en: This is, possibly, the simplest technique we can use to see what parts of the
    video are moving. When we consider a live video stream, the difference between
    successive frames gives us a lot of information. The concept is fairly straightforward!
    We just take the difference between successive frames and display the differences.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是我们用来查看视频哪些部分在移动的最简单技术。当我们考虑实时视频流时，连续帧之间的差异为我们提供了大量信息。这个概念相当直接！我们只需计算连续帧之间的差异并显示这些差异。
- en: 'If I move my laptop rapidly from left to right, we will see something like
    this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我快速地从左到右移动我的笔记本电脑，我们将看到类似这样的效果：
- en: '![Frame differencing](img/B04554_09_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![帧差分](img/B04554_09_01.jpg)'
- en: 'If I rapidly move the TV remote in my hand, it will look something like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我快速地在手中移动电视遥控器，它看起来可能就像这样：
- en: '![Frame differencing](img/B04554_09_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![帧差分](img/B04554_09_02.jpg)'
- en: 'As you can see from the previous images, only the moving parts in the video
    get highlighted. This gives us a good starting point to see what areas are moving
    in the video. Here is the code to do this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的图像中可以看到，视频中只有运动部件被突出显示。这为我们提供了一个很好的起点，可以看到视频中哪些区域在移动。以下是实现这一功能的代码：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Colorspace based tracking
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于色彩空间的跟踪
- en: Frame differencing gives us some useful information, but we cannot use it to
    build anything meaningful. In order to build a good object tracker, we need to
    understand what characteristics can be used to make our tracking robust and accurate.
    So, let's take a step in that direction and see how we can use **colorspaces**
    to come up with a good tracker. As we have discussed in previous chapters, HSVcolorspace
    is very informative when it comes to human perception. We can convert an image
    to the HSV space, and then use `colorspacethresholding` to track a given object.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 帧差分为我们提供了一些有用的信息，但我们不能用它来构建任何有意义的东西。为了构建一个好的对象跟踪器，我们需要了解哪些特征可以用来使我们的跟踪既稳健又准确。因此，让我们迈出这一步，看看我们如何使用**色彩空间**来构建一个好的跟踪器。正如我们在前面的章节中讨论的，HSV色彩空间在人类感知方面非常有信息量。我们可以将图像转换为HSV空间，然后使用`colorspacethresholding`来跟踪给定的对象。
- en: 'Consider the following frame in the video:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑视频中的以下帧：
- en: '![Colorspace based tracking](img/B04554_09_03.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![基于色彩空间的跟踪](img/B04554_09_03.jpg)'
- en: 'If you run it through the colorspace filter and track the object, you will
    see something like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将其通过色彩空间过滤器并跟踪对象，您将看到类似这样的效果：
- en: '![Colorspace based tracking](img/B04554_09_04.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![基于色彩空间的跟踪](img/B04554_09_04.jpg)'
- en: 'As we can see here, our tracker recognizes a particular object in the video,
    based on the color characteristics. In order to use this tracker, we need to know
    the color distribution of our target object. Following is the code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里可以看到的，我们的跟踪器根据颜色特征识别视频中的特定对象。为了使用这个跟踪器，我们需要知道目标对象的颜色分布。以下是如何实现这一功能的代码：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Building an interactive object tracker
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个交互式对象跟踪器
- en: Colorspace based tracker gives us the freedom to track a colored object, but
    we are also constrained to a predefined color. What if we just want to pick an
    object at random? How do we build an object tracker that can learn the characteristics
    of the selected object and just track it automatically? This is where the **CAMShift**
    algorithm, which stands for Continuously Adaptive Meanshift, comes into the picture.
    It's basically an improved version of the **Meanshift** algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于色彩空间的跟踪器为我们提供了跟踪彩色对象的自由，但我们也被限制在预定义的颜色中。如果我们只想随机选择一个对象怎么办？我们如何构建一个可以学习所选对象特征并自动跟踪它的对象跟踪器？这就是**CAMShift**算法的用武之地，它代表连续自适应均值漂移。它基本上是**均值漂移**算法的改进版本。
- en: The concept of Meanshift is actually nice and simple. Let's say we select a
    region of interest and we want our object tracker to track that object. In that
    region, we select a bunch of points based on the color histogram and compute the
    centroid. If the centroid lies at the center of this region, we know that the
    object hasn't moved. But if the centroid is not at the center of this region,
    then we know that the object is moving in some direction. The movement of the
    centroid controls the direction in which the object is moving. So, we move our
    bounding box to a new location so that the new centroid becomes the center of
    this bounding box. Hence, this algorithm is called Meanshift, because the mean
    (i.e. the centroid) is shifting. This way, we keep ourselves updated with the
    current location of the object.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移的概念实际上很棒且简单。假设我们选择了一个感兴趣的区域，并希望我们的目标跟踪器跟踪该对象。在这个区域内，我们根据颜色直方图选择一些点并计算质心。如果质心位于这个区域的中心，我们知道对象没有移动。但如果质心不在这个区域的中心，那么我们知道对象正在某个方向上移动。质心的移动控制着对象移动的方向。因此，我们将边界框移动到新的位置，使新的质心成为这个边界框的中心。因此，这个算法被称为均值漂移，因为均值（即质心）在移动。这样，我们就能保持对对象当前位置的更新。
- en: But the problem with Meanshift is that the size of the bounding box is not allowed
    to change. When you move the object away from the camera, the object will appear
    smaller to the human eye, but Meanshift will not take this into account. The size
    of the bounding box will remain the same throughout the tracking session. Hence,
    we need to use CAMShift. The advantage of CAMShift is that it can adapt the size
    of the bounding box to the size of the object. Along with that, it can also keep
    track of the orientation of the object.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但均值漂移的问题在于边界框的大小不允许改变。当你将对象从摄像机移开时，对象在人类眼中会显得更小，但均值漂移不会考虑这一点。边界框的大小在整个跟踪过程中保持不变。因此，我们需要使用CAMShift。CAMShift的优势在于它可以调整边界框的大小以适应对象的大小。此外，它还可以跟踪对象的方向。
- en: 'Let''s consider the following frame in which the object is highlighted in orange
    (the box in my hand):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下帧，其中对象以橙色突出显示（我手中的框）：
- en: '![Building an interactive object tracker](img/B04554_09_05.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04554_09_05.jpg)'
- en: 'Now that we have selected the object, the algorithm computes the histogram
    `backprojection` and extracts all the information. Let''s move the object and
    see how it''s getting tracked:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了对象，算法计算直方图的反投影并提取所有信息。让我们移动对象，看看它是如何被跟踪的：
- en: '![Building an interactive object tracker](img/B04554_09_06.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04554_09_06.jpg)'
- en: 'Looks like the object is getting tracked fairly well. Let''s change the orientation
    and see if the tracking is maintained:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来对象被跟踪得相当好。让我们改变方向，看看跟踪是否保持：
- en: '![Building an interactive object tracker](img/B04554_09_07.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04554_09_07.jpg)'
- en: 'As we can see, the bounding ellipse has changed its location as well as its
    orientation. Let''s change the perspective of the object and see if it''s still
    able to track it:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，边界椭圆已经改变了其位置和方向。让我们改变对象的角度，看看它是否仍然能够跟踪：
- en: '![Building an interactive object tracker](img/B04554_09_08.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![构建交互式对象跟踪器](img/B04554_09_08.jpg)'
- en: We are still good! The bounding ellipse has changed the aspect ratio to reflect
    the fact that the object looks skewed now (because of the perspective transformation).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然做得很好！边界椭圆已经改变了长宽比，以反映对象现在看起来是倾斜的（由于透视变换）。
- en: 'Following is the code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对应的代码：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Feature based tracking
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于特征的跟踪
- en: Feature based tracking refers to tracking individual feature points across successive
    frames in the video. We use a technique called **optical flow** to track these
    features. Optical flow is one of the most popular techniques in computer vision.
    We choose a bunch of feature points and track them through the video stream.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的跟踪指的是在视频的连续帧中跟踪单个特征点。我们使用一种称为**光流**的技术来跟踪这些特征。光流是计算机视觉中最受欢迎的技术之一。我们选择一些特征点并通过视频流跟踪它们。
- en: When we detect the feature points, we compute the displacement vectors and show
    the motion of those keypoints between consecutive frames. These vectors are called
    motion vectors. There are many ways to do this, but the Lucas-Kanade method is
    perhaps the most popular of all these techniques. You can refer to their original
    paper at [http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf).
    We start the process by extracting the feature points. For each feature point,
    we create 3x3 patches with the feature point in the center. The assumption here
    is that all the points within each patch will have a similar motion. We can adjust
    the size of this window depending on the problem at hand.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检测到特征点时，我们计算位移向量并显示这些关键点在连续帧之间的运动。这些向量被称为运动向量。有许多方法可以做到这一点，但Lucas-Kanade方法可能是所有这些技术中最受欢迎的。您可以参考他们的原始论文[http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf)。我们通过提取特征点开始这个过程。对于每个特征点，我们以特征点为中心创建3x3的补丁。这里的假设是每个补丁内的所有点将具有相似的运动。我们可以根据问题的需要调整这个窗口的大小。
- en: For each feature point in the current frame, we take the surrounding 3x3 patch
    as our reference point. For this patch, we look in its neighborhood in the previous
    frame to get the best match. This neighborhood is usually bigger than 3x3 because
    we want to get the patch that's closest to the patch under consideration. Now,
    the path from the center pixel of the matched patch in the previous frame to the
    center pixel of the patch under consideration in the current frame will become
    the motion vector. We do that for all the feature points and extract all the motion
    vectors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前帧中的每个特征点，我们取周围的3x3补丁作为我们的参考点。对于这个补丁，我们在前一帧的邻域中寻找最佳匹配。这个邻域通常比3x3大，因为我们想要找到与考虑中的补丁最接近的补丁。现在，从前一帧中匹配补丁的中心像素到当前帧中考虑的补丁中心像素的路径将成为运动向量。我们对所有特征点都这样做，并提取所有运动向量。
- en: 'Let''s consider the following frame:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下帧：
- en: '![Feature based tracking](img/B04554_09_09.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![基于特征的跟踪](img/B04554_09_09.jpg)'
- en: 'If I move in a horizontal direction, you will see the motion vectors in a horizontal
    direction:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在水平方向上移动，您将看到水平方向上的运动向量：
- en: '![Feature based tracking](img/B04554_09_10.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![基于特征的跟踪](img/B04554_09_10.jpg)'
- en: 'If I move away from the webcam, you will see something like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我远离网络摄像头，您将看到如下所示：
- en: '![Feature based tracking](img/B04554_09_11.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![基于特征的跟踪](img/B04554_09_11.jpg)'
- en: So, if you want to play around with it, you can let the user select a region
    of interest in the input video (like we did earlier). You can then extract feature
    points from this region of interest and track the object by drawing the bounding
    box. It will be a fun exercise!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您想尝试一下，您可以让用户在输入视频中选择感兴趣的区域（就像我们之前做的那样）。然后，您可以从这个感兴趣的区域提取特征点并通过绘制边界框来跟踪对象。这将是一个有趣的练习！
- en: 'Here is the code to perform optical flow based tracking:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是执行基于光流跟踪的代码：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Background subtraction
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景减法
- en: Background subtraction is very useful in video surveillance. Basically, background
    subtraction technique performs really well for cases where we have to detect moving
    objects in a static scene. As the name indicates, this algorithm works by detecting
    the background and subtracting it from the current frame to obtain the foreground,
    that is, moving objects. In order to detect moving objects, we need to build a
    model of the background first. This is not the same as frame differencing because
    we are actually modeling the background and using this model to detect moving
    objects. So, this performs much better than the simple frame differencing technique.
    This technique tries to detect static parts in the scene and then include it in
    the background model. So, it's an adaptive technique that can adjust according
    to the scene.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 背景减法在视频监控中非常有用。基本上，背景减法技术在需要检测静态场景中移动对象的情况下表现非常出色。正如其名所示，该算法通过检测背景并将其从当前帧中减去以获得前景，即移动对象来工作。为了检测移动对象，我们首先需要建立一个背景模型。这不同于帧差分，因为我们实际上是在建模背景并使用这个模型来检测移动对象。因此，这种方法比简单的帧差分技术表现更好。这种技术试图检测场景中的静态部分，并将其包含在背景模型中。因此，它是一种自适应技术，可以根据场景进行调整。
- en: 'Let''s consider the following image:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下图像：
- en: '![Background subtraction](img/B04554_09_12.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![背景减法](img/B04554_09_12.jpg)'
- en: 'Now, as we gather more frames in this scene, every part of the image will gradually
    become a part of the background model. This is what we discussed earlier as well.
    If a scene is static, the model adapts itself to make sure the background model
    is updated. This is how it looks in the beginning:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们在这一场景中收集更多的帧，图像的每一部分都将逐渐成为背景模型的一部分。这正是我们之前讨论过的。如果一个场景是静态的，模型会自动调整以确保背景模型得到更新。这就是它开始时的样子：
- en: '![Background subtraction](img/B04554_09_13.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![背景减法](img/B04554_09_13.jpg)'
- en: 'Notice how a part of my face has already become a part of the background model
    (the blackened region). The following screenshot shows what we''ll see after a
    few seconds:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我的脸部的一部分已经成为了背景模型的一部分（被变黑的区域）。以下截图显示了几秒钟后我们将看到的情况：
- en: '![Background subtraction](img/B04554_09_14.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![背景减法](img/B04554_09_14.jpg)'
- en: 'If we keep going, everything eventually becomes part of the background model:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续这样做，最终一切都将成为背景模型的一部分：
- en: '![Background subtraction](img/B04554_09_15.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![背景减法](img/B04554_09_15.jpg)'
- en: 'Now, if we introduce a new moving object, it will be detected clearly, as shown
    next:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们引入一个新的移动物体，它将被清晰地检测到，如下所示：
- en: '![Background subtraction](img/B04554_09_16.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![背景减法](img/B04554_09_16.jpg)'
- en: 'Here is the code to do this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现这一功能的代码：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Summary
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about object tracking. We learned how to get motion
    information using frame differencing, and how it can be limiting when we want
    to track different types of objects. We learned about colorspacethresholding and
    how it can be used to track colored objects. We discussed clustering techniques
    for object tracking and how we can build an interactive object tracker using the
    CAMShift algorithm. We discussed how to track features in a video and how we can
    use optical flow to achieve the same. We learned about background subtraction
    and how it can be used for video surveillance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了物体跟踪。我们学习了如何使用帧差分来获取运动信息，以及当我们想要跟踪不同类型的物体时，它可能受到的限制。我们学习了颜色空间阈值化以及它是如何用于跟踪彩色物体的。我们讨论了物体跟踪的聚类技术以及我们如何使用CAMShift算法构建一个交互式物体跟踪器。我们讨论了如何在视频中跟踪特征以及我们如何使用光流来实现相同的效果。我们学习了背景减法以及它是如何用于视频监控的。
- en: In the next chapter, we are going to discuss object recognition, and how we
    can build a visual search engine.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论物体识别，以及我们如何构建一个视觉搜索引擎。
