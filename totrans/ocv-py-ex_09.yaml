- en: Chapter 9. Object Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn about tracking an object in a live video.
    We will discuss the different characteristics that can be used to track an object.
    We will also learn about the different methods and techniques for object tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will know:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use frame differencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use colorspaces to track colored objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build an interactive object tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a feature tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a video surveillance system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame differencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is, possibly, the simplest technique we can use to see what parts of the
    video are moving. When we consider a live video stream, the difference between
    successive frames gives us a lot of information. The concept is fairly straightforward!
    We just take the difference between successive frames and display the differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'If I move my laptop rapidly from left to right, we will see something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frame differencing](img/B04554_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I rapidly move the TV remote in my hand, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frame differencing](img/B04554_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the previous images, only the moving parts in the video
    get highlighted. This gives us a good starting point to see what areas are moving
    in the video. Here is the code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Colorspace based tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Frame differencing gives us some useful information, but we cannot use it to
    build anything meaningful. In order to build a good object tracker, we need to
    understand what characteristics can be used to make our tracking robust and accurate.
    So, let's take a step in that direction and see how we can use **colorspaces**
    to come up with a good tracker. As we have discussed in previous chapters, HSVcolorspace
    is very informative when it comes to human perception. We can convert an image
    to the HSV space, and then use `colorspacethresholding` to track a given object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following frame in the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Colorspace based tracking](img/B04554_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run it through the colorspace filter and track the object, you will
    see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Colorspace based tracking](img/B04554_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see here, our tracker recognizes a particular object in the video,
    based on the color characteristics. In order to use this tracker, we need to know
    the color distribution of our target object. Following is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Building an interactive object tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Colorspace based tracker gives us the freedom to track a colored object, but
    we are also constrained to a predefined color. What if we just want to pick an
    object at random? How do we build an object tracker that can learn the characteristics
    of the selected object and just track it automatically? This is where the **CAMShift**
    algorithm, which stands for Continuously Adaptive Meanshift, comes into the picture.
    It's basically an improved version of the **Meanshift** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of Meanshift is actually nice and simple. Let's say we select a
    region of interest and we want our object tracker to track that object. In that
    region, we select a bunch of points based on the color histogram and compute the
    centroid. If the centroid lies at the center of this region, we know that the
    object hasn't moved. But if the centroid is not at the center of this region,
    then we know that the object is moving in some direction. The movement of the
    centroid controls the direction in which the object is moving. So, we move our
    bounding box to a new location so that the new centroid becomes the center of
    this bounding box. Hence, this algorithm is called Meanshift, because the mean
    (i.e. the centroid) is shifting. This way, we keep ourselves updated with the
    current location of the object.
  prefs: []
  type: TYPE_NORMAL
- en: But the problem with Meanshift is that the size of the bounding box is not allowed
    to change. When you move the object away from the camera, the object will appear
    smaller to the human eye, but Meanshift will not take this into account. The size
    of the bounding box will remain the same throughout the tracking session. Hence,
    we need to use CAMShift. The advantage of CAMShift is that it can adapt the size
    of the bounding box to the size of the object. Along with that, it can also keep
    track of the orientation of the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following frame in which the object is highlighted in orange
    (the box in my hand):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04554_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have selected the object, the algorithm computes the histogram
    `backprojection` and extracts all the information. Let''s move the object and
    see how it''s getting tracked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04554_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looks like the object is getting tracked fairly well. Let''s change the orientation
    and see if the tracking is maintained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04554_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the bounding ellipse has changed its location as well as its
    orientation. Let''s change the perspective of the object and see if it''s still
    able to track it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04554_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are still good! The bounding ellipse has changed the aspect ratio to reflect
    the fact that the object looks skewed now (because of the perspective transformation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Feature based tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature based tracking refers to tracking individual feature points across successive
    frames in the video. We use a technique called **optical flow** to track these
    features. Optical flow is one of the most popular techniques in computer vision.
    We choose a bunch of feature points and track them through the video stream.
  prefs: []
  type: TYPE_NORMAL
- en: When we detect the feature points, we compute the displacement vectors and show
    the motion of those keypoints between consecutive frames. These vectors are called
    motion vectors. There are many ways to do this, but the Lucas-Kanade method is
    perhaps the most popular of all these techniques. You can refer to their original
    paper at [http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf).
    We start the process by extracting the feature points. For each feature point,
    we create 3x3 patches with the feature point in the center. The assumption here
    is that all the points within each patch will have a similar motion. We can adjust
    the size of this window depending on the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: For each feature point in the current frame, we take the surrounding 3x3 patch
    as our reference point. For this patch, we look in its neighborhood in the previous
    frame to get the best match. This neighborhood is usually bigger than 3x3 because
    we want to get the patch that's closest to the patch under consideration. Now,
    the path from the center pixel of the matched patch in the previous frame to the
    center pixel of the patch under consideration in the current frame will become
    the motion vector. We do that for all the feature points and extract all the motion
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature based tracking](img/B04554_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move in a horizontal direction, you will see the motion vectors in a horizontal
    direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature based tracking](img/B04554_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move away from the webcam, you will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature based tracking](img/B04554_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, if you want to play around with it, you can let the user select a region
    of interest in the input video (like we did earlier). You can then extract feature
    points from this region of interest and track the object by drawing the bounding
    box. It will be a fun exercise!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to perform optical flow based tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Background subtraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Background subtraction is very useful in video surveillance. Basically, background
    subtraction technique performs really well for cases where we have to detect moving
    objects in a static scene. As the name indicates, this algorithm works by detecting
    the background and subtracting it from the current frame to obtain the foreground,
    that is, moving objects. In order to detect moving objects, we need to build a
    model of the background first. This is not the same as frame differencing because
    we are actually modeling the background and using this model to detect moving
    objects. So, this performs much better than the simple frame differencing technique.
    This technique tries to detect static parts in the scene and then include it in
    the background model. So, it's an adaptive technique that can adjust according
    to the scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtraction](img/B04554_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, as we gather more frames in this scene, every part of the image will gradually
    become a part of the background model. This is what we discussed earlier as well.
    If a scene is static, the model adapts itself to make sure the background model
    is updated. This is how it looks in the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtraction](img/B04554_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice how a part of my face has already become a part of the background model
    (the blackened region). The following screenshot shows what we''ll see after a
    few seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtraction](img/B04554_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we keep going, everything eventually becomes part of the background model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtraction](img/B04554_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we introduce a new moving object, it will be detected clearly, as shown
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Background subtraction](img/B04554_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about object tracking. We learned how to get motion
    information using frame differencing, and how it can be limiting when we want
    to track different types of objects. We learned about colorspacethresholding and
    how it can be used to track colored objects. We discussed clustering techniques
    for object tracking and how we can build an interactive object tracker using the
    CAMShift algorithm. We discussed how to track features in a video and how we can
    use optical flow to achieve the same. We learned about background subtraction
    and how it can be used for video surveillance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss object recognition, and how we
    can build a visual search engine.
  prefs: []
  type: TYPE_NORMAL
