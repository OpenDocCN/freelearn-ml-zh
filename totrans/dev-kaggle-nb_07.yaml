- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text Analysis Is All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to analyze text data and create machine learning
    models to help us. We will use the *Jigsaw Unintended Bias in Toxicity Classification*
    dataset (see *Reference 1*). This competition had the objective of building models
    that detect toxicity and reduce unwanted bias toward minorities that might be
    wrongly associated with toxic comments. With this competition, we introduce the
    field of **Natural Language Processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: The data used in the competition originates from the Civil Comments platform,
    which was founded by Aja Bogdanoff and Christa Mrgan in 2015 (see *Reference 2*)
    with the aim of solving the problem of civility in online discussions. When the
    platform was closed in 2017, they chose to keep around 2 million comments for
    researchers who want to understand and improve civility in online conversations.
    Jigsaw was the organization that sponsored this effort and then started a competition
    for language toxicity classification. In this chapter, we’re going to transform
    pure text into meaningful, model-ready numbers to be able to classify them into
    groups according to the toxicity of the comments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration of the *Jigsaw Unintended Bias in Toxicity Classification*
    competition dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to NLP-specific processing and analysis techniques, including word
    frequency, tokenization, part-of-speech tagging, named entity recognition, and
    word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The iterative refinement of the preprocessing of text data to prepare a model
    baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model baseline for this text classification competition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is in the data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data from the *Jigsaw Unintended Bias in Toxicity Classification* competition
    dataset contains 1.8 million rows in the training set and 97,300 rows in the test
    set. The test data contains only a **comment** column and does not contain a target
    (the value to predict) column. Training data contains, besides the **comment**
    column, another 43 columns, including the target feature. The target is a number
    between 0 and 1, which represents the annotation that is the objective of the
    prediction for this competition. This target value represents the degree of toxicity
    of a comment (`0` means zero/no toxicity and `1` means maximum toxicity), and
    the other 42 columns are flags related to the presence of certain sensitive topics
    in the comments. The topic is related to five categories: race and ethnicity,
    gender, sexual orientation, religion, and disability. In more detail, these are
    the flags per each of the five categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Race and ethnicity**: `asian`, `black`, `jewish`, `latino`, `other_race_or_ethnicity`,
    and `white`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gender**: `female`, `male`, `transgender`, and `other_gender`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sexual orientation**: `bisexual`, `heterosexual`, `homosexual_gay_or_lesbian`,
    and `other_sexual_orientation`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Religion**: `atheist`, `buddhist`, `christian`, `hindu`, `muslim`, and `other_religion`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disability**: `intellectual_or_learning_disability`, `other_disability`,
    `physical_disability`, and `psychiatric_or_mental_illness`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also a few features (a.k.a. columns in the dataset) that serve for
    identifying the comment: `created_data`, `publication_id`, `parent_id`, and `article_id`.
    Also provided are several user feedback information features associated with the
    comments: `rating`, `funny`, `wow`, `sad`, `likes`, `disagree`, and `sexual_explicit`.
    Finally, there are also two fields relative to annotations: `identity_annotator_count`
    and `toxicity_annotator_count`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a quick analysis of the target feature and the sensitive features.
  prefs: []
  type: TYPE_NORMAL
- en: Target feature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to look first at the distribution of the target feature. Let’s
    look at the histogram for these values’ distribution in *Figure 7.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, plot, line, diagram  Description automatically
    generated](img/B20963_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Distribution of target values (training data, 1.9 million columns)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this histogram, we’ve used a logarithmic scale on the *y* axis; the reason
    behind this is that we want to see the skewed distribution of values toward 0\.
    As we do this, we observe that we have a bimodal distribution: peak values at
    around 0.1 intervals, decreasing in amplitude, and less frequent values with a
    slowly rising trend, superposed. Most of the target values (above 1 million) are
    `0`.'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will look at the distribution of sensitive features as listed earlier (race
    and ethnicity, gender, sexual orientation, religion, and disability). We will
    again use a logarithmic scale on the *y* axis due to the skewness of the distribution
    (similar to the target, we have a concentration at `0`).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.2* shows the distribution of race and ethnicity feature values. These
    look discontinuous and very discrete, with the histogram showing a few separate
    peaks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph with numbers and lines  Description automatically generated with
    medium confidence](img/B20963_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Distribution of race and ethnicity feature values'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe a similar distribution for the gender feature values in *Figure
    7.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, plot, line  Description automatically
    generated](img/B20963_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Distribution of gender feature values'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.4*, we show the distribution of the additional toxicity features
    (`severe_toxicity`, `obscene`, `identity_attack`, `insult`, or `threat`) values.
    As you can see, the distribution is more even, and with an increasing trend for
    `insult`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing screenshot, text, plot, line  Description automatically
    generated](img/B20963_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Distribution of additional toxicity feature values'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us also look at the correlation between the target values and the race
    or ethnicity, gender, sexual orientation, religion, and disability feature values.
    We are not showing here the correlation matrix for all the features, but you can
    inspect it in the notebook associated with this chapter. Here, we only show the
    first 16 features correlated with the target, ordered by correlation factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the top 15 features ordered by correlation factor with the target
    in *Figure 7.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screen shot of a computer code  Description automatically generated](img/B20963_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Top 15 correlation factors of other features with the target feature'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in *Figure 7.6*, we represent the correlation matrix for these selected
    features and the target feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A chart with numbers and a red line  Description automatically generated
    with medium confidence](img/B20963_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Correlation matrix between the target and 15 features with the
    highest correlation with it'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that `target` is highly correlated with `insult` (0.93), `obscene`
    (0.49), and `identity_attack` (0.45). Also, `severe_toxicity` is correlated positively
    with `insult` and `obscene`. `identity_attack` has a small correlation with being
    `white`, `black`, `muslim`, and `homosexual_gay_or_lesbian`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We investigated the distribution of the `target` feature (feature to predict)
    and of the sensitive features. Now we will move to the main topic of analysis
    for this chapter: the comments text. We will apply several NLP-specific analysis
    techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the comments text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP is a field of AI that involves the use of computational techniques to enable
    computers to understand, interpret, transform, and even generate human language.
    NLP uses several techniques, algorithms, and models to process and analyze large
    datasets of text. Among these techniques, we can mention:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization**: Breaks down text into smaller units, like words, parts of
    words, or characters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lemmatization or stemming**: Reduces the words to dictionary form or removes
    the last few characters to get to a common form (stem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part-of-Speech** (**POS**) **tagging**: Assigns a grammatical category (for
    example, nouns, verbs, proper nouns, and adjectives) to each word in a sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named Entity Recognition** (**NER**): Identifies and classifies entities
    (for example, names of people, organizations, and places)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word embeddings**: Use a high-dimensional space to represent the words, a
    space in which the position of each word is determined by its relationship with
    other words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning models**: Train models on annotated datasets to learn patterns
    and relationships in language data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP applications can include sentiment analysis, machine translation, question
    answering, text summarization, and text classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that quick introduction to NLP, let us inspect the actual comment text.
    We will build a few word cloud graphs (using a 20,000-comment subset of the entire
    dataset). We will look first at the overall word distribution (see the notebook
    associated with this chapter), then at the distribution of words with target values
    above 0.75 and below 0.25:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Prevalent words (1-gram) in comments with low target score < 0.25
    (left) and high target score > 0.75 (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '`target` is very highly correlated with `insult`, and we expect to see a rather
    close distribution of words in the word clouds for the two features. This hypothesis
    is confirmed, and *Figure 7.8* illustrates this very well (both for low score
    and high score):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Prevalent words (1-gram) in comments with low insult score < 0.25
    (left) and high insult score > 0.75 (right)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, distributions show similar words with high frequency for both
    low scores and high scores for **target** and **insult**.
  prefs: []
  type: TYPE_NORMAL
- en: More word clouds are available in the associated notebook for `threat`, `obscene`,
    and other features. These word clouds give us a good initial intuition for the
    most frequent words. We will perform a more detailed analysis of word frequency
    in the entire corpus vocabulary in the *Building the vocabulary* and *Checking
    vocabulary coverage* sections. For now, we can observe that the analysis we performed
    is limited to individual word frequency, without capturing how these words are
    grouped over the entire corpus – in other words, how various words are used together
    and, based on this, identifying the main themes in the corpus. Such processing,
    aimed to reveal the underlying semantic structure of the entire corpus, is called
    **topic modeling**. The analysis of co-occurrence patterns of words in this approach
    allows us to reveal the latent topics existent in the text.
  prefs: []
  type: TYPE_NORMAL
- en: The inspiration for the implementation of the topic modeling approach in the
    associated notebook is from a set of articles and tutorials on topic modeling
    using latent Dirichlet allocation (see *References* *5*–*10*).
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by preprocessing the comments text, using the `gensim` library to
    eliminate special characters, frequently used words, connection words (or stopwords),
    and words with lengths less than 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code applies the defined `preprocess` function to all the comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a dictionary of words using `dictionary` from `gensim`/`corpora`.
    We also filter extremes, to eliminate less frequent words and limit the size of
    the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With these restrictions, we go then to the next step and generate a *bag of
    words* (`bow`) corpus from the dictionary. Then we apply **TF-IDF** (**Term Frequency-Inverse
    Document Frequency**) to this corpus, which provides a numerical representation
    of the importance of a word within a document in a collection or corpus of documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf` component measures how frequently a word appears in a document. The
    `idf` component shows the significance of a word across the entire corpus of documents
    (in our case, over the full set of comments). This factor decreases with a higher
    occurrence of a term in the documents. Therefore, after the `tfidf` transform,
    the coefficient for one word and one document is larger for words that are infrequent
    at the corpus level and appear with higher frequency inside the current document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We then apply *Latent Dirichlet Allocation* (`lda`), a topic model that generates
    topics based on word frequency on this corpus, using the `gensim` implementation
    for parallel processing (`LdaMulticore`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s represent the first 10 topics, with 5 words for each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The topic words are shown with the associated relative weight in the topic,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Top 10 topics, with 5 words (most relevant) selected per topic'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we extract the topics, we can go through the documents and identify which
    topics are present in the current document (in our case, comment). In *Figure
    7.10*, we show the dominant topics (with the relative weights) for one document
    (the following is the code used to generate the list of topics for a selected
    comment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B20963_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Topics associated (each with relative importance) with one comment'
  prefs: []
  type: TYPE_NORMAL
- en: We prefer the **pyLDAvis** visualization tool to represent the topics. In *Figure
    7.11*, we show a screenshot of this tool (in the notebook, we generated 20 topics
    for train data and, separately, for test data). The dashboard in *Figure 7.11*
    displays the **Intertopic Distance Map**. Here, the topic’s relative dimension
    (or influence) is represented by the size of the disks and the topic’s relative
    distance by their mutual distance. On the right side, for the currently selected
    topic (in the left-side panel), we can see the top 30 most relevant terms per
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: The disks with a light color (blue in the notebook) represent the overall word
    frequency. The darker colored disks (red in the notebook) represent the estimated
    word frequency within the selected topic. We can use a slide as well to adjust
    the relevance metric (in the picture, this is set to `1`). We can further refine
    this analysis by improving the preprocessing step (for example, we can add more
    stopwords, specific to this corpus), adjusting the parameters for the dictionary
    formation, and controlling the parameters for `tfidf` and `lda`. Due to the complexity
    of the LDA procedure, we also reduced the size of the corpus, by subsampling the
    train data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, on the left panel of the topic modeling
    dashboard generated using the pyLDAvis tool, we see the **Intertopic Distance
    Map** – with relative dimension of topic influence in the corpus and the relative
    topic’s distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Topic modeling dashboard generated using the pyLDAvis tool (left
    panel)'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the right-side panel of the topic modeling dashboard generated using the
    pyLDAvis tool, for the selected topic, we see the top 30 most relevant terms per
    topic, with blue for the overall term frequency in the corpus and red for the
    estimated term frequency within the selected topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Topic modeling dashboard generated using the pyLDAvis tool (right
    panel)'
  prefs: []
  type: TYPE_NORMAL
- en: We can repeat the analysis over the entire corpus, but this will require more
    computational resources than are available on Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we have explored the topics in the comments text corpus, using
    the `lda` method. With this procedure, we revealed one of the hidden (or latent)
    structures in the corpus of the text. Now we can better understand not only the
    frequency of the words but also how words are associated in comments, to form
    topics discussed by the commentators. Let’s continue to explore the corpus, from
    a different perspective. We will take every comment and analyze, using NER, what
    types of concepts are present in the text. Then, we will start to look at the
    syntactic elements and use POS tagging to extract the nouns, verbs, adjectives,
    and other parts of speech.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we review these NLP techniques is twofold. First, we want to give
    you a glimpse of the richness of the tools and techniques available in NLP. Second,
    for more complex machine learning models, you can include features derived using
    these methods. For example, you can add, besides other features extracted from
    the text, features obtained by the use of NER or POS tagging.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s perform NER on a selection of comments. NER is an information extraction
    task that aims to identify and extract named entities in unstructured data (text).
    Named entities are people, organizations, geographical places, dates and times,
    amounts, and currencies. There are several available methods to identify and extract
    named entities, with the most frequently used being `spacy` and `transformers`.
    In our case, we will use `spacy` to perform NER. We prefer `spacy` because it
    requires fewer resources compared with transformers and yet gives good results.
    Something to note here is that `spacy` is also available in 23 languages, including
    English, Portuguese, Spanish, Russian, and Chinese.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize an `nlp` object using the `spacy.load` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will load the `'en_core_web_sm'` (`sm` stands for small) `spacy` pipeline,
    which includes the `tok2vec`, `tagger`, `parser`, `senter`, `ner`, `attribute_ruler`,
    and `lemmatizer` components. We will not use all the functionality provided by
    this pipeline; we are interested in the `nlp` component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create a selection of comments. We filter documents that contain either
    the name `Obama` or the name `Trump` and have less than 100 characters. For the
    purpose of this demonstration, we do not want to manipulate large sentences; it
    will be easier to follow the demonstrations if we operate with smaller sentences.
    The next code fragment will perform the selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can visualize the result of applying NER in two ways. One way is to print
    out the text’s start and end characters and the entity label for each entity identified
    in the current comment. An alternative way is to use `displacy` rendering from
    `spacy`, which will decorate each entity with a selected color and add the entity
    name beside the entity text (see *Figure 7.13*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is for the extraction of entities using `nlp` and the preparation
    of visualization using `displacy`. Before showing the annotated text using `displacy`,
    we are printing each entity text, followed by its position (start and end character
    positions) and the entity label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are multiple labels predefined in `spacy` `nlp`. We can extract the meaning
    of each one with a simple piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting list of labels and the meaning of each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CARDINAL**: Numerals that do not fall under another type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DATE**: Absolute or relative dates or periods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EVENT**: Named hurricanes, battles, wars, sports events, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FAC**: Buildings, airports, highways, bridges, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPE**: Countries, cities, or states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LANGUAGE**: Any named language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LAW**: Named documents made into laws'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LOC**: Non-GPE locations, mountain ranges, or bodies of water'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MONEY**: Monetary values, including units'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NORP**: Nationalities or religious or political groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORDINAL**: `first`, `second`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORG**: Companies, agencies, institutions, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PERCENT**: Percentage, including `%`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PERSON**: People, including fictional'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PRODUCT**: Objects, vehicles, foods, and so on (not services)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QUANTITY**: Measurements, such as weight or distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TIME**: Times less than a day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WORK_OF_ART**: Titles of books, songs, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, line  Description automatically
    generated](img/B20963_07_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: NER using spacy and displacy for visualization of NER results'
  prefs: []
  type: TYPE_NORMAL
- en: In the top example of the preceding screenshot, Bernie (Sanders) is recognized
    correctly as a person (**PERSON**), while (Donald) Trump is identified as an organization
    (**ORG**). This might be because former president Trump frequently used his name
    as part of the name of several of the organizations he founded while being a businessperson.
  prefs: []
  type: TYPE_NORMAL
- en: In the bottom example, Obama (also a former president and a frequent topic in
    disputed political debates) is recognized correctly as **PERSON**. In both cases,
    we are also showing the list of extracted entities, complemented with the starting
    and ending positions of each identified entity.
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With NER analysis, we identified names specific to various entities like people,
    organizations, places, and so on. These help us to associate various terms with
    a certain semantic group. We can go further and explore the comments text so that
    we understand what POS (like noun or verb) each word is, and understand the syntax
    of each phrase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with using `nltk` (an alternative `nlp` library) to extract parts
    of speech from the same small selection of phrases we used for NER experiments.
    We chose `nltk` here because, as well as being even less resource-hungry than
    spacy, it provides good-quality results. We also want to be able to compare the
    results of both (`spacy` and `nltk`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: POS tagging using nltk'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform the same analysis using spacy as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: POS tagging using spacy'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the two outputs in *Figures 7.14* and *7.15*. The two libraries
    generate slightly different POS results. Some of the differences are due to the
    different mapping of actual parts of speech on categories. For `nltk`, the word
    “is” represents an `AUX` (auxiliary), while the same “is” for `spacy` is a verb.
    `spacy` distinguishes between proper nouns (names of persons, places, and so on)
    and regular nouns (`NOUN`), whereas `nltk` does not differentiate.
  prefs: []
  type: TYPE_NORMAL
- en: With some phrases having a non-standard structure, both outputs wrongly identify
    the verb “Go” as a noun (`nltk`) and a proper noun (`spacy`). In the case of `spacy`,
    it is somewhat expected, since “Go” is written in uppercase after a comma. spacy
    differentiates between a coordinating conjunction (**CCONJ**) and a subordinating
    conjunction (**SCONJ**), while `nltk` will only recognize that there are conjunctions
    (**CONJ**).
  prefs: []
  type: TYPE_NORMAL
- en: With the same library extension for spacy that we used to highlight NER in the
    previous subsection, we can also represent the syntactic structure of phrases
    and paragraphs. In *Figure 7.16*, we show one example of such a representation.
    In the notebook, we show all the comment (set of phrases) visualizations using
    `displacy` with the “dep” (dependency) flag.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram, sketch  Description automatically generated](img/B20963_07_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: POS tagging using spacy and dependency to show phrase structure
    with the dependency between the parts of speech'
  prefs: []
  type: TYPE_NORMAL
- en: We saw how we can use dependency to show entities and the category for each
    of them using dependency, and also how we can use the same function to show both
    the parts of speech and the phrase structure. With inspiration from *Reference
    11*, we extended the code sample given there (and transitioned from using `nltk`
    to `spacy` for POS extraction, given that nltk is not fully aligned with spacy)
    so that we can show the parts of speech highlighted in the same way as we represented
    the named entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified code (including some minor bug fixing, besides the changes mentioned
    already) from *Reference 11* is given here (code explanation follows after the
    code block):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let’s understand the preceding code better. In the function `visualise_pos`,
    we first define a mapping between parts of speech and colors (how a part of speech
    will be highlighted). Then, we define the parts of speech that we will consider.
    Then, we correct a bug existing in the original code (from *Reference 11*) using
    some replacements for special characters. We also use a spacy tokenizer and add,
    in the `tags` list, the text and part of speech for each `pos` extracted using
    `nlp` from `spacy`. Then, we calculate the position of each `pos` identified and
    create a dictionary with the `pos` tokens and their position in the text, to be
    able to highlight them with different colors. At the end, we render the document
    with all `pos` highlighted using `displacy`.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7.17*, we show the result of applying this procedure to our sample
    of comments. We can now see some of the errors of spacy easier. In the second
    comment, it misinterprets the second “Go” as a proper noun (**PROPN**). This is
    somewhat explainable, since normally, after a comma, only proper nouns will be
    written in uppercase in English.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: POS tagging using spacy and dependency with the procedure modified
    from Reference 8 to show POSs highlighted inline in text'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe other errors as well. In the first comment, “Trump” appears as
    **NOUN** – that is, a simple noun. The term “republicans” is categorized as **PROPN**,
    which is likely accurate in the context of U.S. politics where “Republicans” is
    treated as a proper noun. However, in our context, this is inaccurate, as it represents
    a simple noun in plural form, identifying a group of individuals advocating for
    a republican government.
  prefs: []
  type: TYPE_NORMAL
- en: We reviewed several NLP techniques that helped us to get a better understanding
    of the word distribution, topics, POS, and concepts present in the text. Optionally,
    we can also use these techniques to generate features to include in a machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will start the analysis targeted at preparing a supervised
    NLP model for the classification of comments.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model preparation, depending on the method we will implement, might be more
    or less complex. In our case, we opt to start the first baseline model with a
    simple deep learning architecture (which was the standard approach at the time
    of the competition), including a word embeddings layer (using pretrained word
    embeddings) and one or more bidirectional LSTM layers. This architecture was a
    common choice at the time when this competition took place, and it is still a
    good option for a baseline for a text classification problem. **LSTM** stands
    for **Long Short-Term Memory**. It is a type of recurrent neural network architecture
    designed to capture and remember long-term dependencies in sequential data. It
    is particularly effective for text classification problems due to its ability
    to handle and model intricate relationships and dependencies in sequences of text.
  prefs: []
  type: TYPE_NORMAL
- en: For this, we will need to perform some comment data preprocessing (we also performed
    preprocessing when preparing to build the topic modeling model). This time, we
    will perform the preprocessing steps gradually, and monitor how these steps are
    affecting the result not of the model, but of one prerequisite of a well-performing
    language model, which is the vocabulary coverage of the word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: We will then use word embeddings in the first baseline model to extend the generalization
    power of our model so that the words that are not present in the train set but
    are in the test set would benefit from the vicinity of words that exist in the
    word embeddings. Finally, to ensure that our approach will be effective, we will
    need the pretrained word embeddings to have as large a vocabulary coverage as
    possible. Thus, we will also measure the vocabulary coverage and suggest methods
    for improving it.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we start by building the initial vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Building the vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We performed the earlier experiments with word frequency, word distribution
    associated with various values of the target and other features, topic modeling,
    NER, and POS tagging with a subset of the entire comments corpus. For the following
    experiment, we will start using the entire dataset. We will use word embeddings
    with an embedding size of 300.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are numerical representations of a word. They map words to vectors.
    The embedding size refers to the number of components (or dimensions) of these
    vectors. This procedure enables computers to understand and compare relationships
    between words. Because all the words are first transformed using word embeddings
    (and in the word embeddings space, the relationship between words is represented
    by relationships between vectors), words with similar meanings will be represented
    by vectors aligned in the word embeddings space.
  prefs: []
  type: TYPE_NORMAL
- en: At testing time, new words, not present in the training data, will be represented
    in the word embeddings space as well, and their relationship with other words,
    present in the training data, will be exploited by the algorithm. The effect will
    be to enhance the algorithm we are using for text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will set the number of characters (or length of comments) to
    a fixed number; we chose this dimension to be 220\. For shorter comments, we will
    pad the comment sequence (that is, add spaces), and for larger comment sequences,
    we will truncate them (to 220 characters). This procedure will ensure we will
    have inputs for the machine learning model with the same dimension. Let’s first
    define a function for building the vocabulary. For building these functions, we
    used sources from *References 12* and *13*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is used to build the vocabulary (that is, the corpus of
    words present in the comments). We apply a split on each comment and gather all
    data in a list of sentences. We then parse all these sentences to create a dictionary
    with the vocabulary. Each time a word parsed is found as a key in the dictionary,
    we increment the value associated with the key. What we obtain is a vocabulary
    dictionary with the count (or overall frequency) of each word in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the overall vocabulary by concatenating `train` and `test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the first 10 elements in the vocabulary to have an intuition of
    what this vocabulary looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The following image shows the result of running the preceding code. It shows
    the most frequent words in the text. As expected, the most used words are some
    of the most frequently used words in the English language.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: Vocabulary without any preprocessing – uppercase and lowercase
    words, and possibly wrongly spelled expressions'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the earlier introduced function, `build_vocabulary`, repeatedly
    every time we perform an additional (sometimes repeated) text transformation.
    We perform successive text transformations to ensure that, while using pretrained
    word embeddings, we have good coverage with the words in the pretrained word embeddings
    of the vocabulary in the comments. With a larger coverage, we ensure a better
    accuracy of the model that we are building. Let’s continue by loading some pretrained
    word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding index and embedding matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now build a dictionary with the words in word embeddings as keys and
    the arrays of their embedding representations as values. We call this dictionary
    the embedding index. We will then also build the embedding matrix, which is a
    matrix representation of embeddings. We will use GloVe’s pretrained embeddings
    (with 300 dimensions) for our experiments. **GloVe** stands for **Global Vectors
    for Word Representation** and is an unsupervised algorithm that produces word
    embeddings. It works by analyzing global text statistics over a very large text
    corpus to create vector representations and capture semantic relationships between
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code loads the pretrained word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The size of the embedding structure obtained is 2.19 million items. Next, we
    create the embedding matrix using the word index and the embedding index we just
    created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We use the parameter `MAX_FEATURES` to limit the dimension of the embedding
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Checking vocabulary coverage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced the functions to read the word embeddings and compute the embedding
    matrix. Now we will continue with introducing the functions to evaluate the vocabulary
    coverage with words from the word embeddings. The larger the vocabulary coverage,
    the better the accuracy of the model we are building.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the coverage of the vocabulary by the embeddings, we are going to
    use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code browses through all the vocabulary items (that is, the words
    present in the comments text) and counts the unknown words (that is, words in
    the text but not in the list of embeddings words). Then, it calculates the percentage
    of words in the vocabulary that exist in the word embeddings index. This percentage
    is calculated in two ways: with each word in the vocabulary unweighted and with
    words weighted by their frequency in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: We will apply this function repeatedly to check the vocabulary coverage after
    each step of preprocessing. Let’s start with checking the vocabulary coverage
    for the initial vocabulary, where we haven’t applied any preprocessing to the
    comments text yet.
  prefs: []
  type: TYPE_NORMAL
- en: Iteratively improving vocabulary coverage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We apply the function `check_coverage` to check the vocabulary coverage, passing
    the two parameters: vocabulary and embedding matrix. In the following notation,
    **oov** stands for **out of vocabulary**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the first iteration is not great. Although we have almost 90%
    of the text covered, only 15.5% of the words in the vocabulary are covered by
    the word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.19: Vocabulary coverage – first iteration'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look at the list of not covered terms. Because, in `oov_glove`,
    we stored the not covered terms in descending order by the number of appearances
    in the corpus, we can see, by selecting the first terms in this list, the most
    important words not included in the word embeddings. In *Figure 7.20*, we show
    the first 10 terms in this list – the top 10 words not covered. Here, *not covered*
    refers to words that appear in the vocabulary (are present in the comments texts)
    but not in the word embeddings index (are not present in the pretrained word embeddings):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_07_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.20: Most frequent 10 words from the vocabulary not covered by the
    word embeddings in the first iteration'
  prefs: []
  type: TYPE_NORMAL
- en: By quickly inspecting the list in *Figure 7.20*, we see that some of the frequent
    words are either contracted, or colloquial, non-standard forms of spoken English.
    It is normal to see such forms in online comments. We will perform several steps
    of preprocessing to try to improve the vocabulary coverage by correcting the issues
    we find. After each step, we will also measure the vocabulary coverage again.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming to lowercase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will start by converting all text to lowercase and adding it to the vocabulary.
    In word embeddings, the words will be all lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply this lowercase transformation to both the train and test sets and
    then we rebuild the vocabulary and calculate the vocabulary coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.21* shows the new vocabulary coverage after we applied the lowercase
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A number with black text  Description automatically generated with medium
    confidence](img/B20963_07_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.21: Vocabulary coverage – second iteration with lowercase of all words'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe a few small improvements in the word percentage and text percentage
    coverage. Let’s continue by removing contractions in the comments text.
  prefs: []
  type: TYPE_NORMAL
- en: Removing contractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we will remove contractions. These are modified forms of words and expressions.
    We will use a predefined dictionary of usually encountered contractions. These
    will be mapped on words that exist in embeddings. Because of limited space, we
    are just including here a few examples of items in the contractions dictionary,
    but the entire resource is available in the notebook associated with this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With the following function, we can get the list of known contractions in GloVe
    embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the next function to clean the known contractions from the vocabulary
    – that is, replace them by using the contractions dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After we apply `clean_contractions` to both the train and test sets and again
    apply the function to build the vocabulary and measure vocabulary coverage, we
    get the new stats about the vocabulary coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A number with numbers on it  Description automatically generated with medium
    confidence](img/B20963_07_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.22: Vocabulary coverage – third iteration, after replacing contractions
    using the contractions dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: Further refinement of the contractions dictionary is possible by inspecting
    expressions without coverage and enhancing it to equate not covered expressions
    in the corpus with words or groups of words where each word is represented in
    the embedding vector.
  prefs: []
  type: TYPE_NORMAL
- en: Removing punctuation and special characters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we will remove punctuation and special characters. The following lists
    and functions are useful for this step. First, we list the unknown punctuation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we clean the special characters and punctuation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Let’s check the vocabulary coverage again in *Figure 7.23*. This time, we increased
    the word vocabulary coverage by word embeddings from around 15% to 54%. Additionally,
    text coverage increased from 90% to 99.7%.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of numbers  Description automatically generated](img/B20963_07_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.23: Vocabulary coverage – fourth iteration, after cleaning punctuation
    and special characters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the top 20 words not covered, we see that we have small words with
    accents, special characters, and idiomatic expressions. We extend the punctuation
    dictionary to include the most frequent special characters, and after we run `build_vocabulary`
    and `check_coverage` again, we get a new status of the vocabulary coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There is a trivial improvement this time, but we can continue with addressing
    either frequent expressions or frequent special character replacements until we
    get a significant improvement.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative way to further improve the comments corpus vocabulary coverage
    by the embeddings is to add an additional embedding source to current pretrained
    embeddings. Let’s try this. We used pretrained embeddings from `GloVe`. We can
    also use `FastText` from Facebook. `FastText` is a very practical industry-standard
    library that is commonly used in search and recommendation engines in several
    companies daily. Let us load the embeddings and recreate the embeddings index
    with the combined embeddings vectors.
  prefs: []
  type: TYPE_NORMAL
- en: After we merge both word embedding dictionaries, with dimensions of 2.19 million
    and 2.0 million entries (both with a vector dimension of 300), we obtain a dictionary
    with a dimension of 2.8 million entries (due to many common words in the two dictionaries).
    We then recalculate the vocabulary coverage. In *Figure 7.24*, we show the result
    of this operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of numbers  Description automatically generated](img/B20963_07_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.24: Vocabulary coverage – fifth iteration, after adding the FastText
    pretrained word embeddings to the initial GloVe embedding dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize our process here, our intention was to build a baseline solution
    based on the use of pretrained word embeddings. We introduced two pretrained word
    embedding algorithms, `GloVe` and `FastText`. Pretrained means that we used the
    already trained algorithms; we didn’t calculate the word embeddings from the corpus
    of comments in our dataset. To be effective, we need to ensure that we have good
    coverage with these word embeddings of the comments text vocabulary. Initially,
    the coverage was rather poor (15% of the vocabulary and 86% of the entire text).
    We improved these statistics gradually by transforming to lowercase, removing
    contractions, removing punctuation, and replacing special characters. In the last
    step, we extended the embeddings dictionary by adding pretrained embeddings from
    an alternative source. In the end, we were able to ensure a 56% coverage of the
    vocabulary and 99.75% of the entire text.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to go ahead and create a baseline model in a separate notebook.
    We will only reuse a part of the functions we created for the experiments in the
    current notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Building a baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These days, everybody will build a baseline model by at least fine-tuning a
    Transformer architecture. Since the 2017 paper *Attention Is All You Need* (*Reference
    14*), the performance of these solutions has continuously improved, and for competitions
    like *Jigsaw Unintended Bias in Toxicity Classification*, a recent Transformer-based
    solution will probably take you easily into the gold zone.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will start with a more classical baseline. The core of
    this solution is based on contributions from Christof Henkel (Kaggle nickname:
    Dieter), Ane Berasategi (Kaggle nickname: Ane), Andrew Lukyanenko (Kaggle nickname:
    Artgor), Thousandvoices (Kaggle nickname), and Tanrei (Kaggle nickname); see *References*
    *12*, *13*, *15*, *16*, *17*, and *18*.'
  prefs: []
  type: TYPE_NORMAL
- en: The solution includes four steps. In the first step, we load the train and test
    data as `pandas` datasets and then we perform preprocessing on the two datasets.
    The preprocessing is largely based on the preprocessing steps we performed before,
    and hence, we won’t repeat those steps here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second step, we perform tokenization and prepare the data to present
    it to the model. The tokenization is performed as shown in the following code
    excerpt (we are not showing the entire procedure here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We used a basic tokenizer here from `keras.preprocessing.text`. After tokenization,
    each input sequence is padded with a predefined `MAX_LEN`, which was selected
    as an optimum considering the average/median length of sequences for the entire
    comments corpus and also considering the available memory and runtime constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third step, we build the embedding matrix and the model structure. The
    code for building the embedding matrix is largely based on the procedures we already
    presented in the previous sections. Here, we just systematize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The model is a deep learning architecture with a word embeddings layer, a `SpatialDropout1D`
    layer, two bidirectional LSTM layers, a concatenation of `GlobalMaxPooling1D`
    with a `GlobalAveragePooling1D`, two dense layers with `'relu'` activation, and
    one dense layer with `'sigmoid'` activation for the target output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the word embedding layer, the input is transformed so that each word is
    represented by its corresponding vector. After this transformation, the information
    about the semantic distance between words in the input is captured by the model.
    The `SpatialDropout1D` layer helps prevent overfitting by randomly deactivating
    neurons during training (the coefficient gives the percentage of neurons deactivated
    each epoch). The bidirectional LSTM layer’s role is to process the input sequences
    in both forward and backward directions, enhancing contextual understanding for
    better predictions. The role of the `GlobalAveragePooling1D` layer is to compute
    the average of each feature across the entire sequence, reducing the dimensionality
    while retaining essential information in the 1D (sequential) data. This amounts
    to revealing a latent representation of the sequences. The dense layers’ output
    is the prediction of the model. See *References 17* and *18* for more details
    regarding the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the fourth step, we run the training, prepare the submission, and submit.
    To reduce the memory used during runtime, we are using temporary storage and performing
    garbage collection after deleting non-used allocated data. We run the model twice
    for a specified number of `NUM_EPOCHS` (representing one complete pass of training
    data through the algorithm) and then average the test predictions using variable
    weights. Then we submit the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With this solution (for the full code, see *Reference 16*), we can obtain, via
    a late submission, a core of 0.9328 and, consequently, a ranking in the upper
    half of the private leaderboard. Next, we will show how, by using a Transformer-based
    solution, we can obtain a higher score, in the upper silver medal or even gold
    medal zone for this competition.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of the competition, BERT and some other Transformer models were
    already available and a few solutions with high scores were provided. Here, we
    will not attempt to replicate them but we will just point out the most accessible
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In *Reference 20*, Qishen Ha combines a few solutions, including BERT-Small
    V2, BERT-Large V2, XLNet, and GPT-2 (fine-tuned models using competition data
    included as datasets) to obtain a 0.94656 private leaderboard score (late submission),
    which would put you in the top 10 (both gold medal and prize area for this competition).
  prefs: []
  type: TYPE_NORMAL
- en: A solution with only the BERT-Small model (see *Reference 21*) will yield a
    private leaderboard score of 0.94295\. Using the BERT-Large model (see *Reference
    22*) will result in a private leaderboard score of 0.94388\. Both these solutions
    will be in the silver medal zone (around places 130 and 80, respectively, in the
    private leaderboard, as late submissions).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to work with text data, using various approaches
    to explore this type of data. We started by analyzing our target and text data,
    preprocessing text data to include it in a machine learning model. We also explored
    various NLP tools and techniques, including topic modeling, NER, and POS tagging,
    and then prepared the text to build a baseline model, passing through an iterative
    process to gradually improve the data quality for the objective set (in this case,
    the objective being to improve the coverage of word embeddings for the vocabulary
    in the corpus of text from the competition dataset).
  prefs: []
  type: TYPE_NORMAL
- en: We introduced and discussed a baseline model (based on the work of several Kaggle
    contributors). This baseline model architecture includes a word embedding layer
    and bidirectional LSTM layers. Finally, we looked at some of the most advanced
    solutions available, based on Transformer architectures, either as single models
    or combined, to get a late submission with a score in the upper part of the leaderboard
    (silver and gold zones).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start working with signal data. We will introduce
    data formats specific to various signal modalities (sound, image, video, experimental,
    or sensor data). We will analyze the data from the *LANL Earthquake Prediction*
    Kaggle competition.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jigsaw Unintended Bias in Toxicity Classification, Kaggle competition dataset:
    [https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/
    )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Aja Bogdanoff, Saying goodbye to Civil Comments, Medium: [https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d](mailto:https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda, Jigsaw Comments Text Exploration: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda, Jigsaw Simple Baseline: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Susan Li, Topic Modeling and Latent Dirichlet Allocation (LDA) in Python: [https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Aneesha Bakharia, Improving the Interpretation of Topic Models: [https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d](https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d
    )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Carson Sievert, Kenneth Shirley, LDAvis: A method for visualizing and interpreting
    topics: [https://www.aclweb.org/anthology/W14-3110](https://www.aclweb.org/anthology/W14-3110)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lucia Dosin, Experiments on Topic Modeling – PyLDAvis: [https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/](https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Renato Aranha, Topic Modelling (LDA) on Elon Tweets: [https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets](https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Latent Dirichlet Allocation, Wikipedia: [https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leonie Monigatti, Visualizing Part-of-Speech Tags with NLTK and SpaCy: [https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e](https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ane, Quora preprocessing + model: [https://www.kaggle.com/anebzt/quora-preprocessing-model](https://www.kaggle.com/anebzt/quora-preprocessing-model)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Christof Henkel (Dieter), How to: Preprocessing when using embeddings: [https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser, Illia Polosukhin, Attention Is All You Need: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Christof Henkel (Dieter), keras baseline lstm + attention 5-fold: [https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold
    )'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Andrew Lukyanenko, CNN in keras on folds: [https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds](https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thousandvoices, Simple LSTM: [https://www.kaggle.com/code/thousandvoices/simple-lstm/s](https://www.kaggle.com/code/thousandvoices/simple-lstm/s)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tanrei, Simple LSTM using Identity Parameters Solution: [https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution](https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda, Jigsaw Simple Baseline: [https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline](https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Qishen Ha, Jigsaw_predict: [https://www.kaggle.com/code/haqishen/jigsaw-predict/](https://www.kaggle.com/code/haqishen/jigsaw-predict/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda, Jigsaw_predict_BERT_small: [https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small](https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda, Jigsaw_predict_BERT_large: [https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large](https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code9220780366773140.png)'
  prefs: []
  type: TYPE_IMG
