["```py\n// sigmoid implements the sigmoid function\n// for use in activation functions.\nfunc sigmoid(x float64) float64 {\n    return 1.0 / (1.0 + math.Exp(-x))\n}\n\n// sigmoidPrime implements the derivative\n// of the sigmoid function for backpropagation.\nfunc sigmoidPrime(x float64) float64 {\n    return x * (1.0 - x)\n}\n```", "```py\n// neuralNet contains all of the information\n// that defines a trained neural network.\ntype neuralNet struct {\n    config neuralNetConfig\n    wHidden *mat.Dense\n    bHidden *mat.Dense\n    wOut *mat.Dense\n    bOut *mat.Dense\n}\n\n// neuralNetConfig defines our neural network\n// architecture and learning parameters.\ntype neuralNetConfig struct {\n    inputNeurons int\n    outputNeurons int\n    hiddenNeurons int\n    numEpochs int\n    learningRate float64\n}\n```", "```py\n// NewNetwork initializes a new neural network.\nfunc newNetwork(config neuralNetConfig) *neuralNet {\n        return &neuralNet{config: config}\n}\n\n// Train trains a neural network using backpropagation.\nfunc (nn *neuralNet) train(x, y *mat.Dense) error {\n\n    // To be filled in...\n}\n```", "```py\n// Initialize biases/weights.\nrandSource := rand.NewSource(time.Now().UnixNano())\nrandGen := rand.New(randSource)\n\nwHiddenRaw := make([]float64, nn.config.hiddenNeurons*nn.config.inputNeurons)\nbHiddenRaw := make([]float64, nn.config.hiddenNeurons)\nwOutRaw := make([]float64, nn.config.outputNeurons*nn.config.hiddenNeurons)\nbOutRaw := make([]float64, nn.config.outputNeurons)\n\nfor _, param := range [][]float64{wHiddenRaw, bHiddenRaw, wOutRaw, bOutRaw} {\n    for i := range param {\n        param[i] = randGen.Float64()\n    }\n}\n\nwHidden := mat.NewDense(nn.config.inputNeurons, nn.config.hiddenNeurons, wHiddenRaw)\nbHidden := mat.NewDense(1, nn.config.hiddenNeurons, bHiddenRaw)\nwOut := mat.NewDense(nn.config.hiddenNeurons, nn.config.outputNeurons, wOutRaw)\nbOut := mat.NewDense(1, nn.config.outputNeurons, bOutRaw)\n```", "```py\n// Define the output of the neural network.\noutput := mat.NewDense(0, 0, nil)\n\n// Loop over the number of epochs utilizing\n// backpropagation to train our model.\nfor i := 0; i < nn.config.numEpochs; i++ {\n\n    // Complete the feed forward process.\n\n    ...\n\n    // Complete the backpropagation.\n\n    ...\n\n    // Adjust the parameters.\n\n    ...\n}\n```", "```py\n// Complete the feed forward process.\nhiddenLayerInput := mat.NewDense(0, 0, nil)\nhiddenLayerInput.Mul(x, wHidden)\naddBHidden := func(_, col int, v float64) float64 { return v + bHidden.At(0, col) }\nhiddenLayerInput.Apply(addBHidden, hiddenLayerInput)\n\nhiddenLayerActivations := mat.NewDense(0, 0, nil)\napplySigmoid := func(_, _ int, v float64) float64 { return sigmoid(v) }\nhiddenLayerActivations.Apply(applySigmoid, hiddenLayerInput)\n\noutputLayerInput := mat.NewDense(0, 0, nil)\noutputLayerInput.Mul(hiddenLayerActivations, wOut)\naddBOut := func(_, col int, v float64) float64 { return v + bOut.At(0, col) }\noutputLayerInput.Apply(addBOut, outputLayerInput)\noutput.Apply(applySigmoid, outputLayerInput)\n```", "```py\n// Complete the backpropagation.\nnetworkError := mat.NewDense(0, 0, nil)\nnetworkError.Sub(y, output)\n\nslopeOutputLayer := mat.NewDense(0, 0, nil)\napplySigmoidPrime := func(_, _ int, v float64) float64 { return sigmoidPrime(v) }\nslopeOutputLayer.Apply(applySigmoidPrime, output)\nslopeHiddenLayer := mat.NewDense(0, 0, nil)\nslopeHiddenLayer.Apply(applySigmoidPrime, hiddenLayerActivations)\n\ndOutput := mat.NewDense(0, 0, nil)\ndOutput.MulElem(networkError, slopeOutputLayer)\nerrorAtHiddenLayer := mat.NewDense(0, 0, nil)\nerrorAtHiddenLayer.Mul(dOutput, wOut.T())\n\ndHiddenLayer := mat.NewDense(0, 0, nil)\ndHiddenLayer.MulElem(errorAtHiddenLayer, slopeHiddenLayer)\n```", "```py\n// Adjust the parameters.\nwOutAdj := mat.NewDense(0, 0, nil)\nwOutAdj.Mul(hiddenLayerActivations.T(), dOutput)\nwOutAdj.Scale(nn.config.learningRate, wOutAdj)\nwOut.Add(wOut, wOutAdj)\n\nbOutAdj, err := sumAlongAxis(0, dOutput)\nif err != nil {\n    return err\n}\nbOutAdj.Scale(nn.config.learningRate, bOutAdj)\nbOut.Add(bOut, bOutAdj)\n\nwHiddenAdj := mat.NewDense(0, 0, nil)\nwHiddenAdj.Mul(x.T(), dHiddenLayer)\nwHiddenAdj.Scale(nn.config.learningRate, wHiddenAdj)\nwHidden.Add(wHidden, wHiddenAdj)\n\nbHiddenAdj, err := sumAlongAxis(0, dHiddenLayer)\nif err != nil {\n    return err\n}\nbHiddenAdj.Scale(nn.config.learningRate, bHiddenAdj)\nbHidden.Add(bHidden, bHiddenAdj)\n```", "```py\n// sumAlongAxis sums a matrix along a\n// particular dimension, preserving the\n// other dimension.\nfunc sumAlongAxis(axis int, m *mat.Dense) (*mat.Dense, error) {\n\n    numRows, numCols := m.Dims()\n\n    var output *mat.Dense\n\n    switch axis {\n    case 0:\n        data := make([]float64, numCols)\n        for i := 0; i < numCols; i++ {\n            col := mat.Col(nil, i, m)\n            data[i] = floats.Sum(col)\n        }\n        output = mat.NewDense(1, numCols, data)\n    case 1:\n        data := make([]float64, numRows)\n        for i := 0; i < numRows; i++ {\n            row := mat.Row(nil, i, m)\n            data[i] = floats.Sum(row)\n        }\n        output = mat.NewDense(numRows, 1, data)\n    default:\n        return nil, errors.New(\"invalid axis, must be 0 or 1\")\n    }\n\n    return output, nil\n}\n```", "```py\n// Define our trained neural network.\nnn.wHidden = wHidden\nnn.bHidden = bHidden\nnn.wOut = wOut\nnn.bOut = bOut\n\nreturn nil\n```", "```py\n// Define our input attributes.\ninput := mat.NewDense(3, 4, []float64{\n    1.0, 0.0, 1.0, 0.0,\n    1.0, 0.0, 1.0, 1.0,\n    0.0, 1.0, 0.0, 1.0,\n})\n\n// Define our labels.\nlabels := mat.NewDense(3, 1, []float64{1.0, 1.0, 0.0})\n\n// Define our network architecture and\n// learning parameters.\nconfig := neuralNetConfig{\n    inputNeurons: 4,\n    outputNeurons: 1,\n    hiddenNeurons: 3,\n    numEpochs: 5000,\n    learningRate: 0.3,\n}\n\n// Train the neural network.\nnetwork := newNetwork(config)\nif err := network.train(input, labels); err != nil {\n    log.Fatal(err)\n}\n\n// Output the weights that define our network!\nf := mat.Formatted(network.wHidden, mat.Prefix(\" \"))\nfmt.Printf(\"\\nwHidden = % v\\n\\n\", f)\n\nf = mat.Formatted(network.bHidden, mat.Prefix(\" \"))\nfmt.Printf(\"\\nbHidden = % v\\n\\n\", f)\n\nf = mat.Formatted(network.wOut, mat.Prefix(\" \"))\nfmt.Printf(\"\\nwOut = % v\\n\\n\", f)\n\nf = mat.Formatted(network.bOut, mat.Prefix(\" \"))\nfmt.Printf(\"\\nbOut = % v\\n\\n\", f)\n```", "```py\nbOut$ go build\n$ ./myprogram \n\nwHidden = [ 2.105009524680073 2.022752980359344 2.5200192466310547]\n [ -2.033896626042324 -1.8652059633980662 -1.5861504822640748]\n [1.821046795894909 2.436963623058306 1.717510016887383]\n [-0.7400175664445425 -0.5800261988090052 -0.9277709997772002]\n\nbHidden = [ 0.06850421921273529 -0.40252869229501825 -0.03392255287230178]\n\nwOut = [ 2.321901257946553]\n [ 3.343178681830189]\n [1.7581701319375231]\n\nbOut = [-3.471613333924047]\n```", "```py\n$ head train.csv \nsepal_length,sepal_width,petal_length,petal_width,setosa,virginica,versicolor\n0.305555555556,0.583333333333,0.118644067797,0.0416666666667,1.0,0.0,0.0\n0.25,0.291666666667,0.491525423729,0.541666666667,0.0,0.0,1.0\n0.194444444444,0.5,0.0338983050847,0.0416666666667,1.0,0.0,0.0\n0.833333333333,0.375,0.898305084746,0.708333333333,0.0,1.0,0.0\n0.555555555556,0.208333333333,0.661016949153,0.583333333333,0.0,0.0,1.0\n0.416666666667,0.25,0.508474576271,0.458333333333,0.0,0.0,1.0\n0.666666666667,0.416666666667,0.677966101695,0.666666666667,0.0,0.0,1.0\n0.416666666667,0.291666666667,0.491525423729,0.458333333333,0.0,0.0,1.0\n0.5,0.416666666667,0.661016949153,0.708333333333,0.0,1.0,0.0\n```", "```py\n$ wc -l *.csv\n 31 test.csv\n 121 train.csv\n 152 total\n```", "```py\n// Open the training dataset file.\nf, err := os.Open(\"train.csv\")\nif err != nil {\n    log.Fatal(err)\n}\ndefer f.Close()\n\n// Create a new CSV reader reading from the opened file.\nreader := csv.NewReader(f)\nreader.FieldsPerRecord = 7\n\n// Read in all of the CSV records\nrawCSVData, err := reader.ReadAll()\nif err != nil {\n    log.Fatal(err)\n}\n\n// inputsData and labelsData will hold all the\n// float values that will eventually be\n// used to form our matrices.\ninputsData := make([]float64, 4*len(rawCSVData))\nlabelsData := make([]float64, 3*len(rawCSVData))\n\n// inputsIndex will track the current index of\n// inputs matrix values.\nvar inputsIndex int\nvar labelsIndex int\n\n// Sequentially move the rows into a slice of floats.\nfor idx, record := range rawCSVData {\n\n    // Skip the header row.\n    if idx == 0 {\n        continue\n    }\n\n    // Loop over the float columns.\n    for i, val := range record {\n\n        // Convert the value to a float.\n        parsedVal, err := strconv.ParseFloat(val, 64)\n        if err != nil {\n            log.Fatal(err)\n        }\n\n        // Add to the labelsData if relevant.\n        if i == 4 || i == 5 || i == 6 {\n            labelsData[labelsIndex] = parsedVal\n            labelsIndex++\n            continue\n        }\n\n        // Add the float value to the slice of floats.\n        inputsData[inputsIndex] = parsedVal\n        inputsIndex++\n    }\n}\n\n// Form the matrices.\ninputs := mat.NewDense(len(rawCSVData), 4, inputsData)\nlabels := mat.NewDense(len(rawCSVData), 3, labelsData)\n```", "```py\n// Define our network architecture and\n// learning parameters.\nconfig := neuralNetConfig{\n    inputNeurons:  4,\n    outputNeurons: 3,\n    hiddenNeurons: 3,\n    numEpochs:     5000,\n    learningRate:  0.3,\n}\n\n// Train the neural network.\nnetwork := newNetwork(config)\nif err := network.train(inputs, labels); err != nil {\n    log.Fatal(err)\n}\n```", "```py\n// predict makes a prediction based on a trained\n// neural network.\nfunc (nn *neuralNet) predict(x *mat.Dense) (*mat.Dense, error) {\n\n    // Check to make sure that our neuralNet value\n    // represents a trained model.\n    if nn.wHidden == nil || nn.wOut == nil || nn.bHidden == nil || nn.bOut == nil {\n        return nil, errors.New(\"the supplied neural net weights and biases are empty\")\n    }\n\n    // Define the output of the neural network.\n    output := mat.NewDense(0, 0, nil)\ngithub.com/tensorflow/tensorflow/tensorflow/go\n    // Complete the feed forward process.\n    hiddenLayerInput := mat.NewDense(0, 0, nil)\n    hiddenLayerInput.Mul(x, nn.wHidden)\n    addBHidden := func(_, col int, v float64) float64 { return v + nn.bHidden.At(0, col) }\n    hiddenLayerInput.Apply(addBHidden, hiddenLayerInput)\n\n    hiddenLayerActivations := mat.NewDense(0, 0, nil)\n    applySigmoid := func(_, _ int, v float64) float64 { return sigmoid(v) }\n    hiddenLayerActivations.Apply(applySigmoid, hiddenLayerInput)\n\n    outputLayerInput := mat.NewDense(0, 0, nil)\n    outputLayerInput.Mul(hiddenLayerActivations, nn.wOut)\n    addBOut := func(_, col int, v float64) float64 { return v + nn.bOut.At(0, col) }\n    outputLayerInput.Apply(addBOut, outputLayerInput)\n    output.Apply(applySigmoid, outputLayerInput)\n\n    return output, nil\n}\n```", "```py\n// Make the predictions using the trained model.\npredictions, err := network.predict(testInputs)\nif err != nil {\n    log.Fatal(err)\n}\n\n// Calculate the accuracy of our model.\nvar truePosNeg int\nnumPreds, _ := predictions.Dims()\nfor i := 0; i < numPreds; i++ {\n\n    // Get the label.\n    labelRow := mat.Row(nil, i, testLabels)\n    var species int\n    for idx, label := range labelRow {\n        if label == 1.0 {\n            species = idx\n            break\n        }\n    }\n\n    // Accumulate the true positive/negative count.\n    if predictions.At(i, species) == floats.Max(mat.Row(nil, i, predictions)) {\n        truePosNeg++\n    }\n}\n\n// Calculate the accuracy (subset accuracy).\naccuracy := float64(truePosNeg) / float64(numPreds)\n\n// Output the Accuracy value to standard out.\nfmt.Printf(\"\\nAccuracy = %0.2f\\n\\n\", accuracy)\n```", "```py\n$ go build\n$ ./myprogram \n\nAccuracy = 0.97\n```", "```py\n$ go get github.com/tensorflow/tensorflow/tensorflow/go\n```", "```py\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\nok github.com/tensorflow/tensorflow/tensorflow/go 0.045s\n```", "```py\n$ mkdir model\n$ cd model\n$ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\n--2017-09-09 18:29:03-- https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.6.112, 2607:f8b0:4009:812::2010\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.6.112|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 49937555 (48M) [application/zip]\nSaving to: ‘inception5h.zip’\n\ninception5h.zip 100%[=========================================================================================================================>] 47.62M 19.0MB/s in 2.5s \n\n2017-09-09 18:29:06 (19.0 MB/s) - ‘inception5h.zip’ saved [49937555/49937555]\n\n$ unzip inception5h.zip\nArchive: inception5h.zip\n inflating: imagenet_comp_graph_label_strings.txt \n inflating: tensorflow_inception_graph.pb \n inflating: LICENSE\n```", "```py\n// Load the serialized GraphDef from a file.\nmodelfile, labelsfile, err := modelFiles(*modeldir)\nif err != nil {\n    log.Fatal(err)\n}\nmodel, err := ioutil.ReadFile(modelfile)\nif err != nil {\n    log.Fatal(err)\n} \n```", "```py\n// Construct an in-memory graph from the serialized form.\ngraph := tf.NewGraph()\nif err := graph.Import(model, \"\"); err != nil {\n    log.Fatal(err)\n}\n\n// Create a session for inference over graph.\nsession, err := tf.NewSession(graph, nil)\nif err != nil {\n    log.Fatal(err)\n}\ndefer session.Close()\n```", "```py\n// Run inference on *imageFile.\n// For multiple images, session.Run() can be called in a loop (and\n// concurrently). Alternatively, images can be batched since the model\n// accepts batches of image data as input.\ntensor, err := makeTensorFromImage(*imagefile)\nif err != nil {\n    log.Fatal(err)\n}\noutput, err := session.Run(\n    map[tf.Output]*tf.Tensor{\n        graph.Operation(\"input\").Output(0): tensor,\n    },\n    []tf.Output{\n        graph.Operation(\"output\").Output(0),\n    },\n    nil)\nif err != nil {\n    log.Fatal(err)\n}\n\n// output[0].Value() is a vector containing probabilities of\n// labels for each image in the \"batch\". The batch size was 1.\n// Find the most probable label index.\nprobabilities := output[0].Value().([][]float32)[0]\nprintBestLabel(probabilities, labelsfile)\n```", "```py\n$ ./myprogram -dir=<path/to/the/model/dir> -image=<path/to/a/jpg/image>\n```", "```py\n$ go build\n$ ./myprogram -dir=model -image=airplane.jpg\n2017-09-09 20:17:30.655757: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:17:30.655807: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:17:30.655814: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:17:30.655818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:17:30.655822: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nBEST MATCH: (86% likely) airliner\n```", "```py\n$ ./myprogram -dir=model -image=pug.jpg \n2017-09-09 20:20:32.323855: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:20:32.323896: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:20:32.323902: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:20:32.323906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:20:32.323911: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nBEST MATCH: (84% likely) pug\n```", "```py\n$ ./myprogram -dir=model -image=gopher.jpg \n2017-09-09 20:25:57.967753: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:25:57.967801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:25:57.967808: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:25:57.967812: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-09-09 20:25:57.967817: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nBEST MATCH: (12% likely) safety pin\n```"]