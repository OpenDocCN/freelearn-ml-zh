- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing Feature Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning algorithms are sensitive to the variable scale. For example,
    the coefficients of linear models depend on the scale of the feature – that is,
    changing the feature scale will change the coefficient’s value. In linear models,
    as well as in algorithms that depend on distance calculations such as clustering
    and principal component analysis, features with larger value ranges tend to dominate
    over features with smaller ranges. Therefore, having features on a similar scale
    allows us to compare feature importance and may help algorithms converge faster,
    improving performance and training times.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling techniques, in general, divide the variables by some constant; therefore,
    it is important to highlight that the shape of the variable distribution does
    not change when we rescale the variables. If you want to change the distribution
    shape, check out [*Chapter 3*](B22396_03.xhtml#_idTextAnchor351), *Transforming*
    *Numerical Variables*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will describe different methods to set features on a similar
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling to the maximum and minimum values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling with the median and quantiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing mean normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing maximum absolute scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling to vector unit length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main libraries that we use in this chapter are scikit-learn (`sklearn`)
    for scaling, `pandas` to handle the data, and `matplotlib` for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Standardization is the process of centering the variable at `0` and standardizing
    the variance to `1`. To standardize features, we subtract the mean from each observation
    and then divide the result by the standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>s</mi><mi>t</mi><mi>d</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/25.png)'
  prefs: []
  type: TYPE_IMG
- en: The result of the preceding transformation is called the **z-score** and represents
    how many standard deviations a given observation *deviates* from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization is generally useful when models require the variables to be
    centered at zero and data is not sparse (centering sparse data will destroy its
    sparse nature). On the downside, standardization is sensitive to outliers and
    the z-score does not keep the symmetric properties if the variables are highly
    skewed, as we discuss in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With standardization, the variable distribution does not change; what changes
    is the magnitude of their values, as we see in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Distribution of a normal and skewed variable before and after
    standardization.](img/B22396_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Distribution of a normal and skewed variable before and after standardization.
  prefs: []
  type: TYPE_NORMAL
- en: The z-score (*x* axis in the bottom panels) indicates how many standard deviations
    an observation deviates from the mean. When the z-score is `1`, the observation
    lies 1 standard deviation to the right of the mean, whereas when the z-score is
    `-1`, the sample is 1 standard deviation to the left of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: In normally distributed variables, we can estimate the probability of a value
    being greater or smaller than a given z-score, and this probability distribution
    is symmetric. The probability of an observation being smaller than a z-score of
    `-1` is equivalent to the probability of a value being greater than `1` (horizontal
    line in the bottom-left panel). This symmetry is fundamental to many statistical
    tests. In skewed distributions, this symmetry does not hold. As illustrated in
    the bottom-right panel of *Figure 7**.1* (horizontal lines), the probability of
    a value being smaller than `-1` is different from that of being greater than `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The mean and the standard deviation are sensitive to outliers; therefore, the
    features may scale differently from each other in the presence of outliers when
    using standardization.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we often apply standardization ignoring the shape of the distribution.
    However, keep in mind that if the models or tests you are using make assumptions
    about the data’s distribution, you might benefit from transforming the variables
    before standardization, or trying a different scaling method.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll apply standardization to the variables of the California
    housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the required Python packages, classes, and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset from scikit-learn into a DataFrame
    and drop the `Latitude` and `Longitude` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s divide the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll set up the `StandardScaler()` function from scikit-learn and fit
    it to the train set so that it learns each variable’s mean and standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn scalers, like any scikit-learn transformer, return NumPy arrays
    by default. To return `pandas` or `polars` DataFrames, we need to specify the
    output container with the `set_output()` method, as we did in *Step 4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s standardize the train and test sets with the trained scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`StandardScaler()` stores the mean and standard deviation learned from the
    training set during `fit()`. Let’s visualize the learned parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we’ll print the mean values that were learned by `scaler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see the mean values of each variable in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.scale_
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: array([1.89109236e+00, 1.25962585e+01, 2.28754018e+00,                          4.52736275e-01,
    1.14954037e+03, 6.86792905e+00])
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print the descriptive statistics from the original variables in the test
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we see that the variables’ mean values are different
    from zero and the variance varies:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Descriptive statistical parameters of the variables before scaling](img/B22396_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Descriptive statistical parameters of the variables before scaling
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now print the descriptive statistical values from the transformed variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we see that the variables’ mean is now centered at
    `0` and the variance is approximately `1`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Descriptive statistical parameters of the scaled variables showing
    a mean of 0 and variance of approximately 1](img/B22396_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Descriptive statistical parameters of the scaled variables showing
    a mean of 0 and variance of approximately 1
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `AveRooms`, `AveBedrms`, and `AveOccup` variables are highly skewed, which
    can lead to observed values in the test set that are much greater or much smaller
    than those in the training set, and hence we see that the variance deviates from
    `1`. This is to be expected because standardization is sensitive to outliers and
    very skewed distributions.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned, in the *Getting ready* section, that the shape of the distribution
    does not change with standardization. Go ahead and corroborate that by executing
    `X_test.hist()` and then `X_test_scaled.hist()` to compare the variables’ distribution
    before and after the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we standardized the variables of the California housing dataset
    by utilizing scikit-learn. We split the data into train and test sets because
    the parameters for the standardization should be learned from the train set. This
    is to avoid leaking data from the test to the train set during the preprocessing
    steps and to ensure the test set remains naïve to all feature transformation processes.
  prefs: []
  type: TYPE_NORMAL
- en: To standardize these features, we used scikit-learn’s `StandardScaler()` function,
    which is able to learn and store the parameters utilized in the transformation.
    Using `fit()`, the scaler learned each variable’s mean and standard deviation
    and stored them in its `mean_` and `scale_` attributes. Using `transform()`, the
    scaler standardized the variables in the train and test sets. The default output
    of `StandardScaler()` is a NumPy array, but through the `set_output()` parameter,
    we can change the output container to a `pandas` DataFrame, as we did in *Step
    4*, or to `polars`, by setting `transform="polars"`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`StandardScaler()` will subtract the mean and divide it by the standard deviation
    by default. If we want to just center the distributions without standardizing
    the variance, we can do so by setting `with_std=False` when initializing the transformer.
    If we want to set the variance to `1`, without cantering the distribution, we
    can do so by setting `with_mean=False` in *Step 4*.'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling to the maximum and minimum values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scaling to the minimum and maximum values squeezes the values of the variables
    between `0` and `1`. To implement this scaling method, we subtract the minimum
    value from all the observations and divide the result by the value range – that
    is, the difference between the maximum and minimum values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>x</mi><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi
    mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>max</mi><mfenced
    open="(" close=")"><mi>x</mi></mfenced><mo>−</mo><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/26.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling to the minimum and maximum is suitable for variables with very small
    standard deviations, when the models do not require data to be centered at zero,
    and when we want to preserve zero entries in sparse data, such as in one-hot encoded
    variables. On the downside, it is sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scaling to the minimum and maximum value does not change the distribution of
    the variables, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Distribution of a normal and skewed variable before and after
    scaling to the minimum and maximum value](img/B22396_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Distribution of a normal and skewed variable before and after scaling
    to the minimum and maximum value
  prefs: []
  type: TYPE_NORMAL
- en: This scaling method standardizes the maximum value of the variables to a unit
    size. Scaling to the minimum and maximum value tends to be the preferred alternative
    to standardization, and it is suitable for variables with very small standard
    deviations and when we want to preserve zero entries in sparse data, such as in
    one-hot encoded variables, or variables derived from counts, such as bag of words.
    However, this procedure does not center the variables at zero, so if the algorithm
    has that requirement, this method might not be the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Scaling to the minimum and maximum values is sensitive to outliers. If outliers
    are present in the training set, the scaling will squeeze the values toward one
    of the tails. If, on the contrary, outliers are in the test set, the variable
    will show values greater than `1` or smaller than `0` after scaling, depending
    on whether the outlier is on the left or right tail.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we’ll scale the variables of the California housing dataset
    to values between `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing `pandas` and the required classes and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset from scikit-learn into a `pandas`
    DataFrame, dropping the `Latitude` and `Longitude` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s divide the data into training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the scaler and then fit it to the train set so that it learns
    each variable’s minimum and maximum values and the value range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s scale the variables in the train and test sets with the trained
    scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`MinMaxScaler()` stores the maximum and minimum values and the value ranges
    in its `data_max_`, `min_`, and `data_range_` attributes, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can corroborate the minimum values of the transformed variables by executing
    `X_test_scaled.min()`, which will return the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: MedInc           1.000000
  prefs: []
  type: TYPE_NORMAL
- en: HouseAge        1.000000
  prefs: []
  type: TYPE_NORMAL
- en: AveRooms        1.071197
  prefs: []
  type: TYPE_NORMAL
- en: AveBedrms      0.750090
  prefs: []
  type: TYPE_NORMAL
- en: Population     0.456907
  prefs: []
  type: TYPE_NORMAL
- en: AveOccup        2.074553
  prefs: []
  type: TYPE_NORMAL
- en: 'dtype: float64'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import RobustScaler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop(labels=[     "Latitude", "Longitude"], axis=1,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: inplace=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = RobustScaler().set_output(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: transform="pandas")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler.transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.center_
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'RobustScaler():'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This scaling procedure does not change the variable distributions. Go ahead
    and compare the distribution of the variables before and after the transformation
    by using histograms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop(labels=[
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Latitude", "Longitude"], axis=1, inplace=True)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: means = X_train.mean(axis=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: MedInc           3.866667
  prefs: []
  type: TYPE_NORMAL
- en: HouseAge        28.618702
  prefs: []
  type: TYPE_NORMAL
- en: AveRooms         5.423404
  prefs: []
  type: TYPE_NORMAL
- en: AveBedrms        1.094775
  prefs: []
  type: TYPE_NORMAL
- en: Population    1425.157323
  prefs: []
  type: TYPE_NORMAL
- en: AveOccup         3.040518
  prefs: []
  type: TYPE_NORMAL
- en: 'dtype: float64'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: ranges = X_train.max(axis=0)-X_train.min(axis=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: MedInc           14.500200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: HouseAge         51.000000
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AveRooms        131.687179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AveBedrms        33.733333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Population    35679.000000
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AveOccup        598.964286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'dtype: float64'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = (X_train - means) / ranges
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = (X_test - means) / ranges
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: from sklearn.preprocessing import (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: StandardScaler, RobustScaler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_mean = StandardScaler(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_mean=True, with_std=False,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ).set_output(transform="pandas")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_minmax = RobustScaler(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_centering=False,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_scaling=True,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: quantile_range=(0, 100)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ).set_output(transform="pandas")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_mean.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler_minmax.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler_minmax.transform(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler_mean.transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler_minmax.transform(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler_mean.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import matplotlib.pyplot as plt
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import MaxAbsScaler
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data = pd.read_csv("bag_of_words.csv")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = MaxAbsScaler().set_output(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: transform="pandas")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: scaler.fit(data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data_scaled = scaler.transform(data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.max_abs_
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: array([ 7.,  6.,  2.,  2., 11.,  4.,  3.,  6., 52.,  2.])
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data.hist(bins=20, figsize=(20, 20))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: plt.show()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: data_scaled.hist(bins=20, figsize=(20, 20))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: plt.show()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import (
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MaxAbsScaler, StandardScaler)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.pipeline import Pipeline
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop( labels=[ "Latitude",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Longitude"], axis=1, inplace=True)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_mean = StandardScaler(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with_mean=True, with_std=False)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler_maxabs = MaxAbsScaler()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = Pipeline([
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ("scaler_mean", scaler_mean),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ("scaler_max", scaler_maxabs),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ']).set_output(transform="pandas")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler.fit(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler.transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import numpy as np
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.datasets import fetch_california_housing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.model_selection import train_test_split
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from sklearn.preprocessing import Normalizer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X, y = fetch_california_housing(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return_X_y=True, as_frame=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X.drop(labels=[
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"Latitude", "Longitude"], axis=1, inplace=True)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train, X_test, y_train, y_test = train_test_split(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X, y, test_size=0.3, random_state=0)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: scaler = Normalizer(norm='l1')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: X_train_scaled = scaler.fit_transform(X_train)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_test_scaled = scaler.transform(X_test)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: np.round(np.linalg.norm(X_train, ord=1, axis=1), 1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: array([ 255.3,  889.1, 1421.7, ...,  744.6, 1099.5,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1048.9])
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: np.round(np.linalg.norm(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X_train_scaled, ord=1, axis=1), 1)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: array([1., 1., 1., ..., 1., 1., 1.])
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
