<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01" class="calibre1"/>Chapter 1. Getting the Most out of Your Camera System</h1></div></div></div><p class="calibre8">Claude Monet, one of the founders of French Impressionist painting, taught his students to paint only what they <em class="calibre10">saw</em>, not what they <em class="calibre10">knew</em>. He even went as far as to say:</p><div><blockquote class="blockquote2"><p class="calibre19"><em class="calibre10">"I wish I had been born blind and then suddenly gained my sight so that I could begin to paint without knowing what the objects were that I could see before me."</em></p></blockquote></div><p class="calibre8">Monet rejected traditional artistic subjects, which tended to be mystical, heroic, militaristic, or revolutionary. Instead, he relied on his own observations of middle-class life: of social excursions; of sunny gardens, lily ponds, rivers, and the seaside; of foggy boulevards and train stations; and of private loss. With deep sadness, he told his friend, Georges Clemenceau (the future President of France):</p><div><blockquote class="blockquote2"><p class="calibre19"><em class="calibre10">"I one day found myself looking at my beloved wife's dead face and just systematically noting the colors according to an automatic reflex!"</em></p></blockquote></div><p class="calibre8">Monet painted everything according to his personal impressions. Late in life, he even painted the symptoms of his own deteriorating eyesight. He adopted a reddish palette while he suffered from cataracts and a brilliant bluish palette after cataract surgery left his eyes more sensitive, possibly to the near ultraviolet range.</p><p class="calibre8">Like Monet's students, we as scholars of computer vision must confront a distinction between <em class="calibre10">seeing</em> and <em class="calibre10">knowing</em> and likewise between input and processing. Light, a lens, a camera, and a digital imaging pipeline can grant a computer a sense of <em class="calibre10">sight</em>. From the resulting <a id="id0" class="calibre1"/>image data, <strong class="calibre9">machine-learning (ML)</strong> algorithms can extract <em class="calibre10">knowledge</em> or at least a set of meta-senses such as detection, recognition, and reconstruction (scanning). Without proper senses or data, a system's learning potential is limited, perhaps even nil. Thus, when designing any computer vision system, we must consider the foundational requirements in terms of lighting conditions, lenses, cameras, and imaging pipelines.</p><p class="calibre8">What do we require in order to clearly see a given subject? This is the central question of our first chapter. Along the way, we will address five subquestions:</p><div><ul class="itemizedlist"><li class="listitem">What do we require to see fast motion or fast changes in light?</li><li class="listitem">What do we require to see distant objects?</li><li class="listitem">What do we require to see with depth perception?</li><li class="listitem">What do we require to see in the dark?</li><li class="listitem">How do we obtain good value-for-money when purchasing lenses and cameras?</li></ul></div><div><h3 class="title2"><a id="tip02" class="calibre1"/>Tip</h3><p class="calibre8">For many practical applications of computer vision, the environment is not a well-lit, white room, and the subject is not a human face at a distance of 0.6m (2')!</p></div><p class="calibre8">The choice of hardware is crucial to these problems. Different cameras and lenses are optimized for different imaging scenarios. However, software can also make or break a solution. On the software side, we will focus on the efficient use of OpenCV. Fortunately, OpenCV's <strong class="calibre9">videoio</strong> module supports many classes of camera systems, including the following:</p><div><ul class="itemizedlist"><li class="listitem">Webcams in Windows, Mac, and Linux via the following frameworks, which come standard with most versions of the operating system:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre9">Windows</strong>: Microsoft Media Foundation (MSMF), DirectShow, or Video for Windows (VfW)</li><li class="listitem"><strong class="calibre9">Mac</strong>: QuickTime</li><li class="listitem"><strong class="calibre9">Linux</strong>: Video4Linux (V4L), Video4Linux2 (V4L2), or libv4l</li></ul></div></li><li class="listitem">Built-in cameras in iOS and Android devices</li><li class="listitem">OpenNI-compliant depth cameras via OpenNI or OpenNI2, which are open-source under the Apache license</li><li class="listitem">Other depth cameras via the proprietary Intel Perceptual Computing SDK</li><li class="listitem">Photo cameras via libgphoto2, which is open source under the GPL license. For a<a id="id1" class="calibre1"/> list of libgphoto2's supported cameras, see <a class="calibre1" href="http://gphoto.org/proj/libgphoto2/support.php">http://gphoto.org/proj/libgphoto2/support.php</a>.<div><h3 class="title2"><a id="note02" class="calibre1"/>Note</h3><p class="calibre8">Note that the GPL license is not appropriate for use in closed source software.</p></div></li><li class="listitem">IIDC/DCAM-compliant industrial cameras via libdc1394, which is open-source under the LGPLv2 license</li><li class="listitem">For Linux, unicap can be used as an alternative interface for IIDC/DCAM-compliant cameras, but unicap is GPL-licensed and thus not appropriate for use in closed-source software.</li><li class="listitem">Other industrial cameras via the following proprietary frameworks:<div><ul class="itemizedlist1"><li class="listitem">Allied Vision Technologies (AVT) PvAPI for GigE Vision cameras</li><li class="listitem">Smartek Vision Giganetix SDK for GigE Vision cameras</li><li class="listitem">XIMEA API</li></ul></div></li></ul></div><div><h3 class="title2"><a id="note03" class="calibre1"/>Note</h3><p class="calibre8">The videoio module is new in OpenCV 3. Previously, in OpenCV 2, video capture and recording were part of the highgui module, but in OpenCV 3, the highgui module is only responsible for GUI functionality. For a complete index of OpenCV's modules, see <a id="id2" class="calibre1"/>the official documentation at <a class="calibre1" href="http://docs.opencv.org/3.0.0/">http://docs.opencv.org/3.0.0/</a>.</p></div><p class="calibre8">However, we are not limited to the features of the videoio module; we can use other APIs to configure cameras and capture images. If an API can capture an array of image data, OpenCV can readily use the data, often without any copy operation or conversion. As an example, we will capture and use images from depth cameras via OpenNI2 (without the videoio module) and from industrial cameras via the FlyCapture SDK by Point Grey Research (PGR).</p><div><h3 class="title2"><a id="note04" class="calibre1"/>Note</h3><p class="calibre8">An<a id="id3" class="calibre1"/> industrial camera or <strong class="calibre9">machine vision camera</strong> typically has interchangeable lenses, a high-speed hardware interface (such as FireWire, Gigabit Ethernet, USB 3.0, or Camera Link), and a complete programming interface for all camera settings.</p><p class="calibre8">Most industrial cameras have SDKs for Windows and Linux. PGR's FlyCapture SDK supports single-camera and multi-camera setups on Windows as well as single-camera setups on Linux. Some of PGR's competitors, such as Allied Vision Technologies (AVT), offer better support for multi-camera setups on Linux.</p></div><p class="calibre8">We will learn about the differences among categories of cameras, and we will test the capabilities of several specific lenses, cameras, and configurations. By the end of the chapter, you will be better qualified to design either consumer-grade or industrial-grade vision systems for yourself, your lab, your company, or your clients. I hope to surprise you with the results that are possible at each price point!</p></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch01lvl1sec09" class="calibre1"/>Coloring the light</h1></div></div></div><p class="calibre8">The human<a id="id4" class="calibre1"/> eye is sensitive to certain wavelengths of electromagnetic radiation. We call these wavelengths "visible light", "colors", or sometimes just "light". However, our definition of "visible light" is anthropocentric as different animals see different wavelengths. For example, bees are blind to red light, but can see ultraviolet light (which is invisible to humans). Moreover, machines can assemble human-viewable images based on almost any stimulus, such as light, radiation, sound, or magnetism. To broaden our horizons, let's consider eight kinds of electromagnetic radiation and their common sources. Here is the list, in order of decreasing wavelength:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Radio waves</strong> radiate from certain astronomical objects and from lightning. They <a id="id5" class="calibre1"/>are also generated by wireless electronics (radio, Wi-Fi, Bluetooth, and so on).</li><li class="listitem"><strong class="calibre9">Microwaves</strong> radiated from the Big Bang<a id="id6" class="calibre1"/> and are present throughout the Universe as background radiation. They are also generated by microwave ovens.</li><li class="listitem"><strong class="calibre9">Far infrared (FIR) light</strong> is an invisible <a id="id7" class="calibre1"/>glow from warm or hot things such as warm-blooded animals and hot-water pipes.</li><li class="listitem"><strong class="calibre9">Near infrared (NIR) light</strong> radiates brightly<a id="id8" class="calibre1"/> from our sun, from flames, and from metal that is red-hot or nearly red-hot. However, it is a relatively weak component in commonplace electric lighting. Leaves and other vegetation brightly reflect NIR light. Skin and certain fabrics are slightly transparent to NIR.</li><li class="listitem"><strong class="calibre9">Visible light</strong> radiates brightly from our <a id="id9" class="calibre1"/>sun and from commonplace electric light sources. Visible light includes the colors red, orange, yellow, green, blue, and violet (in order of decreasing wavelength).</li><li class="listitem"><strong class="calibre9">Ultraviolet (UV) light</strong>, too, is abundant in sunlight. On a sunny day, UV light <a id="id10" class="calibre1"/>can burn our skin and can become slightly visible to us as a blue-gray haze in the distance. Commonplace, silicate glass is nearly opaque to UV light, so we do not suffer sunburn when we are behind windows (indoors or in a car). For the same reason, UV camera systems rely on lenses made of non-silicate materials such as quartz. Many flowers have UV markings that are visible to insects. Certain bodily fluids such as blood and urine are more opaque to UV than to visible light.</li><li class="listitem"><strong class="calibre9">X-rays</strong> radiate<a id="id11" class="calibre1"/> from certain astronomical objects such as black holes. On Earth, radon gas, and certain other radioactive elements are natural X-ray sources.</li><li class="listitem"><strong class="calibre9">Gamma rays</strong> radiate from nuclear explosions, including supernovae. To lesser <a id="id12" class="calibre1"/>extents the sources of gamma rays also include radioactive decay and lightning strikes.</li></ul></div><p class="calibre8">NASA provides the following visualization of the wavelength and temperature associated with each<a id="id13" class="calibre1"/> kind of light or radiation:</p><div><img src="img/00002.jpeg" alt="Coloring the light" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8"><strong class="calibre9">Passive</strong> imaging systems rely on <strong class="calibre9">ambient</strong> (commonplace) light or radiation sources as described<a id="id14" class="calibre1"/> in the preceding list. <strong class="calibre9">Active</strong> imaging systems include <a id="id15" class="calibre1"/>sources of their own so that the light or radiation is <strong class="calibre9">structured</strong> in more predictable ways. For example, an active night vision scope might use a NIR camera plus a NIR light.</p><p class="calibre8">For astronomy, passive imaging is feasible across the entire electromagnetic spectrum; the vast expanse of the Universe is flooded with all kinds of light and radiation from sources old and new. However, for terrestrial (Earth-bound) purposes, passive imaging is mostly limited to the FIR, NIR, visible, and UV ranges. Active imaging is feasible across the entire spectrum, but the practicalities of power consumption, safety, and interference (between our use case and others) limit the extent to which we can flood an environment with excess light and radiation.</p><p class="calibre8">Whether active or passive, an imaging system typically uses a lens to bring light or radiation into focus on the surface of the camera's sensor. The lens and its coatings transmit some wavelengths while blocking others. Additional filters may be placed in front of the lens or sensor to block more wavelengths. Finally, the sensor itself exhibits a varying<a id="id16" class="calibre1"/> <strong class="calibre9">spectral response</strong>, meaning that for some wavelengths, the sensor registers a strong (bright) signal, but for other wavelengths, it registers a weak (dim) signal or no signal. Typically, a mass-produced digital sensor responds most strongly to green, followed by red, blue, and NIR. Depending on the use case, such a sensor might be deployed with a filter to block a range of light (whether NIR or visible) and/or a filter to superimpose a pattern of varying colors. The latter filter allows for the capture of multichannel images, such as RGB images, whereas the unfiltered <a id="id17" class="calibre1"/>sensor would capture <strong class="calibre9">monochrome</strong> (gray) images.</p><p class="calibre8">The sensor's surface<a id="id18" class="calibre1"/> consists of many sensitive points or <strong class="calibre9">photosites</strong>. These <a id="id19" class="calibre1"/>are analogous to pixels in the captured digital image. However, photosites and pixels do not necessarily correspond one-to-one. Depending on the camera system's design and configuration, the signals from several photosites might be blended together to create a neighborhood of multichannel pixels, a brighter pixel, or a less noisy pixel.</p><p class="calibre8">Consider the following pair of images. They show a sensor with a Bayer filter, which is a common type of color filter with two green photosites per red or blue photosite. To compute a single RGB pixel, multiple photosite values are blended. The left-hand image is a photo of the filtered sensor under a microscope, while the right-hand image is a cut-away diagram showing the filter and underlying photosites:</p><div><img src="img/00003.jpeg" alt="Coloring the light" class="calibre11"/></div><p class="calibre12"> </p><div><h3 class="title2"><a id="note05" class="calibre1"/>Note</h3><p class="calibre8">The preceding images come from Wikimedia. They are contributed by the users Natural Philo, under the Creative Commons Attribution-Share Alike 3.0 Unported license (left), and Cburnett, under the GNU Free Documentation License (right).</p></div><p class="calibre8">As we see in this example, a simplistic model (an RGB pixel) might hide important details about the way data are captured and stored. To build efficient image pipelines, we need to think about not just pixels, but also channels and <strong class="calibre9">macropixels</strong>—neighborhoods of pixels that <a id="id20" class="calibre1"/>share <a id="id21" class="calibre1"/>some channels of data and are captured, stored, and processed in one block. Let's consider three categories of image formats:</p><div><ul class="itemizedlist"><li class="listitem">A <strong class="calibre9">raw image</strong> is<a id="id22" class="calibre1"/> a literal representation of the photosites' signals, scaled to some range such as 8, 12, or 16 bits. For photosites in a given row of the sensor, the data are contiguous but for photosites in a given column, they are not.</li><li class="listitem">A <strong class="calibre9">packed image</strong> stores each pixel or macropixel contiguously in memory. That is<a id="id23" class="calibre1"/> to say, data are ordered according to their neighborhood. This is an efficient format if most of our processing pertains to multiple color components at a time. For a typical color camera, a raw image is <em class="calibre10">not</em> packed because each neighborhood's data are split across multiple rows. Packed color images usually use RGB channels, but<a id="id24" class="calibre1"/> alternatively, they may use <strong class="calibre9">YUV channels</strong>, where Y is brightness (grayscale), U is blueness (versus greenness), and V is redness (also versus greenness).</li><li class="listitem">A <strong class="calibre9">planar image</strong> stores each channel contiguously in memory. That is to say, data are<a id="id25" class="calibre1"/> ordered according to the color component they represent. This is an efficient format if most of our processing pertains to a single color component at a time. Packed color images usually use YUV channels. Having a Y channel in a planar format is efficient for computer vision because many algorithms are designed to work on grayscale data alone.</li></ul></div><p class="calibre8">An image from a monochrome camera can be efficiently stored and processed in its raw format or (if it must integrate seamlessly into a color imaging pipeline) as the Y plane in a planar YUV format. Later in this chapter, in the sections <em class="calibre10">Supercharging the PlayStation Eye</em> and <em class="calibre10">Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras</em>, we will discuss code samples that demonstrate efficient handling of various image formats.</p><p class="calibre8">Until now, we have covered a brief taxonomy of light, radiation, and color—their sources, their interaction with optics and sensors, and their representation as channels and neighborhoods. Now, let's explore some more dimensions of image capture: time and space.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec10" class="calibre1"/>Capturing the subject in the moment</h1></div></div></div><p class="calibre8">Robert Capa, a <a id="id26" class="calibre1"/>photojournalist who covered five wars and shot images of the first wave of D-Day landings at Omaha Beach, gave this advice:</p><div><blockquote class="blockquote2"><p class="calibre19"><em class="calibre10">"If your pictures aren't good enough, you're not close enough."</em></p></blockquote></div><p class="calibre8">Like a computer vision program, a photographer is the intelligence behind the lens. (Some would say the photographer is the soul behind the lens.) A good photographer continuously performs detection and tracking tasks—scanning the environment, choosing the subject, predicting actions and expressions that will create the right moment for the photo, and choosing the lens, settings, and viewpoint that will most effectively frame the subject.</p><p class="calibre8">By getting "close enough" to the subject and the action, the photographer can observe details quickly with the naked eye and can move to other viewpoints quickly because the distances are short and because the equipment is typically light (compared to a long lens on a tripod for a distant shot). Moreover, a close-up, wide-angle shot pulls the photographer, and viewer, into a first-person perspective of events, as if we become the subject or the subject's comrade for a single moment.</p><p class="calibre8">Photographic aesthetics concern us further in <a class="calibre1" title="Chapter 2. Photographing Nature and Wildlife with an Automated Camera" href="part0023_split_000.html#LTSU2-940925703e144daa867f510896bffb69">Chapter 2</a>, <em class="calibre10">Photographing Nature and Wildlife with an Automated Camera</em>. For now, let's just establish two cardinal rules: don't miss the subject and don't miss the moment! Poor visibility and unfortunate timing are the worst excuses a photographer or a practitioner of computer vision can give. To hold ourselves to account, let us define some measurements that are relevant to these cardinal rules.</p><p class="calibre8"><strong class="calibre9">Resolution</strong> is<a id="id27" class="calibre1"/> the finest level of detail that the lens and camera can see. For many computer vision applications, recognizable details are the subject of the work, and if the system's resolution is poor, we might miss this subject completely. Resolution is often expressed in terms of the sensor's photosite counts or the captured image's pixel counts, but at best these measurements only tell us about one limiting factor. A better, empirical measurement, which reflects all characteristics of the lens, sensor, and<a id="id28" class="calibre1"/> setup, is called <strong class="calibre9">line pairs per millimeter</strong> (lp/mm). This means the maximum density of black-on-white lines that the lens and camera can resolve, in a given setup. At any higher density than this, the lines in the captured image blur together. Note that lp/mm varies with the subject's distance and the lens's settings, including the focal length (optical zoom) of a zoom lens. When you approach the subject, zoom in, or swap out a short lens for a long lens, the system should of course capture more detail! However, lp/mm does not vary when you crop (digitally zoom) a captured image.</p><p class="calibre8">Lighting <a id="id29" class="calibre1"/>conditions and the camera's <strong class="calibre9">ISO speed</strong> setting also have an effect on lp/mm. High ISO speeds are used in low light and they boost both the signal (which is weak in low light) and the noise (which is as strong as ever). Thus, at high ISO speeds, some details are blotted out by the boosted noise.</p><p class="calibre8">To achieve anything near its potential resolution, the lens must be properly focused. Dante Stella, a contemporary photographer, describes a problem with modern camera technology:</p><div><blockquote class="blockquote2"><p class="calibre19"><em class="calibre10">"For starters, it lacks … thought-controlled predictive autofocus."</em></p></blockquote></div><p class="calibre8">That is to say, autofocus can fail miserably when its algorithm is mismatched to a particular, intelligent use or a particular pattern of evolving conditions in the scene. Long lenses are especially unforgiving with respect to improper focus. The <strong class="calibre9">depth of field</strong> (the distance<a id="id30" class="calibre1"/> between the nearest and farthest points in focus) is <a id="id31" class="calibre1"/>shallower in longer lenses. For some computer vision setups—for example, a camera hanging over an assembly line—the distance to the subject is highly predictable and in such cases manual focus is an acceptable solution.</p><p class="calibre8"><strong class="calibre9">Field of view (FOV)</strong> is <a id="id32" class="calibre1"/>the extent of the lens's vision. Typically, FOV is measured as an angle, but it can be measured as the distance between two peripherally observable points at a given depth from the lens. For example, a FOV of 90 degrees may also be expressed as a FOV of 2m at a depth of 1m or a FOV of 4m at a depth of 2m. Where not otherwise specified, FOV usually means diagonal FOV (the diagonal of the lens's vision), as opposed to horizontal FOV or vertical FOV. A longer lens has a narrower FOV. Typically, a longer lens also has higher resolution and less distortion. If our subject falls outside the FOV, we miss the subject completely! Toward the edges of the FOV, resolution tends to decrease and distortion tends to increase, so preferably the FOV should be wide enough to leave a margin around the subject.</p><p class="calibre8">The camera's <strong class="calibre9">throughput</strong> is<a id="id33" class="calibre1"/> the rate at which it captures image data. For many computer vision applications, a visual event might start and end in a fleeting moment and if the throughput is low, we might miss the moment completely or our image of it might suffer from motion blur. Typically, throughput is measured in frames per second (FPS), though measuring it as a bitrate can be useful, too. Throughput is limited by <a id="id34" class="calibre1"/>the following factors:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Shutter speed</strong> (exposure time): For a well-exposed image, the shutter speed is limited <a id="id35" class="calibre1"/>by lighting conditions, the lens's <strong class="calibre9">aperture setting</strong>, and<a id="id36" class="calibre1"/> the camera's ISO speed setting. (Conversely, a slower shutter speed allows for a narrower aperture setting or slower ISO speed.) We will discuss aperture settings after this list.</li><li class="listitem"><strong class="calibre9">The type of shutter</strong>: A <strong class="calibre9">global shutter</strong> synchronizes the capture across all photosites. A <strong class="calibre9">rolling shutter</strong> does not; rather, the capture is sequential such that <a id="id37" class="calibre1"/>photosites at the bottom of the sensor register their <a id="id38" class="calibre1"/>signals later than photosites at the top. A rolling shutter is inferior because it can make an object appear skewed when the object or the camera moves rapidly. (This is sometimes called the "Jell-O effect" because of the video's resemblance to a wobbling mound of gelatin.) Also, under rapidly flickering lighting, a rolling shutter creates light and dark bands in the image. If the start of the capture is synchronized but the end<a id="id39" class="calibre1"/> is not, the shutter is referred to as a <strong class="calibre9">rolling shutter with global reset</strong>.</li><li class="listitem"><strong class="calibre9">The camera's onboard image processing routines</strong>, such as conversion of raw photosite signals to a given number of pixels in a given format. As the number of pixels and bytes per pixel increase, the throughput decreases.</li><li class="listitem"><strong class="calibre9">The interface between the camera and host computer</strong>: Common camera interfaces, in order of decreasing bit rates, include CoaXPress full, Camera Link full, USB 3.0, CoaXPress base, Camera Link base, Gigabit Ethernet, IEEE 1394b (FireWire full), USB 2.0, and IEEE 1394 (FireWire base).</li></ul></div><p class="calibre8">A wide <a id="id40" class="calibre1"/>aperture setting lets in more light to allow for a faster exposure, a lower ISO speed, or a brighter image. However, a narrower aperture has the advantage of offering a greater depth of field. A lens supports a limited range of aperture settings. Depending on the lens, some aperture settings exhibit higher resolution than others. Long lenses tend to exhibit more stable resolution across aperture settings.</p><p class="calibre8">A lens's aperture size is expressed as an <strong class="calibre9">f-number</strong> or <strong class="calibre9">f-stop</strong>, which is the ratio of the lens's focal <a id="id41" class="calibre1"/>length to the diameter of its aperture. Roughly speaking, <strong class="calibre9">focal length</strong> is related to the length of the lens. More precisely, it is the distance between <a id="id42" class="calibre1"/>the camera's sensor and the lens system's optical <a id="id43" class="calibre1"/>center <a id="id44" class="calibre1"/>when the lens is focused at an infinitely distant target. The focal length should not be confused with the <strong class="calibre9">focus distance</strong>—the distance to objects that are in focus. The following diagram illustrates the meanings of focal length and focal distance as well as FOV:</p><div><img src="img/00004.jpeg" alt="Capturing the subject in the moment" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">With a<a id="id45" class="calibre1"/> higher f-number (a proportionally narrower aperture), a lens transmits a smaller proportion of incoming light. Specifically, the intensity of the transmitted light is inversely proportional to the square of the f-number. For example, when comparing the maximum apertures of two lenses, a photographer might write, "The f/2 lens is twice as fast as the f/2.8 lens." This means that the former lens can transmit twice the intensity of light, allowing an equivalent exposure to be taken in half the time.</p><p class="calibre8">A lens's <strong class="calibre9">efficiency</strong> or <strong class="calibre9">transmittance</strong> (the proportion of light transmitted) depends on not only the f-number but also non-ideal factors. For example, some light is reflected off the lens <a id="id46" class="calibre1"/>elements instead of being transmitted. The <strong class="calibre9">T-number</strong> or <strong class="calibre9">T-stop</strong> is <a id="id47" class="calibre1"/>an<a id="id48" class="calibre1"/> adjustment to the f-number based on empirical findings about a given lens's transmittance. For example, regardless of its f-number, a T/2.4 lens has the same transmittance as an ideal f/2.4 lens. For cinema lenses, manufacturers often provide T-number specifications but for other lenses, it is much more common to see only f-number specifications.</p><p class="calibre8">The sensor's <strong class="calibre9">efficiency</strong> is the proportion of the lens-transmitted light that reaches photosites and gets<a id="id49" class="calibre1"/> converted to a signal. If the efficiency is poor, the sensor misses much of the light! A more efficient sensor will tend to take well-exposed images for a broader range of camera settings, lens settings, and lighting conditions. Thus, efficiency gives the system more freedom to auto-select settings that are optimal for resolution and throughput. For the common type of sensor described in the previous section, <em class="calibre10">Coloring the light</em>, the choice of color filters has a big effect on efficiency. A camera designed to capture visible light in grayscale has high efficiency because it can receive all visible wavelengths at each photosite. A camera designed to capture visible light in multiple color channels typically has much lower efficiency because some wavelengths are filtered out at each photosite. A camera designed to capture NIR alone, by filtering out all visible light, typically has even lower efficiency.</p><p class="calibre8">Efficiency<a id="id50" class="calibre1"/> is a good indication of the system's ability to form <em class="calibre10">some kind of image</em> under diverse lighting (or radiation) conditions. However, depending on the subject and the real lighting, a relatively inefficient system could have higher contrast and better resolution. The advantages of selectively filtering wavelengths are not necessarily reflected in lp/mm, which measures black-on-white resolution.</p><p class="calibre8">By now, we have seen many quantifiable tradeoffs that complicate our efforts to capture a subject in a moment. As Robert Capa's advice implies, getting close with a short lens is a relatively robust recipe. It allows for good resolution with minimal risk of completely missing the framing or the focus. On the other hand, such a setup suffers from high distortion and, by definition, a short working distance. Moving beyond the capabilities of cameras in Capa's day, we have also considered the features and configurations that allow for high-throughput and high-efficiency video capture.</p><p class="calibre8">Having primed ourselves on wavelengths, image formats, cameras, lenses, capture settings, and photographers' common sense, let us now select several systems to study.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec11" class="calibre1"/>Rounding up the unusual suspects</h1></div></div></div><p class="calibre8">This chapter's demo <a id="id51" class="calibre1"/>applications are tested with three cameras, which are described in the following table. The demos are also compatible with many additional cameras; we will discuss compatibility later as part of each demo's detailed description. The three chosen cameras differ greatly in terms of price and features but each one can do things that an ordinary webcam cannot!</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Name</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Price</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Purposes</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Modes</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Optics</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Sony PlayStation Eye</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$10</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Passive, color imaging in visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 60 FPS</p>
<p class="calibre15">320x240 @ 187 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">FOV: 75 degrees or 56 degrees (two zoom settings)</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">ASUS Xtion PRO Live</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$230</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Passive, color imaging in visible light</p>
<p class="calibre15">Active, monochrome imaging in NIR light</p>
<p class="calibre15">Depth estimation</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Color or NIR: 1280x1024 @ 60 FPS</p>
<p class="calibre15">Depth: 640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">FOV: 70 degrees</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">PGR Grasshopper 3 GS3-U3-23S6M-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1000</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Passive, monochrome imaging in visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1920x1200 @ 162 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C-mount lens (not included)</p>
</td></tr></tbody></table></div><div><h3 class="title2"><a id="note06" class="calibre1"/>Note</h3><p class="calibre8">For examples of lenses that we can use with the GS3-U3-23S6M-C camera, refer to the <em class="calibre10">Shopping for glass</em> section, later in this chapter.</p></div><p class="calibre8">We will <a id="id52" class="calibre1"/>try to push these cameras to the limits of their capabilities. Using multiple libraries, we will write applications to access unusual capture modes and to process frames so rapidly that the input remains the bottleneck. To borrow a term from the automobile designers who made 1950s muscle cars, we might say that we want to "supercharge" our systems; we want to supply them with specialized or excess input to see what they can do!</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec12" class="calibre1"/>Supercharging the PlayStation Eye</h1></div></div></div><p class="calibre8">Sony<a id="id53" class="calibre1"/> developed the Eye camera in 2007 as an input device for PlayStation 3 games. Originally, no other system supported the Eye. Since then, third parties have created drivers and SDKs for Linux, Windows, and Mac. The following list describes the current state of some of these third-party projects:</p><div><ul class="itemizedlist"><li class="listitem">For Linux, the gspca_ov534 driver supports the PlayStation Eye and works out of the box with OpenCV's videoio module. This driver comes standard with most recent Linux distributions. Current releases of the driver support modes as fast as 320x240 @ 125 FPS and 640x480 @ 60 FPS. An upcoming release will add support for 320x240 @187 FPS. If you want to upgrade to this future version today, you will need to familiarize yourself with the basics of Linux kernel development, and build the driver yourself.<div><h3 class="title2"><a id="note07" class="calibre1"/>Note</h3><p class="calibre8">See the<a id="id54" class="calibre1"/> driver's latest source code at <a class="calibre1" href="https://github.com/torvalds/linux/blob/master/drivers/media/usb/gspca/ov534.c">https://github.com/torvalds/linux/blob/master/drivers/media/usb/gspca/ov534.c</a>. Briefly, you would need to obtain the source code of your Linux distribution's kernel, merge the new <code class="email">ov534.c</code> file, build the driver as part of the kernel, and finally, load the newly built gspca_ov534 driver.</p></div></li><li class="listitem">For Mac and Windows, developers can add PlayStation Eye support to their applications <a id="id55" class="calibre1"/>using an SDK called PS3EYEDriver, available from <a class="calibre1" href="https://github.com/inspirit/PS3EYEDriver">https://github.com/inspirit/PS3EYEDriver</a>. Despite the name, this project is not a driver; it supports the camera at the application level, but not the OS level. The supported modes include 320x240 @ 187 FPS and 640x480 @ 60 FPS. The project comes with sample application code. Much of the<a id="id56" class="calibre1"/> code in PS3EYEDriver is derived from the GPL-licensed gspca_ov534 driver, and thus, the use of PS3EYEDriver is probably only appropriate to projects that are also GPL-licensed.</li><li class="listitem">For<a id="id57" class="calibre1"/> Windows, a commercial driver and SDK are available from Code Laboratories (CL) at <a class="calibre1" href="https://codelaboratories.com/products/eye/driver/">https://codelaboratories.com/products/eye/driver/</a>. At the time of writing, the CL-Eye Driver costs $3. However, the driver does not work with OpenCV 3's videoio module. The CL-Eye Platform SDK, which depends on the driver, costs an additional $5. The fastest supported modes are 320x240 @ 187 FPS and 640x480 @ 75 FPS.</li><li class="listitem">For recent<a id="id58" class="calibre1"/> versions of Mac, no driver is available. A driver called macam is available at <a class="calibre1" href="http://webcam-osx.sourceforge.net/">http://webcam-osx.sourceforge.net/</a>, but it was last updated in 2009 and does not work on Mac OS X Mountain Lion and newer versions.</li></ul></div><p class="calibre8">Thus, OpenCV in Linux can capture data directly from an Eye camera, but OpenCV in Windows or Mac requires another SDK as an intermediary.</p><p class="calibre8">First, for Linux, let us consider a minimal example of a C++ application that uses OpenCV to record a slow-motion video based on high-speed input from an Eye. Also, the program should log its frame rate. Let's call this application Unblinking Eye.</p><div><h3 class="title2"><a id="note08" class="calibre1"/>Note</h3><p class="calibre8">Unblinking<a id="id59" class="calibre1"/> Eye's source code and build files are in this book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_1/UnblinkingEye">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_1/UnblinkingEye</a>.</p><p class="calibre8">Note that this sample code should also work with other OpenCV-compatible cameras, albeit at a slower frame rate compared to the Eye.</p></div><p class="calibre8">Unblinking Eye can be implemented in a single file, <code class="email">UnblinkingEye.cpp</code>, containing these few lines of code:</p><div><pre class="programlisting">#include &lt;stdio.h&gt;
#include &lt;time.h&gt;

#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/videoio.hpp&gt;

int main(int argc, char *argv[]) {

  const int cameraIndex = 0;
  const bool isColor = true;
  const int w = 320;
  const int h = 240;
  const double captureFPS = 187.0;
  const double writerFPS = 60.0;
  // With MJPG encoding, OpenCV requires the AVI extension.
  const char filename[] = "SlowMo.avi";
  const int fourcc = cv::VideoWriter::fourcc('M','J','P','G');
  const unsigned int numFrames = 3750;

  cv::Mat mat;

  // Initialize and configure the video capture.
  cv::VideoCapture capture(cameraIndex);
  if (!isColor) {
    capture.set(cv::CAP_PROP_MODE, cv::CAP_MODE_GRAY);
  }
  capture.set(cv::CAP_PROP_FRAME_WIDTH, w);
  capture.set(cv::CAP_PROP_FRAME_HEIGHT, h);
  capture.set(cv::CAP_PROP_FPS, captureFPS);

  // Initialize the video writer.
  cv::VideoWriter writer(
      filename, fourcc, writerFPS, cv::Size(w, h), isColor);

  // Get the start time.
  clock_t startTicks = clock();

  // Capture frames and write them to the video file.
  for (unsigned int i = 0; i &lt; numFrames;) {
    if (capture.read(mat)) {
      writer.write(mat);
      i++;
    }
  }

  // Get the end time.
  clock_t endTicks = clock();

  // Calculate and print the actual frame rate.
  double actualFPS = numFrames * CLOCKS_PER_SEC /
      (double)(endTicks - startTicks);
  printf("FPS: %.1f\n", actualFPS);
}</pre></div><p class="calibre8">Note that the camera's specified mode is 320x240 @ 187 FPS. If our version of the gspca_ov534 driver does not support this mode, we can expect it to fall back to 320x240 @ 125 FPS. Meanwhile, the video file's specified mode is 320x240 @ 60 FPS, meaning that the <a id="id60" class="calibre1"/>video will play back at slower-than-real speed as a special effect. Unblinking Eye can be built using a Terminal command such as the following:</p><div><pre class="programlisting">
<strong class="calibre9">$ g++ UnblinkingEye.cpp -o UnblinkingEye -lopencv_core -lopencv_videoio</strong>
</pre></div><p class="calibre8">Build Unblinking Eye, run it, record a moving subject, observe the frame rate, and play back the recorded video, <code class="email">SlowMo.avi</code>. How does your subject look in slow motion?</p><p class="calibre8">On a machine with a slow CPU or slow storage, Unblinking Eye might drop some of the captured frames due to a bottleneck in video encoding or file output. Do not be fooled by the low resolution! The rate of data transfer for a camera in 320x240 @ 187 FPS mode is greater than for a camera in 1280x720 @ 15 FPS mode (an HD resolution at a slightly choppy frame rate). Multiply the pixels by the frame rate to see how many pixels per second are transferred in each mode.</p><p class="calibre8">Suppose we want to reduce the amount of data per frame by capturing and recording monochrome video. Such an option is available when OpenCV 3 is built for Linux with libv4l support. (The relevant CMake definition is <code class="email">WITH_LIBV4L</code>, which is turned on by default.) By changing the following line in the code of Unblinking Eye and then rebuilding it, we can switch to grayscale capture:</p><div><pre class="programlisting">const bool isColor = false;</pre></div><p class="calibre8">Note that the change to this Boolean affects the highlighted portions of the following code:</p><div><pre class="programlisting">  cv::VideoCapture capture(cameraIndex);
  <strong class="calibre9">if (!isColor) {</strong>
<strong class="calibre9">    capture.set(cv::CAP_PROP_MODE, cv::CAP_MODE_GRAY);</strong>
<strong class="calibre9">  }</strong>
  capture.set(cv::CAP_PROP_FRAME_WIDTH, w);
  capture.set(cv::CAP_PROP_FRAME_HEIGHT, h);
  capture.set(cv::CAP_PROP_FPS, captureFPS);

  cv::VideoWriter writer(
      filename, fourcc, writerFPS, cv::Size(w, h), <strong class="calibre9">isColor</strong>);</pre></div><p class="calibre8">Behind the scenes, the <code class="email">VideoCapture</code> and <code class="email">VideoWriter</code> objects are now using a planar YUV format. The captured Y data are copied to a single-channel OpenCV <code class="email">Mat</code> and are ultimately stored in the video file's Y channel. Meanwhile, the video file's U and V color channels are just filled with the mid-range value, 128, for gray. U and V use a lower resolution than Y, so at the time of capture, the YUV format has only 12 bits per pixel (bpp), compared to 24 bpp for OpenCV's default BGR format.</p><div><h3 class="title2"><a id="note09" class="calibre1"/>Note</h3><p class="calibre8">The libv4l interface in OpenCV's videoio module currently supports the following values for <code class="email">cv::CAP_PROP_MODE</code>:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">cv::CAP_MODE_BGR</code> (the default) captures 24 bpp color in BGR format (8 bpp per channel).</li><li class="listitem"><code class="email">cv::CAP_MODE_RGB</code> captures 24 bpp color in RGB format (8 bpp per channel).</li><li class="listitem"><code class="email">cv::CAP_MODE_GRAY</code> extracts 8 bpp grayscale from a 12 bpp planar YUV format.</li><li class="listitem"><code class="email">cv::CAP_MODE_YUYV</code> captures 16 bpp color in a packed YUV format (8 bpp for Y and 4 bpp each for U and V).</li></ul></div></div><p class="calibre8">For <a id="id61" class="calibre1"/>Windows or Mac, we should instead capture data using PS3EYEDriver, CL-Eye Platform SDK, or another library, and then create an OpenCV <code class="email">Mat</code> that references the data. This approach is illustrated in the following partial code sample:</p><div><pre class="programlisting">int width = 320, height = 240;
int matType = CV_8UC3; // 8 bpp per channel, 3 channels
void *pData;

// Use the camera SDK to capture image data.
someCaptureFunction(&amp;pData);

// Create the matrix. No data are copied; the pointer is copied.
cv::Mat mat(height, width, matType, pData);</pre></div><p class="calibre8">Indeed, the same approach applies to integrating almost any source of data into OpenCV. Conversely, to use OpenCV as a source of data for another library, we can get a pointer to the data stored in a matrix:</p><div><pre class="programlisting">void *pData = mat.data;</pre></div><p class="calibre8">Later in this chapter, in <em class="calibre10">Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras</em>, we cover a nuanced example of integrating OpenCV with other libraries, specifically FlyCapture2 for capture and SDL2 for display. PS3EYEDriver comes with a comparable sample, in which the pointer to captured data is passed to SDL2 for display. As an exercise, you might want to adapt these two examples to build a demo that integrates OpenCV with PS3EYEDriver for capture and SDL2 for display.</p><p class="calibre8">Hopefully, after some experimentation, you will conclude that the PlayStation Eye is a more capable camera than its $10 price tag suggests. For fast-moving subjects, its high frame rate is a good tradeoff for its low resolution. Banish motion blur!</p><p class="calibre8">If we are<a id="id62" class="calibre1"/> willing to invest in hardware modifications, the Eye has even more tricks hidden up its sleeve (or in its socket). The lens and IR blocking filter are relatively easy to replace. An aftermarket lens and filter can allow for NIR capture. Furthermore, an aftermarket lens can yield higher resolution, a different FOV, less distortion, and greater efficiency. Peau Productions sells premodified Eye cameras as<a id="id63" class="calibre1"/> well as do-it-yourself (DIY) kits, at <a class="calibre1" href="http://peauproductions.com/store/index.php?cPath=136_1">http://peauproductions.com/store/index.php?cPath=136_1</a>. The company's modifications support interchangeable lenses with an m12 mount or CS mount (two different standards of screw mounts). The website offers detailed recommendations based on lens characteristics such as distortion and IR transmission. Peau's price for a premodified NIR Eye camera plus a lens starts from approximately $85. More expensive options, including distortion-corrected lenses, range up to $585. However, at these prices, it is advisable to compare lens prices across multiple vendors, as described later in this chapter's <em class="calibre10">Shopping for glass</em> section.</p><p class="calibre8">Next, we will examine a camera that lacks high-speed modes, but is designed to separately capture visible and NIR light, with active NIR illumination.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec13" class="calibre1"/>Supercharging the ASUS Xtion PRO Live and other OpenNI-compliant depth cameras</h1></div></div></div><p class="calibre8">ASUS <a id="id64" class="calibre1"/>introduced <a id="id65" class="calibre1"/>the Xtion PRO Live in 2012 as an input device for motion-controlled games, natural user interfaces (NUIs), and computer vision research. It is one of six similar cameras based on sensors designed by PrimeSense, an Israeli company that Apple acquired and shut down in 2013. For a brief comparison between the Xtion PRO Live and the other devices that use PrimeSense sensors, see the following table:</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Name</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Price and Availability</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Highest Res NIR Mode</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Highest Res Color Mode</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Highest Res Depth Mode</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Depth Range</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Microsoft Kinect for Xbox 360</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$135</p>
<p class="calibre15">Available</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS?</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">0.8m to 3.5m?</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">ASUS Xtion PRO</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$200</p>
<p class="calibre15">Discontinued</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x1024 @ 60 FPS?</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">None</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">0.8m to 3.5m</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">ASUS Xtion PRO Live</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$230</p>
<p class="calibre15">Available</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x1024 @ 60 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x1024 @ 60 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">0.8m to 3.5m</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">PrimeSense Carmine 1.08</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$300</p>
<p class="calibre15">Discontinued</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x960 @ 60 FPS?</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x960 @ 60 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">0.8m to 3.5m</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">PrimeSense Carmine 1.09</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$325</p>
<p class="calibre15">Discontinued</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x960 @ 60 FPS?</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1280x960 @ 60 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">0.35m to 1.4m</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Structure Sensor</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$380</p>
<p class="calibre15">Available</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS?</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">None</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">640x480 @ 30 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">0.4m to 3.5m</p>
</td></tr></tbody></table></div><p class="calibre8">All of <a id="id66" class="calibre1"/>these devices include a NIR camera and a <a id="id67" class="calibre1"/>source of NIR illumination. The light source projects a pattern of NIR dots, which might be detectable at a distance of 0.8m to 3.5m, depending on the model. Most of the devices also include an RGB color camera. Based on the appearance of the active NIR image (of the dots) and the passive RGB image, the device can estimate distances and produce a so-called <strong class="calibre9">depth map</strong>, containing<a id="id68" class="calibre1"/> distance estimates for 640x480 points. Thus, the device has up to three modes: NIR (a camera image), color (a camera image), and depth (a processed image).</p><div><h3 class="title2"><a id="note10" class="calibre1"/>Note</h3><p class="calibre8">For <a id="id69" class="calibre1"/>more information on the types of active illumination or structured light that are useful in depth imaging, see the following paper:</p><p class="calibre8">David Fofi, Tadeusz Sliwa, Yvon Voisin, "A comparative survey on invisible structured light", <em class="calibre10">SPIE Electronic Imaging - Machine Vision Applications in Industrial Inspection XII</em>, San José, USA, pp. 90-97, January, 2004.</p><p class="calibre8">The paper is available online at <a class="calibre1" href="http://www.le2i.cnrs.fr/IMG/publications/fofi04a.pdf">http://www.le2i.cnrs.fr/IMG/publications/fofi04a.pdf</a>.</p></div><p class="calibre8">The Xtion, Carmine, and Structure Sensor devices as well as certain versions of the Kinect are compatible with open source SDKs called OpenNI and OpenNI2. Both OpenNI and OpenNI2 are available under the Apache license. On Windows, OpenNI2 comes with support for many cameras. However, on Linux and Mac, support for the Xtion, Carmine, and Structure Sensor devices is provided through an extra module called PrimeSense Sensor, which is also open source under the Apache license. The Sensor module and OpenNI2 have separate installation procedures and the Sensor module must be installed first. Obtain the Sensor module from one of the following URLs, depending on your operating system:</p><div><ul class="itemizedlist"><li class="listitem">Linux x64: <a class="calibre1" href="http://nummist.com/opencv/Sensor-Bin-Linux-x64-v5.1.6.6.tar.bz2">http://nummist.com/opencv/Sensor-Bin-Linux-x64-v5.1.6.6.tar.bz2</a></li><li class="listitem">Linux x86: <a class="calibre1" href="http://nummist.com/opencv/Sensor-Bin-Linux-x86-v5.1.6.6.tar.bz2">http://nummist.com/opencv/Sensor-Bin-Linux-x86-v5.1.6.6.tar.bz2</a></li><li class="listitem">Linux ARM: <a class="calibre1" href="http://nummist.com/opencv/Sensor-Bin-Linux-Arm-v5.1.6.6.tar.bz2">http://nummist.com/opencv/Sensor-Bin-Linux-Arm-v5.1.6.6.tar.bz2</a></li><li class="listitem">Mac: <a class="calibre1" href="http://nummist.com/opencv/Sensor-Bin-MacOSX-v5.1.6.6.tar.bz2">http://nummist.com/opencv/Sensor-Bin-MacOSX-v5.1.6.6.tar.bz2</a></li></ul></div><p class="calibre8">After downloading this archive, decompress it and run <code class="email">install.sh</code> (inside the decompressed folder).</p><div><h3 class="title2"><a id="note11" class="calibre1"/>Note</h3><p class="calibre8">For Kinect compatibility, instead try the SensorKinect fork of the Sensor module. Downloads <a id="id70" class="calibre1"/>for SensorKinect are available at <a class="calibre1" href="https://github.com/avin2/SensorKinect/downloads">https://github.com/avin2/SensorKinect/downloads</a>. SensorKinect only supports Kinect for Xbox 360 and it does not support model 1473. (The model number is printed on the bottom of the device.) Moreover, SensorKinect is only compatible with an old development build of OpenNI (and not OpenNI2). For <a id="id71" class="calibre1"/>download links to the old build of OpenNI, refer to <a class="calibre1" href="http://nummist.com/opencv/">http://nummist.com/opencv/</a>.</p></div><p class="calibre8">Now, on any operating system, we need to build the latest development version of OpenNI2 from source. (Older, stable versions do <a id="id72" class="calibre1"/>not work with the Xtion PRO Live, at least on some systems.) The source code can be downloaded as a ZIP archive from <a class="calibre1" href="https://github.com/occipital/OpenNI2/archive/develop.zip">https://github.com/occipital/OpenNI2/archive/develop.zip</a> or it can be cloned as a Git repository using the following command:</p><div><pre class="programlisting">
<strong class="calibre9">$ git clone –b develop https://github.com/occipital/OpenNI2.git</strong>
</pre></div><p class="calibre8">Let's refer to the unzipped directory or local repository directory as <code class="email">&lt;openni2_path&gt;</code>. This path should contain a Visual Studio project for Windows and a Makefile for Linux or Mac. Build the project (using Visual Studio or the <code class="email">make</code> command). Library files are generated in directories such as <code class="email">&lt;openni2_path&gt;/Bin/x64-Release</code> and <code class="email">&lt;openni2_path&gt;/Bin/x64-Release/OpenNI2/Drivers</code> (or similar names for another architecture besides x64). On Windows, add these two folders to the system's <code class="email">Path</code> so that applications can find the <code class="email">dll</code> files. On Linux or Mac, edit your <code class="email">~/.profile</code> file and add lines such as the following to create environment variables related to OpenNI2:</p><div><pre class="programlisting">export OPENNI2_INCLUDE="&lt;openni2_path&gt;/Include"
export OPENNI2_REDIST="&lt;openni2_path&gt;/Bin/x64-Release"</pre></div><p class="calibre8">At this point, we have set up OpenNI2 with support for the Sensor module, so we can create applications for the Xtion PRO Live or other cameras that are based on the PrimeSense hardware. Source code, Visual Studio projects, and Makefiles for several samples can be found in <code class="email">&lt;openni2_path&gt;/Samples</code>.</p><div><h3 class="title2"><a id="note12" class="calibre1"/>Note</h3><p class="calibre8">Optionally, OpenCV's videoio module can be compiled with support for capturing images via OpenNI or OpenNI2. However, we will capture images directly from OpenNI2 and then convert them for use with OpenCV. By using OpenNI2 directly, we gain more control over the selection of camera modes, such as raw NIR capture.</p></div><p class="calibre8">The Xtion <a id="id73" class="calibre1"/>devices are designed for USB 2.0 and their <a id="id74" class="calibre1"/>standard firmware does not work with USB 3.0 ports. For USB 3.0 compatibility, we need an unofficial firmware update. The firmware updater only runs in Windows, but after the update is applied, the device is USB 3.0-compatible in Linux and Mac, too. To obtain and apply the update, take<a id="id75" class="calibre1"/> the following steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">Download the update from <a class="calibre1" href="https://github.com/nh2/asus-xtion-fix/blob/master/FW579-RD1081-112v2.zip?raw=true">https://github.com/nh2/asus-xtion-fix/blob/master/FW579-RD1081-112v2.zip?raw=true</a> and unzip it to any destination, which we will refer to as <code class="email">&lt;xtion_firmware_unzip_path&gt;</code>.</li><li class="listitem" value="2">Ensure that the Xtion device is plugged in.</li><li class="listitem" value="3">Open Command Prompt and run the following commands:<div><pre class="programlisting">
<strong class="calibre9">&gt; cd &lt;xtion_firmware_unzip_path&gt;\UsbUpdate</strong>
<strong class="calibre9">&gt; !Update-RD108x!</strong>
</pre></div><p class="calibre26">If the firmware updater prints errors, these are not necessarily fatal. Proceed to test the camera using our demo application shown here.</p></li></ol><div></div><p class="calibre8">To understand the Xtion PRO Live's capabilities as either an active or passive NIR camera, we will build a simple application that captures and displays images from the device. Let's call this application Infravision.</p><div><h3 class="title2"><a id="note13" class="calibre1"/>Note</h3><p class="calibre8">Infravision's source<a id="id76" class="calibre1"/> code and build files are in this book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_1/Infravision">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_1/Infravision</a>.</p></div><p class="calibre8">This project needs just one source file, <code class="email">Infravision.cpp</code>. From the C standard library, we will use functionality for formatting and printing strings. Thus, our implementation begins with the following import statements:</p><div><pre class="programlisting">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;</pre></div><p class="calibre8">Infravision will use OpenNI2 and OpenCV. From OpenCV, we will use the core and imgproc modules for basic image manipulation as well as the highgui module for event handling and display. Here are the relevant import statements:</p><div><pre class="programlisting">#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/highgui.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;OpenNI.h&gt;</pre></div><div><h3 class="title2"><a id="note14" class="calibre1"/>Note</h3><p class="calibre8">The <a id="id77" class="calibre1"/>documentation for OpenNI2 as well as OpenNI can be found online at <a class="calibre1" href="http://structure.io/openni">http://structure.io/openni</a>.</p></div><p class="calibre8">The only <a id="id78" class="calibre1"/>function in Infravision is a <code class="email">main</code> function. It<a id="id79" class="calibre1"/> begins with definitions of two constants, which we might want to configure. The first of these specifies the kind of sensor data to be captured via OpenNI. This can be <code class="email">SENSOR_IR</code> (monochrome output from the IR camera), <code class="email">SENSOR_COLOR</code> (RGB output from the color camera), or <code class="email">SENSOR_DEPTH</code> (processed, hybrid data reflecting the estimated distance to each point). The second constant is the title of the application window. Here are the relevant definitions:</p><div><pre class="programlisting">int main(int argc, char *argv[]) {
  
  const openni::SensorType sensorType = openni::SENSOR_IR;
//  const openni::SensorType sensorType = openni::SENSOR_COLOR;
//  const openni::SensorType sensorType = openni::SENSOR_DEPTH;
  const char windowName[] = "Infravision";</pre></div><p class="calibre8">Based on the capture mode, we will define the format of the corresponding OpenCV matrix. The IR and depth modes are monochrome with 16 bpp. The color mode has three channels with 8 bpp per channel, as seen in the following code:</p><div><pre class="programlisting">  int srcMatType;
  if (sensorType == openni::SENSOR_COLOR) {
    srcMatType = CV_8UC3;
  } else {
    srcMatType = CV_16U;
  }</pre></div><p class="calibre8">Let's proceed by taking several steps to initialize OpenNI2, connect to the camera, configure it, and start capturing images. Here is the code for the first step, initializing the library:</p><div><pre class="programlisting">  openni::Status status;

  status = openni::OpenNI::initialize();
  if (status != openni::STATUS_OK) {
    printf(
        "Failed to initialize OpenNI:\n%s\n",
        openni::OpenNI::getExtendedError());
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">Next, we will connect to any available OpenNI-compliant camera:</p><div><pre class="programlisting">  openni::Device device;
  status = device.open(openni::ANY_DEVICE);
  if (status != openni::STATUS_OK) {
    printf(
        "Failed to open device:\n%s\n",
        openni::OpenNI::getExtendedError());
    openni::OpenNI::shutdown();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">We will<a id="id80" class="calibre1"/> ensure that the device has the appropriate <a id="id81" class="calibre1"/>type of sensor by attempting to fetch information about that sensor:</p><div><pre class="programlisting">  const openni::SensorInfo *sensorInfo =
      device.getSensorInfo(sensorType);
  if (sensorInfo == NULL) {
    printf("Failed to find sensor of appropriate type\n");
    device.close();
    openni::OpenNI::shutdown();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">We will also create a stream but not start it yet:</p><div><pre class="programlisting">  openni::VideoStream stream;
  status = stream.create(device, sensorType);
  if (status != openni::STATUS_OK) {
    printf(
        "Failed to create stream:\n%s\n",
        openni::OpenNI::getExtendedError());
    device.close();
    openni::OpenNI::shutdown();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">We will query the supported video modes and iterate through them to find the one with the highest resolution. Then, we will select this mode:</p><div><pre class="programlisting">  // Select the video mode with the highest resolution.
  {
    const openni::Array&lt;openni::VideoMode&gt; *videoModes =
        &amp;sensorInfo-&gt;getSupportedVideoModes();
    int maxResolutionX = -1;
    int maxResolutionIndex = 0;
    for (int i = 0; i &lt; videoModes-&gt;getSize(); i++) {
      int resolutionX = (*videoModes)[i].getResolutionX();
      if (resolutionX &gt; maxResolutionX) {
        maxResolutionX = resolutionX;
        maxResolutionIndex = i;
      }
    }
    stream.setVideoMode((*videoModes)[maxResolutionIndex]);
  }</pre></div><p class="calibre8">We will <a id="id82" class="calibre1"/>start <a id="id83" class="calibre1"/>streaming images from the camera:</p><div><pre class="programlisting">  status = stream.start();
  if (status != openni::STATUS_OK) {
    printf(
        "Failed to start stream:\n%s\n",
        openni::OpenNI::getExtendedError());
    stream.destroy();
    device.close();
    openni::OpenNI::shutdown();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">To prepare for capturing and displaying images, we will create an OpenNI frame, an OpenCV matrix, and a window:</p><div><pre class="programlisting">  openni::VideoFrameRef frame;
  cv::Mat dstMat;
  cv::namedWindow(windowName);</pre></div><p class="calibre8">Next, we will implement the application's main loop. On each iteration, we will capture a frame via OpenNI, convert it to a typical OpenCV format (either grayscale with 8 bpp or BGR with 8 bpp per channel), and display it via the highgui module. The loop ends when the user presses any key. Here is the implementation:</p><div><pre class="programlisting">  // Capture and display frames until any key is pressed.
  while (cv::waitKey(1) == -1) {
    status = stream.readFrame(&amp;frame);
    if (frame.isValid()) {
      cv::Mat srcMat(
          frame.getHeight(), frame.getWidth(), srcMatType,
          (void *)frame.getData(), frame.getStrideInBytes());
      if (sensorType == openni::SENSOR_COLOR) {
        cv::cvtColor(srcMat, dstMat, cv::COLOR_RGB2BGR);
      } else {
        srcMat.convertTo(dstMat, CV_8U);
      }
      cv::imshow(windowName, dstMat);
    }
  }</pre></div><div><h3 class="title2"><a id="note15" class="calibre1"/>Note</h3><p class="calibre8">OpenCV's highgui module has many shortcomings. It does not allow for handling of a standard quit event, such as the clicking of a window's <strong class="calibre9">X</strong> button. Thus, we quit based on a keystroke instead. Also, highgui imposes a delay of at least 1ms (but possibly more, depending on the operating system's minimum time to switch between threads) when polling events such as keystrokes. This delay should not matter for the purpose of demonstrating a camera with a low frame rate, such as the Xtion PRO Live with its 30 FPS limit. However, in the next section, <em class="calibre10">Supercharging the GS3-U3-23S6M-C and other Point Gray Research cameras</em>, we will explore SDL2 as a more efficient alternative to highgui.</p></div><p class="calibre8">After<a id="id84" class="calibre1"/> the loop ends (due to the user pressing a key), we<a id="id85" class="calibre1"/> will clean up the window and all of OpenNI's resources, as shown in the following code:</p><div><pre class="programlisting">  cv::destroyWindow(windowName);

  stream.stop();
  stream.destroy();
  device.close();
  openni::OpenNI::shutdown();
}</pre></div><p class="calibre8">This is the end of the source code. On Windows, Infravision can be built as a Visual C++ Win32 Console Project in Visual Studio. Remember to right-click on the project and edit its <strong class="calibre9">Project Properties</strong> so that <strong class="calibre9">C++</strong> | <strong class="calibre9">General</strong> | <strong class="calibre9">Additional Include Directories</strong> lists the path to OpenCV's and OpenNI's <code class="email">include</code> directories. Also, edit <strong class="calibre9">Linker</strong> | <strong class="calibre9">Input</strong> | <strong class="calibre9">Additional Dependencies</strong> so that it lists the paths to <code class="email">opencv_core300.lib</code> and <code class="email">opencv_imgproc300.lib</code> (or similarly named <code class="email">lib</code> files for other OpenCV versions besides 3.0.0) as well as <code class="email">OpenNI2.lib</code>. Finally, ensure that OpenCV's and OpenNI's <code class="email">dll</code> files are in the system's <code class="email">Path</code>.</p><p class="calibre8">On Linux or Mac, Infravision can be compiled using a Terminal command such as the following (assuming that the <code class="email">OPENNI2_INCLUDE</code> and <code class="email">OPENNI2_REDIST</code> environment variables are defined as described earlier in this section):</p><div><pre class="programlisting">
<strong class="calibre9">$ g++ Infravision.cpp -o Infravision \</strong>
<strong class="calibre9">  -I include -I $OPENNI2_INCLUDE -L $OPENNI2_REDIST \</strong>
<strong class="calibre9">  -Wl,-R$OPENNI2_REDIST -Wl,-R$OPENNI2_REDIST/OPENNI2 \</strong>
<strong class="calibre9">  -lopencv_core -lopencv_highgui -lopencv_imgproc -lOpenNI2</strong>
</pre></div><div><h3 class="title2"><a id="note16" class="calibre1"/>Note</h3><p class="calibre8">The <code class="email">-Wl,-R</code> flags specify an additional path where the executable should search for library files at runtime.</p></div><p class="calibre8">After building Infravision, run it and observe the pattern of NIR dots that the Xtion PRO Live projects onto nearby objects. When reflected from distant objects, the dots are sparsely<a id="id86" class="calibre1"/> spaced, but when reflected from nearby <a id="id87" class="calibre1"/>objects, they are densely spaced or even indistinguishable. Thus, the density of dots is a predictor of distance. Here is a screenshot showing the effect in a sunlit room where NIR light is coming from both the Xtion and the windows:</p><div><img src="img/00005.jpeg" alt="Supercharging the ASUS Xtion PRO Live and other OpenNI-compliant depth cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Alternatively, if you want to use the Xtion as a passive NIR camera, simply cover up the camera's NIR emitter. Your fingers will not block all of the emitter's light, but a piece of electrical tape will. Now, point the camera at a scene that has moderately bright NIR illumination. For<a id="id88" class="calibre1"/> example, the Xtion <a id="id89" class="calibre1"/>should be able to take a good passive NIR image in a sunlit room or beside a campfire at night. However, the camera will not cope well with a sunlit outdoor scene because this is vastly brighter than the conditions for which the device was designed. Here is a screenshot showing the same sunlit room as in the previous example but this time, the Xtion's NIR emitter is covered up:</p><div><img src="img/00006.jpeg" alt="Supercharging the ASUS Xtion PRO Live and other OpenNI-compliant depth cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Note that <a id="id90" class="calibre1"/>all the dots have disappeared and the new <a id="id91" class="calibre1"/>image looks like a relatively normal black-and-white photo. However, do any objects appear to have a strange glow?</p><p class="calibre8">Feel free to modify the code to use <code class="email">SENSOR_DEPTH</code> or <code class="email">SENSOR_COLOR</code> instead of <code class="email">SENSOR_IR</code>. Recompile, rerun the application, and observe the effects. The depth sensor provides a depth map, which appears bright in nearby regions and dark in faraway regions or regions of unknown distance. The color sensor provides a normal-looking image based on the visible spectrum, as seen in the following screenshot of the same sunlit room:</p><div><img src="img/00007.jpeg" alt="Supercharging the ASUS Xtion PRO Live and other OpenNI-compliant depth cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Compare the previous two screenshots. Note that the leaves of the roses are much brighter in the NIR image. Moreover, the printed pattern on the footstool (beneath the roses) is invisible in the NIR image. (When designers choose pigments, they usually do not think about how the object will look in NIR!)</p><p class="calibre8">Perhaps <a id="id92" class="calibre1"/>you want to use the Xtion as an active NIR imaging device—capable of night vision at short range—but you do not want the pattern of NIR dots. Just cover the illuminator with something to diffuse the NIR light, such as your fingers or a piece of fabric.</p><p class="calibre8">As <a id="id93" class="calibre1"/>an example of this diffused lighting, look at the following screenshot, showing a woman's wrists in NIR:</p><div><img src="img/00008.jpeg" alt="Supercharging the ASUS Xtion PRO Live and other OpenNI-compliant depth cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Note <a id="id94" class="calibre1"/>that the veins are more distinguishable<a id="id95" class="calibre1"/> than they would be in visible light. Similarly, active NIR cameras have a superior ability to capture identifiable details in the iris of a person's eye, as demonstrated in <a class="calibre1" title="Chapter 6. Efficient Person Identification Using Biometric Properties" href="part0052_split_000.html#1HIT82-940925703e144daa867f510896bffb69">Chapter 6</a>, <em class="calibre10">Efficient Person Identification Using Biometric Properties</em>. Can you find other examples of things that look much different in the NIR and visible wavelengths?</p><p class="calibre8">By now, we have seen that OpenNI-compatible cameras can be configured (programmatically and physically) to take several kinds of images. However, these cameras are designed for a specific task—depth estimation in indoor scenes—and they do not necessarily cope well with alternative uses such as outdoor NIR imaging. Next, we will look at a more diverse, more configurable, and more expensive family of cameras.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec14" class="calibre1"/>Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras</h1></div></div></div><p class="calibre8">Point <a id="id96" class="calibre1"/>Grey<a id="id97" class="calibre1"/> Research (PGR), a Canadian company, manufactures industrial cameras with a wide variety of features. A few examples are listed in the following table:</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Family and Model</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Price</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Color Sensitivity</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Highest Res Mode</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Sensor Format and Lens Mount</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Interface</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Shutter</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Firefly MV</p>
<p class="calibre15">FMVU-03MTC-CS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$275</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Color</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">752x480 @ 60 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1/3"</p>
<p class="calibre15">CS mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 2.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Firefly MV</p>
<p class="calibre15">FMVU-03MTM-CS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$275</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Gray from visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">752x480 @ 60 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1/3"</p>
<p class="calibre15">CS mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 2.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Flea 3</p>
<p class="calibre15">FL3-U3-88S2C-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$900</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Color</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4096x2160 @ 21 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1/2.5"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Rolling with global reset</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Grasshopper 3</p>
<p class="calibre15">GS3-U3-23S6C-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1,000</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Color</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1920x1200 @ 162 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1/1.2"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Grasshopper 3</p>
<p class="calibre15">GS3-U3-23S6M-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1,000</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Gray from visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1920x1200 @ 162 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1/1.2"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Grasshopper 3</p>
<p class="calibre15">GS3-U3-41C6C-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1,300</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Color</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2048x2048 @ 90 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Grasshopper 3</p>
<p class="calibre15">GS3-U3-41C6M-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1,300</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Gray from visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2048x2048 @ 90 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Grasshopper 3</p>
<p class="calibre15">GS3-U3-41C6NIR-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1,300</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Gray from NIR light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2048x2048 @ 90 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USB 3.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Gazelle</p>
<p class="calibre15">GZL-CL-22C5M-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$1,500</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Gray from visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2048x1088 @ 280 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2/3"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Camera Link</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Gazelle</p>
<p class="calibre15">GZL-CL-41C6M-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$2,200</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Gray from visible light</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2048x2048 @ 150 FPS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
<p class="calibre15">C mount</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Camera Link</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Global</p>
</td></tr></tbody></table></div><div><h3 class="title2"><a id="note17" class="calibre1"/>Note</h3><p class="calibre8">To<a id="id98" class="calibre1"/> browse the features of many more PGR cameras, see the company's Camera Selector tool at <a class="calibre1" href="http://www.ptgrey.com/Camera-selector">http://www.ptgrey.com/Camera-selector</a>. For performance statistics about<a id="id99" class="calibre1"/> the sensors in PGR cameras, see the company's series of Camera Sensor Review publications such as the ones posted at <a class="calibre1" href="http://www.ptgrey.com/press-release/10545">http://www.ptgrey.com/press-release/10545</a>.</p><p class="calibre8">For more information on sensor formats and lens mounts, see the <em class="calibre10">Shopping for glass</em> section, later in this chapter.</p></div><p class="calibre8">Some <a id="id100" class="calibre1"/>of PGR's recent cameras use the Sony Pregius<a id="id101" class="calibre1"/> brand of sensors. This sensor technology is notable for its combination of high resolution, high frame rate, and efficiency, as<a id="id102" class="calibre1"/> described in PGR's white paper at <a class="calibre1" href="http://ptgrey.com/white-paper/id/10795">http://ptgrey.com/white-paper/id/10795</a>. For example, the GS3-U3-23S6M-C (a monochrome camera) and GS3-U3-23S6C-C (a color camera) use a Pregius sensor called the Sony IMX174 CMOS. Thanks to the sensor and a fast USB 3.0 interface, these cameras are capable of capturing 1920x1200 @ 162 FPS.</p><p class="calibre8">The code in this section is tested with the GS3-U3-23S6M-C camera. However, it should work with other PGR cameras, too. Being a monochrome camera, the GS3-U3-23S6M-C allows us to see the full potential of the sensor's resolution and efficiency, without any color filter.</p><p class="calibre8">The GS3-U3-23S6M-C, like most PGR cameras, does not come with a lens; rather, it uses a standard C mount for interchangeable lenses. Examples of low-cost lenses for this mount are discussed later in this chapter, in the section <em class="calibre10">Shopping for glass</em>.</p><p class="calibre8">The GS3-U3-23S6M-C requires a USB 3.0 interface. For a desktop computer, a USB 3.0 interface can be added via a PCIe expansion card, which might cost between $15 and $60. PGR sells PCIe expansion cards that are guaranteed to work with its cameras; however, I have had success with other brands, too.</p><p class="calibre8">Once we are armed with the necessary hardware, we need to obtain an application called FlyCapture2 for configuring and testing our PGR camera. Along with this application, we will obtain FlyCapture2 SDK, which is a complete programming interface for all the functionality of our PGR camera. Go to <a class="calibre1" href="http://www.ptgrey.com/support/downloads">http://www.ptgrey.com/support/downloads</a> and download the relevant installer. (You <a id="id103" class="calibre1"/>will be prompted to register a user <a id="id104" class="calibre1"/>account if you have not already done so.) At the time of writing, the relevant download links have the following names:</p><div><ul class="itemizedlist"><li class="listitem">FlyCapture 2.8.3.1 SDK - Windows (64-bit)</li><li class="listitem">FlyCapture 2.8.3.1 SDK- Windows (32-bit)</li><li class="listitem">FlyCapture 2.8.3.1 SDK- Linux Ubuntu (64-bit)</li><li class="listitem">FlyCapture 2.8.3.1 SDK- Linux Ubuntu (32-bit)</li><li class="listitem">FlyCapture 2.8.3.1 SDK- ARM Hard Float</li></ul></div><div><h3 class="title2"><a id="note18" class="calibre1"/>Note</h3><p class="calibre8">PGR does <a id="id105" class="calibre1"/>not offer an application or SDK for Mac. However, in principle, third-party applications or SDKs might be able use PGR cameras on Mac, as most PGR cameras are compliant with standards such as IIDC/DCAM.</p></div><p class="calibre8">For <a id="id106" class="calibre1"/>Windows, run the installer that you downloaded. If in doubt, choose a <strong class="calibre9">Complete</strong> installation when prompted. A shortcut, <strong class="calibre9">Point Grey FlyCap2</strong>, should appear in your <strong class="calibre9">Start</strong> menu.</p><p class="calibre8">For Linux, decompress the downloaded archive. Follow the installation instructions in the README file (inside the decompressed folder). A launcher, <strong class="calibre9">FlyCap2</strong>, should appear in your applications menu.</p><p class="calibre8">After installation, plug in your PGR camera and open the application. You should see a window entitled <strong class="calibre9">FlyCapture2 Camera Selection</strong>, as in the following screenshot:</p><div><img src="img/00009.jpeg" alt="Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Ensure that <a id="id107" class="calibre1"/>your camera is selected and then<a id="id108" class="calibre1"/> click the <strong class="calibre9">Configure Selected</strong> button. Another window should appear. Its title includes the camera name, such as <strong class="calibre9">Point Grey Research Grasshopper3 GS3-U3-23S6M</strong>. All the camera's settings can be configured in this window. I find that the <strong class="calibre9">Camera Video Modes</strong> tab is particularly useful. Select it. You should see options relating to the capture mode, pixel format, cropped<a id="id109" class="calibre1"/> region (called <strong class="calibre9">region of interest</strong> or <strong class="calibre9">ROI</strong>), and data transfer, as shown in the following screenshot:</p><div><img src="img/00010.jpeg" alt="Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">For<a id="id110" class="calibre1"/> more information about the available<a id="id111" class="calibre1"/> modes and other settings, refer to the<a id="id112" class="calibre1"/> camera's Technical Reference Manual, which can be downloaded from <a class="calibre1" href="http://www.ptgrey.com/support/downloads">http://www.ptgrey.com/support/downloads</a>. Do not worry that you might permanently mess up any settings; they are reset every time you unplug the camera. When you are satisfied with the settings, click <strong class="calibre9">Apply</strong> and close the window. Now, in the <strong class="calibre9">Camera Selection</strong> window, click the <strong class="calibre9">OK</strong> button. On Linux, the FlyCapture2 application exits now. On Windows, we should see a new window, which also has the camera's name in its title bar. This window displays a live video feed and statistics. To ensure that the whole video is visible, select the menu option <strong class="calibre9">View</strong> | <strong class="calibre9">Stretched To Fit</strong>. Now, you should see the video letterboxed inside the window, as in the following screenshot:</p><div><img src="img/00011.jpeg" alt="Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">If the<a id="id113" class="calibre1"/> video looks corrupted (for example, if <a id="id114" class="calibre1"/>you see pieces of multiple frames at one time), the most likely reason is that the host computer is failing to handle the data transfer at a sufficiently high speed. There are two possible approaches to solving this problem:</p><div><ul class="itemizedlist"><li class="listitem">We can transfer less data. For example, go back to the <strong class="calibre9">Camera Video Modes</strong> tab of the configuration window and select either a smaller region of interest or a mode with a lower resolution.</li><li class="listitem">We can configure the operating system and BIOS to give high priority to the task<a id="id115" class="calibre1"/> of processing incoming data. For details, see the following Technical Application Note (TAN) by PGR: <a class="calibre1" href="http://www.ptgrey.com/tan/10367">http://www.ptgrey.com/tan/10367</a>.</li></ul></div><p class="calibre8">Feel free to experiment with other features of the FlyCapture2 application, such as video recording. When you are done, close the application.</p><p class="calibre8">Now that we have seen a PGR camera in action, let us write our own application to capture and display frames at high speed. It will support both Windows and Linux. We will call this application LookSpry. ("Spry" means quick, nimble, or lively, and a person who possesses these qualities is said to "look spry". If our high-speed camera application were a person, we might describe it this way.)</p><div><h3 class="title2"><a id="note19" class="calibre1"/>Note</h3><p class="calibre8">LookSpry's source<a id="id116" class="calibre1"/> code and build files are in this book's GitHub repository at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_1/LookSpry">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_1/LookSpry</a>.</p></div><p class="calibre8">Like our<a id="id117" class="calibre1"/> other demos in this chapter, LookSpry can be<a id="id118" class="calibre1"/> implemented in a single source file, <code class="email">LookSpry.cpp</code>. To begin the implementation, we need to import some of the C standard library's functionality, including string formatting and timing:</p><div><pre class="programlisting">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;time.h&gt;</pre></div><p class="calibre8">LookSpry <a id="id119" class="calibre1"/>will use three additional libraries: <strong class="calibre9">FlyCapture2 SDK</strong> (<strong class="calibre9">FC2)</strong>, OpenCV, and <strong class="calibre9">Simple DirectMedia Layer 2</strong> (<strong class="calibre9">SDL2)</strong>. (SDL2 is a<a id="id120" class="calibre1"/> cross-platform hardware abstraction layer for writing multimedia applications.) From OpenCV, we will use the core and imgproc modules for basic image manipulation, as well as the objdetect module for face detection. The role of face detection in this demo is simply to show that we can perform a real computer vision task with high-resolution input and a high frame rate. Here are the relevant import statements:</p><div><pre class="programlisting">#include &lt;flycapture/C/FlyCapture2_C.h&gt;
#include &lt;opencv2/core.hpp&gt;
#include &lt;opencv2/imgproc.hpp&gt;
#include &lt;opencv2/objdetect.hpp&gt;
#include &lt;SDL2/SDL.h&gt;</pre></div><div><h3 class="title2"><a id="note20" class="calibre1"/>Note</h3><p class="calibre8">FC2 is closed-source but owners of PGR cameras receive a license to use it. The library's documentation can be found in the installation directory.</p><p class="calibre8">SDL2 is<a id="id121" class="calibre1"/> open-source under the zlib license. The library's documentation can be found online at <a class="calibre1" href="https://wiki.libsdl.org">https://wiki.libsdl.org</a>.</p></div><p class="calibre8">Throughout LookSpry, we use a string formatting function—either <code class="email">sprintf_s</code> in the Microsoft Visual C libraries or <code class="email">snprintf</code> in standard C libraries. For our purposes, the two functions are equivalent. We will use the following macro definition so that <code class="email">snprintf</code> is mapped to <code class="email">sprintf_s</code> on Windows:</p><div><pre class="programlisting">#ifdef _WIN32
#define snprintf sprintf_s
#endif</pre></div><p class="calibre8">At several points, the application can potentially encounter an error while calling functions in FlyCapture2 or SDL2. Such an error should be shown in a dialog box. The two following helper functions get and show the relevant error message from FC2 or SDL2:</p><div><pre class="programlisting">void showFC2Error(fc2Error error) {
  if (error != FC2_ERROR_OK) {
    SDL_ShowSimpleMessage(SDL_MESSAGEBOX_ERROR,
            "FlyCapture2 Error",
            fc2ErrorToDescription(error), NULL);
  }
}

void showSDLError() {
  SDL_ShowSimpleMessageBox(
      SDL_MESSAGEBOX_ERROR, "SDL2 Error", SDL_GetError(), NULL);
}</pre></div><p class="calibre8">The<a id="id122" class="calibre1"/> rest of LookSpry is simply implemented<a id="id123" class="calibre1"/> in a <code class="email">main</code> function. At the start of the function, we will define several constants that we might want to configure, including the parameters of image capture, face detection, frame rate measurement, and display:</p><div><pre class="programlisting">int main(int argc, char *argv[]) {

  const unsigned int cameraIndex = 0u;
  const unsigned int numImagesPerFPSMeasurement = 240u;
  const int windowWidth = 1440;
  const int windowHeight = 900;
  const char cascadeFilename[] = "haarcascade_frontalface_alt.xml";
  const double detectionScaleFactor = 1.25;
  const int detectionMinNeighbours = 4;
  const int detectionFlags = CV_HAAR_SCALE_IMAGE;
  const cv::Size detectionMinSize(120, 120);
  const cv::Size detectionMaxSize;
  const cv::Scalar detectionDrawColor(255.0, 0.0, 255.0);
  char strBuffer[256u];
  const size_t strBufferSize = 256u;</pre></div><p class="calibre8">We will declare an image format, which will help OpenCV interpret captured image data. (A value will be assigned to this variable later, when we start capturing images.) We will also declare an OpenCV matrix that will store an equalized, grayscale version of the captured image. The declarations are as follows:</p><div><pre class="programlisting">  int matType;
  cv::Mat equalizedGrayMat;</pre></div><div><h3 class="title2"><a id="note21" class="calibre1"/>Note</h3><p class="calibre8">Equalization is a kind of contrast adjustment that makes all levels of brightness equally common in the output image. This adjustment makes a subject's appearance more stable with respect to variations in lighting. Thus, it is common practice to equalize an image before attempting to detect or recognize subjects (such as faces) in it.</p></div><p class="calibre8">For face <a id="id124" class="calibre1"/>detection, we will create a <code class="email">CascadeClassifier</code> object (from OpenCV's objdetect module). The classifier loads a cascade file, for<a id="id125" class="calibre1"/> which we must specify an absolute path on Windows or a relative path on Unix. The following code constructs the path, the classifier, and a vector in which face detection results will be stored:</p><div><pre class="programlisting">#ifdef _WIN32
  snprintf(strBuffer, strBufferSize, "%s/../%s", argv[0], cascadeFilename);
  cv::CascadeClassifier detector(strBuffer);
#else
  cv::CascadeClassifier detector(cascadeFilename);
#endif
  if (detector.empty()) {
    snprintf(strBuffer, strBufferSize, "%s could not be loaded.",
              cascadeFilename);
    SDL_ShowSimpleMessageBox(
      SDL_MESSAGEBOX_ERROR, "Failed to Load Cascade File", strBuffer,NULL);
    return EXIT_FAILURE;
  }
  std::vector&lt;cv::Rect&gt; detectionRects;</pre></div><p class="calibre8">Now, we must set up several things related to FlyCapture2. First, the following code creates an image header that will receive captured data and metadata:</p><div><pre class="programlisting">  fc2Error error;

  fc2Image image;
  error = fc2CreateImage(&amp;image);
  if (error != FC2_ERROR_OK) {
    showFC2Error(error);
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">The following code creates an FC2 context, which is responsible for querying, connecting to, and capturing from available cameras:</p><div><pre class="programlisting">  fc2Context context;
  error = fc2CreateContext(&amp;context);
  if (error != FC2_ERROR_OK) {
    showFC2Error(error);
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">The following lines use the context to fetch the identifier of the camera with the specified index:</p><div><pre class="programlisting">  fc2PGRGuid cameraGUID;
  error = fc2GetCameraFromIndex(context, cameraIndex, &amp;cameraGUID);
  if (error != FC2_ERROR_OK) {
    showFC2Error(error);
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">We <a id="id126" class="calibre1"/>connect to the camera:</p><div><pre class="programlisting">  error = fc2Connect(context, &amp;cameraGUID);
  if (error != FC2_ERROR_OK) {
    showFC2Error(error);
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">We<a id="id127" class="calibre1"/> finish our initialization of FC2 variables by starting the capture session:</p><div><pre class="programlisting">  error = fc2StartCapture(context);
  if (error != FC2_ERROR_OK) {
    fc2Disconnect(context);
    showFC2Error(error);
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">Our use of SDL2 also requires several initialization steps. First, we must load the library's main module and video module, as seen in the following code:</p><div><pre class="programlisting">  if (SDL_Init(SDL_INIT_VIDEO) &lt; 0) {
    fc2StopCapture(context);
    fc2Disconnect(context);
    showSDLError();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">Next, in the following code, we create a window with a specified title and size:</p><div><pre class="programlisting">  SDL_Window *window = SDL_CreateWindow(
      "LookSpry", SDL_WINDOWPOS_UNDEFINED, SDL_WINDOWPOS_UNDEFINED,
      windowWidth, windowHeight, 0u);
  if (window == NULL) {
    fc2StopCapture(context);
    fc2Disconnect(context);
    showSDLError();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">We will<a id="id128" class="calibre1"/> create a renderer that is capable <a id="id129" class="calibre1"/>of drawing textures (image data) to the window's surface. The parameters in the following code permit SDL2 to select any rendering device and any optimizations:</p><div><pre class="programlisting">  SDL_Renderer *renderer = SDL_CreateRenderer(window, -1, 0u);
  if (renderer == NULL) {
    fc2StopCapture(context);
    fc2Disconnect(context);
    SDL_DestroyWindow(window);
    showSDLError();
    return EXIT_FAILURE;
  }</pre></div><p class="calibre8">Next, we will query the renderer to see which rendering backend was selected by SDL2. The possibilities include Direct3D, OpenGL, and software rendering. Depending on the back-end, we might request a high-quality scaling mode so that the video does not appear pixelated when we scale it. Here is the code for querying and configuring the renderer:</p><div><pre class="programlisting">  SDL_RendererInfo rendererInfo;
  SDL_GetRendererInfo(renderer, &amp;rendererInfo);

  if (strcmp(rendererInfo.name, "direct3d") == 0) {
    SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, "best");
  } else if (strcmp(rendererInfo.name, "opengl") == 0) {
    SDL_SetHint(SDL_HINT_RENDER_SCALE_QUALITY, "linear");
  }</pre></div><p class="calibre8">To provide feedback to the user, we will display the name of the rendering backend in the window's title bar:</p><div><pre class="programlisting">  snprintf(strBuffer, strBufferSize, "LookSpry | %s",
      rendererInfo.name);
  SDL_SetWindowTitle(window, strBuffer);</pre></div><p class="calibre8">We will declare variables relating to the image data rendered each frame. SDL2 uses a texture as an interface to these data:</p><div><pre class="programlisting">  SDL_Texture *videoTex = NULL;
  void *videoTexPixels;
  int pitch;</pre></div><p class="calibre8">We will also declare variables relating to frame rate measurements:</p><div><pre class="programlisting">  clock_t startTicks = clock();
  clock_t endTicks;
  unsigned int numImagesCaptured = 0u;</pre></div><p class="calibre8">Three more variables will track the application's state—whether it should continue running, whether it should be detecting faces, and whether it should be mirroring the image (flipping it horizontally) for display. Here are the relevant declarations:</p><div><pre class="programlisting">  bool running = true;
  bool detecting = true;
  bool mirroring = true;</pre></div><p class="calibre8">Now, we <a id="id130" class="calibre1"/>are ready to enter the application's <a id="id131" class="calibre1"/>main loop. On each iteration, we poll the SDL2 event queue for any and all events. A quit event (which arises, for example, when the window's close button is clicked) causes the <code class="email">running</code> flag to be cleared and the <code class="email">main</code> loop to exit at the iteration's end. When the user presses <em class="calibre10">D</em> or <em class="calibre10">M</em>, respectively, the <code class="email">detecting</code> or <code class="email">mirroring</code> flag is negated. The following code implements the event handling logic:</p><div><pre class="programlisting">  SDL_Event event;
  while (running) {
    while (SDL_PollEvent(&amp;event)) {
      if (event.type == SDL_QUIT) {
        running = false;
        break;
      } else if (event.type == SDL_KEYUP) {
        switch(event.key.keysym.sym) {
        // When 'd' is pressed, start or stop [d]etection.
        case SDLK_d:
          detecting = !detecting;
          break;
        // When 'm' is pressed, [m]irror or un-mirror the video.
        case SDLK_m:
          mirroring = !mirroring;
          break;
        default:
          break;
        }
      }
    }</pre></div><p class="calibre8">Still in the main loop, we attempt to retrieve the next image from the camera. The following code does this synchronously:</p><div><pre class="programlisting">    error = fc2RetrieveBuffer(context, &amp;image);
    if (error != FC2_ERROR_OK) {
       fc2Disconnect(context);
       SDL_DestroyTexture(videoTex);
       SDL_DestroyRenderer(renderer);
       SDL_DestroyWindow(window);
       showFC2Error(error);
       return EXIT_FAILURE;
    }</pre></div><div><h3 class="title2"><a id="tip03" class="calibre1"/>Tip</h3><p class="calibre8">Given the high throughput of the GS3-U3-23S6M-C and many other Point Grey cameras, synchronous capture is justifiable here. Images are coming in so quickly that we can expect zero or negligible wait time until a buffered frame is available. Thus, the user will not experience any perceptible lag in the processing of events. However, FC2 also offers asynchronous capture, with a callback, via the <code class="email">fc2SetCallbck</code> function. The asynchronous option might be better for low-throughput cameras and, in this case, capture and rendering would not occur in the same loop as event polling.</p></div><p class="calibre8">If we<a id="id132" class="calibre1"/> have just captured the first frame in this<a id="id133" class="calibre1"/> run of the application, we still need to initialize several variables; for example, the texture is <code class="email">NULL</code>. Based on the captured image's dimensions, we can set the size of the equalized matrix and of the renderer's (pre-scaling) buffer, as seen in the following code:</p><div><pre class="programlisting">    if (videoTex == NULL) {
      equalizedGrayMat.create(image.rows, image.cols, CV_8UC1);
      SDL_RenderSetLogicalSize(renderer, image.cols, image.rows);</pre></div><p class="calibre8">Based on the captured image's pixel format, we can select closely matching formats for OpenCV matrices and for the SDL2 texture. For monochrome capture—and raw capture, which we assume to be monochrome—we will use single-channel matrices and a YUV texture (specifically, the Y channel). The following code handles the relevant cases:</p><div><pre class="programlisting">      Uint32 videoTexPixelFormat;
      switch (image.format) {
        // For monochrome capture modes, plan to render captured data
        // to the Y plane of a planar YUV texture.
        case FC2_PIXEL_FORMAT_RAW8:
        case FC2_PIXEL_FORMAT_MONO8:
          videoTexPixelFormat = SDL_PIXELFORMAT_YV12;
          matType = CV_8UC1;
          break;</pre></div><p class="calibre8">For color capture in YUV, RGB, or BGR format, we select a matching texture format and a number of matrix channels based on the format's bytes per pixel:</p><div><pre class="programlisting">        // For color capture modes, plan to render captured data
        // to the entire space of a texture in a matching color
        // format.
        case FC2_PIXEL_FORMAT_422YUV8:
          videoTexPixelFormat = SDL_PIXELFORMAT_UYVY;
          matType = CV_8UC2;
          break;
        case FC2_PIXEL_FORMAT_RGB:
          videoTexPixelFormat = SDL_PIXELFORMAT_RGB24;
          matType = CV_8UC3;
          break;
        case FC2_PIXEL_FORMAT_BGR:
          videoTexPixelFormat = SDL_PIXELFORMAT_BGR24;
          matType = CV_8UC3;
          break;</pre></div><p class="calibre8">Some <a id="id134" class="calibre1"/>capture formats, including those with <a id="id135" class="calibre1"/>16 bpp per channel, are not currently supported in LookSpry and are considered failure cases, as seen in the following code:</p><div><pre class="programlisting">        default:
          fc2StopCapture(context);
          fc2Disconnect(context);
          SDL_DestroyTexture(videoTex);
          SDL_DestroyRenderer(renderer);
          SDL_DestroyWindow(window);
                SDL_ShowSimpleMessageBox(
          SDL_MESSAGEBOX_ERROR,
          "Unsupported FlyCapture2 Pixel Format",
          "LookSpry supports RAW8, MONO8, 422YUV8, RGB, and BGR.",
          NULL);
          return EXIT_FAILURE;
      }</pre></div><p class="calibre8">We will create a texture with the given format and the same size as the captured image:</p><div><pre class="programlisting">      videoTex = SDL_CreateTexture(
          renderer, videoTexPixelFormat, SDL_TEXTUREACCESS_STREAMING,
          image.cols, image.rows);
      if (videoTex == NULL) {
        fc2StopCapture(context);
        fc2Disconnect(context);
        SDL_DestroyRenderer(renderer);
        SDL_DestroyWindow(window);
        showSDLError();
        return EXIT_FAILURE;
      }</pre></div><p class="calibre8">Using the following code, let's update the window's title bar to show the pixel dimensions of the captured image and the rendered image, in pixels:</p><div><pre class="programlisting">      snprintf(
          strBuffer, strBufferSize, "LookSpry | %s | %dx%d --&gt; %dx%d",
          rendererInfo.name, image.cols, image.rows, windowWidth,
          windowHeight);
      SDL_SetWindowTitle(window, strBuffer);
    }</pre></div><p class="calibre8">Next, if the <a id="id136" class="calibre1"/>application is in its face detection <a id="id137" class="calibre1"/>mode, we will convert the image to an equalized, grayscale version, as seen in the following code:</p><div><pre class="programlisting">    cv::Mat srcMat(image.rows, image.cols, matType, image.pData,
            image.stride);
    if (detecting) {
      switch (image.format) {
        // For monochrome capture modes, just equalize.
        case FC2_PIXEL_FORMAT_RAW8:
        case FC2_PIXEL_FORMAT_MONO8:
          cv::equalizeHist(srcMat, equalizedGrayMat);
          break;
        // For color capture modes, convert to gray and equalize.
        cv::cvtColor(srcMat, equalizedGrayMat,
               cv::COLOR_YUV2GRAY_UYVY);
          cv::equalizeHist(equalizedGrayMat, equalizedGrayMat);
          break;
        case FC2_PIXEL_FORMAT_RGB:
          cv::cvtColor(srcMat, equalizedGrayMat, cv::COLOR_RGB2GRAY);
          cv::equalizeHist(equalizedGrayMat, equalizedGrayMat);
          break;
        case FC2_PIXEL_FORMAT_BGR:
          cv::cvtColor(srcMat, equalizedGrayMat, cv::COLOR_BGR2GRAY);
          cv::equalizeHist(equalizedGrayMat, equalizedGrayMat);
          break;
        default:
          break;
      }</pre></div><p class="calibre8">We will perform face detection on the equalized image. Then, in the original image, we will draw rectangles around any detected faces:</p><div><pre class="programlisting">      // Run the detector on the equalized image.
      detector.detectMultiScale(
          equalizedGrayMat, detectionRects, detectionScaleFactor,
          detectionMinNeighbours, detectionFlags, detectionMinSize,
          detectionMaxSize);
      // Draw the resulting detection rectangles on the original image.
      for (cv::Rect detectionRect : detectionRects) {
        cv::rectangle(srcMat, detectionRect, detectionDrawColor);
      }
    }</pre></div><p class="calibre8">At this<a id="id138" class="calibre1"/> stage, we have finished our computer <a id="id139" class="calibre1"/>vision task for this frame and we need to consider our output task. The image data are destined to be copied to the texture and then rendered. First, we will lock the texture, meaning that we will obtain write access to its memory. This is accomplished in the following SDL2 function call:</p><div><pre class="programlisting">    SDL_LockTexture(videoTex, NULL, &amp;videoTexPixels, &amp;pitch);</pre></div><p class="calibre8">Remember, if the camera is in a monochrome capture mode (or a raw mode, which we assume to be monochrome), we are using a YUV texture. We need to fill the U and V channels with the mid-range value, 128, to ensure that the texture is gray. The following code accomplishes this efficiently by using the <code class="email">memset</code> function from the C standard library:</p><div><pre class="programlisting">    switch (image.format) {
    case FC2_PIXEL_FORMAT_RAW8:
    case FC2_PIXEL_FORMAT_MONO8:
      // Make the planar YUV video gray by setting all bytes in its U
      // and V planes to 128 (the middle of the range).
      memset(((unsigned char *)videoTexPixels + image.dataSize), 128,
             image.dataSize / 2u);
      break;
    default:
      break;
    }</pre></div><p class="calibre8">Now, we are ready to copy the image data to the texture. If the <code class="email">mirroring</code> flag is set, we will copy and mirror the data at the same time. To accomplish this efficiently, we will wrap the destination array in an OpenCV <code class="email">Mat</code> and then use OpenCV's <code class="email">flip</code> function to flip and copy the data simultaneously. Alternatively, if the <code class="email">mirroring</code> flag is not set, we will simply copy the data using the standard C <code class="email">memcpy</code> function. The following code implements these two alternatives:</p><div><pre class="programlisting">    if (mirroring) {
      // Flip the image data while copying it to the texture.
      cv::Mat dstMat(image.rows, image.cols, matType, videoTexPixels,
                     image.stride);
      cv::flip(srcMat, dstMat, 1);
    } else {
      // Copy the image data, as-is, to the texture.
      // Note that the PointGrey image and srcMat have pointers to the
      // same data, so the following code does reference the data that
      // we modified earlier via srcMat.
      memcpy(videoTexPixels, image.pData, image.dataSize);
    }</pre></div><div><h3 class="title2"><a id="tip04" class="calibre1"/>Tip</h3><p class="calibre8">Typically, the <code class="email">memcpy</code> function (from the C standard library) compiles to block transfer instructions, meaning that it provides the best possible hardware acceleration for copying large arrays. However, it does not support any modification or reordering of data while copying. An article by David Nadeau benchmarks <code class="email">memcpy</code> against four other copying techniques, using four<a id="id140" class="calibre1"/> compilers for each technique , and can be found at: <a class="calibre1" href="http://nadeausoftware.com/articles/2012/05/c_c_tip_how_copy_memory_quickly">http://nadeausoftware.com/articles/2012/05/c_c_tip_how_copy_memory_quickly</a>.</p></div><p class="calibre8">Now that<a id="id141" class="calibre1"/> we have written the frame's data to<a id="id142" class="calibre1"/> the texture, we will unlock the texture (potentially causing data to be uploaded to the GPU) and we will tell the renderer to render it:</p><div><pre class="programlisting">    SDL_UnlockTexture(videoTex);
    SDL_RenderCopy(renderer, videoTex, NULL, NULL);
    SDL_RenderPresent(renderer);</pre></div><p class="calibre8">After a specified number of frames, we will update our FPS measurement and display it in the window's title bar, as seen in the following code:</p><div><pre class="programlisting">    numImagesCaptured++;
    if (numImagesCaptured &gt;= numImagesPerFPSMeasurement) {
      endTicks = clock();
      snprintf(
        strBuffer, strBufferSize,
        "LookSpry | %s | %dx%d --&gt; %dx%d | %ld FPS",
        rendererInfo.name, image.cols, image.rows, windowWidth,
        windowHeight,
        numImagesCaptured * CLOCKS_PER_SEC /
         (endTicks - startTicks));
      SDL_SetWindowTitle(window, strBuffer);
      startTicks = endTicks;
      numImagesCaptured = 0u;
    }
  }</pre></div><p class="calibre8">There is nothing more in the application's main loop. Once the loop ends (as a result of the user closing the window), we will clean up FC2 and SDL2 resources and exit:</p><div><pre class="programlisting">  fc2StopCapture(context);
  fc2Disconnect(context);
  SDL_DestroyTexture(videoTex);
  SDL_DestroyRenderer(renderer);
  SDL_DestroyWindow(window);
  return EXIT_SUCCESS;
}</pre></div><p class="calibre8">On <a id="id143" class="calibre1"/>Windows, LookSpry can be built as a Visual C++ Win32 <a id="id144" class="calibre1"/>Console Project in Visual Studio. Remember to right-click on the project and edit its <strong class="calibre9">Project Properties</strong> so that <strong class="calibre9">C++</strong> | <strong class="calibre9">General</strong> | <strong class="calibre9">Additional Include Directories</strong> lists the paths to OpenCV's, FlyCapture 2's, and SDL 2's <code class="email">include</code> directories. Similarly, edit <strong class="calibre9">Linker</strong> | <strong class="calibre9">Input</strong> | <strong class="calibre9">Additional Dependencies</strong> so that it lists the paths to <code class="email">opencv_core300.lib</code>, <code class="email">opencv_imgproc300.lib</code>, and <code class="email">opencv_objdetect300.lib</code> (or similarly named <code class="email">lib</code> files for other OpenCV versions besides 3.0.0) as well as <code class="email">FlyCapture2_C.lib</code>, <code class="email">SDL2.lib</code>, and <code class="email">SDL2main.lib</code>. Finally, ensure that OpenCV's <code class="email">dll</code> files are in the system's <code class="email">Path</code>.</p><p class="calibre8">On Linux, a Terminal command such as the following should succeed in building LookSpry:</p><div><pre class="programlisting">
<strong class="calibre9">$ g++ LookSpry.cpp -o LookSpry `sdl2-config --cflags --libs` \</strong>
<strong class="calibre9">  -lflycapture-c -lopencv_core -lopencv_imgproc -lopencv_objdetect</strong>
</pre></div><p class="calibre8">Ensure that the GS3-U3-23S6M-C camera (or another PGR camera) is plugged in and that it is properly configured using the FlyCap2 GUI application. Remember that the configuration is reset whenever the camera is unplugged.</p><div><h3 class="title2"><a id="note22" class="calibre1"/>Note</h3><p class="calibre8">All the camera settings in the FlyCap2 GUI application can also be set programmatically via the FlyCapture2 SDK. Refer to the official documentation and samples that come with the SDK.</p></div><p class="calibre8">When you are satisfied with the camera's configuration, close the FlyCap2 GUI application and run LookSpry. Try different image processing modes by pressing <em class="calibre10">M</em> to un-mirror or mirror the video and <em class="calibre10">D</em> to stop or restart detection. How many frames per second are processed in each mode? How is the frame rate in detection mode affected by the number of faces?</p><div><img src="img/00012.jpeg" alt="Supercharging the GS3-U3-23S6M-C and other Point Grey Research cameras" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Hopefully, you<a id="id145" class="calibre1"/> have observed that in some<a id="id146" class="calibre1"/> or all modes, LookSpry processes frames at a much faster rate than a typical monitor's 60Hz refresh rate. The real-time video would look even smoother if we viewed it on a high-quality 144Hz gaming monitor. However, even if the refresh rate is a bottleneck, we can still appreciate the low latency or responsiveness of this real-time video.</p><p class="calibre8">Since the GS3-U3-23S6M-C and other PGR cameras take interchangeable, C-mount lenses, we should now educate ourselves about the big responsibility of buying a lens!</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec15" class="calibre1"/>Shopping for glass</h1></div></div></div><p class="calibre8">There is nothing quite like a well-crafted piece of glass that has survived many years and continued to sparkle because someone cared for it. On the mantle at home, my parents have a<a id="id147" class="calibre1"/> few keepsakes like this. One of them—a little, colorful glass flower—comes from a Parisian shopping mall where my brother and I ate many cheap but good meals during our first trip away from home and family. Other pieces go back earlier than I remember.</p><p class="calibre8">Some of the secondhand lenses that I use are 30 or 40 years old; their country of manufacture no longer exists; yet their glass and coatings are in perfect condition. I like to think that these lenses earned such good care by taking many fine pictures for previous owners and that they might still be taking fine pictures for somebody else 40 years from now.</p><p class="calibre8">Glass lasts. So do lens designs. For example, the Zeiss Planar T* lenses, one of the most respected names in the trade, are based on optical designs from the 1890s and coating processes from the 1930s. Lenses have gained electronic and motorized components to support autoexposure, autofocus, and image stabilization. However, the evolution of optics and coatings has been relatively slow. Mechanically, many old lenses are excellent.</p><div><h3 class="title2"><a id="tip05" class="calibre1"/>Tip</h3><p class="calibre8"><strong class="calibre9">Chromatic and spherical aberrations</strong></p><p class="calibre8">An ideal lens causes all incoming rays of light to converge at one focal point. However, real lenses suffer from aberrations, such that the rays do not converge at precisely the same point. If different colors or wavelengths converge at different <a id="id148" class="calibre1"/>points, the lens suffers from <strong class="calibre9">chromatic aberrations</strong>, which are visible in the image as colorful haloes around high-contrast edges. If rays from the center and edge of the lens converge at different points, the <a id="id149" class="calibre1"/>lens suffers from <strong class="calibre9">spherical aberrations</strong>, which appear in the image as bright haloes around high-contrast edges in out-of-focus regions.</p><p class="calibre8">Starting in the 1970s, new manufacturing techniques have allowed for better correction of chromatic and spherical aberrations in high-end lenses. Apochromatic or "APO" lenses use highly refractive materials (often, rare earth elements) to correct chromatic aberrations. Aspherical or "ASPH" lenses use complex curves to correct spherical aberrations. These corrected lenses tend to be more expensive, but you should keep an eye out for them as they may sometimes appear at bargain prices.</p><p class="calibre8">Due to shallow depth of field, wide apertures produce the most visible aberrations. Even with non-APO and non-ASPH lenses, aberrations may be insignificant at most aperture settings, and in most scenes.</p></div><p class="calibre8">With a few bargain-hunting skills, we can find half a dozen good lenses from the 1960s to 1980s for the price of one good, new lens. Moreover, even recent lens models might sell at a 50 percent discount on the secondhand market. Before discussing specific examples of bargains, let's <a id="id150" class="calibre1"/>consider five steps we might follow when shopping for any secondhand lens:</p><div><ol class="orderedlist"><li class="listitem" value="1">Understand the requirements. As we discussed earlier in the <em class="calibre10">Capturing the subject in the moment</em> section, many parameters of the lens and camera interact to determine whether we can take a clear and timely picture. At minimum, we should consider the appropriate focal length, f-number or T-number, and nearest focusing distance for a given application and given camera. We should also attempt to gauge the importance of high resolution and low distortion to the given application. Wavelengths of light matter, too. For example, if a lens is optimized for visible light (as most are), it is not necessarily efficient at transmitting NIR.</li><li class="listitem" value="2">Study the supply. If you live in a large city, perhaps local merchants have a good stock of used lenses at low prices. Otherwise, the lowest prices can typically be found on auction sites such as eBay. Search based on the requirements that we defined in step 1, such as "100 2.8 lens" if we are looking for a focal length of 100mm and an f-number or T-number of 2.8. Some of the lenses you find will not have the same type of mount as your camera. Check whether adapters are available. Adapting a lens is often an economical option, especially in the case of long lenses, which tend not to be mass-produced for cameras with a small sensor. Create a shortlist of the available lens models that seem to meet the requirements at an attractive price.</li><li class="listitem" value="3">Study the lens models. Online, have users published detailed specifications, sample<a id="id151" class="calibre1"/> images, test data, comparisons, and opinions? MFlenses (<a class="calibre1" href="http://www.mflenses.com/">http://www.mflenses.com/</a>) is an excellent source of information on old, manual focus lenses. It offers numerous reviews and an active forum. Image <a id="id152" class="calibre1"/>and video hosting sites such as Flickr (<a class="calibre1" href="https://www.flickr.com/">https://www.flickr.com/</a>) are also good places to search for reviews and sample output of old and unusual lenses, including cine (video) lenses, nightvision scopes, and more! Find out about variations in each lens model over the years of its manufacture. For example, early versions of a given lens might be single-coated, while newer versions would be multicoated for better transmittance, contrast, and durability.</li><li class="listitem" value="4">Pick an item in good condition. The lens should have no fungus or haze. Preferably, it should have no scratches or cleaning marks (smudges in the coatings), though the effect on image quality might be small if the damage is to the front lens element (the element farthest from the camera).<p class="calibre26">A little dust inside the lens should not affect image quality. The aperture and focusing mechanism should move smoothly and, preferably, there should be no oil on the aperture blades.</p></li><li class="listitem" value="5">Bid, make an offer, or buy at the seller's price. If you think the bidding or negotiation is reaching too high a price, save your money for another deal. Remember, despite what sellers might tell you, most bargain items are not rare and most bargain prices will occur again. Keep looking!</li></ol><div></div><p class="calibre8">Some brands<a id="id153" class="calibre1"/> enjoy an enduring reputation for excellence. Between the 1860s and 1930s, German manufacturers such as Carl Zeiss, Leica, and Schneider Kreuznach solidified their fame as creators of premium optics. (Schneider Kreuznach is best known for cine lenses.) Other, venerable European brands include Alpa (of Switzerland) and Angéniuex (of France), which are both best known for cine lenses. By the 1950s, Nikon began to gain recognition as the first Japanese manufacturer to rival the quality of German lenses. Subsequently, Fuji and Canon became regarded as makers of high-end lenses for both cine and photo cameras.</p><p class="calibre8">Although Willy Loman (from Arthur Miller's play <em class="calibre10">Death of a Salesman</em>) might advise us to buy "a well-advertised machine", this is not necessarily the best bargain. Assuming we are buying lenses for their practical value and not their collectible value, we would be delighted if we found excellent quality in a mass-produced, no-name lens. Some lenses come reasonably close to this ideal.</p><p class="calibre8">East Germany and the Soviet Union mass-produced good lenses for photo cameras, cine cameras, projectors, microscopes, night vision scopes, and other devices. Optics were also important in major projects ranging from submarines to spacecraft! East German manufacturers included Carl Zeiss Jena, Meyer, and Pentacon. Soviet (later Russian, Ukrainian, and Belarusian) manufacturers included KMZ, BelOMO, KOMZ, Vologda, LOMO, Arsenal, and many others. Often, lens designs and manufacturing processes were copied and modified by multiple manufacturers in the Eastern Bloc, giving a recognizable character to lenses of this region and era.</p><div><h3 class="title2"><a id="note23" class="calibre1"/>Note</h3><p class="calibre8">Many lenses from the Eastern Bloc were no-name goods insofar as they did not bear a manufacturer's name. Some models, for export, were simply labeled "MADE IN USSR" or "aus JENA" (from Jena, East Germany). However, most Soviet lenses bear a symbol and serial number that encode the location and date of manufacture. For a catalogue of these markings and a description of their historical <a id="id154" class="calibre1"/>significance, see Nathan Dayton's article, "An Attempt to Clear the FOG", at <a class="calibre1" href="http://www.commiecameras.com/sov/">http://www.commiecameras.com/sov/</a>.</p></div><p class="calibre8">Some of the lesser-known Japanese brands tend to be bargains, too. For example, Pentax makes good lenses but it has never commanded quite the same premium as its competitors do. The company's older photo lenses come in an M42 mount, and these are especially plentiful at low prices. Also, search for the company's C-mount cine lenses, which were formerly branded Cosmicar.</p><p class="calibre8">Let's see<a id="id155" class="calibre1"/> how we can couple some bargain lenses with the GS3-U3-23S6M-C to produce fine images. Remember that the GS3-U3-23S6M-C has a C mount and a sensor whose format is 1/1.2". For C-mount and CS-mount cameras, the name of the sensor format does not refer to any actual dimension of the sensor! Rather, for historical reasons, the measurement such as 1/1.2" refers to the diameter that a vacuum tube would have if video cameras still used vacuum tubes! The following table lists the conversions between common sensors formats and the actual dimensions of the sensor:</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Format Name</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Typical Lens Mounts</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Typical Uses</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Diagonal (mm)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Width (mm)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Height (mm)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Aspect Ratio</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1/4"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">CS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">3.2</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">2.4</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1/3"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">CS</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">6.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4.8</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">3.6</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1/2.5"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">6.4</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">5.1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">3.8</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1/2"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">8.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">6.4</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4.8</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1/1.8"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">9.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">7.2</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">5.4</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">2/3"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">11.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">8.8</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">6.6</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">16mm</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Cine</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">12.7</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">10.3</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">7.5</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1/1.2"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">13.3</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">10.7</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">8.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Super 16</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Cine</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">14.6</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">12.5</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">7.4</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">5:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">16.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">12.8</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">9.6</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Four Thirds</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Four Thirds</p>
<p class="calibre15">Micro Four Thirds</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Photography</p>
<p class="calibre15">Cine</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">21.6</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">17.3</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">13.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">4:3</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">APS-C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Various proprietary such as Nikon F</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Photography</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">27.3</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">22.7</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">15.1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">3:2</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">35mm</p>
<p class="calibre15">("Full Frame")</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">M42</p>
<p class="calibre15">Various proprietary such as Nikon F</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Photography</p>
<p class="calibre15">Machine vision</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">43.3</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">36.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">24.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">3:2</p>
</td></tr></tbody></table></div><div><h3 class="title2"><a id="note24" class="calibre1"/>Note</h3><p class="calibre8">For<a id="id156" class="calibre1"/> more information about typical sensor sizes in machine vision cameras, see the following article from Vision-Doctor.co.uk: <a class="calibre1" href="http://www.vision-doctor.co.uk/camera-technology-basics/sensor-and-pixel-sizes.html">http://www.vision-doctor.co.uk/camera-technology-basics/sensor-and-pixel-sizes.html</a>.</p></div><p class="calibre8">A lens<a id="id157" class="calibre1"/> casts a circular image, which needs to have a large enough diameter to cover the diagonal of the sensor. Otherwise, the captured image will suffer from <strong class="calibre9">vignetting</strong>, meaning <a id="id158" class="calibre1"/>that the corners will be blurry and dark. For example, vignetting is obvious in the following image of a painting, <em class="calibre10">Snow Monkeys</em>, by Janet Howse. The image was captured using a 1/1.2" sensor, but the lens was designed for a 1/2" sensor, and therefore the image is blurry and dark, except for a circular region in the center:</p><div><img src="img/00013.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">A lens designed for one format can cover any format that is approximately the same size or smaller, without vignetting; however, we might need an adapter to mount the lens. The system's diagonal field of view, in degrees, depends on the sensor's diagonal size and the lens's focal length, according to the following formula:</p><div><pre class="programlisting">diagonalFOVDegrees =
  2 * atan(0.5 * sensorDiagonal / focalLength) * 180/pi</pre></div><p class="calibre8">For example, if a lens has the same focal length as the sensor's diagonal, the diagonal FOV is 53.1 degrees. Such<a id="id159" class="calibre1"/> a lens is called a <strong class="calibre9">normal lens</strong> (with respect to the given sensor format) and a FOV of 53.1 degrees is considered neither wide nor narrow.</p><p class="calibre8">Consulting the table above, we see that the 1/1.2" format is similar to the 16mm and Super 16 formats. Thus, the GS3-U3-23S6M-C (and similar cameras) should be able to use most C-mount cine lenses without vignetting, without an adapter, and with approximately the same FOV as the lens designers intended.</p><p class="calibre8">Suppose <a id="id160" class="calibre1"/>we want a narrow FOV to capture a subject at a distance. We might need a long focal length that is not commonly available in C-mount. Comparing diagonal sizes, note that the 35mm format is larger than the 1/1.2" format by a factor of 3.25. A normal lens for 35mm format becomes a long lens with a 17.5 degree FOV when mounted for 1/1.2" format! An M42 to C mount adapter might cost $30 and lets us use a huge selection of lenses!</p><p class="calibre8">Suppose we want to take close-up pictures of a subject but none of our lenses can focus close enough. This<a id="id161" class="calibre1"/> problem has a low-tech solution—an <strong class="calibre9">extension tube</strong>, which increases the distance between the lens and camera. For C-mount, a set of several extension tubes, of various lengths, might cost $30. When the lens's total extension (from tubes plus its built-in focusing mechanism) is equal to the focal length, the subject is projected onto the sensor at 1:1 magnification. For example, a 13.3mm subject would fill the 13.3mm diagonal of a 1/1.2" sensor. The following formula holds true:</p><div><pre class="programlisting">magnificationRatio = totalExtension / focalLength</pre></div><p class="calibre8">However, for a high magnification ratio, the subject must be very close to the front lens element in order to come into focus. Sometimes, the use of extension tubes creates an impractical optical system that cannot even focus as far as its front lens element! Other times, parts of the subject and lens housing (such as a built-in lens shade) might bump into each other. Some experimentation might be required in order to determine the practical limits of extending the lens.</p><p class="calibre8">The following <a id="id162" class="calibre1"/>table lists some of the lenses and lens accessories that I have recently purchased for use with the GS3-U3-23S6M-C:</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Name</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Price</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Origin</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Mount and Intended Format</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Focal Length (mm)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">FOV for 1/1.2" Sensor (degrees)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Max f-number or T-number</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Cosmicar TV Lens 12.5mm 1:1.8</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$41</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Japan</p>
<p class="calibre15">1980s?</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
<p class="calibre15">Super 16</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">12.5</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">56.1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">T/1.8</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Vega-73</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$35</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">LZOS factory</p>
<p class="calibre15">Lytkarino, USSR</p>
<p class="calibre15">1984</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
<p class="calibre15">Super 16</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">20.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">36.9</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">T/2.0</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Jupiter-11A</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$50</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">KOMZ factory</p>
<p class="calibre15">Kazan, USSR</p>
<p class="calibre15">1975</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">M42</p>
<p class="calibre15">35mm</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">135.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">5.7</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">f/4.0</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">C-mount extension tubes: 10mm, 20mm, and 40mm</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$30</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Japan</p>
<p class="calibre15">New</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">-</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">-</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">-</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Fotodiox M42 to C adapter</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$30</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">USA</p>
<p class="calibre15">New</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">M42 to C</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">-</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">-</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">-</p>
</td></tr></tbody></table></div><p class="calibre8">For <a id="id163" class="calibre1"/>comparison, consider the following table, which gives examples <a id="id164" class="calibre1"/>of new lenses that are specifically intended for the machine vision:</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Name</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Price</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Origin</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Mount and Intended Format</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Focal Length (mm)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">FOV for 1/1.2" Sensor (degrees)</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Max f-number or T-number</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Fujinon CF12.5HA-1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$268</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Japan</p>
<p class="calibre15">New</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">12.5</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">56.1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">T/1.4</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Fujinon CF25HA-1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$270</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Japan</p>
<p class="calibre15">New</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">25.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">29.9</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">T/1.4</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Fujinon CF50HA-1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$325</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Japan</p>
<p class="calibre15">New</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">50.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">15.2</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">T/1.8</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">Fujinon CF75HA-1</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">$320</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">Japan</p>
<p class="calibre15">New</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">C</p>
<p class="calibre15">1"</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">75.0</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">10.2</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">T/1.4</p>
</td></tr></tbody></table></div><div><h3 class="title2"><a id="note25" class="calibre1"/>Note</h3><p class="calibre8">Prices <a id="id165" class="calibre1"/>in the preceding table are from B&amp;H (<a class="calibre1" href="http://www.bhphotovideo.com">http://www.bhphotovideo.com</a>), a major photo and video supplier in the United States. It offers a good selection of machine vision lenses, often at lower prices than listed by industrial suppliers.</p></div><p class="calibre8">These new, machine vision lenses are probably excellent. I have not tested them. However, let's look at a few sample shots from our selection of old, used, photo and cine lenses that are cheaper by a factor of six or more. All the images are captured at a resolution of 1920x1200 using FlyCapture2, but they are resized for inclusion in this book.</p><p class="calibre8">First, let's try the Cosmicar 12.5mm lens. For a wide-angle lens such as this, a scene with many straight lines can help us judge the amount of distortion. The following sample shot shows a reading <a id="id166" class="calibre1"/>room with many bookshelves:</p><div><img src="img/00014.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">To better see the detail, such as the text on books' spines, look at the following crop of 480x480 pixels (25 percent of the original width) from the center of the image:</p><div><img src="img/00015.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The following<a id="id167" class="calibre1"/> image is a close-up of blood vessels in my eye, captured with the Vega-73 lens and a 10mm extension tube:</p><div><img src="img/00016.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Again, the<a id="id168" class="calibre1"/> detail can be better appreciated in a 480x480 crop from the center, as shown here:</p><div><img src="img/00017.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Finally, let's <a id="id169" class="calibre1"/>capture a distant subject with the Jupiter-11A. Here is the Moon on a clear night:</p><div><img src="img/00018.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Once <a id="id170" class="calibre1"/>more, let's examine a 480x480 crop from the image's center in order to see the level of detail captured by the lens:</p><div><img src="img/00019.jpeg" alt="Shopping for glass" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">By now, our<a id="id171" class="calibre1"/> discussion of bargain lenses has spanned many decades and the distance from here to the Moon! I encourage you to explore even further on your own. What are the most interesting and capable optical systems that you can assemble within a given budget?</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec16" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter has been an opportunity to consider the inputs of a computer vision system at the hardware and software levels. Also, we have addressed the need to efficiently marshal image data from the input stage to the processing and output stages. Our specific accomplishments have included the following:</p><div><ul class="itemizedlist"><li class="listitem">Understanding differences among various wavelengths of light and radiation</li><li class="listitem">Understanding various properties of sensors, lenses, and images</li><li class="listitem">Capturing and recording slow-motion video with the PlayStation Eye camera and OpenCV's videoio module</li><li class="listitem">Comparing subjects' appearance in visible and NIR light using the ASUS Xtion PRO Live camera with OpenNI2 and OpenCV's highgui module</li><li class="listitem">Capturing and rendering HD video at high speed while detecting faces, using FlyCapture2, OpenCV, SDL2, and the GS3-U3-23S6M-C camera</li><li class="listitem">Shopping for cheap, old, used lenses that are surprisingly good!</li></ul></div><p class="calibre8">Next, in <a class="calibre1" title="Chapter 2. Photographing Nature and Wildlife with an Automated Camera" href="part0023_split_000.html#LTSU2-940925703e144daa867f510896bffb69">Chapter 2</a>, <em class="calibre10">Photographing Nature and Wildlife with an Automated Camera</em>, we will deepen our appreciation of high-quality images as we build a smart camera that captures and edits documentary footage of a natural environment!</p></div></body></html>