- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Preparation and Transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have probably heard that data scientists spend most of their time working
    on data-preparation-related activities. It is now time to explain why that happens
    and what types of activities they work on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to deal with categorical and numerical features,
    as well as how to apply different techniques to transform your data, such as one-hot
    encoding, binary encoders, ordinal encoding, binning, and text transformations.
    You will also learn how to handle missing values and outliers in your data, which
    are two important tasks you can implement to build good **machine learning** (**ML**)
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying types of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with categorical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with numerical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with unbalanced datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter is a little longer than the others and will require more patience.
    Knowing about these topics in detail will put you in a good position for the AWS
    Machine Learning Specialty exam.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying types of features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You *cannot* start modeling without knowing what a feature is and what type
    of information it can store. You have already read about the different processes
    that deal with features. For example, you know that feature engineering is related
    to the task of building and preparing features for your models; you also know
    that feature selection is related to the task of choosing the best set of features
    to feed a particular algorithm. These two tasks have one behavior in common: they
    may vary according to the types of features they are processing.'
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to understand this behavior (feature type versus applicable
    transformations) because it will help you eliminate invalid answers during your
    exam (and, most importantly, you will become a better data scientist).
  prefs: []
  type: TYPE_NORMAL
- en: By *types of features*, you refer to the data type that a particular feature
    is supposed to store. *Figure 4**.1* shows how you could potentially describe
    the different types of features of a model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Feature types](img/B21197_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Feature types
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine Learning Fundamentals*,
    you were introduced to the feature classification shown in *Figure 4**.1*. Now,
    look at some real examples, so that you can eliminate any remaining questions
    you may have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature type** | **Feature sub-type** | **Definition** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| **Categorical** | Nominal | Labelled variables with no quantitative value
    | Cloud provider: AWS, MS, Google |'
  prefs: []
  type: TYPE_TB
- en: '| **Categorical** | Ordinal | Adds the sense of order to the labelled variable
    | Job title: junior data scientist, senior data scientist, chief data scientist
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Categorical** | Binary | A variable with only two allowed values | Fraud
    classification: fraud, not fraud |'
  prefs: []
  type: TYPE_TB
- en: '| **Numerical** | Discrete | Individual and countable items | Number of students:
    100 |'
  prefs: []
  type: TYPE_TB
- en: '| **Numerical** | Continuous | Infinite number of possible measurements and
    they often carry decimal points | Total amount: $150.35 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Real examples of feature values
  prefs: []
  type: TYPE_NORMAL
- en: 'Although looking at the values of the variable may help you find its type,
    you should never rely only on this approach. The nature of the variable is also
    very important for making such decisions. For example, someone could encode the
    cloud provider variable shown in *Table 4.1* as follows: 1 (AWS), 2 (MS), 3 (Google).
    In that case, the variable is still a nominal feature, even if it is now represented
    by discrete numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are building an ML model and you don’t tell your algorithm that this
    variable is not a discrete number but is instead a nominal variable, the algorithm
    will treat it as a number and the model won’t be interpretable anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Before feeding any ML algorithm with data, make sure your feature types have
    been properly identified.
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, if you are happy with your features and have properly classified
    each of them, you should be ready to go into the modeling phase of the CRISP-DM
    methodology, shouldn’t you? Well, maybe not. There are many reasons you may want
    to spend a little more time on data preparation, even after you have correctly
    classified your features:'
  prefs: []
  type: TYPE_NORMAL
- en: Some ML libraries, such as `scikit-learn`, may not accept string values on your
    categorical features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data distribution of your variable may not be the most optimal distribution
    for your algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your ML algorithm may be impacted by the scale of your data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some observations of your variable may be missing information and you will have
    to fix it. These are also known as missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may find outlier values of your variable that can potentially add bias to
    your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your variable may be storing different types of information and you may only
    be interested in a few of them (for example, a date variable can store the day
    of the week or the week of the month).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might want to find a mathematical representation for a text variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Believe me, this list has no real end!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, you will understand how to address all these points,
    starting with categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation methods for categorical features will vary according to
    the sub-type of your variable. In the upcoming sections, you will understand how
    to transform nominal and ordinal features.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming nominal features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have to create numerical representations of your categorical features
    before applying ML algorithms to them. Some libraries may have embedded logic
    to handle that transformation for you, but most of them do not.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first transformation you will learn is known as label encoding. A label
    encoder is suitable for categorical/nominal variables, and it will just associate
    a number with each distinct label of your variables. *Table 4.2* shows how a label
    encoder works:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Countr****y** | **Label encoding** |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Canada | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Brazil | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Australia | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Label encoder in action
  prefs: []
  type: TYPE_NORMAL
- en: A label encoder will always ensure that a unique number is associated with each
    distinct label. In the preceding table, although “India” appears twice, the same
    number was assigned to it.
  prefs: []
  type: TYPE_NORMAL
- en: You now have a numerical representation of each country, but this does not mean
    you can use that numerical representation in your models! In this particular case,
    you are transforming a nominal feature, *which does not have* *an order*.
  prefs: []
  type: TYPE_NORMAL
- en: According to *Table 4.2*, if you pass the encoded version of the *country* variable
    to a model, it will make assumptions such as “Brazil (3) is greater than Canada
    (2),” which does not make any sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible solution for that scenario is applying another type of transformation
    on top of “*country”*: **one-hot encoding**. This transformation will represent
    all the categories from the original feature as individual features (also known
    as dummy **variables**), which will store the presence or absence of each category.
    *Table 4.3* is transforming the same information from *Table 4.2*, but this time
    it’s applying one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Country** | **India** | **Canada** | **Brazil** | **Australia** |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Canada | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Brazil | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Australia | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – One-hot encoding in action
  prefs: []
  type: TYPE_NORMAL
- en: You can now use the one-hot encoded version of the *country* variable as a feature
    of an ML model. However, your work as a skeptical data scientist is never done,
    and your critical thinking ability will be tested in the AWS Machine Learning
    Specialty exam.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have 150 distinct countries in your dataset. How many dummy variables
    would you come up with? 150, right? Here, you just ran into a potential issue:
    apart from adding complexity to your model (which is not a desired characteristic
    of any model at all), dummy variables also add **sparsity** to your data.'
  prefs: []
  type: TYPE_NORMAL
- en: A sparse dataset has a lot of variables filled with zeros. Often, it is hard
    to fit this type of data structure into memory (you can easily run out of memory),
    and it is very time-consuming for ML algorithms to process sparse structures.
  prefs: []
  type: TYPE_NORMAL
- en: You can work around the sparsity problem by grouping your original data and
    reducing the number of categories, and you can even use custom libraries that
    compress your sparse data and make it easier to manipulate (such as `scipy.sparse.csr_matrix`,
    from Python).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, during the exam, remember that one-hot encoding is definitely the
    right way to go when you need to transform categorical/nominal data to feed ML
    models; however, take the number of unique categories of your original feature
    into account and think about whether it makes sense to create dummy variables
    for all of them (it might not make sense if you have a very large number of unique
    categories).
  prefs: []
  type: TYPE_NORMAL
- en: Applying binary encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For those types of variables with a higher number of unique categories, a potential
    approach to creating a numerical representation for them is applying binary encoding.
    In this approach, the goal is transforming a categorical variable into multiple
    binary columns, but minimizing the number of new columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process consists of three basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The categorical data is converted into numerical data after being passed through
    an ordinal encoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting number is then converted into a binary value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The binary value is split into different columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Table 4.4* shows how to convert the data from *Table 4.2* into a binary variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Country** | **Label encoder** | **Binary** | **Col1** | **Col2** | **Col3**
    |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 | 001 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Canada | 2 | 010 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Brazil | 3 | 011 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Australia | 4 | 100 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 | 001 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.4 – Binary encoding in action
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are now three columns (Col1, Col2, and Col3) instead of
    four columns from the one-hot encoding transformation in *Table 4.3*.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming ordinal features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ordinal features have a very specific characteristic: *they have an order*.
    Because they have this quality, it does *not* make sense to apply one-hot encoding
    to them; if you do so, the underlying algorithm that is used to train your model
    will not be able to differentiate the implicit order of the data points associated
    with this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common transformation for this type of variable is known as **ordinal**
    **encoding**. An ordinal encoder will associate a number with each distinct label
    of your variable, just like a label encoder does, but this time, it will respect
    the order of each category. The following table shows how an ordinal encoder works:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Education** | **Ordinal encoding** |'
  prefs: []
  type: TYPE_TB
- en: '| Trainee | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Junior data analyst | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Senior data analyst | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Chief data scientist | 4 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.5 – Ordinal encoding in action
  prefs: []
  type: TYPE_NORMAL
- en: You can now pass the encoded variable to ML models and they will be able to
    handle this variable properly, with no need to apply one-hot encoding transformations.
    This time, comparisons such as “senior data analyst is greater than junior data
    analyst” make total sense.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding confusion in our train and test datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Do not forget the following statement: encoders are fitted on training data
    and transformed on test and production data. This is how your ML pipeline should
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have created a one-hot encoder that fits the data from *Table 4.2*
    and returns data according to *Table 4.3*. In this example, assume this is your
    training data. Once you have completed your training process, you may want to
    apply the same one-hot encoding transformation to your testing data to check the
    model’s results.
  prefs: []
  type: TYPE_NORMAL
- en: In the scenario that was just described (which is a very common situation in
    modeling pipelines), you *cannot* retrain your encoder on top of the testing data!
    You should just reuse the previous encoder object that you have created on top
    of the training data. Technically, you shouldn’t use the `fit` method again but
    the `transform` method instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may already know the reasons why you should follow this rule, but just
    as a reminder: the testing data was created to extract the performance metrics
    of your model, so you should not use it to extract any other knowledge. If you
    do so, your performance metrics will be biased by the testing data, and you cannot
    infer that the same performance (shown in the test data) is likely to happen in
    production (when new data will come in).'
  prefs: []
  type: TYPE_NORMAL
- en: Alright, all good so far. However, what if your testing set has a new category
    that was not present in the training set? How are you supposed to transform this
    data?
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to the one-hot encoding example in *Figure 4**.3* (input data) and
    *Table 4.3* (output data), this encoder knows how to transform the following countries:
    Australia, Brazil, Canada, and India. If you had a different country in the testing
    set, the encoder would not know how to transform it, and that’s why you need to
    define how it will behave in scenarios where there are exceptions.'
  prefs: []
  type: TYPE_NORMAL
- en: Most ML libraries provide specific parameters for these situations. In the previous
    example, you could configure the encoder to either raise an error or set all zeros
    on our dummy variables, as shown in *Table 4.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Country** | **India** | **Canada** | **Brazil** | **Australia** |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Canada | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Brazil | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Australia | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| India | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Portugal** | **0** | **0** | **0** | **0** |'
  prefs: []
  type: TYPE_TB
- en: Table 4.6 – Handling unknown values on one-hot encoding transformations
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, Portugal was not present in the training set (*Table 4.2*),
    so during the transformation, it will keep the same list of known countries and
    say that Portugal *is not* among them (all zeros).
  prefs: []
  type: TYPE_NORMAL
- en: As the very good skeptical data scientist you are becoming, should you be concerned
    about the fact that you have a particular category that has not been used during
    training? Well, maybe. This type of analysis really depends on your problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: Handling unknown values is very common and something that you should expect
    to do in your ML pipeline. However, you should also ask yourself, due to the fact
    that you did not use that particular category during your training process, whether
    your model can be extrapolated and generalized.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, your testing data must follow the same data distribution as your training
    data, and you are very likely to find all (or at least most) of the categories
    (of a categorical feature) either in the training or testing sets. Furthermore,
    if you are facing overfitting issues (doing well in the training, but poorly in
    the testing set) and, at the same time, you realize that your categorical encoders
    are transforming a lot of unknown values in the test set, guess what? It’s likely
    that your training and testing samples are not following the same distribution,
    invalidating your model entirely.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, slowly, you are getting there. You are learning about bias and
    investigation strategies in fine-grained detail – that is so exciting! Now, move
    on and look at performing transformations on numerical features. Yes, each type
    of data matters and drives your decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with numerical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In terms of numerical features (discrete and continuous), you can think of transformations
    that rely on the training data and others that rely purely on the (individual)
    observation being transformed.
  prefs: []
  type: TYPE_NORMAL
- en: Those who rely on the training data will use the training set to learn the necessary
    parameters during `fit`, and then use them to transform any test or new data.
    The logic is pretty much the same as what you just learned for categorical features;
    however, this time, the encoder will learn different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, those that rely purely on (individual) observations do not
    depend on training or testing sets. They will simply perform a mathematical computation
    on top of an individual value. For example, you could apply an exponential transformation
    to a particular variable by squaring its value. There is no dependency on learned
    parameters from anywhere – just get the value and square it.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might be thinking about dozens of available transformations
    for numerical features! Indeed, there are so many options, and you will not learn
    all of them here. However, you are not supposed to know all of them for the AWS
    Machine Learning Specialty exam. You will learn the most important ones (for the
    exam), but you should not limit your modeling skills: take a moment to think about
    the unlimited options you have by creating custom transformations according to
    your use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Data normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applying data **normalization** means changing the scale of the data. For example,
    your feature may store employee salaries that range between 20,000 and 200,000
    dollars/year and you want to put this data in the range of 0 and 1; where 20,000
    (the minimum observed value) will be transformed as 0; and 200,000 (the maximum
    observed value) will be transformed as 1.
  prefs: []
  type: TYPE_NORMAL
- en: This type of technique is especially important when you want to fit your training
    data on top of certain types of algorithms that are impacted by the scale/magnitude
    of the underlying data. For instance, you can think about those algorithms that
    use the dot product of the input variables (such as neural networks or linear
    regression) and those algorithms that rely on distance measures (such as k-**nearest
    neighbors (KNN)** or **k-means**).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, applying data normalization will not result in performance
    improvements for rule-based algorithms, such as decision trees, since they will
    be able to check the predictive power of the features (either via entropy or information
    gain analysis), regardless of the scale of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You will learn about these algorithms, along with the appropriate details, in
    the later chapters of this book. For instance, you can look at entropy and information
    gain as two types of metrics used by decision trees to check feature importance.
    Knowing the predictive power of each feature helps the algorithm define the optimal
    root, intermediaries, and leaf nodes of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Take a moment and use the following example to understand why data normalization
    will help those types of algorithms. You already know that the goal of a clustering
    algorithm is to find groups or clusters in your data, and one of the most used
    clustering algorithms is known as k-means.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.2* shows how different scales of the variable could change the
    hyper plan’s projection of k-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Plotting data of different scales in a hyper plan](img/B21197_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Plotting data of different scales in a hyper plan
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of *Figure 4**.2*, you can see a single data point plotted
    in a hyper plan that has three dimensions (x, y, and z). All three dimensions
    (also known as features) were normalized to the scale of 0 and 1\. On the right-hand
    side, you can see the same data point, but this time, the x dimension was *not*
    normalized. You can clearly see that the hyper plan has changed.
  prefs: []
  type: TYPE_NORMAL
- en: In a real scenario, you would have far more dimensions and data points. The
    difference in the scale of the data would change the centroids of each cluster
    and could potentially change the assigned clusters of some points. This same problem
    will happen on other algorithms that rely on distance calculation, such as KNN.
  prefs: []
  type: TYPE_NORMAL
- en: Other algorithms, such as neural networks and linear regression, will compute
    weighted sums using your input data. Usually, these types of algorithms will perform
    operations such as *W1*X1 + W2*X2 + Wi*Xi*, where *Xi* and *Wi* refer to a particular
    feature value and its weight, respectively. Again, you will learn details of neural
    networks and linear models later, but can you see the data scaling problem by
    just looking at the calculations that were just described? It can easily come
    up with very large values if *X* (feature) and *W* (weight) are large numbers.
    That will make the algorithm’s optimizations much more complex. In neural networks,
    this problem is known as gradient exploding.
  prefs: []
  type: TYPE_NORMAL
- en: You now have a very good understanding of the reasons you should apply data
    normalization (and when you should not). Data normalization is often implemented
    in ML libraries as **Min Max Scaler**. If you find this term in the exam, then
    remember that it is the same as data normalization.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, data normalization does not necessarily need to transform your
    feature into a range between 0 and 1\. In reality, you can transform the feature
    into any range you want. *Figure 4**.3* shows how normalization is formally defined.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Normalization formula](img/B21197_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Normalization formula
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Xmin* and *Xmax* are the lower and upper values of the range; *X* is
    the value of the feature. Apart from data normalization, there is another very
    important technique regarding numerical transformations that you *must* be aware
    of, not only for the exam but also for your data science career. You’ll look at
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data standardization is another scaling method that transforms the distribution
    of the data, so that the mean will become 0 and the standard deviation will become
    1\. *Figure 4**.4* formally describes this scaling technique, where *X* represents
    the value to be transformed, *µ* refers to the mean of *X*, and *σ* is the standard
    deviation of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Standardization formula](img/B21197_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Standardization formula
  prefs: []
  type: TYPE_NORMAL
- en: Unlike normalization, data standardization will *not* result in a predefined
    range of values. Instead, it will transform your data into a standard Gaussian
    distribution, where your transformed values will represent the number of standard
    deviations of each value to the mean of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gaussian distribution, also known as the normal distribution, is one of
    the most used distributions in statistical models. This is a continuous distribution
    with two main controlled parameters: *µ* (mean) and *σ* (standard deviation).
    Normal distributions are symmetric around the mean. In other words, most of the
    values will be close to the mean of the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data standardization is often referred to as the zscore and is widely used
    to identify outliers on your variable, which you will see later in this chapter.
    For the sake of demonstration, *Table 4.7* simulates the data standardization
    of a small dataset. The input value is shown in the Age column, while the scaled
    value is shown in the Zscore column:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Mean** | **Standard deviation** | **Zscore** |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 31,83 | 25,47 | -1,05 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 31,83 | 25,47 | -0,46 |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 31,83 | 25,47 | -0,31 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 31,83 | 25,47 | 0,01 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 31,83 | 25,47 | -0,07 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 31,83 | 25,47 | 1,89 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.7 – Data standardization in action
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you are confident when applying normalization and standardization
    by hand in the AWS Machine Learning Specialty exam. They might provide a list
    of values, as well as mean and standard deviation, and ask you for the scaled
    value of each element in the list.
  prefs: []
  type: TYPE_NORMAL
- en: Applying binning and discretization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Binning** is a technique where you can group a set of values into a bucket
    or bin – for example, grouping people between 0 and 14 years old into a bucket
    named “children,” another group of people between 15 and 18 years old into a bucket
    named “teenager,” and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discretization** is the process of transforming a continuous variable into
    discrete or nominal attributes. These continuous values can be discretized by
    multiple strategies, such as **equal-width** and **equal-frequency**.'
  prefs: []
  type: TYPE_NORMAL
- en: An equal-width strategy will split your data across multiple bins of the same
    width. Equal-frequency will split your data across multiple bins with the same
    number of frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following example. Suppose you have the following list containing
    16 numbers: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 90\. As
    you can see, this list ranges between 10 and 90\. Assuming you want to create
    four bins using an equal-width strategy, you could come up with the following
    bins:'
  prefs: []
  type: TYPE_NORMAL
- en: Bin >= 10 <= 30 > 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 30 <= 50 >
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 50 <= 70 >
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 71 <= 90 > 90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the width of each bin is the same (20 units), but the observations
    are not equally distributed. Now, the next example simulates an equal-frequency
    strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Bin >= 10 <= 13 > 10, 11, 12, 13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 13 <= 17 > 14, 15, 16, 17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 17 <= 21 > 18, 19, 20, 21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 21 <= 90 > 22, 23, 24, 90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, all the bins have the same frequency of observations, although
    they have been built with different bin widths to make that possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have computed your bins, you should be wondering what’s next, right?
    Here, you have some options:'
  prefs: []
  type: TYPE_NORMAL
- en: You can name your bins and use them as a nominal feature on your model! Of course,
    as a nominal variable, you should think about applying one-hot encoding before
    feeding an ML model with this data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might want to order your bins and use them as an ordinal feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maybe you want to remove some noise from your feature by averaging the minimum
    and maximum values of each bin and using that value as your transformed feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at *Table 4.8* to understand these approaches using our equal-frequency
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ordinal** **value** | **Bin** | **Transforming to a** **nominal feature**
    | **Transforming to an** **ordinal feature** | **Removing noise** |'
  prefs: []
  type: TYPE_TB
- en: '| **10** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **11** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **12** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **13** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **14** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **15** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **16** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **17** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **18** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **19** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **20** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **21** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **22** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **23** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **24** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  prefs: []
  type: TYPE_TB
- en: '| **90** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.8 – Different approaches to working with bins and discretization
  prefs: []
  type: TYPE_NORMAL
- en: Again, playing with different binning strategies will give you different results
    and you should analyze/test the best approach for your dataset. There is no standard
    answer here – it is all about data exploration!
  prefs: []
  type: TYPE_NORMAL
- en: Applying other types of numerical transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Normalization and standardization rely on your training data to fit their parameters:
    minimum and maximum values, in the case of normalization, and mean and standard
    deviation in the case of standard scaling. This also means you must fit those
    parameters using *only* your training data and never the testing data.'
  prefs: []
  type: TYPE_NORMAL
- en: However, there are other types of numerical transformations that do not require
    parameters from training data to be applied. These types of transformations rely
    purely on mathematical computations. For example, one of these transformations
    is known as logarithmic transformation. This is a very common type of transformation
    in ML models and is especially beneficial for skewed features. If you don’t know
    what a skewed distribution is, take a look at *Figure 4**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Skewed distributions](img/B21197_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Skewed distributions
  prefs: []
  type: TYPE_NORMAL
- en: In the middle, you have a normal distribution (or Gaussian distribution). On
    the left- and right-hand sides, you have skewed distributions. In terms of skewed
    features, there will be some values far away from the mean in one single direction
    (either left or right). Such behavior will push both the median and mean values
    of this distribution in the same direction, that of the long tail you can see
    in *Figure 4**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: One very clear example of data that used to be skewed is the annual salaries
    of a particular group of professionals in a given region, such as senior data
    scientists working in Florida, US. This type of variable usually has most of its
    values close to the others (because people used to earn an average salary) and
    just has a few very high values (because a small group of people makes much more
    money than others).
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you can now easily understand why the mean and median values will
    move to the tail direction, right? The big salaries will push them in that direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, but why will a logarithmic transformation be beneficial for this type
    of feature? The answer to this question can be explained by the math behind it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Logarithmic properties](img/B21197_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Logarithmic properties
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the log of a number is the inverse of the exponential function. Log
    transformation will then reduce the scale of your number according to a given
    base (such as base 2, base 10, or base e, in the case of a natural logarithm).
    Looking at the salary distribution from the previous example, you would bring
    all those numbers down so that the higher the number, the higher the reduction;
    however, you would do this in a log scale and not in a linear fashion. Such behavior
    will remove the outliers of this distribution (making it closer to a normal distribution),
    which is beneficial for many ML algorithms, such as linear regression. *Table
    4.9* shows you some of the differences when transforming a number in a linear
    scale versus a log scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ordinal value** | **Linear** **scale (normalization)** | **Log scale (****base
    10)** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.0001 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1,000 | 0.01 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 10,000 | 0.1 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 100,000 | 1 | 5 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.9 – Differences between linear transformation and log transformation
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the linear transformation kept the original magnitude of the
    data (you can still see outliers, but in another scale), while the log transformation
    removed those differences of magnitude and still kept the order of the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Would you be able to think about another type of mathematical transformation
    that follows the same behavior of *log* (making the distribution closer to Gaussian)?
    OK, here you have another: square root. Take the square root of those numbers
    shown in *Table 4.9* and see yourself!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, pay attention to this: both log and square root belong to a set of transformations
    known as power transformations, and there is a very popular method, which is likely
    to be mentioned on your AWS exam, that can perform a range of power transformations
    like those you have seen. This method was proposed by George Box and David Cox
    and its name is **Box-Cox**.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: During your exam, if you see questions about the Box-Cox transformation, remember
    that it is a method that can perform many power transformations (according to
    a lambda parameter), and its end goal is to make the original distribution closer
    to a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Just to conclude this discussion regarding why mathematical transformations
    can really make a difference to ML models, here is an example of **exponential
    transformations.**
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a set of data points, such as those on the left-hand side of
    *Figure 4**.7*. Your goal is to draw a line that will perfectly split blue and
    red points. Just by looking at the original data (again, on the left-hand side),
    you know that your best guess for performing this linear task would be the one
    you can see in the same figure. However, the science (not magic) happens on the
    right-hand side of the figure! By squaring those numbers and plotting them in
    another hyper plan, you can perfectly separate each group of points.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Exponential transformation in action](img/B21197_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Exponential transformation in action
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that there are infinite ways in which you can deal with
    your data. Although this is true, you should always take the business scenario
    you are working on into account and plan the work accordingly. Remember that model
    improvements or exploration is always possible, but you have to define your goals
    (remember the CRISP-DM methodology) and move on.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, data transformation is important, but it is just one piece of your
    work as a data scientist. Your modeling journey still needs to move to other important
    topics, such as missing values and outliers handling. However, before that, you
    may have noticed that you were introduced to Gaussian distributions during this
    section, so why not go deeper into it?
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the Gaussian distribution is probably the most common distribution
    for statistical and machine learning models, you should be aware that it is not
    the only one. There are other types of data distributions, such as the Bernoulli,
    binomial, and Poisson distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bernoulli distribution is a very simple one, as there are only two types
    of possible events: success or failure. The success event has a probability *p*
    of happening, while the failure one has a probability of *1-p*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples that follow a Bernoulli distribution are rolling a six-sided
    die or flipping a coin. In both cases, you must define the event of success and
    the event of failure. For example, assume the following success and failure events
    when rolling a die:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Success: Getting a number 6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Failure: Getting any other number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can then say that there is a *p* probability of success (1/6 = 0.16 = 16%)
    and a *1-p* probability of failure (1 - 0.16 = 0.84 = 84%).
  prefs: []
  type: TYPE_NORMAL
- en: The binomial distribution generalizes the Bernoulli distribution. The Bernoulli
    distribution has only one repetition of an event, while the binomial distribution
    allows the event to be repeated many times, and you must count the number of successes.
    Continue with the prior example, that is, counting the number of times you got
    a 6 out of our 10 dice rolls. Due to the nature of this example, binomial distribution
    has two parameters, *n* and *p*, where *n* is the number of repetitions and *p*
    is the probability of success in every repetition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a Poisson distribution allows you to find a number of events in a
    period of time, given the number of times an event occurs in an interval. It has
    three parameters: lambda, *e*, and *k*, where lambda is the average number of
    events per interval, *e* is the Euler number, and *k* is the number of times an
    event occurs in an interval.'
  prefs: []
  type: TYPE_NORMAL
- en: With all those distributions, including the Gaussian one, it is possible to
    compute the expected mean value and variance based on their parameters. This information
    is usually used in hypothesis tests to check whether some sample data follows
    a given distribution, by comparing the mean and variance **of the sample** against
    the **expected** mean and variance of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: You are now more familiar with data distributions, not only Gaussian distributions.
    You will keep learning about data distributions throughout this book. For now,
    it is time to move on to missing values and outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, missing values refer to the absence of data. Such absences
    are usually represented by tokens, which may or may not be implemented in a standard
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Although using tokens is standard, the way those tokens are displayed may vary
    across different platforms. For example, relational databases represent missing
    data with *NULL*, core Python code will use *None*, and some Python libraries
    will represent missing numbers as **Not a** **Number (NaN)**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For numerical fields, don’t replace those standard missing tokens with *zeros*.
    By default, zero is not a missing value, but another number.
  prefs: []
  type: TYPE_NORMAL
- en: However, in real business scenarios, you may or may not find those standard
    tokens. For example, a software engineering team might have designed the system
    to automatically fill missing data with specific tokens, such as “unknown” for
    strings or “-1” for numbers. In that case, you would have to search by those two
    tokens to find missing data. People can set anything.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the software engineering team was still kind enough
    to give you standard tokens. However, there are many cases where legacy systems
    do not add any data quality layer in front of the user, and you may find an address
    field filled with, “I don’t want to share,” or a phone number field filled with,
    “Don’t call me.” This is clearly missing data, but not as standard as the previous
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many more nuances that you will learn regarding missing data, all
    of which you will learn in this section, but be advised: before you start making
    decisions about missing values, you should prepare a good data exploration and
    make sure you find those values. You can either compute data frequencies or use
    missing plots, but please do something. Never assume that your missing data is
    represented only by those handy standard tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Why should you care about this type of data? Well, first, because most algorithms
    (apart from decision trees implemented on very specific ML libraries) will raise
    errors when they find a missing value. Second (and maybe most important), by grouping
    all the missing data in the same bucket, you are assuming that they are all the
    same, but in reality, you don’t know that.
  prefs: []
  type: TYPE_NORMAL
- en: Such a decision will not only add bias to your model – it will reduce its interpretability,
    as you will be unable to explain the missing data. Once you know why you want
    to treat the missing values, then you can analyze your options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, you can classify missing values into two main groups: **MCAR**
    or **MNAR**. MCAR stands for **Missing Completely at Random** and states that
    there is no pattern associated with the missing data. On the other hand, MNAR
    stands for **Missing Not at Random** and means that the underlying process used
    to generate the data is strictly connected to the missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: Look at the following example about MNAR missing values. Suppose you are collecting
    user feedback about a particular product in an online survey. Your process of
    asking questions is dynamic and depends on user answers. When a user specifies
    an age lower than 18 years old, you never ask his/her marital status. In this
    case, missing values of marital status are connected to the age of the user (MNAR).
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the class of missing values that you are dealing with will help you
    understand whether you have any control over the underlying process that generates
    the data. Sometimes, you can come back to the source process and, somehow, complete
    your missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Although, in real scenarios, you usually need to treat missing data via exclusion
    or imputation, never forget that you can always try to look at the source process
    and check if you can retrieve (or, at least, better understand) the missing data.
    You may face this option in the exam.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have an opportunity to recover your missing data from anywhere,
    then you should move on to other approaches, such as **listwise deletion** and
    **imputation**.
  prefs: []
  type: TYPE_NORMAL
- en: Listwise deletion refers to the process of discarding some data, which is the
    downside of this choice. This may happen at the row level or the column level.
    For example, suppose you have a DataFrame containing four columns and one of them
    has 90% of its data missing. In such cases, what usually makes more sense is dropping
    the entire feature (column), since you don’t have that information for the majority
    of your observations (rows).
  prefs: []
  type: TYPE_NORMAL
- en: From a row perspective, you may have a DataFrame with a small number of observations
    (rows) containing missing data in one of its features (columns). In such scenarios,
    instead of removing the entire feature, what makes more sense is removing only
    those few observations.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using this method is the simplicity of dropping a row or a column.
    Again, the downside is losing information. If you don’t want to lose information
    while handling your missing data, then you should go for an imputation strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imputation is also known as replacement, where you will replace missing values
    by substituting a value. The most common approach to imputation is replacing the
    missing value with the mean of the feature. Please take note of this approach
    because it is likely to appear in your exam:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** |'
  prefs: []
  type: TYPE_TB
- en: '| 35 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| 25 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.10 – Replacing missing values with the mean or median
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 4.10* shows a very simple dataset with one single feature and five observations,
    where the third observation has a missing value. If you decide to replace that
    missing data with the mean value of the feature, you will come up with 49\. Sometimes,
    when there are outliers in the data, the median might be more appropriate (in
    this case, the median would be 35):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Job status** |'
  prefs: []
  type: TYPE_TB
- en: '| **35** | Employee |'
  prefs: []
  type: TYPE_TB
- en: '| **30** | Employee |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retired |'
  prefs: []
  type: TYPE_TB
- en: '| **25** | Employee |'
  prefs: []
  type: TYPE_TB
- en: '| **80** | Retired |'
  prefs: []
  type: TYPE_TB
- en: '| **75** | Retired |'
  prefs: []
  type: TYPE_TB
- en: Table 4.11 – Replacing missing values with the mean or median of the group
  prefs: []
  type: TYPE_NORMAL
- en: If you want to go deeper, you could find the mean or median value according
    to a given group of features. For example, *Table 4.11* expanded the previous
    dataset from *Table 4.10* by adding the *Job status* column. Now, there is some
    evidence that the initial approach of changing the missing value by using the
    overall median (35 years old) was likely to be wrong (since that person is retired).
  prefs: []
  type: TYPE_NORMAL
- en: What you can do now is replace the missing value with the mean or median of
    the other observations that belong to the same job status. Using this new approach,
    you can change the missing information to 77.5\. Considering that the person is
    retired, 77.5 makes more sense than 35 years old.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In the case of categorical variables, you can replace the missing data with
    the value that has the highest occurrence in your dataset. The same logic of grouping
    the dataset according to specific features is still applicable.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use more sophisticated methods of imputation, including constructing
    an ML model to predict the value of your missing data. The downside of these imputation
    approaches (either by averaging or predicting the value) is that you are making
    inferences about the data that are not necessarily right and will add bias to
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: To sum this up, the trade-off while dealing with missing data is having a balance
    between losing data or adding bias to the dataset. Unfortunately, there is no
    scientific recipe that you can follow, whatever your problem is. To decide on
    what you are going to do, you must look at your success criteria, explore your
    data, run experiments, and then make your decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will now move to another headache for many ML algorithms: outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are not on this studying journey just to pass the AWS Machine Learning Specialty
    exam but also to become a better data scientist. There are many different ways
    to look at the outlier problem purely from a mathematical perspective; however,
    the datasets used in real life are derived from the underlying business process,
    so you must include a business perspective during an outlier analysis.
  prefs: []
  type: TYPE_NORMAL
- en: An outlier is an atypical data point in a set of data. For example, *Figure
    4**.8* shows some data points that have been plotted in a two-dimension plan;
    that is, x and y. The red point is an outlier since it is an atypical value in
    this series of data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Identifying an outlier](img/B21197_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Identifying an outlier
  prefs: []
  type: TYPE_NORMAL
- en: It is important to treat outlier values because some statistical methods are
    impacted by them. Still, in *Figure 4**.8*, you can see this behavior in action.
    On the left-hand side, there has been drawn a line that best fits those data points,
    ignoring the red point. On the right-hand side, the same line was drawn, but including
    the red point.
  prefs: []
  type: TYPE_NORMAL
- en: You can visually conclude that, by ignoring the outlier point, you will come
    up with a better solution on the plan of the left-hand side of the preceding chart
    since it was able to pass closer to most of the values. You can also prove this
    by computing an associated error for each line (which you will learn later in
    this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth reminding that you have also seen the outlier issue in action in
    another situation in this book: specifically, in *Table 4.10*, while dealing with
    missing values. In that example, the median was used to work around the problem.
    Feel free to go back and read it again, but what should be very clear at this
    point is that median values are less impacted by outliers than average (mean)
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: You now know what outliers are and why you should treat them. You should always
    consider your business perspective while dealing with outliers, but there are
    mathematical methods to find them. Now, you are ready to move on and look at some
    methods for outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have already learned about the most common method: zscore. In *Table 4.7*,
    you saw a table containing a set of ages. Refer to it again to refresh your memory.
    In the last column of that table, it was computed the zscore of each age, according
    to the equation shown in *Figure 4**.4*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no well-defined range for those zscore values; however, in a normal
    distribution *without* outliers, they will mostly range between -3 and 3\. Remember:
    zscore will give you the number of standard deviations from the mean of the distribution.
    *Table 4.10* shows some of the properties of a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Normal distribution properties. Image adapted from  https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg](img/B21197_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg
  prefs: []
  type: TYPE_NORMAL
- en: According to the normal distribution properties, 95% of values will belong to
    the range of -2 and 2 standard deviations from the mean, while 99% of the values
    will belong to the range of -3 and 3\. Coming back to the outlier detection context,
    you can set thresholds on top of those zscore values to specify whether a data
    point is an outlier or not!
  prefs: []
  type: TYPE_NORMAL
- en: There is no standard threshold that you can use to classify outliers. Ideally,
    you should look at your data and see what makes more sense for you… usually (this
    is not a rule), you will use some number between 2 and 3 standard deviations from
    the mean to flag outliers, since less than 5% of your data will be selected by
    this rule (again, this is just a reference threshold, so that you can select some
    data from further scructizing). You may remember that there are outliers *below*
    and *above* the mean value of the distribution, as shown in *Table 4.12*, where
    the outliers were flagged with an **absolute** zscore greater than 3 (the value
    column is hidden for the sake of this demonstration).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value** | **Zscore** | **Is outlier?** |'
  prefs: []
  type: TYPE_TB
- en: '| ... | 1.3 | NO |'
  prefs: []
  type: TYPE_TB
- en: '| ... | 0.8 | NO |'
  prefs: []
  type: TYPE_TB
- en: '| ... | **3.1** | **YES** |'
  prefs: []
  type: TYPE_TB
- en: '| ... | -2.9 | NO |'
  prefs: []
  type: TYPE_TB
- en: '| ... | **-****3.5** | **YES** |'
  prefs: []
  type: TYPE_TB
- en: '| ... | 1.0 | NO |'
  prefs: []
  type: TYPE_TB
- en: '| ... | 1.1 | NO |'
  prefs: []
  type: TYPE_TB
- en: Table 4.12 – Flagging outliers according to the zscore value
  prefs: []
  type: TYPE_NORMAL
- en: 'Two outliers were found in *Table 4.12*: row number three and row number five.
    Another way to find outliers in the data is by applying the box plot logic. When
    you look at a numerical variable, it is possible to extract many descriptive statistics
    from it, not only the mean, median, minimum, and maximum values, as you have seen
    previously. Another property that’s present in data distributions is known as
    quantiles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantiles are cut-off points that are established at regular intervals from
    the cumulative distribution function of a random variable. Those regular intervals,
    also known as *q-quantiles*, will be nearly the same size and will receive special
    names in some situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The 4-quantiles are called quartiles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 10-quantiles are called deciles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 100-quantiles are called percentiles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, the 20th percentile (of a 100-quantile regular interval) specifies
    that 20% of the data is below that point. In a box plot, you can use regular intervals
    of 4-quantiles (also known as *quartiles*) to expose the distribution of the data
    (Q1 and Q3), as shown in *Figure 4**.10.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Box plot definition](img/B21197_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Box plot definition
  prefs: []
  type: TYPE_NORMAL
- en: Q1 is also known as the lower quartile or 25th quartile, and this means that
    25% of the data is below that point in the distribution. Q3 is also known as the
    upper quartile or 75th quartile, and this means that 75% of the data is below
    that point in the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the difference between Q1 and Q3 will give you the **interquartile
    range (IQR)** value, which you can then use to compute the limits of the box plot,
    shown by the “minimum” and “maximum” labels in the preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: After all, you can finally infer that anything below the “minimum” value or
    above the “maximum” value of the box plot will be flagged as an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have now learned about two different ways you can flag outliers on your
    data: zscore and box plot. You can decide whether you are going to remove these
    points from your dataset, transform them, or create another variable to specify
    that they exist (as shown in *Table 4.11*).'
  prefs: []
  type: TYPE_NORMAL
- en: Moving further on this journey of data preparation and transformation, you will
    face other types of problems in real life. Next, you will learn that several use
    cases contain something known as **rare events**, which makes ML algorithms focus
    on the wrong side of the problem and propose bad solutions. Luckily, you will
    learn how to either tune hyperparameters or prepare the data to facilitate algorithm
    convergence while fitting rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with unbalanced datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you might have realized why data preparation is probably the
    longest part of the data scientist’s work. You have learned about data transformation,
    missing data values, and outliers, but the list of problems goes on. Don’t worry
    – you are on the right journey to master this topic!
  prefs: []
  type: TYPE_NORMAL
- en: Another well-known problem with ML models, specifically with classification
    problems, is unbalanced classes. In a classification model, you can say that a
    dataset is unbalanced when most of its observations belong to one (or some) of
    the classes (target variable).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is very common in fraud identification systems: for example, where most
    of the events belong to a regular operation, while a very small number of events
    belong to a fraudulent operation. In this case, you can also say that fraud is
    a rare event.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no strong rule for defining whether a dataset is unbalanced or not,
    it really depends on the context of your business domain. Most challenge problems
    will contain more than 99% of the observations in the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with unbalanced datasets is very simple: ML algorithms will try
    to find the best fit in the training data to maximize their accuracy. In a dataset
    where 99% of the cases belong to one single class, without any tuning, the algorithm
    is likely to prioritize the assertiveness of the majority class. In the worst-case
    scenario, it will classify all the observations as the majority class and ignore
    the minority one, which is usually our interest when modeling rare events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with unbalanced datasets, you have two major directions to follow:
    tuning the algorithm to handle the issue or resampling the data to make it more
    balanced.'
  prefs: []
  type: TYPE_NORMAL
- en: By tuning the algorithm, you have to specify the weight of each class under
    classification. This class weight configuration belongs to the algorithm, not
    to the training data, so it is a hyperparameter setting. It is important to keep
    in mind that not all algorithms will have that type of configuration, and that
    not all ML frameworks will expose it, either. As a quick reference, the `DecisionTreeClassifier`
    class, from the scikit-learn ML library, is a good example that does implement
    the class weight hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to work around unbalanced problems is changing the training dataset
    by applying **undersampling** or **oversampling**. If you decide to apply undersampling,
    all you have to do is remove some observations from the majority class until you
    get a more balanced dataset. Of course, the downside of this approach is that
    you may lose important information about the majority class that you are removing
    observations from.
  prefs: []
  type: TYPE_NORMAL
- en: The most common approach for undersampling is known as random undersampling,
    which is a naïve resampling approach where you randomly remove some observations
    from the training set.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, you can decide to go for oversampling, where you will create
    new observations/samples of the minority class. The simplest approach is the naïve
    one, where you randomly select observations from the training set (with replacement)
    for duplication. The downside of this method is the potential issue of overfitting,
    since you will be duplicating/highlighting the observed pattern of the minority
    class.
  prefs: []
  type: TYPE_NORMAL
- en: To either underfit or overfit your model, you should always test the fitted
    model on your testing set.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The testing set cannot be under/oversampled: only the training set should pass
    through these resampling techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also oversample the training set by applying synthetic sampling techniques.
    Random oversample does not add any new information to the training set: it just
    duplicates the existing ones. By creating synthetic samples, you are deriving
    those new observations from the existing ones (instead of simply copying them).
    This is a type of data augmentation technique known as the **Synthetic Minority
    Oversampling** **Technique (SMOTE).**'
  prefs: []
  type: TYPE_NORMAL
- en: Technically, what SMOTE does is plot a line in the feature space of the minority
    class and extract points that are close to that line.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'You may find questions in your exam where the term SMOTE has been used. If
    that happens, keep in mind the context where this term is applied: oversampling.'
  prefs: []
  type: TYPE_NORMAL
- en: Alright – in the next section, you will learn how to prepare text data for ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have already learned how to transform categorical features into numerical
    representations, either using label encoders, ordinal encoders, or one-hot encoding.
    However, what if you have fields containing long pieces of text in your dataset?
    How are you supposed to provide a mathematical representation for them in order
    to properly feed ML algorithms? This is a common issue in **Natural Language Processing
    (NLP),** a subfield of AI.
  prefs: []
  type: TYPE_NORMAL
- en: NLP models aim to extract knowledge from texts; for example, translating text
    between languages, identifying entities in a corpus of text (also known as **Name
    Entity Recognition**, or **NER** for short), classifying sentiments from a user
    review, and many other applications.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B21197_08.xhtml#_idTextAnchor1058)*, AWS Application Services
    for AI/ML*, you will learn about some AWS application services that apply NLP
    to their solutions, such as Amazon Translate and Amazon Comprehend. During the
    exam, you might be asked to think about the fastest or easiest way (with the least
    development effort) to build certain types of NLP applications. The fastest or
    easiest way is usually to use those out-of-the-box AWS services, since they offer
    pre-trained models for some use cases (especially machine translation, sentiment
    analysis, topic modeling, document classification, and entity recognition).
  prefs: []
  type: TYPE_NORMAL
- en: In a few chapters’ time, you will also learn about some built-in AWS algorithms
    for NLP applications, such as BlazingText, **Latent Dirichlet Allocation** (**LDA**),
    **Neural Topic Modeling** (**NTM**), and the **Sequence-to-Sequence** algorithm.
    Those algorithms also let you create the same NLP solutions that are created by
    those out-of-the-box services; however, you have to use them on SageMaker and
    write your own solution. In other words, they offer more flexibility but demand
    more development effort.
  prefs: []
  type: TYPE_NORMAL
- en: Keep that in mind for your exam!
  prefs: []
  type: TYPE_NORMAL
- en: Although AWS offers many out-of-the-box services and built-in algorithms that
    allow you to create NLP applications, you will not look at those AWS product features
    now (you will do in [*Chapter 6*](B21197_06.xhtml#_idTextAnchor708)*, Applying
    Machine Learning Algorithms*, and [*Chapter 8*](B21197_08.xhtml#_idTextAnchor1058)*,
    AWS Application Services for AI/ML*). You will finish this chapter by looking
    at some data preparation techniques that are extremely important for preparing
    your data for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first one you will learn is known as bag of words (BoW). This is a very
    common and simple technique, applied to text data, that creates matrix representations
    to describe the number of words within the text. BoW consists of two main steps:
    creating a vocabulary and creating a representation of the presence of those known
    words from the vocabulary in the text. These steps can be seen in *Figure 4**.11.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – BoW in action](img/B21197_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – BoW in action
  prefs: []
  type: TYPE_NORMAL
- en: First things first, you usually can’t use raw text to prepare a BoW representation.
    There is a data cleansing step where you lowercase the text; split each work into
    tokens; remove punctuation, non-alphabetical, and stop words; and, whenever necessary,
    apply any other custom cleansing techniques you may want.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have cleansed your raw text, you can add each word to a global vocabulary.
    Technically, this is usually a dictionary of tuples, in the form {(word, number
    of occurrences)} – for example, {(apple, 10), (watermelon, 20)}. As I mentioned
    previously, this is a global dictionary, and you should consider all the texts
    you are analyzing.
  prefs: []
  type: TYPE_NORMAL
- en: Now, with the cleansed text and updated vocabulary, you can build your text
    representation in the form of a matrix, where each column represents one word
    from the global vocabulary and each row represents a text you have analyzed. The
    way you represent those texts on each row may vary according to different strategies,
    such as binary, frequency, and count. Next, you will learn these strategies a
    little more.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.11*, a single piece of text is being processed with the three
    different strategies for BoW. That’s why you can see three rows on that table,
    instead of just one (in a real scenario, you have to choose one of them for implementation).
  prefs: []
  type: TYPE_NORMAL
- en: In the first row, it was used a binary strategy, which will assign 1 if the
    word exists in the global vocabulary and 0 if not. Because the vocabulary was
    built on a single text, all the words from that text belong to the vocabulary
    (the reason you can only see 1s in the binary strategy).
  prefs: []
  type: TYPE_NORMAL
- en: In the second row, it was used a frequency strategy, which will check the number
    of occurrences of each word within the text and divide it by the total number
    of words within the text. For example, the word “this” appears just once (1) and
    there are seven other words in the text (7), so 1/7 is equal to 0.14.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the third row, it was used a count strategy, which is a simple count
    of occurrences of each word within the text.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This note is really important – you are likely to find it in your exam. You
    may have noticed that the BoW matrix contains unique words in the *columns* and
    each text representation is in the *rows*. If you have 100 long pieces of text
    with only 50 unique words across them, your BoW matrix will have 50 columns and
    100 rows. During your exam, you are likely to receive a list of pieces of text
    and be asked to prepare the BoW matrix.
  prefs: []
  type: TYPE_NORMAL
- en: There is one more extremely important concept you should know about BoW, which
    is the n-gram configuration. The term n-gram is used to describe the way you would
    like to look at your vocabulary, either via single words (uni-gram), groups of
    two words (bi-gram), groups of three words (tri-gram), or even groups of *n* words
    (n-gram). So far, you have seen BoW representations using a uni-gram approach,
    but more sophisticated representations of BoW may use bi-grams, tri-grams, or
    n-grams.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main logic itself does not change, but you need to know how to represent
    n-grams in BoW. Still using the example from *Figure 4**.11*, a bi-gram approach
    would combine those words in the following way: [this movie, movie really, really
    good, good although, although old, old production]. Make sure you understand this
    before taking the exam.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The power and simplicity of BoW come from the fact that you can easily come
    up with a training set to train your algorithms. If you look at *Figure 4**.11*,
    can you see that having more data and just adding a classification column to that
    table, such as good or bad review, would allow us to train a binary classification
    model to predict sentiments?
  prefs: []
  type: TYPE_NORMAL
- en: Alright – you might have noticed that many of the awesome techniques that you
    have been introduced to come with some downsides. The problem with BoW is the
    challenge of maintaining its vocabulary. You can easily see that, in a huge corpus
    of texts, the vocabulary size tends to become bigger and bigger and the matrices’
    representations tend to be sparse (yes – the sparsity issue again).
  prefs: []
  type: TYPE_NORMAL
- en: One possible way to solve the vocabulary size issue is by using word hashing
    (also known in ML as the **hashing** **trick**). Hash functions map data of arbitrary
    sizes to data of a fixed size. This means you can use the hash trick to represent
    each text with a fixed number of features (regardless of the vocabulary’s size).
    Technically, this hashing space allows collisions (different texts represented
    by the same features), so this is something to take into account when you are
    implementing feature hashing.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another problem that comes with BoW, especially when you use the frequency strategy
    to build the feature space, is that more frequent words will strongly boost their
    scores due to the high number of occurrences within the document. It turns out
    that, often, those words with high occurrences are not the key words of the document,
    but just other words that *also* appear many times in several other documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Term Frequency-Inverse Document Frequency (TF-IDF)** helps penalize these
    types of words, by checking how frequent they are in other documents and using
    that information to rescale the frequency of the words within the document.'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the process, TF-IDF tends to give more importance to words that
    are unique to the document (document-specific words). Next, let’s look at a concrete
    example so that you can understand it in depth.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that you have a text corpus containing 100 words and the word “Amazon”
    appears three times. The Term Frequency (TF) of this word would be 3/100, which
    is equal to 0.03\. Now, suppose you have other 1,000 documents and that the word
    “Amazon” appears in 50 of these. In this case, the Inverse Document Frequency
    (IDF) would be given by the log as 1,000/50, which is equal to 1.30\. The TF-IDF
    score of the word “Amazon,” in that specific document under analysis, will be
    the product of TF * IDF, which is 0.03 * 1.30 (*0.039*).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that instead of 50 documents, the word “Amazon” had also appeared on
    another 750 documents – in other words, much more frequently than in the prior
    scenario. In this case, the TF part of this equation will not change – it is still
    0.03\. However, the IDF piece will change a little, since this time it will be
    log 1,000/750, which is equal to *0.0036*. As you can see, now the word “Amazon”
    has much less importance than in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike traditional approaches, such as BoW and TD-IDF, modern methods of text
    representation will take care of the context of the information, as well as the
    presence or frequency of words. One very popular and powerful approach that follows
    this concept is known as **word embedding.** Word embeddings create a dense vector
    of a fixed length that can store information about the context and meaning of
    the document.
  prefs: []
  type: TYPE_NORMAL
- en: Each word is represented by a data point in a multidimensional hyper plan, which
    is known as embedding space. This embedding space will have *n* dimensions, where
    each of these dimensions refers to a particular position of this dense vector.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may sound confusing, the concept is actually pretty simple. Suppose
    you have a list of four words, and you want to plot them in an embedding space
    of five dimensions. The words are king, queen, live, and castle. *Table 4.13*
    shows how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Dim 1** | **Dim 2** | **Dim 3** | **Dim 4** | **Dim 5** |'
  prefs: []
  type: TYPE_TB
- en: '| **King** | 0.22 | 0.76 | 0.77 | 0.44 | 0.33 |'
  prefs: []
  type: TYPE_TB
- en: '| **Queen** | 0.98 | 0.09 | 0.67 | 0.89 | 0.56 |'
  prefs: []
  type: TYPE_TB
- en: '| **Live** | 0.13 | 0.99 | 0.88 | 0.01 | 0.55 |'
  prefs: []
  type: TYPE_TB
- en: '| **Castle** | 0.01 | 0.89 | 0.34 | 0.02 | 0.90 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.13 – An embedding space representation
  prefs: []
  type: TYPE_NORMAL
- en: Forget the hypothetical numbers in *Table 4.13* and focus on the data structure;
    you will see that each word is now represented by *n* dimensions in the embedding
    space. This process of transforming words into vectors can be performed by many
    different methods, but the most popular ones are word2vec and GloVe.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have each word represented as a vector of a fixed length, you can apply
    many other techniques to do whatever you need. One very common task is plotting
    those “words” (actually, their dimensions) in a hyper plan and visually checking
    how close they are to each other!
  prefs: []
  type: TYPE_NORMAL
- en: Technically, you don’t use this to plot them as-is, since human brains cannot
    interpret more than three dimensions. Furthermore, you usually apply a dimensionality
    reduction technique (such as principal component analysis, which you will learn
    about later) to reduce the number of dimensions to two, and finally plot the words
    in a Cartesian plan. That’s why you might have seen pictures like the one at the
    bottom of *Table 4.15*. Have you ever asked yourself how it is possible to plot
    words on a graph?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Plotting words](img/B21197_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Plotting words
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how the five dimensions shown in *Figure 4**.12* were built.
    Again, there are different methods to do this, but you will learn the most popular,
    which uses a co-occurrence matrix with a fixed context window.
  prefs: []
  type: TYPE_NORMAL
- en: First, you have to come up with some logic to represent each word, keeping in
    mind that you also have to take their context into consideration. To solve the
    context requirement, you need to define a **fixed context window**, which is going
    to be responsible for specifying how many words will be grouped together for context
    learning. For instance, assume this fixed context window as 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will create a co-occurrence matrix, which will count the number of
    occurrences of each pair of words, according to the pre-defined context window.
    Consider the following text: “I will pass this exam, you will see. I will pass
    it.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The context window of the first word “pass” would be the ones in *bold*: “*I
    will* pass *this exam*, you will see. I will pass it.” Considering this logic,
    have a look at how many times each pair of words appears in the context window
    (*Figure 4**.13*).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Co-occurrence matrix](img/B21197_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Co-occurrence matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the pair of words “I will” appears three times when a context
    window of size 2 is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I will* pass this exam, you will see. I will pass it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will pass this exam, you *will* see. *I* will pass it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will pass this exam, you will see. *I will* pass it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at *Figure 4**.13*, the same logic should be applied to all other pairs
    of words, replacing “…” with the associated number of occurrences. You now have
    a numerical representation for each word!
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware that there are many alternatives to co-occurrence matrices
    with a fixed context window, such as using TD-IDF vectorization or even simpler
    counters of words per document. The most important message here is that, somehow,
    you must come up with a numerical representation for each word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is finally finding those dimensions shown in *Table 4.13*. You
    can do this by creating a multilayer model, usually based on neural networks,
    where the hidden layer will represent your embedding space. *Figure 4**.14* shows
    a simplified example where you could potentially compress those words shown in
    *Figure 4**.13* into an embedding space of five dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Building embedding spaces with neural networks](img/B21197_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Building embedding spaces with neural networks
  prefs: []
  type: TYPE_NORMAL
- en: You will learn about neural networks in more detail later in this book. For
    now, understanding where the embedding vector comes from is already an awesome
    achievement!
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing you should keep in mind while modeling natural language
    problems is that you can reuse a pre-trained embedding space in your models. Some
    companies have created modern neural network architectures, trained on billions
    of documents, which have become the state of the art in this field. For your reference,
    take a look at Bidirectional Encoder Representations from Transformers (BERT),
    which was proposed by Google and has been widely used by the data science community
    and industry.
  prefs: []
  type: TYPE_NORMAL
- en: You have now reached the end of this long – but very important – chapter about
    data preparation and transformation. Take this opportunity to do a quick recap
    of the awesome things you have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you were introduced to the different types of features that you might
    have to work with. Identifying the type of variable you’ll be working with is
    very important for defining the types of transformations and techniques that can
    be applied to each case.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned how to deal with categorical features. You saw that, sometimes,
    categorical variables do have an order (such as the ordinal ones), while other
    times, they don’t (such as the nominal ones). You learned that one-hot encoding
    (or dummy variables) is probably the most common type of transformation for nominal
    features; however, depending on the number of unique categories, after applying
    one-hot encoding, your data might suffer from sparsity issues. Regarding ordinal
    features, you shouldn’t create dummy variables on top of them, since you would
    be losing the information about the order that has been incorporated into the
    variable. In those cases, ordinal encoding is the most appropriate transformation.
  prefs: []
  type: TYPE_NORMAL
- en: You continued your journey by looking at numerical features, where you learned
    how to deal with continuous and discrete data. You walked through the most important
    types of transformations, such as normalization, standardization, binning, and
    discretization. You saw that some types of transformation rely on the underlying
    data to find their parameters, so it is very important to avoid using the testing
    set to learn anything from the data (it must strictly be used only for testing).
  prefs: []
  type: TYPE_NORMAL
- en: You have also seen that you can even apply pure math to transform your data;
    for example, you learned that power transformations can be used to reduce the
    skewness of your feature and make it more similar to a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: After that, you looked at missing data and got a sense of how important this
    task is. When you are modeling, you *can’t* look at the missing values as a simple
    computational problem, where you just have to replace *x* with *y*. This is a
    much bigger problem, and you need to start solving it by exploring your data and
    then checking whether your missing data was generated at random or not.
  prefs: []
  type: TYPE_NORMAL
- en: When you are making the decision to remove or replace missing data, you must
    be aware that you are either losing information or adding bias to the dataset,
    respectively. Remember to review all the important notes of this chapter, since
    they are likely to be relevant to your exam.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned about outlier detection. You looked at different ways to find
    outliers, such as the zscore and box plot approaches. Most importantly, you learned
    that you can either flag or smooth them.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning, you were advised that this chapter would be a long but worthwhile
    journey about data preparation. You have also learned how to deal with rare events,
    since this is one of the most challenging problems in ML. Now you are aware that,
    sometimes, your data might be unbalanced, and you must either trick your algorithm
    (by changing the class weights) or resample your data (applying undersampling
    or oversampling).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to deal with text data for NLP. You should now be able
    to manually compute BoW and TF-IDF matrices! You went even deeper and learned
    how word embedding works. During this subsection, you learned that you can either
    create your own embedding space (using many different methods) or reuse a pre-trained
    one, such as BERT.
  prefs: []
  type: TYPE_NORMAL
- en: You are done! In the next chapter, you will dive into data visualization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH04](https://packt.link/MLSC01E2_CH04).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 4**.15*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 4**.16*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Chapter Review Questions for Chapter 4](img/B21197_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Chapter Review Questions for Chapter 4
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 4.14 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
