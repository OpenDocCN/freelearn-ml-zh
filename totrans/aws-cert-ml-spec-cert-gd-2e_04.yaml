- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Data Preparation and Transformation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备和转换
- en: You have probably heard that data scientists spend most of their time working
    on data-preparation-related activities. It is now time to explain why that happens
    and what types of activities they work on.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过数据科学家大部分时间都在从事与数据准备相关的活动。现在是时候解释为什么会这样，以及他们从事的活动类型了。
- en: In this chapter, you will learn how to deal with categorical and numerical features,
    as well as how to apply different techniques to transform your data, such as one-hot
    encoding, binary encoders, ordinal encoding, binning, and text transformations.
    You will also learn how to handle missing values and outliers in your data, which
    are two important tasks you can implement to build good **machine learning** (**ML**)
    models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何处理类别和数值特征，以及如何应用不同的技术来转换你的数据，例如独热编码、二进制编码器、序数编码、分箱和文本转换。你还将学习如何处理数据中的缺失值和异常值，这两个重要任务可以帮助你构建好的
    **机器学习**（**ML**）模型。
- en: 'In this chapter, you will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将涵盖以下主题：
- en: Identifying types of features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别特征类型
- en: Dealing with categorical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理类别特征
- en: Dealing with numerical features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数值特征
- en: Understanding data distributions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据分布
- en: Handling missing values
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Dealing with outliers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Dealing with unbalanced datasets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: Dealing with text data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: This chapter is a little longer than the others and will require more patience.
    Knowing about these topics in detail will put you in a good position for the AWS
    Machine Learning Specialty exam.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章比其他章节要长一些，需要更多的耐心。详细了解这些主题将使你在 AWS 机器学习专业考试中处于有利位置。
- en: Identifying types of features
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别特征类型
- en: 'You *cannot* start modeling without knowing what a feature is and what type
    of information it can store. You have already read about the different processes
    that deal with features. For example, you know that feature engineering is related
    to the task of building and preparing features for your models; you also know
    that feature selection is related to the task of choosing the best set of features
    to feed a particular algorithm. These two tasks have one behavior in common: they
    may vary according to the types of features they are processing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道特征是什么以及它可以存储什么类型的信息之前，你无法开始建模。你已经阅读了关于处理特征的各个过程的内容。例如，你知道特征工程与为你的模型构建和准备特征的任务相关；你也知道特征选择与为特定算法选择最佳特征集的任务相关。这两个任务有一个共同的行为：它们可能根据它们处理的特征类型而变化。
- en: It is very important to understand this behavior (feature type versus applicable
    transformations) because it will help you eliminate invalid answers during your
    exam (and, most importantly, you will become a better data scientist).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这种行为（特征类型与适用转换）非常重要，因为它将帮助你消除考试中的无效答案（最重要的是，你将成为一名更好的数据科学家）。
- en: By *types of features*, you refer to the data type that a particular feature
    is supposed to store. *Figure 4**.1* shows how you could potentially describe
    the different types of features of a model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *特征类型*，你指的是特定特征应该存储的数据类型。*图 4.1* 展示了如何潜在地描述模型的不同特征类型。
- en: '![Figure 4.1 – Feature types](img/B21197_04_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 特征类型](img/B21197_04_01.jpg)'
- en: Figure 4.1 – Feature types
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 特征类型
- en: 'In [*Chapter 1*](B21197_01.xhtml#_idTextAnchor018)*, Machine Learning Fundamentals*,
    you were introduced to the feature classification shown in *Figure 4**.1*. Now,
    look at some real examples, so that you can eliminate any remaining questions
    you may have:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 1 章*](B21197_01.xhtml#_idTextAnchor018)*，机器学习基础* 中，你被介绍了 *图 4.1* 中显示的特征分类。现在，看看一些真实示例，这样你可以消除你可能有的任何疑问：
- en: '| **Feature type** | **Feature sub-type** | **Definition** | **Example** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **特征类型** | **特征子类型** | **定义** | **示例** |'
- en: '| **Categorical** | Nominal | Labelled variables with no quantitative value
    | Cloud provider: AWS, MS, Google |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **名义** | 无定量值的标记变量 | 云服务提供商：AWS、MS、Google |'
- en: '| **Categorical** | Ordinal | Adds the sense of order to the labelled variable
    | Job title: junior data scientist, senior data scientist, chief data scientist
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **序数** | 为标记变量添加顺序感 | 职位：初级数据科学家，高级数据科学家，首席数据科学家 |'
- en: '| **Categorical** | Binary | A variable with only two allowed values | Fraud
    classification: fraud, not fraud |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **二元** | 只允许两个值的变量 | 欺诈分类：欺诈，非欺诈 |'
- en: '| **Numerical** | Discrete | Individual and countable items | Number of students:
    100 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **数值** | 离散 | 单个和可数的项目 | 学生人数：100 |'
- en: '| **Numerical** | Continuous | Infinite number of possible measurements and
    they often carry decimal points | Total amount: $150.35 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **数值** | 连续 | 有无限多的可能测量值，并且它们通常带有小数点 | 总金额：$150.35 |'
- en: Table 4.1 – Real examples of feature values
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 – 特征值的真实示例
- en: 'Although looking at the values of the variable may help you find its type,
    you should never rely only on this approach. The nature of the variable is also
    very important for making such decisions. For example, someone could encode the
    cloud provider variable shown in *Table 4.1* as follows: 1 (AWS), 2 (MS), 3 (Google).
    In that case, the variable is still a nominal feature, even if it is now represented
    by discrete numbers.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然查看变量的值可能有助于您找到其类型，但您绝不应该只依赖这种方法。变量的性质对于做出此类决定也非常重要。例如，有人可以将*表4.1*中显示的云服务提供商变量编码如下：1（AWS），2（MS），3（Google）。在这种情况下，即使现在由离散数字表示，该变量仍然是一个名义特征。
- en: If you are building an ML model and you don’t tell your algorithm that this
    variable is not a discrete number but is instead a nominal variable, the algorithm
    will treat it as a number and the model won’t be interpretable anymore.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在构建机器学习模型，并且没有告诉您的算法这个变量不是一个离散数字，而是一个名义变量，那么算法将把它当作一个数字处理，模型将不再可解释。
- en: Important note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before feeding any ML algorithm with data, make sure your feature types have
    been properly identified.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在向任何机器学习算法提供数据之前，请确保您的特征类型已经被正确识别。
- en: 'In theory, if you are happy with your features and have properly classified
    each of them, you should be ready to go into the modeling phase of the CRISP-DM
    methodology, shouldn’t you? Well, maybe not. There are many reasons you may want
    to spend a little more time on data preparation, even after you have correctly
    classified your features:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，如果您对自己的特征满意并且已经正确地分类了每个特征，您应该准备好进入CRISP-DM方法的建模阶段，不是吗？好吧，也许不是。即使您已经正确地分类了特征，您可能还有许多原因想要在数据准备上花更多的时间：
- en: Some ML libraries, such as `scikit-learn`, may not accept string values on your
    categorical features.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些机器学习库，如`scikit-learn`，可能不接受分类特征上的字符串值。
- en: The data distribution of your variable may not be the most optimal distribution
    for your algorithm.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您变量的数据分布可能不是您算法的最优分布。
- en: Your ML algorithm may be impacted by the scale of your data.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的机器学习算法可能会受到您数据规模的影响。
- en: Some observations of your variable may be missing information and you will have
    to fix it. These are also known as missing values.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您变量的某些观察结果可能缺少信息，您将不得不修复它们。这些也被称为缺失值。
- en: You may find outlier values of your variable that can potentially add bias to
    your model.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能会发现变量中的异常值，这些值可能会给您的模型带来潜在的偏差。
- en: Your variable may be storing different types of information and you may only
    be interested in a few of them (for example, a date variable can store the day
    of the week or the week of the month).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的变量可能存储着不同类型的信息，而您可能只对其中的一些感兴趣（例如，日期变量可以存储星期几或月份的星期）。
- en: You might want to find a mathematical representation for a text variable.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能想要为文本变量找到一个数学表示。
- en: Believe me, this list has no real end!
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相信我，这个列表没有真正的尽头！
- en: In the following sections, you will understand how to address all these points,
    starting with categorical features.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将了解如何解决所有这些问题，从分类特征开始。
- en: Dealing with categorical features
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分类特征
- en: Data transformation methods for categorical features will vary according to
    the sub-type of your variable. In the upcoming sections, you will understand how
    to transform nominal and ordinal features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类特征的数值转换方法将根据您变量的子类型而有所不同。在接下来的章节中，您将了解如何转换名义和有序特征。
- en: Transforming nominal features
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换名义特征
- en: You may have to create numerical representations of your categorical features
    before applying ML algorithms to them. Some libraries may have embedded logic
    to handle that transformation for you, but most of them do not.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用机器学习算法之前，您可能需要创建分类特征的数值表示。一些库可能已经内置了处理这种转换的逻辑，但大多数都没有。
- en: 'The first transformation you will learn is known as label encoding. A label
    encoder is suitable for categorical/nominal variables, and it will just associate
    a number with each distinct label of your variables. *Table 4.2* shows how a label
    encoder works:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习的第一个转换称为标签编码。标签编码器适用于分类/名义变量，它将为你的变量的每个不同标签分配一个数字。*表 4.2* 展示了标签编码器是如何工作的：
- en: '| **Countr****y** | **Label encoding** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **国家** | **标签编码** |'
- en: '| India | 1 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 |'
- en: '| Canada | 2 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 加拿大 | 2 |'
- en: '| Brazil | 3 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 巴西 | 3 |'
- en: '| Australia | 4 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 4 |'
- en: '| India | 1 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 |'
- en: Table 4.2 – Label encoder in action
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2 – 标签编码器应用示例
- en: A label encoder will always ensure that a unique number is associated with each
    distinct label. In the preceding table, although “India” appears twice, the same
    number was assigned to it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码器将始终确保每个不同的标签都关联一个唯一的数字。在上面的表中，尽管“印度”出现了两次，但它被分配了相同的数字。
- en: You now have a numerical representation of each country, but this does not mean
    you can use that numerical representation in your models! In this particular case,
    you are transforming a nominal feature, *which does not have* *an order*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在有了每个国家的数值表示，但这并不意味着你可以在模型中使用这种数值表示！在这个特定的情况下，你正在转换一个没有*顺序*的命名特征。
- en: According to *Table 4.2*, if you pass the encoded version of the *country* variable
    to a model, it will make assumptions such as “Brazil (3) is greater than Canada
    (2),” which does not make any sense.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表 4.2，如果你将*国家*变量的编码版本传递给模型，它将做出诸如“巴西（3）大于加拿大（2）”这样的假设，这是没有意义的。
- en: 'One possible solution for that scenario is applying another type of transformation
    on top of “*country”*: **one-hot encoding**. This transformation will represent
    all the categories from the original feature as individual features (also known
    as dummy **variables**), which will store the presence or absence of each category.
    *Table 4.3* is transforming the same information from *Table 4.2*, but this time
    it’s applying one-hot encoding:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这种情况的一个可能解决方案是在“*国家*”之上应用另一种类型的转换：**独热编码**。这种转换将表示原始特征中的所有类别作为单独的特征（也称为虚拟**变量**），这将存储每个类别的存在或不存在。*表
    4.3* 将转换与*表 4.2* 相同的信息，但这次它应用了独热编码：
- en: '| **Country** | **India** | **Canada** | **Brazil** | **Australia** |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **国家** | **印度** | **加拿大** | **巴西** | **澳大利亚** |'
- en: '| India | 1 | 0 | 0 | 0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 | 0 | 0 | 0 |'
- en: '| Canada | 0 | 1 | 0 | 0 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 加拿大 | 0 | 1 | 0 | 0 |'
- en: '| Brazil | 0 | 0 | 1 | 0 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 巴西 | 0 | 0 | 1 | 0 |'
- en: '| Australia | 0 | 0 | 0 | 1 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 0 | 0 | 0 | 1 |'
- en: '| India | 1 | 0 | 0 | 0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 | 0 | 0 | 0 |'
- en: Table 4.3 – One-hot encoding in action
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.3 – 独热编码应用示例
- en: You can now use the one-hot encoded version of the *country* variable as a feature
    of an ML model. However, your work as a skeptical data scientist is never done,
    and your critical thinking ability will be tested in the AWS Machine Learning
    Specialty exam.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用*国家*变量的独热编码版本作为机器学习模型的特征。然而，作为一名持怀疑态度的数据科学家，你的工作永远不会结束，你的批判性思维能力将在 AWS
    机器学习专业考试中得到考验。
- en: 'Suppose you have 150 distinct countries in your dataset. How many dummy variables
    would you come up with? 150, right? Here, you just ran into a potential issue:
    apart from adding complexity to your model (which is not a desired characteristic
    of any model at all), dummy variables also add **sparsity** to your data.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的数据集中有 150 个不同的国家。你会想出多少个虚拟变量？150 个，对吧？在这里，你遇到了一个潜在的问题：除了增加模型的复杂性（这绝不是任何模型所希望的特性）之外，虚拟变量还会给你的数据增加**稀疏性**。
- en: A sparse dataset has a lot of variables filled with zeros. Often, it is hard
    to fit this type of data structure into memory (you can easily run out of memory),
    and it is very time-consuming for ML algorithms to process sparse structures.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据集有很多变量填充了零。通常，很难将这种数据结构拟合到内存中（你很容易耗尽内存），并且对于机器学习算法来说，处理稀疏结构非常耗时。
- en: You can work around the sparsity problem by grouping your original data and
    reducing the number of categories, and you can even use custom libraries that
    compress your sparse data and make it easier to manipulate (such as `scipy.sparse.csr_matrix`,
    from Python).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过对原始数据进行分组并减少类别数量来解决稀疏问题，甚至可以使用自定义库压缩你的稀疏数据，使其更容易操作（例如 Python 中的 `scipy.sparse.csr_matrix`）。
- en: Therefore, during the exam, remember that one-hot encoding is definitely the
    right way to go when you need to transform categorical/nominal data to feed ML
    models; however, take the number of unique categories of your original feature
    into account and think about whether it makes sense to create dummy variables
    for all of them (it might not make sense if you have a very large number of unique
    categories).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在考试期间，请记住，当你需要将分类/名义数据转换为机器学习模型输入时，独热编码绝对是正确的选择；然而，要考虑你原始特征的唯一类别数量，并思考是否为所有这些类别创建虚拟变量是否有意义（如果你有非常多的唯一类别，这可能没有意义）。
- en: Applying binary encoding
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用二进制编码
- en: For those types of variables with a higher number of unique categories, a potential
    approach to creating a numerical representation for them is applying binary encoding.
    In this approach, the goal is transforming a categorical variable into multiple
    binary columns, but minimizing the number of new columns.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有更多唯一类别的变量类型，创建它们的数值表示的一个潜在方法是应用二进制编码。在这种方法中，目标是转换一个分类变量为多个二进制列，但最小化新列的数量。
- en: 'This process consists of three basic steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程包括三个基本步骤：
- en: The categorical data is converted into numerical data after being passed through
    an ordinal encoder.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在通过顺序编码器处理后，分类数据被转换为数值数据。
- en: The resulting number is then converted into a binary value.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将得到的数字转换为二进制值。
- en: The binary value is split into different columns.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二进制值被分割成不同的列。
- en: '*Table 4.4* shows how to convert the data from *Table 4.2* into a binary variable.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*表4.4*展示了如何将*表4.2*中的数据转换为二进制变量。'
- en: '| **Country** | **Label encoder** | **Binary** | **Col1** | **Col2** | **Col3**
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **国家** | **标签编码器** | **二进制** | **Col1** | **Col2** | **Col3** |'
- en: '| India | 1 | 001 | 0 | 0 | 1 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 | 001 | 0 | 0 | 1 |'
- en: '| Canada | 2 | 010 | 0 | 1 | 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 加拿大 | 2 | 010 | 0 | 1 | 0 |'
- en: '| Brazil | 3 | 011 | 0 | 1 | 1 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 巴西 | 3 | 011 | 0 | 1 | 1 |'
- en: '| Australia | 4 | 100 | 1 | 0 | 0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 4 | 100 | 1 | 0 | 0 |'
- en: '| India | 1 | 001 | 0 | 0 | 1 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 | 001 | 0 | 0 | 1 |'
- en: Table 4.4 – Binary encoding in action
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.4 – 二进制编码的实际应用
- en: As you can see, there are now three columns (Col1, Col2, and Col3) instead of
    four columns from the one-hot encoding transformation in *Table 4.3*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，现在有三列（Col1、Col2和Col3），而不是像*表4.3*中的独热编码转换那样有四列。
- en: Transforming ordinal features
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换顺序特征
- en: 'Ordinal features have a very specific characteristic: *they have an order*.
    Because they have this quality, it does *not* make sense to apply one-hot encoding
    to them; if you do so, the underlying algorithm that is used to train your model
    will not be able to differentiate the implicit order of the data points associated
    with this feature.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序特征具有一个非常特定的特征：*它们有顺序*。因为它们具有这种特性，所以对它们应用独热编码是没有意义的；如果你这样做，用于训练你的模型的底层算法将无法区分与该特征相关的数据点的隐含顺序。
- en: 'The most common transformation for this type of variable is known as **ordinal**
    **encoding**. An ordinal encoder will associate a number with each distinct label
    of your variable, just like a label encoder does, but this time, it will respect
    the order of each category. The following table shows how an ordinal encoder works:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型变量的最常见转换方式被称为**顺序****编码**。顺序编码器将每个变量的不同标签与一个数字关联起来，就像标签编码器一样，但这次，它将尊重每个类别的顺序。以下表格展示了顺序编码器是如何工作的：
- en: '| **Education** | **Ordinal encoding** |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **教育** | **顺序编码** |'
- en: '| Trainee | 1 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 训练生 | 1 |'
- en: '| Junior data analyst | 2 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 初级数据分析师 | 2 |'
- en: '| Senior data analyst | 3 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 高级数据分析师 | 3 |'
- en: '| Chief data scientist | 4 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 首席数据科学家 | 4 |'
- en: Table 4.5 – Ordinal encoding in action
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.5 – 顺序编码的实际应用
- en: You can now pass the encoded variable to ML models and they will be able to
    handle this variable properly, with no need to apply one-hot encoding transformations.
    This time, comparisons such as “senior data analyst is greater than junior data
    analyst” make total sense.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将编码后的变量传递给机器学习模型，它们将能够正确处理这个变量，无需应用独热编码转换。这次，“高级数据分析师大于初级数据分析师”这样的比较是完全有意义的。
- en: Avoiding confusion in our train and test datasets
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免在训练和测试数据集中产生混淆
- en: 'Do not forget the following statement: encoders are fitted on training data
    and transformed on test and production data. This is how your ML pipeline should
    work.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记以下声明：编码器在训练数据上拟合，并在测试和生产数据上转换。这就是你的机器学习管道应该工作的方式。
- en: Suppose you have created a one-hot encoder that fits the data from *Table 4.2*
    and returns data according to *Table 4.3*. In this example, assume this is your
    training data. Once you have completed your training process, you may want to
    apply the same one-hot encoding transformation to your testing data to check the
    model’s results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经创建了一个适合*表4.2*中数据的one-hot编码器，并返回根据*表4.3*的数据。在这个例子中，假设这是你的训练数据。一旦你完成了训练过程，你可能想要将相同的one-hot编码转换应用到你的测试数据上，以检查模型的结果。
- en: In the scenario that was just described (which is a very common situation in
    modeling pipelines), you *cannot* retrain your encoder on top of the testing data!
    You should just reuse the previous encoder object that you have created on top
    of the training data. Technically, you shouldn’t use the `fit` method again but
    the `transform` method instead.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚才描述的场景中（这在建模管道中是一个非常常见的情况），你*不能*在测试数据上重新训练你的编码器！你应该只是重新使用你在训练数据上创建的之前的编码器对象。技术上，你不应该再次使用`fit`方法，而应该使用`transform`方法。
- en: 'You may already know the reasons why you should follow this rule, but just
    as a reminder: the testing data was created to extract the performance metrics
    of your model, so you should not use it to extract any other knowledge. If you
    do so, your performance metrics will be biased by the testing data, and you cannot
    infer that the same performance (shown in the test data) is likely to happen in
    production (when new data will come in).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道为什么你应该遵循这条规则的原因，但只是为了提醒一下：测试数据是为了提取你模型性能指标而创建的，因此你不应该用它来提取任何其他知识。如果你这样做，你的性能指标将会受到测试数据的偏差，你也不能推断出相同的性能（在测试数据中显示）在生产环境中（当新数据到来时）很可能会发生。
- en: Alright, all good so far. However, what if your testing set has a new category
    that was not present in the training set? How are you supposed to transform this
    data?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，到目前为止一切都很顺利。然而，如果你的测试集有一个在训练集中没有的新类别，你该如何转换这些数据呢？
- en: 'Going back to the one-hot encoding example in *Figure 4**.3* (input data) and
    *Table 4.3* (output data), this encoder knows how to transform the following countries:
    Australia, Brazil, Canada, and India. If you had a different country in the testing
    set, the encoder would not know how to transform it, and that’s why you need to
    define how it will behave in scenarios where there are exceptions.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 回到*图4**.3*（输入数据）和*表4.3*（输出数据）中的one-hot编码示例。这个编码器知道如何转换以下国家：澳大利亚、巴西、加拿大和印度。如果你在测试集中有不同国家，编码器将不知道如何转换它，这就是为什么你需要定义它在有异常情况下的行为方式。
- en: Most ML libraries provide specific parameters for these situations. In the previous
    example, you could configure the encoder to either raise an error or set all zeros
    on our dummy variables, as shown in *Table 4.6*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习库为这些情况提供了特定的参数。在先前的例子中，你可以配置编码器在虚拟变量上要么引发错误，要么设置所有为零，如*表4.6*所示。
- en: '| **Country** | **India** | **Canada** | **Brazil** | **Australia** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **国家** | **印度** | **加拿大** | **巴西** | **澳大利亚** |'
- en: '| India | 1 | 0 | 0 | 0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 | 0 | 0 | 0 |'
- en: '| Canada | 0 | 1 | 0 | 0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 加拿大 | 0 | 1 | 0 | 0 |'
- en: '| Brazil | 0 | 0 | 1 | 0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 巴西 | 0 | 0 | 1 | 0 |'
- en: '| Australia | 0 | 0 | 0 | 1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 0 | 0 | 0 | 1 |'
- en: '| India | 1 | 0 | 0 | 0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 1 | 0 | 0 | 0 |'
- en: '| **Portugal** | **0** | **0** | **0** | **0** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **葡萄牙** | **0** | **0** | **0** | **0** |'
- en: Table 4.6 – Handling unknown values on one-hot encoding transformations
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.6 – 在one-hot编码转换中处理未知值
- en: As you can see, Portugal was not present in the training set (*Table 4.2*),
    so during the transformation, it will keep the same list of known countries and
    say that Portugal *is not* among them (all zeros).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，葡萄牙没有出现在训练集中（*表4.2*），所以在转换过程中，它将保持已知国家的相同列表，并说葡萄牙*不在*其中（所有为零）。
- en: As the very good skeptical data scientist you are becoming, should you be concerned
    about the fact that you have a particular category that has not been used during
    training? Well, maybe. This type of analysis really depends on your problem domain.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为你正在成为的非常优秀的怀疑性数据科学家，你应该担心这样一个事实，即你有一个在训练过程中没有使用过的特定类别吗？好吧，也许吧。这种分析类型真的取决于你的问题领域。
- en: Handling unknown values is very common and something that you should expect
    to do in your ML pipeline. However, you should also ask yourself, due to the fact
    that you did not use that particular category during your training process, whether
    your model can be extrapolated and generalized.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 处理未知值是非常常见的事情，也是你在你的机器学习管道中应该预期要做的事情。然而，你也应该问自己，由于你在训练过程中没有使用那个特定类别，你的模型是否可以被外推和推广。
- en: Remember, your testing data must follow the same data distribution as your training
    data, and you are very likely to find all (or at least most) of the categories
    (of a categorical feature) either in the training or testing sets. Furthermore,
    if you are facing overfitting issues (doing well in the training, but poorly in
    the testing set) and, at the same time, you realize that your categorical encoders
    are transforming a lot of unknown values in the test set, guess what? It’s likely
    that your training and testing samples are not following the same distribution,
    invalidating your model entirely.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你的测试数据必须遵循与训练数据相同的数据分布，你很可能在训练或测试集中找到所有（或至少大多数）分类特征（的类别）。此外，如果你面临过拟合问题（在训练中表现良好，但在测试集中表现不佳），同时，你意识到你的分类编码器在测试集中转换了大量的未知值，猜猜看？很可能你的训练和测试样本没有遵循相同的分布，这完全无效化了你的模型。
- en: As you can see, slowly, you are getting there. You are learning about bias and
    investigation strategies in fine-grained detail – that is so exciting! Now, move
    on and look at performing transformations on numerical features. Yes, each type
    of data matters and drives your decisions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，你正在慢慢接近目标。你正在学习关于偏差和调查策略的精细细节——这真是太令人兴奋了！现在，继续前进，看看对数值特征进行转换。是的，每种类型的数据都很重要，并驱动着你的决策。
- en: Dealing with numerical features
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数值特征
- en: In terms of numerical features (discrete and continuous), you can think of transformations
    that rely on the training data and others that rely purely on the (individual)
    observation being transformed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值特征（离散和连续）方面，你可以考虑依赖于训练数据的转换以及其他仅依赖于（单个）被转换观察的转换。
- en: Those who rely on the training data will use the training set to learn the necessary
    parameters during `fit`, and then use them to transform any test or new data.
    The logic is pretty much the same as what you just learned for categorical features;
    however, this time, the encoder will learn different parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于训练数据的那些将使用训练集在`fit`过程中学习必要的参数，然后使用这些参数来转换任何测试或新数据。逻辑基本上与你刚刚学习用于分类特征的逻辑相同；然而，这次，编码器将学习不同的参数。
- en: On the other hand, those that rely purely on (individual) observations do not
    depend on training or testing sets. They will simply perform a mathematical computation
    on top of an individual value. For example, you could apply an exponential transformation
    to a particular variable by squaring its value. There is no dependency on learned
    parameters from anywhere – just get the value and square it.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，那些仅依赖于（单个）观察的转换不依赖于训练或测试集。它们将简单地在一个单个值上执行数学计算。例如，你可以通过平方其值来对一个特定变量应用指数转换。这里没有依赖于任何地方学习到的参数——只需获取值并平方它。
- en: 'At this point, you might be thinking about dozens of available transformations
    for numerical features! Indeed, there are so many options, and you will not learn
    all of them here. However, you are not supposed to know all of them for the AWS
    Machine Learning Specialty exam. You will learn the most important ones (for the
    exam), but you should not limit your modeling skills: take a moment to think about
    the unlimited options you have by creating custom transformations according to
    your use case.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经在想数值特征的数十种可用转换了！确实，有这么多选项，你在这里不会学习所有这些。然而，你不需要为了AWS机器学习专业考试而了解所有这些。你将学习最重要的那些（用于考试），但你不应限制你的建模技能：花点时间思考一下，根据你的用例创建自定义转换时，你拥有的无限选项。
- en: Data normalization
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Applying data **normalization** means changing the scale of the data. For example,
    your feature may store employee salaries that range between 20,000 and 200,000
    dollars/year and you want to put this data in the range of 0 and 1; where 20,000
    (the minimum observed value) will be transformed as 0; and 200,000 (the maximum
    observed value) will be transformed as 1.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数据**标准化**意味着改变数据的尺度。例如，你的特征可能存储员工年薪，范围在每年20,000到200,000美元之间，而你希望将此数据放入0到1的范围内；其中20,000（观察到的最小值）将被转换为0；而200,000（观察到的最大值）将被转换为1。
- en: This type of technique is especially important when you want to fit your training
    data on top of certain types of algorithms that are impacted by the scale/magnitude
    of the underlying data. For instance, you can think about those algorithms that
    use the dot product of the input variables (such as neural networks or linear
    regression) and those algorithms that rely on distance measures (such as k-**nearest
    neighbors (KNN)** or **k-means**).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要将训练数据拟合到受底层数据规模/幅度影响的特定类型的算法上时，这种技术尤为重要。例如，你可以考虑那些使用输入变量的点积（如神经网络或线性回归）的算法，以及那些依赖于距离度量的算法（如k-**最近邻（KNN）**或**k-means**）。
- en: On the other hand, applying data normalization will not result in performance
    improvements for rule-based algorithms, such as decision trees, since they will
    be able to check the predictive power of the features (either via entropy or information
    gain analysis), regardless of the scale of the data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，应用数据归一化对于基于规则的算法，如决策树，不会导致性能提升，因为它们将能够检查特征（无论是通过熵或信息增益分析）的预测能力，而不管数据的规模如何。
- en: Important note
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You will learn about these algorithms, along with the appropriate details, in
    the later chapters of this book. For instance, you can look at entropy and information
    gain as two types of metrics used by decision trees to check feature importance.
    Knowing the predictive power of each feature helps the algorithm define the optimal
    root, intermediaries, and leaf nodes of the tree.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本书的后续章节中了解这些算法，以及相关的详细信息。例如，你可以将熵和信息增益视为决策树用来检查特征重要性的两种度量指标。了解每个特征的预测能力有助于算法定义树的根节点、中间节点和叶节点。
- en: Take a moment and use the following example to understand why data normalization
    will help those types of algorithms. You already know that the goal of a clustering
    algorithm is to find groups or clusters in your data, and one of the most used
    clustering algorithms is known as k-means.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请花点时间，使用以下示例来理解为什么数据归一化将有助于那些类型的算法。你已经知道聚类算法的目的是在你的数据中找到组或集群，而最常用的聚类算法之一就是k-means。
- en: '*Figure 4**.2* shows how different scales of the variable could change the
    hyper plan’s projection of k-means clustering:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.2* 展示了不同尺度的变量如何改变k-means聚类的超平面投影：'
- en: '![Figure 4.2 – Plotting data of different scales in a hyper plan](img/B21197_04_02.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 在超平面上绘制不同尺度的数据](img/B21197_04_02.jpg)'
- en: Figure 4.2 – Plotting data of different scales in a hyper plan
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 在超平面上绘制不同尺度的数据
- en: On the left-hand side of *Figure 4**.2*, you can see a single data point plotted
    in a hyper plan that has three dimensions (x, y, and z). All three dimensions
    (also known as features) were normalized to the scale of 0 and 1\. On the right-hand
    side, you can see the same data point, but this time, the x dimension was *not*
    normalized. You can clearly see that the hyper plan has changed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图4.2* 的左侧，你可以看到一个数据点在一个三维超平面上绘制（x，y，z）。所有三个维度（也称为特征）都被归一化到0和1的规模。在右侧，你可以看到相同的数据点，但这次x维度没有被归一化。你可以清楚地看到超平面已经改变。
- en: In a real scenario, you would have far more dimensions and data points. The
    difference in the scale of the data would change the centroids of each cluster
    and could potentially change the assigned clusters of some points. This same problem
    will happen on other algorithms that rely on distance calculation, such as KNN.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，你会有更多的维度和数据点。数据规模的不同会改变每个集群的中心点，并可能改变某些点的分配集群。同样的问题也会出现在依赖于距离计算的算法上，如KNN。
- en: Other algorithms, such as neural networks and linear regression, will compute
    weighted sums using your input data. Usually, these types of algorithms will perform
    operations such as *W1*X1 + W2*X2 + Wi*Xi*, where *Xi* and *Wi* refer to a particular
    feature value and its weight, respectively. Again, you will learn details of neural
    networks and linear models later, but can you see the data scaling problem by
    just looking at the calculations that were just described? It can easily come
    up with very large values if *X* (feature) and *W* (weight) are large numbers.
    That will make the algorithm’s optimizations much more complex. In neural networks,
    this problem is known as gradient exploding.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法，如神经网络和线性回归，将使用你的输入数据进行加权求和。通常，这些类型的算法将执行如 *W1*X1 + W2*X2 + Wi*Xi* 的操作，其中
    *Xi* 和 *Wi* 分别代表特定的特征值及其权重。同样，你将在稍后学习神经网络和线性模型的细节，但你能仅通过查看刚才描述的计算来看到数据缩放问题吗？如果
    *X*（特征）和 *W*（权重）是很大的数字，它很容易产生非常大的值。这将使算法的优化变得更加复杂。在神经网络中，这个问题被称为梯度爆炸。
- en: You now have a very good understanding of the reasons you should apply data
    normalization (and when you should not). Data normalization is often implemented
    in ML libraries as **Min Max Scaler**. If you find this term in the exam, then
    remember that it is the same as data normalization.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对应该应用数据归一化的原因（以及何时不应应用）有了非常好的理解。数据归一化通常在机器学习库中作为 **Min Max Scaler** 实现。如果在考试中遇到这个术语，请记住它与数据归一化是相同的。
- en: Additionally, data normalization does not necessarily need to transform your
    feature into a range between 0 and 1\. In reality, you can transform the feature
    into any range you want. *Figure 4**.3* shows how normalization is formally defined.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据归一化不一定需要将你的特征值转换到0到1的范围内。实际上，你可以将其转换到你想要的任何范围内。*图4.3* 展示了归一化是如何正式定义的。
- en: '![Figure 4.3 – Normalization formula](img/B21197_04_03.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 归一化公式](img/B21197_04_03.jpg)'
- en: Figure 4.3 – Normalization formula
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 归一化公式
- en: Here, *Xmin* and *Xmax* are the lower and upper values of the range; *X* is
    the value of the feature. Apart from data normalization, there is another very
    important technique regarding numerical transformations that you *must* be aware
    of, not only for the exam but also for your data science career. You’ll look at
    this in the next section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*Xmin* 和 *Xmax* 是范围的上下限值；*X* 是特征的值。除了数据归一化之外，还有另一种关于数值转换的非常重要的技术，你必须意识到，这不仅是为了考试，也是为了你的数据科学职业生涯。你将在下一节中了解这一点。
- en: Data standardization
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: 'Data standardization is another scaling method that transforms the distribution
    of the data, so that the mean will become 0 and the standard deviation will become
    1\. *Figure 4**.4* formally describes this scaling technique, where *X* represents
    the value to be transformed, *µ* refers to the mean of *X*, and *σ* is the standard
    deviation of *X*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化是另一种缩放方法，它将数据的分布转换为均值变为0，标准差变为1。*图4.4* 正式描述了这种缩放技术，其中 *X* 代表要转换的值，*µ* 指的是
    *X* 的均值，而 *σ* 是 *X* 的标准差：
- en: '![Figure 4.4 – Standardization formula](img/B21197_04_04.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 标准化公式](img/B21197_04_04.jpg)'
- en: Figure 4.4 – Standardization formula
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 标准化公式
- en: Unlike normalization, data standardization will *not* result in a predefined
    range of values. Instead, it will transform your data into a standard Gaussian
    distribution, where your transformed values will represent the number of standard
    deviations of each value to the mean of the distribution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与归一化不同，数据标准化不会导致一个预定义的值范围。相反，它将你的数据转换成标准高斯分布，其中你的转换值将代表每个值相对于分布均值的标准差数。
- en: Important note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The Gaussian distribution, also known as the normal distribution, is one of
    the most used distributions in statistical models. This is a continuous distribution
    with two main controlled parameters: *µ* (mean) and *σ* (standard deviation).
    Normal distributions are symmetric around the mean. In other words, most of the
    values will be close to the mean of the distribution.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布，也称为正态分布，是统计模型中最常用的分布之一。这是一个具有两个主要控制参数的连续分布：*µ*（均值）和*σ*（标准差）。正态分布围绕均值是对称的。换句话说，大多数值将接近分布的均值。
- en: 'Data standardization is often referred to as the zscore and is widely used
    to identify outliers on your variable, which you will see later in this chapter.
    For the sake of demonstration, *Table 4.7* simulates the data standardization
    of a small dataset. The input value is shown in the Age column, while the scaled
    value is shown in the Zscore column:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化通常被称为 z 分数，并且广泛用于识别变量的异常值，你将在本章后面看到。为了演示，*表 4.7* 模拟了一个小数据集的数据标准化。输入值显示在“年龄”列中，而缩放值显示在“Z
    分数”列中：
- en: '| **Age** | **Mean** | **Standard deviation** | **Zscore** |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **平均值** | **标准差** | **Z 分数** |'
- en: '| 5 | 31,83 | 25,47 | -1,05 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 31,83 | 25,47 | -1,05 |'
- en: '| 20 | 31,83 | 25,47 | -0,46 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 31,83 | 25,47 | -0,46 |'
- en: '| 24 | 31,83 | 25,47 | -0,31 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 31,83 | 25,47 | -0,31 |'
- en: '| 32 | 31,83 | 25,47 | 0,01 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 31,83 | 25,47 | 0,01 |'
- en: '| 30 | 31,83 | 25,47 | -0,07 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 31,83 | 25,47 | -0,07 |'
- en: '| 80 | 31,83 | 25,47 | 1,89 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 80 | 31,83 | 25,47 | 1,89 |'
- en: Table 4.7 – Data standardization in action
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.7 – 数据标准化实践
- en: Make sure you are confident when applying normalization and standardization
    by hand in the AWS Machine Learning Specialty exam. They might provide a list
    of values, as well as mean and standard deviation, and ask you for the scaled
    value of each element in the list.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 机器学习专业考试中，确保你在手动应用归一化和标准化时充满信心。他们可能会提供一个值列表，以及平均值和标准差，并要求你给出列表中每个元素的缩放值。
- en: Applying binning and discretization
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用分箱和离散化
- en: '**Binning** is a technique where you can group a set of values into a bucket
    or bin – for example, grouping people between 0 and 14 years old into a bucket
    named “children,” another group of people between 15 and 18 years old into a bucket
    named “teenager,” and so on.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**分箱**是一种技术，可以将一组值分组到一个桶或区间中——例如，将 0 至 14 岁的人分组到名为“儿童”的桶中，将 15 至 18 岁的人分组到名为“青少年”的桶中，依此类推。'
- en: '**Discretization** is the process of transforming a continuous variable into
    discrete or nominal attributes. These continuous values can be discretized by
    multiple strategies, such as **equal-width** and **equal-frequency**.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散化**是将连续变量转换为离散或名义属性的过程。这些连续值可以通过多种策略进行离散化，例如**等宽**和**等频率**。'
- en: An equal-width strategy will split your data across multiple bins of the same
    width. Equal-frequency will split your data across multiple bins with the same
    number of frequencies.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 等宽策略将在多个相同宽度的区间中分割你的数据。等频率将你的数据分割成多个具有相同频率的区间。
- en: 'Look at the following example. Suppose you have the following list containing
    16 numbers: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 90\. As
    you can see, this list ranges between 10 and 90\. Assuming you want to create
    four bins using an equal-width strategy, you could come up with the following
    bins:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的例子。假设你有一个包含 16 个数字的列表：10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
    23, 24, 90。正如你所见，这个列表的范围在 10 到 90 之间。假设你想要使用等宽策略创建四个区间，你可以得到以下区间：
- en: Bin >= 10 <= 30 > 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 >= 10 <= 30 > 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
- en: Bin > 30 <= 50 >
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 30 <= 50 >
- en: Bin > 50 <= 70 >
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 50 <= 70 >
- en: Bin > 71 <= 90 > 90
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 71 <= 90 > 90
- en: 'In this case, the width of each bin is the same (20 units), but the observations
    are not equally distributed. Now, the next example simulates an equal-frequency
    strategy:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个区间的宽度相同（20 个单位），但观测值分布不均。现在，下一个示例模拟了一个等频率策略：
- en: Bin >= 10 <= 13 > 10, 11, 12, 13
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 >= 10 <= 13 > 10, 11, 12, 13
- en: Bin > 13 <= 17 > 14, 15, 16, 17
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 13 <= 17 > 14, 15, 16, 17
- en: Bin > 17 <= 21 > 18, 19, 20, 21
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 17 <= 21 > 18, 19, 20, 21
- en: Bin > 21 <= 90 > 22, 23, 24, 90
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区间 > 21 <= 90 > 22, 23, 24, 90
- en: In this case, all the bins have the same frequency of observations, although
    they have been built with different bin widths to make that possible.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，所有区间都有相同的观测频率，尽管它们使用了不同的区间宽度来实现这一点。
- en: 'Once you have computed your bins, you should be wondering what’s next, right?
    Here, you have some options:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算出了你的区间，你可能想知道下一步是什么，对吧？在这里，你有一些选择：
- en: You can name your bins and use them as a nominal feature on your model! Of course,
    as a nominal variable, you should think about applying one-hot encoding before
    feeding an ML model with this data.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以为你的区间命名，并将它们用作模型上的名义特征！当然，作为一个名义变量，你应该在将数据输入到机器学习模型之前考虑应用独热编码。
- en: You might want to order your bins and use them as an ordinal feature.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能想要对区间进行排序，并将它们用作有序特征。
- en: Maybe you want to remove some noise from your feature by averaging the minimum
    and maximum values of each bin and using that value as your transformed feature.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许您想通过平均每个箱子的最小值和最大值来从您的特征中去除一些噪声，并使用该值作为您的转换特征。
- en: 'Take a look at *Table 4.8* to understand these approaches using our equal-frequency
    example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 查看*表4.8*以了解这些方法，使用我们的等频示例：
- en: '| **Ordinal** **value** | **Bin** | **Transforming to a** **nominal feature**
    | **Transforming to an** **ordinal feature** | **Removing noise** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **序数值** | **箱** | **转换为** **名义特征** | **转换为** **序数特征** | **去除噪声** |'
- en: '| **10** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **10** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
- en: '| **11** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **11** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
- en: '| **12** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **12** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
- en: '| **13** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **13** | Bin >= 10 <= 13 | Bin A | 1 | 11,5 |'
- en: '| **14** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **14** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
- en: '| **15** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **15** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
- en: '| **16** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **16** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
- en: '| **17** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **17** | Bin > 13 <= 17 | Bin B | 2 | 15,5 |'
- en: '| **18** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **18** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
- en: '| **19** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **19** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
- en: '| **20** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **20** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
- en: '| **21** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **21** | Bin > 17 <= 21 | Bin C | 3 | 19,5 |'
- en: '| **22** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **22** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
- en: '| **23** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **23** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
- en: '| **24** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **24** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
- en: '| **90** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **90** | Bin > 21 <= 90 | Bin D | 4 | 55,5 |'
- en: Table 4.8 – Different approaches to working with bins and discretization
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.8 – 处理箱和离散化的不同方法
- en: Again, playing with different binning strategies will give you different results
    and you should analyze/test the best approach for your dataset. There is no standard
    answer here – it is all about data exploration!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，尝试不同的箱划分策略将给出不同的结果，您应该分析/测试最适合您数据集的最佳方法。这里没有标准答案——一切都关于数据探索！
- en: Applying other types of numerical transformations
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用其他类型的数值变换
- en: 'Normalization and standardization rely on your training data to fit their parameters:
    minimum and maximum values, in the case of normalization, and mean and standard
    deviation in the case of standard scaling. This also means you must fit those
    parameters using *only* your training data and never the testing data.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化和标准化依赖于您的训练数据来调整其参数：在归一化的情况下，最小值和最大值，在标准缩放的情况下，平均值和标准差。这也意味着您必须仅使用您的训练数据来调整这些参数，而绝不能使用测试数据。
- en: However, there are other types of numerical transformations that do not require
    parameters from training data to be applied. These types of transformations rely
    purely on mathematical computations. For example, one of these transformations
    is known as logarithmic transformation. This is a very common type of transformation
    in ML models and is especially beneficial for skewed features. If you don’t know
    what a skewed distribution is, take a look at *Figure 4**.5*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他类型的数值变换，它们不需要从训练数据中提取参数来应用。这些类型的变换完全依赖于数学计算。例如，其中一种变换被称为对数变换。这是机器学习模型中非常常见的一种变换，特别适用于偏斜特征。如果您不知道什么是偏斜分布，请查看*图4.5*。
- en: '![Figure 4.5 – Skewed distributions](img/B21197_04_05.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 偏斜分布](img/B21197_04_05.jpg)'
- en: Figure 4.5 – Skewed distributions
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 偏斜分布
- en: In the middle, you have a normal distribution (or Gaussian distribution). On
    the left- and right-hand sides, you have skewed distributions. In terms of skewed
    features, there will be some values far away from the mean in one single direction
    (either left or right). Such behavior will push both the median and mean values
    of this distribution in the same direction, that of the long tail you can see
    in *Figure 4**.5*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间，您有一个正态分布（或高斯分布）。在左侧和右侧，您有偏斜分布。在偏斜特征方面，将有一些值远离平均值，在一个单一方向上（要么是左侧，要么是右侧）。这种行为将推动这个分布的中位数和平均值向同一方向移动，即您在*图4.5*中可以看到的长尾方向。
- en: One very clear example of data that used to be skewed is the annual salaries
    of a particular group of professionals in a given region, such as senior data
    scientists working in Florida, US. This type of variable usually has most of its
    values close to the others (because people used to earn an average salary) and
    just has a few very high values (because a small group of people makes much more
    money than others).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据曾经是偏斜的非常明显的例子是某个特定地区一组专业人士的年度薪酬，例如在美国佛罗里达州工作的资深数据科学家。这类变量通常大部分值都接近其他值（因为人们过去通常赚取平均薪酬），只有少数几个非常高的值（因为一小部分人的收入远高于其他人）。
- en: Hopefully, you can now easily understand why the mean and median values will
    move to the tail direction, right? The big salaries will push them in that direction.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在您能轻松理解为什么均值和中位数会移动到尾部方向，对吧？高薪将推动它们向那个方向移动。
- en: 'Alright, but why will a logarithmic transformation be beneficial for this type
    of feature? The answer to this question can be explained by the math behind it:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，但为什么对数变换会对这种特征类型有益呢？这个问题的答案可以通过其背后的数学来解释：
- en: '![Figure 4.6 – Logarithmic properties](img/B21197_04_06.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 对数性质](img/B21197_04_06.jpg)'
- en: Figure 4.6 – Logarithmic properties
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 对数性质
- en: 'Computing the log of a number is the inverse of the exponential function. Log
    transformation will then reduce the scale of your number according to a given
    base (such as base 2, base 10, or base e, in the case of a natural logarithm).
    Looking at the salary distribution from the previous example, you would bring
    all those numbers down so that the higher the number, the higher the reduction;
    however, you would do this in a log scale and not in a linear fashion. Such behavior
    will remove the outliers of this distribution (making it closer to a normal distribution),
    which is beneficial for many ML algorithms, such as linear regression. *Table
    4.9* shows you some of the differences when transforming a number in a linear
    scale versus a log scale:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 计算一个数的对数是指数函数的逆。因此，对数变换将根据给定的基数（例如，基数 2、基数 10 或自然对数的情况下的基数 e）来缩小您的数字的规模。从上一个例子中观察薪酬分布，您会将所有这些数字降低，使得数值越高，降低的幅度越大；然而，您会以对数尺度而不是线性方式来做这件事。这种行为将消除这个分布的异常值（使其更接近正态分布），这对许多机器学习算法（如线性回归）有益。*表
    4.9* 展示了在将数字从线性尺度转换为对数尺度时的一些差异：
- en: '| **Ordinal value** | **Linear** **scale (normalization)** | **Log scale (****base
    10)** |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| **序数值** | **线性尺度（归一化）** | **对数尺度（**基数 10**）** |'
- en: '| 10 | 0.0001 | 1 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.0001 | 1 |'
- en: '| 1,000 | 0.01 | 3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 1,000 | 0.01 | 3 |'
- en: '| 10,000 | 0.1 | 4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 10,000 | 0.1 | 4 |'
- en: '| 100,000 | 1 | 5 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 100,000 | 1 | 5 |'
- en: Table 4.9 – Differences between linear transformation and log transformation
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.9 – 线性变换与对数变换之间的差异
- en: As you can see, the linear transformation kept the original magnitude of the
    data (you can still see outliers, but in another scale), while the log transformation
    removed those differences of magnitude and still kept the order of the values.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，线性变换保持了数据的原始幅度（您仍然可以看到异常值，但它们位于另一个尺度上），而对数变换则消除了这些幅度的差异，同时仍然保持了值的顺序。
- en: 'Would you be able to think about another type of mathematical transformation
    that follows the same behavior of *log* (making the distribution closer to Gaussian)?
    OK, here you have another: square root. Take the square root of those numbers
    shown in *Table 4.9* and see yourself!'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您能想到另一种遵循相同行为（使分布更接近高斯分布）的数学变换类型吗？好的，这里还有一个：平方根。取 *表 4.9* 中显示的这些数字的平方根，看看结果吧！
- en: 'Now, pay attention to this: both log and square root belong to a set of transformations
    known as power transformations, and there is a very popular method, which is likely
    to be mentioned on your AWS exam, that can perform a range of power transformations
    like those you have seen. This method was proposed by George Box and David Cox
    and its name is **Box-Cox**.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意这一点：对数和平方根都属于一组称为幂变换的变换，而且有一个非常流行的、可能在您的 AWS 考试中提到的方法，可以执行一系列您所看到的幂变换。这种方法是由乔治·博克斯和大卫·考克斯提出的，其名称为**Box-Cox**。
- en: Important note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: During your exam, if you see questions about the Box-Cox transformation, remember
    that it is a method that can perform many power transformations (according to
    a lambda parameter), and its end goal is to make the original distribution closer
    to a normal distribution.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的考试中，如果你看到有关Box-Cox变换的问题，请记住这是一个可以进行许多幂变换（根据lambda参数）的方法，其最终目标是使原始分布更接近正态分布。
- en: Just to conclude this discussion regarding why mathematical transformations
    can really make a difference to ML models, here is an example of **exponential
    transformations.**
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 只为了结束关于为什么数学变换真的可以对机器学习模型产生影响的讨论，这里有一个指数变换的例子。
- en: Suppose you have a set of data points, such as those on the left-hand side of
    *Figure 4**.7*. Your goal is to draw a line that will perfectly split blue and
    red points. Just by looking at the original data (again, on the left-hand side),
    you know that your best guess for performing this linear task would be the one
    you can see in the same figure. However, the science (not magic) happens on the
    right-hand side of the figure! By squaring those numbers and plotting them in
    another hyper plan, you can perfectly separate each group of points.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一组数据点，例如*图4.7*左侧的数据点。你的目标是画一条线，能够完美地分割蓝色和红色点。仅通过观察原始数据（再次，在左侧），你就知道执行这个线性任务的最佳猜测就是你在同一张图中看到的那个。然而，科学（不是魔法）发生在图的右侧！通过将这些数字平方并在另一个超平面上绘制它们，你可以完美地分离每一组点。
- en: '![Figure 4.7 – Exponential transformation in action](img/B21197_04_07.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – 指数变换的实际应用](img/B21197_04_07.jpg)'
- en: Figure 4.7 – Exponential transformation in action
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 指数变换的实际应用
- en: You might be thinking that there are infinite ways in which you can deal with
    your data. Although this is true, you should always take the business scenario
    you are working on into account and plan the work accordingly. Remember that model
    improvements or exploration is always possible, but you have to define your goals
    (remember the CRISP-DM methodology) and move on.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，你有无限种处理数据的方法。虽然这是真的，但你应该始终考虑你正在处理的业务场景并相应地规划工作。记住，模型改进或探索总是可能的，但你必须定义你的目标（记住CRISP-DM方法论）并继续前进。
- en: By the way, data transformation is important, but it is just one piece of your
    work as a data scientist. Your modeling journey still needs to move to other important
    topics, such as missing values and outliers handling. However, before that, you
    may have noticed that you were introduced to Gaussian distributions during this
    section, so why not go deeper into it?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，数据转换很重要，但它只是你作为数据科学家工作的一部分。你的建模之旅仍需要转移到其他重要主题，例如处理缺失值和异常值。然而，在此之前，你可能已经注意到在这一节中你被介绍了高斯分布，那么为什么不深入研究它呢？
- en: Understanding data distributions
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据分布
- en: Although the Gaussian distribution is probably the most common distribution
    for statistical and machine learning models, you should be aware that it is not
    the only one. There are other types of data distributions, such as the Bernoulli,
    binomial, and Poisson distributions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管高斯分布可能是统计和机器学习模型中最常见的分布，但你应该知道它不是唯一的。还有其他类型的数据分布，例如伯努利分布、二项分布和泊松分布。
- en: 'The Bernoulli distribution is a very simple one, as there are only two types
    of possible events: success or failure. The success event has a probability *p*
    of happening, while the failure one has a probability of *1-p*.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布是一个非常简单的分布，因为只有两种可能的事件类型：成功或失败。成功事件发生的概率为*p*，而失败事件的概率为*1-p*。
- en: 'Some examples that follow a Bernoulli distribution are rolling a six-sided
    die or flipping a coin. In both cases, you must define the event of success and
    the event of failure. For example, assume the following success and failure events
    when rolling a die:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一些遵循伯努利分布的例子包括掷一个六面骰子或抛硬币。在这两种情况下，你必须定义成功事件和失败事件。例如，假设掷骰子时的以下成功和失败事件：
- en: 'Success: Getting a number 6'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功：掷出数字6
- en: 'Failure: Getting any other number'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失败：得到任何其他数字
- en: You can then say that there is a *p* probability of success (1/6 = 0.16 = 16%)
    and a *1-p* probability of failure (1 - 0.16 = 0.84 = 84%).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样说，成功的概率为*p*（1/6 = 0.16 = 16%），失败的概率为*1-p*（1 - 0.16 = 0.84 = 84%）。
- en: The binomial distribution generalizes the Bernoulli distribution. The Bernoulli
    distribution has only one repetition of an event, while the binomial distribution
    allows the event to be repeated many times, and you must count the number of successes.
    Continue with the prior example, that is, counting the number of times you got
    a 6 out of our 10 dice rolls. Due to the nature of this example, binomial distribution
    has two parameters, *n* and *p*, where *n* is the number of repetitions and *p*
    is the probability of success in every repetition.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 二项分布是伯努利分布的推广。伯努利分布只有一个事件的重复，而二项分布允许事件重复多次，你必须计算成功的次数。继续使用之前的例子，即计算在10次掷骰子中，得到6的次数。由于这个例子本身的性质，二项分布有两个参数，*n*
    和 *p*，其中 *n* 是重复的次数，*p* 是每次重复成功的概率。
- en: 'Finally, a Poisson distribution allows you to find a number of events in a
    period of time, given the number of times an event occurs in an interval. It has
    three parameters: lambda, *e*, and *k*, where lambda is the average number of
    events per interval, *e* is the Euler number, and *k* is the number of times an
    event occurs in an interval.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，泊松分布允许你在给定事件在某个时间段内发生的次数的情况下，找到该时间段内的事件数量。它有三个参数：lambda，*e* 和 *k*，其中 lambda
    是每个区间内事件平均发生的次数，*e* 是欧拉数，*k* 是事件在每个区间内发生的次数。
- en: With all those distributions, including the Gaussian one, it is possible to
    compute the expected mean value and variance based on their parameters. This information
    is usually used in hypothesis tests to check whether some sample data follows
    a given distribution, by comparing the mean and variance **of the sample** against
    the **expected** mean and variance of the distribution.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些分布中，包括高斯分布，你可以根据它们的参数计算期望的均值和方差。这些信息通常用于假设检验，通过比较样本的均值和方差与分布的**期望**均值和方差，来检查某些样本数据是否遵循给定的分布。
- en: You are now more familiar with data distributions, not only Gaussian distributions.
    You will keep learning about data distributions throughout this book. For now,
    it is time to move on to missing values and outlier detection.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在对数据分布更加熟悉了，不仅限于高斯分布。你将在整本书中继续学习关于数据分布的内容。现在，是时候继续学习缺失值和异常值检测了。
- en: Handling missing values
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: As the name suggests, missing values refer to the absence of data. Such absences
    are usually represented by tokens, which may or may not be implemented in a standard
    way.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，缺失值指的是数据的缺失。这些缺失通常由标记表示，这些标记可能或可能不会以标准方式实现。
- en: Although using tokens is standard, the way those tokens are displayed may vary
    across different platforms. For example, relational databases represent missing
    data with *NULL*, core Python code will use *None*, and some Python libraries
    will represent missing numbers as **Not a** **Number (NaN)**.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用标记是标准的，但这些标记在不同平台上的显示方式可能会有所不同。例如，关系型数据库用 *NULL* 表示缺失数据，核心Python代码将使用 *None*，而一些Python库将缺失数字表示为**非数字（NaN）**。
- en: Important note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For numerical fields, don’t replace those standard missing tokens with *zeros*.
    By default, zero is not a missing value, but another number.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值字段，不要用 *零* 替换那些标准缺失标记。默认情况下，零不是缺失值，而是另一个数字。
- en: However, in real business scenarios, you may or may not find those standard
    tokens. For example, a software engineering team might have designed the system
    to automatically fill missing data with specific tokens, such as “unknown” for
    strings or “-1” for numbers. In that case, you would have to search by those two
    tokens to find missing data. People can set anything.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际的商业场景中，你可能找到或找不到那些标准标记。例如，一个软件工程团队可能已经设计了一个系统，自动用特定的标记填充缺失数据，例如字符串中的“未知”或数字中的“-1”。在这种情况下，你必须通过这两个标记来查找缺失数据。人们可以设置任何东西。
- en: In the previous example, the software engineering team was still kind enough
    to give you standard tokens. However, there are many cases where legacy systems
    do not add any data quality layer in front of the user, and you may find an address
    field filled with, “I don’t want to share,” or a phone number field filled with,
    “Don’t call me.” This is clearly missing data, but not as standard as the previous
    example.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的例子中，软件工程团队仍然非常友好地给了你标准的标记。然而，有许多情况是遗留系统在用户面前没有添加任何数据质量层，你可能会发现地址字段填满了“我不想分享”，或者电话号码字段填满了“不要给我打电话”。这显然是缺失数据，但不如之前的例子标准化。
- en: 'There are many more nuances that you will learn regarding missing data, all
    of which you will learn in this section, but be advised: before you start making
    decisions about missing values, you should prepare a good data exploration and
    make sure you find those values. You can either compute data frequencies or use
    missing plots, but please do something. Never assume that your missing data is
    represented only by those handy standard tokens.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缺失数据，你将学习到更多细微之处，所有这些你都将在本节中学习，但请注意：在你开始对缺失值做出决策之前，你应该做好良好的数据探索，并确保你找到那些值。你可以计算数据频率或使用缺失图，但请做点什么。永远不要假设你的缺失数据只由那些方便的标准标记表示。
- en: Why should you care about this type of data? Well, first, because most algorithms
    (apart from decision trees implemented on very specific ML libraries) will raise
    errors when they find a missing value. Second (and maybe most important), by grouping
    all the missing data in the same bucket, you are assuming that they are all the
    same, but in reality, you don’t know that.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么你应该关注这种类型的数据？首先，因为大多数算法（除了在非常具体的ML库上实现的决策树之外）在发现缺失值时会引发错误。其次（也许是最重要的），通过将所有缺失数据归入同一个类别，你假设它们都是相同的，但现实中你并不知道这一点。
- en: Such a decision will not only add bias to your model – it will reduce its interpretability,
    as you will be unable to explain the missing data. Once you know why you want
    to treat the missing values, then you can analyze your options.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的决定不仅会给你的模型带来偏差，还会降低其可解释性，因为你将无法解释缺失的数据。一旦你知道为什么你想处理缺失值，然后你可以分析你的选项。
- en: 'Theoretically, you can classify missing values into two main groups: **MCAR**
    or **MNAR**. MCAR stands for **Missing Completely at Random** and states that
    there is no pattern associated with the missing data. On the other hand, MNAR
    stands for **Missing Not at Random** and means that the underlying process used
    to generate the data is strictly connected to the missing values.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，你可以将缺失值分为两大类：**MCAR**或**MNAR**。MCAR代表**完全随机缺失**，表示缺失数据没有关联的模式。另一方面，MNAR代表**非随机缺失**，意味着用于生成数据的底层过程严格与缺失值相关。
- en: Look at the following example about MNAR missing values. Suppose you are collecting
    user feedback about a particular product in an online survey. Your process of
    asking questions is dynamic and depends on user answers. When a user specifies
    an age lower than 18 years old, you never ask his/her marital status. In this
    case, missing values of marital status are connected to the age of the user (MNAR).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下关于MNAR缺失值的例子。假设你在在线调查中收集关于特定产品的用户反馈。你的提问过程是动态的，并取决于用户的回答。当一个用户指定年龄低于18岁时，你永远不会询问他的婚姻状况。在这种情况下，婚姻状况的缺失值与用户的年龄相关（MNAR）。
- en: Knowing the class of missing values that you are dealing with will help you
    understand whether you have any control over the underlying process that generates
    the data. Sometimes, you can come back to the source process and, somehow, complete
    your missing data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 了解你正在处理的缺失值的类别将帮助你理解你是否对生成数据的底层过程有任何控制权。有时，你可以回到源过程，并 somehow 完成你的缺失数据。
- en: Important note
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Although, in real scenarios, you usually need to treat missing data via exclusion
    or imputation, never forget that you can always try to look at the source process
    and check if you can retrieve (or, at least, better understand) the missing data.
    You may face this option in the exam.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然，在现实场景中，你通常需要通过排除或插补来处理缺失数据，但永远不要忘记，你总是可以尝试查看源过程，并检查你是否可以检索（或者至少更好地理解）缺失数据。你可能会在考试中遇到这个选项。
- en: If you don’t have an opportunity to recover your missing data from anywhere,
    then you should move on to other approaches, such as **listwise deletion** and
    **imputation**.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有机会从任何地方恢复你的缺失数据，那么你应该转向其他方法，例如**逐行删除**和**插补**。
- en: Listwise deletion refers to the process of discarding some data, which is the
    downside of this choice. This may happen at the row level or the column level.
    For example, suppose you have a DataFrame containing four columns and one of them
    has 90% of its data missing. In such cases, what usually makes more sense is dropping
    the entire feature (column), since you don’t have that information for the majority
    of your observations (rows).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列删除法指的是丢弃一些数据的过程，这是这种选择的一个缺点。这可能在行级别或列级别发生。例如，假设你有一个包含四个列的DataFrame，其中一列有90%的数据缺失。在这种情况下，通常更有意义的是删除整个特征（列），因为你对于大多数观察（行）没有这些信息。
- en: From a row perspective, you may have a DataFrame with a small number of observations
    (rows) containing missing data in one of its features (columns). In such scenarios,
    instead of removing the entire feature, what makes more sense is removing only
    those few observations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从行的角度来看，你可能有一个包含少量观察（行）的DataFrame，其中其特征（列）之一包含缺失数据。在这种情况下，与其删除整个特征，不如只删除那些少数观察。
- en: The benefit of using this method is the simplicity of dropping a row or a column.
    Again, the downside is losing information. If you don’t want to lose information
    while handling your missing data, then you should go for an imputation strategy.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法的好处是删除一行或一列的简单性。然而，缺点是会丢失信息。如果你在处理缺失数据时不想丢失信息，那么你应该选择一个插补策略。
- en: 'Imputation is also known as replacement, where you will replace missing values
    by substituting a value. The most common approach to imputation is replacing the
    missing value with the mean of the feature. Please take note of this approach
    because it is likely to appear in your exam:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 插补也被称为替换，其中你将用替代值替换缺失值。插补最常见的方法是将缺失值替换为特征的均值。请注意这种方法，因为它可能会出现在你的考试中：
- en: '| **Age** |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** |'
- en: '| 35 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 35 |'
- en: '| 30 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 30 |'
- en: '|  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| 25 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 25 |'
- en: '| 80 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 80 |'
- en: '| 75 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 75 |'
- en: Table 4.10 – Replacing missing values with the mean or median
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.10 – 使用均值或中位数替换缺失值
- en: '*Table 4.10* shows a very simple dataset with one single feature and five observations,
    where the third observation has a missing value. If you decide to replace that
    missing data with the mean value of the feature, you will come up with 49\. Sometimes,
    when there are outliers in the data, the median might be more appropriate (in
    this case, the median would be 35):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '*表4.10*显示了一个非常简单的数据集，只有一个特征和五个观察值，其中第三个观察值有缺失值。如果你决定用特征的均值来替换那个缺失数据，你会得到49。有时，当数据中有异常值时，中位数可能更合适（在这种情况下，中位数将是35）：'
- en: '| **Age** | **Job status** |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **工作状态** |'
- en: '| **35** | Employee |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| **35** | 员工 |'
- en: '| **30** | Employee |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| **30** | 员工 |'
- en: '|  | Retired |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  | 退休 |'
- en: '| **25** | Employee |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| **25** | 员工 |'
- en: '| **80** | Retired |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| **80** | 退休 |'
- en: '| **75** | Retired |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| **75** | 退休 |'
- en: Table 4.11 – Replacing missing values with the mean or median of the group
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.11 – 使用组均值或中位数替换缺失值
- en: If you want to go deeper, you could find the mean or median value according
    to a given group of features. For example, *Table 4.11* expanded the previous
    dataset from *Table 4.10* by adding the *Job status* column. Now, there is some
    evidence that the initial approach of changing the missing value by using the
    overall median (35 years old) was likely to be wrong (since that person is retired).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要深入了解，你可以根据给定的特征组找到平均值或中位数。例如，*表4.11*通过添加*工作状态*列扩展了*表4.10*中的先前数据集。现在，有一些证据表明，最初通过使用整体中位数（35岁）来更改缺失值的方法可能是错误的（因为那个人已经退休了）。
- en: What you can do now is replace the missing value with the mean or median of
    the other observations that belong to the same job status. Using this new approach,
    you can change the missing information to 77.5\. Considering that the person is
    retired, 77.5 makes more sense than 35 years old.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以做的，是将缺失值替换为属于同一工作状态的其它观察值的平均值或中位数。使用这种方法，你可以将缺失信息更改为77.5。考虑到这个人已经退休，77.5比35岁更有意义。
- en: Important note
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the case of categorical variables, you can replace the missing data with
    the value that has the highest occurrence in your dataset. The same logic of grouping
    the dataset according to specific features is still applicable.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，你可以用数据集中出现频率最高的值来替换缺失数据。根据特定特征对数据集进行分组的逻辑仍然适用。
- en: You can also use more sophisticated methods of imputation, including constructing
    an ML model to predict the value of your missing data. The downside of these imputation
    approaches (either by averaging or predicting the value) is that you are making
    inferences about the data that are not necessarily right and will add bias to
    the dataset.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用更复杂的插补方法，包括构建一个机器学习模型来预测你的缺失数据值。这些插补方法（无论是通过平均还是预测值）的缺点是，你正在对数据做出可能不正确的推断，并将偏差添加到数据集中。
- en: To sum this up, the trade-off while dealing with missing data is having a balance
    between losing data or adding bias to the dataset. Unfortunately, there is no
    scientific recipe that you can follow, whatever your problem is. To decide on
    what you are going to do, you must look at your success criteria, explore your
    data, run experiments, and then make your decisions.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，处理缺失数据时的权衡是在丢失数据或向数据集添加偏差之间保持平衡。不幸的是，没有一种科学的方法可以遵循，无论你的问题是什么。为了决定你要做什么，你必须查看你的成功标准，探索你的数据，运行实验，然后做出你的决定。
- en: 'You will now move to another headache for many ML algorithms: outliers.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将转向许多机器学习算法的另一个头痛问题：异常值。
- en: Dealing with outliers
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异常值
- en: You are not on this studying journey just to pass the AWS Machine Learning Specialty
    exam but also to become a better data scientist. There are many different ways
    to look at the outlier problem purely from a mathematical perspective; however,
    the datasets used in real life are derived from the underlying business process,
    so you must include a business perspective during an outlier analysis.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你参加这次学习之旅不仅仅是为了通过AWS机器学习专业考试，更是为了成为一名更好的数据科学家。从纯粹数学的角度来看，有许多不同的方式来观察异常值问题；然而，实际生活中使用的数据集是从底层业务流程中派生出来的，因此在进行异常值分析时，你必须包括业务视角。
- en: An outlier is an atypical data point in a set of data. For example, *Figure
    4**.8* shows some data points that have been plotted in a two-dimension plan;
    that is, x and y. The red point is an outlier since it is an atypical value in
    this series of data.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是一组数据中的非典型数据点。例如，*图4.8*显示了在二维平面中绘制的一些数据点；即x和y。红色点是异常值，因为它在这个数据系列中是一个非典型值。
- en: '![Figure 4.8 – Identifying an outlier](img/B21197_04_08.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – 识别异常值](img/B21197_04_08.jpg)'
- en: Figure 4.8 – Identifying an outlier
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 识别异常值
- en: It is important to treat outlier values because some statistical methods are
    impacted by them. Still, in *Figure 4**.8*, you can see this behavior in action.
    On the left-hand side, there has been drawn a line that best fits those data points,
    ignoring the red point. On the right-hand side, the same line was drawn, but including
    the red point.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值很重要，因为一些统计方法会受到它们的影响。然而，在*图4.8*中，你可以看到这种行为在行动中的表现。在左侧，已经绘制了一条最佳拟合线，忽略了红色点。在右侧，同样的线被绘制，但包括了红色点。
- en: You can visually conclude that, by ignoring the outlier point, you will come
    up with a better solution on the plan of the left-hand side of the preceding chart
    since it was able to pass closer to most of the values. You can also prove this
    by computing an associated error for each line (which you will learn later in
    this book).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过忽略异常值点，你可以得出一个更好的解决方案，在前面图表的左侧计划中，因为它能够更接近大多数值。你也可以通过计算每条线相关的误差来证明这一点（你将在本书后面学到）。
- en: 'It is worth reminding that you have also seen the outlier issue in action in
    another situation in this book: specifically, in *Table 4.10*, while dealing with
    missing values. In that example, the median was used to work around the problem.
    Feel free to go back and read it again, but what should be very clear at this
    point is that median values are less impacted by outliers than average (mean)
    values.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 值得提醒的是，你也在这本书的另一个情境中看到了异常值问题：具体来说，在*表4.10*中处理缺失值时。在那个例子中，中位数被用来解决这个问题。你可以随时回去再读一遍，但此时应该非常清楚的是，中位数值比平均值（均值）值受异常值的影响较小。
- en: You now know what outliers are and why you should treat them. You should always
    consider your business perspective while dealing with outliers, but there are
    mathematical methods to find them. Now, you are ready to move on and look at some
    methods for outlier detection.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道了什么是异常值以及为什么你应该处理它们。在处理异常值时，你应该始终考虑你的业务视角，但也有一些数学方法可以找到它们。现在，你准备好继续前进，看看一些异常值检测的方法。
- en: 'You have already learned about the most common method: zscore. In *Table 4.7*,
    you saw a table containing a set of ages. Refer to it again to refresh your memory.
    In the last column of that table, it was computed the zscore of each age, according
    to the equation shown in *Figure 4**.4*.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no well-defined range for those zscore values; however, in a normal
    distribution *without* outliers, they will mostly range between -3 and 3\. Remember:
    zscore will give you the number of standard deviations from the mean of the distribution.
    *Table 4.10* shows some of the properties of a normal distribution:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Normal distribution properties. Image adapted from  https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg](img/B21197_04_09.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: According to the normal distribution properties, 95% of values will belong to
    the range of -2 and 2 standard deviations from the mean, while 99% of the values
    will belong to the range of -3 and 3\. Coming back to the outlier detection context,
    you can set thresholds on top of those zscore values to specify whether a data
    point is an outlier or not!
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: There is no standard threshold that you can use to classify outliers. Ideally,
    you should look at your data and see what makes more sense for you… usually (this
    is not a rule), you will use some number between 2 and 3 standard deviations from
    the mean to flag outliers, since less than 5% of your data will be selected by
    this rule (again, this is just a reference threshold, so that you can select some
    data from further scructizing). You may remember that there are outliers *below*
    and *above* the mean value of the distribution, as shown in *Table 4.12*, where
    the outliers were flagged with an **absolute** zscore greater than 3 (the value
    column is hidden for the sake of this demonstration).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value** | **Zscore** | **Is outlier?** |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| ... | 1.3 | NO |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| ... | 0.8 | NO |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| ... | **3.1** | **YES** |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| ... | -2.9 | NO |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| ... | **-****3.5** | **YES** |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| ... | 1.0 | NO |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| ... | 1.1 | NO |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: Table 4.12 – Flagging outliers according to the zscore value
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'Two outliers were found in *Table 4.12*: row number three and row number five.
    Another way to find outliers in the data is by applying the box plot logic. When
    you look at a numerical variable, it is possible to extract many descriptive statistics
    from it, not only the mean, median, minimum, and maximum values, as you have seen
    previously. Another property that’s present in data distributions is known as
    quantiles.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantiles are cut-off points that are established at regular intervals from
    the cumulative distribution function of a random variable. Those regular intervals,
    also known as *q-quantiles*, will be nearly the same size and will receive special
    names in some situations:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The 4-quantiles are called quartiles.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 10-quantiles are called deciles.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 100-quantiles are called percentiles.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100分位数被称为百分位数。
- en: For example, the 20th percentile (of a 100-quantile regular interval) specifies
    that 20% of the data is below that point. In a box plot, you can use regular intervals
    of 4-quantiles (also known as *quartiles*) to expose the distribution of the data
    (Q1 and Q3), as shown in *Figure 4**.10.*
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，20百分位数（100分位数常规区间的百分位数）指定20%的数据分布在该点以下。在箱线图中，你可以使用4分位数（也称为*四分位数*）的常规区间来展示数据的分布（Q1和Q3），如图*图4**.10*所示。
- en: '![Figure 4.10 – Box plot definition](img/B21197_04_10.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![图4.10 – 箱线图定义](img/B21197_04_10.jpg)'
- en: Figure 4.10 – Box plot definition
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 箱线图定义
- en: Q1 is also known as the lower quartile or 25th quartile, and this means that
    25% of the data is below that point in the distribution. Q3 is also known as the
    upper quartile or 75th quartile, and this means that 75% of the data is below
    that point in the distribution.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Q1也被称为下四分位数或25百分位数，这意味着25%的数据分布在该点以下。Q3也被称为上四分位数或75百分位数，这意味着75%的数据分布在该点以下。
- en: Computing the difference between Q1 and Q3 will give you the **interquartile
    range (IQR)** value, which you can then use to compute the limits of the box plot,
    shown by the “minimum” and “maximum” labels in the preceding diagram.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Q1和Q3之间的差异将给出**四分位距（IQR）**值，然后你可以使用这个值来计算箱线图的界限，如图中所示的“最小”和“最大”标签。
- en: After all, you can finally infer that anything below the “minimum” value or
    above the “maximum” value of the box plot will be flagged as an outlier.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，你最终可以推断出，任何低于箱线图“最小”值或高于“最大”值的点都将被标记为异常值。
- en: 'You have now learned about two different ways you can flag outliers on your
    data: zscore and box plot. You can decide whether you are going to remove these
    points from your dataset, transform them, or create another variable to specify
    that they exist (as shown in *Table 4.11*).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了两种可以在数据上标记异常值的方法：z分数和箱线图。你可以决定是否要从数据集中删除这些点，转换它们，或者创建另一个变量来指定它们的存在（如*表4.11*所示）。
- en: Moving further on this journey of data preparation and transformation, you will
    face other types of problems in real life. Next, you will learn that several use
    cases contain something known as **rare events**, which makes ML algorithms focus
    on the wrong side of the problem and propose bad solutions. Luckily, you will
    learn how to either tune hyperparameters or prepare the data to facilitate algorithm
    convergence while fitting rare events.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续数据准备和转换的旅程中，你将面临现实生活中其他类型的问题。接下来，你将了解到一些用例中包含被称为**罕见事件**的内容，这使得机器学习算法关注问题的错误方面，并提出不良解决方案。幸运的是，你将学习如何调整超参数或准备数据以促进算法收敛，同时适应罕见事件。
- en: Dealing with unbalanced datasets
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: At this point, you might have realized why data preparation is probably the
    longest part of the data scientist’s work. You have learned about data transformation,
    missing data values, and outliers, but the list of problems goes on. Don’t worry
    – you are on the right journey to master this topic!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经意识到为什么数据准备可能是数据科学家工作中最耗时的部分。你已经学习了数据转换、缺失数据值和异常值，但问题列表还在继续。别担心——你正走在掌握这一主题的正确道路上！
- en: Another well-known problem with ML models, specifically with classification
    problems, is unbalanced classes. In a classification model, you can say that a
    dataset is unbalanced when most of its observations belong to one (or some) of
    the classes (target variable).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型，特别是分类问题中，另一个众所周知的问题是类别不平衡。在分类模型中，当数据集中的大多数观察值属于一个（或一些）类别（目标变量）时，可以说数据集是不平衡的。
- en: 'This is very common in fraud identification systems: for example, where most
    of the events belong to a regular operation, while a very small number of events
    belong to a fraudulent operation. In this case, you can also say that fraud is
    a rare event.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这在欺诈识别系统中非常常见：例如，其中大多数事件属于常规操作，而极少数事件属于欺诈操作。在这种情况下，你也可以说欺诈是一个罕见事件。
- en: There is no strong rule for defining whether a dataset is unbalanced or not,
    it really depends on the context of your business domain. Most challenge problems
    will contain more than 99% of the observations in the majority class.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定义数据集是否不平衡并没有强规则，这实际上取决于你的业务领域背景。大多数挑战问题将包含超过99%的观察值属于多数类。
- en: 'The problem with unbalanced datasets is very simple: ML algorithms will try
    to find the best fit in the training data to maximize their accuracy. In a dataset
    where 99% of the cases belong to one single class, without any tuning, the algorithm
    is likely to prioritize the assertiveness of the majority class. In the worst-case
    scenario, it will classify all the observations as the majority class and ignore
    the minority one, which is usually our interest when modeling rare events.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集的问题非常简单：机器学习算法将试图在训练数据中找到最佳拟合以最大化其准确性。在一个99%的案例属于单个类的数据集中，如果没有调整，算法可能会优先考虑多数类的确定性。在最坏的情况下，它将把所有观察值分类为多数类，并忽略少数类，而这通常是我们建模罕见事件时的兴趣所在。
- en: 'To deal with unbalanced datasets, you have two major directions to follow:
    tuning the algorithm to handle the issue or resampling the data to make it more
    balanced.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理不平衡数据集，你有两个主要方向可以选择：调整算法来处理这个问题，或者重新采样数据以使其更平衡。
- en: By tuning the algorithm, you have to specify the weight of each class under
    classification. This class weight configuration belongs to the algorithm, not
    to the training data, so it is a hyperparameter setting. It is important to keep
    in mind that not all algorithms will have that type of configuration, and that
    not all ML frameworks will expose it, either. As a quick reference, the `DecisionTreeClassifier`
    class, from the scikit-learn ML library, is a good example that does implement
    the class weight hyperparameter.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整算法，你必须指定分类中每个类的权重。这种类权重配置属于算法，而不是训练数据，因此它是一个超参数设置。重要的是要记住，并非所有算法都会有这种配置，也不是所有机器学习框架都会公开它。作为一个快速参考，来自scikit-learn机器学习库的`DecisionTreeClassifier`类是一个很好的例子，它实现了类权重超参数。
- en: Another way to work around unbalanced problems is changing the training dataset
    by applying **undersampling** or **oversampling**. If you decide to apply undersampling,
    all you have to do is remove some observations from the majority class until you
    get a more balanced dataset. Of course, the downside of this approach is that
    you may lose important information about the majority class that you are removing
    observations from.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决不平衡问题的方法是通过对训练集应用**欠采样**或**过采样**来改变训练集。如果你决定应用欠采样，你所要做的就是从多数类中移除一些观察值，直到你得到一个更平衡的数据集。当然，这种方法的一个缺点是，你可能会丢失关于你正在移除观察值的多数类的重要信息。
- en: The most common approach for undersampling is known as random undersampling,
    which is a naïve resampling approach where you randomly remove some observations
    from the training set.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的欠采样方法是随机欠采样，这是一种简单的重采样方法，其中你随机从训练集中移除一些观察值。
- en: On the other hand, you can decide to go for oversampling, where you will create
    new observations/samples of the minority class. The simplest approach is the naïve
    one, where you randomly select observations from the training set (with replacement)
    for duplication. The downside of this method is the potential issue of overfitting,
    since you will be duplicating/highlighting the observed pattern of the minority
    class.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，你可以选择进行过采样，其中你将创建少数类的新的观察值/样本。最简单的方法是简单的方法，其中你随机从训练集中选择观察值（带替换）进行复制。这种方法的一个缺点是可能存在过拟合的问题，因为你将复制/突出显示少数类的观察模式。
- en: To either underfit or overfit your model, you should always test the fitted
    model on your testing set.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 要避免模型欠拟合或过拟合，你应该始终在测试集上测试拟合后的模型。
- en: Important note
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The testing set cannot be under/oversampled: only the training set should pass
    through these resampling techniques.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集不能进行欠采样或过采样：只有训练集应该通过这些重采样技术。
- en: 'You can also oversample the training set by applying synthetic sampling techniques.
    Random oversample does not add any new information to the training set: it just
    duplicates the existing ones. By creating synthetic samples, you are deriving
    those new observations from the existing ones (instead of simply copying them).
    This is a type of data augmentation technique known as the **Synthetic Minority
    Oversampling** **Technique (SMOTE).**'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过应用合成采样技术来对训练集进行过采样。随机过采样不会向训练集添加任何新信息：它只是复制现有的信息。通过创建合成样本，你是从现有样本中推导出这些新观察值（而不是简单地复制它们）。这是一种称为**合成少数过采样技术（SMOTE）**的数据增强技术。
- en: Technically, what SMOTE does is plot a line in the feature space of the minority
    class and extract points that are close to that line.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'You may find questions in your exam where the term SMOTE has been used. If
    that happens, keep in mind the context where this term is applied: oversampling.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Alright – in the next section, you will learn how to prepare text data for ML
    models.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with text data
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have already learned how to transform categorical features into numerical
    representations, either using label encoders, ordinal encoders, or one-hot encoding.
    However, what if you have fields containing long pieces of text in your dataset?
    How are you supposed to provide a mathematical representation for them in order
    to properly feed ML algorithms? This is a common issue in **Natural Language Processing
    (NLP),** a subfield of AI.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: NLP models aim to extract knowledge from texts; for example, translating text
    between languages, identifying entities in a corpus of text (also known as **Name
    Entity Recognition**, or **NER** for short), classifying sentiments from a user
    review, and many other applications.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B21197_08.xhtml#_idTextAnchor1058)*, AWS Application Services
    for AI/ML*, you will learn about some AWS application services that apply NLP
    to their solutions, such as Amazon Translate and Amazon Comprehend. During the
    exam, you might be asked to think about the fastest or easiest way (with the least
    development effort) to build certain types of NLP applications. The fastest or
    easiest way is usually to use those out-of-the-box AWS services, since they offer
    pre-trained models for some use cases (especially machine translation, sentiment
    analysis, topic modeling, document classification, and entity recognition).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: In a few chapters’ time, you will also learn about some built-in AWS algorithms
    for NLP applications, such as BlazingText, **Latent Dirichlet Allocation** (**LDA**),
    **Neural Topic Modeling** (**NTM**), and the **Sequence-to-Sequence** algorithm.
    Those algorithms also let you create the same NLP solutions that are created by
    those out-of-the-box services; however, you have to use them on SageMaker and
    write your own solution. In other words, they offer more flexibility but demand
    more development effort.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Keep that in mind for your exam!
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Although AWS offers many out-of-the-box services and built-in algorithms that
    allow you to create NLP applications, you will not look at those AWS product features
    now (you will do in [*Chapter 6*](B21197_06.xhtml#_idTextAnchor708)*, Applying
    Machine Learning Algorithms*, and [*Chapter 8*](B21197_08.xhtml#_idTextAnchor1058)*,
    AWS Application Services for AI/ML*). You will finish this chapter by looking
    at some data preparation techniques that are extremely important for preparing
    your data for NLP.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first one you will learn is known as bag of words (BoW). This is a very
    common and simple technique, applied to text data, that creates matrix representations
    to describe the number of words within the text. BoW consists of two main steps:
    creating a vocabulary and creating a representation of the presence of those known
    words from the vocabulary in the text. These steps can be seen in *Figure 4**.11.*'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – BoW in action](img/B21197_04_11.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – BoW in action
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: First things first, you usually can’t use raw text to prepare a BoW representation.
    There is a data cleansing step where you lowercase the text; split each work into
    tokens; remove punctuation, non-alphabetical, and stop words; and, whenever necessary,
    apply any other custom cleansing techniques you may want.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Once you have cleansed your raw text, you can add each word to a global vocabulary.
    Technically, this is usually a dictionary of tuples, in the form {(word, number
    of occurrences)} – for example, {(apple, 10), (watermelon, 20)}. As I mentioned
    previously, this is a global dictionary, and you should consider all the texts
    you are analyzing.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Now, with the cleansed text and updated vocabulary, you can build your text
    representation in the form of a matrix, where each column represents one word
    from the global vocabulary and each row represents a text you have analyzed. The
    way you represent those texts on each row may vary according to different strategies,
    such as binary, frequency, and count. Next, you will learn these strategies a
    little more.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.11*, a single piece of text is being processed with the three
    different strategies for BoW. That’s why you can see three rows on that table,
    instead of just one (in a real scenario, you have to choose one of them for implementation).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: In the first row, it was used a binary strategy, which will assign 1 if the
    word exists in the global vocabulary and 0 if not. Because the vocabulary was
    built on a single text, all the words from that text belong to the vocabulary
    (the reason you can only see 1s in the binary strategy).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: In the second row, it was used a frequency strategy, which will check the number
    of occurrences of each word within the text and divide it by the total number
    of words within the text. For example, the word “this” appears just once (1) and
    there are seven other words in the text (7), so 1/7 is equal to 0.14.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the third row, it was used a count strategy, which is a simple count
    of occurrences of each word within the text.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: This note is really important – you are likely to find it in your exam. You
    may have noticed that the BoW matrix contains unique words in the *columns* and
    each text representation is in the *rows*. If you have 100 long pieces of text
    with only 50 unique words across them, your BoW matrix will have 50 columns and
    100 rows. During your exam, you are likely to receive a list of pieces of text
    and be asked to prepare the BoW matrix.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: There is one more extremely important concept you should know about BoW, which
    is the n-gram configuration. The term n-gram is used to describe the way you would
    like to look at your vocabulary, either via single words (uni-gram), groups of
    two words (bi-gram), groups of three words (tri-gram), or even groups of *n* words
    (n-gram). So far, you have seen BoW representations using a uni-gram approach,
    but more sophisticated representations of BoW may use bi-grams, tri-grams, or
    n-grams.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'The main logic itself does not change, but you need to know how to represent
    n-grams in BoW. Still using the example from *Figure 4**.11*, a bi-gram approach
    would combine those words in the following way: [this movie, movie really, really
    good, good although, although old, old production]. Make sure you understand this
    before taking the exam.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: The power and simplicity of BoW come from the fact that you can easily come
    up with a training set to train your algorithms. If you look at *Figure 4**.11*,
    can you see that having more data and just adding a classification column to that
    table, such as good or bad review, would allow us to train a binary classification
    model to predict sentiments?
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Alright – you might have noticed that many of the awesome techniques that you
    have been introduced to come with some downsides. The problem with BoW is the
    challenge of maintaining its vocabulary. You can easily see that, in a huge corpus
    of texts, the vocabulary size tends to become bigger and bigger and the matrices’
    representations tend to be sparse (yes – the sparsity issue again).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: One possible way to solve the vocabulary size issue is by using word hashing
    (also known in ML as the **hashing** **trick**). Hash functions map data of arbitrary
    sizes to data of a fixed size. This means you can use the hash trick to represent
    each text with a fixed number of features (regardless of the vocabulary’s size).
    Technically, this hashing space allows collisions (different texts represented
    by the same features), so this is something to take into account when you are
    implementing feature hashing.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another problem that comes with BoW, especially when you use the frequency strategy
    to build the feature space, is that more frequent words will strongly boost their
    scores due to the high number of occurrences within the document. It turns out
    that, often, those words with high occurrences are not the key words of the document,
    but just other words that *also* appear many times in several other documents.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '**Term Frequency-Inverse Document Frequency (TF-IDF)** helps penalize these
    types of words, by checking how frequent they are in other documents and using
    that information to rescale the frequency of the words within the document.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the process, TF-IDF tends to give more importance to words that
    are unique to the document (document-specific words). Next, let’s look at a concrete
    example so that you can understand it in depth.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Consider that you have a text corpus containing 100 words and the word “Amazon”
    appears three times. The Term Frequency (TF) of this word would be 3/100, which
    is equal to 0.03\. Now, suppose you have other 1,000 documents and that the word
    “Amazon” appears in 50 of these. In this case, the Inverse Document Frequency
    (IDF) would be given by the log as 1,000/50, which is equal to 1.30\. The TF-IDF
    score of the word “Amazon,” in that specific document under analysis, will be
    the product of TF * IDF, which is 0.03 * 1.30 (*0.039*).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that instead of 50 documents, the word “Amazon” had also appeared on
    another 750 documents – in other words, much more frequently than in the prior
    scenario. In this case, the TF part of this equation will not change – it is still
    0.03\. However, the IDF piece will change a little, since this time it will be
    log 1,000/750, which is equal to *0.0036*. As you can see, now the word “Amazon”
    has much less importance than in the previous example.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike traditional approaches, such as BoW and TD-IDF, modern methods of text
    representation will take care of the context of the information, as well as the
    presence or frequency of words. One very popular and powerful approach that follows
    this concept is known as **word embedding.** Word embeddings create a dense vector
    of a fixed length that can store information about the context and meaning of
    the document.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Each word is represented by a data point in a multidimensional hyper plan, which
    is known as embedding space. This embedding space will have *n* dimensions, where
    each of these dimensions refers to a particular position of this dense vector.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Although it may sound confusing, the concept is actually pretty simple. Suppose
    you have a list of four words, and you want to plot them in an embedding space
    of five dimensions. The words are king, queen, live, and castle. *Table 4.13*
    shows how to do this.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Dim 1** | **Dim 2** | **Dim 3** | **Dim 4** | **Dim 5** |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| **King** | 0.22 | 0.76 | 0.77 | 0.44 | 0.33 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| **Queen** | 0.98 | 0.09 | 0.67 | 0.89 | 0.56 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| **Live** | 0.13 | 0.99 | 0.88 | 0.01 | 0.55 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| **Castle** | 0.01 | 0.89 | 0.34 | 0.02 | 0.90 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: Table 4.13 – An embedding space representation
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Forget the hypothetical numbers in *Table 4.13* and focus on the data structure;
    you will see that each word is now represented by *n* dimensions in the embedding
    space. This process of transforming words into vectors can be performed by many
    different methods, but the most popular ones are word2vec and GloVe.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Once you have each word represented as a vector of a fixed length, you can apply
    many other techniques to do whatever you need. One very common task is plotting
    those “words” (actually, their dimensions) in a hyper plan and visually checking
    how close they are to each other!
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Technically, you don’t use this to plot them as-is, since human brains cannot
    interpret more than three dimensions. Furthermore, you usually apply a dimensionality
    reduction technique (such as principal component analysis, which you will learn
    about later) to reduce the number of dimensions to two, and finally plot the words
    in a Cartesian plan. That’s why you might have seen pictures like the one at the
    bottom of *Table 4.15*. Have you ever asked yourself how it is possible to plot
    words on a graph?
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Plotting words](img/B21197_04_12.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Plotting words
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how the five dimensions shown in *Figure 4**.12* were built.
    Again, there are different methods to do this, but you will learn the most popular,
    which uses a co-occurrence matrix with a fixed context window.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: First, you have to come up with some logic to represent each word, keeping in
    mind that you also have to take their context into consideration. To solve the
    context requirement, you need to define a **fixed context window**, which is going
    to be responsible for specifying how many words will be grouped together for context
    learning. For instance, assume this fixed context window as 2.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will create a co-occurrence matrix, which will count the number of
    occurrences of each pair of words, according to the pre-defined context window.
    Consider the following text: “I will pass this exam, you will see. I will pass
    it.”'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'The context window of the first word “pass” would be the ones in *bold*: “*I
    will* pass *this exam*, you will see. I will pass it.” Considering this logic,
    have a look at how many times each pair of words appears in the context window
    (*Figure 4**.13*).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Co-occurrence matrix](img/B21197_04_13.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Co-occurrence matrix
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the pair of words “I will” appears three times when a context
    window of size 2 is used:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '*I will* pass this exam, you will see. I will pass it.'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will pass this exam, you *will* see. *I* will pass it.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will pass this exam, you will see. *I will* pass it.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at *Figure 4**.13*, the same logic should be applied to all other pairs
    of words, replacing “…” with the associated number of occurrences. You now have
    a numerical representation for each word!
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware that there are many alternatives to co-occurrence matrices
    with a fixed context window, such as using TD-IDF vectorization or even simpler
    counters of words per document. The most important message here is that, somehow,
    you must come up with a numerical representation for each word.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is finally finding those dimensions shown in *Table 4.13*. You
    can do this by creating a multilayer model, usually based on neural networks,
    where the hidden layer will represent your embedding space. *Figure 4**.14* shows
    a simplified example where you could potentially compress those words shown in
    *Figure 4**.13* into an embedding space of five dimensions:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Building embedding spaces with neural networks](img/B21197_04_14.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Building embedding spaces with neural networks
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: You will learn about neural networks in more detail later in this book. For
    now, understanding where the embedding vector comes from is already an awesome
    achievement!
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing you should keep in mind while modeling natural language
    problems is that you can reuse a pre-trained embedding space in your models. Some
    companies have created modern neural network architectures, trained on billions
    of documents, which have become the state of the art in this field. For your reference,
    take a look at Bidirectional Encoder Representations from Transformers (BERT),
    which was proposed by Google and has been widely used by the data science community
    and industry.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: You have now reached the end of this long – but very important – chapter about
    data preparation and transformation. Take this opportunity to do a quick recap
    of the awesome things you have learned.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you were introduced to the different types of features that you might
    have to work with. Identifying the type of variable you’ll be working with is
    very important for defining the types of transformations and techniques that can
    be applied to each case.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned how to deal with categorical features. You saw that, sometimes,
    categorical variables do have an order (such as the ordinal ones), while other
    times, they don’t (such as the nominal ones). You learned that one-hot encoding
    (or dummy variables) is probably the most common type of transformation for nominal
    features; however, depending on the number of unique categories, after applying
    one-hot encoding, your data might suffer from sparsity issues. Regarding ordinal
    features, you shouldn’t create dummy variables on top of them, since you would
    be losing the information about the order that has been incorporated into the
    variable. In those cases, ordinal encoding is the most appropriate transformation.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: You continued your journey by looking at numerical features, where you learned
    how to deal with continuous and discrete data. You walked through the most important
    types of transformations, such as normalization, standardization, binning, and
    discretization. You saw that some types of transformation rely on the underlying
    data to find their parameters, so it is very important to avoid using the testing
    set to learn anything from the data (it must strictly be used only for testing).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: You have also seen that you can even apply pure math to transform your data;
    for example, you learned that power transformations can be used to reduce the
    skewness of your feature and make it more similar to a normal distribution.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: After that, you looked at missing data and got a sense of how important this
    task is. When you are modeling, you *can’t* look at the missing values as a simple
    computational problem, where you just have to replace *x* with *y*. This is a
    much bigger problem, and you need to start solving it by exploring your data and
    then checking whether your missing data was generated at random or not.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: When you are making the decision to remove or replace missing data, you must
    be aware that you are either losing information or adding bias to the dataset,
    respectively. Remember to review all the important notes of this chapter, since
    they are likely to be relevant to your exam.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: You also learned about outlier detection. You looked at different ways to find
    outliers, such as the zscore and box plot approaches. Most importantly, you learned
    that you can either flag or smooth them.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning, you were advised that this chapter would be a long but worthwhile
    journey about data preparation. You have also learned how to deal with rare events,
    since this is one of the most challenging problems in ML. Now you are aware that,
    sometimes, your data might be unbalanced, and you must either trick your algorithm
    (by changing the class weights) or resample your data (applying undersampling
    or oversampling).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to deal with text data for NLP. You should now be able
    to manually compute BoW and TF-IDF matrices! You went even deeper and learned
    how word embedding works. During this subsection, you learned that you can either
    create your own embedding space (using many different methods) or reuse a pre-trained
    one, such as BERT.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: You are done! In the next chapter, you will dive into data visualization techniques.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH04](https://packt.link/MLSC01E2_CH04).
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 4**.15*):'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_04_15.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 4**.16*:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Chapter Review Questions for Chapter 4](img/B21197_04_16.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Chapter Review Questions for Chapter 4
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: Table 4.14 – Sample timing practice drills on the online platform
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
