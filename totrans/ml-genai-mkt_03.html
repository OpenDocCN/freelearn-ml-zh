<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer126">
    <h1 class="chapterNumber">3</h1>
    <h1 class="chapterTitle" id="_idParaDest-70">Unveiling the Dynamics of Marketing Success</h1>
    <p class="normal"><strong class="keyWord">Key performance indicators</strong> (<strong class="keyWord">KPIs</strong>) are great at capturing the current status or results of marketing <a id="_idIndexMarker183"/>initiatives. However, they lack the ability to show what drives such results and how different factors may have affected the end results of marketing campaigns. We briefly looked at how you can further analyze relationships between various variables and KPIs with correlation analysis in the last chapter. In this <a id="_idIndexMarker184"/>chapter, we are going to delve deeper into how we <a id="_idIndexMarker185"/>can utilize <strong class="keyWord">data science</strong> (<strong class="keyWord">DS</strong>) and <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) techniques and tools to unveil the dynamics of marketing success.</p>
    <p class="normal">This chapter touches on three critical phases of the customer journey: engagement, conversion, and churn. These three types of customer behaviors are the key components and marketing goals to optimize for, and marketers often question what factors actually affect certain outcomes, such as why people engage more or less, why people end up converting more or less, and why people churn or don’t churn. The three most common approaches that are taken to unveil the factors that affect marketing outcomes, on top of the correlation analysis we have done in the last chapter, are (1) regression analysis, (2) decision tree interpretation, and (3) causal inference. In this chapter, we will discuss how we can perform those ML techniques in Python to have a better and more in-depth interpretation of the drivers behind marketing successes and failures.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Why people engage with regression analysis</li>
      <li class="bulletList">Why people convert with decision tree interpretation</li>
      <li class="bulletList">Why people churn with causal inference</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-71">Why people engage in regression analysis</h1>
    <p class="normal">Usually, increasing the engagement rate is the very first step in marketing. Potential customers need to engage with the company or products first before they can convert into paying customers and then, eventually, repeat or loyal customers. There will be numerous factors that affect how potential customers engage with or click on your marketing campaigns. Customers with certain socio-economic backgrounds may be more attracted to your services and/or products than others. People from certain regions may engage more frequently in your marketing campaigns than others. Some who have been exposed to your previous sales may have a more favorable appetite for your services or products that results in higher engagement rates than others.</p>
    <p class="normal">Understanding how potential customers interact with different components is the key to achieving <a id="_idIndexMarker186"/>higher engagement rates and the start of targeted or personalized marketing. <strong class="keyWord">Regression analysis</strong>, a well-known and widely used technique across various domains and often considered a fundamental concept in ML, is an intuitive and effective approach that we can use to understand the interactions among various components that may affect higher or lower engagement among your target customers. The two <a id="_idIndexMarker187"/>most commonly used types of regression <a id="_idIndexMarker188"/>analysis are <strong class="keyWord">linear regression</strong> and <strong class="keyWord">logistic regression</strong>.</p>
    <p class="normal">A linear regression assumes a linear relationship between a target variable and other factors. The linear relationship is defined as:</p>
    <p class="center"><a id="_idIndexMarker189"/><img alt="" src="../Images/B30999_03_001.png"/></p>
    <p class="normal">where <em class="italic">Y</em> is the target variable, which is the outcome that you observe, each <a id="_idIndexMarker190"/><img alt="" src="../Images/B30999_03_002.png"/> is the factor that may affect the outcome, each <a id="_idIndexMarker191"/><img alt="" src="../Images/B30999_03_003.png"/> is the coefficient and degree of impact each factor has on the target variable, and <em class="italic">a</em> is the intercept. Linear regression is used when the target variable is a continuous variable, such as customer lifetime value, sales volume, and customer tenure.</p>
    <p class="normal">Logistic regression, on the other hand, is used when the target variable is binary, such as <code class="inlineCode">Pass</code> vs. <code class="inlineCode">Fail</code>, <code class="inlineCode">True</code> vs. <code class="inlineCode">False</code>, and 1 vs. 0. When linear regression estimates the value of an outcome, logistic regression estimates the odds of success, and the relationship is defined as:</p>
    <p class="center"><a id="_idIndexMarker192"/><img alt="" src="../Images/B30999_03_004.png"/></p>
    <p class="normal">Where log denotes the natural logarithm and <em class="italic">P</em>(<em class="italic">y </em>= 1) is the probability of the outcome being 1</p>
    <p class="normal">As logistic regression estimates the probability of an outcome, it is a great tool to use when understanding the impacts of various factors on a binary outcome, such as whether a <a id="_idIndexMarker193"/>customer will respond to email marketing or whether a customer will click on an email message. In this section, we will use an auto insurance marketing dataset to discuss how logistic regression can be used to unveil the drivers behind customer engagement.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>:</p>
      <p class="normal"><a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Engage.ipynb"><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Engage.ipynb</span></a></p>
      <p class="normal">Data source: <a href="https://www.kaggle.com/datasets/pankajjsh06/ibm-watson-marketing-customer-value-data"><span class="url">https://www.kaggle.com/datasets/pankajjsh06/ibm-watson-marketing-customer-value-data</span></a></p>
    </div>
    <h2 class="heading-2" id="_idParaDest-72">Target variable</h2>
    <p class="normal">The first thing we need to do is define the target or outcome we want to analyze. As we are interested <a id="_idIndexMarker194"/>in understanding how various factors affect customer engagement, we will have the <code class="inlineCode">Response</code> variable as our target variable, which tells <a id="_idIndexMarker195"/>us whether a customer responded to a marketing call or not. The following code can be used for encoding the target variable with <code class="inlineCode">0</code> for when a customer did not respond and <code class="inlineCode">1</code> for when a customer did respond:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.read_csv(<span class="hljs-string">"./engage-data.csv"</span>)
df[<span class="hljs-string">"Engaged"</span>] = df[<span class="hljs-string">"Response"</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">"Yes"</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">This code first loads the data into a variable, <code class="inlineCode">df</code>, by using the <code class="inlineCode">pandas</code> library’s <code class="inlineCode">read_csv</code> function, and we create a new variable, <code class="inlineCode">Engaged</code>, by encoding <code class="inlineCode">Yes</code> as <code class="inlineCode">1</code> and <code class="inlineCode">No</code> as <code class="inlineCode">0</code> from the <code class="inlineCode">Response</code> column.</p>
    <h2 class="heading-2" id="_idParaDest-73">Continuous variables</h2>
    <p class="normal">There are <a id="_idIndexMarker196"/>largely two types of variants of factors that may affect the outcome or target variable:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Continuous variables</strong>: These are the variables that have real numbers as the values and can include fractions or decimals, such as monetary amounts, sales numbers, and pageview counts.</li>
      <li class="bulletList"><strong class="keyWord">Categorical variables</strong>: These are discrete variables that have a distinct set of options or categories as values, such as education levels, genders, and customer account types.</li>
    </ul>
    <p class="normal">We will first look at the relationships between some of the continuous variables in our auto insurance marketing dataset and the target variable, <code class="inlineCode">Engaged</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">df.describe()
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_01.png"/></figure>
    <p class="packt_figref">Figure 3.1: Distributions of continuous variables in the dataset</p>
    <p class="normal">As this screenshot shows, when we run the <code class="inlineCode">df.describe()</code> function, it shows the overall statistics of the continuous variables. In our auto insurance example dataset, there are eight continuous variables: <code class="inlineCode">Customer_Lifetime_Value</code>, <code class="inlineCode">Income</code>, <code class="inlineCode">Monthly_Premium_Auto</code>, <code class="inlineCode">Months_Since_Last_Claim</code>, <code class="inlineCode">Months_Since_Policy_Inception</code>, <code class="inlineCode">Number_of_Open_Complaints</code>, <code class="inlineCode">Number_of_Policies</code>, and <code class="inlineCode">Total_Claim_Amount</code>.</p>
    <p class="normal">In order for us to analyze the relationships between these variables and the outcome variable, <code class="inlineCode">Engaged</code>, we are going to use <code class="inlineCode">Logit</code> within the <code class="inlineCode">statsmodels</code> package. The code to fit a logistic regression model is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
continuous_vars = [
    x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> df.dtypes[(df.dtypes == <span class="hljs-built_in">float</span>) | (df.dtypes == <span class="hljs-built_in">int</span>)].index <span class="hljs-keyword">if</span> x != <span class="hljs-string">"Engaged"</span>
]
logit = sm.Logit(
    df[<span class="hljs-string">'Engaged'</span>],
    df[continuous_vars]
).fit()
</code></pre>
    <p class="normal">Here, we first <a id="_idIndexMarker197"/>import the required module, <code class="inlineCode">statsmodels.api</code>, and define the continuous variable, <code class="inlineCode">continuous_vars</code>. Then, we can use the <code class="inlineCode">Logit</code> object in the <code class="inlineCode">statsmodels</code> package to train a logistic regression model. We can <a id="_idIndexMarker198"/>view the trained logistic regression model with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">logit.summary()
</code></pre>
    <p class="normal">The output of this code looks as in the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_02.png"/></figure>
    <p class="packt_figref">Figure 3.2: Summary of logistic regression results</p>
    <p class="normal">Let’s analyze <a id="_idIndexMarker199"/>this logistic regression output. The two most important things to look at are, <em class="italic">coef</em> and <em class="italic">P</em>&gt;|<em class="italic">z</em>|. The leftmost column, <code class="inlineCode">coef</code>, in the bottom table is the coefficient for each variable. For example, the coefficient for the variable <code class="inlineCode">Monthly_Premium_Auto</code> is <code class="inlineCode">-0.0084</code> and the coefficient for the variable <code class="inlineCode">Total_Claim_Amount</code> is <code class="inlineCode">0.0001</code>. This means that customers paying higher monthly premiums are less likely to respond to marketing campaigns, which aligns with the heuristics that high-value customers often are the harder ones to get and encourage engagement. On the other hand, customers with higher claim amounts are more likely to engage, as they may be the ones utilizing insurance policies the most.</p>
    <p class="normal">The <em class="italic">P</em>&gt;|<em class="italic">z</em>| column suggests the statistical significance each variable has on the outcome. The values lower than <code class="inlineCode">0.05</code> are typically considered to have a statistically significant impact on the outcome, as p-values lower than <code class="inlineCode">0.05</code> suggest there is strong evidence against the null hypothesis that there is no impact of this variable on the target variable. However, values larger than <code class="inlineCode">0.05</code> do not necessarily suggest it does not have any significance <a id="_idIndexMarker200"/>on the outcome. You may also notice the coefficients for <code class="inlineCode">Customer_Lifetime_Value</code> and <code class="inlineCode">Income</code> are very small. </p>
    <p class="normal">This does not suggest <a id="_idIndexMarker201"/>they have low impacts on the outcome. As you may have guessed, these two variables have monetary values and the scales for these two variables are in thousands, which results in small coefficient values, but with significant impact on the outcome.</p>
    <h2 class="heading-2" id="_idParaDest-74">Categorical variables</h2>
    <p class="normal">We have seen how continuous variables interact with the outcome, <code class="inlineCode">Engaged</code>, in a positive <a id="_idIndexMarker202"/>or negative way. You may wonder, then, what <a id="_idIndexMarker203"/>about categorical or discrete variables? We have a handful of categorical variables within our auto insurance example dataset; however, we will use the <code class="inlineCode">Education</code> and <code class="inlineCode">Gender</code> variables as examples to discuss how to handle categorical variables when performing regression analysis.</p>
    <h3 class="heading-3" id="_idParaDest-75">Factorize</h3>
    <p class="normal">The first <a id="_idIndexMarker204"/>approach we can take is to utilize the <code class="inlineCode">factorize</code> functionality within the <code class="inlineCode">pandas</code> library. Using <code class="inlineCode">Education</code> as an example, you can run the following code to encode a textual discrete variable into a numerical variable:</p>
    <pre class="programlisting code"><code class="hljs-code">    labels, levels = df['Education'].factorize()
</code></pre>
    <p class="normal">The results of this factorization look like the following:</p>
    <pre class="programlisting code"><code class="hljs-code">labels, levels = df[<span class="hljs-string">'Education'</span>].factorize()
labels
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">array([0, 0, 0, ..., 0, 1, 1])
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">levels
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Index(['Bachelor', 'College', 'Master', 'High School or Below', 'Doctor'], dtype='object')
</code></pre>
    <p class="normal">As you can see from this code snippet, we apply the <code class="inlineCode">factorize</code> function for the <code class="inlineCode">Education</code> variable and get the two variables, <code class="inlineCode">labels</code> and <code class="inlineCode">levels</code>. The newly created <code class="inlineCode">labels</code> variable contains numerical values for each record and the <code class="inlineCode">levels</code> variable contains information <a id="_idIndexMarker205"/>about what each numerical value of the <code class="inlineCode">labels</code> variable means. In this example, <code class="inlineCode">Bachelor</code> is encoded as <code class="inlineCode">0</code>, <code class="inlineCode">College</code> as <code class="inlineCode">1</code>, <code class="inlineCode">Master</code> as <code class="inlineCode">2</code>, <code class="inlineCode">High School</code> or <code class="inlineCode">Below</code> as <code class="inlineCode">3</code>, and <code class="inlineCode">Doctor</code> as <code class="inlineCode">4</code>.</p>
    <h3 class="heading-3" id="_idParaDest-76">Categorical</h3>
    <p class="normal">The second <a id="_idIndexMarker206"/>approach we can take to encode categorical variables is to use the <code class="inlineCode">Categorical</code> function within the <code class="inlineCode">pandas</code> library. The following code shows an example of how to use the <code class="inlineCode">Categorical</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">categories = pd.Categorical(
    df[<span class="hljs-string">'Education'</span>], 
    categories=[<span class="hljs-string">'High School or Below'</span>, <span class="hljs-string">'Bachelor'</span>, <span class="hljs-string">'College'</span>, <span class="hljs-string">'Master'</span>, <span class="hljs-string">'Doctor'</span>],
categories.categories
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Index(['High School or Below', 'Bachelor', 'College', 'Master', 'Doctor'], dtype='object')
</code></pre>
    <pre class="programlisting code"><code class="hljs-code">categories.codes
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">array([1, 1, 1, ..., 1, 2, 2], dtype=int8)
</code></pre>
    <p class="normal">The <code class="inlineCode">categories</code> argument in the <code class="inlineCode">Categorical</code> function lets you define the order of the categories. Here, we defined the <code class="inlineCode">Education</code> category to order from <code class="inlineCode">High School or Below</code> to <code class="inlineCode">Doctor</code>. You can access the categories by calling the <code class="inlineCode">categories</code> attribute and you can get the encodings for each record by calling the <code class="inlineCode">codes</code> attribute. For example, in our case, <code class="inlineCode">High School or Below</code> is encoded with <code class="inlineCode">0</code>, <code class="inlineCode">Bachelor</code> with <code class="inlineCode">1</code>, and <code class="inlineCode">Doctor</code> with <code class="inlineCode">4</code>.</p>
    <h3 class="heading-3" id="_idParaDest-77">Dummy variables</h3>
    <p class="normal">One other <a id="_idIndexMarker207"/>approach that can be taken to encode categorical variables is to create dummy variables. Dummy variables are one-hot encoded variables for each category. </p>
    <p class="normal">For instance, in our example, the following dummy variables can be created for the <code class="inlineCode">Education</code> variable:</p>
    <pre class="programlisting code"><code class="hljs-code">pd.get_dummies(df['Education']).head(<span class="hljs-number">10</span>)
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_03.png"/></figure>
    <p class="packt_figref">Figure 3.3: Dummy variables created for the Education variable</p>
    <p class="normal">As you can see from this example, the <code class="inlineCode">pandas</code> library’s <code class="inlineCode">get_dummies</code> function was used to create dummy variables for the <code class="inlineCode">Education</code> variable. This creates five new variables: <code class="inlineCode">Bachelor</code>, <code class="inlineCode">College</code>, <code class="inlineCode">Doctor</code>, <code class="inlineCode">High School or Below</code>, and <code class="inlineCode">Master</code>. Each of these newly created variables is encoded <code class="inlineCode">0/1</code> (or <code class="inlineCode">False/True</code>), where if a given record belongs to a given category, it is encoded as <code class="inlineCode">1</code> or <code class="inlineCode">True</code>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">How to choose which method to use for categorical variable encoding</strong></p>
      <p class="normal"><strong class="keyWord">One-hot encoding</strong>/<strong class="keyWord">dummy variables</strong>: Use these when there is no natural ordering in the variable. Both methods create binary variables for each category. Note that creating one-hot encoded and dummy variables for all categories can result in large dimensional data and make data sparse. Thus, you may want to subselect important categories for one-hot encoding/dummy variables. </p>
      <p class="normal"><strong class="keyWord">Factorize</strong>: Use this as another quick and efficient method when you have nominal categorical variables that do not have any inherent order. </p>
      <p class="normal"><strong class="keyWord">Categorical</strong>: When there is a natural ordering in the categorical variable, such as education levels, it is best to use <code class="inlineCode">Categorical()</code> with a specified order. This ensures that the encoding respects the hierarchy of the categories, leading to accurate model predictions and evaluations such as AUC-ROC in analyses where order matters.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-78">Regression analysis</h3>
    <p class="normal">Now that <a id="_idIndexMarker208"/>we have explored different approaches that can be taken to encode categorical variables, we can run a regression analysis on how some of the categorical variables affect the outcome. Using the newly created categorical variables from the previous section, <code class="inlineCode">GenderFactorized</code> and <code class="inlineCode">EducationFactorized</code>, we can fit a logistic regression model with the following code to model the relationships and impacts of the <code class="inlineCode">Gender</code> and <code class="inlineCode">Education</code> levels on the level of marketing engagement:</p>
    <pre class="programlisting code"><code class="hljs-code">logit = sm.Logit(
    df[<span class="hljs-string">'Engaged'</span>],
    df[[
        <span class="hljs-string">'GenderFactorized'</span>,
        <span class="hljs-string">'EducationFactorized'</span>
    ]]
).fit()
</code></pre>
    <p class="normal">Running <code class="inlineCode">logit.summary()</code> will result in the following summary of the fitted logistic regression model:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_04.png"/></figure>
    <p class="packt_figref">Figure 3.4: Summary of logistic regression results</p>
    <p class="normal">As you can <a id="_idIndexMarker209"/>see from this logistic regression model output, both variables, <code class="inlineCode">GenderFactorized</code> and <code class="inlineCode">EducationFactorized</code>, have negative coefficients or impact on the outcome, <code class="inlineCode">Engaged</code>. As <code class="inlineCode">Gender</code> is encoded as <code class="inlineCode">0</code> for female and <code class="inlineCode">1</code> for male, this suggests that males are less likely to respond than females. </p>
    <p class="normal">Similarly, education levels are encoded from 0 to 4, and having a negative coefficient suggests that the higher the education level is, the less likely it is for a customer to respond.</p>
    <p class="normal">Along with the results we have seen from regression analysis with continuous variables in the previous section, these are great insights that show directional relationships among various factors against the engagement outcome.</p>
    <h2 class="heading-2" id="_idParaDest-79">Interaction variables</h2>
    <p class="normal">Regression <a id="_idIndexMarker210"/>models are great at identifying <a id="_idIndexMarker211"/>and estimating linear relationships between the variables and the target variable. However, variables are likely to have some relationships among themselves. For example, income may have a high correlation with education levels. One approach that can be taken to address such intrinsic relationships within the variables in regression analysis is to introduce multiplicative terms, as shown in the following formula:</p>
    <figure class="mediaobject"><a id="_idIndexMarker212"/><img alt="" src="../Images/B30999_03_005.png"/></figure>
    <p class="normal">This way, the regression model considers the interactions between variables that have relationships within themselves. The same <code class="inlineCode">Logit</code> within the <code class="inlineCode">statsmodels</code> library can be used for <a id="_idIndexMarker213"/>running regression analysis with interaction <a id="_idIndexMarker214"/>terms in Python. The following code will fit a logistic regression model with interaction terms:</p>
    <pre class="programlisting code"><code class="hljs-code">logit = sm.Logit.from_formula(
  data=df,
  formula=<span class="hljs-string">"Engaged ~ Income + EducationFactorized + Income:EducationFactorized"</span>
).fit()
logit.summary()
</code></pre>
    <p class="normal">As shown in the code, we are fitting a logistic regression model with two variables, <code class="inlineCode">Income</code> and <code class="inlineCode">Education</code>, and one interaction term, <code class="inlineCode">Income x Education</code>. The following is the summary of the fitted logistic regression model:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_05.png"/></figure>
    <p class="packt_figref">Figure 3.5: Summary of logistic regression results</p>
    <p class="normal">Looking at the first two components, <code class="inlineCode">Income</code><em class="italic"> </em>and <code class="inlineCode">EducationFactorized</code>, this output suggests that <code class="inlineCode">Income</code> and <code class="inlineCode">Education</code> have positive impacts on engagement likelihood. However, the interaction term, <code class="inlineCode">Income:EducationFactorized</code>, negates the individual positive effects of <code class="inlineCode">Income</code> and <code class="inlineCode">Education</code>, as it has a negative coefficient. In our case of marketing, this means that when you compare customers within the same income level, the higher the education level is, the lower their engagement rate will be. Similarly, this also suggests that for customers with the same education level, the higher their income is, the lower <a id="_idIndexMarker215"/>their engagement rate will be. As you <a id="_idIndexMarker216"/>can see from this example, by introducing the interaction terms, we can analyze the relationships among the variables more deeply and gain insights that may not be unveiled when analyzed individually.</p>
    <p class="normal">As we have seen in this section, regression analysis is a powerful tool to unveil the dynamics of the drivers behind the successes and failures of marketing efforts. In our example, we have seen how various variables have negative and positive impacts on customer engagement rates. As shown, regression analysis is great at identifying linear relationships between various factors and the outcome and can be used to understand the interactions within the factors. </p>
    <p class="normal">However, it is generally considered that regression models lack the understanding of intervariable relationships. Part of this limitation is because we have to introduce interaction terms for all possible intervariable relationships. As we saw when we created <code class="inlineCode">Income</code> and <code class="inlineCode">Education</code> interaction terms previously, we will have to build similar interaction terms to address other possible interactions among the variables. Furthermore, if you want to introduce or consider interactions among more than two variables, it starts to get complicated.</p>
    <p class="normal">Decision trees, unlike linear regression analysis, have a strength in addressing complex interactions among variables. We will examine the use of decision trees in the following section, which captures the interactions among various factors and how these interactions ultimately affect the final outcome very well.</p>
    <h1 class="heading-1" id="_idParaDest-80">Why people convert with decision tree interpretation</h1>
    <p class="normal">Once <a id="_idIndexMarker217"/>potential customers start engaging with your marketing campaigns, it is time to start converting them into paying customers. Similar to how various backgrounds may affect the tendency to engage, there are numerous factors affecting who converts more often than others. Socio-economic factors, of course, will always play a vital role in affecting the conversion rates. Age, region, and gender may also be important factors in different conversion results. Months or seasons of the year can also cause conversion rates to vary, depending on the services or products you provide.</p>
    <p class="normal">Not only single factors but also combinations of various factors may have significant influences on who may convert. A person who is employed and owns a house is more likely to convert for refinance loans than a person who owns a house but is retired. A 25-year-old student is less likely to look for a credit card than a 25-year-old entrepreneur. We have briefly discussed how regression analysis can help identify how factors interacting <a id="_idIndexMarker218"/>with each other affect customer behaviors by introducing interaction or multiplicative factors. Another great approach to uncovering the hidden insights around why certain people convert more than others is to use <strong class="keyWord">decision trees</strong> and their <a id="_idIndexMarker219"/>power to understand the interactions and relationships among various factors.</p>
    <p class="normal">Decision trees, as the name suggests, learn from data by growing trees. From a root node, a tree branches out by splitting the data into subcategories and eventually reaching the leaf nodes, from which you can understand what factors are affected in which ways to reach the corresponding leaf nodes. A sample of a decision tree may look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_06.png"/></figure>
    <p class="packt_figref">Figure 3.6: Sample of a decision tree</p>
    <p class="normal">We will have a detailed discussion on how to interpret the decision tree results with an example dataset, so we will save further explanation and discussion for later. Let’s talk first about how a decision tree branches out from each node.</p>
    <p class="normal">There are <a id="_idIndexMarker220"/>mainly two methods that are commonly used for splitting the data or branching out into child nodes:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Gini Impurity</strong>: Gini impurity <a id="_idIndexMarker221"/>measures how impure a partition is. The formula for the Gini impurity measure is as follows:</li>
    </ul>
    <figure class="mediaobject"><a id="_idIndexMarker222"/><img alt="" src="../Images/B30999_03_006.png"/></figure>
    <p class="normal-one">In this formula, <em class="italic">c</em> stands for the class labels, and <a id="_idIndexMarker223"/><img alt="" src="../Images/B30999_03_007.png"/> stands for the probability of a record with the class label <em class="italic">i</em> being chosen. When all records in each node of a tree are pure with all records being in a single target class, the Gini impurity measure reaches <code class="inlineCode">0</code>.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Entropy Information Gain</strong>: Entropy measures how much information it gains <a id="_idIndexMarker224"/>from splitting the data with the criteria being tested. The formula for <em class="italic">Entropy</em> is as follows:</li>
    </ul>
    <figure class="mediaobject"><a id="_idIndexMarker225"/><img alt="" src="../Images/B30999_03_008.png"/></figure>
    <p class="normal-one">As for the <em class="italic">Gini</em> measure, <em class="italic">c</em> stands for the class labels, and <a id="_idIndexMarker226"/><img alt="" src="../Images/B30999_03_009.png"/> stands for the probability of a record with the class label <em class="italic">i</em> being chosen. The split that gives the biggest change in the entropy measure will be chosen to branch out to child nodes, as it suggests that results in the highest information gain.</p>
    <p class="normal">With this fundamental knowledge about decision trees, we will dive into applying one to understand what drives successful marketing campaigns for better conversion rates.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>:</p>
      <p class="normal"><a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Convert.ipynb "><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Convert.ipynb</span></a></p>
      <p class="normal"><strong class="keyWord">Data source:</strong></p>
      <p class="normal"><a href="https://archive.ics.uci.edu/dataset/222/bank+marketing"><span class="url">https://archive.ics.uci.edu/dataset/222/bank+marketing</span></a></p>
    </div>
    <h2 class="heading-2" id="_idParaDest-81">Target variable</h2>
    <p class="normal">We will be using a bank marketing dataset for this exercise, where the marketing campaign <a id="_idIndexMarker227"/>was to convert customers to subscribe <a id="_idIndexMarker228"/>to term deposits. First things first, let’s load the data into a DataFrame and encode our target variable. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">df = pd.read_csv(<span class="hljs-string">"./convert-data.csv"</span>, sep=<span class="hljs-string">";"</span>)
df[<span class="hljs-string">"conversion"</span>] = df[<span class="hljs-string">"y"</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">"yes"</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Here, we are loading the data into a variable, <code class="inlineCode">df</code>, and encoding the <code class="inlineCode">y</code> column into <code class="inlineCode">1</code> for <code class="inlineCode">yes</code> and <code class="inlineCode">0</code> for <code class="inlineCode">no</code>. When you run <code class="inlineCode">df["conversion"].mean()</code>, you should see about 12% of the customers converted and subscribed to term deposits.</p>
    <h2 class="heading-2" id="_idParaDest-82">Continuous versus categorical variable</h2>
    <p class="normal">For our <a id="_idIndexMarker229"/>analysis, we will be using the <a id="_idIndexMarker230"/>following <a id="_idIndexMarker231"/>variables, which have continuous values: <code class="inlineCode">age</code>, <code class="inlineCode">balance</code>, <code class="inlineCode">duration</code>, <code class="inlineCode">campaign</code>, and <code class="inlineCode">previous</code>. You can use the following code to see the distribution for these variables:</p>
    <pre class="programlisting code"><code class="hljs-code">continuous_vars = [
    <span class="hljs-string">"age"</span>, <span class="hljs-string">"balance"</span>, <span class="hljs-string">"duration"</span>, <span class="hljs-string">"campaign"</span>, <span class="hljs-string">"previous"</span>
]
df[continuous_vars].describe()
</code></pre>
    <p class="normal">The distribution looks as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_07.png"/></figure>
    <p class="packt_figref">Figure 3.7: Distribution of continuous variables</p>
    <p class="normal">Similar <a id="_idIndexMarker232"/>to the previous exercise, there <a id="_idIndexMarker233"/>are some categorical variables <a id="_idIndexMarker234"/>in this dataset as well. The first discrete text variable that we will convert into numerical values is <code class="inlineCode">month</code>. If you look at the values of the variable <code class="inlineCode">month</code>, you will notice that the values are 3-letter month abbreviations. As months have an inherent order, we will convert the values into corresponding numerical values using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">months = [<span class="hljs-string">'jan'</span>, <span class="hljs-string">'feb'</span>, <span class="hljs-string">'mar'</span>, <span class="hljs-string">'apr'</span>, <span class="hljs-string">'may'</span>, <span class="hljs-string">'jun'</span>, <span class="hljs-string">'jul'</span>, <span class="hljs-string">'aug'</span>, <span class="hljs-string">'</span><span class="hljs-string">sep'</span>, <span class="hljs-string">'oct'</span>, <span class="hljs-string">'nov'</span>, <span class="hljs-string">'dec'</span>]
df[<span class="hljs-string">'month'</span>] = df[<span class="hljs-string">'month'</span>].apply(
    <span class="hljs-keyword">lambda</span> x: months.index(x)+<span class="hljs-number">1</span>
)
</code></pre>
    <p class="normal">Now, the variable <code class="inlineCode">month</code> will have values from <code class="inlineCode">1</code> to <code class="inlineCode">12</code> for months respectively from <code class="inlineCode">January</code> to <code class="inlineCode">December</code>.</p>
    <p class="normal">The variable <code class="inlineCode">job</code> is another categorical variable. Unlike the <code class="inlineCode">month</code> variable, the <code class="inlineCode">job</code> variable does not have an inherent order within itself. Thus, we will create dummy variables for each of the <code class="inlineCode">job</code> categories, which will be one-hot encoded for the corresponding job categories. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">jobs_encoded_df = pd.get_dummies(df[<span class="hljs-string">'job'</span>]
jobs_encoded_df.columns = [<span class="hljs-string">'job_%s'</span> % x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> jobs_encoded_df.columns]
</code></pre>
    <p class="normal">The output of this code will look like the following:</p>
    <pre class="programlisting code"><code class="hljs-code">jobs_encoded_df.head()
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_08.png"/></figure>
    <p class="packt_figref">Figure 3.8: Encoded dummy variables</p>
    <p class="normal">Similar <a id="_idIndexMarker235"/>to the previous exercise, we have used the <code class="inlineCode">get_dummies</code> function to create dummy <a id="_idIndexMarker236"/>variables for each of the job <a id="_idIndexMarker237"/>categories in the dataset. You can use the following code to add these newly created dummy variables back to the DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">df = pd.concat([df, jobs_encoded_df], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">With this code run, the DataFrame, <code class="inlineCode">df</code>, will now have newly created dummy variables of the <code class="inlineCode">job</code> column added to it.</p>
    <p class="normal">Lastly, we are going to encode <code class="inlineCode">marital</code> and <code class="inlineCode">housing</code> variables as well using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">marital_encoded_df = pd.get_dummies(df[<span class="hljs-string">'marital'</span>])
marital_encoded_df.columns = [<span class="hljs-string">'marital_%s'</span> % x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> marital_encoded_df.columns]
df = pd.concat([df, marital_encoded_df], axis=<span class="hljs-number">1</span>)
df[<span class="hljs-string">'housing'</span>] = df[<span class="hljs-string">'housing'</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">'yes'</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">As you can see from this code, we have created dummy variables for each of the marital categories, which are <code class="inlineCode">divorced</code>, <code class="inlineCode">married</code>, and <code class="inlineCode">single</code>. Then, we concatenated these newly created dummy variables back to the DataFrame, <code class="inlineCode">df</code>. We have also converted the textual values of the <code class="inlineCode">housing</code> column into <code class="inlineCode">1</code> for <em class="italic">yes</em> and <code class="inlineCode">0</code> for <em class="italic">no</em>.</p>
    <div class="packt_tip">
      <p class="normal">As categorical variable encoding is somewhat covered in depth in the previous exercise, we quickly went through this step for this exercise. Take your time going through this categorical variable encoding step and try different approaches that we discussed during the previous exercise as well!</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-83">Decision tree analysis</h2>
    <p class="normal">We have <a id="_idIndexMarker238"/>compiled the variables of our interest, which are <code class="inlineCode">age</code>, <code class="inlineCode">balance</code>, <code class="inlineCode">duration</code>, <code class="inlineCode">campaign</code>, <code class="inlineCode">previous</code>, <code class="inlineCode">housing</code>, <code class="inlineCode">month</code>, <code class="inlineCode">job</code>, and <code class="inlineCode">marital</code><em class="italic"> </em>status. We are <a id="_idIndexMarker239"/>going to start looking into how these factors have affected the conversion rate outcome of this marketing campaign. First, we are going to define our features and the target or response variable, as in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">features = (
    continuous_vars
    + [<span class="hljs-string">"housing"</span>, <span class="hljs-string">"month"</span>]
    + <span class="hljs-built_in">list</span>(jobs_encoded_df.columns)
    + <span class="hljs-built_in">list</span>(marital_encoded_df.columns)
)
response_var = <span class="hljs-string">'conversion'</span>
</code></pre>
    <p class="normal">As noted in this code, we are using the <code class="inlineCode">conversion</code> column as our outcome or target variable and the continuous variables and encoded categorical variables that we have created previously. The resulting list of features should look as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">features
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_09.png"/></figure>
    <p class="packt_figref">Figure 3.9: List of features</p>
    <p class="normal">With the <a id="_idIndexMarker240"/>feature set and target variable <a id="_idIndexMarker241"/>defined, we are ready to build a decision tree. Take a look at the following code first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree
dt_model = tree.DecisionTreeClassifier(
    max_depth=<span class="hljs-number">3</span>
)
dt_model.fit(df[features], df[response_var])
</code></pre>
    <p class="normal">We are using the <code class="inlineCode">tree</code> module within the <code class="inlineCode">sklearn</code> or <code class="inlineCode">scikit-learn</code> library and using the <code class="inlineCode">DecisionTreeClassifier</code> class to train a decision tree model. You will notice one argument that we have defined, <code class="inlineCode">max_depth</code>. This argument controls how much the tree grows. The deeper the tree grows, the more accurately the tree learns the data; however, it will likely overfit to the dataset and not be generalizable, meaning it will perform worse for the data that has not been observed. It is best to limit how much a tree grows and balance between how well and accurately a tree learns the data and how well this knowledge can be generalized. In this exercise, we will set <code class="inlineCode">max_depth</code> to <code class="inlineCode">3</code>. Finally, you can use the <code class="inlineCode">fit</code> function to let the tree learn from the data.</p>
    <div class="packt_tip">
      <p class="normal">Try finding a golden spot for how deep a decision tree should grow by splitting the dataset into training and testing sets and choosing the depth that minimizes the difference between the training and testing sets.</p>
    </div>
    <p class="normal">The most intuitive method to use the knowledge that the decision tree gained from the data is to <a id="_idIndexMarker242"/>visualize how it branched out from each node and how <a id="_idIndexMarker243"/>the interactions between the variables lead to the leaf nodes. To do this, we will use the <code class="inlineCode">graphviz</code> package, which you can install with the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">conda install python-graphviz
</code></pre>
    <p class="normal">You can use the following code to draw the trained decision tree:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> graphviz
dot_data = tree.export_graphviz(
    dt_model,
    out_file=<span class="hljs-literal">None</span>,
    feature_names=features, 
    class_names=[<span class="hljs-string">'0'</span>, <span class="hljs-string">'1'</span>], 
    filled=<span class="hljs-literal">True</span>,
    rounded=<span class="hljs-literal">True</span>, 
    special_characters=<span class="hljs-literal">True</span>
)
graph = graphviz.Source(dot_data, <span class="hljs-built_in">format</span>=<span class="hljs-string">"png"</span>)
graph.render(<span class="hljs-string">"</span><span class="hljs-string">conversion-dt-depth-3"</span>)
</code></pre>
    <p class="normal">As you can see from the code, we are giving the function the trained model, <code class="inlineCode">dt_model</code>, and the features, <code class="inlineCode">features</code>, that were used for training the model. Then, you can output the graph into an image file with the <code class="inlineCode">render</code> function of the graph. This code will generate an image file named <code class="inlineCode">conversion-dt-depth-3.png</code> and it will look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_10.png"/></figure>
    <p class="packt_figref">Figure 3.10: Decision tree diagram</p>
    <p class="normal">As you can see from this decision tree graph, the depth of the tree is 3, which is what we defined it to be when we were training the decision tree. Let’s take a closer look at this graph.</p>
    <p class="normal">The root <a id="_idIndexMarker244"/>node is being split based on the variable <code class="inlineCode">duration</code>. As the arrows suggest, if the duration is less than or equal to <code class="inlineCode">521.5</code>, then it moves <a id="_idIndexMarker245"/>to the left child node; otherwise, it moves to the right child node. If we traverse to the rightmost leaf node by following the path with a duration greater than <code class="inlineCode">827.5</code> and <code class="inlineCode">marital_married</code> greater than <code class="inlineCode">0.5</code>, the majority of the customers in this leaf node have converted. In other words, those customers who have had a contact duration that is longer than <code class="inlineCode">827.5</code> seconds and who are married have converted more.</p>
    <p class="normal">On the other hand, if we traverse to the leftmost leaf node, it suggests that those customers who have had a contact duration that is shorter than or equal to <code class="inlineCode">521.5</code> seconds (root node), 0 previous contacts (left child node at depth 1), and their age is less than or equal to <code class="inlineCode">60.5</code> (leftmost child node at depth 2) are more likely not to convert than the others.</p>
    <div class="packt_tip">
      <p class="normal">There are other parameters you can use to build decision trees. Try fine-tuning your trees and see how different parameters affect the formation of the decision trees and how you can deduce various other insights from the same dataset!</p>
    </div>
    <p class="normal">As you can see from this example, by traversing through the tree and its branches and nodes, we can see the impacts of different factors of marketing on the customer behavior of conversion. Compared to the regression analysis that we did in the previous section, this decision tree has the advantage of better understanding more complex interactions <a id="_idIndexMarker246"/>and inter-relationships among different factors and how <a id="_idIndexMarker247"/>they affect the outcome variable. One thing to note is that both regression analysis and decision tree analysis results show correlations between the factors and the outcome rather than the causal relationships of what causes certain outcomes. Examining the causality of certain outcomes is not an easy task; however, in the following section, we will discuss how we can estimate the causal relationships between the features and the outcome from the data.</p>
    <h1 class="heading-1" id="_idParaDest-84">Why people churn with causal inference</h1>
    <p class="normal">A high churn rate is especially a problem in marketing as it negates all the good marketing <a id="_idIndexMarker248"/>income that was generated from previous marketing campaigns. It is critical to have an in-depth understanding of why people churn and what factors need to be optimized to reduce the customer churn rate. As we have seen in previous sections, regression analysis and decision tree analysis are great at identifying linear relationships between the potential factors and the outcome and the inter-relationships between various factors and the outcome variable. However, as noted before, these identified relationships or correlations do not necessarily mean causations.</p>
    <p class="normal">Identifying the causes of certain outcomes (for example, causes of customer churn) is often a difficult <a id="_idIndexMarker249"/>and complex task to achieve. This is where <strong class="keyWord">causal analysis</strong> comes in. If regression analysis was used to identify the relationships among the variables and decision tree analysis was used to identify the <em class="italic">interactions</em> among the variables, causal analysis is used to identify the <em class="italic">causes</em> and <em class="italic">effects</em> of certain outcomes. </p>
    <p class="normal">This is typically done through various experiments with control groups and treatment or experimental groups. The control group, as the name suggests, is the group to which no treatment is given. In a medical experiment setting, this control group is the one that will get a placebo. The treatment group, on the other hand, is the group that actually gets the treatment. Similarly, in a medical experiment setting, this treatment group is the one that gets the actual medical treatments, such as new medicine pills being developed.</p>
    <p class="normal">This paradigm of the necessity of setting up experiments prior to running a marketing campaign restricts the after-the-fact analysis. However, with the data, we can still run a causal analysis to estimate the effects of different factors on the outcome. We will be utilizing a Python package, <code class="inlineCode">dowhy</code>, to run causal analysis based on the marketing campaign data. We will be using bank churn data as an example to discuss how a causal analysis can <a id="_idIndexMarker250"/>be done to unveil the causes and effects of certain variables on the marketing outcome.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>:</p>
      <p class="normal"><a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Churn.ipynb "><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/blob/main/ch.3/Why%20People%20Churn.ipynb</span></a></p>
      <p class="normal"><strong class="keyWord">Data source</strong>:</p>
      <p class="normal"><a href="https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling "><span class="url">https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling</span></a></p>
      <p class="normal">Installation of the <code class="inlineCode">dowhy</code> package:</p>
      <pre class="programlisting con"><code class="hljs-con">pip install dowhy
</code></pre>
      <p class="normal">(Note: you may require additional installations for the <code class="inlineCode">dowhy</code> package. Please refer to the official documentation at <a href="https://pypi.org/project/dowhy/"><span class="url">https://pypi.org/project/dowhy/</span></a>.)</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-85">Causal effect estimation</h2>
    <p class="normal">The first <a id="_idIndexMarker251"/>step to estimating the causal effects <a id="_idIndexMarker252"/>of certain variables on the outcome is defining the problem with some assumptions. Let’s take the bank churn dataset as an example. First, when you load the data into a DataFrame, you will see variables as in the following:</p>
    <pre class="programlisting code"><code class="hljs-code">df.info()
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_11.png"/></figure>
    <p class="packt_figref">Figure 3.11: Snapshot of the DataFrame</p>
    <p class="normal">Here, the <code class="inlineCode">Exited</code> column is the target outcome we are interested in analyzing the causes for. As you <a id="_idIndexMarker253"/>may notice, there are several other columns <a id="_idIndexMarker254"/>that may be interesting to see causal effects on customer churn. Some may be interested in seeing whether a high or low credit score affects customer churn. Some others may be interested in seeing whether a long or short tenure affects customer churn. You may also be curious whether having multiple products affects customer churn.</p>
    <p class="normal">For our exercise in this chapter, we will define our problem or a causal effect of our interest as follows:</p>
    <blockquote class="packt_quote">
      <p class="quote">“Does having multiple products affect customer churn?”</p>
    </blockquote>
    <p class="normal">With this problem definition, we can build some assumptions that eventually lead to customer churn:</p>
    <ul>
      <li class="bulletList">Having multiple products may affect the churn rate, as it is likely that the people with multiple products will find it more difficult or cumbersome to switch to different banks.</li>
      <li class="bulletList"><code class="inlineCode">CreditScore</code> may affect whether a customer can have multiple products or not, as the credit score is used to decide whether a person is qualified to have certain bank products or not, such as credit cards or term loans.</li>
      <li class="bulletList"><code class="inlineCode">Age</code> can also affect whether a customer has multiple products or not, as the older you are, the more likely you will have needed more bank products.</li>
      <li class="bulletList">Similarly, <code class="inlineCode">Tenure</code>, <code class="inlineCode">Balance</code>, and <code class="inlineCode">Salary</code> can affect whether a customer may have multiple products or not. These factors may also affect customer churn.</li>
    </ul>
    <p class="normal">With these <a id="_idIndexMarker255"/>assumptions and the problem <a id="_idIndexMarker256"/>statement, let’s analyze the causal effect of a <code class="inlineCode">MultipleProduct</code> variable on <a id="_idIndexMarker257"/>customer churn, which, using a do-calculus notation, can be written as:</p>
    <figure class="mediaobject"><a id="_idIndexMarker258"/><img alt="" src="../Images/B30999_03_010.png"/></figure>
    <p class="normal">This, in turn, means the expected change in the likelihood of customer churn due to a change in the variable <code class="inlineCode">MultipleProduct</code>. As with our assumptions, there can be a set of other variables that affect the outcome and we may be interested in estimating the causal effect of these covariates on the outcome. In our example, these covariates are <code class="inlineCode">Tenure</code>, <code class="inlineCode">Balance</code>, and <code class="inlineCode">Salary</code>, and the estimated causal effect of these covariates on customer churn can be written as:</p>
    <figure class="mediaobject"><a id="_idIndexMarker259"/><img alt="" src="../Images/B30999_03_011.png"/></figure>
    <h3 class="heading-3" id="_idParaDest-86">Causal model</h3>
    <p class="normal">With the <a id="_idIndexMarker260"/>pre-defined problem statement and our <a id="_idIndexMarker261"/>assumptions, let’s start building a causal model.</p>
    <p class="normal">First, we will need to build a variable that we will use as the treatment. Take a look at the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">"MultipleProduct"</span>] = df[<span class="hljs-string">"NumOfProducts"</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">In our dataset, there is a column called <code class="inlineCode">NumOfProducts</code>. We are simply encoding this into 0s and 1s, where customers with more than one product will be encoded as 1s and the rest as 0s.</p>
    <p class="normal">Now, we will <a id="_idIndexMarker262"/>use the <code class="inlineCode">CausalModel</code> class within the <code class="inlineCode">dowhy</code> package to define <a id="_idIndexMarker263"/>the causal model based on our assumptions, as in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> dowhy <span class="hljs-keyword">import</span> CausalModel
model = CausalModel(
    data=df,
    treatment=<span class="hljs-string">"MultipleProduct"</span>,
    outcome=<span class="hljs-string">"Exited"</span>,
    common_causes=[<span class="hljs-string">"Balance"</span>, <span class="hljs-string">"EstimatedSalary"</span>, <span class="hljs-string">"Tenure"</span>],
    instruments=[<span class="hljs-string">"Age"</span>, <span class="hljs-string">"CreditScore"</span>]
)
</code></pre>
    <p class="normal">As you can see from this code, we defined the newly created variable, <code class="inlineCode">MultipleProduct</code>, as the treatment for which we are interested in learning the causal effect on the outcome, <code class="inlineCode">Exited</code>. We defined <code class="inlineCode">Balance</code>, <code class="inlineCode">EstimatedSalary</code>, and <code class="inlineCode">Tenure</code> as common causes that may have causal effects on the customer churn and on whether a customer may have multiple products or not. Lastly, we defined <code class="inlineCode">Age</code> and <code class="inlineCode">CreditScore</code> as instruments as they may affect a customer’s ability to have multiple products.</p>
    <p class="normal">You can visualize this causal model using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">model.view_model()
</code></pre>
    <p class="normal">The resulting causal graph looks as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_12.png"/></figure>
    <p class="packt_figref">Figure 3.12: Visualization of the causal graph</p>
    <p class="normal">Note that <a id="_idIndexMarker264"/>the actual output may differ each <a id="_idIndexMarker265"/>time this code is run.</p>
    <p class="normal">Here, the instrument variables, <code class="inlineCode">CreditScore</code> and <code class="inlineCode">Age</code>, affect the treatment variable, <code class="inlineCode">MultipleProduct</code>, which affects the outcome variable, <code class="inlineCode">Exited</code>, and the covariates or common causes, <code class="inlineCode">Balance</code>, <code class="inlineCode">EstimatedSalary</code>, and <code class="inlineCode">Tenure</code>, affect the treatment variable as well as the outcome variable.</p>
    <p class="normal">Then, we can have the model identify the causal effects, as with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">estimands = model.identify_effect()
<span class="hljs-built_in">print</span>(estimands)
</code></pre>
    <p class="normal">This code will show the following expressions for the causal effect under our assumptions:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_13.png"/></figure>
    <p class="packt_figref">Figure 3.13: Code expressions for causal effect</p>
    <p class="normal">This should <a id="_idIndexMarker266"/>look familiar as this is the same as the causal effect formulas <a id="_idIndexMarker267"/>we discussed previously.</p>
    <h3 class="heading-3" id="_idParaDest-87">Estimation</h3>
    <p class="normal">With the <a id="_idIndexMarker268"/>causal model defined, we can now estimate <a id="_idIndexMarker269"/>the effects of the treatment variable, <code class="inlineCode">MultipleProduct</code>. The <code class="inlineCode">estimate_effect</code> function makes the estimation of these effects straightforward, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">estimate = model.estimate_effect(
    estimands,
    method_name = <span class="hljs-string">"backdoor.propensity_score_weighting"</span>
)
<span class="hljs-built_in">print</span>(estimate)
</code></pre>
    <p class="normal">This should produce the following results:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_14.png"/></figure>
    <p class="packt_figref">Figure 3.14: Causal estimate results</p>
    <p class="normal">Based on this result, we estimate the mean effect of the treatment variable, <code class="inlineCode">MultipleProduct</code>, on the outcome, <code class="inlineCode">Exited</code>, to be <code class="inlineCode">-0.1390</code>. This means that having multiple <a id="_idIndexMarker270"/>products causes a decrease in the probability of customer churn of around 14%. This confirms our assumption that having multiple products will create stickiness toward the bank and reduce the chance of customer churn.</p>
    <div class="note">
      <p class="normal">There are various methods you can use for estimation methods. To name a few:</p>
      <ul>
        <li class="bulletList">Linear regression</li>
        <li class="bulletList">Distance matching</li>
        <li class="bulletList">Propensity score stratification</li>
        <li class="bulletList">Propensity score matching</li>
        <li class="bulletList">Propensity score weighting</li>
      </ul>
      <p class="normal">We used propensity score weighting for our example, but we recommend trying different estimation methods and analyzing how they affect the causal effect estimations.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-88">Refutation</h3>
    <p class="normal">Once we have estimated the causal effect of the treatment on the outcome, we need to validate it. This <a id="_idIndexMarker271"/>assumption validation step is called <strong class="keyWord">refutation</strong>. We are <a id="_idIndexMarker272"/>going to run robustness checks of our assumptions with a few approaches.</p>
    <p class="normal">The first approach is to introduce a random common cause or covariate variable. You can run this check as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">refute_results = model.refute_estimate(
    estimands,
    estimate,
    <span class="hljs-string">"random_common_cause"</span>
)
<span class="hljs-built_in">print</span>(refute_results)
</code></pre>
    <p class="normal">The output looks like the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Refute: Add a random common cause
Estimated effect:-0.13904650583124545
New effect:-0.13904650583124542
</code></pre>
    <p class="normal">We are using the <code class="inlineCode">refute_estimate</code> function within our causal model, <code class="inlineCode">model</code>, with the argument ″<code class="inlineCode">random_common_cause</code>". This refutation method introduces a new, randomly generated variable to test if the causal estimate significantly changes, helping assess the model’s robustness against omitted variable bias. As you can see from the output, the original estimated effect and the new effect after introducing a random common cause are the same with about -0.1390 as the estimated causal effect value. This confirms the fact that our assumption was correct.</p>
    <p class="normal">The second approach to refute our assumption is to randomly subsample the data and test on this random subset. You can run this check as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">refute_results = model.refute_estimate(
    estimands,
    estimate,
    <span class="hljs-string">"data_subset_refuter"</span>
)
<span class="hljs-built_in">print</span>(refute_results)
</code></pre>
    <p class="normal">The output looks like the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Refute: Use a subset of data
Estimated effect:-0.13904650583124545
New effect:-0.13842175180225635
</code></pre>
    <p class="normal">Similar to the previous case, we used the same <code class="inlineCode">refute_estimate</code> function but with the ″<code class="inlineCode">data_subset_refuter</code>" argument this time. The output suggests that the original estimated effect and the new estimated effect with a random subset of the data are very close. This <a id="_idIndexMarker273"/>confirms our assumption was correct <a id="_idIndexMarker274"/>about the causal relationship between the <code class="inlineCode">MultipleProduct</code> variable and the <code class="inlineCode">Exited</code> outcome variable and is a generalizable result across the dataset.</p>
    <div class="note">
      <p class="normal">Similar to the multiple estimation method options, there are multiple ways you can run refutations. We have experimented with random common cause addition and random subset replacement in this chapter. However, there are various other ways you can validate your assumptions. To name a few:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Replace treatment with placebo</strong>: Replace the treatment variable with one known not to have a causal effect on the outcome.</li>
        <li class="bulletList"><strong class="keyWord">Replace with dummy outcome</strong>: Instead of replacing the treatment variable, replace the outcome variable with one unrelated to the treatment.</li>
        <li class="bulletList"><strong class="keyWord">Refute with bootstrapped random sample</strong>: Generate multiple bootstrapped samples from the data and estimate the causal effect on each sample.</li>
      </ul>
      <p class="normal">We recommend trying different refutation methods as well! The examples in the official documentation may be a set of useful resources and can be found here: <a href="https://www.pywhy.org/dowhy/v0.11.1/example_notebooks/nb_index.html"><span class="url">https://www.pywhy.org/dowhy/v0.11.1/example_notebooks/nb_index.html</span></a>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-89">Graphical causal model</h2>
    <p class="normal">While the <a id="_idIndexMarker275"/>previous causal effect estimation <a id="_idIndexMarker276"/>was to analyze and identify the effects of treatment variables, the <strong class="keyWord">graphical causal model</strong> enables the root cause analysis and the analysis of causal effects and linkages of various factors. This will be a great tool to have when you want to understand the causal effects of underlying variables and how they eventually lead to certain outcomes. </p>
    <p class="normal">With the same assumptions as previously, we will learn how to use the <code class="inlineCode">dowhy</code> graphical causal model to better understand causal linkages among the variables.</p>
    <h3 class="heading-3" id="_idParaDest-90">Causal influence</h3>
    <p class="normal">One of <a id="_idIndexMarker277"/>the key differences using the graphical <a id="_idIndexMarker278"/>causal model is that we need to build a directed graph, where we define a start node to the next node. Let’s discuss this further with an example by running the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx
causal_graph = nx.DiGraph([
    (<span class="hljs-string">'CreditScore'</span>, <span class="hljs-string">'MultipleProduct'</span>),
    (<span class="hljs-string">'</span><span class="hljs-string">Age'</span>, <span class="hljs-string">'MultipleProduct'</span>),
    (<span class="hljs-string">'MultipleProduct'</span>, <span class="hljs-string">'Exited'</span>),
    (<span class="hljs-string">'Balance'</span>, <span class="hljs-string">'MultipleProduct'</span>),
    (<span class="hljs-string">'EstimatedSalary'</span>, <span class="hljs-string">'MultipleProduct'</span>),
    (<span class="hljs-string">'Tenure'</span>, <span class="hljs-string">'MultipleProduct'</span>),
    (<span class="hljs-string">'</span><span class="hljs-string">Balance'</span>, <span class="hljs-string">'Exited'</span>),
    (<span class="hljs-string">'EstimatedSalary'</span>, <span class="hljs-string">'Exited'</span>),
    (<span class="hljs-string">'Tenure'</span>, <span class="hljs-string">'Exited'</span>),
])
</code></pre>
    <div class="packt_tip">
      <p class="normal">To install the <code class="inlineCode">networkx</code> package:</p>
      <pre class="programlisting con"><code class="hljs-con">pip install networkx
</code></pre>
    </div>
    <p class="normal">As you can see, we are using the <code class="inlineCode">networkx</code> package’s <code class="inlineCode">DiGraph</code> class to create a directed graph. You can visualize this directed graph with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">    nx.draw_networkx(causal_graph, arrows=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">The output will look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_15.png"/></figure>
    <p class="packt_figref">Figure 3.15: Directed graph</p>
    <p class="normal">As you may notice, this is a similar graph to one we saw in the previous section, where there <a id="_idIndexMarker279"/>are directed edges that go from <code class="inlineCode">Age</code> and <code class="inlineCode">CreditScore</code> to <code class="inlineCode">MultipleProduct</code>. Similarly, there are also directed edges that go from <code class="inlineCode">Balance</code>, <code class="inlineCode">EstimatedSalary</code>, and <code class="inlineCode">Tenure</code> to <code class="inlineCode">MultipleProduct</code> and <code class="inlineCode">Exited</code>. Finally, there is a directed <a id="_idIndexMarker280"/>edge that goes from <code class="inlineCode">MultipleProduct</code> to <code class="inlineCode">Exited</code>. In summary, this is a visualization of our assumptions about how different variables may have causal effects on one another.</p>
    <p class="normal">With this directed graph, we can now fit a graphical causal model as in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> dowhy <span class="hljs-keyword">import</span> gcm
scm = gcm.StructuralCausalModel(causal_graph)
gcm.auto.assign_causal_mechanisms(scm, df)
gcm.fit(scm, df)
</code></pre>
    <p class="normal">Here, we are using the <code class="inlineCode">gcm</code> module in the <code class="inlineCode">dowhy</code> package and defining the causal model with the <code class="inlineCode">StructuralCausalModel</code> class within the <code class="inlineCode">gcm</code> module. Then, we assign our dataset to each node in the graph with the <code class="inlineCode">assign_causal_mechanisms</code> function and, finally, fit the graphical causal model with the <code class="inlineCode">fit</code> function and the structural causal model and dataset as the input to the <code class="inlineCode">fit</code> function.</p>
    <p class="normal">Now that <a id="_idIndexMarker281"/>our graphical causal model has learned <a id="_idIndexMarker282"/>the causal effects of various variables on the outcome, we can quantify these learned relationships using the <code class="inlineCode">arrow_strength</code> function, as in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">arrow_strengths = gcm.arrow_strength(scm, target_node=<span class="hljs-string">'Exited'</span>)
</code></pre>
    <p class="normal">You can visualize this by using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">total = <span class="hljs-built_in">sum</span>(arrow_strengths.values())
arrow_strengths_perc = {key: val/total*<span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> arrow_strengths.items()}
gcm.util.plot(
    causal_graph,
    causal_strengths=arrow_strengths_perc,
    figure_size=[<span class="hljs-number">8</span>, <span class="hljs-number">5</span>]
)
</code></pre>
    <p class="normal">As you may notice from this code, we are normalizing the arrow strengths into percentages of the total in the first two lines of this code. The resulting graph should look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_16.png"/></figure>
    <p class="packt_figref">Figure 3.16: Graph with arrow strengths</p>
    <p class="normal">As this graph suggests, there is the strongest causal relationship between <code class="inlineCode">MultipleProduct</code> and <code class="inlineCode">Exited</code>, with about 45% contribution, and <code class="inlineCode">Tenure</code> follows as the second most impactful variable for customer churn with about 25% contribution. This directed graph with estimated causal impacts on the outcome provides great insight for understanding the quantified causal effects of different variables on the outcome.</p>
    <p class="normal">While the <a id="_idIndexMarker283"/>arrow strengths show us the impacts of direct linkages to the outcome, they do not show us indirect relationships between the instrument variables, <code class="inlineCode">Age</code> and <code class="inlineCode">CreditScore</code>, and the outcome variable, <code class="inlineCode">Exited</code>. One approach we can take to <a id="_idIndexMarker284"/>compute and visualize the impacts of all the causal factors on the outcome variable is to quantify the intrinsic causal contributions that attribute the variance of the outcome variable to all the upstream nodes in the directed graph.</p>
    <p class="normal">We can use the <code class="inlineCode">intrinsic_causal_influence</code> function to quantify the intrinsic causal contributions, as in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">influence = gcm.intrinsic_causal_influence(
    scm,
    target_node=<span class="hljs-string">'Exited'</span>,
    num_samples_randomization=<span class="hljs-number">100</span>
)
</code></pre>
    <p class="normal">Here, we are using 100 random samples for evaluations, as you see from the <code class="inlineCode">num_samples_randomization</code> parameter input. Before we visualize the quantified intrinsic causal influences, we will normalize the scale so that they sum up to 100% first and then visualize them with a bar plot, using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">total = <span class="hljs-built_in">sum</span>([val <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> influence.items() <span class="hljs-keyword">if</span> key != <span class="hljs-string">"Exited"</span>])
influence_perc = {key: val/total*<span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> influence.items() <span class="hljs-keyword">if</span> key != <span class="hljs-string">"Exited"</span>}
xlabels = <span class="hljs-built_in">sorted</span>(influence_perc.keys())
yvals = [influence_perc[x] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xlabels]
plt.bar(xlabels, yvals)
plt.xticks(rotation=<span class="hljs-number">45</span>)
plt.grid()
plt.ylabel(<span class="hljs-string">"Variance attribution in %"</span>)
plt.show()
</code></pre>
    <p class="normal">The output <a id="_idIndexMarker285"/>of this code should produce a bar <a id="_idIndexMarker286"/>plot like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_17.png"/></figure>
    <p class="packt_figref">Figure 3.17: Bar plot of variance attribution for each factor</p>
    <p class="normal">As expected, the variable <code class="inlineCode">MultipleProduct</code> has the highest contribution to the variance of the outcome variable among all the variables. <code class="inlineCode">Balance</code> comes next as the second most contributing variable. The variables <code class="inlineCode">Age</code> and <code class="inlineCode">CreditScore</code> seem to have very little attribution to the variance of the outcome, <code class="inlineCode">Exited</code>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">What is a Shapley value?</strong></p>
      <p class="normal">You may <a id="_idIndexMarker287"/>notice that “Shapley values” appear while running causal inference code. Simply put, Shapley values are the estimations of the degrees of impact of individual variables on the target variable. If you would like to learn more, we suggest you take a look at the package documentation on how it computes the feature relevance here: <a href="https://www.pywhy.org/dowhy/main/user_guide/causal_tasks/root_causing_and_explaining/feature_relevance.html"><span class="url">https://www.pywhy.org/dowhy/main/user_guide/causal_tasks/root_causing_and_explaining/feature_relevance.html</span></a>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-91">Anomaly attribution</h3>
    <p class="normal">So far, we have seen ways to analyze the overall causal effects of individual variables on the outcome variable. However, there will be cases where we want to dig deeper into understanding <a id="_idIndexMarker288"/>which variables may have contributed in what way. For example, within our dataset, one may be curious about how <a id="_idIndexMarker289"/>a certain customer’s <code class="inlineCode">Balance</code> amount may have caused them to churn or how that person’s <code class="inlineCode">Tenure</code> with the bank may have impacted their churn. In order to figure out the causal contributions of different variables for individual customer churn, we can use the <code class="inlineCode">attribute_anomalies</code> function, as in the following:</p>
    <pre class="programlisting code"><code class="hljs-code">attributions = gcm.attribute_anomalies(
    scm,
    target_node=<span class="hljs-string">'Exited'</span>,
    anomaly_samples=df.loc[df[<span class="hljs-string">"Exited"</span>] == <span class="hljs-number">1</span>].iloc[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>]
)
</code></pre>
    <p class="normal">In our example, we chose the first customer who churned as an example, as you can see from the input for the <code class="inlineCode">anomaly_samples</code> parameter. We can visualize the individual attributions on the outcome using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">total = <span class="hljs-built_in">sum</span>([<span class="hljs-built_in">abs</span>(val[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> attributions.items() <span class="hljs-keyword">if</span> key != <span class="hljs-string">"Exited"</span>])
attributions_perc = {
    key: val[<span class="hljs-number">0</span>]/total*<span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> attributions.items() <span class="hljs-keyword">if</span> key != <span class="hljs-string">"Exited"</span>
}
xlabels = <span class="hljs-built_in">sorted</span>(attributions_perc.keys())
yvals = [attributions_perc[x] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xlabels]
plt.bar(xlabels, yvals)
plt.xticks(rotation=<span class="hljs-number">45</span>)
plt.grid()
plt.ylabel(<span class="hljs-string">"Anomaly Attribution (%)"</span>)
plt.show()
</code></pre>
    <p class="normal">As this <a id="_idIndexMarker290"/>code suggests, we are normalizing the attributions so that they <a id="_idIndexMarker291"/>sum up to 100% and then visualizing individual attributions with a bar plot. </p>
    <p class="normal">The resulting bar plot will look like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_18.png"/></figure>
    <p class="packt_figref">Figure 3.18: Bar plot of anomaly attribution for each factor</p>
    <p class="normal">Before we go deeper into the interpretations of this bar plot, let’s first look at the sample we chose for quantifying individual causal contributions. Take a look at the following code to take a closer look at an individual sample:</p>
    <pre class="programlisting code"><code class="hljs-code">    df.loc[df[<span class="hljs-string">"Exited"</span>] == <span class="hljs-number">1</span>].iloc[<span class="hljs-number">0</span>][<span class="hljs-built_in">list</span>(attributions_perc.keys())]
</code></pre>
    <p class="normal">The output of this code will look as follows:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_19.png"/></figure>
    <figure class="mediaobject">Figure 3.19: Selected sample</figure>
    <p class="normal">If we combine the attribution results with this customer’s info, we can dissect which factors <a id="_idIndexMarker292"/>were the most influential that ended up making <a id="_idIndexMarker293"/>this customer churn. We see from this customer’s record that this person only had one product with the bank and this was the most contributing factor to why this customer churned. On the other hand, <code class="inlineCode">Balance</code>, <code class="inlineCode">EstimatedSalary</code>, and <code class="inlineCode">Tenure</code> actually reduced the chance of customer churn as they have negative scores. Let’s look at another sample, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">    df.loc[df[<span class="hljs-string">"Exited"</span>] == <span class="hljs-number">1</span>].iloc[<span class="hljs-number">1</span>][<span class="hljs-built_in">list</span>(attributions_perc.keys())]
</code></pre>
    <p class="normal">This has output like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_20.png"/></figure>
    <p class="packt_figref">Figure 3.20: Selected sample</p>
    <p class="normal">This customer’s causal contribution bar plot looks like the following:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_03_21.png"/></figure>
    <p class="packt_figref">Figure 3.21: Bar plot of customer’s causal contribution</p>
    <p class="normal">As you can see from this second example, this customer had a much higher balance, longer tenure, and more than one product with the bank, compared to the first example. For this customer, this higher balance, longer tenure, and higher estimated salary had <a id="_idIndexMarker294"/>a positive impact on why the customer churned, while the <a id="_idIndexMarker295"/>fact that this customer had more than one product reduced the chance of this customer churning.</p>
    <p class="normal">As we have seen from these examples, causal analysis and causal effect estimations bring deep insights into why people churn from your products. When used along with other methods and AI/ML models, this causal analysis will bring actionable insights that you can employ in your future marketing campaigns.</p>
    <h1 class="heading-1" id="_idParaDest-92">Summary</h1>
    <p class="normal">In this chapter, we gained an in-depth understanding of the factors affecting certain customer behaviors. We explored how <strong class="keyWord">regression analysis</strong> can help us understand the directional relationships between various factors and the outcome of customer behavior. Using our auto insurance marketing dataset as an example, we saw how to implement the <code class="inlineCode">statsmodels</code> package in Python to run regression analysis and unveil the successes behind engagement rate marketing campaigns. We also discussed how <strong class="keyWord">decision trees</strong> can help us identify complex interactions that result in certain outcomes. Using a bank marketing dataset as an example and the <code class="inlineCode">scikit-learn</code> package in Python, we successfully built a decision tree that unveiled the hidden interactions among various factors that lead to customer conversions. Lastly, with the bank churn dataset and the <code class="inlineCode">dowhy</code> package in Python, we saw how <strong class="keyWord">causal analysis</strong> can bring deep insights into the root causes and directional contributions to the outcome of an event.</p>
    <p class="normal">In the next chapter, we are going to explore time-series data. In marketing, there are certain temporal patterns that repeat in certain ways. We will be discussing how to break down time-series data into overall trend, seasonality, and random noise components and how to utilize the trends and seasonalities that are identified through time-series decomposition for marketing initiatives, as well as how to do time-series forecasting, which can equip you with better planning and preparedness for certain events that may happen in the future.</p>
    <h1 class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</p>
    <p class="normal"><a href="https://packt.link/genai"><span class="url">https://packt.link/genai</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code12856128601808671.png"/></p>
  </div>
</body></html>