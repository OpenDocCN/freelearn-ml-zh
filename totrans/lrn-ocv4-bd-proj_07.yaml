- en: Detecting Face Parts and Overlaying Masks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](83822325-00be-4874-813c-b90097030d85.xhtml), *Learning Object
    Classification*, we learned about object classification and how machine learning
    can be used to achieve it. In this chapter, we are going to learn how to detect
    and track different face parts. We will start the discussion by understanding
    the face detection pipeline and how it's built. We will then use this framework
    to detect face parts, such as the eyes, ears, mouth, and nose. Finally, we will
    learn how to overlay funny masks on these face parts in a live video.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, we should be familiar with the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Haar cascades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integral images and why we need them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a generic face detection pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting and tracking faces, eyes, ears, noses, and mouths in a live video
    stream from the webcam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically overlaying a face mask, sunglasses, and a funny nose on a person's
    face in a video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires basic familiarity with the C++ programming language. All
    the code used in this chapter can be downloaded from the following GitHub link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_07](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_07). The
    code can be executed on any operating system, though it is only tested on Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2SlpTK6](http://bit.ly/2SlpTK6)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Haar cascades
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Haar cascades are cascade classifiers that are based on Haar features. What
    is a cascade classifier? It is simply a concatenation of a set of weak classifiers
    that can be used to create a strong classifier. What do we mean by **weak** and
    **strong** classifiers? Weak classifiers are classifiers whose performance is
    limited. They don''t have the ability to classify everything correctly. If you
    keep the problem really simple, they might perform at an acceptable level. Strong
    classifiers, on the other hand, are really good at classifying our data correctly.
    We will see how it all comes together in the next couple of paragraphs. Another
    important part of Haar cascades is **Haar features**. These features are simple
    summations of rectangles and differences of those areas across the image. Let''s
    consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50bdb239-0bd8-49d2-af12-631b7735f346.png)'
  prefs: []
  type: TYPE_IMG
- en: If we want to compute the Haar features for region ABCD, we just need to compute
    the difference between the white pixels and the blue pixels in that region. As
    we can see from the four diagrams, we use different patterns to build Haar features.
    There are a lot of other patterns that are used as well. We do this at multiple
    scales to make the system scale-invariant. When we say multiple scales, we just
    scale the image down to compute the same features again. This way, we can make
    it robust against size variations of a given object.
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, this concatenation system is a very good method for detecting
    objects in an image. In 2001, Paul Viola and Michael Jones published a seminal
    paper where they described a fast and effective method for object detection. If
    you are interested in learning more about it, you can check out their paper at [http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf](http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive deeper into it to understand what they actually did. They basically
    described an algorithm that uses a boosted cascade of simple classifiers. This
    system is used to build a strong classifier that can perform really well. Why
    did they use these simple classifiers instead of complex classifiers, which can
    be more accurate? Well, using this technique they were able to avoid the problem
    of having to build a single classifier that can perform with high precision. These
    single-step classifiers tend to be complex and computationally intensive. The
    reason their technique works so well is because the simple classifiers can be
    weak learners, which means they don't need to be complex. Consider the problem
    of building a table detector. We want to build a system that will automatically
    learn what a table looks like. Based on that knowledge, it should be able to identify
    whether there is a table in any given image. To build this system, the first step
    is to collect images that can be used to train our system. There are a lot of
    techniques available in the machine learning world that can be used to train a
    system such as this. Bear in mind that we need to collect a lot of table and non-table
    images if we want our system to perform well. In machine learning lingo, table
    images are called **positive** samples and the non-table images are called **negative**
    samples. Our system will ingest this data and then learn to differentiate between
    these two classes. In order to build a real-time system, we need to keep our classifier
    nice and simple. The only concern is that simple classifiers are not very accurate.
    If we try to make them more accurate, then the process will end up being computationally
    intensive, and hence slow. This trade-off between accuracy and speed is very common
    in machine learning. So, we overcome this problem by concatenating a bunch of
    weak classifiers to create a strong and unified classifier. We don't need the
    weak classifiers to be very accurate. To ensure the quality of the overall classifier,
    Viola and Jones have described a nifty technique in the cascading step. You can
    go through the paper to understand the full system.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the general pipeline, let's see how to build a system
    that can detect faces in a live video. The first step is to extract features from
    all the images. In this case, the algorithms need these features to learn and
    understand what faces look like. They used Haar features in their paper to build
    the feature vectors. Once we extract these features, we pass them through a cascade
    of classifiers. We just check all the different rectangular sub-regions and keep
    discarding the ones that don't have faces in them. This way, we arrive at the
    final answer quickly to see whether a given rectangle contains a face or not.
  prefs: []
  type: TYPE_NORMAL
- en: What are integral images?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to extract these Haar features, we will have to calculate the sum
    of the pixel values enclosed in many rectangular regions of the image. To make
    it scale-invariant, we are required to compute these areas at multiple scales
    (for various rectangle sizes). Implemented naively, this would be a very computationally-intensive
    process; we would have to iterate over all the pixels of each rectangle, including
    reading the same pixels multiple times if they are contained in different overlapping
    rectangles. If you want to build a system that can run in real-time, you cannot
    spend so much time in computation. We need to find a way to avoid this huge redundancy
    during the area computation because we iterate over the same pixels multiple times.
    To avoid it, we can use something called integral images. These images can be
    initialized at a linear time (by iterating only twice over the image) and then
    provide the sum of the pixels inside any rectangle of any size by reading only
    four values. To understand it better, let''s look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17189123-7d03-4a17-a954-222e9ca17c79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to calculate the area of any rectangle in our diagram, we don''t
    have to iterate through all the pixels in that region. Let''s consider a rectangle
    formed by the top-left point in the image and any point, P, as the opposite corner.
    Let A[P] denote the area of this rectangle. For example, in the previous image,
    A[B] denotes the area of the 5 x 2 rectangle formed by taking the top-left point
    and **B** as opposite corners. Let''s look at the following diagram for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb75616a-63ba-49d5-8c05-d2cdfae54485.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s consider the top-left square in the previous image. The blue pixels
    indicate the the area between the top-left pixel and point **A**. This is denoted
    by A[A]. The remaining diagrams are denoted by their respective names: A[B], A[C],
    and A[D]. Now, if we want to calculate the area of the ABCD rectangle, as shown
    in the preceding diagram, we would use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Area of the rectangle**: *ABCD* = *A[C]* - (*A[B]* + *A[D]* - *A[A]*)'
  prefs: []
  type: TYPE_NORMAL
- en: What's so special about this particular formula? As we know, extracting Haar
    features from the image includes computing these summations and we would have
    to do it for a lot of rectangles at multiple scales in the image. A lot of those
    calculations are repetitive because we would be iterating over the same pixels
    over and over again. It is so slow that building a real-time system wouldn't be
    feasible. Hence, we need this formula. As you can see, we don't have to iterate
    over the same pixels multiple times. If we want to compute the area of any rectangle,
    all the values on the right-hand side of the preceding equation are readily available
    in our integral image. We just pick up the right values, substitute them in the
    preceding equation, and extract the features.
  prefs: []
  type: TYPE_NORMAL
- en: Overlaying a face mask in a live video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV provides a nice face detection framework. We just need to load the cascade
    file and use it to detect the faces in an image. When we capture a video stream
    from the webcam, we can overlay funny masks on our faces. It will look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b3881f9-9bf6-4642-864b-b47979fcd9dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the main parts of the code to see how to overlay this mask on
    the face in the input video stream. The full code is available in the downloadable
    code bundle provided along with this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a quick stop to see what happened here. We start reading input
    frames from the webcam and resize it to our size of choice. The captured frame
    is a color image and face detection works on grayscale images. So, we convert
    it to grayscale and equalize the histogram. Why do we need to equalize the histogram?
    We need to do this to compensate for any issues, such as lighting or saturation.
    If the image is too bright or too dark, the detection will be poor. So, we need
    to equalize the histogram to ensure that our image has a healthy range of pixel
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we know where the face is. So we extract the region of interest
    to overlay the mask in the right position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We isolate the pixels associated with the face mask. We want to overlay the
    mask in such a way that it doesn''t look like a rectangle. We want the exact boundaries
    of the overlaid object so that it looks natural. Let''s go ahead and overlay the
    mask now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What happened in the code?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing to note is that this code takes two input arguments—the **face
    cascade XML** file and the **mask image**. You can use the `haarcascade_frontalface_alt.xml`
    and `facemask.jpg` files that are provided under the `resources` folder. We need
    a classifier model that can be used to detect faces in an image and OpenCV provides
    a prebuilt XML file that can be used for this purpose. We use the `faceCascade.load()` function
    to load the XML file and also check whether the file is loaded correctly. We initiate
    the video-capture object to capture the input frames from the webcam. We then
    convert it to grayscale to run the detector. The `detectMultiScale` function is
    used to extract the boundaries of all the faces in the input image. We may have
    to scale down the image according to our needs, so the second argument in this
    function takes care of this. This scaling factor is the jump we take at each scale;
    since we need to look for faces at multiple scales, the next size will be 1.1
    times bigger than the current size. The last parameter is a threshold that specifies
    the number of adjacent rectangles needed to keep the current rectangle. It can
    be used to increase the robustness of the face detector. We start the `while`
    loop and keep detecting the face in every frame until the user presses the *Esc*
    key. Once we detect a face, we need to overlay a mask on it. We may have to modify
    the dimensions slightly to ensure that the mask fits nicely. This customization
    is slightly subjective and it depends on the mask that's being used. Now that
    we have extracted the region of interest, we need to place our mask on top of
    this region. If we overlay the mask with its white background, it will look weird.
    We have to extract the exact curvy boundaries of the mask and then overlay it.
    We want the skull mask pixels to be visible and the remaining area should be transparent.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the input mask has a white background. So, we create a mask by
    applying a threshold to the mask image. Using trial and error, we can see that
    a threshold of `240` works well. In the image, all the pixels with an intensity
    value greater than `240` will become `0`, and all others will become `255`. As
    far as the region of interest is concerned, we have to black out all the pixels
    in this region. To do that, we simply use the inverse of the mask that was just
    created. In the last step, we just add the masked versions to produce the final
    output image.
  prefs: []
  type: TYPE_NORMAL
- en: Get your sunglasses on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand how to detect faces, we can generalize that concept
    to detect different parts of the face. We will be using an eye detector to overlay
    sunglasses in a live video. It''s important to understand that the Viola-Jones
    framework can be applied to any object. The accuracy and robustness will depend
    on the uniqueness of the object. For example, the human face has very unique characteristics,
    so it''s easy to train our system to be robust. On the other hand, an object such
    as a towel is too generic, and it has no distinguishing characteristics as such,
    so it''s more difficult to build a robust towel detector. Once you build the eye
    detector and overlay the glasses, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27045d2d-9d41-4a3f-8b1a-1906be8ea46a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the main parts of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see here, we run the eye detector only in the face region. We don''t
    need to search the entire image for eyes because we know eyes will always be on
    a face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We detect the eyes and store them only when we find both of them. We then use
    their coordinates to determine which one is the left eye and which one is the
    right eye:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we adjusted the size of the sunglasses to fit the scale
    of our faces in the webcam. Let''s check the remaining code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Looking inside the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may have noticed that the flow of the code looks similar to the face detection
    code that we discussed in the *Overlaying a face mask in a live video* section.
    We load a face detection cascade classifier as well as the eye detection cascade
    classifier. Now, why do we need to load the face cascade classifier when we are
    detecting eyes? Well, we don't really need to use the face detector, but it helps
    us in limiting our search for the eyes' location. We know that the eyes are always
    located on somebody's face, so we can limit eye detection to the face region.
    The first step would be to detect the face and then run our eye detector code
    on this region. Since we would be operating on a smaller region, it would be faster
    and way more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: For each frame, we start by detecting the face. We then go ahead and detect
    the location of the eyes by operating on this region. After this step, we need
    to overlay the sunglasses. To do that, we need to resize the sunglasses image
    to make sure it fits our face. To get the proper scale, we can consider the distance
    between the two eyes that are being detected. We overlay the sunglasses only when
    we detect both eyes. That's why we run the eye detector first, collect all the
    centers, and then overlay the sunglasses. Once we have this, we just need to overlay
    the sunglasses mask. The principle used for masking is very similar to the principle
    we used to overlay the face mask. You may have to customize the sizing and position
    of the sunglasses, depending on what you want. You can play around with different
    types of sunglasses to see what they look like.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking the nose, mouth, and ears
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you know how to track different things using the framework, you can
    try tracking your nose, mouth, and ears too. Let''s use a nose detector to overlay
    a funny nose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b51f1ac4-21ab-427a-bed0-ad67d269f491.png)'
  prefs: []
  type: TYPE_IMG
- en: You can refer to the code files for a full implementation of this detector.
    The `haarcascade_mcs_nose.xml`, `haarcascade_mcs_mouth.xml`, `haarcascade_mcs_leftear.xml`,
    and `haarcascade_mcs_rightear.xml` cascade files can be used to track the different
    face parts. Play around with them and try to overlay a mustache or Dracula ears
    on yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Haar cascades and integral images. We looked at
    how the face detection pipeline is built. We learned how to detect and track faces
    in a live video stream. We discussed using the face detection framework to detect
    various face parts, such as eyes, ears, nose, and mouth. Finally, we learned how
    to overlay masks on the input image using the results of face part detection.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn about video surveillance, background
    removal, and morphological image processing.
  prefs: []
  type: TYPE_NORMAL
