<html><head></head><body>
		<div id="_idContainer120">
			<h1 id="_idParaDest-126"><em class="italic"><a id="_idTextAnchor125"/>Chapter 9</em>: Training ML Models at Scale in SageMaker Studio</h1>
			<p>A typical ML life cycle starts with prototyping and will transition to a production scale where the data gets larger, models get more complicated, and the runtime environment gets more complex. Getting a training job done requires the right set of tools. Distributed training using multiple computers to share the load addresses situations that involve large datasets and large models. However, as complex ML training jobs use more compute resources, and more costly infrastructure (such as <strong class="bold">Graphical Processing Units</strong> (<strong class="bold">GPUs</strong>)), being able to effectively train a complex ML model on large data is important for a data scientist and an ML engineer. Being able to see and monitor how a training script interacts with data and compute instances is critical to optimizing the model training strategy in the training script so that it is time- and cost-effective. Speaking of cost when training at a large scale, did you know you can easily save more than 70% when training models in SageMaker? SageMaker Studio makes training ML models at scale easier and cost-effective. </p>
			<p>In this chapter, we will be learning about the following:</p>
			<ul>
				<li>Performing distributed training in SageMaker Studio</li>
				<li>Monitoring model training and compute resources with SageMaker Debugger</li>
				<li>Managing long-running jobs with check-pointing and spot training</li>
			</ul>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>Technical requirements</h1>
			<p>For this chapter, you need to access the code provided at <a href="https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter09">https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter09</a>.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor127"/>Performing distributed training in SageMaker Studio</h1>
			<p>As the field of deep learning advances, ML models and training data are growing to a point <a id="_idIndexMarker576"/>that one single device is no longer <a id="_idIndexMarker577"/>sufficient for conducting effective model training. The neural networks are getting deeper and deeper, and gaining more and more parameters for training: </p>
			<ul>
				<li><strong class="bold">LeNet-5</strong>, one <a id="_idIndexMarker578"/>of the first <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) models <a id="_idIndexMarker579"/>proposed in 1989 that <a id="_idIndexMarker580"/>uses 2 convolutional layers and 3 dense layers, has around 60,000 trainable parameters. </li>
				<li><strong class="bold">AlexNet</strong>, a deeper <a id="_idIndexMarker581"/>CNN architecture <a id="_idIndexMarker582"/>with 5 layers of convolutional layers and 3 dense layers proposed in 2012, has around 62 million trainable parameters.</li>
				<li><strong class="bold">Bidirectional Transformers for Language Understanding</strong> (<strong class="bold">BERT</strong>), a language <a id="_idIndexMarker583"/>representation model using a transformer proposed <a id="_idIndexMarker584"/>in 2018, has 110 million and 340 million trainable parameters in the base and large models respectively.</li>
				<li><strong class="bold">Generative Pre-trained Transformer 2</strong> (<strong class="bold">GPT-2</strong>), a large transformer-based generative <a id="_idIndexMarker585"/>model proposed <a id="_idIndexMarker586"/>in 2019, has 1.5 billion trainable parameters. </li>
				<li><strong class="bold">GPT-3</strong> is the <a id="_idIndexMarker587"/>next version, proposed in 2020, <a id="_idIndexMarker588"/>that reaches 175 billion trainable parameters. </li>
			</ul>
			<p>Having more parameters to train means that there is a larger memory footprint during training. Additionally, the training data size needed to fit a complex model has also gone up significantly. For computer vision, one of the most commonly used training datasets, ImageNet, has 1.2 million <a id="_idIndexMarker589"/>images. For <strong class="bold">Natural Language Processing </strong>(<strong class="bold">NLP</strong>), GPT-3 is trained with 499 billion tokens, for example. </p>
			<p>However, the latest and greatest GPU device would still struggle to hold up for such training requirements. The latest GPU device from NVIDIA, the A100 Tensor Core GPU available on AWS P4d.24xlarge instances, has 40 GB of GPU memory, but it would not be sufficient <a id="_idIndexMarker590"/>to hold the GPT-3 <a id="_idIndexMarker591"/>model, which has 175 billion parameters, as such a network would need <em class="italic">175 x 109 x 4 bytes = 700 GB</em> when using the <em class="italic">FP32</em> precision. Therefore, developers are going beyond single GPU device training and resorting to distributed training – that is, training using multiple GPU devices and multiple compute instances.</p>
			<p>Let's understand why and how distributed training helps.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>Understanding the concept of distributed training</h2>
			<p>In ML model training, the training data is fed into the loss optimization process in order to compute <a id="_idIndexMarker592"/>the gradients and weights for the next step. When data and parameters are much larger, as in the case of deep learning, having a full dataset that fits into the optimization becomes less feasible due to the GPU memory available <a id="_idIndexMarker593"/>on the device. It is common to use the <strong class="bold">stochastic gradient descent optimization</strong> approach, which estimates the gradients with a subset (<strong class="bold">batch size</strong>) of the full training dataset in each step, to overcome the GPU memory limitation. However, when a model or each data point is too large to have a meaningful batch size for the model training, we will not be able to converge to an optimal, accurate model in a reasonable timeframe.</p>
			<p>Distributed training is a practice to distribute parts of the computation to multiple GPU devices and multiple compute instances (also called nodes), and synchronize the computation <a id="_idIndexMarker594"/>from all devices before <a id="_idIndexMarker595"/>proceeding to the next iteration. There are two strategies in distributed training: <strong class="bold">data parallelism</strong> and <strong class="bold">model parallelism</strong>. </p>
			<p>Data parallelism <a id="_idIndexMarker596"/>distributes the training dataset during epochs from disk to multiple devices and instances while each device contains a portion of data and a <em class="italic">complete replica</em> of the model. Each node performs a forward and backward propagation pass using different batches of data and shares trainable weight updates with other nodes for synchronization at the end of a pass. With data parallelism, you can increase the batch size by <em class="italic">n</em>-fold, where <em class="italic">n</em> is the number of GPU devices across nodes. An appropriately large batch size allows better generalization during the estimation of gradients and also reduces the number of steps needed to run through the entire pass (<strong class="bold">an epoch</strong>). </p>
			<p class="callout-heading">Note</p>
			<p class="callout">It has also been observed in practice that an overly large batch size will hurt the quality and generalization of a model. This is model- and dataset-dependent and requires experimentations and tuning to find out an appropriate batch size.</p>
			<p>Data parallelism <a id="_idIndexMarker597"/>is illustrated in <em class="italic">Figure 9.1</em>:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B17447_09_001.jpg" alt="Figure 9.1 – The training data is distributed across GPU devices in data parallelism. A complete replica of the model is placed on each GPU device&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – The training data is distributed across GPU devices in data parallelism. A complete replica of the model is placed on each GPU device</p>
			<p>Alternatively, model parallelism <a id="_idIndexMarker598"/>distributes a large model across nodes. Partitioning of a model is performed at a layers and a weights level. Each node possesses a partition of the model. Forward and backward propagations take place as a pipeline, with the data batches going through the model partitions on all nodes before the weight updates. To be more specific, each data batch is split into micro-batches and feeds into each part of the model, located on devices for forward and backward passes. With model parallelism, you can more effectively train a large model that needs a higher GPU memory footprint than a single GPU device using memory collectively from multiple GPU devices. Model parallelism is illustrated in <em class="italic">Figure 9.2</em>:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17447_09_002.jpg" alt="Figure 9.2 – The model is partitioned across GPU devices in model parallelism. The training data is split into micro-batches and fed into the GPUs, each of which has a part of the model as a pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – The model is partitioned across GPU devices in model parallelism. The training data is split into micro-batches and fed into the GPUs, each of which has a part of the model as a pipeline</p>
			<p><em class="italic">When should we use data parallelism or model parallelism?</em> It depends on the data size, batch, and model sizes in training. Data parallelism is suitable for situations when a single data <a id="_idIndexMarker599"/>point is too large to have a desirable batch size during training. The immediate trade-off of having a small batch size is having a longer runtime to finish an epoch. You may want to increase the batch size so that you can complete an epoch under a reasonable timeframe. You can use data parallelism to distribute a larger batch size to multiple GPU devices. However, if your model is large and takes up most GPU memory in a single device, you will not enjoy the scale benefit of data parallelism much. This is because, in data parallelism, an ML model is fully replicated onto each of the GPU devices, leaving little space for any data. You should use model parallelism when you have a large model in relation to the GPU memory.</p>
			<p>SageMaker makes running distributed training for large datasets and large models easy in the <a id="_idIndexMarker600"/>cloud. SageMaker's <strong class="bold">distributed training libraries</strong> support data parallelism and model parallelism for the two most popular deep learning <a id="_idIndexMarker601"/>frameworks, <strong class="bold">TensorFlow</strong> and <strong class="bold">PyTorch</strong>, when used in SageMaker. SageMaker's <strong class="bold">distributed data parallel library</strong> scales your model training with <a id="_idIndexMarker602"/>near-linear scaling efficiency, meaning that the reduction in training time <a id="_idIndexMarker603"/>in relation to the number of nodes is close to linear. SageMaker's <strong class="bold">distributed model parallel library</strong> automatically analyzes your neural network architecture and splits the model across GPU devices and orchestrates the pipeline execution efficiently.</p>
			<p>In the following sections, we'll learn how we can implement data parallelism and model parallelism <a id="_idIndexMarker604"/>in SageMaker Studio for our training scripts written in TensorFlow and PyTorch. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Both TensorFlow and PyTorch are supported by the two distributed training libraries. The distributed training concepts remain the same between the two deep learning frameworks. We will focus on TensorFlow for the data parallel library and PyTorch for the model parallel library.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>The data parallel library with TensorFlow</h2>
			<p>SageMaker's distributed data parallel library implements simple APIs that look similar to TensorFlow's <a id="_idIndexMarker605"/>way of performing model training in <a id="_idIndexMarker606"/>a distributed fashion but conduct distributed training that is optimized with AWS's compute infrastructure. This means that you can easily adopt SageMaker's API without making sophisticated changes to your existing distributed training code written in TensorFlow. If this is your first model training with distribution, we will demonstrate the modification needed to adapt SageMaker's distributed data parallel library to your existing model training script.</p>
			<p>Let's go to SageMaker Studio and start working with the <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb</strong> notebook. This example is built on top of the training example we walked through in <a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a>, <em class="italic">Building and Training ML Models with the SageMaker Studio IDE</em> (<a href="http://Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb">Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb</a>), where we trained a deep learning model using the TensorFlow Keras API on an IMDB review dataset. Back in <a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a>, <em class="italic">Building and Training ML Models with SageMaker Studio IDE</em>, we ran the training script on one <strong class="source-inline">ml.p3.2xlarge</strong> instance, which only has one NVIDIA Tesla V100 GPU. Now, in this chapter, we will use SageMaker's distributed data parallel library to extend the code to work with multiple GPU devices, either from an instance or from multiple instances. And remember that we can always <a id="_idIndexMarker607"/>easily specify the number of instances <a id="_idIndexMarker608"/>and the type of the instances in the <strong class="source-inline">sagemaker.tensorflow.TensorFlow</strong> estimator. Let's open the notebook and select the <strong class="bold">Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)</strong> kernel and an <strong class="bold">ml.t3.medium</strong> instance, and run the first six cells to prepare the SageMaker session and the dataset. The cell leading with <strong class="source-inline">%%writefile code/smdp_tensorflow_sentiment.py</strong> is where modification to adopt the distributed training script will go. Follow the next steps to see the changes that need to be made to enable the distributed data parallel library:</p>
			<ol>
				<li>First, import the TensorFlow module of the data parallel library:<p class="source-code">import smdistributed.dataparallel.tensorflow as sdp</p></li>
				<li>After the library is imported, we need to initialize the SageMaker distributed data parallel library in order to use it during runtime. We can implement it right after the <strong class="source-inline">import</strong> statements or in <strong class="source-inline">main</strong> (<strong class="source-inline">if __name__ == "__main__"</strong>):<p class="source-code">sdp.init()</p></li>
				<li>Then, we discover all the GPU devices available in the compute instance fleet and configure the GPUs so that they are aware of the ranking within an instance. If an instance has eight GPU devices, each of them will get assigned a rank from zero to seven. The way to think about this is that each GPU device establishes a process to run the script and gets a unique ranking from <strong class="source-inline">sdp.local_rank()</strong>:<p class="source-code">gpus = tf.config.experimental.list_physical_devices('GPU')</p><p class="source-code">if gpus:</p><p class="source-code">    local_gpu = gpus[sdp.local_rank()]</p><p class="source-code">    tf.config.experimental.set_visible_devices(local_gpu, 'GPU')</p></li>
				<li>We also configure the GPUs to allow memory growth. This is specific to running TensorFlow with the SageMaker distributed data parallel library:<p class="source-code">for gpu in gpus:</p><p class="source-code">    tf.config.experimental.set_memory_growth(gpu, True)</p></li>
			</ol>
			<p>The compute environment is now ready to perform distributed training.</p>
			<ol>
				<li value="5">We scale <a id="_idIndexMarker609"/>the learning rate by the number <a id="_idIndexMarker610"/>of devices. Because of data parallelism, we will be able to fit in a larger batch size. With a larger batch size, it is recommended to scale the learning rate proportionally:<p class="source-code">args.learning_rate = args.learning_rate * sdp.size()</p></li>
				<li>Previously in <a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a>, <em class="italic">Building and Training ML Models with SageMaker Studio IDE</em>, we trained the model using Keras' <strong class="source-inline">model.fit()</strong> API, but we have to make some changes to the model training. SageMaker's distributed data parallel library does not yet support Keras' <strong class="source-inline">.fit()</strong> API and only works with TensorFlow core modules. To use SageMaker's distributed data parallel library, we can use the automatic differentiation (<strong class="source-inline">tf.GradientTape</strong>) and eager execution from TensorFlow 2.x. After defining the model using Keras layers in the <strong class="source-inline">get_model()</strong> function, instead of compiling it with an optimizer, we write the forward and backward pass explicitly with the <strong class="source-inline">loss</strong> function, the optimizer, and also the accuracy metrics defined explicitly:<p class="source-code">model = get_model(args)</p><p class="source-code">loss = tf.losses.BinaryCrossentropy(name = 'binary_crossentropy')</p><p class="source-code">acc = tf.metrics.BinaryAccuracy(name = 'accuracy')</p><p class="source-code">optimizer = tf.optimizers.Adam(learning_rate = args.learning_rate)</p><p class="source-code">with tf.GradientTape() as tape:</p><p class="source-code">    probs = model(x_train, training=True)</p><p class="source-code">    loss_value = loss(y_train, probs)</p><p class="source-code">    acc_value = acc(y_train, probs)</p></li>
			</ol>
			<p>We then wrap <strong class="source-inline">tf.GradientTape</strong> with SMDataParallel's <strong class="source-inline">DistributedGradientTape</strong> to optimize the <strong class="source-inline">AllReduce</strong> operation during the multi-GPU training. <strong class="source-inline">AllReduce</strong> is an operation that reduces the matrixes from all distributed processes:</p>
			<p class="source-code">tape = sdp.DistributedGradientTape(tape, sparse_as_dense = True)</p>
			<p>Note that the <strong class="source-inline">sparse_as_dense</strong> argument is set to <strong class="source-inline">True</strong> because we have an embedding layer in the model that will generate a spare matrix.</p>
			<ol>
				<li value="7">At the <a id="_idIndexMarker611"/>start of the training, broadcast <a id="_idIndexMarker612"/>the initial model variables from the head node (<strong class="source-inline">rank 0</strong>) to all other worker nodes (<strong class="source-inline">rank 1</strong> onward). We use a <strong class="source-inline">first_batch</strong> variable to denote the start of the training epochs:<p class="source-code">if first_batch:</p><p class="source-code">    sdp.broadcast_variables(model.variables, root_rank=0)</p><p class="source-code">    sdp.broadcast_variables(optimizer.variables(), root_rank=0)</p></li>
				<li>Average the <a id="_idIndexMarker613"/>loss and accuracy across devices; this process is called <strong class="bold">all-reduce</strong>:<p class="source-code">loss_value = sdp.oob_allreduce(loss_value)</p><p class="source-code">acc_value = sdp.oob_allreduce(acc_value)</p></li>
				<li>Put these steps in a <strong class="source-inline">training_step()</strong> function to perform a forward and backward pass, decorated with <strong class="source-inline">@tf.function</strong>. Run this training step in a nested <strong class="source-inline">for</strong> loop to go over epochs and batches of training data. We need to make sure that all GPU devices are getting an equal amount of data during a pass. We do this by taking data that is divisible by the total number of GPU devices in the inner <strong class="source-inline">for</strong> loop:<p class="source-code">train_dataset.take(len(train_dataset)//sdp.size())</p></li>
				<li>After the training <strong class="source-inline">epoch</strong> loop, we save the model, only using the leader device:<p class="source-code">if sdp.rank() == 0:</p><p class="source-code">    model.save(os.path.join(args.model_dir, '1'))</p></li>
				<li>Last but not least in the training script, we convert the training data into a <strong class="source-inline">tf.data.Dataset</strong> object and set up the batching in the <strong class="source-inline">get_train_data()</strong> function so that it will work with our eager execution implementation. Note that we need <strong class="source-inline">drop_remainder</strong> to prevent the dataset from being of an equal batch_size across devices:<p class="source-code">dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))</p><p class="source-code">dataset = dataset.batch(batch_size, drop_remainder=True)</p></li>
				<li>We then <a id="_idIndexMarker614"/>move on to SageMaker's TensorFlow <a id="_idIndexMarker615"/>estimator construct. To enable the SageMaker distributed data parallel library in a training job, we need to provide a dictionary:<p class="source-code">distribution = {'smdistributed': {'dataparallel': {'enabled': True}}}</p></li>
			</ol>
			<p>This is given to the estimator, as follows.</p>
			<p class="source-code">train_instance_type = 'ml.p3.16xlarge'</p>
			<p class="source-code">estimator = TensorFlow(source_dir='code',</p>
			<p class="source-code">     entry_point='smdp_tensorflow_sentiment.py',</p>
			<p class="source-code">     ...</p>
			<p class="source-code">     distribution=distribution)</p>
			<p>Also, we need to choose a SageMaker instance from the following instance types that supports SageMaker's distributed data parallel library: <strong class="bold">ml.p4d.24xlarge</strong>, <strong class="bold">ml.p3dn.24xlarge</strong>, and <strong class="bold">ml.p3.16xlarge</strong>: </p>
			<ol>
				<li>The <strong class="source-inline">ml.p4d.24xlarge</strong> instance <a id="_idIndexMarker616"/>equips with 8 NVIDIA A100 Tensor Core GPUs, each with 40 GB of GPU memory. </li>
				<li>The <strong class="source-inline">ml.p3dn.24xlarge</strong> instance <a id="_idIndexMarker617"/>comes with 8 NVIDIA Tesla V100 GPUs, each with 32 GB of GPU memory. </li>
				<li>The <strong class="source-inline">ml.p3.16xlarge</strong> instance <a id="_idIndexMarker618"/>also comes with 8 NVIDIA Tesla V100 GPUs, each with 16 GB of GPU memory. </li>
			</ol>
			<p>For demonstration purposes, we will choose ml.p3.16xlarge, which is the least expensive <a id="_idIndexMarker619"/>one among the three options. One <a id="_idIndexMarker620"/>single ml.p3.16xlarge is sufficient to run distributed data parallel training in SageMaker, as there will be 8 GPU devices to perform the training. </p>
			<p>As there are more GPU devices and GPU memory to carry out the batching in an epoch, we can now increase <strong class="source-inline">batch_size</strong>. We scale <strong class="source-inline">batch_size</strong> 8 times from what we used in <a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a>, <em class="italic">Building and Training ML Models with SageMaker Studio IDE</em> – that is, <em class="italic">64 x 8 = 512</em>.</p>
			<ol>
				<li value="13">With the estimator, we can proceed to call <strong class="source-inline">estimator.fit()</strong> to start the training. </li>
			</ol>
			<p>To verify that the training is run with multiple GPU devices, the simplest way to tell is from the standard output. You can see a prefix of <strong class="source-inline">[x, y]&lt;stdout&gt;: message</strong> being added to indicate the process ranking from which the message is produced, as shown in <em class="italic">Figure 9.3</em>. We will learn more about this topic in the <em class="italic">Monitoring model training and compute resource with SageMaker Debugger</em> section:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17447_09_003.jpg" alt="Figure 9.3 – The standard output from the cell, showing messages printed from process ranks – [1,0] to [1,7]. In our example, we use one ml.p3.16xlarge instance that has eight GPU devices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – The standard output from the cell, showing messages printed from process ranks – [1,0] to [1,7]. In our example, we use one ml.p3.16xlarge instance that has eight GPU devices</p>
			<p>Even though here I am not using PyTorch to demonstrate SageMaker's distributed data parallel library, PyTorch is indeed supported by the library under the <strong class="source-inline">smdistributed.dataparallel.torch</strong> module. This module has a set of APIs that are similar to PyTorch's native distributed data parallel library. This means that you do not require many coding <a id="_idIndexMarker621"/>changes to adopt SageMaker's distributed <a id="_idIndexMarker622"/>data parallel library for PyTorch, which is optimized for training using SageMaker's infrastructure. You can find more details on <a id="_idIndexMarker623"/>how to adopt it in your PyTorch scripts at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-pt.html">https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp-pt.html</a>.</p>
			<p>In the next section, we will run a PyTorch example and adopt model parallelism. </p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor130"/>Model parallelism with PyTorch</h2>
			<p>Model parallelism <a id="_idIndexMarker624"/>is particularly useful when you have a <a id="_idIndexMarker625"/>large network model that does not fit into the memory of a single GPU device. SageMaker's distributed model parallel library implements two features that enable efficient training for large models so that you can easily adapt the library to your existing training scripts:</p>
			<ul>
				<li><strong class="bold">Automated model partitioning</strong>, which maximizes GPU utilization, balances the <a id="_idIndexMarker626"/>memory footprint, and minimizes communication among GPU devices. In contrast, you can also manually partition the model using the library.</li>
				<li><strong class="bold">Pipeline execution</strong>, which determines the order of computation and data movement <a id="_idIndexMarker627"/>across parts of the model that <a id="_idIndexMarker628"/>are on different GPU devices. There are two pipeline implementations: <strong class="bold">interleaved</strong> and <strong class="bold">simple</strong>. An interleaved pipeline <a id="_idIndexMarker629"/>prioritizes the backward passes whenever possible. It uses GPU memory more efficiently and minimizes the idle time of any GPU device in the fleet without waiting for the forward pass to complete to start the backward pass, as shown in <em class="italic">Figure 9.4</em>: </li>
			</ul>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17447_09_004.jpg" alt="Figure 9.4 – An interleaved pipeline over two GPUs (GPU0 and GPU1). F0 represents a forward pass for the first micro-batch and B1 represents a backward pass for the second micro-batch. Backward passes are prioritized whenever possible&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – An interleaved pipeline over two GPUs (GPU0 and GPU1). F0 represents a forward pass for the first micro-batch and B1 represents a backward pass for the second micro-batch. Backward passes are prioritized whenever possible</p>
			<p>A simple pipeline, on the other hand, waits for the forward pass to complete before starting the backward pass, resulting in a simpler execution schedule, as shown in <em class="italic">Figure 9.5</em>:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B17447_09_005.jpg" alt="Figure 9.5 – A simple pipeline over two GPUs. Backward passes are run only after &#13;&#10;the forward passes finish&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – A simple pipeline over two GPUs. Backward passes are run only after the forward passes finish</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Images in <em class="italic">Figure 9.4</em> and <em class="italic">9.5</em> are from:  <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html</a> </p>
			<p>Let's start <a id="_idIndexMarker630"/>an example with the notebook in <strong class="source-inline">chapter09/02-smmp-pytorch_mnist.ipynb</strong>, where we are going to apply SageMaker's distributed model <a id="_idIndexMarker631"/>parallel library to train a PyTorch model to classify digit handwriting using the famous MNIST digit dataset. Open the notebook in SageMaker Studio and use the <strong class="bold">Python 3 (Data Science)</strong> kernel and an <strong class="source-inline">ml.t3.medium</strong> instance:</p>
			<ol>
				<li value="1">As usual, set up the SageMaker session and import the dependencies in the first cell. </li>
				<li>Then, create a model training script written in PyTorch. This is a new training script. Essentially, it is training a convolutional neural network model on the MNIST handwriting digit dataset from the <strong class="source-inline">torchvision</strong> library. The model is defined using the <strong class="source-inline">torch.nn</strong> module. The optimizer used is the AdamW optimization algorithm. We implement the training epochs and batching, as it allows us to have the most flexibility to adopt SageMaker's distributed model parallel library.</li>
				<li>SageMaker's distributed model parallel library for PyTorch can be imported from <strong class="source-inline">smdistributed.modelparallel.torch</strong>:<p class="source-code">import smdistributed.modelparallel.torch as smp</p></li>
				<li>After the library is imported, initialize the SageMaker distributed model parallel library in order to use it during runtime. We can implement it right after the import statements or in <strong class="source-inline">main</strong> (<strong class="source-inline">if __name__ == "__main__"</strong>):<p class="source-code">smp.init()</p></li>
				<li>We will then ping and set the GPU devices with their local ranks: <p class="source-code">torch.cuda.set_device(smp.local_rank())</p><p class="source-code">device = torch.device('cuda')</p></li>
				<li>The data downloading process from <strong class="source-inline">torchvision</strong> should only take place in the leader node (<strong class="source-inline">local_rank</strong> = <strong class="source-inline">0</strong>), while all the other processes (on other GPUs) should wait until the leader node completes the download:<p class="source-code">if smp.local_rank() == 0:</p><p class="source-code">    dataset1 = datasets.MNIST('../data', train=True, </p><p class="source-code">                  download=True, transform=transform)        </p><p class="source-code">smp.barrier() # Wait for all processes to be ready</p></li>
				<li>Then, wrap <a id="_idIndexMarker632"/>the model and the optimizer with <a id="_idIndexMarker633"/>SageMaker's distributed model parallel library's implementations:<p class="source-code">model = smp.DistributedModel(model)</p><p class="source-code">optimizer = smp.DistributedOptimizer(optimizer)</p></li>
			</ol>
			<p>Up to now, the implementation between SageMaker's distributed data parallel library and model parallel library has been quite similar. The following is where things get different for the model parallel library.</p>
			<ol>
				<li value="8">We create a <strong class="source-inline">train_step()</strong> for function forward and backward passes and decorate it with <strong class="source-inline">@smp.step</strong>:<p class="source-code">@smp.step</p><p class="source-code">def train_step(model, data, target):</p><p class="source-code">    output = model(data)</p><p class="source-code">    loss = F.nll_loss(output, target, reduction='mean')</p><p class="source-code">    model.backward(loss)</p><p class="source-code">    return output, loss</p></li>
			</ol>
			<p>Create another <strong class="source-inline">train()</strong> function to implement the batching within an epoch. This is where we call <strong class="source-inline">train_step()</strong> to perform the forward and backward passes for a batch of data. Importantly, the data-related <strong class="source-inline">to.(device)</strong> calls need to be placed before <strong class="source-inline">train_step()</strong> while the typical <strong class="source-inline">model.to(device)</strong> is not required. Placing the model to a device is done automatically by the library. </p>
			<p>Before <a id="_idIndexMarker634"/>stepping to the next batch, we need to <a id="_idIndexMarker635"/>average the loss across micro-batches with <strong class="source-inline">.reduce_mean()</strong>. Also, note that <strong class="source-inline">optimizer.step()</strong> needs to take place outside of <strong class="source-inline">train_step()</strong>:</p>
			<p class="source-code">def train(model, device, train_loader, optimizer, epoch):</p>
			<p class="source-code">    model.train()</p>
			<p class="source-code">    for batch_idx, (data, target) in enumerate(train_loader):</p>
			<p class="source-code">        data, target = data.to(device), target.to(device)</p>
			<p class="source-code">        optimizer.zero_grad()</p>
			<p class="source-code">        _, loss_mb = train_step(model, data, target)</p>
			<p class="source-code">        # Average the loss across microbatches.</p>
			<p class="source-code">        loss = loss_mb.reduce_mean()</p>
			<p class="source-code">        optimizer.step()</p>
			<ol>
				<li value="9">Implement <strong class="source-inline">test_step()</strong>, decorated with <strong class="source-inline">@smp.step</strong>, and <strong class="source-inline">test()</strong> similarly for model evaluation. This allows model parallelism in model evaluation too.</li>
				<li>After the epochs loop, save the model with <strong class="source-inline">smp.dp_rank()==0</strong> to avoid data racing and ensure the gathering happens properly. Note that we set <strong class="source-inline">partial=True</strong> if we want to be able to load the model later and further train it:<p class="source-code">if smp.dp_rank() == 0:</p><p class="source-code">    model_dict = model.local_state_dict()</p><p class="source-code">    opt_dict = optimizer.local_state_dict()</p><p class="source-code">    model = {'model_state_dict': model_dict, 'optimizer_state_dict': opt_dict}</p><p class="source-code">    model_output_path = f'{args.model_dir}/pt_mnist_checkpoint.pt'</p><p class="source-code">    smp.save(model, model_output_path, partial=True)</p></li>
				<li>We then move on to the SageMaker PyTorch estimator construct. To enable SageMaker's distributed model parallel library in a training job, we need to provide a <a id="_idIndexMarker636"/>dictionary to configure SageMaker's distributed model parallel library and the <strong class="bold">Message Passing Interface</strong> (<strong class="bold">MPI</strong>). SageMaker's distributed model parallel library uses the MPI to communicate across nodes, so it needs to be enabled. The following snippet instructs SageMaker to partition the model into two <strong class="source-inline">'partitions': 2</strong>, to optimize for speed when partitioning the model <strong class="source-inline">'optimize': 'speed'</strong>, to use a micro-batch of four <strong class="source-inline">'microbatches': 4</strong>, to employ an interleaved pipeline schedule ('pipeline': 'interleaved'), and to disable distribute data parallel <strong class="source-inline">'ddp': False</strong>. The MPI is enabled with four processes per host '<strong class="source-inline">mpi':{'enabled': True</strong>, <strong class="source-inline">'processes_per_host': 2}}</strong>, which should be smaller than or equal to the number of GPU devices:<p class="source-code">distribution = {'smdistributed': {</p><p class="source-code">                    'modelparallel': {</p><p class="source-code">                        'enabled': True,</p><p class="source-code">                        'parameters': {</p><p class="source-code">                            'partitions': 2,</p><p class="source-code">                             'optimize': 'speed',</p><p class="source-code">                            'microbatches': 4,</p><p class="source-code">                            'pipeline': 'interleaved',</p><p class="source-code">                            'ddp': False</p><p class="source-code">                        }</p><p class="source-code">                    }</p><p class="source-code">                },</p><p class="source-code">                'mpi': {</p><p class="source-code">                    'enabled': True,</p><p class="source-code">                    'processes_per_host': 2</p><p class="source-code">                }</p><p class="source-code">            }</p></li>
			</ol>
			<p>You can find <a id="_idIndexMarker637"/>the full list of parameters for <strong class="source-inline">distribution</strong> at <a href="https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#smdistributed-parameters">https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html#smdistributed-parameters</a>.</p>
			<ol>
				<li value="12">We then apply the <strong class="source-inline">distribution</strong> dictionary to the PyTorch estimator and use one ml.p3.8xlarge instance, which has four NVIDIA Tesla V100 GPUs. Unlike SageMaker's distributed data parallel library, SageMaker's distributed model parallel library is supported by all instances with multiple GPU devices.</li>
				<li>We can then proceed to call <strong class="source-inline">estimator.fit()</strong> to start the training. </li>
			</ol>
			<p>Adopting a <a id="_idIndexMarker638"/>TensorFlow training script with SageMaker's <a id="_idIndexMarker639"/>distributed model parallel library employs similar concepts that we can just walk through. You can find out more about how <a id="_idIndexMarker640"/>to use the <strong class="source-inline">smdistributed.modelparallel.tensorflow</strong> module at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23">https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-customize-training-script-tf.html#model-parallel-customize-training-script-tf-23</a>.</p>
			<p>When training with multiple GPU devices, one of the main challenges is to understand how expensive GPU resources are utilized. In the next section, we will discuss SageMaker Debugger, a feature that helps us analyze the utilization of compute resources during a SageMaker training job.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor131"/>Monitoring model training and compute resources with SageMaker Debugger</h1>
			<p>Training <a id="_idIndexMarker641"/>ML models <a id="_idIndexMarker642"/>using <strong class="source-inline">sagemaker.estimator.Estimator</strong> and related classes, such as <strong class="source-inline">sagemaker.pytorch.estimator.PyTorch</strong> and <strong class="source-inline">sagemaker.tensorflow.estimator.TensorFlow</strong>, gives us the flexibility and scalability we need <a id="_idIndexMarker643"/>when developing <a id="_idIndexMarker644"/>in SageMaker Studio. However, due to the use of remote compute resources, it is rather different debugging and monitoring training jobs on a local machine or a single EC2 machine to how you would on a SageMaker Studio notebook. Being an IDE for ML, SageMaker Studio <a id="_idIndexMarker645"/>provides a comprehensive view of the managed training jobs through <strong class="bold">SageMaker Debugger</strong>. SageMaker Debugger helps developers monitor the compute resource utilization, detect modeling-related issues, profile deep learning operations, and identify bottlenecks during the runtime of your training jobs. </p>
			<p>SageMaker Debugger supports TensorFlow, PyTorch, MXNet, and XGBoost. By default, SageMaker Debugger is enabled in every SageMaker estimator. It collects instance metrics such as GPU, CPU, and memory utilization every 500 milliseconds and basic tensor output such as loss and accuracy every 500 steps. The data is saved in your S3 bucket. You can inspect the monitoring results live or after the job finishes in the SageMaker Studio IDE. You can also retrieve the monitoring results from S3 into a notebook and run additional analyses and custom visualization. If the default setting is not sufficient, you can configure the SageMaker Debugger programmatically for your <strong class="source-inline">Estimator</strong> to get the level of information you need.</p>
			<p>To get started, we can first inspect the information from the default Debugger configuration for the job we ran in <strong class="source-inline">Machine-Learning-Development-with-Amazon-SageMaker-Studio/chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb</strong>:</p>
			<ol>
				<li value="1">Find the job name you have run. It is in the <strong class="source-inline">jobname</strong> variable, in the form of <strong class="source-inline">imdb-smdp-tf-YYYY-mm-DD-HH-MM-SS</strong>. You can also find it in the output of the last cell. </li>
				<li>Navigate to the <strong class="bold">SageMaker Components and registries</strong> icon on the left sidebar, select <strong class="bold">Experiments and trials</strong> from the drop-down menu, and locate the entry with <strong class="source-inline">jobname</strong>; double-click the entry. You will see a trial component named <strong class="bold">Training</strong>, as shown in <em class="italic">Figure 9.6</em>. Right-click on the entry and select <strong class="bold">Open Debugger for insights</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B17447_09_006.jpg" alt="Figure 9.6 – Opening the SageMaker Debugger UI from Experiments and trials&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Opening the SageMaker Debugger UI from Experiments and trials</p>
			<ol>
				<li value="3">A <a id="_idIndexMarker646"/>new window <a id="_idIndexMarker647"/>in the main <a id="_idIndexMarker648"/>working area will <a id="_idIndexMarker649"/>pop up. The window will become available in a couple of minutes, as SageMaker Studio is launching <a id="_idIndexMarker650"/>a dedicated instance to process and render the data in the UI. This is called the <strong class="bold">SageMaker Debugger insights dashboard</strong>. Once available, you can see the results in the <strong class="bold">Overview</strong> and <strong class="bold">Nodes</strong> tabs, as shown in <em class="italic">Figure 9.7</em>:</li>
			</ol>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17447_09_007.jpg" alt="Figure 9.7 – The SageMaker Debugger insights dashboard showing the CPU and network utilization over the course of the training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – The SageMaker Debugger insights dashboard showing the CPU and network utilization over the course of the training</p>
			<p>In <a id="_idIndexMarker651"/>the <strong class="bold">Nodes</strong> tab, the <a id="_idIndexMarker652"/>mean utilization of the <a id="_idIndexMarker653"/>CPU, the network, the GPU, and <a id="_idIndexMarker654"/>the GPU memory are shown in the charts. You can narrow down the chart to a specific CPU or GPU to see whether there is any uneven utilization over the devices, as shown in <em class="italic">Figure 9.8</em>. From these charts, we can tell the following:</p>
			<ul>
				<li>The average CPU utilization peaked at around 60%, 3 minutes after the start of the job. This indicates that the training was taking place, and there was much activity on the CPU side to read in the data batches and feed into the GPU devices. </li>
				<li>The <a id="_idIndexMarker655"/>average GPU <a id="_idIndexMarker656"/>utilization over <a id="_idIndexMarker657"/>eight devices <a id="_idIndexMarker658"/>peaked at around 25%, also at 3 minutes after the start of the job. At the same time, there was around 5% of GPU memory used on average. This is considered low GPU utilization, potentially due to the small batch size compared to the now much larger compute capacity from an ml.p3.16xlarge instance.</li>
				<li>On the other hand, there was some network utilization in the first 3 minutes. This is the period when SageMaker's fully managed training downloaded the training data from the S3 bucket:</li>
			</ul>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17447_09_008.jpg" alt="Figure 9.8 – The SageMaker Debugger insights dashboard showing the GPU utilization over the course of the training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – The SageMaker Debugger insights dashboard showing the GPU utilization over the course of the training</p>
			<p>At the <a id="_idIndexMarker659"/>bottom of the page, a <a id="_idIndexMarker660"/>heatmap of the CPU/GPU <a id="_idIndexMarker661"/>utilization in a holistic <a id="_idIndexMarker662"/>view is displayed. As an exercise, feel free to open the Debugger for the training job submitted at <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter06/02-tensorflow_sentiment_analysis.ipynb</strong> and compare the difference in the CPU/GPU utilization between single-device training and distributed training.</p>
			<p>Next, we'll move on to learn how to lower the cost of training ML models in SageMaker Studio with fully managed spot training and how to create checkpointing for long-running jobs and spot jobs.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>Managing long-running jobs with checkpointing and spot training</h1>
			<p>Training ML models at scale can be costly. Even with SageMaker's pay-as-you-go pricing model on <a id="_idIndexMarker663"/>the training instances, performing <a id="_idIndexMarker664"/>long-running deep learning training and using multiple expensive instances can add up quickly. SageMaker's fully managed spot training and checkpointing features allow us to manage and resume long-running jobs easily, helping us reduce costs up to 90% on training instances over on-demand instances.</p>
			<p>SageMaker-managed <a id="_idIndexMarker665"/>Spot training <a id="_idIndexMarker666"/>uses the concept of spot instances from Amazon EC2. EC2 spot instances let you take advantage of any unused instance capacity in an AWS Region at a much lower cost compared to regular on-demand instances. The spot instances are cheaper but can be interrupted when there is a higher demand for instances from other users on AWS. SageMaker-managed spot training manages the use of spot instances, including safe interruption and timely resumption of your training when the spot instances are available again. </p>
			<p>Along with the spot training feature, managed checkpointing is a key to managing your long-running job. Checkpoints in ML refer to intermediate ML models saved during training. Data scientists regularly create checkpoints and keep track of the best accuracy during the epochs. They compare accuracy against the best one during progression and use the checkpoint model that has the highest accuracy, rather than the model from the last epoch. </p>
			<p>Data scientists can also resume and continue the training from any particular checkpoint if they want to fine-tune a model. As SageMaker trains a model on remote compute instances using containers, the checkpoints are saved in a local directory in the container. SageMaker automatically uploads the checkpoints from the local bucket to your S3 bucket. You can reuse the checkpoints in another training job easily by specifying their location in S3. In the context of SageMaker-managed spot training, you do not need to worry about uploading and downloading the checkpoint files in case there is any interruption and resumption of a training job. SageMaker handles it for us.</p>
			<p>Let's run an example to see how things work. Open <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter09/03-spot_training_checkpointing.ipynb</strong> using the <strong class="bold">Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)</strong> kernel and an <strong class="bold">ml.t3.medium</strong> instance. In this notebook, we will be reusing our TensorFlow model training for the IMDB review dataset from <a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a>, <em class="italic">Building and Training ML Models with SageMaker Studio IDE</em>, and make some changes to the code to demonstrate <a id="_idIndexMarker667"/>how you can enable <a id="_idIndexMarker668"/>the checkpointing and <a id="_idIndexMarker669"/>managed spot training using SageMaker:</p>
			<ol>
				<li value="1">Run the <a id="_idIndexMarker670"/>first five cells to set up the SageMaker session, and prepare the dataset. If you ran the first <strong class="source-inline">chapter09/01-smdp_tensorflow_sentiment_analysis.ipynb</strong> notebook, the dataset should be available already.</li>
				<li>The cell leading with <strong class="source-inline">%%writefile code/tensorflow_sentiment_with_checkpoint.py</strong> is where we will make changes to the TensorFlow/Keras code. First of all, we are adding a new <strong class="source-inline">--checkpoint_dir</strong> argument in the <strong class="source-inline">parse_args()</strong> function to assign a default <strong class="source-inline">/opt/ml/checkpoints</strong> location set by SageMaker. </li>
				<li>In <strong class="source-inline">__name__ == '__main__'</strong>, we will add a check to see whether <strong class="source-inline">checkpoint_dir</strong> exists locally in the container or not. If it does, list the directory to see whether there are any existing checkpoint files:<p class="source-code">if not os.listdir(args.checkpoint_dir):</p><p class="source-code">    model = get_model(args)</p><p class="source-code">    initial_epoch_number = 0</p><p class="source-code">else:    </p><p class="source-code">    model, initial_epoch_number = load_model_from_checkpoints(args.checkpoint_dir)</p></li>
			</ol>
			<p>If <strong class="source-inline">checkpoint_dir</strong> does not contain valid checkpoint files, it means that there is no prior training job and checkpoints attached to the container and that <strong class="source-inline">checkpoint_dir</strong> is newly created for brand-new model training. If it does contain files, it means that previous checkpoint files are plugged into this training job and should be used as a starting point of the training, implemented in the <strong class="source-inline">load_model_from_checkpoints()</strong> function.</p>
			<ol>
				<li value="4">Implement <strong class="source-inline">load_model_from_checkpoints()</strong> to list all the checkpoint files, ending with <strong class="source-inline">.h5</strong>, as this is how Keras saved the model, in a given directory and use <strong class="source-inline">regex</strong> from the <strong class="source-inline">re</strong> library to filter the epoch number in the filename. We can then identify the latest checkpoint to load and continue the training with such a model. We assume the epoch number ranges from <strong class="source-inline">0</strong> to <strong class="source-inline">999</strong> in the regular expression operation.</li>
				<li>After <a id="_idIndexMarker671"/>the model is loaded, either <a id="_idIndexMarker672"/>a new one or from a checkpoint, implement a <strong class="source-inline">tf.keras.callbacks.ModelCheckpoint</strong> callback in Keras to save a model checkpoint to <strong class="source-inline">args.checkpoint_dir</strong> after every epoch.</li>
				<li>When <a id="_idIndexMarker673"/>setting up the <strong class="source-inline">sagemaker.tensorflow.TensorFlow</strong> estimator, provide the following additional <a id="_idIndexMarker674"/>arguments to the <a id="_idIndexMarker675"/>estimator:<ol><li><strong class="source-inline">use_spot_instances</strong>: A Boolean to elect to use SageMaker spot instances for training.</li><li><strong class="source-inline">max_wait</strong>: A required argument when <strong class="source-inline">use_spot_instances</strong> is <strong class="source-inline">True</strong>. This is a timeout in seconds waiting for the spot training job. After this timeout, the job will be stopped.</li><li><strong class="source-inline">checkpoint_s3_uri</strong>: The S3 bucket location to save the checkpoint files persistently. If you pass an S3 bucket location that already has checkpoint models and pass a higher epoch number, the script will pick up the latest checkpoint and resume training. For example, by providing <strong class="source-inline">checkpoint_s3_uri</strong>, which has checkpoints from a previous 50-epoch run and an <strong class="source-inline">epochs</strong> hyperparameter of 60, our script will resume the training from the fiftieth checkpoint and continue for another 10 epochs.</li><li><strong class="source-inline">max_run</strong>: The maximum runtime in seconds allowed for training. After this timeout, the job will be stopped. This value needs to be smaller than or equal to <strong class="source-inline">max_wait</strong>.</li></ol></li>
			</ol>
			<p>The <a id="_idIndexMarker676"/>following code snippet <a id="_idIndexMarker677"/>will construct an estimator <a id="_idIndexMarker678"/>to train a model <a id="_idIndexMarker679"/>with managed spot instances and checkpointing:</p>
			<p class="source-code">use_spot_instances = True</p>
			<p class="source-code">max_run = 3600</p>
			<p class="source-code">max_wait = 3600</p>
			<p class="source-code">checkpoint_suffix = str(uuid.uuid4())[:8]</p>
			<p class="source-code">checkpoint_s3_uri = f's3://{bucket}/{prefix}/checkpoint-{checkpoint_suffix}'</p>
			<p class="source-code">estimator = TensorFlow(use_spot_instances=use_spot_instances,</p>
			<p class="source-code">                       checkpoint_s3_uri=checkpoint_s3_uri,</p>
			<p class="source-code">                       max_run=max_run,</p>
			<p class="source-code">                       max_wait=max_wait,</p>
			<p class="source-code">                       ...)</p>
			<ol>
				<li value="7">The rest of the steps remain the same. We specify the hyperparameters, data input, and experiment configuration before we invoke <strong class="source-inline">.fit()</strong> to start the training job.</li>
				<li>Wonder how much we save by using spot instances? From <strong class="bold">Experiments and trials</strong> in the left sidebar, we can bring up the AWS settings details of the trial, as shown in <em class="italic">Figure 9.9</em>, and see a <strong class="bold">70%</strong> saving by simply using managed spot training instances:</li>
			</ol>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17447_09_009.jpg" alt="Figure 9.9 – A 70% saving using managed spot training, as seen in the trial details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – A 70% saving using managed spot training, as seen in the trial details</p>
			<p>A 70% saving <a id="_idIndexMarker680"/>is quite significant. This is <a id="_idIndexMarker681"/>especially beneficial to large-scale <a id="_idIndexMarker682"/>model training use cases <a id="_idIndexMarker683"/>that need expensive compute instances and have a long training time. Just four additional arguments to the estimator and some changes in the training script earn us a 70% saving. </p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor133"/>Summary</h1>
			<p>In this chapter, we walked through how to train deep learning models using SageMaker distributed training libraries: data parallel and model parallel. We ran a TensorFlow example to show how you can modify a script to use SageMaker's distributed data parallel library with eight GPU devices, instead of one from what we learned previously. This enables us to increase the batch size and reduce the iterations needed to go over the entire dataset in an epoch, improving the model training runtime. We then showed how you can adapt SageMaker's distributed model parallel library to model training written in PyTorch. This enables us to train a much larger neural network model by partitioning the large model to all GPU devices. We further showed you how you can easily monitor the compute resource utilization in a training job using SageMaker Debugger and visualize the metrics in the SageMaker Debugger insights dashboard. Lastly, we explained how to adapt your training script to use the fully managed spot training and checkpointing to save costs when training models in SageMaker.</p>
			<p>In the next chapter, we will be switching gear to learn how to monitor ML models in production. ML models in production taking unseen inference data may or may not produce quality predictions as expected from evaluations conducted prior to deployment. It is crucial in an ML life cycle to set up a monitoring strategy to ensure that your models are operating at a satisfactory level. SageMaker Studio has the functionality needed to help you set up model monitoring easily to monitor ML models in production. We will learn how to configure SageMaker Model Monitor and how to use it as part of our ML life cycle.</p>
		</div>
	</body></html>