<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer080">
<h1 class="chapter-number" id="_idParaDest-167"><a id="_idTextAnchor168"/>9</h1>
<h1 id="_idParaDest-168"><a id="_idTextAnchor169"/>Using Google Cloud ML Best Practices</h1>
<p>In this chapter, we will discuss the best practices for implementing <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) in Google<a id="_idIndexMarker529"/> Cloud. We will go through an implementation of a customer-trained ML model development process in GCP and provide recommendations throughout.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>ML environment setup</li>
<li>ML data storage and processing</li>
<li>ML model training</li>
<li>ML model deployment</li>
<li>ML workflow orchestration</li>
<li>ML model continuous monitoring</li>
</ul>
<p>This chapter aims to integrate the knowledge we have learned so far in this book and apply it to a customer-trained ML project. We will start by setting up the ML environment.</p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor170"/>ML environment setup</h1>
<p>In <a href="B18333_04.xhtml#_idTextAnchor094"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing and Deploying ML Models</em>, in the <em class="italic">Preparing the platform</em> section, we learned<a id="_idIndexMarker530"/> about the ML platform in the Cloud. Then, in <a href="B18333_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Google Cloud Vertex AI</em>, we introduced the Vertex AI services.  For a customer-trained model development platform, we<a id="_idIndexMarker531"/> recommend <strong class="bold">Vertex AI Workbench user-managed notebooks</strong>. Let’s look at the details from the prospects of performance, cost, and security.</p>
<p>With Vertex AI Workbench user-managed notebooks, you have the flexibility and options to <a id="_idIndexMarker532"/>implement <strong class="bold">performance excellency</strong>. You can create an instance with the existing deep learning VM images that have the latest ML and data science libraries preinstalled, along with the latest accelerator drivers. Depending on your data, model, and workloads, you can choose the right VM instance type to fit your environment and optimize performance, from general-purpose compute (E2, N1, N2, and N2D), to memory-optimized (M1 and M2), to compute-optimized (C2), and so on. You can also create a notebooks instance based on a custom container to tailor your ML environment. </p>
<p>With Vertex AI Workbench user-managed notebooks, you can go with GCP best practices and <strong class="bold">reduce costs</strong>. As<a id="_idIndexMarker533"/> with any Google Cloud services, treat your notebook’s VM instances as flexible and disposable resources; when you train ML models on the VM instances, make sure that you store all your data in Cloud Storage or BigQuery, instead of storing it in the instances' local storage, such as persistent disks, so that you can stop or delete the instances when you are done with the ML experimenting or training. During the ML model development process, always monitor the VM instances’ performances and <a id="_idIndexMarker534"/>costs, and scale out/in based on the workloads while utilizing Google’s Managed instance groups (MIGs).</p>
<p><strong class="bold">Security</strong> is always<a id="_idIndexMarker535"/> an important area we need to address in the Cloud. Some of the best security practices are as follows:</p>
<ul>
<li>With Vertex AI Workbench user-managed notebooks, we recommend that a user-managed notebooks instance is created for each member of the data science team. If a team member is involved in multiple projects, we recommend using multiple user-managed notebooks instances for the member and treating each instance as a virtual workspace. </li>
<li>Operating Vertex AI requires collaboration among a variety of teams, and it is super important to determine which groups or systems will be responsible for which functions. From a networking point of view, you should configure user-managed notebooks to use shared VPCs if possible, to minimize access to the notebook instances. Restrictive firewall rules must also be enabled to limit access to notebook instances and other Vertex AI resources. </li>
<li>For data and model storing, we also recommend storing the training data and training model in the same project for reproducibility. In a Google organization with multiple folders and multiple projects, the best practice is leveraging Google IAM roles and groups. </li>
<li>For data protection, we recommend using Google Cloud organization policies and Data Loss Prevention (DLP) tools to protect Personal Identifiable Information (PII) data. Data encryption is also recommended for storing data at<a id="_idIndexMarker536"/> rest and in transit in the Vertex AI notebooks instances. Vertex AI supports <strong class="bold">Customer-Managed Encryption Keys</strong> (<strong class="bold">CMEK</strong>) across <a id="_idIndexMarker537"/>most of its components. </li>
</ul>
<p>Now that we have understood the environment setup, let’s move to data storage and processing.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor171"/>ML data storage and processing</h1>
<p>As we <a id="_idIndexMarker538"/>discussed <a id="_idIndexMarker539"/>in <a href="B18333_04.xhtml#_idTextAnchor094"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing and Deploying ML Models</em>, storing data involves collecting raw data from various data sources and storing it in a centralized repository. On the other hand, data processing includes both data engineering and feature engineering. Data engineering is the process of converting raw data (the data in its source form) into prepared data (the dataset in the form that is ready to be input into ML tasks). Feature engineering then tunes the prepared data to create the features expected by the ML model. </p>
<p>For structured data, we recommend<a id="_idIndexMarker540"/> using <strong class="bold">Google Cloud BQ</strong> to store and process it. For unstructured data, videos, audio, and image data, we recommend <a id="_idIndexMarker541"/>using <strong class="bold">Google Cloud</strong> object <a id="_idIndexMarker542"/>storage to store<a id="_idIndexMarker543"/> them and <strong class="bold">Google Cloud Dataflow</strong> or <strong class="bold">Dataproc</strong> to process them. As we have discussed, <strong class="bold">Dataflow</strong> is a managed service that uses the <strong class="bold">Apache Beam</strong> programming <a id="_idIndexMarker544"/>model to convert unstructured data into binary formats and can improve data ingestion performance. Dataproc is a <a id="_idIndexMarker545"/>managed <strong class="bold">Apache Spark</strong> and <strong class="bold">Apache Hadoop</strong> service that<a id="_idIndexMarker546"/> leverages open source data tools for batch processing, querying, streaming, and ML. </p>
<p>For supervised ML, which <a id="_idIndexMarker547"/>needs labeled datasets, we recommend using the <strong class="bold">Google Vertex AI Data Labeling</strong> service, especially for unstructured data. From a security point of view, we recommend using <strong class="bold">Google Cloud IAM</strong> to <a id="_idIndexMarker548"/>manage data access in Cloud Storage and BQ, using GCP DLP to manage PII and other sensitive data, and using GCP Key Management Services (KMS) for data encryption key management.</p>
<p>Once the data has been preprocessed, we recommend using a Vertex AI-managed dataset to create a link between your data and custom-trained models and provide descriptive statistics to split data into training, validation, and testing subsets.</p>
<p>Depending on the ML model features, many methods can be utilized in feature engineering. We recommend<a id="_idIndexMarker549"/> using <strong class="bold">Vertex AI Feature Store</strong>, which can be used to create new features from the data lakes, schedule data processing, and feature engineering<a id="_idIndexMarker550"/> jobs, ingest them into Vertex Feature Store for <a id="_idIndexMarker551"/>online or batch serving, and share the common features within the data science team. </p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor172"/>ML model training</h1>
<p>ML model <a id="_idIndexMarker552"/>training is a critical phase in ML development, which is why we recommend using GCP Vertex AI Training. Instead of manually adjusting hyperparameters with numerous training runs for optimal values, we recommend the automated Vertex AI training model enhancer to test different hyperparameter configurations, and <strong class="bold">Google Vertex AI TensorBoard</strong> to<a id="_idIndexMarker553"/> track, share, and compare model metrics such as loss functions to visualize model graphs. This allows you to compare various experiments for parameter tuning and model optimization.</p>
<p>Using Vertex AI Workbench user-managed notebooks, you can develop your code conveniently and interactively, and we recommend operationalizing your code for reproducibility and scalability and running your code in either <a id="_idIndexMarker554"/>Vertex training or <strong class="bold">Vertex AI Pipelines</strong>.</p>
<p>After model training, it is recommended that <a id="_idIndexMarker555"/>you use <strong class="bold">Vertex Explainable AI</strong> to study and gain insights regarding feature contributions and understand your model’s behavior. Vertex Explainable AI helps you understand your model’s outputs – it tells you how much each feature in the data contributed to the predicted result. Then, you can use this information to see whether your model is behaving as expected, to recognize bias (if there is any) in your models, and get some ideas to improve your model and your training data.</p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>ML model deployment</h1>
<p>ML model<a id="_idIndexMarker556"/> deployment refers to putting a model into production. Once an ML model has been deployed into production, it can be used to predict new data. We recommend using the Vertex AI console or API to deploy a trained ML model. With Vertex AI, we can serve the model in production with batch prediction; we recommend specifying the appropriate hardware for your model and determining how to pass inputs to the model. With Vertex AI, we can also serve the model with online endpoint prediction; we recommend using Vertex AI Feature Store’s online serving API and turning on automatic scaling with a minimum of two nodes. </p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/>ML workflow orchestration</h1>
<p>As we<a id="_idIndexMarker557"/> discussed in <a href="B18333_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Google Cloud Vertex AI</em>, Vertex AI Pipelines is a fully managed service that allows you to retrain your models as often as necessary so that you can adapt to changes and maintain performance over time. We recommend Vertex AI Pipelines for Cloud ML workflow orchestration. </p>
<p>If you’re<a id="_idIndexMarker558"/> using the <strong class="bold">Google TensorFlow framework</strong>, we recommend<a id="_idIndexMarker559"/> using <strong class="bold">TensorFlow Extended</strong> to define your pipeline and the operations for each step, then executing it on Vertex AI’s serverless pipeline system. TensorFlow provides pre-built components for common steps in the Vertex AI workflow, such as data ingestion, data validation, and training.</p>
<p>If you are using other frameworks, we recommend<a id="_idIndexMarker560"/> using <strong class="bold">Kubeflow Pipeline</strong>, which is very flexible and allows you to use simple code to construct pipelines. Kubeflow Pipeline also provides Google Cloud pipeline components such as Vertex AI AutoML. </p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>ML model continuous monitoring</h1>
<p>Once you’ve<a id="_idIndexMarker561"/> deployed your model into production, you need to monitor model performance continuously to ensure that it is performing as expected. We recommend using Vertex AI, which provides two ways to monitor your ML<a id="_idIndexMarker562"/> models:</p>
<ul>
<li><strong class="bold">Skew detection</strong>, which <a id="_idIndexMarker563"/>looks for the degree of distortion between your model training and production data.</li>
<li><strong class="bold">Drift detection</strong>, which<a id="_idIndexMarker564"/> looks for drift in your production data. Drift occurs when the statistical properties of the inputs and the target change over time and cause predictions to become less accurate as time passes.</li>
</ul>
<p>For skew and drift detection, we recommend setting up a model monitoring job by providing a pointer to the training data that you used to train your model, and then tuning the thresholds that are used for alerting to measure skew or drift occurring in your data. </p>
<p>You can also use feature attributions in Vertex Explainable AI to detect data drift or skew as an early indicator that model performance may be degrading. For example, let’s say that your<a id="_idIndexMarker565"/> model originally relied on five features to make predictions in training and test data, but when going into production, it began to rely on entirely different features.</p>
<h1 id="_idParaDest-175"><a id="_idTextAnchor176"/>Summary</h1>
<p>In this chapter, we discussed the best practices for implementing ML in Google Cloud, with a focus on custom-trained models based on your data and code. </p>
<p>This chapter concludes <em class="italic">Part 3</em> of this book, in which we have discussed Google BQ and BQML for training ML models from structured data, Google ML training frameworks such as TensorFlow and Keras, the Google ML training suite Vertex AI, Google Cloud ML APIs, and the best ML practices in Google Cloud.</p>
<p>In the fourth part of this book, we will prepare for the Google Cloud Certified Professional ML Engineer certification by understanding the certification’s requirements and deep diving into some of the certification’s practice questions.</p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor177"/>Further reading</h1>
<p>To learn more about what was covered in this chapter, take a look at the following resources: </p>
<ul>
<li><a href="https://www.tensorflow.org/tfx">https://www.tensorflow.org/tfx</a></li>
<li><a href="https://www.kubeflow.org/">https://www.kubeflow.org/</a></li>
<li><a href="https://cloud.google.com/architecture/ml-on-gcp-best-practices">https://cloud.google.com/architecture/ml-on-gcp-best-practices</a></li>
<li><a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</a></li>
</ul>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer081">
<h1 id="_idParaDest-177"><a id="_idTextAnchor178"/>Part 4: Accomplishing GCP ML Certification</h1>
<p>In this part, we focus on the Google Cloud Professional Machine Learning Engineer certification. We introduce the GCP ML certification and Google’s official guides. We study the certification exam questions by integrating the knowledge and skills learned from the book.</p>
<p>This part comprises the following chapter:</p>
<ul>
<li><a href="B18333_10.xhtml#_idTextAnchor179"><em class="italic">Chapter 10</em></a>, Achieving the GCP ML Certification</li>
</ul>
</div>
</div>
</body></html>