["```py\nbatch_input = \"s3://{}/{}/{}/\".format(s3_bucket, s3_prefix, 'test')\nbatch_output = \"s3://{}/{}/{}/\".format(s3_bucket, \"xgboost-sample\", 'xform')\ntransformer = estimator.transformer(instance_count=1, \ninstance_type='ml.m5.4xlarge', output_path=batch_output, max_payload=3)\ntransformer.transform(data=batch_input, data_type='S3Prefix', \ncontent_type=content_type, split_type='Line')\n```", "```py\nfrom sagemaker.deserializers import JSONDeserializer\nfrom sagemaker.serializers import CSVSerializer\npredictor = estimator.deploy(initial_instance_count=1,\n                            instance_type='ml.m5.2xlarge',\n                            serializer=CSVSerializer(),\n                            deserializer=JSONDeserializer()\n                             )\n```", "```py\nresult = predictor.predict(csv_payload)\nprint(result)\n```", "```py\n    hyperparameters_v2 = {\n            \"max_depth\":\"10\",\n            \"eta\":\"0.2\",\n            \"gamma\":\"4\",\n            \"min_child_weight\":\"6\",\n            \"subsample\":\"0.7\",\n            \"objective\":\"reg:squarederror\",\n            \"num_round\":\"5\"}\n    estimator_v2 = \\ sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                        hyperparameters=hyperparameters,\n                        role=sagemaker.get_execution_role(),\n                        instance_count=1, \n                        instance_type='ml.m5.12xlarge', \n                        volume_size=200, # 5 GB \n                        output_path=output_path)\n    predictor_v2 = estimator_v2.deploy(initial_instance_count=1,\n                               instance_type='ml.m5.2xlarge',\n                                serializer=CSVSerializer(),\n                              deserializer=JSONDeserializer()\n                                 )\n    ```", "```py\n    model1 = predictor._model_names[0]\n    model2 = predictor_v2._model_names[0]\n    from sagemaker.session import production_variant\n    variant1 = production_variant(model_name=model1,\n                                instance_type=\"ml.m5.xlarge\",\n                                  initial_instance_count=1,\n                                  variant_name='Variant1',\n                                  initial_weight=1)\n    variant2 = production_variant(model_name=model2,\n                                instance_type=\"ml.m5.xlarge\",\n                                  initial_instance_count=1,\n                                  variant_name='Variant2',\n                                  initial_weight=1)\n    ```", "```py\n    from sagemaker.session import Session\n    smsession = Session()\n    smsession.endpoint_from_production_variants(\n        name='mmendpoint',\n        production_variants=[variant1, variant2]\n    )\n    ```", "```py\n    from sagemaker.deserializers import JSONDeserializer\n    from sagemaker.serializers import CSVSerializer\n    import boto3\n    from botocore.response import StreamingBody\n    smrt = boto3.Session().client(\"sagemaker-runtime\")\n    for tl in t_lines[0:50]:\n        result = smrt.invoke_endpoint(EndpointName='mmendpoint',\n             ContentType=\"text/csv\", Body=tl.strip())\n        rbody = StreamingBody( \\\n    raw_stream=result['Body'], \\\n    content_length= \\\n    int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n        print(f\"Result from {result['InvokedProductionVariant']} = \" + \\\n    f\"{rbody.read().decode('utf-8')}\")\n    ```", "```py\n    Result from Variant2 = 0.16384175419807434\n    Result from Variant1 = 0.16383948922157288\n    Result from Variant1 = 0.16383948922157288\n    Result from Variant2 = 0.16384175419807434\n    Result from Variant1 = 0.16384175419807434\n    Result from Variant2 = 0.16384661197662354\n    ```", "```py\n    spark_processor.run(\n        submit_app=\"scripts/preprocess_param.py\",\n        submit_jars=[\"s3://crawler-public/json/serde/json-serde.jar\"],\n        arguments=['--s3_input_bucket', s3_bucket,\n                  '--s3_input_key_prefix', s3_prefix_parquet,\n                   '--s3_output_bucket', s3_bucket,\n                   '--s3_output_key_prefix', f\"{s3_output_prefix}/o3\",\n                   '--parameter', 'o3',],\n        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(s3_bucket, 'sparklogs'),\n        logs=True,\n        configuration=configuration\n    )\n    ```", "```py\n    estimator_o3 = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n                        hyperparameters=hyperparameters,\n                        role=sagemaker.get_execution_role(),\n                        instance_count=1, \n                        instance_type='ml.m5.12xlarge', \n                        volume_size=200,  \n                        output_path=output_path)\n    content_type = \"csv\"\n    train_input = TrainingInput(\"s3://{}/{}/{}/{}/\".format(s3_bucket, s3_output_prefix, 'o3', 'train'), content_type=content_type)\n    validation_input = TrainingInput(\"s3://{}/{}/{}/{}/\".format(s3_bucket, s3_output_prefix, 'o3', 'validation'), content_type=content_type)\n    # execute the XGBoost training job\n    estimator_o3.fit({'train': train_input, 'validation': validation_input})\n    ```", "```py\n    model = estimator_o3.create_model(role=sagemaker.get_execution_role(), image_uri=xgboost_container)\n    from sagemaker.multidatamodel import MultiDataModel\n    model_data_prefix = f's3://{s3_bucket}/{m_prefix}/mma/'\n    model_name = 'xgboost-mma'\n    mme = MultiDataModel(name=model_name,\n                         model_data_prefix=model_data_prefix,\n                         model=model) \n    ```", "```py\n    predictor = mme.deploy(initial_instance_count=1,\n                           instance_type='ml.m5.2xlarge',\n                           endpoint_name=model_name,\n                          serializer=CSVSerializer(),\n                        deserializer=JSONDeserializer())\n    ```", "```py\n    for est in [estimator_o3, estimator_pm25]:\n        artifact_path = \\ est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n        m_name = artifact_path.split('/')[4]+'.tar.gz'\n\n        # This is copying over the model artifact to the S3 location for the MME.\n        mme.add_model(model_data_source=artifact_path, model_data_path=m_name)\n\n    list(mme.list_models())\n    ```", "```py\n    s3.download_file(s3_bucket, f\"{s3_output_prefix}/pm25/test/part-00120-81a51ddd-c8b5-47d0-9431-0a5da6158754-c000.csv\", 'pm25.csv')\n    s3.download_file(s3_bucket, f\"{s3_output_prefix}/o3/test/part-00214-ae1a5b74-e187-4b62-ae4a-385afcbaa766-c000.csv\", 'o3.csv')\n    ```", "```py\n    with open('pm25.csv', 'r') as TF:\n        pm_lines = TF.readlines()\n    with open('o3.csv', 'r') as TF:\n        o_lines = TF.readlines()\n    for tl in pm_lines[0:5]:\n        result = predictor.predict(data = tl.strip(), target_model='pm25.tar.gz')\n        print(result)\n    for tl in o_lines[0:5]:\n        result = predictor.predict(data = tl.strip(), target_model='o3.tar.gz')\n        print(result)\n    ```", "```py\npredictor_ei = predictor.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge', \n                    serializer=CSVSerializer(),\n                    deserializer=JSONDeserializer(),\n                    accelerator_type='ml.eia2.medium')\n```", "```py\n    ncols = len(t_lines[0].split(','))\n    ```", "```py\n    import sagemaker\n    from sagemaker.model import Model\n    n_prefix = 'xgboost-sample-neo'\n    n_output_path = 's3://{}/{}/{}/output'.format(s3_bucket, n_prefix, 'xgboost-neo')\n    m1 = Model(xgboost_container,model_data=estimator\\    .latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts'], \n               role=sagemaker.get_execution_role())\n    neo_model = m1.compile('ml_m5', \n               {'data':[1, ncols]}, \n               n_output_path, \n               sagemaker.get_execution_role(), \n               framework='xgboost', \n               framework_version='latest',\n               job_name = 'neojob')\n    ```", "```py\n    neo_predictor = neo_model.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge', \n                        serializer=CSVSerializer(),\n                        deserializer=JSONDeserializer(),\n                        endpoint_name='neo_endpoint')\n    ```", "```py\n    for tl in t_lines[0:5]:\n        result = smrt.invoke_endpoint(EndpointName='neo_endpoint',\n                        ContentType=\"text/csv\",\n                        Body=tl.strip())\n        rbody = \\ StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n        print(f\"Result from {result['InvokedProductionVariant']} = {rbody.read().decode('utf-8')}\")\n    ```"]