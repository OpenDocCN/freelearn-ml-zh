<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;2.&#xA0;Tidying Data and Measuring Performance" id="I3QM1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Tidying Data and Measuring Performance</h1></div></div></div><p class="calibre8">In this chapter, we will cover the topics of tidying your data in preparation for predictive modeling, performance metrics, cross-validation, and learning curves.</p><p class="calibre8">In statistics, it is an accepted concept that there are two types of data, which are: </p><div class="book"><ul class="itemizedlist"><li class="listitem">Untidy</li><li class="listitem">Tidy</li></ul></div><p class="calibre8">Untidy data is considered to be raw or messy; tidy data is data that has gone through a quality assurance process and is ready to be used.</p></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Tidying Data and Measuring Performance" id="I3QM1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Getting started"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec12" class="calibre1"/>Getting started</h1></div></div></div><p class="calibre8">Before <a id="id141" class="calibre1"/>we get started with discussing the process of tidying data, it would be very prudent to point out that whatever you do to tidy your data, you should be sure to:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Create <a id="id142" class="calibre1"/>and save your scripts so that you can use them again for new or similar data sources. This is referred to as <span class="strong"><strong class="calibre2">reusability</strong></span>. Why spend time recreating the same code, rules, or logic if you don't have to? This applies to <span class="strong"><em class="calibre9">new data</em></span> within the <span class="strong"><em class="calibre9">same project</em></span> (that the scripts were developed for) or new projects you may be involved with in the future.</li><li class="listitem" value="2">Tidy your data as "far upstream" as possible, perhaps even at the original source. In other words, save and maintain the original data, but use programmatic scripts to clean it, fix mistakes, and save that <span class="strong"><em class="calibre9">cleaned</em></span> dataset for further analysis.</li></ol><div class="calibre13"/></div></div></div>
<div class="book" title="Tidying data" id="J2B81-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec13" class="calibre1"/>Tidying data</h1></div></div></div><p class="calibre8">It is worth <a id="id143" class="calibre1"/>clarifying what the idea of <span class="strong"><em class="calibre9">tidying data</em></span> means. Tidying data is the process of <span class="strong"><em class="calibre9">reorganizing</em></span> (or perhaps just <span class="strong"><em class="calibre9">organizing</em></span>) data, as well as addressing perceived issues or concerns someone has identified within your data. Issues affect the quality of data. Data quality, of course, is relative to the proposed purpose of use (of the data).</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Categorizing data quality" id="K0RQ1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec14" class="calibre1"/>Categorizing data quality</h1></div></div></div><p class="calibre8">It is <a id="id144" class="calibre1"/>perhaps an accepted notion that issues with data quality may be categorized into one of the following areas:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Accuracy</li><li class="listitem">Completeness</li><li class="listitem">Update status</li><li class="listitem">Relevance</li><li class="listitem">Consistency (across sources)</li><li class="listitem">Reliability</li><li class="listitem">Appropriateness</li><li class="listitem">Accessibility</li></ul></div><p class="calibre8">The quality <a id="id145" class="calibre1"/>or level of quality of your data can be affected by the way it is entered, stored, and managed. The process of addressing data quality (referred to most often as <span class="strong"><strong class="calibre2">data quality assurance</strong></span> (<span class="strong"><strong class="calibre2">DQA</strong></span>)) requires a routine and regular review and evaluation of the data and performing ongoing processes termed <span class="strong"><em class="calibre9">profiling</em></span> and <span class="strong"><em class="calibre9">scrubbing</em></span> (this is vital even if the data is stored in multiple disparate systems, making these processes difficult).</p><p class="calibre8">Here, tidying the data will be much more project centric in that we're probably not concerned with creating a formal DQA process, but are only concerned with making certain that the data is correct for your particular predictive project.</p><p class="calibre8">In statistics, data unobserved or not yet reviewed by the data scientist is considered <span class="strong"><em class="calibre9">raw</em></span> and cannot be reliably used in predictive projects. The process of tidying the data will usually involve several steps. Taking the extra time to break out the work is strongly recommended (rather than haphazardly addressing multiple data issues together).</p></div>

<div class="book" title="Categorizing data quality" id="K0RQ1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="The first step"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec22" class="calibre1"/>The first step</h2></div></div></div><p class="calibre8">The first <a id="id146" class="calibre1"/>step requires bringing the data to what may be called <span class="strong"><em class="calibre9">mechanical</em></span> correctness. In this first step, you focus on things such as:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">File format and organization</strong></span>: Field order, column headers, number of records, and so on</li><li class="listitem"><span class="strong"><strong class="calibre2">Record data typing</strong></span> (such as numeric values stored as strings)</li><li class="listitem"><span class="strong"><strong class="calibre2">Date and time processing</strong></span> (typically reformatting values into standard formats or consistent formats)</li><li class="listitem"><span class="strong"><strong class="calibre2">Miss-content</strong></span>: Wrong category labels, unknown or unexpected character encoding, and so on</li></ul></div></div></div>

<div class="book" title="Categorizing data quality" id="K0RQ1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="The next step"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec23" class="calibre1"/>The next step</h2></div></div></div><p class="calibre8">The second <a id="id147" class="calibre1"/>step is to address the <span class="strong"><em class="calibre9">statistical soundness</em></span> of the data. Here we correct issues that may be <span class="strong"><em class="calibre9">mechanically correct</em></span> but will most likely (depending upon the subject matter) impact a statistical outcome. </p><p class="calibre8">These issues may include:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Positive/negative mismatch</strong></span>: Age variables may be reported as negative</li><li class="listitem"><span class="strong"><strong class="calibre2">Invalid (based on accepted logic) data</strong></span>: An under-aged person may be registered to possess a driver's license</li><li class="listitem"><span class="strong"><strong class="calibre2">Missing data</strong></span>: Key data values may just be missing from the data source</li></ul></div></div></div>

<div class="book" title="Categorizing data quality" id="K0RQ1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="The final step"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec24" class="calibre1"/>The final step</h2></div></div></div><p class="calibre8">Finally, the <a id="id148" class="calibre1"/>last step (before actually attempting to use the data) may be the <span class="strong"><em class="calibre9">re-formatting</em></span> step. In this step, the data scientist will determine the form that the data must be in in order to most efficiently process it, based upon the intended use or objective. </p><p class="calibre8">For example, one might decide to:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Reorder or repeat</strong></span> columns; that is to say, some final processing may require redundant or repeated data be generated within a file source to be correctly or more easily processed</li><li class="listitem"><span class="strong"><strong class="calibre2">Drop</strong></span> columns and/or records (based upon specific criteria)</li><li class="listitem"><span class="strong"><strong class="calibre2">Set decimal places</strong></span></li><li class="listitem"><span class="strong"><strong class="calibre2">Pivot</strong></span> data</li><li class="listitem"><span class="strong"><strong class="calibre2">Truncate or rename</strong></span> values</li><li class="listitem">And so on</li></ul></div><p class="calibre8">There are a variety of somewhat routine methods for using R to resolve the aforementioned <a id="id149" class="calibre1"/>data errors.</p><p class="calibre8">For example:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Changing a data type</strong></span>: Also referred to as "data type conversion," one can utilize the R <code class="email">is</code> functions to test for an object's data type and the <code class="email">as</code> functions for an explicit conversion. A simplest example is shown here:<div class="mediaobject"><img src="../images/00018.jpeg" alt="The final step" class="calibre10"/></div><p class="calibre16"> </p></li><li class="listitem"><span class="strong"><strong class="calibre2">Date and time</strong></span>: There are multiple ways to manage date information with R. In fact, we can extend the preceding example and mention the <code class="email">as.Date</code> function. Typically, date values are important to a statistical model and therefore it is important to take the time to understand the format of a model's date fields and ensure that they are properly dealt with. Mostly, dates and times will appear in raw data format as strings, which can be converted and formatted as required. In the following code, the string fields containing a <code class="email">saledate</code> and a <code class="email">returndate</code> are converted to date type values and used with a common time function, <code class="email">difftime</code>:<div class="mediaobject"><img src="../images/00019.jpeg" alt="The final step" class="calibre10"/></div><p class="calibre16"> </p></li><li class="listitem">Category labels are critical to statistical modeling as well as data visualization. An example of using labels with a sample of categorized data might be assigning a label to a participant in a study, perhaps by <span class="strong"><em class="calibre9">level of education</em></span>: 1 = Doctoral, 2 = Masters, 3 = Bachelors, 4 = Associates, 5 = Nondegree, 6 = Some College, 7 = High School, or 8 = None:<div class="informalexample"><pre class="programlisting">&gt; participant&lt;-c(1,2,3,4,5,6,7,8)
&gt; recode&lt;-c(Doctoral=1, Masters=2, Bachelors=3, Associates=4, Nondegree=5, SomeCollege=6, HighSchool=7, None=8))
&gt; (participant&lt;-factor (participant, levels=recode, labels=names(recode)))
[1] Doctoral Masters Bachelors Associates Nondegree SomeCollege HighSchool None       
Levels: Doctoral Masters Bachelors Associates Nondegree SomeCollege HighSchool None</pre></div></li><li class="listitem">Assigning labels to data not only helps with readability, but allows a machine learning algorithm to learn from the sample, and apply the same labels to other, unlabeled data.</li><li class="listitem"><span class="strong"><strong class="calibre2">Missing data parameters</strong></span>: many times missing data can be excluded from a calculation simply by setting an appropriate parameter value. For example, the R functions <code class="email">var</code>, <code class="email">cov</code>, and <code class="email">cor</code> compute variance, covariance or correlation of variables. These functions have the option to set <code class="email">na.rm</code> to TRUE. Doing this tells R to exclude any and all records or cases with missing values.</li><li class="listitem">Various <a id="id150" class="calibre1"/>other <span class="strong"><em class="calibre9">data tidying</em></span> nuisances can exist within your data, such as incorrectly signed numeric data (that is, a negative value for data such as a participant's age), invalid data values based upon accepted scenario logic (for example, participant's age versus level of education, in that it isn't feasible that a 10-year-old would have earned a Master's degree), data values simply missing (is a participant's lack of response an indication of a not applicable question or an error?), and more. Thankfully, there are at least several approaches to these data scenarios with R.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performance metrics"><div class="book" id="KVCC2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec15" class="calibre1"/>Performance metrics</h1></div></div></div><p class="calibre8">In the <a id="id151" class="calibre1"/>previous chapter, where we talked about the predictive modeling process, we delved into the importance of assessing a trained model's performance using training and test datasets. In this section, we will look at specific measures of performance that we will frequently encounter when describing the predictive accuracy of different models. It turns out that depending on the class of the problem, we will need to use slightly different ways of assessing (the model's) performance. As we focus on supervised models in this book, we will look at how to assess regression models and classification models. For classification models, we will also discuss some additional metrics used for the binary classification task, which is a very important and frequently encountered type of problem.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note07" class="calibre1"/>Note</h3><p class="calibre8">Note: In statistics, the term performance is usually interchangeable with accuracy.</p></div></div>

<div class="book" title="Performance metrics">
<div class="book" title="Assessing regression models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec25" class="calibre1"/>Assessing regression models</h2></div></div></div><p class="calibre8">In a <a id="id152" class="calibre1"/>regression scenario, let's recall that through our model we are building a function that is an estimate of a theoretical underlying <a id="id153" class="calibre1"/>target function <span class="strong"><em class="calibre9">f</em></span>. The model's inputs are the values of our chosen input features. If we apply this function to every observation, <span class="strong"><em class="calibre9">x<sub class="calibre14">i</sub></em></span>, in our training data, which is labeled with the true value of the function, <span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span>, we will obtain a <a id="id154" class="calibre1"/>set of pairs. To make sure we are clear on this last point, the first entry is the actual value of the output variable in our training data for the <span class="strong"><em class="calibre9">i<sup class="calibre15">th</sup></em></span> observation, and the second entry is the predicted value for this particular observation produced by using our model on the feature values for this observation.</p><p class="calibre8">If our model has fit the data well, both values will be very close to each other in the training set. If this is also true for our test set, then we consider that our model is likely to perform well for future unseen observations. To quantify the notion that the predicted and correct values are close to each other for all the observations in a dataset, we define a measure known as the <span class="strong"><strong class="calibre2">Mean Square Error</strong></span> (<span class="strong"><strong class="calibre2">MSE</strong></span>), as follows:</p><div class="mediaobject"><img src="../images/00020.jpeg" alt="Assessing regression models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre9">n</em></span> is the total number of observations in the dataset. Consequently, this equation tells us to first compute the squared difference between an output value and its predicted value for every observation, <span class="strong"><em class="calibre9">i</em></span>, in the test set, and then take the average of all these values by summing them up and dividing by the number of observations. Thus, it should be clear <a id="id155" class="calibre1"/>why this measure is called the mean square error. The lower this number, the lower the average error between the actual value of the <a id="id156" class="calibre1"/>output variable in our observations and what we predict and, therefore, the more accurate our model. We sometimes make reference to the <span class="strong"><strong class="calibre2">Root Mean Square Error</strong></span> (<span class="strong"><strong class="calibre2">RMSE</strong></span>), which is just the square root of the MSE and the <span class="strong"><strong class="calibre2">Sum of Squared Error</strong></span> (<span class="strong"><strong class="calibre2">SSE</strong></span>), which is similar to the MSE but without the normalization which results from dividing by the number of training examples, <span class="strong"><em class="calibre9">n</em></span>. These quantities, when computed on the training dataset, are valuable in the sense that a low number will indicate that we have trained a model sufficiently well. We know that we aren't expecting this to be zero in general, and we also cannot decide between models on the basis of these quantities because of the problem of overfitting. </p><p class="calibre8">The key place to compute these measures <a id="id157" class="calibre1"/>is on the test data. In a majority of cases, a model's training data MSE (or equally, RMSE or SSE) will be lower than the corresponding measure computed on the test data. A model <span class="strong"><em class="calibre9">m<sub class="calibre14">1</sub></em></span> that overfits the data compared to <a id="id158" class="calibre1"/>another model <span class="strong"><em class="calibre9">m<sub class="calibre14">2</sub></em></span> can often be identified as such when the <span class="strong"><em class="calibre9">m<sub class="calibre14">1</sub></em></span> model produces a lower training MSE but higher test MSE than model <span class="strong"><em class="calibre9">m<sub class="calibre14">2</sub></em></span>.</p></div></div>

<div class="book" title="Performance metrics">
<div class="book" title="Assessing classification models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec26" class="calibre1"/>Assessing classification models</h2></div></div></div><p class="calibre8">In regression <a id="id159" class="calibre1"/>models, the degree to which our predicted function incorrectly approximates an output, <span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span>, for a particular <a id="id160" class="calibre1"/>observation, <span class="strong"><em class="calibre9">x<sub class="calibre14">i</sub></em></span>, is taken into account by the MSE. Specifically, large errors are squared, so a very large deviation on one data point can have a more significant impact than a few small deviations across more than one data point. It is precisely because we are dealing with a numerical output in regression that we can measure not only for which observations we aren't doing a good job at predicting, but also how far off we are.</p><p class="calibre8">For models that perform classification, we can again define an error rate, but here we can only talk about the number of misclassifications that were made by our model. Specifically, we have an error rate given by:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Assessing classification models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This measure uses the <code class="email">indicator</code> function to return the value of 1 when the predicted class is not the same as the labeled class. Thus, the error rate is computed by counting the number of times the class of the output variable is incorrectly predicted, and dividing this count by the number of observations in the dataset. In this way, we can see that the error rate is actually the percentage of misclassified observations made by our model. It should be noted that this measure treats all types of misclassifications as equal. If the cost of some misclassifications is higher than others, then this measure can be adjusted by adding in weights that multiply each misclassification by an amount proportional to its cost.</p><p class="calibre8">If we want to diagnose the greatest source of error in a regression problem, we tend to look at the points for which we have the largest error between our predicted value and the actual value. When doing classifications, it is often very useful to compute what is known as the confusion matrix. This is a matrix that shows all pairwise misclassifications that were made on our data. We shall now return to our iris species classification problem. In a previous section, we trained three kNN models. We'll now see how we can assess their performance. Like many classification models, kNN can return predictions either as final class labels or via a set of scores pertaining to each possible output class. Sometimes, as is the case here, these scores are actually probabilities that the model has assigned to every possible output. Regardless of whether the scores are actual probabilities, we can decide on which output label to pick on the basis of these scores, typically by simply choosing the <a id="id161" class="calibre1"/>label with the highest score.</p><p class="calibre8">In R, the most common function to make model predictions is the <code class="email">predict()</code> function, which <a id="id162" class="calibre1"/>we will use with our kNN models:</p><div class="informalexample"><pre class="programlisting">&gt; knn_predictions_prob &lt;- predict(knn_model, iris_test,
  type = "prob")
&gt; tail(knn_predictions_prob, n = 3)
      setosa versicolor virginica
[28,]      0        0.0       1.0
[29,]      0        0.4       0.6
[30,]      0        0.0       1.0</pre></div><p class="calibre8">In the kNN model, we can assign output scores as direct probabilities by computing the ratio of the nearest neighbors that belong to each output label. In the three test examples shown, the virginica species has unit probabilities in two of them, but only 60 percent probability for the remaining example. The other 40 percent belong to the versicolor species, so it seems that in the latter case, three out of five nearest neighbors were of the virginica species, whereas the other two were of the versicolor species. It is clear that we should be more confident about the two former classifications than the latter. </p><p class="calibre8">We'll now compute class predictions for the three models on the test data:</p><div class="informalexample"><pre class="programlisting">&gt; knn_predictions &lt;- predict(knn_model, iris_test, type = "class")
&gt; knn_predictions_z &lt;- predict(knn_model_z, iris_test_z, type = "class")
&gt; knn_predictions_pca &lt;- predict(knn_model_pca, iris_test_pca, type = "class")</pre></div><p class="calibre8">We can use the <code class="email">postResample()</code> function from the <code class="email">caret</code> package to display test set accuracy metrics for our models:</p><div class="informalexample"><pre class="programlisting">&gt; postResample(knn_predictions, iris_test_labels)
 Accuracy     Kappa 
0.9333333 0.9000000 
&gt; postResample(knn_predictions_z, iris_test_labels)
 Accuracy     Kappa 
0.9666667 0.9500000 
&gt; postResample(knn_predictions_pca, iris_test_labels)
Accuracy    Kappa 
    0.90     0.85</pre></div><p class="calibre8">Here, accuracy is one minus the error rate and is thus the percentage of correctly classified observations. We can see that all the models perform very closely in terms of accuracy, with the model that uses a Z-score normalization prevailing. This difference is not significant given the small size of the test set. </p><p class="calibre8">This is defined as follows:</p><div class="mediaobject"><img src="../images/00022.jpeg" alt="Assessing classification models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The Kappa statistic is designed to counterbalance the effect of random chance and takes values <a id="id163" class="calibre1"/>in the interval, [-1,1], where 1 indicates perfect accuracy, -1 indicates perfect inaccuracy, and 0 occurs when the accuracy is exactly what would be obtained by a random guesser. Note that a random <a id="id164" class="calibre1"/>guesser for a classification model guesses the most frequent class. In the case of our iris classification model, the three species are equally represented in the data, and so the expected accuracy is one-third. The reader is encouraged to check that by using this value for the expected accuracy; we can obtain the observed values of the Kappa statistic from the accuracy values.</p><p class="calibre8">We can also examine the specific misclassifications that our model makes, using a confusion matrix. </p><p class="calibre8">This can be simply constructed by cross-tabulating the predictions with the correct output labels:</p><div class="informalexample"><pre class="programlisting">&gt; table(knn_predictions, iris_test_labels) 
               iris_test_labels
knn_predictions setosa versicolor virginica
     setosa         10          0         0
     versicolor      0          9         1
     virginica       0          1         9</pre></div><p class="calibre8">The <code class="email">caret</code> package also has the very useful <code class="email">confusionMatrix()</code> function, which automatically computes this table as well as several other performance metrics, the explanation of which can be found at <a class="calibre1" href="http://topepo.github.io/caret/other.html">http://topepo.github.io/caret/other.html</a>.</p><p class="calibre8">In the preceding confusion matrix, we can see that the total number of correctly classified observations is 28, which is the sum of the numbers <code class="email">10</code>, <code class="email">9</code>, and <code class="email">9</code> on the leading diagonal. The table in the output shows us that the setosa species seems to be easier to predict with our model, as it is never confused with other species. The <code class="email">versicolor</code> and <code class="email">virginica</code> species, however, can be confused with each other, and the model has misclassified one instance of each. We can therefore surmise that computing the confusion matrix serves as a useful exercise. Spotting class pairs that are frequently confused will guide us to improve <a id="id165" class="calibre1"/>our model, for example, by <a id="id166" class="calibre1"/>looking for features that might help distinguish these classes.</p><div class="book" title="Assessing binary classification models"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec07" class="calibre1"/>Assessing binary classification models</h3></div></div></div><p class="calibre8">A special <a id="id167" class="calibre1"/>case of classification known as a binary classification occurs when we have two classes. Here are some typical binary classification scenarios:</p><div class="book"><ul class="itemizedlist"><li class="listitem">We want to classify incoming emails as spam or not spam using the email's content and header</li><li class="listitem">We want to classify a patient as having a disease or not using their symptoms and medical history</li><li class="listitem">We want to classify a document from a large database of documents as being relevant to a search query, based on the words in the query and the words in the document</li><li class="listitem">We want to classify a product from an assembly line as faulty or not</li><li class="listitem">We want to predict whether a customer applying for credit at a bank will default on their payments, based on their credit score and financial situation</li></ul></div><p class="calibre8">In a binary classification task, we usually refer to our two classes as the positive class and the negative class. By convention, the positive class corresponds to a special case that our model is trying to predict, and is often rarer than the negative class. From the preceding examples, we would use the positive class label for our spam emails, faulty assembly line products, defaulting customers, and so on. Now consider an example in the medical diagnosis domain, where we are trying to train a model to diagnose a disease that we know is only present in 1 in 10,000 of the population. We would assign the positive class to patients that have this disease. Notice that in such a scenario, the error rate alone is not an adequate measure of a model. For example, we can design the simplest of classifiers that will have an error rate of only 0.01 percent by predicting that every patient will be healthy, but such a classifier would be useless. We can come up with more useful metrics by examining the confusion matrix. Suppose that we had built a model to diagnose our rare disease and on a test sample of 100,000 patients, we obtained the following confusion matrix:</p><div class="informalexample"><pre class="programlisting">&gt; table(actual,predicted)
          predicted
actual     negative positive
  negative    99900       78
  positive        9       13</pre></div><p class="calibre8">The binary classification problem is so common that the cells of the binary confusion matrix have their own names. On the leading diagonal, which contains the correctly classified entries, we refer to the elements as the true negatives and true positives. In our case, we had 99900 true negatives and 13 true positives. When we misclassify an observation <a id="id168" class="calibre1"/>as belonging to the positive class when it actually belongs to the negative class, then we have a false positive, also known as a Type I error. A false negative or Type II error occurs when we misclassify a positive observation as belonging to the negative class. In our case, our model had 78 false positives and 9 false negatives.</p><p class="calibre8">We'll now introduce two very important measures in the context of binary classification, which are precision and recall. Precision is defined as the ratio of the number of correctly predicted instances of the positive class to the total number of predicted instances of the positive class. Using the labels from the preceding binary confusion matrix, precision is given by:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Assessing binary classification models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Precision, thus, essentially measures how accurate we are in making predictions for the positive class. By definition, we can achieve 100 percent precision by never making any predictions for the positive class, as this way we are guaranteed to never make any mistakes. Recall, by contrast, is defined as the number of correct predictions for the positive class over all the members of the positive class in our dataset. Once again, using the labels from the binary confusion matrix, we can see the definition of recall as:</p><div class="mediaobject"><img src="../images/00024.jpeg" alt="Assessing binary classification models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Recall measures our ability to identify all the positive class members from our dataset. We can easily achieve maximum recall by always predicting the positive class for all our data points. We will make a lot of mistakes, but we will never have any false negatives. Notice that precision and recall form a tradeoff in our model's performance. At one end, if we don't predict the positive class for any of our data points, we will have zero recall but maximum precision. At the other end, if all our data points are predicted as belonging to the positive class (which, remember, is usually a rare class), we will have maximum recall but extremely low precision. Put differently, trying to reduce the Type I error leads to increasing the Type II error and vice versa. This inverse relationship is often plotted for a particular problem on a precision-recall curve. By using an appropriate threshold parameter, we can often tune the performance of our model in such a way that we achieve a specific <a id="id169" class="calibre1"/>point on this precision-recall curve that is appropriate for our circumstances. For example, in some problem domains, we tend to be biased toward having a higher recall than a higher precision, because of the high cost of misclassifying an observation from the positive class into the negative class. As we often want to describe the performance of a model using a single number, we define a measure known as the F1 score, which combines precision and recall. Specifically, the F1 score is defined as the harmonic mean between precision and recall:</p><div class="mediaobject"><img src="../images/00025.jpeg" alt="Assessing binary classification models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The reader should verify that in our example confusion matrix, precision is 14.3 percent, recall is 59.1 percent, and the F1 score is 0.23.</p></div></div></div>
<div class="book" title="Cross-validation" id="LTSU1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec16" class="calibre1"/>Cross-validation</h1></div></div></div><p class="calibre8">Cross-validation (which you may hear some data scientists refer to as <span class="strong"><em class="calibre9">rotation estimation</em></span>, or simply <a id="id170" class="calibre1"/>a general technique for assessing models), is another method for assessing a model's performance (or its accuracy). </p><p class="calibre8">Mainly used with predictive modeling to estimate how accurately a model might perform in practice, one might see cross-validation used to check how a model will potentially generalize; in other words, how the model will apply what it infers from samples, to an entire population (or dataset).</p><p class="calibre8">With cross-validation, you identify a (known) dataset as your validation dataset on which training is run, along with a dataset of unknown data (or first seen data) against which the model will be tested (this is known as your testing dataset). The objective is to ensure that problems such as overfitting (allowing non-inclusive information to influence results) are controlled, as well as provide an insight on how the model will generalize a real problem or on a real data file.</p><p class="calibre8">This process will consist of separating data into samples of similar subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set):</p><p class="calibre8">Separation → Analysis → Validation</p><p class="calibre8">To reduce variability, multiple iterations (also called folds or rounds) of cross-validation are performed using different partitions, and the validation results are averaged over the rounds. Typically, a data scientist will use a model's stability to determine the actual number <a id="id171" class="calibre1"/>of rounds of cross-validation that should be performed.</p><p class="calibre8">Again, the cross-validation method can perhaps be better understood by thinking about selecting a subset of data and manually calculating the results. Once you know the correct results, they can be compared to the model-produced results (using a separate subset of data). This is one round. Multiple rounds would be performed and the compared results averaged and reviewed, eventually providing a fair estimate of a model's prediction performance.</p><p class="calibre8">Suppose a university provides data on its student body over time. The students are described as having various characteristics, such as having a High School GPA greater or less than 3.0, if they have a family member that graduated from the school, if the student was active in non-program activities, was a resident (lived on campus), was a student athlete, and so on. Our predictive model wants to predict what characteristics students who graduate early have.</p><p class="calibre8">The following table is a representation of the results of using a five-round cross-validation process to predict our model's expected accuracy:</p><div class="mediaobject"><img src="../images/00026.jpeg" alt="Cross-validation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Given the preceding figures, I'd say our predictive model is expected to be very accurate!</p><p class="calibre8">In summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance. This method is typically used <a id="id172" class="calibre1"/>in cases where there is not enough data available to test without losing significant modeling or testing quality.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Learning curves" id="MSDG1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec17" class="calibre1"/>Learning curves</h1></div></div></div><p class="calibre8">Another <a id="id173" class="calibre1"/>method of assessing a model's performance is by evaluating the model's growth of learning or the model's ability to improve learning (obtain a better score) with additional experience (for example, more rounds of cross-validation).</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note08" class="calibre1"/>Note</h3><p class="calibre8">Learning is the act of acquiring new, or modifying and reinforcing existing, knowledge.</p></div><p class="calibre8">The information indicating a model's result or score with a data file population can be combined with other scores to show a line or curve, which is known as a model's learning curve.</p><p class="calibre8">A learning curve is a graphical representation of the growth of learning (the scores shown in a vertical axis) with practice (the individual data files or rounds shown in the horizontal axis).</p><p class="calibre8">This can also be conceptualized as:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The same task repeated in a series</li><li class="listitem">A body of knowledge learned over time</li></ul></div><p class="calibre8">The following figure illustrates a hypothetical learning curve, showing the improved learning of a predictive model using resultant scores by cross-validation round:</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="Learning curves" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Source link: <a class="calibre1" href="https://en.wikipedia.org/wiki/File:Alanf777_Lcd_fig01.png">https://en.wikipedia.org/wiki/File:Alanf777_Lcd_fig01.png</a>
</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip07" class="calibre1"/>Tip</h3><p class="calibre8">It's funny; one might know that the familiar expression <span class="strong"><em class="calibre9">it's a steep learning curve</em></span> is intended to describe an activity that is tough to learn, but in statistics, a learning curve with a steep start would actually represent a rapidly improving progress.</p></div><p class="calibre8">Learning <a id="id174" class="calibre1"/>curves relating model performance to experience are commonly found to be used when performing model assessments.</p><p class="calibre8">As we have mentioned earlier in this section, performance (or the scores) is meant to be the accuracy of a model while experience (or round) may be the number of training examples, datasets, or iterations used in optimizing the model parameters.</p></div>

<div class="book" title="Learning curves" id="MSDG1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Plot and ping"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec27" class="calibre1"/>Plot and ping</h2></div></div></div><p class="calibre8">Using two generic R functions, we can demonstrate a simple learning curve visualization. Ping will <a id="id175" class="calibre1"/>open an image file which will hold our learning curve <a id="id176" class="calibre1"/>visualization so we can easily include it in a document later, and plot will draw our graphic.</p><p class="calibre8">The following are our example R code statements:</p><div class="informalexample"><pre class="programlisting"># -- 5 rounds of numeric test scores saved in a vector named "v"
v &lt;-c(74,79, 88, 90, 99)

# -- create an image file for the visualization for later use
png(file = "c:/simple example/learning curve.png", type = c("windows", "cairo", "cairo-png"))

# -- plot the model scores round by round
plot(v, type = "o", col = "red", xlab = "Round", ylab = "Score", main = "Learning Curve")

# -- close output 
dev.off()</pre></div><p class="calibre8">The <a id="id177" class="calibre1"/>preceding <a id="id178" class="calibre1"/>statements create the following graphic as a file:</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Plot and ping" class="calibre10"/></div><p class="calibre11"> </p></div></div>
<div class="book" title="Summary" id="NQU21-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec18" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we explored the fundamental ideas surrounding issues and concerns with data quality and how to categorize quality issues by their type, as well as presented ideas for tidying up your data.</p><p class="calibre8">In order to compare the performance of the different models that one may create, we went on to establish some fundamental notions of model performance, such as the <span class="strong"><strong class="calibre2">mean squared error</strong></span> (<span class="strong"><strong class="calibre2">MSE</strong></span>) for regression and the classification error rate for classification.</p><p class="calibre8">We also introduced cross-validation as a generic assessment technique to be used in cases where there is a limited amount of data available.</p><p class="calibre8">Finally, learning curves were discussed as a way to judge the ability of a model to improve its scores or ability to learn.</p><p class="calibre8">With a firm grounding in the basics of the predictive modeling process, we will look at linear regression in the next chapter.</p></div></body></html>