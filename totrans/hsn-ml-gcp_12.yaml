- en: Preventing Overfitting with Regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则化防止过拟合
- en: So far, in the previous chapters, we understood about building neural network,
    evaluating the TensorBoard results, and varying the hyperparameters of the neural
    network model to improve the accuracy of the model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前面的章节中，我们了解了构建神经网络、评估TensorBoard结果以及调整神经网络模型的超参数以提高模型准确率。
- en: While the hyperparameters in general help with improving the accuracy of model,
    certain configuration of hyperparameters results in the model overfitting to the
    training data, while not generalizing for testing data is the problem of overfitting
    to the training data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然超参数通常有助于提高模型的准确率，但某些超参数的配置会导致模型过度拟合训练数据，而无法泛化测试数据，这就是过拟合训练数据的问题。
- en: 'A key parameter that can help us in avoiding overfitting while generalizing
    on an unseen dataset is the regularization technique. Some of the key regularization
    techniques are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键参数可以帮助我们在泛化未见数据集的同时避免过拟合，那就是正则化技术。以下是一些关键的正则化技术：
- en: L2 regularization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化
- en: L1 regularization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1正则化
- en: Dropout
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout
- en: Scaling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放
- en: Batch normalization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化
- en: Weight initialization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重初始化
- en: 'In this chapter, we will go through the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下内容：
- en: Intuition of over/under fitting
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度/欠拟合的直觉
- en: Reducing overfitting using regularization
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化减少过拟合
- en: Improving the underfitting scenario
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善欠拟合场景
- en: Intuition of over/under fitting
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过度/欠拟合的直觉
- en: Before we understand about how the preceding techniques are useful, let's build
    a scenario, so that we understand the phenomenon of overfitting.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解前面技术如何有用之前，让我们构建一个场景，以便我们了解过拟合的现象。
- en: '**Scenario 1: A case of not generalizing on an unseen dataset**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景1：在未见数据集上未泛化的案例**'
- en: 'In this scenario, we will create a dataset, for which there is a clear linearly
    separable mapping between input and output. For example, whenever the independent
    variables are positive, the output is `[1,0]`, and when the input variables are
    negative, the output is `[0,1]`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们将创建一个数据集，其中输入和输出之间存在清晰的线性可分映射。例如，每当独立变量为正时，输出为 `[1,0]`，而当输入变量为负时，输出为
    `[0,1]`：
- en: '![](img/1f2ad6e3-05f0-47df-9eee-b87485c35826.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f2ad6e3-05f0-47df-9eee-b87485c35826.png)'
- en: 'To that dataset, we will add a small amount of noise (10% of the preceding
    dataset created) by adding some data points that follow the opposite of the preceding
    pattern, that is, when the input variables are positive, the output is `[0,1]`,
    and the output is `[1,0]` when the input variables are negative:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了那个数据集，我们将通过添加一些遵循先前模式相反的数据点来添加少量噪声（前一个数据集的10%），即当输入变量为正时，输出为 `[0,1]`，而当输入变量为负时，输出为
    `[1,0]`：
- en: '![](img/13da13ac-d612-4e7e-a42a-f04ca911688a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/13da13ac-d612-4e7e-a42a-f04ca911688a.png)'
- en: 'Appending the datasets obtained by the preceding two steps gives us the training
    dataset, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将前两个步骤获得的数据集附加在一起，就得到了训练数据集，如下所示：
- en: '![](img/14edfbdf-5912-4c66-a5fc-4973132f5bbb.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14edfbdf-5912-4c66-a5fc-4973132f5bbb.png)'
- en: 'In the next step, we create the test dataset, where it follows the criterion
    followed by the majority of the training dataset, that is, when the input is positive,
    the output is `[1,0]`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将创建测试数据集，其中它遵循大多数训练数据集的准则，即当输入为正时，输出为 `[1,0]`：
- en: '![](img/f9d36cea-4bc8-454e-be9f-8d0541614933.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f9d36cea-4bc8-454e-be9f-8d0541614933.png)'
- en: Now that we have created the dataset, let's go ahead and build a model to predict
    the output with the given inputs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了数据集，让我们继续构建一个模型来预测给定的输入的输出。
- en: The intuition here is that, if training accuracy improves by more than 90.91%
    it is a classic case of overfitting, as the model tries to fit for the minority
    of the observations which do not generalize for an unseen dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉是，如果训练准确率提高了超过90.91%，那么它就是一个典型的过拟合案例，因为模型试图拟合那些不适用于未见数据集的少数观察结果。
- en: 'To check that—let''s first import all the relevant packages to build a model
    in `keras`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查这一点——让我们首先导入所有相关的包来在 `keras` 中构建一个模型：
- en: 'We build a model with three layers where the layers have 1,000, 500 and 100
    units in each respective hidden layer:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个具有三个层的模型，其中每个隐藏层有1,000、500和100个单元：
- en: '![](img/0848aeeb-7310-4057-a8c1-430d6cdc486a.png)![](img/5afb4ede-cd47-4e44-856f-04e064ce4615.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0848aeeb-7310-4057-a8c1-430d6cdc486a.png)![图片](img/5afb4ede-cd47-4e44-856f-04e064ce4615.png)'
- en: 'A TensorBoard visualization of loss and accuracy on train and test datasets
    is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是训练和测试数据集上损失和准确性的 TensorBoard 可视化：
- en: '![](img/bbe00504-88bf-46cf-8a26-fd16fd21e95e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bbe00504-88bf-46cf-8a26-fd16fd21e95e.png)'
- en: From the first two graphs, we can see that, as loss on train dataset decreased,
    its accuracy improved.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从前两个图中，我们可以看到，随着训练数据集上的损失减少，其准确性提高。
- en: Also, note that the training loss was not reducing smoothly—potentially indicating
    to us that it is overfitting to the training data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，训练损失并没有平滑地减少——这可能会向我们表明它正在过度拟合训练数据。
- en: You should observe that, the validation accuracy (test accuracy) started to
    decrease as training dataset accuracy improved—again indicating to us that the
    model does not generalize well to unseen dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该观察到，随着训练数据集准确性的提高，验证准确性（测试准确性）开始下降——再次向我们表明该模型对未见数据集的泛化能力不佳。
- en: This phenomenon typically happens when the model is too complex and tries to
    fit the last few misclassifications to reduce the training loss.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象通常发生在模型过于复杂，试图拟合最后几个错误分类以减少训练损失时。
- en: Reducing overfitting
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少过拟合
- en: 'Typically, overfitting results in some weights being very high relative to
    others. To understand that, let''s look at the histogram of weights that are obtained
    by running the model on the artificially created dataset in *scenario 1*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，过拟合会导致一些权重相对于其他权重非常高。为了理解这一点，让我们看看在 *场景1* 中通过运行模型在人工创建的数据集上获得的权重直方图：
- en: '![](img/b2634e8c-5d8d-4d48-8397-159249df98e9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2634e8c-5d8d-4d48-8397-159249df98e9.png)'
- en: We see that there are some weights that have a high value (> 0.1) and a majority
    that are centered around zero.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，有一些权重值很高（> 0.1），而大多数权重则集中在零附近。
- en: Let's now explore the impact of penalizing for having a high weight value through
    L1 and L2 regularizations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探索通过 L1 和 L2 正则化对具有高权重值进行惩罚的影响。
- en: 'The intuition of regularization is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的直觉如下：
- en: If the weight values are shrunk to as minimal as possible, it is less likely
    that some of those weights contribute more towards fine-tuning our model to the
    few outlier cases
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果将权重值缩小到尽可能小，那么这些权重中的一些对微调我们的模型以适应少数异常情况贡献更大的可能性就较小。
- en: Implementing L2 regularization
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现L2正则化
- en: Now that we have seen how overfitting occurs on our dataset, we will explore
    the impact of L2 regularization in reducing overfitting on the dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了我们的数据集上过拟合的发生，我们将探讨L2正则化在减少数据集上过拟合的影响。
- en: 'L2 regularization on a dataset can be defined as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集上的 L2 正则化可以定义为如下：
- en: '![](img/320843d0-3683-4422-80b2-c2913f8d02d4.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/320843d0-3683-4422-80b2-c2913f8d02d4.png)'
- en: Note that the loss function is the traditional loss function where *y* is the
    dependent variable, *x* is the independent variables, and *W* is the kernel (weight
    matrices).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失函数是传统的损失函数，其中 *y* 是因变量，*x* 是自变量，*W* 是核（权重矩阵）。
- en: The regularization term is added to the loss function. Note the regularization
    value is the sum of squared weight values across all the dimensions of a weight
    matrix. Given that we are minimizing the sum of squared value of weights along
    with loss function, the cost function ensures that there is no weight value that
    is large—thereby ensuring that less overfitting occurs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项被添加到损失函数中。注意正则化值是权重矩阵所有维度的权重值的平方和。鉴于我们是在最小化权重值的平方和以及损失函数，成本函数确保没有权重值过大——从而确保过拟合现象减少。
- en: The lambda parameter is a hyperparameter that adjusts the weightage that we
    give to regularization term.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 参数是一个超参数，用于调整我们对正则化项赋予的权重。
- en: 'Let''s explore the impact of adding L2 regularization to the model defined
    in *scenario 1*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索在 *场景1* 中定义的模型中添加 L2 正则化的影响：
- en: '![](img/b2fc3fc5-0869-4653-b2b0-37f83c18f3e6.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2fc3fc5-0869-4653-b2b0-37f83c18f3e6.png)'
- en: Note that we modified the code that we have seen in *scenario 1* by adding `kernel_regularizer`,
    which, in this case, is the L2 regularizer with a lambda value of `0.01`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们修改了在 *场景1* 中看到的代码，通过添加 `kernel_regularizer`，在这种情况下，是具有 `0.01` Lambda 值的
    L2 正则化器。
- en: 'Note the output of TensorBoard, as we train the preceding model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 TensorBoard 的输出，正如我们训练前面的模型：
- en: '![](img/91fc92ac-abda-4234-b963-1f3b085ec531.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91fc92ac-abda-4234-b963-1f3b085ec531.png)'
- en: The training loss kept on decreasing and the validation accuracy remained stable
    while the training accuracy is 90.9% without accounting for the overfitting scenario.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失持续下降，验证准确率保持稳定，而训练准确率为90.9%，没有考虑到过拟合的情况。
- en: 'Let''s explore the distribution of weights to understand the difference between
    weight distributions when L2 regularization was done and there was no regularization:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索权重的分布，以了解在执行L2正则化和没有正则化时权重分布的差异：
- en: '![](img/49101e45-d418-4a7f-9a2c-73156b597df6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/49101e45-d418-4a7f-9a2c-73156b597df6.png)'
- en: You should notice that the kernels (primarily the kernels at `dense_2` and `dense_3`)
    have a lot sharper peak at zero in the L2 regularization scenario when compared
    to the no regularization scenario.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，与没有正则化的情况相比，在L2正则化的场景下，核（主要是`dense_2`和`dense_3`层的核）在零点有一个更尖锐的峰值。
- en: 'To further understand the peak distributions, we will modify the lambda value
    and give regularization a higher weightage of 0.1 instead of 001 and see how the
    weights look:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步了解峰值分布，我们将修改lambda值，并将正则化的权重提高至0.1而不是0.001，看看权重看起来如何：
- en: '![](img/f084eba1-7195-4b34-98bb-396d7b514043.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f084eba1-7195-4b34-98bb-396d7b514043.png)'
- en: Note that, with a higher weightage given to the regularization term, weights
    have a much sharper distribution around the center (a value of 0).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于给正则化项更高的权重，权重在中心（值为0）周围的分布要尖锐得多。
- en: Also, you should notice that the kernel is `dense_4` and is not changed by a
    lot, as we did not apply regularization at this layer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该注意到，核是`dense_4`，并且变化不大，因为我们没有在这个层应用正则化。
- en: From the preceding points we conclude that, by implementing L2 regularization,
    we can reduce the over-fitting issue that we see when there is no regularization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的点我们可以得出结论，通过实现L2正则化，我们可以减少在没有正则化时看到的过拟合问题。
- en: Implementing L1 regularization
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现L1正则化
- en: 'L1 regularization works in a similar way as that of L2; however, the cost function
    for L1 regularization is different than L2 regularization, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化与L2正则化类似；然而，L1正则化的成本函数与L2正则化不同，如下所示：
- en: '![](img/9a15e389-ccb9-4097-984b-4a9d79d2bcba.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9a15e389-ccb9-4097-984b-4a9d79d2bcba.png)'
- en: Note that, in the preceding equation, all the terms remain the same; just the
    regularization term is the summation of absolute values of weights than squared
    values of weights.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上述方程中，所有项都保持不变；只是正则化项是权重绝对值的总和，而不是权重平方值的总和。
- en: 'Let''s implement the L1 regularization in code; now we see the corresponding
    outputs as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中实现L1正则化；现在我们看到相应的输出如下：
- en: '![](img/54e4fb02-51be-487d-a0d9-a4bbf6682839.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54e4fb02-51be-487d-a0d9-a4bbf6682839.png)'
- en: Note that, as the regularization term does not involve squaring in L1 regularization,
    we may have to lower the lambda value in L1 when compared to L2 (given most of
    the weights are less than one, squaring them would make the weight values even
    smaller).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于L1正则化项不涉及平方，我们可能需要在L1中降低lambda值，与L2相比（考虑到大多数权重小于一，平方它们会使权重值更小）。
- en: 'Post-defining the model (this time with regularization), we fit it, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型（这次带有正则化）后，我们拟合它，如下所示：
- en: '![](img/b8c44bd0-f88f-47af-8d96-c715a0b001f8.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8c44bd0-f88f-47af-8d96-c715a0b001f8.png)'
- en: 'The preceding code fit results in the accuracy on train and test datasets,
    as per our expectation, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码拟合结果在训练和测试数据集上的准确率，正如我们所预期，如下所示：
- en: '![](img/433b5c22-116a-4544-a484-2cd92929b8b8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/433b5c22-116a-4544-a484-2cd92929b8b8.png)'
- en: 'Let''s also look into the weight distribution across the layers in the Histograms
    tab:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看看在直方图标签页中各层的权重分布：
- en: '![](img/cb992e90-c72a-4bbc-aff0-7327f8ef4523.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cb992e90-c72a-4bbc-aff0-7327f8ef4523.png)'
- en: We should note that, the kernel distribution here is similar to the kernel distribution
    when the lambda value of the L2 regularization was high.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，这里的核分布与L2正则化lambda值高时的核分布相似。
- en: Implementing dropout
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现dropout
- en: Another way to reduce overfitting is by implementing the dropout technique.
    While performing weight updates in a typical back propagation, we ensure that
    some random portion of the weights are left out from updating the weights in a
    given epoch—hence the name dropout.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少过拟合的方法是实现dropout技术。在执行典型反向传播中的权重更新时，我们确保在给定的一轮中，一些随机部分的权重被排除在权重更新之外——因此得名dropout。
- en: Dropout as a technique can also help in reducing overfitting, as reduction in
    the number of weights that need to be updated in a single epoch results in less
    chance that the output depends on few input values.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout作为一种技术，也可以帮助减少过拟合，因为减少单个epoch中需要更新的权重数量，从而减少了输出依赖于少数输入值的机会。
- en: 'Dropout can be implemented as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout可以这样实现：
- en: '![](img/df7d0745-e877-446d-aa79-c679ca7b1568.png)![](img/023c7d4b-fbe1-4b85-b5cc-af57587abf9a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df7d0745-e877-446d-aa79-c679ca7b1568.png)![](img/023c7d4b-fbe1-4b85-b5cc-af57587abf9a.png)'
- en: 'The result of the model fit is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型拟合的结果如下：
- en: '![](img/f711d1e4-d7bf-4fb6-8d10-dcaf33f095c8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f711d1e4-d7bf-4fb6-8d10-dcaf33f095c8.png)'
- en: 'You should note that dropout in the given configuration has resulted in a slightly
    broad distribution of weights when compared to the no regularization scenario:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意，与没有正则化的场景相比，给定配置中的dropout导致权重分布略宽：
- en: '![](img/6b30b541-925b-4466-9d07-5e78387e8b96.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b30b541-925b-4466-9d07-5e78387e8b96.png)'
- en: Reducing underfitting
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少欠拟合
- en: 'Underfitting typically happens when:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当以下情况发生时，通常会出现欠拟合：
- en: The model is extremely complex and is run for fewer epochs
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型极其复杂，并且运行了较少的epoch
- en: The data is not normalized
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据未进行归一化
- en: '**Scenario 2: Underfitting in action on the MNIST dataset**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景2：MNIST数据集上的欠拟合实际操作**'
- en: 'In the following scenario, we see the case of underfitting in action on the
    MNIST dataset:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下场景中，我们看到MNIST数据集上欠拟合的实际案例：
- en: '![](img/f7260cbc-bab0-4c15-ae6b-7a48d40255d5.png)![](img/1bbeec24-6d34-49c3-b32f-655d258e79e4.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7260cbc-bab0-4c15-ae6b-7a48d40255d5.png)![](img/1bbeec24-6d34-49c3-b32f-655d258e79e4.png)'
- en: 'Note that, in the preceding code, we have not scaled our data—the training
    and test dataset columns have values ranging from 0 to 255:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的代码中，我们没有缩放我们的数据——训练和测试数据集的列值范围在0到255之间：
- en: '![](img/a685d65a-fbc3-4c17-a157-923532de8e01.png)![](img/735eefa4-1f2b-45e4-8236-c853ac466c30.png)![](img/b4a0932f-f7f9-44c1-a306-0b5ca8e91011.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a685d65a-fbc3-4c17-a157-923532de8e01.png)![](img/735eefa4-1f2b-45e4-8236-c853ac466c30.png)![](img/b4a0932f-f7f9-44c1-a306-0b5ca8e91011.png)'
- en: 'The TensorBoard visualization of accuracy and loss on train and test datasets
    for the preceding model is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前面模型的训练和测试数据集上的TensorBoard准确率和损失可视化如下：
- en: '![](img/522b8ce1-dfed-4ea7-bf30-6cdfa835e8df.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/522b8ce1-dfed-4ea7-bf30-6cdfa835e8df.png)'
- en: Note that, in the preceding charts, both the loss and the accuracy of the training
    dataset have hardly changed (note the *y* axis values in both).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的图表中，训练数据集的损失和准确率几乎没有变化（注意两个图表的*y*轴值）。
- en: This scenario (where the loss hardly changes), typically happens when the input
    has numbers that are very high (typically >5).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况（损失几乎没有变化）通常发生在输入有非常高的数字（通常>5）时。
- en: 'The preceding can be rectified by performing any of the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的情况可以通过执行以下任何一项来纠正：
- en: Scaling the data
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据缩放
- en: Batch normalization
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化
- en: 'Scaling the data is as simple as repeating the preceding architecture, but
    with the small modification of scaling the train and test datasets:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缩放就像重复前面的架构一样简单，但需要稍作修改，即缩放训练和测试数据集：
- en: '![](img/41fe7b3c-541e-4d7e-99f7-d5c55518c983.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41fe7b3c-541e-4d7e-99f7-d5c55518c983.png)'
- en: 'Batch normalization can be performed (even on an unscaled MNIST dataset) as
    follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 批标准化可以在以下方式下执行（甚至可以在未缩放的MNIST数据集上）：
- en: '![](img/85ead417-162b-400b-9e8c-b871b1c57a81.png)![](img/d6668e63-cf79-4284-9b18-b301c8f9b51a.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85ead417-162b-400b-9e8c-b871b1c57a81.png)![](img/d6668e63-cf79-4284-9b18-b301c8f9b51a.png)'
- en: 'A visualization of training and test accuracies can be seen as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试准确率的可视化如下：
- en: '![](img/0dab88fe-6f7a-491d-92c7-f313c7962fc3.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dab88fe-6f7a-491d-92c7-f313c7962fc3.png)'
- en: In the preceding scenario, we see that, even on unscaled dataset, the test accuracy
    is quite high.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的场景中，我们看到即使在未缩放的数据集上，测试准确率也相当高。
- en: '**Scenario 3: Incorrect weight initialization**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景3：错误的权重初始化**'
- en: 'Just like the previous scenario, it is highly likely that we will encounter
    an underfitting scenario if the weights are not initialized properly (even if
    the dataset is a properly scaled dataset). For example, in the following code,
    we initialize all the weights (kernels) to zero and then notice the accuracy on
    the test dataset:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面的场景一样，如果权重没有正确初始化（即使数据集是正确缩放的），我们很可能遇到欠拟合的场景。例如，在以下代码中，我们将所有权重（核）初始化为零，然后注意到测试数据集上的准确率：
- en: '![](img/f8e06ae8-7d32-4846-b1f2-c39b9e784415.png)![](img/ea349dcc-c086-4e33-8bb3-ee8ee01a54ff.png)![](img/516e7457-73df-4d0e-a2cf-4c08d1efa8fa.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8e06ae8-7d32-4846-b1f2-c39b9e784415.png)![](img/ea349dcc-c086-4e33-8bb3-ee8ee01a54ff.png)![](img/516e7457-73df-4d0e-a2cf-4c08d1efa8fa.png)'
- en: 'The output of the preceding code results in the following TensorBoard visualization:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出结果导致以下 TensorBoard 可视化：
- en: '![](img/5e2789c5-d820-4ec6-bf07-0aae69762961.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5e2789c5-d820-4ec6-bf07-0aae69762961.png)'
- en: Similar to *scenario 2*, the preceding charts indicate that there is no learning
    that is happening through the preceding defined architecture.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与*场景 2*类似，前述图表表明，通过先前定义的架构没有发生学习。
- en: No learning is happening, as the weights are initialized to zero.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重被初始化为零，没有发生学习。
- en: 'It is advisable to initialize the weights to normal initialization. The other
    initializations that can be tried out to test whether accuracy could improve are:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 建议将权重初始化为正态初始化。其他可以尝试的初始化方法，以测试是否可以提高准确率的有：
- en: '`glorot_normal`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glorot_normal`'
- en: '`lecun_uniform`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lecun_uniform`'
- en: '`glorot_uniform`'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glorot_uniform`'
- en: '`he_normal`'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`he_normal`'
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have seen the characteristics of over fitting and how they
    can be handled through L1 and L2 regularizations, and dropout. Similarly, we have
    seen the scenario where there was quite a lot of underfitting and how scaling
    or batch normalization helped us in improving the under-fitting scenario.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了过拟合的特征以及如何通过 L1 和 L2 正则化以及 dropout 来处理它们。同样，我们也看到了存在大量欠拟合的场景，以及如何通过缩放或批量归一化来帮助我们改善欠拟合的情况。
