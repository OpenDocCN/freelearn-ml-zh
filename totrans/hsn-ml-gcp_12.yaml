- en: Preventing Overfitting with Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous chapters, we understood about building neural network,
    evaluating the TensorBoard results, and varying the hyperparameters of the neural
    network model to improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: While the hyperparameters in general help with improving the accuracy of model,
    certain configuration of hyperparameters results in the model overfitting to the
    training data, while not generalizing for testing data is the problem of overfitting
    to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key parameter that can help us in avoiding overfitting while generalizing
    on an unseen dataset is the regularization technique. Some of the key regularization
    techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Intuition of over/under fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing overfitting using regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving the underfitting scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuition of over/under fitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we understand about how the preceding techniques are useful, let's build
    a scenario, so that we understand the phenomenon of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 1: A case of not generalizing on an unseen dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, we will create a dataset, for which there is a clear linearly
    separable mapping between input and output. For example, whenever the independent
    variables are positive, the output is `[1,0]`, and when the input variables are
    negative, the output is `[0,1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f2ad6e3-05f0-47df-9eee-b87485c35826.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To that dataset, we will add a small amount of noise (10% of the preceding
    dataset created) by adding some data points that follow the opposite of the preceding
    pattern, that is, when the input variables are positive, the output is `[0,1]`,
    and the output is `[1,0]` when the input variables are negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13da13ac-d612-4e7e-a42a-f04ca911688a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Appending the datasets obtained by the preceding two steps gives us the training
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14edfbdf-5912-4c66-a5fc-4973132f5bbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we create the test dataset, where it follows the criterion
    followed by the majority of the training dataset, that is, when the input is positive,
    the output is `[1,0]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9d36cea-4bc8-454e-be9f-8d0541614933.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have created the dataset, let's go ahead and build a model to predict
    the output with the given inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition here is that, if training accuracy improves by more than 90.91%
    it is a classic case of overfitting, as the model tries to fit for the minority
    of the observations which do not generalize for an unseen dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check that—let''s first import all the relevant packages to build a model
    in `keras`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We build a model with three layers where the layers have 1,000, 500 and 100
    units in each respective hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0848aeeb-7310-4057-a8c1-430d6cdc486a.png)![](img/5afb4ede-cd47-4e44-856f-04e064ce4615.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A TensorBoard visualization of loss and accuracy on train and test datasets
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbe00504-88bf-46cf-8a26-fd16fd21e95e.png)'
  prefs: []
  type: TYPE_IMG
- en: From the first two graphs, we can see that, as loss on train dataset decreased,
    its accuracy improved.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the training loss was not reducing smoothly—potentially indicating
    to us that it is overfitting to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: You should observe that, the validation accuracy (test accuracy) started to
    decrease as training dataset accuracy improved—again indicating to us that the
    model does not generalize well to unseen dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This phenomenon typically happens when the model is too complex and tries to
    fit the last few misclassifications to reduce the training loss.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Typically, overfitting results in some weights being very high relative to
    others. To understand that, let''s look at the histogram of weights that are obtained
    by running the model on the artificially created dataset in *scenario 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2634e8c-5d8d-4d48-8397-159249df98e9.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that there are some weights that have a high value (> 0.1) and a majority
    that are centered around zero.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now explore the impact of penalizing for having a high weight value through
    L1 and L2 regularizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition of regularization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the weight values are shrunk to as minimal as possible, it is less likely
    that some of those weights contribute more towards fine-tuning our model to the
    few outlier cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing L2 regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen how overfitting occurs on our dataset, we will explore
    the impact of L2 regularization in reducing overfitting on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'L2 regularization on a dataset can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320843d0-3683-4422-80b2-c2913f8d02d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the loss function is the traditional loss function where *y* is the
    dependent variable, *x* is the independent variables, and *W* is the kernel (weight
    matrices).
  prefs: []
  type: TYPE_NORMAL
- en: The regularization term is added to the loss function. Note the regularization
    value is the sum of squared weight values across all the dimensions of a weight
    matrix. Given that we are minimizing the sum of squared value of weights along
    with loss function, the cost function ensures that there is no weight value that
    is large—thereby ensuring that less overfitting occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The lambda parameter is a hyperparameter that adjusts the weightage that we
    give to regularization term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore the impact of adding L2 regularization to the model defined
    in *scenario 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2fc3fc5-0869-4653-b2b0-37f83c18f3e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we modified the code that we have seen in *scenario 1* by adding `kernel_regularizer`,
    which, in this case, is the L2 regularizer with a lambda value of `0.01`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the output of TensorBoard, as we train the preceding model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91fc92ac-abda-4234-b963-1f3b085ec531.png)'
  prefs: []
  type: TYPE_IMG
- en: The training loss kept on decreasing and the validation accuracy remained stable
    while the training accuracy is 90.9% without accounting for the overfitting scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore the distribution of weights to understand the difference between
    weight distributions when L2 regularization was done and there was no regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49101e45-d418-4a7f-9a2c-73156b597df6.png)'
  prefs: []
  type: TYPE_IMG
- en: You should notice that the kernels (primarily the kernels at `dense_2` and `dense_3`)
    have a lot sharper peak at zero in the L2 regularization scenario when compared
    to the no regularization scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further understand the peak distributions, we will modify the lambda value
    and give regularization a higher weightage of 0.1 instead of 001 and see how the
    weights look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f084eba1-7195-4b34-98bb-396d7b514043.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, with a higher weightage given to the regularization term, weights
    have a much sharper distribution around the center (a value of 0).
  prefs: []
  type: TYPE_NORMAL
- en: Also, you should notice that the kernel is `dense_4` and is not changed by a
    lot, as we did not apply regularization at this layer.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding points we conclude that, by implementing L2 regularization,
    we can reduce the over-fitting issue that we see when there is no regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing L1 regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'L1 regularization works in a similar way as that of L2; however, the cost function
    for L1 regularization is different than L2 regularization, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a15e389-ccb9-4097-984b-4a9d79d2bcba.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding equation, all the terms remain the same; just the
    regularization term is the summation of absolute values of weights than squared
    values of weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the L1 regularization in code; now we see the corresponding
    outputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54e4fb02-51be-487d-a0d9-a4bbf6682839.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, as the regularization term does not involve squaring in L1 regularization,
    we may have to lower the lambda value in L1 when compared to L2 (given most of
    the weights are less than one, squaring them would make the weight values even
    smaller).
  prefs: []
  type: TYPE_NORMAL
- en: 'Post-defining the model (this time with regularization), we fit it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8c44bd0-f88f-47af-8d96-c715a0b001f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding code fit results in the accuracy on train and test datasets,
    as per our expectation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/433b5c22-116a-4544-a484-2cd92929b8b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s also look into the weight distribution across the layers in the Histograms
    tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb992e90-c72a-4bbc-aff0-7327f8ef4523.png)'
  prefs: []
  type: TYPE_IMG
- en: We should note that, the kernel distribution here is similar to the kernel distribution
    when the lambda value of the L2 regularization was high.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to reduce overfitting is by implementing the dropout technique.
    While performing weight updates in a typical back propagation, we ensure that
    some random portion of the weights are left out from updating the weights in a
    given epoch—hence the name dropout.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout as a technique can also help in reducing overfitting, as reduction in
    the number of weights that need to be updated in a single epoch results in less
    chance that the output depends on few input values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropout can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7d0745-e877-446d-aa79-c679ca7b1568.png)![](img/023c7d4b-fbe1-4b85-b5cc-af57587abf9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The result of the model fit is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f711d1e4-d7bf-4fb6-8d10-dcaf33f095c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You should note that dropout in the given configuration has resulted in a slightly
    broad distribution of weights when compared to the no regularization scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b30b541-925b-4466-9d07-5e78387e8b96.png)'
  prefs: []
  type: TYPE_IMG
- en: Reducing underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Underfitting typically happens when:'
  prefs: []
  type: TYPE_NORMAL
- en: The model is extremely complex and is run for fewer epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is not normalized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario 2: Underfitting in action on the MNIST dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following scenario, we see the case of underfitting in action on the
    MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7260cbc-bab0-4c15-ae6b-7a48d40255d5.png)![](img/1bbeec24-6d34-49c3-b32f-655d258e79e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that, in the preceding code, we have not scaled our data—the training
    and test dataset columns have values ranging from 0 to 255:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a685d65a-fbc3-4c17-a157-923532de8e01.png)![](img/735eefa4-1f2b-45e4-8236-c853ac466c30.png)![](img/b4a0932f-f7f9-44c1-a306-0b5ca8e91011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The TensorBoard visualization of accuracy and loss on train and test datasets
    for the preceding model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/522b8ce1-dfed-4ea7-bf30-6cdfa835e8df.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding charts, both the loss and the accuracy of the training
    dataset have hardly changed (note the *y* axis values in both).
  prefs: []
  type: TYPE_NORMAL
- en: This scenario (where the loss hardly changes), typically happens when the input
    has numbers that are very high (typically >5).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding can be rectified by performing any of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling the data is as simple as repeating the preceding architecture, but
    with the small modification of scaling the train and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41fe7b3c-541e-4d7e-99f7-d5c55518c983.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Batch normalization can be performed (even on an unscaled MNIST dataset) as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85ead417-162b-400b-9e8c-b871b1c57a81.png)![](img/d6668e63-cf79-4284-9b18-b301c8f9b51a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A visualization of training and test accuracies can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dab88fe-6f7a-491d-92c7-f313c7962fc3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding scenario, we see that, even on unscaled dataset, the test accuracy
    is quite high.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 3: Incorrect weight initialization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the previous scenario, it is highly likely that we will encounter
    an underfitting scenario if the weights are not initialized properly (even if
    the dataset is a properly scaled dataset). For example, in the following code,
    we initialize all the weights (kernels) to zero and then notice the accuracy on
    the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8e06ae8-7d32-4846-b1f2-c39b9e784415.png)![](img/ea349dcc-c086-4e33-8bb3-ee8ee01a54ff.png)![](img/516e7457-73df-4d0e-a2cf-4c08d1efa8fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the preceding code results in the following TensorBoard visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e2789c5-d820-4ec6-bf07-0aae69762961.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to *scenario 2*, the preceding charts indicate that there is no learning
    that is happening through the preceding defined architecture.
  prefs: []
  type: TYPE_NORMAL
- en: No learning is happening, as the weights are initialized to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is advisable to initialize the weights to normal initialization. The other
    initializations that can be tried out to test whether accuracy could improve are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`glorot_normal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lecun_uniform`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glorot_uniform`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`he_normal`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen the characteristics of over fitting and how they
    can be handled through L1 and L2 regularizations, and dropout. Similarly, we have
    seen the scenario where there was quite a lot of underfitting and how scaling
    or batch normalization helped us in improving the under-fitting scenario.
  prefs: []
  type: TYPE_NORMAL
