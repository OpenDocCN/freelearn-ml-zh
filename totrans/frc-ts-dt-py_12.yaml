- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of keeping training data and testing data separate is sacrosanct
    in machine learning and statistics. You should never train a model and test its
    performance on the same data. Setting data aside for testing purposes has a downside,
    though: that data has valuable information that you would want to include in training.
    **Cross-validation** is a technique that’s used to circumvent this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: You may be familiar with **k-fold cross-validation**, but if you are not, we
    will briefly cover it in this chapter. K-fold cross-validation, however, will
    not work on time series data. It requires that the data be independent, an assumption
    that time series data does not hold. An understanding of k-fold cross-validation
    will help you learn how forward-chaining cross-validation works and why it is
    necessary for time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'After learning how to perform cross-validation in Prophet, you will learn how
    to speed up the computing of cross-validation through Prophet’s ability to parallelize
    several processes. All in all, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing k-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing forward-chaining cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the Prophet cross-validation DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data files and code for examples in this chapter can be found at [https://github.com/PacktPublishing/Forecasting-Time-Series-Data-with-Prophet-Second-Edition](https://github.com/PacktPublishing/Forecasting-Time-Series-Data-with-Prophet-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: Performing k-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll be using a new dataset in this chapter – the sales of an online retailer
    in the United Kingdom. This data has been anonymized, but it represents 3 years
    of daily sales amounts, as displayed in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Daily sales of an anonymous online retailer](img/Fig_12.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Daily sales of an anonymous online retailer
  prefs: []
  type: TYPE_NORMAL
- en: This retailer has not seen dramatic growth over the 3 years of data, but it
    has seen a massive boost in sales at the end of each year. The main customers
    of this retailer are wholesalers, who typically make their purchases during the
    work week. This is why when we plot the components of Prophet’s forecast, you’ll
    see that Saturday and Sunday’s sales are the lowest. We’ll use this data to perform
    cross-validation in Prophet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get to modeling, though, let’s first review traditional validation
    techniques used to tune a model’s hyperparameters and report performance. The
    most basic method is to take your full dataset and split it into three subsets:
    a **training set**, **validation set**, and **test set**, after randomly shuffling
    it around. This is sometimes called **hold-out** validation. Usually, the training
    set is the largest and the validation and testing sets are smaller. For example,
    a 60/20/20 split would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Traditional train/validation/test sets](img/Fig_12.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Traditional train/validation/test sets
  prefs: []
  type: TYPE_NORMAL
- en: After the full data has been split, your model is trained on the train set,
    and performance is evaluated on the validation set. A new set of hyperparameters
    is chosen for the given algorithm and the model is retrained on the train set
    and re-evaluated on the validation set. This process is repeated for however many
    combinations of hyperparameters you want to try.
  prefs: []
  type: TYPE_NORMAL
- en: The set of hyperparameters with the highest performance on the validation set
    is chosen for the model; the train and validation sets are combined to train a
    final model, and this final model is evaluated on the test set. This evaluation
    is then reported as the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: With this technique, though, only 60% of your full data is available to use
    to tune the model. It would be advantageous to use more data for tuning but using
    smaller validation and testing sets could introduce bias into your model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, k-fold cross-validation was developed. In k-fold cross-validation,
    the data is still randomly shuffled and has a test set split out, maybe 20% again.
    The remaining 80% of the data is all used for training. This 80% of data is split
    into *k* sections, with each section called a *fold*. This is what the process
    looks like with five folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – k-fold cross-validation with five folds](img/Fig_12.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – k-fold cross-validation with five folds
  prefs: []
  type: TYPE_NORMAL
- en: For each set of hyperparameters you want to evaluate, you train your model five
    times. The first time, you set aside the first fold and train on the remaining
    four. You evaluate on that first fold. You repeat this process for each fold and
    take the average of your performance metric across the five folds. Then, you move
    on to the next set of hyperparameters and repeat.
  prefs: []
  type: TYPE_NORMAL
- en: The process of tuning your hyperparameters takes much longer in this case because
    of the training for each fold. The advantage, though, is that you can use more
    data for training without introducing bias into your model.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, time series data is sequential and dependent. You cannot shuffle
    it. You cannot train on future data to predict previous data. This is why both
    of the methods just demonstrated will not work. We need a way to maintain the
    order of our data while still setting some aside for testing and validation. That’s
    why forward chaining was developed.
  prefs: []
  type: TYPE_NORMAL
- en: Performing forward-chaining cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Forward-chaining cross-validation**, also called **rolling-origin cross-validation**,
    is similar to k-fold cross-validation but is better suited to sequential data
    such as time series. There is no random shuffling of data to begin with, but a
    test set may be set aside. The test set must be the final portion of data, so
    if each fold is going to be 10% of your data (as it would be in 10-fold cross-validation),
    then your test set will be the final 10% of your date range.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the remaining data, you choose an initial amount of data to train on,
    let’s say five folds in this example, and then you evaluate on the sixth fold
    and save that performance metric. You retrain now on the first six folds and evaluate
    on the seventh. You repeat this until all folds are exhausted and again take the
    average of your performance metric. The folds using this technique would look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Forward-chaining cross-validation with five folds](img/Fig_12.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Forward-chaining cross-validation with five folds
  prefs: []
  type: TYPE_NORMAL
- en: In this way, you are able to train your data on sequential data points and evaluate
    unseen data, and you are also able to minimize bias by training and testing on
    a variety of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Prophet has a built-in diagnostics tool for performing forward-chaining cross-validation.
    Let’s now see how to use it with our retail sales dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Prophet cross-validation DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform cross-validation in Prophet, first, you need a fitted model. So,
    we’ll begin with the same procedure we’ve completed throughout this book. This
    dataset is very cooperative, so we’ll be able to use plenty of Prophet’s default
    parameters. We will plot the changepoints, so be sure to include that function
    with your other imports before loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset does not have very complicated seasonality, so we’ll reduce the
    Fourier order of yearly seasonality when instantiating our model, but keep everything
    else default, before fitting, predicting, and plotting. We’ll use a 1-year future
    forecast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, this plot shows the same data as *Figure 12**.1* where the data
    was introduced. There were no significant trend changepoints identified and a
    very gently sloping upward trend. There appears to be a mild increase in sales
    during the summer but a dramatic increase over the winter holiday season, as can
    be seen in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Online retail sales forecast](img/Fig_12.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Online retail sales forecast
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the components now to better understand our seasonalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`trend`, `weekly` seasonality, and `yearly` seasonality show clear patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 12.6 – Online retail sales components plot\uFEFF](img/Fig_12.6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Online retail sales components plot
  prefs: []
  type: TYPE_NORMAL
- en: As we predicted, the `yearly` seasonality reflects that winter spike. As I mentioned
    when introducing this data, the retailer largely caters to wholesalers, not consumers.
    So, their purchasing occurs during the business week far more than on the weekends.
    `Friday` sales are even down compared with the rest of the week.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s perform the actual cross-validation. To do that, we first need to
    import the function from Prophet’s `diagnostics` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we see how to use that function, there are some terms we need to discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initial` is the first training period. In *Figure 12**.5*, it would be the
    first five blocks of data in the first fold. It is the minimum amount of data
    needed to begin your training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horizon` is the length of time you want to evaluate your forecast over. Let’s
    say that this retail outlet is building its model so that it can predict sales
    over the next month. A horizon set to 30 days would make sense here so that they
    evaluate their model on the same parameter setting that they wish to use it on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`period` is the amount of time between each fold. It can be either greater
    than the horizon or less than it, or even equal to it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cutoffs` are the dates when each horizon will begin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This vocabulary is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Cross-validation terminology](img/Fig_12.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Cross-validation terminology
  prefs: []
  type: TYPE_NORMAL
- en: For each `cutoff`, the model will be trained on all data up to that `cutoff`,
    and then a prediction will be made for the `horizon` period. That prediction will
    be compared to the known values and evaluated. Then, the model will be retrained
    on all data up to the second `cutoff` and the process will be repeated. The final
    performance evaluation will be the average of the performance at each cutoff.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that this retail outlet wants a model that is able to predict
    the next month of daily sales, and they plan on running the model at the beginning
    of each quarter. They have 3 years of data and want (as is recommended for Prophet)
    at least 2 full cycles of seasonality, which because they are modeling yearly
    seasonality will be 2 years.
  prefs: []
  type: TYPE_NORMAL
- en: They would set their initial training data to be 2 years then. They want to
    predict the next month of sales, and so would set `horizon` to 30 days. They plan
    to run the model each business quarter, and so would set the period to be 90 days.
    That’s what was shown previously in *Figure 12**.7*. Now, let’s apply this to
    Prophet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cross_validation` function takes two required arguments, the fitted model
    and `horizon`. Also, `period` and `initial` can be stated, but they are not required.
    If left as their defaults, `period` is half of `horizon` and `initial` will be
    three times `horizon`. The output of the function is the cross-validation DataFrame.
    Let’s create this DataFrame and call it `df_cv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each of the `horizon`, `period`, and `initial` arguments takes a string with
    a style compatible with pandas’ `Timedelta` format, for example, `'5 days'`, `'3
    hours'`, or `'10 seconds'`. In this example, we’re switching the `horizon` and
    `period` values from those shown in *Figure 12**.7*. The retail outlet wants to
    predict 3 months of daily sales and update their predictions every month (this
    is probably a more realistic use of the forecast; these parameters are reversed
    in the image merely to avoid overlapping the horizons to keep the image clear).
  prefs: []
  type: TYPE_NORMAL
- en: We begin our training with an initial period of 2 years, which is `'730 days'`.
    We set `horizon='90 days'` to evaluate our forecast over a 90-day prediction interval.
    And finally, we set `period='30 days'`, so we retrain and re-evaluate our model
    every 30 days. This results in a total of 10 forecasts to compare with the final
    year of data.
  prefs: []
  type: TYPE_NORMAL
- en: You may also specify the `cutoff` values, but this is usually unnecessary. However,
    we’ll cover a specific instance in [*Chapter 13*](B19630_13.xhtml#_idTextAnchor839),
    *Evaluating Performance Metrics*, where you will want to set them yourself. Prophet’s
    default behavior is to set them automatically by working backward from the end
    of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at this DataFrame by displaying the first five rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run that code in a Jupyter notebook, you’ll see the following formatted
    output (your values for `yhat_lower` and `yhat_upper` may slightly differ due
    to randomness in the optimization algorithms):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – The cross-validation DataFrame](img/Fig_12.8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – The cross-validation DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: For each unique `cutoff` in the DataFrame, you will find 90 days in the `ds`
    column, corresponding to the 90-day horizon. Each date in `ds` has a true value,
    `y`, which is the same value from your training data, `df['y']`, and the value
    forecast in that fold for that date in the `yhat` column.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a different `yhat` from that in the `forecast` DataFrame,
    as those values were calculated with the full dataset, not with a cross-validated
    fold. The cross-validation DataFrame also contains uncertainty intervals for these
    forecasts, in `yhat_upper` and `yhat_lower`.
  prefs: []
  type: TYPE_NORMAL
- en: This DataFrame allows you to compare forecasted values with actual values across
    the range of date-time values in your data. In the `forecast` DataFrame, all `yhat`
    values for dates in the future obviously have no true `y` value for comparison.
    For dates in the past, there is a corresponding `df['y']` value to compare your
    `forecast['yhat']` value with, but the forecast was trained on this value. The
    `forecast['yhat']` values are biased, whereas the `df_cv['yhat']` values are unbiased
    and therefore will provide a more accurate representation of what you can expect
    your model to predict on new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a lot of iteration going on during cross-validation and these are
    tasks that can be parallelized to speed things up. All you need to do to take
    advantage of this is use the `parallel` keyword. There are four options you may
    choose: `None`, `''processes''`, `''threads''`, or `''dask''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Setting `parallel='processes'` uses Python’s `concurrent.futures.ProcessPoolExecutor`
    class, whereas `parallel='threads'` uses `concurrent.futures.ThreadPoolExecutor`.
    If you’re unsure which of these to use, go with `'processes'`. It will give the
    best performance on a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: '`None` will perform no parallelism, which can be good if you plan to do other
    work on your machine while Prophet calculates and you don’t want Prophet to take
    up all of your machine’s resources. If using `''dask''`, you will need to install
    Dask separately and use the `Client` from `dask.distributed` to connect to the
    cluster (the following code will result in an error if Dask has not been separately
    installed and set up):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although you can use Dask on your laptop, its power really comes into effect
    when using multiple computing clusters across many machines. If you don’t have
    access to this type of computing power, `parallel='processes'` will usually be
    the faster option.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began this chapter with a discussion of why k-fold cross-validation was developed
    in traditional machine learning applications, and we then learned why it will
    not work with time series. You then learned about forward-chaining, also called
    rolling-origin cross-validation, for use with time series data.
  prefs: []
  type: TYPE_NORMAL
- en: You learned the keywords of `initial`, `horizon`, `period`, and `cutoff`, which
    are used to define your cross-validation parameters, and you learned how to implement
    them in Prophet. Finally, you learned the different options Prophet has for parallelization
    in order to speed up model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques provide you with a statistically robust way to evaluate and
    compare models. By isolating the data used in training and testing, you remove
    any bias in the process and can be more certain that your model will perform well
    when making new predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you’ll apply what you learned here to measure your model’s
    performance and tune it for optimal results.
  prefs: []
  type: TYPE_NORMAL
