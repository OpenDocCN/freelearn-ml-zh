["```py\nint iteration = 0;\n             TabuSearchExploration tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy;\n             EpsilonGreedyExploration explorationPolicy = (EpsilonGreedyExploration)tabuPolicy.BasePolicy;\n\n             while ((!needToStop) && (iteration < learningIterations))\n             {\n                 explorationPolicy.Epsilon = explorationRate - ((double)iteration / learningIterations) * explorationRate;\n                 qLearning.LearningRate = learningRate - ((double)iteration / learningIterations) * learningRate;\n                 tabuPolicy.ResetTabuList();\n\n                 var agentCurrentX = agentStartX;\n                 var agentCurrentY = agentStartY;\n\n                 int steps = 0;\n                 while ((!needToStop) && ((agentCurrentX != agentStopX) || (agentCurrentY != agentStopY)))\n                 {\n                     steps++;\n                     int currentState = GetStateNumber(agentCurrentX, agentCurrentY);\n                     int action = qLearning.GetAction(currentState);\n                     double reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action);\n                     int nextState = GetStateNumber(agentCurrentX, agentCurrentY);\n\n                     // do learning of the agent - update his Q-function, set Tabu action\n                     qLearning.UpdateState(currentState, action, reward, nextState);\n                     tabuPolicy.SetTabuAction((action + 2) % 4, 1);\n                 }\n\n                 System.Diagnostics.Debug.WriteLine(steps);\n                 iteration++;\n\n                 SetText(iterationBox, iteration.ToString());\n             }\n```", "```py\nint iteration = 0;\n             TabuSearchExploration tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy;\n             EpsilonGreedyExploration explorationPolicy = (EpsilonGreedyExploration)tabuPolicy.BasePolicy;\n\n             while ((!needToStop) && (iteration < learningIterations))\n             {\n                 explorationPolicy.Epsilon = explorationRate - ((double)iteration / learningIterations) * explorationRate;\n                 sarsa.LearningRate = learningRate - ((double)iteration / learningIterations) * learningRate;\n                 tabuPolicy.ResetTabuList();\n\n                 var agentCurrentX = agentStartX;\n                 var agentCurrentY = agentStartY;\n                 int steps = 1;\n                 int previousState = GetStateNumber(agentCurrentX, agentCurrentY);\n                 int previousAction = sarsa.GetAction(previousState);\n                 double reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, previousAction);\n\n                 while ((!needToStop) && ((agentCurrentX != agentStopX) || (agentCurrentY != agentStopY)))\n                 {\n                     steps++;\n\n                     tabuPolicy.SetTabuAction((previousAction + 2) % 4, 1);\n                     int nextState = GetStateNumber(agentCurrentX, agentCurrentY);\n                     int nextAction = sarsa.GetAction(nextState);\n                     sarsa.UpdateState(previousState, previousAction, reward, nextState, nextAction);\n                     reward = UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, nextAction);\n                     previousState = nextState;\n                     previousAction = nextAction;\n                 }\n\n                 if (!needToStop)\n                 {\n                     sarsa.UpdateState(previousState, previousAction, reward);\n                 }\n\n                 System.Diagnostics.Debug.WriteLine(steps);\n\n                 iteration++;\n\n                 SetText(iterationBox, iteration.ToString());\n             }\n```", "```py\nTabuSearchExploration tabuPolicy;\n\n             if (qLearning != null)\n                 tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy;\n             else if (sarsa != null)\n                 tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy;\n             else\n                 throw new Exception();\n\n             var explorationPolicy = (EpsilonGreedyExploration)tabuPolicy?.BasePolicy;\n             explorationPolicy.Epsilon = 0;\n             tabuPolicy?.ResetTabuList();\n             int agentCurrentX = agentStartX, agentCurrentY = agentStartY;\n             Array.Copy(map, mapToDisplay, mapWidth * mapHeight);\n             mapToDisplay[agentStartY, agentStartX] = 2;\n             mapToDisplay[agentStopY, agentStopX] = 3;\n```", "```py\nwhile (!needToStop)\n             {\n                 cellWorld.Map = mapToDisplay;\n                 Thread.Sleep(200);\n\n                 if ((agentCurrentX == agentStopX) && (agentCurrentY == agentStopY))\n                 {\n                     mapToDisplay[agentStartY, agentStartX] = 2;\n                     mapToDisplay[agentStopY, agentStopX] = 3;\n                     agentCurrentX = agentStartX;\n                     agentCurrentY = agentStartY;\n                     cellWorld.Map = mapToDisplay;\n                     Thread.Sleep(200);\n                 }\n\n                 mapToDisplay[agentCurrentY, agentCurrentX] = 0;\n                 int currentState = GetStateNumber(agentCurrentX, agentCurrentY);\n                 int action = qLearning?.GetAction(currentState) ?? sarsa.GetAction(currentState);\n                 UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action);\n                 mapToDisplay[agentCurrentY, agentCurrentX] = 2;\n             }\n```", "```py\n            TabuSearchExploration tabuPolicy;\n\n             if (qLearning != null)\n                 tabuPolicy = (TabuSearchExploration)qLearning.ExplorationPolicy;\n             else if (sarsa != null)\n                 tabuPolicy = (TabuSearchExploration)sarsa.ExplorationPolicy;\n             else\n                 throw new Exception();\n\n             var explorationPolicy = (EpsilonGreedyExploration)tabuPolicy?.BasePolicy;\n             explorationPolicy.Epsilon = 0;\n             tabuPolicy?.ResetTabuList();\n```", "```py\nwhile (!needToStop)\n             {\n                 cellWorld.Map = mapToDisplay;\n                 Thread.Sleep(200);\n\n                 if ((agentCurrentX == agentStopX) && (agentCurrentY == agentStopY))\n                 {\n                     mapToDisplay[agentStartY, agentStartX] = 2;\n                     mapToDisplay[agentStopY, agentStopX] = 3;\n                     agentCurrentX = agentStartX;\n                     agentCurrentY = agentStartY;\n                     cellWorld.Map = mapToDisplay;\n                     Thread.Sleep(200);\n                 }\n\n                 mapToDisplay[agentCurrentY, agentCurrentX] = 0;\n                 int currentState = GetStateNumber(agentCurrentX, agentCurrentY);\n                 int action = qLearning?.GetAction(currentState) ?? sarsa.GetAction(currentState);\n                 UpdateAgentPosition(ref agentCurrentX, ref agentCurrentY, action);\n                 mapToDisplay[agentCurrentY, agentCurrentX] = 2;\n             }\n```", "```py\nprivate void TrainQMatrix(int _StatesMaxCount)\n{\n  pickedActions = new Dictionary<int, int>();\n// list of available actions (will be based on R matrix which\n// contains the allowed next actions starting from some state as 0 values in the array\n  List<int> nextActions = new List<int>();\n  int counter = 0;\n  int rIndex = 0;\n// _StatesMaxCount is the number of all possible states of a puzzle\n// from my experience with this application, 4 times the number\n// of all possible moves has enough episodes to train Q matrix\n  while (counter < 3 * _StatesMaxCount)\n  {\n    var init = Utility.GetRandomNumber(0, _StatesMaxCount);\n    do\n  {\n// get available actions\n nextActions = GetNextActions(_StatesMaxCount, init);\n// Choose any action out of the available actions randomly\n    if (nextActions != null)\n    {\n      var nextStep = Utility.GetRandomNumber(0, nextActions.Count);\n nextStep = nextActions[nextStep];\n// get available actions\n nextActions = GetNextActions(_StatesMaxCount, nextStep);\n// set the index of the action to take from this state\n      for (int i = 0; i < 3; i++)\n      {\n        if (R != null && R[init, i, 1] == nextStep)\n rIndex = i;\n      }\n// this is the value iteration update rule, discount factor is 0.8\n      Q[init, nextStep] = R[init, rIndex, 0] + 0.8 * Utility.GetMax(Q, nextStep, nextActions);\n// set the next step as the current step\n init = nextStep;\n      }\n    }\n  while (init != FinalStateIndex);\n counter++;\n  }\n}\n```"]