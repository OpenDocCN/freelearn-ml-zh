<html><head></head><body>
		<div id="_idContainer046">
			<h1 class="chapter-number"><a id="_idTextAnchor038"/>3</h1>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor039"/>Data in Software Systems  – Text, Images, Code, and Their Annotations</h1>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) systems <a id="_idIndexMarker091"/>are data-hungry applications, and they like their data well prepared for training and inference. Although it may sound obvious, it is more important to scrutinize the properties of data than to select an algorithm to process the data. The data, however, can come in many different formats and can be from different sources. We can consider data in its raw format – for example, a text document or an image file. We can also consider data in a format that is specific to a task at hand – for example, tokenized text (where words are divided into tokens) or an image with bounding boxes (where objects are identified and enclosed <span class="No-Break">in rectangles).</span></p>
			<p>When considering the end user system, what we can do with the data and how we handle the data becomes crucial. However, identifying important elements in the data and transforming it into a format that is useful for ML algorithms depends on what our task is and which algorithm we use. Therefore, in this chapter, we will work both with data and with algorithms to <span class="No-Break">process it.</span></p>
			<p>In this chapter, we will introduce three data types – images, text, and formatted text (program source code). We will explore how each of these types of data can be used in ML, how they should be annotated, and for <span class="No-Break">what purpose.</span></p>
			<p>Introducing these three types of data provides us with the possibility to explore different ways of annotating these sources of data. Therefore, in this chapter, we will focus on <span class="No-Break">the following:</span></p>
			<ul>
				<li>Raw data and features – what are <span class="No-Break">the differences?</span></li>
				<li>Every data has its purpose – annotations <span class="No-Break">and tasks</span></li>
				<li>Where different types of data can be used together – an outlook on multi-modal <span class="No-Break">data models</span></li>
			</ul>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor040"/>Raw data and features – what are the differences?</h1>
			<p>ML systems are data-hungry. They rely on the data to be trained and to make inferences. However, not all data is equally important. Before the era of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>), the<a id="_idIndexMarker092"/> data was supposed to be processed in order to be used in ML. Before DL, the algorithms were limited in the amount of data that could be used for training. The storage and memory limitations were also limited, and therefore, ML engineers <a id="_idIndexMarker093"/>had to prepare the data much more than for DL. For example, ML engineers needed to spend more effort to find a small but still representative sample of data for training. After the introduction of DL, ML models can find complex patterns in much larger datasets. Therefore, the work of ML engineers is now focused on finding sufficiently large, and <span class="No-Break">representative, datasets.</span></p>
			<p>Classical ML systems – that is, non-DL systems – require data in a tabular form in order to make inferences, and <a id="_idIndexMarker094"/>therefore it is important to design the right feature extraction mechanisms for this kind <span class="No-Break">of system.</span></p>
			<p>DL systems, on <a id="_idIndexMarker095"/>the other hand, require minimal data processing and can learn patterns from data in its (almost) raw format. Minimal processing of data is needed as DL systems need a bit of different information about the data for different tasks; they also extract information from the raw data by themselves. For example, they can capture the context of a text without the need to manually process it. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em> illustrates these differences between different types of data based on the tasks that can be performed on them. In this case, the data is in the form <span class="No-Break">of images:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer029">
					<img alt="Figure 3.1 – Types of learning systems and the data that they require for images" src="image/B19548_03_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Types of learning systems and the data that they require for images</p>
			<p>Raw images <a id="_idIndexMarker096"/>are often used for further processing, but they can be used in such tasks as image classification. The task of image classification relates to when the input to the algorithm is the raw image, and the output is the class of the image. We often see these kinds of tasks when we talk about images that contain “cats,” “dogs,” <span class="No-Break">or “cars.”</span></p>
			<p>There are considerable practical applications of this task. One application is in insurance. Several insurance companies have changed their business model and digitalized their businesses. Before the mid-2010s, insurance companies required an initial visit to a workshop to make a first assessment of damage to cars. Today, that first damage is assessed automatically by image classification algorithms. We take a picture of the damaged part with a smartphone and send it to the insurance company’s software, where trained ML algorithms are used to make an assessment. In rare, difficult cases, the image needs to be scrutinized by a human operator. This kind of workflow saves money and time and provides a better experience for the handling of <span class="No-Break">damage claims.</span></p>
			<p>Another application is medical image classification, where radiology images are classified automatically to provide an initial diagnosis, and therefore, reduce the burden on medical specialists (in this <span class="No-Break">case, radiologists).</span></p>
			<p>Masked images <a id="_idIndexMarker097"/>are processed using filters to emphasize aspects of interest. Most often, these filters are black-and-white filters or grayscale filters. They emphasize the differences between light and dark parts of the images to make it easier to recognize shapes and then to classify these shapes and trace them (in the case of video feeds). This kind of application is often used in perception systems – for example, <span class="No-Break">in cars.</span></p>
			<p>One practical application of a perception system that uses masked images is the recognition of horizontal road markings, such as lane markings. The vehicle’s camera takes a picture of the road in front of the car, then its software masks the image and sends it to an ML algorithm for detection and classification. <strong class="bold">OpenCV</strong> is one of the libraries <a id="_idIndexMarker098"/>used for this kind of task. Other practical applications include face recognition<a id="_idIndexMarker099"/> or <strong class="bold">optical character </strong><span class="No-Break"><strong class="bold">recognition</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">OCR</strong></span><span class="No-Break">).</span></p>
			<p>Semantic map images include overlays that describe what can be seen in the image, covering a part of an image that contains specific information such as the sky, a car, a person, or a building. A semantic map can cover a part of the image that contains a car, the road that the car is on, the surroundings, and the sky. The semantic map provides rich information about the image that is used for advanced vision perception algorithms, which, in turn, provide information for decision algorithms. Vision perception is particularly needed in automotive systems for <span class="No-Break">autonomous vehicles.</span></p>
			<p>One of the applications of semantic maps is in the active safety systems of vehicles. Images captured by the front camera are processed using <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) to<a id="_idIndexMarker100"/> add a semantic map and then used in decision algorithms. These decision algorithms either provide feedback to the driver or take actions autonomously. We can see that often when a car reacts to driving too close to another car or when it detects an obstacle in <span class="No-Break">its way.</span></p>
			<p>Other applications of semantic maps include medical image analyses, whereby ML algorithms provide input to medical specialists as to what the image contains. An example can be brain tumor segmentation <a id="_idIndexMarker101"/>using <strong class="bold">deep </strong><span class="No-Break"><strong class="bold">CNNs</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DCNNs</strong></span><span class="No-Break">).</span></p>
			<p>Finally, bounding-box images contain information about the boundaries of objects in images. For each shape of interest, such as a car, pedestrian, or tumor, there is a bounding box surrounding that part of the image, annotated with the class of that shape. These kinds of images are used for detecting objects and providing that information to <span class="No-Break">other algorithms.</span></p>
			<p>One of the applications where we use this kind of image is object recognition in robot coordination systems. A robot’s camera registers an<a id="_idIndexMarker102"/> image, the CNN identifies objects, and the robot’s decision software traces the object in order to avoid collisions. Tracing each object is used to change the behavior of the autonomous robot to reduce the risk of collisions and damage, as well as to optimize the operations of the robot and <span class="No-Break">its environment.</span></p>
			<p>Hence my first best practice in <span class="No-Break">this chapter.</span></p>
			<p class="callout-heading">Best practice #14</p>
			<p class="callout">Design the entire software system based on the task that you need to solve, not only the <span class="No-Break">ML model.</span></p>
			<p>Since every <a id="_idIndexMarker103"/>kind of algorithm that we use requires different processing of the image and provides different kinds of information, we need to understand how to create the entire system around it. In the previous chapter, we discussed pipelines, which include only the ML data pipeline, but a software system requires much more. For safety-critical functionality, we need to design safety cages and signaling to reduce the risk of wrong classifications/detections from ML models. Therefore, we need to understand what we want to do – whether the information is only for making simple decisions (for example, damaged bumper in a car versus not) or whether the classification is part of complex behavior decisions (for example, should the robot turn to the right to avoid the obstacle or should it slow down to let another robot <span class="No-Break">move past?).</span></p>
			<p>Images are one type of data that we use in ML; another one is text. The use of text has been popularized in recent years with the <a id="_idIndexMarker104"/>introduction of <strong class="bold">recurrent NNs</strong> (<strong class="bold">RNNs</strong>) and transformers. These NN architectures are DL networks that are capable of capturing the context (and, by extension, basic semantics) of words. These models find statistical connections between tokens (and therefore, words) and so can identify similarities that classical ML models are not capable of. Machine translations were a popular application of these models in the beginning, but now, the applications are much wider than this – for example, in understanding programming languages. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> shows the type of text data that can be used with different types <span class="No-Break">of models:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer030">
					<img alt="Figure 3.2 – Types of learning systems and the data they require for text" src="image/B19548_03_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Types of learning systems and the data they require for text</p>
			<p>The raw text data is used today in <a id="_idIndexMarker105"/>training <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), but historically, this kind of data was used for various machine translation models. One of the models that is used in these tasks is the <strong class="source-inline">word2vec</strong> model, which translates text tokens into <a id="_idIndexMarker106"/>vectors of numbers – embeddings – which are distances from that token to other tokens in the vocabulary. We saw an example of this in <a href="B19548_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, where we counted the number of words in sentences. By using this technology, the <strong class="source-inline">word2vec</strong> model captures the context of tokens, also known as their similarity. This similarity can be extended to entire sentences or even paragraphs, depending on the size and depth of <span class="No-Break">the model.</span></p>
			<p>Another application of raw text, although in a structured format, is in <strong class="bold">sentiment analysis</strong> (<strong class="bold">SA</strong>). We<a id="_idIndexMarker107"/> use a tabular format of the text data in order to analyze whether the text’s sentiment is positive, negative, or neutral. An extension of that task is understanding the intent of the text – whether it is an explanation, a query, or <span class="No-Break">a description.</span></p>
			<p>Masked text data <a id="_idIndexMarker108"/>refers to when we mask one or more tokens in a sequence of tokens, and we train the models to predict the token. This is an example of self-supervised training as the model is trained on data that is not annotated, but by masking tokens in different ways (for example, random, based on similarity, human annotations), the model can understand which tokens can be used in that specific context. The larger the model – the transformer – the more data is needed, and a more complex training process <span class="No-Break">is needed.</span></p>
			<p>Finally, annotated text refers to when we label pieces of text with a specific class, just as with images. An example of such annotation is a sentiment. Then, the model captures patterns in the data, and therefore, can repeat these patterns. An example of a task in this area is sentiment recognition, where the model is trained to recognize whether a piece of text is positive or negative in <span class="No-Break">its tone.</span></p>
			<p>A special case of textual data is programming language source code. The use of ML models for programming language tasks has become more popular in the last few years as it provides the possibility to increase the speed and quality of software development. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> shows types of programming language data and <span class="No-Break">typical tasks:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer031">
					<img alt="Figure 3.3 – Types of programming language data and typical tasks" src="image/B19548_03_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Types of programming language data and typical tasks</p>
			<p>Raw source <a id="_idIndexMarker109"/>code data is used for tasks that are related to programming language understanding – for example, translations between different programming languages, such as the one using the TransCoder model. This task is similar to translation between natural languages, although it adds additional steps to make the program compile and pass <span class="No-Break">test cases.</span></p>
			<p>Masked programming language code is often used for training models with the purpose of repairing defects – the model is trained on a set of programs that correct defects and then applied on programs with defects. Masked programs are used to train models that can identify problems and provide fixes for them. These tasks are quite experimental at the moment of writing this book but with very <span class="No-Break">promising results.</span></p>
			<p>Annotated source code is used for a variety of tasks. These tasks include defect predictions, code reviews, and identification of design patterns or company-specific design rules. ML models provide much better results than any other techniques for these tasks – for example, compared to static code <span class="No-Break">analysis tools.</span></p>
			<p>Source code is <a id="_idIndexMarker110"/>used to train models for advanced software engineering tasks, such as creating programs. GitHub Copilot is one such tool that has been very successful, both in research and in <span class="No-Break">commercial applications.</span></p>
			<p>Now, the aforementioned three types of data illustrate only a small number of applications of ML. The sky is the limit for those who want to utilize ML models for designing software systems. Before designing the systems, however, we need to understand how we work with data in <span class="No-Break">more detail.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor041"/>Images</h2>
			<p>Raw image data<a id="_idIndexMarker111"/> is often stored in files with annotations in other files. Raw image data presents aspects that are relevant to the system in question. An example of data used for training active safety algorithms is presented in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer032">
					<img alt="Figure 3.4 – Front-camera image from a vehicle" src="image/B19548_03_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Front-camera image from a vehicle</p>
			<p>The image with the car is used, in this example, to train a CNN to recognize whether it is safe to drive (for example, whether the road ahead is free from obstacles). With the data annotated on the image level – that is, without masks and bounding boxes – the ML models can either classify the entire image or identify objects. When identifying objects, the models add bounding-box information to <span class="No-Break">the images.</span></p>
			<p>In order to train a CNN for images that contain many objects of significant size (such as high-definition resolution of 1920 x 1080 pixels), we need both large datasets and large computational resources. There are a few reasons <span class="No-Break">for this.</span></p>
			<p>First, colors require a<a id="_idIndexMarker112"/> lot of data to be recognized correctly. Although we humans see the color red as almost uniform, the actual pixel intensity of that color varies a lot, which means that we need to create a CNN so that it can understand that different shades of red are sometimes important to recognize <span class="No-Break">braking vehicles.</span></p>
			<p>Second, the large size of images contains details that are not relevant. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em> presents how a CNN is designed. This is a CNN in a <span class="No-Break">LeNet style:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer033">
					<img alt="Figure 3.5 – Conceptual design of a CNN" src="image/B19548_03_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Conceptual design of a CNN</p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em> shows that the NN takes as input an image of size 192 x 108 pixels (10 times smaller than an HD-quality image). It then uses <strong class="source-inline">MaxPool</strong> layers (for example) to reduce the number of elements, and then it uses convolutions to identify shapes. Finally, it uses two dense layers to classify the images into a vector of 64 different classes. The size of the images determines the complexity of the network. The larger the images, the more convolutions are required, and the larger the first layer. Larger networks take more time to train (the difference may be measured in days) and require more data (the difference may be measured in tens of thousands of images, depending on the number of classes and quality <span class="No-Break">of images).</span></p>
			<p>Therefore, for many applications, we use grayscale images and downsize them significantly. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6</em> shows the same image as previously but in grayscale, downsized to 192 x 108 pixels. The size of the image has been significantly reduced and so have the requirements for the first <span class="No-Break">convolutional layers:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer034">
					<img alt="Figure 3.6 – Black-and-white transformed image (lower-quality lossy transformation illustrated on purpose)" src="image/B19548_03_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Black-and-white transformed image (lower-quality lossy transformation illustrated on purpose)</p>
			<p>However, the<a id="_idIndexMarker113"/> object in the image is still perfectly visible and can be used for further analysis. Therefore, here is the next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #15</p>
			<p class="callout">Downsize the size of your images and use as few colors as possible to reduce the computational complexity of <span class="No-Break">your system.</span></p>
			<p>Before designing the system, we need to understand what kinds of images we have and how we can use them. Then, we can perform these kinds of transformations so that the system that we design can handle the tasks that it is designed for. However, it’s important to note that downsizing images can also result in loss of information, which can affect the accuracy of the ML model. It’s important to carefully balance the trade-offs between computational complexity and information loss when deciding how to preprocess images for an <span class="No-Break">ML task.</span></p>
			<p>Downsizing and<a id="_idIndexMarker114"/> converting images to grayscale is a common practice in ML. In fact, there exist several well-known and widely used benchmark datasets that use this technique. One of them is the <strong class="source-inline">MNIST</strong> dataset of handwritten numbers. The dataset is available for download as part of the most popular ML libraries such as TensorFlow and Keras. Just use the following code to get hold of <span class="No-Break">the images:</span></p>
			<pre class="source-code">
# import the Keras library that contains the MNIST dataset
from keras.datasets import mnist
# load the dataset directly from the Keras website
# and use the standard train/test splits
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
# import a Matplot library to plot the images
from matplotlib import pyplot
# plot first few images
for i in range(9):
# define subplot to be 330 pixels wide
pyplot.subplot(330 + 1 + i)
# plot raw pixel data
   pyplot.imshow(X_train[i],
                   cmap=pyplot.get_cmap('gray'))
# show the figure
pyplot.show()</pre>			<p>The code illustrates how to download the dataset, which is already split into <strong class="source-inline">test</strong> and <strong class="source-inline">train</strong> data, with<a id="_idIndexMarker115"/> annotations. It also shows how to visualize the dataset, which results in the images seen in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer035">
					<img alt="Figure 3.7 – Visualization of the first few images in the MNIST dataset; the images are rasterized on purpose to illustrate their real size" src="image/B19548_03_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Visualization of the first few images in the MNIST dataset; the images are rasterized on purpose to illustrate their real size</p>
			<p>The size of the images in the MNIST dataset is 28 x 28 pixels, which is perfectly sufficient to train and test new ML models. Although the dataset is well known and used in ML, it is relatively small and uniform – only grayscale numbers. Therefore, for more advanced tasks, we should look for more <span class="No-Break">diverse datasets.</span></p>
			<p>Images of handwritten numbers are naturally useful, but we often want to use more complex images, and therefore, the standard libraries contain images with more than just 10 classes (number of digits). One such dataset is the <span class="No-Break">Fashion-MNIST dataset.</span></p>
			<p>We can download it by using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from keras.datasets import fashion_mnist
(X_train, Y_train), (X_test, Y_test)=fashion_mnist.load_data()</pre>			<p>The code, which we can use as part of the visualization shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.7</em>, produces the set of images seen in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em>. The images are of the same size and the same number of classes, but with a <span class="No-Break">larger complexity:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer036">
					<img alt="Figure 3.8 – Fashion-MNIST dataset; the images are rasterized on purpose to illustrate their real size" src="image/B19548_03_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Fashion-MNIST dataset; the images are rasterized on purpose to illustrate their real size</p>
			<p>Finally, we<a id="_idIndexMarker116"/> can also use libraries that contain color images, such as the CIFAR-10 dataset. The dataset can be accessed with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from keras.datasets import cifar10
# load dataset
(X_train, Y_train), (X_test, Y_test)== cifar10.load_data()</pre>			<p>The dataset contains images of 10 different classes of similar size (32 x 32 pixels), but with colors, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer037">
					<img alt="Figure 3.9 – CIFAR-10 dataset; the images are rasterized on purpose to illustrate their real size" src="image/B19548_03_9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – CIFAR-10 dataset; the images are rasterized on purpose to illustrate their real size</p>
			<p>This is not the end of such benchmark datasets. Some datasets contain more classes, larger images, or both. Therefore, it is important to take a look at these datasets beforehand and in connection with the task that our system has <span class="No-Break">to perform.</span></p>
			<p>In the majority <a id="_idIndexMarker117"/>of cases, grayscale images are perfectly fine for classification tasks. They provide the ability to quickly get an orientation in the data, and they are small enough that the classification is of <span class="No-Break">good quality.</span></p>
			<p>The usual size of the benchmark datasets is about 50,000–100,000 images. This illustrates that even for such a small number of classes and for such small images, the number is significant. Just imagine annotating those 100,000 images. For more complex images, the size of the datasets can be significantly larger. For example, the BDD100K dataset used in automotive software contains over <span class="No-Break">100,000 images.</span></p>
			<p>Therefore, here is my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #16</p>
			<p class="callout">Use a reference dataset (such as MNIST or STL) for benchmarking whether the system works <span class="No-Break">or not.</span></p>
			<p>In order to understand whether the entire system works or not, such benchmark datasets are very useful. They provide us with a preconfigured train/test split, and there are plenty of algorithms that can be used to understand the quality of <span class="No-Break">our algorithm.</span></p>
			<p>We should also consider the next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #17</p>
			<p class="callout">Whenever possible, use models that are already pre-trained for specific tasks (for example, neural models for image classification or <span class="No-Break">semantic segmentation).</span></p>
			<p>Just as we should <a id="_idIndexMarker118"/>strive to reuse images for benchmarking, we should also strive to reuse models that are pre-trained. This saves previous design resources and reduces the risk of spending too much time to find optimal architectures of NN models or to find the optimal set of parameters (even if we use <span class="No-Break"><strong class="source-inline">GradientSearch</strong></span><span class="No-Break"> algorithms).</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor042"/>Text</h2>
			<p>One of the types of analysis done on text is SA – classification of whether a piece of text (a sentence, for example) is positive <span class="No-Break">or negative.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.10</em> presents <a id="_idIndexMarker119"/>an example of data that can be used for SA. The data is publicly available and has been created from Amazon product reviews. Data for this kind of analysis is often structured in a table, where we have entities such as <strong class="source-inline">ProductId</strong> (I’ve truncated the <strong class="source-inline">Id</strong> columns for brevity) or <strong class="source-inline">UserId</strong>, as well as <strong class="source-inline">Score</strong> for reference and <strong class="source-inline">Text</strong> <span class="No-Break">to classify.</span></p>
			<p>This data structure<a id="_idIndexMarker120"/> provides us with the possibility to quickly summarize the text and visualize it. The visualization can be done in several ways – for example, by plotting a histogram of the scores. However, the most interesting visualizations are the ones that are provided by the statistics of words/tokens used in <span class="No-Break">the text:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Id</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">ProductId</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">UserId</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Score</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Summary</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Text</span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">B001KFG0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">UHU8GW</span></p>
						</td>
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p>Good Quality <span class="No-Break">Dog Food</span></p>
						</td>
						<td class="No-Table-Style">
							<p>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better <span class="No-Break">than most.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">B008GRG4</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">ZCVE5NK</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>Not as <span class="No-Break">Advertised</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product <span class="No-Break">as “Jumbo”.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">B000OCH0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">WJIXXAIN</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p>“Delight” says <span class="No-Break">it all</span></p>
						</td>
						<td class="No-Table-Style">
							<p>This is a confection that has been around a few centuries. It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar. And it is a tiny mouthful of heaven. Not too chewy, and very flavorful. I highly recommend this yummy treat. If you are familiar with the story of C.S. Lewis’ “The Lion, The Witch, and The Wardrobe” - this is the treat that seduces Edmund into selling out his Brother and Sisters to <span class="No-Break">the Witch.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">B000A0QIQ</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">C6FGVXV</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p>Cough <span class="No-Break">Medicine</span></p>
						</td>
						<td class="No-Table-Style">
							<p>If you are looking for the secret ingredient in Robitussin I believe I have found it. I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda. The flavor is <span class="No-Break">very medicinal.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">B0062ZZ7K</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">LF8GW1T</span></p>
						</td>
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Great taffy</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Great taffy at a great price. There was a wide assortment of yummy taffy. Delivery was very quick. If your a taffy lover, this is <span class="No-Break">a deal.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Example of data for product reviews, structured for SA; only the first five rows are shown</p>
			<p>One way <a id="_idIndexMarker121"/>to visualize the data is to use the word cloud visualization technique. A simple script for visualizing this kind of data is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
# Create stopwords list:
stopwords = set(STOPWORDS)
stopwords.update(["br", "href"])
# create text for the wordcloud
textt = " ".join(review for review in dfRaw.Text)
# generation of wordcloud
wordcloud = WordCloud(stopwords=stopwords,
                  max_words=100,
                  background_color="white").generate(textt)
# showing the image
# and saving it to the png file
plt.figure(figsize = [12,9])
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()</pre>			<p>The result of running this script is shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.11</em>. A word cloud shows trends in terms of frequency of the use of words – words used more frequently are larger than words used <span class="No-Break">less frequently:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer038">
					<img alt="Figure 3.11 – Word cloud visualization of the Text column" src="image/B19548_03_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – Word cloud visualization of the Text column</p>
			<p>Hence, my next best practice is <span class="No-Break">as follows.</span></p>
			<p class="callout-heading">Best practice #18</p>
			<p class="callout">Visualize your raw data to get an understanding of patterns in <span class="No-Break">your data.</span></p>
			<p>Visual representation<a id="_idIndexMarker122"/> of data is important to understand the underlying patterns. I <a id="_idIndexMarker123"/>cannot stress that enough. I use both Python’s Matplotlib and Seaborn as well as visual analytics tools such as TIBCO Spotfire to plot charts and understand my data. Without such a visualization, and thus without such an understanding of the patterns, we are bound to make wrong conclusions and even design systems with flaws that are difficult to remove without a <span class="No-Break">complete redesign.</span></p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor043"/>Visualization of output from more advanced text processing</h2>
			<p>Visualization <a id="_idIndexMarker124"/>of text helps us to understand what the text contains, but it does not capture the meaning of it. In this book, we will work with advanced text processing algorithms – feature extractors. Therefore, we need to understand how to create visualizations of the output from <span class="No-Break">these algorithms.</span></p>
			<p>One way of working with feature extraction is to use word embeddings – a method to convert words or sentences into vectors of numbers. <strong class="source-inline">word2vec</strong> is one model that can do that, but there are more powerful ones too. OpenAI’s GPT-3 model is one of the largest models that are openly available. Obtaining embeddings of paragraphs is quite straightforward. First, we connect to the OpenAI API and then query it for the embeddings. Here is the code that does the querying of the OpenAI API (<span class="No-Break">in boldface):</span></p>
			<pre class="source-code">
# first we combine the title and the content of the review
df['combined'] = "Title: " + df.Summary.str.strip() + "; Content: " + df.Text.str.strip()
# we define a function to get embeddings, to make the code more straightforward
def get_embedding(text, engine="text-similarity-davinci-001"):
   text = text.replace("\n", " ")
   r<strong class="bold">eturn openai.Embedding.create(input = [text], engine=engine)['data'][0]['embedding']</strong>
# and then we get embeddings for the first 5 rows
df['babbage_similarity'] = df.head(5).combined.apply(lambda x: get_embedding(x, engine='text-similarity-babbage-001'))</pre>			<p>What we obtain by running this piece of code is <strong class="source-inline">5</strong> vectors (one for each row) of <strong class="source-inline">2048</strong> numbers, which we call embeddings. The entire vector is too large to be included in the page, but the first elements look something like this: <strong class="source-inline">[-0.005302980076521635, 0.018141526728868484, -0.018141526728868484, </strong><span class="No-Break"><strong class="source-inline">0.004692177753895521, …</strong></span><span class="No-Break">.</span></p>
			<p>These <a id="_idIndexMarker125"/>numbers do not say much to us humans, but they have a meaning for the language model. The meaning is in the distance between them – words/tokens/sentences that are like one another are placed closer than words/tokens/sentences that are not similar. In order to understand these similarities, we <a id="_idIndexMarker126"/>use transformations that reduce the dimensions – one of them is <strong class="bold">t-distribution Stochastic Neighbor Embedding</strong> (<strong class="bold">t-SNE</strong>). <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.12</em> presents this kind of visualization of the five embeddings that <span class="No-Break">we obtained:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer039">
					<img alt="Figure 3.12 – t-SNE visualization of the embedding vectors for the five reviews" src="image/B19548_03_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – t-SNE visualization of the embedding vectors for the five reviews</p>
			<p>Each dot represents one review, and each cross represents the center of the cluster. The clusters are designated by the <strong class="source-inline">Score</strong> column from the original dataset. The screenshot shows that the text in each of the reviews is different (dots do not overlap) and that the clusters are separate – the crosses are positioned in a different part of <span class="No-Break">the screenshot.</span></p>
			<p>Therefore, my next best practice is <span class="No-Break">about that.</span></p>
			<p class="callout-heading">Best practice #19</p>
			<p class="callout">Visualize your data when it has been turned into features to monitor whether the same patterns are <span class="No-Break">still observable.</span></p>
			<p>Just as with<a id="_idIndexMarker127"/> the previous best practice, we need to visualize the data to check whether patterns that we observed in the raw data are still observable. This step is important as we need to know that the ML model can indeed learn these patterns. As these models are statistical in nature, they always capture patterns, but when a pattern is not there, they capture something that is not useful – even though it resembles <span class="No-Break">a pattern.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor044"/>Structured text – source code of programs</h2>
			<p>The<a id="_idIndexMarker128"/> source code of programs is a special case of text data. It has the same type of modality – text – but it contains additional information in the form of the grammatical/syntactical structure of the program. Since every programming language is based on grammar, there are specific rules for how a program should be structured. For instance, in C, there should be a specific function called <strong class="source-inline">main</strong>, which is the entry point for <span class="No-Break">the program.</span></p>
			<p>These specific rules make the text structured in a specific way. They may make it more difficult to understand the text for a human being, but this structure can certainly be helpful. One of the models that <a id="_idIndexMarker129"/>uses this structure is <strong class="source-inline">code2vec</strong> (<a href="https://code2vec.org/">https://code2vec.org/</a>). The <strong class="source-inline">code2vec</strong> model is similar to word2vec, but it takes as input the <strong class="bold">Abstract Syntax Tree</strong> (<strong class="bold">AST</strong>) of<a id="_idIndexMarker130"/> the program that it analyzes – for example, the <span class="No-Break">following program:</span></p>
			<pre class="source-code">
void main()
{
    Console.println("Hello World");
}</pre>			<p>This can be represented by the AST in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer040">
					<img alt="Figure 3.13 – AST for a simple “Hello World” program" src="image/B19548_03_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – AST for a simple “Hello World” program</p>
			<p>The <a id="_idIndexMarker131"/>example program is visualized as a set of instructions with their context and the role that they play in the program. For example, the words <strong class="source-inline">void</strong> and <strong class="source-inline">main</strong> are two parts of the method declaration, together with the block statement (<strong class="source-inline">BlockStmt</strong>), which is the body of <span class="No-Break">the method.</span></p>
			<p><strong class="source-inline">code2vec</strong> is<a id="_idIndexMarker132"/> an example of a model that uses programming language information (in this case, grammar) as input to the model. Tasks that the model can do include finding similarities between words (such as word2vec models), finding combinations, and identifying analogies. For example, the model can identify all combinations of the words <strong class="source-inline">int</strong> and <strong class="source-inline">main</strong> and provide the following answers (with probabilities): <strong class="source-inline">realMain</strong> (71%), <strong class="source-inline">isInt</strong> (71%), and <strong class="source-inline">setIntField</strong> (69%). These tasks, by extension, can be used for program repair, where the model can identify mistakes and <span class="No-Break">repair them.</span></p>
			<p>However, using an AST or similar information has disadvantages. The main disadvantage is that the analyzed program must compile. This means that we cannot use these kinds of models in the context where we want to analyze programs that are incomplete – for example, in the context<a id="_idIndexMarker133"/> of <strong class="bold">continuous integration</strong> (<strong class="bold">CI</strong>) or modern code reviews. When we analyze only a small part of the code, the model cannot parse it, obtain its AST, and use it. Therefore, here’s my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #20</p>
			<p class="callout">Only use the necessary information as input to ML models. Too much information may require additional processing and make the training hard to <span class="No-Break">converge (finish).</span></p>
			<p>When designing <a id="_idIndexMarker134"/>the processing pipeline, make sure that the information provided to the model is necessary, as every piece of information poses new requirements for the entire software system. As in the example of an AST, when it is necessary, it is powerful information, but if not available, it can be a huge hindrance to getting the data analysis pipeline <span class="No-Break">to work.</span></p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor045"/>Every data has its purpose – annotations and tasks</h1>
			<p>Data in raw format is important, but only the first step in the development and operations of ML software. The most important part, and the costliest one, is the annotation of the data. To train an ML model and then use it to make inferences, we need to define a task. Defining a task is both conceptual and operational. The conceptual definition is to define what we want the software to do, but the operational definition is how we want to achieve that goal. The operational definition boils down to a definition of what we see in the data and what we want the ML model <span class="No-Break">to identify/replicate.</span></p>
			<p>Annotations<a id="_idIndexMarker135"/> are the mechanisms by which we direct the ML algorithms. Every piece of data that we use requires some sort of label to denote what it is. In the raw format of the data, this annotation can be a label of what the data point contains. For example, such a label can be that the image contains the number 1 (from the MNIST dataset) or a car (from the CIFAR-10 dataset). However, these simple annotations are important only in dedicated tasks. For more advanced tasks, the annotations need to <span class="No-Break">be richer.</span></p>
			<p>One of the types of annotations relates to when we designate part of the data as interesting. In the case of images, this is done by drawing bounding boxes around objects of interest. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14</em> presents such <span class="No-Break">an image:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer041">
					<img alt="Figure 3.14 – Image with bounding boxes" src="image/B19548_03_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – Image with bounding boxes</p>
			<p>The image <a id="_idIndexMarker136"/>contains boxes around elements that we want the model to recognize. In this case, we want to recognize objects that are vehicles (green boxes), other road users (orange boxes), and important background objects (gray boxes). These kinds of annotations are used for the ML model to learn shapes and to identify such shapes in new objects. In the case of this example, the bounding boxes identify elements that are important for active safety systems in cars, but this is not the <span class="No-Break">only application.</span></p>
			<p>Other applications of such bounding boxes include medical image analysis, where the task is to identify tissue that needs further analysis. These could also be systems for face recognition and <span class="No-Break">object detection.</span></p>
			<p>Although<a id="_idIndexMarker137"/> this task, and the bounding boxes, could be seen as a special case of annotating raw data, it is a bit different. Every box could be seen as a separate image that has a label, but the challenge is that every box is of a different size. Therefore, using such differently shaped images would require preprocessing (for example, rescaling). It would also only work for the training because, in the inference, we would need to identify objects before they are classified – and that’s exactly the task we need the NN to do <span class="No-Break">for us.</span></p>
			<p>My best practice for using this kind of data is set <span class="No-Break">out next.</span></p>
			<p class="callout-heading">Best practice #21</p>
			<p class="callout">Use bounding boxes in the data when the task requires the detection and tracking <span class="No-Break">of objects.</span></p>
			<p>Since bounding boxes <a id="_idIndexMarker138"/>allow us to identify objects, the natural use of this data is in tracking systems. An example application is a system that monitors parking spots using a camera. It detects parking spots and tracks whether there is a vehicle parked in the spot <span class="No-Break">or not.</span></p>
			<p>An extension of the object detection task is a<a id="_idIndexMarker139"/> perception task, where our ML software needs to make decisions based on the context of the data – or <span class="No-Break">a situation.</span></p>
			<p>For image data, this context can be described by a semantic map. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.15</em> shows such <span class="No-Break">a map:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer042">
					<img alt="Figure 3.15 – Image with a semantic map; building is one of the labels for the semantic map" src="image/B19548_03_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Image with a semantic map; building is one of the labels for the semantic map</p>
			<p>The screenshot shows an overlay of different colors that cover objects of specific types. The orange overlay shows vehicles, while the purple one shows vulnerable road users, which is the pedestrian in this image. Finally, the pink color indicates buildings, and the red color covers <span class="No-Break">the background/sky.</span></p>
			<p>The<a id="_idIndexMarker140"/> semantic map provides more flexibility than bounding boxes (as some objects are more interesting than others) and allows the ML system to get the context of the image. By identifying what kinds of elements are present in the image, the ML model can provide information about where the image was taken to the decision algorithm of the software system that <span class="No-Break">we design.</span></p>
			<p>Therefore, here’s my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #22</p>
			<p class="callout">Use semantic maps when you need to get the context of the image or you need details of a <span class="No-Break">specific area.</span></p>
			<p>Semantic maps <a id="_idIndexMarker141"/>require heavy computations to be used effectively; therefore, we should use them scarcely. We should use these maps when we have tasks related to the context, such as perception algorithms or image alteration – for example, changing the color of the sky in the image. Regarding the accuracy of the information, it is generally true that semantic maps require heavy computations and are therefore used selectively. An example of a tool that does such a semantic mapping <span class="No-Break">is Segments.ai.</span></p>
			<p>Semantic maps are useful when the task requires understanding the context of an image or details of a specific area. For example, in autonomous driving, a semantic map can be used to identify objects on the road and their relationship to each other, allowing the vehicle to make informed decisions about its movement. However, specific use cases for semantic maps may vary depending on <span class="No-Break">the application.</span></p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor046"/>Annotating text for intent recognition</h1>
			<p>SA, which we <a id="_idIndexMarker142"/>mentioned before, is only one type of annotation of textual data. It is useful for assessing whether the text is positive or negative. However, instead of annotating text with a sentiment, we can annotate the text with – for example – the intent and train an ML model to recognize intent from other text passages. The table in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.16</em> provides such an annotation, based on the same review data <span class="No-Break">as before:</span></p>
			<table class="No-Table-Style" id="table002-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Id</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Summary</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Text</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Intent</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p>Good Quality <span class="No-Break">Dog Food</span></p>
						</td>
						<td class="No-Table-Style">
							<p>I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better <span class="No-Break">than most.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Advertise</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>Not as <span class="No-Break">Advertised</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product <span class="No-Break">as “Jumbo”.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Criticize</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p>“Delight” says <span class="No-Break">it all</span></p>
						</td>
						<td class="No-Table-Style">
							<p>This is a confection that has been around a few centuries. It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar. And it is a tiny mouthful of heaven. Not too chewy, and very flavorful. I highly recommend this yummy treat. If you are familiar with the story of C.S. Lewis’ “The Lion, The Witch, and The Wardrobe” - this is the treat that seduces Edmund into selling out his Brother and Sisters to <span class="No-Break">the Witch.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Describe</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Cough</span>  <span class="No-Break">Medicine</span></p>
						</td>
						<td class="No-Table-Style">
							<p>If you are looking for the secret ingredient in Robitussin I believe I have found it. I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda. The flavor is <span class="No-Break">very medicinal.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Criticize</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Great taffy</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Great taffy at a great price. There was a wide assortment of yummy taffy. Delivery was very quick. If your a taffy lover, this is <span class="No-Break">a deal.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Advertise</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 –  Textual data annotation for intent recognition</p>
			<p>The last<a id="_idIndexMarker143"/> column of the table shows the annotation of the text – the intent. Now, we can use the intent as a label to train an ML model to recognize intent in the new text. Usually, this task requires a two-step approach, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.17</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer043">
					<img alt="Figure 3.17 – A two-step approach for training models based on text" src="image/B19548_03_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – A two-step approach for training models based on text</p>
			<p>The annotated text is organized into two parts. The first part is the text itself (for example, the <strong class="source-inline">Text</strong> column in our example table), and the second part is the annotation (e.g., the <strong class="source-inline">Intent</strong> column in our example). The text is processed using a model such as the word2vec model or a transformer, which encodes the text as a vector or a matrix. The annotations are encoded into a vector using techniques such as one-hot encoding so that they can be used as decision classes for the classification algorithm. The classification <a id="_idIndexMarker144"/>algorithm takes both the encoded annotations and the vectorized text. Then, it is trained to find the best fit of the vectorized text (<em class="italic">X</em>) to the <span class="No-Break">annotation (</span><span class="No-Break"><em class="italic">Y</em></span><span class="No-Break">).</span></p>
			<p>Here is my best practice on how to <span class="No-Break">perform that.</span></p>
			<p class="callout-heading">Best practice #23</p>
			<p class="callout">Use a pre-trained embedding model such as GPT-3 or an existing BERT model to vectorize <span class="No-Break">your text.</span></p>
			<p>From my experience, working with text is often easier if we use a predefined language model to vectorize the text. The <em class="italic">Hugging Face</em> website (<a href="http://www.huggingface.com">www.huggingface.com</a>) is an excellent<a id="_idIndexMarker145"/> source of these models. Since LLMs require significant resources to train, the existing models are often good enough for most of the tasks. Since we develop the classifier model as the next step in the pipeline, we can focus our efforts on making that model better and align it with <span class="No-Break">our task.</span></p>
			<p>Another type of annotation of text data is the context in terms of the <strong class="bold">part of speech</strong> (<strong class="bold">POS</strong>). It can<a id="_idIndexMarker146"/> be seen as the semantic map used in the image data. Each word is annotated, whether it is a noun, verb, or adjective, regardless of which part of the sentence it belongs to. An example of such an annotation can be presented visually, using the<a id="_idIndexMarker147"/> Allen Institute’s AllenNLP <strong class="bold">Semantic Role Labeling</strong> (<strong class="bold">SRL</strong>) tool (<a href="https://demo.allennlp.org/semantic-role-labeling">https://demo.allennlp.org/semantic-role-labeling</a>). <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.18</em> presents a screenshot of such labeling for a simple sentence, while <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.19</em> presents the labeling for a more <span class="No-Break">complex one:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer044">
					<img alt="Figure 3.18 – SRL using ﻿the AllenNLP toolset" src="image/B19548_03_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – SRL using the AllenNLP toolset</p>
			<p>In this <a id="_idIndexMarker148"/>sentence, the role of each word is emphasized, and we see that there are three verbs with different associations – the last one is the main one as it links the other parts of <span class="No-Break">the sentence:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer045">
					<img alt="Figure 3.19 – SRL for a more complex sentence" src="image/B19548_03_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 – SRL for a more complex sentence</p>
			<p>The <a id="_idIndexMarker149"/>complex sentence has a larger semantic role frame, as it contains two distinct parts of the sentence. We use this role labeling in order to extract the meaning of passages of text. It is particularly useful when designing software systems based on <a id="_idIndexMarker150"/>so-called <em class="italic">grounded models</em>, which are models that check the information toward a ground truth. Such models parse the text data, find the right anchor (for example, what the question is about), and find the relevant answer in their database. These are opposed to <em class="italic">ungrounded models</em>, which create<a id="_idIndexMarker151"/> answers based on which word fits best to finish the sentence – for <span class="No-Break">example, ChatGPT.</span></p>
			<p>Therefore, my best practice is <span class="No-Break">shown next.</span></p>
			<p class="callout-heading">Best practice #24</p>
			<p class="callout">Use role labels when designing software that needs to provide <span class="No-Break">grounded decisions.</span></p>
			<p>Grounded decisions are often more difficult to provide, as the model needs to understand the context of the sentence, capture its meaning, and provide relevant answers. However, this is not always needed or even desired. Ungrounded models are often good enough <a id="_idIndexMarker152"/>for suggestions that can be later fixed by specialists. An example of such a software tool is ChatGPT, which provides answers that are sometimes incorrect and require manual intervention. However, they are a very <span class="No-Break">good start.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor047"/>Where different types of data can be used together – an outlook on multi-modal data models</h1>
			<p>This<a id="_idIndexMarker153"/> chapter introduced three types of data – images, text, and structured text. These three types of data are examples of data that is in a numerical form, such as matrices of numbers, or in forms of time series. Regardless of the form, however, working with data and ML systems is very similar. We need to extract the data from a source system, then transform it into a format that we can annotate, and then use this as input to an <span class="No-Break">ML model.</span></p>
			<p>When we consider different types of data, we could start to think about whether we could use two types of data in the same system. There are a few ways of doing that. The first one is when we use different ML systems in different pipelines, but we connect the pipelines. GitHub Copilot is such a system. It uses a pipeline for processing a natural language to find similar programs and to transform them so that they fit the context of the program being <span class="No-Break">developed now.</span></p>
			<p>Another example is a system that generates textual descriptions of images. It takes an image as an input, identifies objects in it, and then generates text based on these objects. The generation of text is done by a completely different ML model than the classification <span class="No-Break">of images.</span></p>
			<p>However, there are new models that use two different modalities – images and texts – in the same NN – for example, the Gato model. By using the input from two sources and using a very narrow (in terms of the number of neurons) network in the middle, the model is trained to generalize concepts described by two different modalities. In this way, the model is trained to understand that an image of a cat and the word “cat” should be placed in the same embedding space very close to one another, if not exactly in the same places. Although still experimental, these kinds of networks are intended to mimic the human understanding <span class="No-Break">of concepts.</span></p>
			<p>In the next chapter, we will dive a bit deeper into the understanding of data by making a deeper dive into the process of <span class="No-Break">feature engineering.</span></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor048"/>References</h1>
			<ul>
				<li><em class="italic">Tao, J. et al., An object detection system based on YOLO in traffic scene. In 2017 6th International Conference on Computer Science and Network Technology (ICCSNT). </em><span class="No-Break"><em class="italic">2017. IEEE.</em></span></li>
				<li><em class="italic">Artan, C.T. and T. Kaya, Car Damage Analysis for Insurance Market Using Convolutional Neural Networks. In International Conference on Intelligent and Fuzzy Systems. </em><span class="No-Break"><em class="italic">2019. Springer.</em></span></li>
				<li><em class="italic">Nakaura, T. et al., A primer for understanding radiology articles about machine learning and deep learning. Diagnostic and Interventional Imaging, 2020. 101(12): </em><span class="No-Break"><em class="italic">p. 765-770.</em></span></li>
				<li><em class="italic">Bradski, G., The OpenCV Library. Dr. Dobb’s Journal: Software Tools for the Professional Programmer, 2000. 25(11): </em><span class="No-Break"><em class="italic">p. 120-123.</em></span></li>
				<li><em class="italic">Memon, J. et al., Handwritten optical character recognition (OCR): A comprehensive systematic literature review (SLR). IEEE Access, 2020. 8: </em><span class="No-Break"><em class="italic">p. 142642-142668.</em></span></li>
				<li><em class="italic">Mosin, V. et al., Comparing autoencoder-based approaches for anomaly detection in highway driving scenario images. SN Applied Sciences, 2022. 4(12): </em><span class="No-Break"><em class="italic">p. 1-25.</em></span></li>
				<li><em class="italic">Zeineldin, R.A. et al., DeepSeg: deep neural network framework for automatic brain tumor segmentation using magnetic resonance FLAIR images. International journal of computer assisted radiology and surgery, 2020. 15(6): </em><span class="No-Break"><em class="italic">p. 909-920.</em></span></li>
				<li><em class="italic">Reid, R. et al., Cooperative multi-robot navigation, exploration, mapping and object detection with ROS. In 2013 IEEE Intelligent Vehicles Symposium (IV). </em><span class="No-Break"><em class="italic">2013. IEEE.</em></span></li>
				<li><em class="italic">Mikolov, T. et al., Recurrent neural network based language model. In Interspeech. </em><span class="No-Break"><em class="italic">2010. Makuhari.</em></span></li>
				<li><em class="italic">Vaswani, A. et al., Attention is all you need. Advances in neural information processing systems, </em><span class="No-Break"><em class="italic">2017. 30.</em></span></li>
				<li><em class="italic">Ma, L. and Y. Zhang, Using Word2Vec to process big text data. In 2015 IEEE International Conference on Big Data (Big Data). </em><span class="No-Break"><em class="italic">2015. IEEE.</em></span></li>
				<li><em class="italic">Ouyang, X. et al., Sentiment analysis using convolutional neural network. In 2015 IEEE International Conference on Computer and Information Technology; ubiquitous computing and communications; dependable, autonomic and secure computing; pervasive intelligence and computing. </em><span class="No-Break"><em class="italic">2015. IEEE.</em></span></li>
				<li><em class="italic">Roziere, B. et al., Unsupervised translation of programming languages. Advances in Neural Information Processing Systems, 2020. 33: </em><span class="No-Break"><em class="italic">p. 20601-20611.</em></span></li>
				<li><em class="italic">Yasunaga, M. and P. Liang, Break-it-fix-it: Unsupervised learning for program repair. In International Conference on Machine Learning. </em><span class="No-Break"><em class="italic">2021. PMLR.</em></span></li>
				<li><em class="italic">Halali, S. et al., Improving defect localization by classifying the affected asset using machine learning. In International Conference on Software Quality. </em><span class="No-Break"><em class="italic">2019. Springer.</em></span></li>
				<li><em class="italic">Ochodek, M. et al., Recognizing lines of code violating company-specific coding guidelines using machine learning. In Accelerating Digital Transformation. 2019, Springer. </em><span class="No-Break"><em class="italic">p. 211-251.</em></span></li>
				<li><em class="italic">Nguyen, N. and S. Nadi, An empirical evaluation of GitHub copilot’s code suggestions. In Proceedings of the 19th International Conference on Mining Software </em><span class="No-Break"><em class="italic">Repositories. 2022.</em></span></li>
				<li><em class="italic">Zhang, C.W. et al., Pedestrian detection based on improved LeNet-5 convolutional neural network. Journal of Algorithms &amp; Computational Technology, 2019. 13: </em><span class="No-Break"><em class="italic">p. 1748302619873601.</em></span></li>
				<li><em class="italic">LeCun, Y. et al., Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. 86(11): </em><span class="No-Break"><em class="italic">p. 2278-2324.</em></span></li>
				<li><em class="italic">Xiao, H., K. Rasul, and R. Vollgraf, Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:1708.07747, 2017.</em></span></li>
				<li><em class="italic">Recht, B. et al., Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:1806.00451, 2018.</em></span></li>
				<li><em class="italic">Robert, T., N. Thome, and M. Cord, HybridNet: Classification and reconstruction cooperation for semi-supervised learning. In Proceedings of the European Conference on Computer Vision (</em><span class="No-Break"><em class="italic">ECCV). 2018.</em></span></li>
				<li><em class="italic">Yu, F. et al., Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687, 2018. 2(5): </em><span class="No-Break"><em class="italic">p. 6.</em></span></li>
				<li><em class="italic">McAuley, J.J. and J. Leskovec, From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In Proceedings of the 22nd International Conference on World Wide </em><span class="No-Break"><em class="italic">Web. 2013.</em></span></li>
				<li><em class="italic">Van der Maaten, L. and G. Hinton, Visualizing data using t-SNE. Journal of Machine Learning Research, </em><span class="No-Break"><em class="italic">2008. 9(11).</em></span></li>
				<li><em class="italic">Sengupta, S. et al., Automatic dense visual semantic mapping from street-level imagery. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. </em><span class="No-Break"><em class="italic">2012. IEEE.</em></span></li>
				<li><em class="italic">Palmer, M., D. Gildea, and N. Xue, Semantic role labeling. Synthesis Lectures on Human Language Technologies, 2010. 3(1): </em><span class="No-Break"><em class="italic">p. 1-103.</em></span></li>
				<li><em class="italic">Reed, S. et al., A generalist agent. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:2205.06175, 2022.</em></span></li>
			</ul>
		</div>
	</body></html>