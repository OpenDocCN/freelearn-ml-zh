- en: Image Captioning with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Primarily, this chapter will provide a brief overview of creating a detailed
    English language description of an image. Using the image captioning model based
    on TensorFlow, we will be able to replace a single word or compound words/phrases
    with detailed captions that perfectly describe the image. We will first use a
    pre-trained model for image captioning and then retrain the model from scratch
    to run on a set of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Brain im2txt captioning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running our captioning code in Jupyter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retraining the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Along with knowledge of Python, the basics of image processing, and computer
    vision, we will need the following Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The codes used in the chapter have been added to the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3](https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to image captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image captioning is a process in which textual description is generated based
    on an image. To better understand image captioning, we need to first differentiate
    it from image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Difference between image classification and image captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image classification is a relatively simple process that only tells us what
    is in an image. For example, if there is a boy on a bike, image classification
    will not give us a description; it will just provide the result as **boy **or
    **bike**. Image classification can tell us whether there is a woman or a dog in
    the image, or an action, such as snowboarding. This is not a desirable result
    as there is no description of what exactly is going on in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the result we get using image classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f7f3a53-c971-43ed-9e6b-52e3f449daee.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparatively, image captioning will provide a result with a description. For
    the preceding example, the result of image captioning would be **a boy riding
    on a bike** or **a man is snowboarding**. This could be useful for generating
    content for a book or maybe helping the hearing or visually impaired.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the result we get using image captioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f449b9bd-d21a-4c25-9193-9507bade3c26.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this is considerably more challenging as conventional neural networks
    are powerful, but they're not very compatible with sequential data. Sequential
    data is where we have data that's coming in an order and that order actually matters.
    In audio or video, we have words coming in a sequential order; jumbling the words
    might change the meaning of the sentence or just make it complete gibberish.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks with long short-term memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As powerful as **convolutional neural networks** (**CNNs**) are, they don't
    handle sequential data so well; however, they are great for non-sequential tasks,
    such as image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'How CNNs work is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8176ff9-a902-4bdf-b868-6fae3abe7c5e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recurrent neural networks** (**RNNs**), which really are state of the art,
    can handle sequential tasks. An RNN consists of CNNs where data is received in
    a sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How RNNs work is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be749fa7-8b9b-4378-a461-f83bed83e489.png)'
  prefs: []
  type: TYPE_IMG
- en: Data coming in a sequence (**x[i]**) goes through the neural network and we
    get an output (**y[i]**). The output is then fed through to another iteration
    and forms a loop. This helps us remember the data coming from before and is helpful
    for sequential data tasks such as audio and speech recognition, language translation,
    video identification, and text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Another concept that has been around for a while and is very helpful is **long
    short-term memory** (**LSTM**) with RNNs. It is a way to handle long-term memory
    and avoid just passing data from one iteration to the next. It handles the data
    from the iterations in a robust way and it allows us to effectively train RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Google Brain im2txt captioning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Brain im2txt was used by Google in a paper *2015 MSCOCO Image Captioning
    Challenge,* and will form the foundation of the image captioning code that we
    will implement in our project.
  prefs: []
  type: TYPE_NORMAL
- en: The Google's GitHub TensorFlow page can be found at [https://github.com/tensorflow/models/tree/master/research/im2txt](https://github.com/tensorflow/models/tree/master/research/im2txt)[.](https://github.com/tensorflow/models/tree/master/research/im2txt)
  prefs: []
  type: TYPE_NORMAL
- en: In the research directory, we will find the `im2txt` file, which was used by
    Google in the paper, *2015 MSCOCO Image Captioning Challenge*, which is available
    for free at [https://arxiv.org/abs/1609.06647](https://arxiv.org/abs/1609.06647).
    It covers RNNs, LSTM, and fundamental algorithms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: We can check how CNNs are used for image classification and also learn how to
    use the LSTM RNNs for actually generating sequential caption outputs.
  prefs: []
  type: TYPE_NORMAL
- en: We can download the code from the GitHub link; however, it has not been set
    up to run easily as it does not include a pre-trained model, so we may face some
    challenges. We have provided you with a pre-trained model to avoid training an
    image classifier from scratch, since it is a time-consuming process. There have
    been some modifications made to the code that will make the code easy to run on
    a Jupyter Notebook or to incorporate in your own projects. The pre-trained model
    is very quick to learn using just a CPU. The same code without a pre-trained model
    might actually take weeks to learn, even on a good GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Running the captioning code on Jupyter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now run our own version of the code on a Jupyter Notebook. We can start
    up own own Jupyter Notebook and load the `Section_1-Tensorflow_Image_Captioning.ipynb` file
    from the GitHub repository ([https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb](https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we load the file on a Jupyter Notebook, it will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cf6b337-2a16-48f8-9c27-327a28fb278f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first part, we are going to load some essential libraries, including `math`, `os`,
    and `tensorflow`. We will also use our handy utility function, `%pylab inline`,
    to easily read and display images within the Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the first code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we hit *Ctrl* + *Enter* to execute the code in the cell, we will get the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5279fc38-3cc4-4768-9c74-05fc8cbbb101.png)'
  prefs: []
  type: TYPE_IMG
- en: We need to now load the TensorFlow/Google Brain base code, which we can get
    from [https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3](https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple utility functions, but we will be using and executing only
    a few of them in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to tell our function where to find the trained model and vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for the trained model and vocabulary have been added in the GitHub
    repository, and you can access it from this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3](https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The folder contains `checkpoint`, `word_counts.txt`, and the pre-trained model.
    We need to make sure that we use these files and avoid using other outdated files
    that might not be compatible with the latest version of TensorFlow. The `word_counts.txt`
    file contains a vocabulary list with the number of counts from our trained model,
    which our image caption generator is going to need.
  prefs: []
  type: TYPE_NORMAL
- en: Once these steps have been completed, we can look at our `main` function, which
    will generate the captions for us. The function can take an input as a string
    of input files (comma separated) or could be just one file that we want to process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The verbosity is set to `tf.logging.FATAL` out of the different logging levels
    available, as it will tell us whether something has gone really wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30533d4c-b33a-4aa2-b3fb-062758390569.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the initial part of the main code, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the verbosity level to `tf.logging.FATAL`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load our pre-trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the inference wrapper from our utility file provided by Google.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load our pre-trained model from the `checkpoint` path that we established in
    the previous cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the `finalize` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the vocabulary file again from the cell that we previously ran:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Pre-process the filenames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the `Glob` action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a list of filenames so you can know on which file the image caption
    generator is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a session. We need to use the `restore` function since we are using
    a pre-trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for these steps is included here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now move to the second half of the main code. Once the session has been
    restored, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load `caption_generator` from our model and the vocabulary stored in an object
    called `generator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a caption list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate the files and load them in the generator called `beam_search` to analyze
    the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the captions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate to create multiple captions with the iteration already set for the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Return `captionlist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Run the code to generate the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following code block for the complete code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the next code block, we will execute the code on sample stock photos from
    a `test` folder. The code will create a figure, show it, and then run the caption
    generator. We can then display the output using the `print` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code we use to select the image for computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run our first test image, `dog.jpeg`, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b760c014-a429-4f3d-8387-50867828e243.png)'
  prefs: []
  type: TYPE_IMG
- en: The result, `a woman and a dog are standing on the grass`, is a good caption
    for the image. Since all the three results are pretty similar, we can say that
    our model is working pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the result captions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a few examples to check our model. When we execute `football.jpeg`,
    we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9aaa6e6-3abc-4202-936d-33952a1612a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we clearly have American football going on in the image, and `a couple
    of men playing a game of football` is a very good result. However, the first result, `a
    couple of men playing a game of frisbee`, is not the desired output, nor is `a
    couple of men playing a game of soccer`. So, in this case, the second caption
    is generally going to be the best, but it is not always going to be perfect, depending
    on the log probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try one more example, `giraffes.jpeg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edfbad17-b3e0-4342-9c32-190f28865bec.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, we have an image of giraffes, and the first caption, `a group of giraffe
    standing next to each other`, seems to be correct, except for the grammar issue.
    The other two results are `a group of giraffes are standing in a field` and `a
    group of giraffe standing next to each other on a field`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at one more example, `headphones.jpeg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/589ac694-b233-4dac-b54c-c5d32e45a5a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we selected `headphones.jpeg`, but the results did not include headphones
    as an output. The result was `a woman holding a cell phone in her hand`, which
    is a good result. The second result, `a woman holding a cell phone up to her ear`, is
    technically incorrect, but these are some good captions overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take one last example, `ballons.jpeg`. When we run the image, we get
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c60032d5-a1ea-45a8-9d20-fd35908a06b6.png)'
  prefs: []
  type: TYPE_IMG
- en: The results we get for this image are `a woman standing on a beach flying a
    kite`, `a woman is flying a kite on the beach`, and `a young girl flying a kite
    on a beach`. So, the model got the `woman` or a `young girl`, but it got a `kite` instead
    of the balloon, even though "balloon" is in the vocabulary. So, we can infer that
    the model is not perfect, but it is impressive and can be included in your own
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Running the captioning code on Jupyter for multiple images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple images can also be added as an input string by separating the image
    path of the different images using commas. The execution time of a string of images
    will be greater than the times we've seen thus far.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of multiple input files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will not be displaying the images, so the output will include only the results.
    We can see that some of the results are better than others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c759ff9d-66fb-4db3-b062-de2575419d27.png)'
  prefs: []
  type: TYPE_IMG
- en: This wraps up running the pre-trained image captioning model. We will now cover
    training our model from scratch and running it on captioned images.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining the captioning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, now that we have seen image captioning code in action, we are going to retrain
    the image captioner on our own desired data. However, we need to know that it
    will be very time consuming and will need over 100 GB of hard drive space for
    computations if we want it to process in a reasonable time. Even with a good GPU,
    it may take a few days or a week to complete the computation. Since we are inclined
    toward implementing it and have the resources, let's start retraining the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Notebook, the first step is to download the pre-trained Inception model.
    The `webbrowser` module will make it easy to open the URL and to download the
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following will be the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/726b3308-e90e-4581-ac71-9b977bd35bef.png)'
  prefs: []
  type: TYPE_IMG
- en: When we select the code block and execute it, we might not be able to view the
    content on the web page, but we can click save on the dialog box to download the
    file. Unzip the file to get the inception v3 checkpoint file. We can use any of
    the unzipping utility available, but it is preferable to use 7-zip to get the
    Inception v3 checkpoint file and store it in `im2txt/data` in the project directory.
  prefs: []
  type: TYPE_NORMAL
- en: The `cd` command is used to navigate to the `im2txt/data` directory, where all
    our files are present. The `run_build_mscoco_data.py` Python script will grab
    and process all the image data and the pre-made caption data. This process might
    take over 100 GB of space and take over an hour to complete its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the computation is complete, we will see the three ZIP files in our project''s
    directory. We can unzip these files to get the following directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec271f43-3ccd-485b-83a0-1d7bf5354776.png)'
  prefs: []
  type: TYPE_IMG
- en: The training and validation JSON files are present in the `annotations` folder.
    The other directories have image training and validation data. Under the `train2014`
    directory, we will find a bunch of JPEG images corresponding to the training data.
    Similarly, the resources corresponding to the validation data will be present
    in the `val2014` folder. We can substitute our own images as well and edit the
    corresponding JSON file in the `annotations` folder. We will need many examples,
    as a handful examples will not provide effective results. There are over 80,000
    images in the `train2014` directory and processing them will require intensive
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we execute the `run_build_mscoco_data.py` command, we need to load the
    required modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We need to load `configuration` and `show_and_tell_model` in the `im2txt` folder
    along with TensorFlow. We can run the `cd ..` command to be in the right directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will be defining the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_file_pattern`: Defines the files pointing to the pre-trained Inception
    checkpoint, which will be generated from our model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_dir`: Contains the path where the training data was stored after we
    downloaded and unzipped it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_inception`: Set to `false` since we will not be training our Inception
    model for the initial run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_steps`: One million steps for our function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_every_n_steps`: Set `1` for our function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s define our `train` function. The steps performed in the `train`
    function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the `train` directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the graph file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the essential files
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the required variables for TensorFlow to start training the model to get
    the learning rate with the number of batches per epoch delay step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the layers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the saver for saving and restoring the model checkpoint
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call TensorFlow and do the training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is our `train` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define (but don''t run yet) our captioning training function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the training directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the TensorFlow graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the learning rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the training ops:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the `Saver` for saving and restoring model checkpoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Hit *Ctrl* + *Enter* for this code cell, since we can execute this now. After
    that, we need to call the `train` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will take a long time to process, even on a good GPU, but if we have the
    resources and still want to refine the model, run the following code to fine-tuning
    our `inception` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The model will run for three million steps. It actually continues from where
    the initial training completed its process and generate new checkpoints and refined
    models, before running the `train` function again. This will take even more time
    to process and provide a good result. We can do this in our Jupyter Notebook by
    correctly pointing our `checkpoint` path and the path for the vocabulary file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: After that, we can rerun code block 4 from the Jupyter Notebook file at [https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb](https://github.com/PacktPublishing/Computer-Vision-Projects-with-OpenCV-and-Python-3/blob/master/Chapter01/Section_1-Tensorflow_Image_Captioning.ipynb) to
    find `gen_caption`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to run the following code, as we did before in the *Running
    the captioning code on Jupyter* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Once the computation has been completed, we should get some good results. This
    wraps up image captioning with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to different image captioning methods. We
    learned about the Google Brain im2txt captioning model. While working on the project,
    we were able to run our pre-trained model on a Jupyter Notebook and analyze the
    model based on the results. In the last section of the chapter, we retrained our
    image captioning model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover reading license plates with OpenCV.
  prefs: []
  type: TYPE_NORMAL
