<html><head></head><body>
		<div id="_idContainer162">
			<h1 id="_idParaDest-121" class="chapter-number"><a id="_idTextAnchor125"/>9</h1>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor126"/>Evaluating Trained and Tested ML Models</h1>
			<p>In <a href="B19500_08.xhtml#_idTextAnchor118"><span class="No-Break"><em class="italic">Chapter 8</em></span></a> of this book, you built three ML models in Power BI. The models were trained and tested using FAA Wildlife Strike data and attempted to predict <span class="No-Break">the following:</span></p>
			<ul>
				<li>Whether wildlife striking an aircraft <span class="No-Break">caused damage</span></li>
				<li>The size of the wildlife that struck <span class="No-Break">the aircraft</span></li>
				<li>The height at which the wildlife <span class="No-Break">strike occurred</span></li>
			</ul>
			<p>This chapter will review the results of the testing that Power BI does after training the ML models. After reviewing the testing results, you will make changes to the training data with the intent of improving predictive capabilities of the ML models. At the end of this chapter, all three ML models will be ready to deploy and configure for use with Power BI. </p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor127"/>Technical requirements</h1>
			<p>There are a few key terms that you may want to research before reading this chapter if you are new to ML. The definitions given here are taken verbatim from the documentation at this <span class="No-Break">link: </span><a href="https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2#classification-metrics"><span class="No-Break">https://learn.microsoft.com/en-us/azure/machine-learning/how-to-understand-automated-ml?view=azureml-api-2#classification-metrics</span></a><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">The area under the curve</strong> (<strong class="bold">AUC</strong>): The <a id="_idIndexMarker447"/>AUC can be interpreted as the proportion of correctly classified samples. More precisely, the AUC is the probability that the classifier will rank a randomly chosen positive sample higher than a randomly chosen <span class="No-Break">negative sample.</span></li>
				<li><strong class="bold">Recall</strong>: Recall is<a id="_idIndexMarker448"/> the ability of a model to detect all <span class="No-Break">positive samples.</span></li>
				<li><strong class="bold">Precision</strong>: Precision is the <a id="_idIndexMarker449"/>ability of a model to avoid labeling negative samples <span class="No-Break">as positive.</span></li>
			</ul>
			<p>As with the previous chapters, you’ll need <span class="No-Break">the following:</span></p>
			<ul>
				<li>FAA Wildlife Strike data files from either the FAA website or the Packt <span class="No-Break">GitHub site</span></li>
				<li>A Power BI <span class="No-Break">Pro license</span></li>
				<li>One of the following Power BI licensing options for access to Power <span class="No-Break">BI dataflows:</span><ul><li>Power <span class="No-Break">BI Premium</span></li><li>Power BI Premium <span class="No-Break">Per User</span></li></ul></li>
				<li>One of the following options for getting data into the Power BI <span class="No-Break">cloud service:</span><ul><li>Microsoft OneDrive (with connectivity to the Power BI <span class="No-Break">cloud service)</span></li><li>Microsoft Access + Power <span class="No-Break">BI Gateway</span></li><li>Azure Data Lake (with connectivity to the Power BI <span class="No-Break">cloud service)</span></li></ul></li>
			</ul>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor128"/>Evaluating test results for the Predict Damage ML model in Power BI</h1>
			<p>After <a id="_idIndexMarker450"/>the three ML models have<a id="_idIndexMarker451"/> completed training, you can take a look at the testing results for each of those models using a pre-built report in Power BI. The training report will provide metrics to help you determine whether the models have some feedback about the value of the predictions. You start with <strong class="bold">Predict Damage ML Model</strong>, which is a binary prediction ML model. While in your workspace, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Click on the <strong class="bold">Predict Damage ML </strong><span class="No-Break"><strong class="bold">Model</strong></span><span class="No-Break"> dataflow.</span></li>
				<li>On the ribbon, select <strong class="bold">Machine </strong><span class="No-Break"><strong class="bold">learning models</strong></span><span class="No-Break">.</span></li>
				<li>Under the <strong class="bold">ACTIONS</strong> column, click the clipboard to access <strong class="bold">View training report</strong>, per the <span class="No-Break">following screenshot:</span></li>
			</ol>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B19500_09_001.jpg" alt="Figure 9.1 – Navigation for the Predict Damage ML Model training report"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Navigation for the Predict Damage ML Model training report</p>
			<ol>
				<li value="4">The<a id="_idIndexMarker452"/> report should<a id="_idIndexMarker453"/> open to look like the <span class="No-Break">following figure:</span></li>
			</ol>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B19500_09_002.jpg" alt="Figure 9.2 – The training report for Predict Damage ML Model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – The training report for Predict Damage ML Model</p>
			<p>Note that your metrics on this page may differ due to random sampling of the testing data and any changes to the scope of source data used for training and testing. Also, Power BI <a id="_idIndexMarker454"/>ML will randomly split <a id="_idIndexMarker455"/>the data into testing and training subsets, which may be different every time <span class="No-Break">it runs.</span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor129"/>Model performance for Predict Damage ML Model</h2>
			<p>The <strong class="bold">Predict Damage ML Model</strong> testing has yielded some interesting results. In the example provided <a id="_idIndexMarker456"/>above, you can see that the model <a id="_idIndexMarker457"/>performance, or AUC, is listed at 91%. The closer that the AUC is to 100%, the better an ML model is at overall correct predictions. Your first inclination might be to claim success, but there are some additional details <span class="No-Break">to consider.</span></p>
			<p>For a binary prediction ML model, the prediction is for either a value of 1 (yes) or 0 (no). When predicting whether damage happened due to a wildlife strike, there are four possible outcomes for each row of data and comparing it to the <span class="No-Break">real result:</span></p>
			<table id="table001-7" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Prediction</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">What </strong><span class="No-Break"><strong class="bold">really happened</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Outcome</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Damage happened</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Damage <span class="No-Break">really happened</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">True positive</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Damage happened</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Damage <span class="No-Break">didn’t happen</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">False positive</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Damage <span class="No-Break">didn’t happen</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Damage <span class="No-Break">really happened</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">False negative</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Damage <span class="No-Break">didn’t happen</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Damage <span class="No-Break">didn’t happen</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">True negative</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Four possible outcomes of testing the ML model versus reality</p>
			<p>As seen in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, there is a sliding bar filter in the bottom-right portion of the report called <strong class="bold">Probability Threshold</strong>. The probability threshold is a value assigned to each prediction between zero and one to indicate the certainty of the prediction. A probability score of 99 could be interpreted as “<em class="italic">The ML model is 99% certain that this incident caused damage</em>.” Can you rely on the certainty of the ML model, and does that number reflect reality? The testing results can help you <span class="No-Break">find out!</span></p>
			<p>You move <strong class="bold">Probability Threshold</strong> to <strong class="source-inline">0.50</strong> and view the results. Notice that other values on the <span class="No-Break">page change:</span></p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B19500_09_004.jpg" alt="Figure 9.4 – Model performance report with Probability Threshold set to 0.50"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Model performance report with Probability Threshold set to 0.50</p>
			<p>Starting with<a id="_idIndexMarker458"/> the grid having four boxes, you <a id="_idIndexMarker459"/>note that 484 incidents were correctly predicted to have had damage. However, 69 incidents that had real damage were missed by the ML model. 484/(484 + 69) is about an <strong class="bold">88%</strong> success rate for correctly flagging incidents with damage. This is the <strong class="bold">Recall</strong> value on the report. Also, 1.09K incidents were incorrectly predicted to have caused damage when they did not. <strong class="bold">Precision</strong> indicates that with <strong class="bold">Probability Threshold</strong> set to <strong class="source-inline">0.50</strong>, only <strong class="bold">30%</strong> of the incidents that are flagged as causing damage will actually have <span class="No-Break">caused damage.</span></p>
			<p>With <strong class="bold">Probability Threshold</strong> set to <strong class="source-inline">0.80</strong>, the <span class="No-Break">metrics change:</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B19500_09_005.jpg" alt="Figure 9.5 – The model performance report with Probability Threshold set to 0.80"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – The model performance report with Probability Threshold set to 0.80</p>
			<p>With <strong class="bold">Probability Threshold</strong> set to <strong class="source-inline">0.80</strong>, <strong class="bold">Precision</strong> increases to <strong class="bold">49%</strong>. 49% sounds better, but <a id="_idIndexMarker460"/>there’s a catch! Yes, more <a id="_idIndexMarker461"/>incidents that were predicted to cause damage did in fact cause damage, but only 362 incidents were flagged out of 551 damaging incidents! Setting <strong class="bold">Probability Threshold</strong> at <strong class="source-inline">0.80</strong> might eliminate false positives but 33% (1 – <strong class="bold">Recall</strong> of 0.67) of actual real damage events would <span class="No-Break">be missed.</span></p>
			<p>In summary, <strong class="bold">Probability Threshold</strong> functions as a cut-off value that can impact the four possible outcomes of this binary prediction ML model, as illustrated in the <span class="No-Break">following charts.</span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B19500_09_006.jpg" alt="Figure 9.6 – Impact of Probability Threshold on results versus real tested results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Impact of Probability Threshold on results versus real tested results</p>
			<p>Let’s break down <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<ul>
				<li>The top-left chart <a id="_idIndexMarker462"/>shows the real results of <a id="_idIndexMarker463"/>each incident with <strong class="bold">ML Prediction Score</strong> on the <span class="No-Break"><em class="italic">x</em></span><span class="No-Break"> axis</span></li>
				<li>The top-right <a id="_idIndexMarker464"/>chart <a id="_idIndexMarker465"/>shows the<a id="_idIndexMarker466"/> impact of<a id="_idIndexMarker467"/> setting <strong class="bold">Probability Threshold</strong> to <strong class="source-inline">0.50</strong> on <strong class="bold">False Negative</strong> (<strong class="bold">FN</strong>), <strong class="bold">False Positive</strong> (<strong class="bold">FP</strong>), <strong class="bold">True Negative</strong> (<strong class="bold">TN</strong>), and <strong class="bold">True </strong><span class="No-Break"><strong class="bold">Positive</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">TP</strong></span><span class="No-Break">)</span></li>
				<li>The bottom-left chart shows the impact of setting <strong class="bold">Probability Threshold</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.40</strong></span></li>
				<li>The bottom-right chart shows the impact of setting <strong class="bold">Probability Threshold</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.70</strong></span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em> sums up the impact of changing <strong class="bold">Probability Threshold</strong> for this ML model in <span class="No-Break">practical terms:</span></p>
			<table id="table002-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Testing outcome</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Increase </strong><span class="No-Break"><strong class="bold">Probability Threshold</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Decrease </strong><span class="No-Break"><strong class="bold">Probability Threshold</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">True Positive</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fewer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">More</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">True Negative</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">More</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fewer</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">False Positive</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fewer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">More</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">False Negative</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">More</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fewer</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Impact of Probability Threshold on testing outcomes</p>
			<p>You’ll<a id="_idIndexMarker468"/> need to go back to your stakeholders <a id="_idIndexMarker469"/>to ask for feedback about the importance of the different possible outcomes in <em class="italic">Figures 9.6</em> and <em class="italic">9.7</em>. Verbalizing this impact to non-technical stakeholders can be a challenge. The optimal <strong class="bold">Probability Threshold</strong> will depend upon the requirements of your stakeholders. You’ll need to ask something like <span class="No-Break">the following:</span></p>
			<p><em class="italic">Do you want to flag incidents more frequently to capture as many real incidents with damage as possible, at the expense of having more incorrectly flagged incidents that did not cause damage? Or do you want fewer flagged incidents, with fewer incorrectly flagged incidents, but at the expense of not flagging and missing some incidents </em><span class="No-Break"><em class="italic">with damage?</em></span></p>
			<p>The screen has been split in half to improve visibility. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.8</em> shows the left half of <span class="No-Break">the screen:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B19500_09_008.jpg" alt="Figure 9.8 – Top predictors for the Predict Damage ML binary prediction model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Top predictors for the Predict Damage ML binary prediction model</p>
			<p>In the upper-right portion of the report page (see <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.9</em>), you click a yellow box labeled <strong class="bold">See top predictors</strong>. A list of the top predictive features is shown, and you click on <strong class="bold">Size </strong><span class="No-Break"><strong class="bold">is Small</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B19500_09_009.jpg" alt="Figure 9.9 – Top predictors for the Predict Damage ML binary prediction model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Top predictors for the Predict Damage ML binary prediction model</p>
			<p>You see <a id="_idIndexMarker470"/>that when you click on a predictor (feature) value, a<a id="_idIndexMarker471"/> chart pops up on the right to visualize that feature. You can also hover over bars on these charts for greater detail. For <strong class="bold">Size is Small</strong>, the chart shows that when <strong class="bold">Size is Large</strong>, damage happened about 45% of the time. When <strong class="bold">Size is Small</strong>, damage happened about 2.5% of the time. That’s a big difference! You can click on each of the top predictors and take notes on <span class="No-Break">the findings.</span></p>
			<p>At the bottom of the <strong class="bold">MODEL PERFORMANCE</strong> page, you find a <strong class="bold">Cost-Benefit Analysis</strong> chart with filters, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B19500_09_010.jpg" alt="Figure 9.10 – Cost-Benefit Analysis for Predict Damage ML"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – Cost-Benefit Analysis for Predict Damage ML</p>
			<p>The best way to summarize this chart, in practical terms, is that it allows you to determine the impact of different <strong class="bold">Probability Threshold</strong> settings. If the total count of predictions (<strong class="bold">Population Size</strong>) is set to <strong class="source-inline">10,000</strong>, the cost of a prediction (<strong class="bold">Unit Cost</strong>) is set to <strong class="source-inline">1</strong>, and the relative benefit of a correct prediction is set to <strong class="source-inline">2</strong>, then the maximum profit per the testing would be <strong class="source-inline">202.09</strong> if the threshold were set to <strong class="source-inline">0.94</strong>, and only <strong class="source-inline">4.73%</strong> of the population would be targeted. With this particular use case, the setting of <strong class="bold">Probability Threshold</strong> will probably require a conversation with your stakeholders. While an<a id="_idIndexMarker472"/> interesting discussion point, there <a id="_idIndexMarker473"/>are other factors to consider for the implications of wildlife striking aircraft, such as passenger safety, impact on wildlife, end user use of the predictions, <span class="No-Break">and more.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor130"/>Accuracy report for Predict Damage ML</h2>
			<p>Next, you move to the<a id="_idIndexMarker474"/> next page of the <a id="_idIndexMarker475"/>training report, named <strong class="bold">ACCURACY REPORT</strong>, as you can see in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B19500_09_011.jpg" alt="Figure 9.11 – Accuracy report for the Predict Damage ML binary prediction model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – Accuracy report for the Predict Damage ML binary prediction model</p>
			<p>The top of the accuracy <a id="_idIndexMarker476"/>report provides detailed <a id="_idIndexMarker477"/>explanations for <strong class="bold">Cumulative Gains Chart</strong> and <strong class="bold">ROC Curve</strong> breakdowns. Scroll down the page to view those charts. <strong class="bold">Cumulative Gains Chart</strong> is on the left and shows how the ML model performs compared to a perfect model and random guessing. As you hover over the line and move to the right, you can see the performance as <strong class="bold">Probability </strong><span class="No-Break"><strong class="bold">Threshold</strong></span><span class="No-Break"> decreases:</span></p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B19500_09_012.jpg" alt="Figure 9.12 – Cumulative Gains Chart (left) displays testing results against a perfect model and random guesses"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – Cumulative Gains Chart (left) displays testing results against a perfect model and random guesses</p>
			<p><strong class="bold">ROC Curve</strong> on the <a id="_idIndexMarker478"/>right side of the page shows how well the <a id="_idIndexMarker479"/>model predicts positives as positives (true positive) and negatives as negatives (true negative). A curve elevated upward and to the left indicates <span class="No-Break">good performance:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B19500_09_013.jpg" alt="Figure 9.13 – ROC Curve (right) visualizes the ML model’s ability to distinguish the target outcome"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – ROC Curve (right) visualizes the ML model’s ability to distinguish the target outcome</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor131"/>Training Details for Predict Damage ML</h2>
			<p>Finally, you<a id="_idIndexMarker480"/> move to the last page of the report, <a id="_idIndexMarker481"/>named <strong class="bold">TRAINING DETAILS</strong>. The top of this page displays details about the testing such as the number of rows sampled, the number of rows used for testing, the type of model that was selected as having the best results, and the number of iterations run to determine the best-fit model. The page can be seen in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B19500_09_014.jpg" alt=" Figure 9.14 – TRAINING DETAILS for the Predict Damage ML binary prediction ML model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 9.14 – TRAINING DETAILS for the Predict Damage ML binary prediction ML model</p>
			<p>You'll also notice the <strong class="bold">Model quality over iterations</strong> chart, showing ML model quality comparisons during <a id="_idIndexMarker482"/>the iterative training and testing. Scrolling <a id="_idIndexMarker483"/>down the page, you can view details about the ML model such as the features in the model, the data types and imputation of those features, and the parameters that were used to create the model. Here’s <span class="No-Break">a screenshot:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B19500_09_015.jpg" alt="Figure 9.15 – Details about the ML model including features and parameters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.15 – Details about the ML model including features and parameters</p>
			<p>Scrolling down to the <a id="_idIndexMarker484"/>bottom third of the <strong class="bold">TRAINING DETAILS</strong> page, you<a id="_idIndexMarker485"/> will see a donut chart of the different ML algorithms used as part of an ensemble model. Hovering over an algorithm, you can see details about how it has been used. With Power BI, you use a SaaS ML tool, so there isn’t a need to dive deeper into these details. If a data science team wants to extend your findings by building custom ML models in a tool such as Azure ML, this information might be of value to them as they plan their <span class="No-Break">follow-up project:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B19500_09_016.jpg" alt="Figure 9.16 – Ensemble machine learning model algorithms for the Predict Damage ML model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.16 – Ensemble machine learning model algorithms for the Predict Damage ML model</p>
			<p>Looking back at <a id="_idIndexMarker486"/>the process to create the <strong class="bold">Predict Damage ML</strong> model, you <a id="_idIndexMarker487"/>included a long list of features that you considered possible candidates for having predictive capabilities. In <a href="B19500_10.xhtml#_idTextAnchor139"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, you will revisit this ML model to retrain it with a more succinct and carefully chosen list of features before deploying the ML model. Now, you are ready to move on to the next ML model for <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor132"/>Evaluating test results for Predict Size ML Model in  Power BI</h1>
			<p>Your next ML model<a id="_idIndexMarker488"/> to review is a classification<a id="_idIndexMarker489"/> model for predicting the size of wildlife that struck an aircraft. These predictions don’t necessarily indicate the size of the actual animal. For example, a large flock of smaller birds might also be considered a large impact. Predicting these values could help understand the likelihood of incidents that are perceived to be <span class="No-Break">more severe.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor133"/>Model performance for Predict Size ML</h2>
			<p>Moving on to the test<a id="_idIndexMarker490"/> results for your ML model to predict the size<a id="_idIndexMarker491"/> of an animal or animals that struck an aircraft, you follow similar steps as in the previous section to open the report. From your Power BI workspace, do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Click on the <strong class="bold">Predict Size ML </strong><span class="No-Break"><strong class="bold">Model</strong></span><span class="No-Break"> dataflow.</span></li>
				<li>On the ribbon, select <strong class="bold">Machine </strong><span class="No-Break"><strong class="bold">learning models</strong></span><span class="No-Break">.</span></li>
				<li>Under the <strong class="bold">ACTIONS</strong> column, click the clipboard to access <strong class="bold">View </strong><span class="No-Break"><strong class="bold">training report</strong></span><span class="No-Break">.</span></li>
				<li>The report should look like the following figure, and on the bar chart, you can click on the <span class="No-Break"><strong class="bold">Small</strong></span><span class="No-Break"> class:</span></li>
			</ol>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B19500_09_017.jpg" alt="Figure 9.17 – The training report for the Predict Size ML classification model with the Small class selected"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.17 – The training report for the Predict Size ML classification model with the Small class selected</p>
			<p>First, you notice that this report looks slightly different since it is for a different type of ML model, a classification model. The AUC, at the top right of the report page, is <strong class="source-inline">60%</strong>. As a general rule of thumb, an AUC under 70% is not very good. An AUC of 50% generally represents random guessing, and 60% is only slightly above that value. You’ll need to dive deeper into the testing results to find opportunities <span class="No-Break">for improvement.</span></p>
			<p>The <strong class="bold">Small</strong> class (you clicked on in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.16</em>) had a <strong class="bold">Precision</strong> rating of <strong class="bold">88%</strong>, which means that when a prediction of <strong class="bold">Small</strong> was made, it turned out to be true 88% of the time. <strong class="bold">Recall</strong> for <strong class="bold">Small</strong> was only <strong class="bold">65%</strong>, which means that<a id="_idIndexMarker492"/> only 65% of actual small <a id="_idIndexMarker493"/>incidents <span class="No-Break">were captured.</span></p>
			<p>Now, click on the <span class="No-Break"><strong class="bold">Medium</strong></span><span class="No-Break"> class:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B19500_09_018.jpg" alt="Figure 9.18 – Training report for the Predict Size ML classification model with the Medium class selected"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.18 – Training report for the Predict Size ML classification model with the Medium class selected</p>
			<p>The results for predicting <strong class="bold">Medium</strong> are not very good. With a <strong class="bold">Precision</strong> rating of <strong class="bold">25%</strong>, most of the predictions for <strong class="bold">Medium</strong> turned out to be wrong. Adding to the disappointment, only <strong class="bold">36%</strong> of actual medium strike incidents were captured by the <span class="No-Break"><strong class="bold">Medium</strong></span><span class="No-Break"> prediction.</span></p>
			<p>Let's move on to the <span class="No-Break"><strong class="bold">Large</strong></span><span class="No-Break"> class:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B19500_09_019.jpg" alt="Figure 9.19 – The training report for the Predict Size ML classification model with the Large class selected"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.19 – The training report for the Predict Size ML classification model with the Large class selected</p>
			<p>The <strong class="bold">Large</strong> class had a <strong class="bold">Precision</strong> rating of <strong class="bold">22%</strong>, indicating that a prediction of <strong class="bold">Large</strong> is only correct <a id="_idIndexMarker494"/>about 1 out of 5 times. <strong class="bold">66%</strong> of <a id="_idIndexMarker495"/>actual large-sized incidents were captured in those predictions. In summary, a prediction of <strong class="bold">Large</strong> is usually inaccurate but about two-thirds of actual large events are captured in <span class="No-Break">that prediction.</span></p>
			<p>Scrolling to the bottom of the <strong class="bold">MODEL PERFORMANCE</strong> page, you can click on the different classes that were predicted to see the top predictors. Within the <strong class="bold">Large</strong> class, you click on the bar for <strong class="bold">Indicated Damage</strong> to pop up a chart on the right, which shows details about <span class="No-Break">that feature:</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B19500_09_020.jpg" alt="Figure 9.20 – Top Predictors for the Large class with Indicated Damage details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.20 – Top Predictors for the Large class with Indicated Damage details</p>
			<p>When the <a id="_idIndexMarker496"/>class is <strong class="bold">Large</strong>, the features on the left<a id="_idIndexMarker497"/> side chart of <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.18</em> were determined to influence that class. You’ve clicked on the <strong class="bold">Indicated Damage</strong> feature and can see that when that value is <strong class="bold">1</strong>, the class will be <strong class="bold">Large</strong> 35% of the time. When set to <strong class="bold">0</strong>, a classification of <strong class="bold">Large</strong> occurs less than 5% of the time. You can click through the different classes (<strong class="bold">Large</strong>, <strong class="bold">Medium</strong>, and <strong class="bold">Small</strong>) to view the features that were top influencers and take notes for your next iteration of the <span class="No-Break">ML model.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>Training details for Predict Size ML</h2>
			<p>Moving on<a id="_idIndexMarker498"/> to the <strong class="bold">TRAINING DETAILS</strong> page for <strong class="bold">Predict Size ML</strong>, you’ll see a<a id="_idIndexMarker499"/> similar page to that for <strong class="bold">Predict </strong><span class="No-Break"><strong class="bold">Damage ML</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B19500_09_021.jpg" alt="Figure 9.21 – TRAINING DETAILS includes details about training the ML model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.21 – TRAINING DETAILS includes details about training the ML model</p>
			<p>Interestingly, the<a id="_idIndexMarker500"/> quality of the ML model declined with an<a id="_idIndexMarker501"/> increase in the number of iterations. With machine learning, more data is not always better, and greater complexity does not always yield better results. As a novice with these tools, sometimes, trial and error is the best way to familiarize yourself with <span class="No-Break">these concepts.</span></p>
			<p>Scrolling to the middle of the <strong class="bold">TRAINING DETAILS</strong> page, you can view details about the features used in the ML model and the final parameters. Again, this is a similar page to the <strong class="bold">Predict Damage </strong><span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break"> report:</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B19500_09_022.jpg" alt="Figure 9.22 – Features and parameters for the Predict Size ML model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.22 – Features and parameters for the Predict Size ML model</p>
			<p>You can also scroll down <a id="_idIndexMarker502"/>to the bottom third of the page and view <a id="_idIndexMarker503"/>the <strong class="bold">Ensemble machine learning</strong> information. As stated for <strong class="bold">Predict Damage ML</strong>, this information is interesting but mostly useful for data scientists who might want to extend the findings of this project into a tool such as Azure ML. You’ll revisit this ML model in the next chapter, but for now, you can move on to the <strong class="bold">Predict Height </strong><span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break"> model.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor135"/>Evaluating test results for the Predict Height ML model in Power BI</h1>
			<p>Finally, you<a id="_idIndexMarker504"/> review the test results for the <strong class="bold">Predict Height ML</strong> model. This was a regression model attempting to <a id="_idIndexMarker505"/>predict the height at which wildlife strikes happened to aircraft. This model does not predict a yes/no answer or a categorical value, but rather a numeric value representing the height in feet from <span class="No-Break">the ground.</span></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor136"/>Model performance for Predict Height ML</h2>
			<p>Start by<a id="_idIndexMarker506"/> navigating to the <strong class="bold">MODEL PERFORMANCE</strong> page of <a id="_idIndexMarker507"/>the <span class="No-Break">training report:</span></p>
			<ol>
				<li>Click on the <strong class="bold">Predict Height ML </strong><span class="No-Break"><strong class="bold">Model</strong></span><span class="No-Break"> dataflow.</span></li>
				<li>On the ribbon, select <strong class="bold">Machine </strong><span class="No-Break"><strong class="bold">learning models</strong></span><span class="No-Break">.</span></li>
				<li>Under the <strong class="bold">ACTIONS</strong> column, click the clipboard to access <strong class="bold">View </strong><span class="No-Break"><strong class="bold">training report</strong></span><span class="No-Break">.</span></li>
				<li>The report should look like the <span class="No-Break">following figure:</span></li>
			</ol>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B19500_09_023.jpg" alt="Figure 9.23 – Performance for the Predict Height ML regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.23 – Performance for the Predict Height ML regression model</p>
			<p>First, you notice at the top of the page that <strong class="bold">Model Performance</strong> is <strong class="bold">80%</strong>. With a regression model, the numeric differences between predicted and real results are represented by this value. The features provide predictability, but you’d like to get that number even higher for <span class="No-Break">better results.</span></p>
			<p>The chart on the left side of the page, <strong class="bold">Predicted vs Actual Height</strong> plots <strong class="bold">Actual value</strong> versus <strong class="bold">Predicted value</strong> for the height at which an impact occurred. Variation is expected due to different aircraft models, weather, species, and many other factors. Perfect predictability would display the dots on the line separating the red and blue portions of the chart. You can see that the regression line generally follows <span class="No-Break">through results.</span></p>
			<p>On the right side of the page in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.22</em>, the chart for <strong class="bold">Residual error by Height</strong> shows the average error for different values that were tested. A value of -5% would mean that the prediction is usually 5% too low for a range of values. You notice that the first bubble on the <em class="italic">x</em> axis appears close to 0%. When you hover over it, you see the values, and you can<a id="_idIndexMarker508"/> click it to filter the chart on <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker509"/></span><span class="No-Break"> left:</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B19500_09_024.jpg" alt="Figure 9.24 – The lowest range in height had an extremely high number of residual errors"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.24 – The lowest range in height had an extremely high number of residual errors</p>
			<p>Looking at the <strong class="bold">Predicted vs Actual Height</strong> chart, you can see that a low height above the ground had a residual error <span class="No-Break">of </span><span class="No-Break"><strong class="bold">1%</strong></span><span class="No-Break">.</span></p>
			<p>If you click on the third bubble from the left, a height of 6,500-7,800 feet, the average residual error is <span class="No-Break">now </span><span class="No-Break"><strong class="bold">-40%</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B19500_09_025.jpg" alt="Figure 9.25 – The bins covering heights of 3,200-4,500 feet have a lower average number of residual errors"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.25 – The bins covering heights of 3,200-4,500 feet have a lower average number of residual errors</p>
			<p>On the <strong class="bold">Predicted vs Actual Height</strong> chart for the third bin, notice that many actual values are<a id="_idIndexMarker510"/> reported at a height of 6,500, 7,000, and 7,500 when you click on the bubble to filter the chart on the left. Looking into reasons<a id="_idIndexMarker511"/> for this conformity might help you understand the nuances in the source data. Are pilots estimating the height of a wildlife strike incident? Are these common stable altitudes for aircraft flight plans when they are not increasing or decreasing height? Is the conformity just a coincidence? The root cause of this pattern can only be discovered with <span class="No-Break">additional investigation.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor137"/>Training details for Predict Height ML</h2>
			<p>Moving on to the <a id="_idIndexMarker512"/>final page of the training report for <strong class="bold">Predict Height ML,</strong> named <strong class="bold">TRAINING DETAILS</strong>, you see a similar report structure to the previous<a id="_idIndexMarker513"/> two ML model training reports. The top of the page displays the number of rows sampled, the number of rows used for training, the final model used, and the number of iterations that <span class="No-Break">were run:</span></p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B19500_09_026.jpg" alt="Figure 9.26 – TRAINING DETAILS for the Predict Height ML regression model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.26 – TRAINING DETAILS for the Predict Height ML regression model</p>
			<p>Scrolling down the page, you will again see similar charts and information about the features used, the <a id="_idIndexMarker514"/>parameters, and the algorithms used by <a id="_idIndexMarker515"/>automated ML in <span class="No-Break">Power BI.</span></p>
			<p>Having completed your review of the testing and training report for all three ML models, you are ready to move on to the next chapter of <span class="No-Break">this book.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor138"/>Summary</h1>
			<p>This chapter took a deep dive into the training and testing reports for your <strong class="bold">Predict Damage ML</strong>, <strong class="bold">Predict Size ML</strong>, and <strong class="bold">Predict Height ML</strong> models. In doing so, you reviewed the reports for all three types of ML models in Power BI: binary prediction, classification, and regression. You evaluated how well each of these models made predictions by reviewing the testing data in Power BI. You also explored lists of features that were <span class="No-Break">highly predictive.</span></p>
			<p>In the next chapter, you will modify the filter criteria and features selected for your ML models, with the goal of improving the predictive capabilities. Iterative training and testing are the best way to improve your ML models, and this process will help you prepare for your own Power BI ML projects beyond the scope of <span class="No-Break">this book.</span></p>
		</div>
	</body></html>