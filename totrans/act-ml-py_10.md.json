["```py\npip install scikit-learn\npip install modAL-python\n```", "```py\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils import shuffle\nimport numpy as np\nimport random\nfrom modAL.models import ActiveLearner, Committee\nfrom sklearn.ensemble import RandomForestClassifier\nfrom modAL.uncertainty import uncertainty_sampling\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom modAL.disagreement import vote_entropy_sampling\n```", "```py\nX = np.array([[34, 20000], [42, 30000], [23, 25000], [32, 45000], \n    [38, 30000]])\n```", "```py\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n```", "```py\nclusters = kmeans.predict(X)\n```", "```py\nX_unlabeled = np.array([[28, 22000], [45, 55000], [37, 35000], \n    [50, 48000], [29, 27000], [41, 32000]])\n```", "```py\nclassifier = LogisticRegression()\nclassifier.fit(X, clusters)\n```", "```py\ndef obtain_labels(data):\n    return np.random.choice([0, 1], size=len(data))\n```", "```py\nnum_iterations = 10\nnum_to_label = 2\n```", "```py\nfor iteration in range(num_iterations):\n    if len(X_unlabeled) == 0:\n        break  # No more data to label\n    # Predict on unlabeled data\n    predictions = classifier.predict_proba(X_unlabeled)\n    uncertainty = np.max(predictions, axis=1)\n    # Select num_to_label instances with least confidence\n    uncertain_indices = np.argsort(uncertainty)[:num_to_label]\n    # Obtain labels for these instances\n    new_labels = obtain_labels(X_unlabeled[uncertain_indices])\n    # Update our dataset\n    X = np.vstack([X, X_unlabeled[uncertain_indices]])\n    clusters = np.hstack([clusters, new_labels])\n    # Re-train classifier and KMeans\n    classifier.fit(X, clusters)\n    kmeans.fit(X)\n    print(f\"Iteration {iteration+1}, Labeled Data: {\n        X_unlabeled[uncertain_indices]} with Labels: {new_labels}\")\n    # Remove labeled instances from unlabeled data\n    X_unlabeled = np.delete(X_unlabeled, uncertain_indices, axis=0)\n    # Shuffle unlabeled data to avoid any order bias\n    X_unlabeled = shuffle(X_unlabeled)\n```", "```py\nIteration 1, Labeled Data: [[45 55000] [29 27000]] with Labels: [0 1]\nIteration 2, Labeled Data: [[37 35000] [28 22000]] with Labels: [1 1]\nIteration 3, Labeled Data: [[41 32000] [50 48000]] with Labels: [0 0]\n```", "```py\ndef load_data():\n    # Define the transformation\n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert images to PyTorch tensors\n    ])\n        # Load the CIFAR10 dataset\n    dataset = CIFAR10(root='data', train=True, download=True, \n        transform=transform)\n    # Load all data into memory (for small datasets)\n    dataloader = DataLoader(dataset, batch_size=len(dataset), \n        shuffle=False)\n    data_iter = iter(dataloader)\n    images, labels = next(data_iter)\n    # Convert images and labels to numpy arrays\n    X_all = images.numpy()\n    y_all = np.array(labels)\n    # Convert images from 3D to 1D (batch_size, 3, 32, 32) -> (batch_size, 3072) for RandomForest\n    X_all = X_all.reshape(X_all.shape[0], -1)\n    # Map numerical labels to string labels\n    class_names = dataset.classes\n    y_all = np.array([class_names[label] for label in y_all])\n    return X_all, y_all\n```", "```py\nX_initial, X_unlabeled, y_initial, _ = train_test_split(X_all, y_all, \n    test_size=0.75, random_state=42)\n```", "```py\nlearner = ActiveLearner(\n    estimator=RandomForestClassifier(),\n    query_strategy=uncertainty_sampling,\n    X_training=X_initial_flat, y_training=y_initial\n)\nActiveLearner object is created. This learner uses RandomForestClassifier as its estimator. RandomForest is a popular ensemble learning method for classification, which operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. The query strategy is set to X_initial_flat initial training data and y_training labels are provided to the learner.\n```", "```py\nfor i in range(5):\n    query_idx, _ = learner.query(X_unlabeled)\n    actual_label = y_all[query_idx[0]] \n    print(f\"Selected unlabeled query is sample number {query_idx[0]}. Actual label: {actual_label}\")\n    learner.teach(X_unlabeled[query_idx].reshape(1, -1), actual_label.reshape(1,))\n    X_unlabeled = np.delete(X_unlabeled, query_idx, axis=0)\n    y_all = np.delete(y_all, query_idx)\n```", "```py\nSelected unlabeled query is sample number 3100\\. Actual label: cat\nSelected unlabeled query is sample number 7393\\. Actual label: deer\nSelected unlabeled query is sample number 4728\\. Actual label: horse\nSelected unlabeled query is sample number 447\\. Actual label: deer\nSelected unlabeled query is sample number 17968\\. Actual label: bird\n```", "```py\nX, y = load_iris(return_X_y=True)\nX_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n    X, y, test_size=0.9, random_state=42)\n```", "```py\nn_learners = 20\nlearners = [ActiveLearner(\n        estimator=RandomForestClassifier(), X_training=X_labeled, \\\n        y_training=y_labeled\n    ) for _ in range(n_learners)]\ncommittee = Committee(learner_list=learners, \n    query_strategy=vote_entropy_sampling)\n```", "```py\nn_queries = 5\nfor idx in range(n_queries):\n    query_idx, query_instance = committee.query(X_unlabeled)\n    print(f\"\\nSelected unlabeled query is sample number {query_idx}. We simulate labeling this sample which is labeled as: {y_unlabeled[query_idx]}\")\n    committee.teach(X_unlabeled[query_idx], y_unlabeled[query_idx])\n    # Remove the queried instance from the pool\n    X_unlabeled = np.delete(X_unlabeled, query_idx, axis=0)\n    y_unlabeled = np.delete(y_unlabeled, query_idx)\n    print(f\"Number of unlabeled samples is {len(X_unlabeled)}\")\n    # Calculate and print committee score\n    committee_score = committee.score(X, y)\n    print(f\"Iteration {idx+1}, Committee Score: {committee_score}\")\n```", "```py\nSelected unlabeled query is sample number [8]. We simulate labeling this sample which is labeled as: [0]\nNumber of unlabeled samples is 129\nIteration 1, Committee Score: 0.96\nSelected unlabeled query is sample number [125]. We simulate labeling this sample which is labeled as: [2]\nNumber of unlabeled samples is 128\nIteration 2, Committee Score: 0.9466666666666667\nSelected unlabeled query is sample number [42]. We simulate labeling this sample which is labeled as: [2]\nNumber of unlabeled samples is 127\nIteration 3, Committee Score: 0.9466666666666667\nSelected unlabeled query is sample number [47]. We simulate labeling this sample which is labeled as: [1]\nNumber of unlabeled samples is 126\nIteration 4, Committee Score: 0.9733333333333334\nSelected unlabeled query is sample number [95]. We simulate labeling this sample which is labeled as: [1]\nNumber of unlabeled samples is 125\nIteration 5, Committee Score: 0.9733333333333334\n```"]