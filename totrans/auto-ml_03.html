<html><head></head><body>
		<div id="_idContainer019">
			<h1 id="_idParaDest-47"><em class="italic"><a id="_idTextAnchor049"/>Chapter 2</em>: Automated Machine Learning, Algorithms, and Techniques</h1>
			<p class="author-quote">"Machine intelligence is the last invention that humanity will ever need to make." </p>
			<p class="author-quote">– Nick Bostrom</p>
			<p class="author-quote">"The key to artificial intelligence has always been the representation." </p>
			<p class="author-quote">– Jeff Hawkins</p>
			<p class="author-quote">"By far, the greatest danger of artificial intelligence is that people conclude too early that they understand it." </p>
			<p class="author-quote">– Eliezer Yudkowsky</p>
			<p>Automating the automation sounds like one of those wonderful Zen meta ideas, but learning to learn is not without its challenges. In the last chapter, we covered the <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) development life cycle, and defined automated ML, with a brief overview of how it works. </p>
			<p>In this chapter, we will explore under-the-hood technologies, techniques, and tools used to make automated ML possible. <a id="_idTextAnchor050"/>Here, you will see how <strong class="bold">AutoML</strong> actually works, the algorithms and techniques of automated feature engineering, automated model and hyperparameter turning, and automated deep learning. You will learn about meta-learning as well as state-of-the-art techniques, including Bayesian optimization, reinforcement learning, evolutionary algorithms, and gradient-based approaches. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Automated ML – Opening the hood</li>
				<li>Automated feature engineering </li>
				<li>Hyperparameter optimization</li>
				<li>Neural architecture search</li>
			</ul>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor051"/>Automated ML – Opening the hood</h1>
			<p>To oversimplify, a typical ML pipeline comprises data cleaning, feature selection, pre-processing, model <a id="_idIndexMarker085"/>development, deployment, and consumption steps, as seen in the following workflow:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/Figure_2.1_B16890.jpg" alt="Figure 2.1 – The ML life cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – The ML life cycle</p>
			<p>The goal of automated <a id="_idIndexMarker086"/>ML is to simplify and democratize the steps of this pipeline so that it is accessible by citizen data scientists. Originally, the key focus of the automated ML community was model selection and hyperparameter tuning, that is, finding the best-performing model for the job and the corresponding parameters that work best for the problem. However, in recent years, it has been shifted to include the entire pipeline as shown in the following diagram:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/Figure_2.2_B16890.jpg" alt="Figure 2.2 – A simplified AutoML pipeline by Waring et al.&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – A simplified AutoML pipeline by Waring et al.</p>
			<p>The notion of meta-learning, that is, learning to learn, is an overarching theme in the automated ML landscape. Meta-learning techniques are used to learn optimal hyperparameters and architectures by observing learning algorithms, similar tasks, and those from previous models. Techniques<a id="_idIndexMarker087"/> such as learning task similarity, active testing, surrogate model transfer, Bayesian optimization, and stacking are used to learn these meta-features to improve the automated ML pipeline based on similar tasks; essentially, a warm start. The automated ML pipeline function does not really end at deployment – an iterative feedback loop is required to monitor the predictions that arise for drift and consistency. This feedback loop ensures that the outcome distribution of prediction matches the business metrics, and that there are anomalies in terms of hardware resource consumption. From an operational point of view, the logs of errors and warnings, including custom error logs, are audited and monitored in an automated manner. All these best practices also apply to the training cycle, where the concept drift, model drift, or data drift can wreak havoc on your predictions; heed the caveat emptor warning.</p>
			<p>Now, let's explore some of the key automated ML terms you will see in this and future chapters. </p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor052"/>The taxonomy of automated ML terms </h2>
			<p>For newcomers to automated ML, one of the biggest challenges is to become familiar with<a id="_idIndexMarker088"/> the industry jargon – large numbers of new or overlapping terminologies can overwhelm and discourage those exploring the automated ML landscape. Therefore, in this book we try to keep things simple and generalize as much as possible without losing any depth. You will repeatedly see in this book, as well as in other automated ML literature, the emphasis being placed on three key areas – namely, automated feature engineering, automated hyperparameter turning, and automated neural architecture search methods. </p>
			<p>Automated feature<a id="_idIndexMarker089"/> engineering is further classified into feature extraction, selection, and generation or construction. Automated <a id="_idIndexMarker090"/>hyperparameter tuning, or the learning of hyperparameters for a specific model, sometimes gets bundled with learning the model itself, and hence becomes part of a larger neural architecture<a id="_idIndexMarker091"/> search area. This approach is known as the <a id="_idIndexMarker092"/><strong class="bold">Full Model Selection</strong> (<strong class="bold">FMS</strong>) or <strong class="bold">Combined Algorithm Selection and Hyperparameter</strong> (<strong class="bold">CASH</strong>) optimization problem. Neural architecture search is also known as <strong class="bold">automated deep </strong><strong class="bold"><a id="_idIndexMarker093"/></strong><strong class="bold">learning</strong> (abbreviated as <strong class="bold">AutoDL</strong>), or simply architecture search. The following diagram outlines how <strong class="bold">data preparation</strong>, <strong class="bold">feature engineering</strong>, <strong class="bold">model generation</strong>, and <strong class="bold">evaluation</strong>, along with their subcategories, become part of the larger ML pipeline: </p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/Figure_2.3_B16890.jpg" alt="Figure 2.3 – Automated ML pipeline via state-of-the-art AutoML survey, He, et al., 2019&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Automated ML pipeline via state-of-the-art AutoML survey, He, et al., 2019</p>
			<p>The techniques used to perform these three key tenets of automated ML have a few things in<a id="_idIndexMarker094"/> common. Bayesian optimization, reinforcement learning, evolutionary algorithms, gradient-free, and gradient-based approaches are used in almost all these different areas, with variations as shown in the following diagram:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/Figure_2.4_B16890.jpg" alt="Figure 2.4 – Automated ML techniques&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Automated ML techniques</p>
			<p>So, you may get perplexed looks if you refer to using genetic programming in automated feature engineering, while someone considers evolutionary hierarchical ML systems as a hyperparameter optimization algorithm. That is because you can apply the same class of techniques, such as reinforcement learning, evolutionary algorithms, gradient descent, or random search, to different parts of automated ML pipelines, and that works just fine. </p>
			<p>We hope that the<a id="_idIndexMarker095"/> information provided between <em class="italic">Figure 2.2</em> and <em class="italic">Figure 2.4</em> help you to understand the relationship between ML pipelines, automated ML salient traits, and techniques/algorithms used to achieve those three key characteristic traits. The mental model you will build in this chapter will go a long way, especially when you encounter preposterous terms coined by marketing (yes Todd, I am talking about you!), such as deep-learning-based-hyperparameter-optimization-product-with-bitcoins-and-hyperledger. </p>
			<p>The next stop is automated feature engineering, the first pillar of the automated ML pipeline. </p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor053"/>Automated feature engineering </h1>
			<p>Feature engineering is the art and <a id="_idIndexMarker096"/>science of extracting and selecting the right attributes from the dataset. It is an art because it not only requires subject matter expertise, but also domain knowledge and an understanding of ethical and social concerns. From a scientific perspective, the importance of a feature is highly correlated with its resulting impact on the outcome. Feature importance in predictive modeling measures how much a feature influences the target, hence making it easier in retrospect to assign ranking to attributes with the most impact. The following diagram explains how the iterative process of automated feature generation works, by generating candidate features, ranking them, and then selecting the specific ones to become part of the final feature set:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/Figure_2.5_B16890.jpg" alt="Figure 2.5 – Iterative feature generation process by Zoller et al. Benchmark and survey of automated ML frameworks, 2020&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Iterative feature generation process by Zoller et al. Benchmark and survey of automated ML frameworks, 2020</p>
			<p>Extracting a feature from the dataset requires the generation of categorical binary features based on columns with multiple possible values, scaling the features, eliminating highly correlated features, adding feature interactions, substituting cyclic features, and handling<a id="_idIndexMarker097"/> data/time scenarios. Date fields, for instance, result in several features, such as year, month, day, season, weekend/weekday, holiday, and enrollment period. Once extracted, selecting a feature from a dataset requires the removal of sparse and low variance features, as well as applying dimensionality reduction techniques such as <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) to make the number of features manageable. We will now investigate hyperparameter optimization, which used to be a synonym for automated ML, and is still a fundamental entity in the space. </p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor054"/>Hyperparameter optimization</h1>
			<p>Due to its ubiquity and ease of <a id="_idIndexMarker098"/>framing, hyperparameter optimization is sometimes regarded as being synonymous with automated ML. Depending on the search space, if you include features, hyperparameter optimization, also dubbed hyperparameter tuning and hyperparameter learning, is known as automated pipeline<a id="_idIndexMarker099"/> learning. All these terms can be bit daunting for something as simple as finding the right parameters for a model, but graduating students must publish, and I digress.</p>
			<p>There are a couple of key points regarding hyperparameters that are important to note as we look further into these constructs. It is well established that the default parameters are not optimized. Olson et al., in their NIH paper, demonstrated how the default parameters are almost always a bad idea. Olson mentions that "<em class="italic">Tuning often improves an algorithm's accuracy by 3–5%, depending on the algorithm…. In some cases, parameter tuning led to CV accuracy improvements of 50%</em>." This was observed in <em class="italic">Cross-validation accuracy improvement – Data-driven advice for applying ML to bioinformatics problems</em>, by Olson et al.: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/</a>.</p>
			<p>The second important <a id="_idIndexMarker100"/>point is that a comparative analysis of these models leads to greater accuracy; as you will see in forthcoming chapters, the entire pipeline (model, automated features, hyperparameters) are all key to getting the best accuracy trade-off. The <em class="italic">Heatmap for comparative analysis of algorithms</em> section in <em class="italic">Data-driven advice for applying ML to bioinformatics problems</em>, by Olson et al. (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/</a>) shows the experiment performed by Olson et al., where 165 datasets were used against multiple different algorithms to determine the best accuracy, ranked from top to bottom based on performance. The takeaway from this experiment is that no single algorithm can be considered best-performing across all the datasets. Therefore, there is a definite need to consider different ML algorithms when solving these data science problems. </p>
			<p>Let's do a quick recap of what the hyperparameters are.  Each model has its internal and external parameters. Internal parameters or model parameters are intrinsic to the model, such as weight or the predictor matrix, while external parameters also known as hyperparameters, are "outside" the model; for example learning rate and the number of iterations. For instance in k-means, k stands for the number of clusters required and epochs  are used to specify the number of passes done over the training data. Both of these are examples of hyperparameters, that is, parameters that are not intrinsic to the model itself. Similarly, the learning rate for training a neural network, C and sigma for <strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVMs</strong>), <em class="italic">k</em> number of leaves or depth of a tree, latent factors in a matrix factorization, the number of hidden layers in a deep neural network, and so on are all examples of hyperparameters. </p>
			<p>To find the correct hyperparameters, there are a number of approaches, but first let's see what different types of hyperparameters there are. Hyperparameters can be continuous, for example:</p>
			<ul>
				<li>The learning rate of a model</li>
				<li>The number of hidden layers</li>
				<li>The number of iterations </li>
				<li>Batch size</li>
			</ul>
			<p>Hyperparameters can also be<a id="_idIndexMarker101"/> categorical, for example, the type of operator, activation function, or the choice of algorithm. They can also be conditional, for example, selecting the convolutional kernel size if a convolutional layer is used, or<a id="_idIndexMarker102"/> the kernel width if a <strong class="bold">R</strong><strong class="bold">adial Basis Function</strong> (<strong class="bold">RBF</strong>) kernel is selected in an SVM. Since there are multiple types of <a id="_idIndexMarker103"/>hyperparameters, there are also a variety of hyperparameter optimization techniques. Grid, random search, Bayesian optimization, evolutionary techniques, multi-arm bandit approaches, and gradient descent-based techniques are all used for hyperparameter optimization: </p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/Figure_2.6_B16890.jpg" alt="Figure 2.6 – Grid and random search layout. Bergstra and Bengio – JMLR 2012&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Grid and random search layout. Bergstra and Bengio – JMLR 2012</p>
			<p>The simplest techniques for hyperparameter tuning are manual, grid, and random search. Manual turning, as the name suggests, is based on intuition and guessing based on past experiences. Grid search and random search are slightly different, as you pick a set of hyperparameters either for each combination (grid), or randomly and iterate through to keep the best performing ones. However, as you can imagine, this can get computationally out of hand quickly as the search space gets bigger.</p>
			<p>The other<a id="_idIndexMarker104"/> prominent technique is Bayesian optimization, in which you start with a random combination of hyperparameters and use it to construct a surrogate model. Then you use this surrogate model to predict how other combinations of hyperparameters would work. As a general principle, Bayesian optimization builds a probability model to minimize the objective's function, using past performance to select the future values, and that is exactly what's Bayesian about it. As known in the Bayesian universe, your observations are less important than your prior belief. </p>
			<p>The greedy nature of Bayesian optimization is controlled by exploration and exploitation trade-off (expected<a id="_idIndexMarker105"/> improvement), allocating fixed-time evaluations, setting thresholds, and suchlike. There are variations of these surrogate models that exist, such as random forest surrogate and gradient boosting surrogate, which use the aforementioned techniques to minimize the surrogate's function: </p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/Figure_2.7_B16890.jpg" alt="Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi et al., 2019&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – A taxonomy of hyperparameter optimization techniques, Elshawi et al., 2019</p>
			<p>The class of population-based <a id="_idIndexMarker106"/>methods (also called meta-heuristic techniques or optimization from samples methods) is also widely used to perform hyperparameter tuning, with genetic programming (evolutionary algorithms) being the most popular, where hyperparameters are added, mutated, selected, crossed over, and tuned. A particle swarm moves toward the best individual configurations when the configuration space is updated at each iteration. On the other hand, evolutionary algorithms work by maintaining a configuration space, and improve it by making smaller changes and combining individual solutions to build a new generation of hyperparameter configuration. Let's now explore the final piece of the automated ML puzzle – neural architecture search. </p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor055"/>Neural architecture search</h1>
			<p>Selecting models can be challenging. In the case of regression, that is, predicting a numerical value, you have a <a id="_idIndexMarker107"/>choice of linear regression, decision trees, random forest, lasso versus ridge regression, k-means elastic net, gradient boosting methods, including <strong class="bold">XGBoost</strong>, and SVMs, among many others. </p>
			<p>For classification, that in other words, separating out things by classes, you have <strong class="bold">logistic regression</strong>, <strong class="bold">random forest</strong>, <strong class="bold">AdaBoost</strong>, <strong class="bold">gradient boost</strong>, and <strong class="bold">SVM-based classifiers</strong> at your disposal. </p>
			<p>Neural architecture has the notion of search space, which defines which architectures can be used in principle. Then, a search strategy must be defined that outlines how to explore using the exploration-exploitation trade-off. Finally, there has to be a performance estimation strategy, which estimates the candidate's performance. This includes training and validation of the architecture. </p>
			<p>There are several techniques for performing the exploration of search space. The most common ones include chain structured neural networks, multi-branch networks, cell-based search, and optimizing approaches using existing architecture. Search strategies include random search, evolutionary approaches, Bayesian optimization, reinforcement learning, and gradient-free versus gradient-based optimization approaches, such as <strong class="bold">Differentiable Architecture Search</strong> (<strong class="bold">DARTS</strong>). The search strategy to hierarchically explore the architectural search spaces, using Monte Carlo tree search or hill climbing, is popular as it helps discover high-quality architectures by rapidly approaching better performing architectures. These are the gradient "free" methods. In gradient-based methods, the underlying assumption of a continuous search space facilitates DARTS, which, unlike traditional reinforcement learning or evolutionary search approaches, explores the search space using gradient descent. A visual taxonomy of neural architectural search can be seen in the following diagram:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/Figure_2.8_B16890.jpg" alt="Figure 2.8 – A taxonomy of neural architecture search techniques, Elshawi et al., 2019&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – A taxonomy of neural architecture search techniques, Elshawi et al., 2019</p>
			<p>To evaluate which approach works best for the specific dataset, the performance estimation strategies have a<a id="_idIndexMarker108"/> spectrum of simple to more complex (albeit optimized) approaches. Simplest among the estimation strategies is to just train the candidate architecture and evaluate its performance on test data – if it works out, great. Otherwise, toss it out and try a different architectural combination. This approach can quickly become prohibitively resource-intensive as the number of candidate architectures grows; hence, the low-fidelity strategies, such as shorter training times, subset training, and fewer filters per layer are introduced, which are not nearly as exhaustive. Early stopping, in other words, estimating an architecture's performance by extrapolating its learning curve, is also a helpful optimization for such an approximation. Morphing a trained neural architecture, and one-short searches treating all architectures as a subgraph of a super graph, are also effective approaches as regards one-shot architecture search. </p>
			<p>Several surveys have been conducted <a id="_idIndexMarker109"/>in relation to automated ML that provide an in-depth overview of these techniques. The specific techniques also have their own publications, with well-articulated benchmark data, challenges, and triumphs – all of which is beyond the scope of this manuscript. In the next chapter however, we will use the libraries that utilize these techniques, so you will get better hands-on exposure vis-à-vis their usability.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor056"/>Summary</h1>
			<p>Today, the success of ML within an enterprise largely depends on human ML experts who can construct business-specific features and workflows. Automated ML aims to change this, as it aims to automate ML so as to provide off-the-shelf ML methods that can be utilized without expert knowledge. To understand how automated ML works, we need to review the underlying four subfields, or pillars, of automated ML: hyperparameter optimization; automated feature engineering; neural architecture search; and meta-learning.</p>
			<p>In this chapter, we explained what is under the hood in terms of the technologies, techniques, and tools used to make automated ML possible. We hope that this chapter has introduced you to automated ML techniques and that you are now ready to do a deeper dive into the implementation phase. </p>
			<p>In the next chapter, we will review the open source tools and libraries that implement these algorithms to get a hands-on overview of how to use these concepts in practice, so stay tuned. </p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor057"/>Further reading</h1>
			<p>For more information on the following topics, refer to the suggested resources and links:</p>
			<ul>
				<li><em class="italic">Automated ML: Methods, Systems, Challenges</em>: Frank Hutter (Editor), Lars Kotthoff (Editor), and Joaquin Vanschoren (Editor). The Springer Series on Challenges in ML </li>
				<li><em class="italic">Hands-On Automated ML</em>: A Beginner's Guide to Building Automated ML Systems Using AutoML and Python, by Sibanjan Das and Umit Mert Cakmak, Packt </li>
				<li><em class="italic">Neural Architecture Search with Reinforcement Learning</em>: <a href="https://arxiv.org/pdf/1611.01578.pdf">https://arxiv.org/pdf/1611.01578.pdf</a></li>
				<li><em class="italic">Learning Transferable Architectures for Scalable Image Recognition</em>: <a href="https://arxiv.org/pdf/1707.07012.pdf">https://arxiv.org/pdf/1707.07012.pdf</a></li>
				<li><em class="italic">Progressive Neural Architecture Search</em>: <a href="https://arxiv.org/pdf/1712.00559.pdf">https://arxiv.org/pdf/1712.00559.pdf</a></li>
				<li><em class="italic">Efficient Neural Architecture Search via Parameter Sharing</em>: <a href="https://arxiv.org/pdf/1802.03268.pdf">https://arxiv.org/pdf/1802.03268.pdf</a></li>
				<li><em class="italic">Efficient Architecture Search by Network Transformation</em>: <a href="https://arxiv.org/pdf/1707.04873.pdf">https://arxiv.org/pdf/1707.04873.pdf</a></li>
				<li><em class="italic">Network Morphism</em>: <a href="https://arxiv.org/pdf/1603.01670.pdf">https://arxiv.org/pdf/1603.01670.pdf</a> </li>
				<li><em class="italic">Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution</em>: <a href="https://arxiv.org/pdf/1804.09081.pdf">https://arxiv.org/pdf/1804.09081.pdf</a></li>
				<li><em class="italic">Auto-Keras</em>: <em class="italic">An Efficient Neural Architecture Search System</em>: <a href="https://arxiv.org/pdf/1806.10282.pdf">https://arxiv.org/pdf/1806.10282.pdf</a></li>
				<li><em class="italic">Convolutional Neural Fabrics</em>: <a href="https://arxiv.org/pdf/1606.02492.pdf">https://arxiv.org/pdf/1606.02492.pdf</a></li>
				<li><em class="italic">DARTS</em>: <em class="italic">Differentiable Architecture Search</em>: <a href="https://arxiv.org/pdf/1806.09055.pdf">https://arxiv.org/pdf/1806.09055.pdf</a></li>
				<li><em class="italic">Neural Architecture Optimization</em>: <a href="https://arxiv.org/pdf/1808.07233.pdf">https://arxiv.org/pdf/1808.07233.pdf</a></li>
				<li><em class="italic">SMASH</em>: One-Shot Model Architecture Search through HyperNetworks: <a href="https://arxiv.org/pdf/1708.05344.pdf">https://arxiv.org/pdf/1708.05344.pdf</a></li>
				<li><em class="italic">DARTS in PyTorch</em>: <a href="https://github.com/quark0/darts">https://github.com/quark0/darts</a></li>
				<li>Hyperparameter Tuning Using Simulated Annealing: <a href="https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html">https://santhoshhari.github.io/2018/05/18/hyperparameter-tuning-using-simulated-annealing.html</a></li>
				<li>Bayesian Optimization: <a href="http://krasserm.github.io/2018/03/21/bayesian-optimization/">http://krasserm.github.io/2018/03/21/bayesian-optimization/</a></li>
				<li>Neural Architecture Search: A Survey: <a href="https://www.jmlr.org/papers/volume20/18-598/18-598.pdf">https://www.jmlr.org/papers/volume20/18-598/18-598.pdf</a></li>
				<li>Data-driven advice for applying ML to bioinformatics problems: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5890912/</a></li>
			</ul>
		</div>
	</body></html>