- en: Scala for Dimensionality Reduction and Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw several examples of supervised learning, covering
    both classification and regression. We performed supervised learning techniques
    on structured and labelled data. However, as we mentioned previously, with the
    rise of cloud computing, IoT, and social media, unstructured data is increasing
    unprecedentedly. Collectively, more than 80% of this data is unstructured and
    which most of them are unlabeled.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning techniques, such as clustering analysis and dimensionality
    reduction, are two of the key applications in data-driven research and industry
    settings for finding hidden structures in unstructured datasets. There are many
    clustering algorithms being proposed for this, such as k-means, bisecting k-means,
    and the Gaussian mixture model. However, these algorithms cannot perform with
    high-dimensional input datasets and often suffer from the *curse of dimensionality*.
    So, reducing the dimensionality using algorithms like **principal component analysis**
    (**PCA**) and feeding the latent data is a useful technique for clustering billions
    of data points.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use a genetic variant (one kind of genomic data) to
    cluster the population according to their predominant ancestry, also called geographic
    ethnicity. We will evaluate the clustering analysis result, followed by the dimensionality
    reduction technique, to avoid the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning clustering—clustering geographic ethnicity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction with PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering with reduced dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapters can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2ISwb3o](http://bit.ly/2ISwb3o)'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In unsupervised learning, an input set is provided to the system during the
    training phase. In contrast to supervised learning, the input objects are not
    labeled with their class. Although in classification analysis the training dataset
    is labeled, we do not always have that advantage when we collect data in the real
    world, but still we want to find important values or hidden structures of the
    data. In NeuralIPS'' 2016, Facebook AI Chief Yann LeCun introduced the *cake analogy*:'
  prefs: []
  type: TYPE_NORMAL
- en: '"If intelligence was a cake, unsupervised learning would be the cake, supervised
    learning would be the icing on the cake, and reinforcement learning would be the
    cherry on the cake. We know how to make the icing and the cherry, but we don''t
    know how to make the cake."'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create such a cake, several unsupervised learning tasks, including
    clustering, dimensionality reduction, anomaly detection, and association rule
    mining, are used. If unsupervised learning algorithms help find previously unknown
    patterns in a dataset without needing a label, we can learn the following analogy
    for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means is a popular clustering analysis algorithm for grouping similar data
    points together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dimensionality reduction algorithm, such as PCA, helps find the most relevant
    features in a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll discuss these two techniques for cluster analysis with
    a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering analysis and dimensionality reduction are the two most popular examples
    of unsupervised learning, which we will discuss throughout this chapter with examples.
    Suppose you have a large collection of legal MP3 files in your computer or smartphones.
    In such a case, how could you possibly group songs together if you do not have
    direct access to their metadata?
  prefs: []
  type: TYPE_NORMAL
- en: One possible approach could be to mix various ML techniques, but clustering
    is often the best solution. This is because we can develop a clustering model
    in order to automatically group similar songs and organize them into your favorite
    categories, such as country, rap, or rock.
  prefs: []
  type: TYPE_NORMAL
- en: Although the data points are not labeled, we can still do the necessary feature
    engineering and group similar objects together, which is commonly referred to
    as clustering.
  prefs: []
  type: TYPE_NORMAL
- en: A cluster refers to a collection of data points grouped together based on certain
    similarity measures.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is not easy for a human. Instead, a standard approach is to define
    a similarity measure between two objects and then look for any cluster of objects
    that is more similar to each other than it is to the objects in the other clusters.
    Once we have done the clustering of the data points (that is, the MP3 files) and
    the validation is completed, we know the pattern of the data (that is, what type
    of MP3 files fall in which group).
  prefs: []
  type: TYPE_NORMAL
- en: 'The left-hand side diagram shows all the **MP3 tracks in a playlist**, which
    are scattered. The right-hand side part shows how the MP3 are clustered based
    on genre:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a19db36-cb05-4e82-92d5-44ed0b747360.png)'
  prefs: []
  type: TYPE_IMG
- en: Clustering analysis algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of a clustering algorithm is to group a similar set of unlabeled data
    points together to discover underlying patterns. Here are some of the algorithms
    that have been proposed and used for clustering analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting k-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian mixture model** (**GMM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power iteration clustering** (**PIC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming k-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means, bisecting k-means, and GMM are the most widely used. They will be covered
    in detail to show a quick-start clustering analysis. However, we will also look
    at an example based on only k-means.
  prefs: []
  type: TYPE_NORMAL
- en: K-means for clustering analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means looks for a fixed number *k* of clusters (that is, the number of centroids), partitions
    the data points into *k* clusters, and allocates every data point to the nearest
    cluster by keeping the centroids as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: A centroid is an imaginary or real location that represents the center of the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'K-means computes the distance (usually the Euclidean distance) between data
    points and the center of the *k* clusters by minimizing the cost function, called
    **within-cluster sum of squares** (**WCSS**). The k-means algorithm proceeds by
    alternating between two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: Each data point is assigned to the cluster whose
    mean has the least-squared Euclidean distance, yielding the lowest WCSS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroid update step**: The new means of the observations in the new clusters
    are calculated and used as the new centroids'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding steps can be represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df03a615-1cf4-4351-b4b6-7939864b274e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The k-means algorithm is done when the centroids have stabilized or when the
    predefined number of iterations have been iterated. Although k-means uses Euclidean
    distance, there are other ways to calculate the distance too, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Chebyshev distance** can be used to measure the distance by considering
    only the most notable dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Hamming distance** algorithm can identify the difference between two strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make the distance metric scale-undeviating, **Mahalanobis distance** can
    be used to normalize the covariance matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Manhattan distance** is used to measure the distance by considering only
    axis-aligned directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Minkowski distance** algorithm is used to make the Euclidean distance,
    Manhattan distance, and Chebyshev distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **haversine distance** is used to measure the great-circle distances between
    two points on a sphere from the location, that is, longitudes and latitudes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bisecting k-means can be thought of as a combination of k-means and hierarchical
    clustering, which starts with all the data points in a single cluster. Then, it
    randomly picks a cluster to split, which returns two sub-clusters using basic
    k-means. This is called the **bisecting step**.
  prefs: []
  type: TYPE_NORMAL
- en: The bisecting k-means algorithm is based on a paper titled *A Comparison of
    Document Clustering Techniques* by *Michael Steinbach et al.*, *KDD Workshop on
    Text Mining, 2000*, which has been extended to fit with Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the bisecting step is iterated for a predefined number of times (usually
    set by the user/developer), and all the splits are collected that produce the
    cluster with the highest similarity. These steps are continued until the desired
    number of clusters is reached. Although bisecting k-means is faster than regular
    k-means, it produces different clustering because bisecting k-means initializes
    clusters randomly.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian mixture model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GMM is a probabilistic model with a strong assumption that all the data points
    are generated from a mixture of a finite number of Gaussian distributions with
    unknown parameters. So, it is a distribution-based clustering algorithm too, which
    is based on an expectation-maximization approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'GMM can also be considered as a generalized k-means where the model parameters
    are optimized iteratively to fit the model better to the training dataset. The
    overall process can be written in a three-step pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Objective function**: Compute and maximize the log-likelihood using **expectation-maximization**
    (**EM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EM step**: This EM step consists of two sub-steps called expectation and
    maximization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step E**: Compute the posterior probability of the nearer data points'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step M**: Update and optimize the model parameters for fitting mixture-of-Gaussian
    models'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assignment**: Perform soft assignment during *step E*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding steps can be visualized very naively as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e09fc2f-8686-40aa-acb1-18c837d64212.png)'
  prefs: []
  type: TYPE_IMG
- en: Other clustering analysis algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other clustering algorithms includes PIC, which is used to cluster the nodes
    of a graph based on the given pairwise similarities, such as edge. The LDA is
    used often in text clustering use cases, such as topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, streaming k-means is similar to k-means but applicable for
    streaming data. For example, when we want to estimate the clusters dynamically
    so that the clustering assignment will be updated when new data arrives, using
    streaming k-means is a good option. For a more detailed discussion with examples,
    interested readers can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML-based clustering algorithms ([https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark MLlib-based clustering algorithms ([https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering analysis through examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most important tasks in clustering analysis is the analysis of genomic
    profiles to attribute individuals to specific ethnic populations, or the analysis
    of nucleotide haplotypes for diseases susceptibility. Human ancestry from Asia,
    Europe, Africa, and the Americas can be separated based on their genomic data.
    Research has shown that the Y chromosome lineage can be geographically localized,
    forming the evidence for clustering the human alleles of the human genotypes.
    According to National Cancer Institute ([https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant](https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant)):'
  prefs: []
  type: TYPE_NORMAL
- en: '"Genetic variants are an alteration in the most common DNA nucleotide sequence.
    The term variant can be used to describe an alteration that may be benign, pathogenic,
    or of unknown significance. The term variant is increasingly being used in place
    of the term mutation."'
  prefs: []
  type: TYPE_NORMAL
- en: A better understanding of genetic variations assists us in finding correlating
    population groups, identifying patients who are predisposed to common diseases,
    and solving rare diseases. In short, the idea is to cluster geographic ethnic
    groups based on their genetic variants. However, before going into this any further,
    let's get to know the data.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data from the 1,000 Genomes Project is a large catalog of human genetic
    variants. This project is meant for determining genetic variants with frequencies
    above 1% in the populations that were studied. The third phase of the 1,000 Genomes
    Project finished in September 2014, covering 2,504 individuals from 26 populations
    and 84,4000,000 million genetic variants. The population samples are grouped into
    five super-population groups, according to their predominant ancestry:'
  prefs: []
  type: TYPE_NORMAL
- en: East Asian (CHB, JPT, CHS, CDX, and KHV)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: European (CEU, TSI, FIN, GBR, and IBS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: African (YRI, LWK, GWD, MSL, ESN, ASW, and ACB)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: American (MXL, PUR, CLM, and PEL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: South Asian (GIH, PJL, BEB, STU, and ITU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each genotype comprises 23 chromosomes and a separate panel file that contains
    sample and population information. The data in **Variant Call Format** (**VCF**)
    as well the panel file can be downloaded from [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the programming environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the third release of the 1,000 Genome Project contributes about 820 GB
    of data, using scalable software and hardware is required to process them. To
    do so, we will use a software stack that consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ADAM**: This can be used to achieve the scalable genomics-data-analytics
    platform with support for the VCF file format so that we can transform genotype-based
    RDD into Spark DataFrames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparkling Water**:H20 is an AI platform for machine learning and a web-based
    data-processing UI with support for programming languages such as Java, Python,
    and R. In short, Sparkling Water equals H2O plus Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark-ML-based k-means is trained for clustering analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this example, we need to use multiple technology and software stacks, such
    as Spark, H2O, and Adam. Before using H20, make sure that your laptop has at least
    16 GB RAM and sufficient storage space. I will develop this solution as a Maven
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the properties tag on the `pom.xml` file for the Maven-friendly
    project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once you create a Maven project on Eclipse (from an IDE or using the `mvn install` command),
    all the required dependencies will be downloaded!
  prefs: []
  type: TYPE_NORMAL
- en: Clustering geographic ethnicity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '24 VCF files contribute around 820 GB of data, which will impose a great computational
    challenge. To overcome this, use the genetic variants from the smallest chromosome,
    Y. The size of the VCF file for this is around 160 MB. Let''s get started by creating
    `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s show Spark the path of both VCF and the PANEL file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We process the PANEL file using Spark to access the target population data
    and identify the population groups. First, we create a set of `populations` that
    we want to form clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to create a map between the sample ID and the given population
    so that we can filter out the samples we are not interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The panel file produces the sample ID of all individuals, population groups,
    ethnicity, super-population groups, and genders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d3126f4-ea27-4581-8734-c56bb5f15441.png)'
  prefs: []
  type: TYPE_IMG
- en: Check out the details of the panel file at [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, load the ADAM genotypes and filter the genotypes so that we are left
    with only those in the populations we are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, convert the `Genotype` objects into our own `SampleVariant` objects to
    conserve memory. Then, the `genotype` object is converted into a `SampleVariant`
    object that contains the data that needs to be processed further:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample ID**: To uniquely identify a particular sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variant ID**: To uniquely identify a particular genetic variant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternate alleles count**: Needed when the sample differs from the reference
    genome'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The signature that prepares a `SampleVariant` is given as follows, which takes
    `sampleID`, `variationId`, and the `alternateCount` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to find the `variantID` from the genotype file. A `varitantId`
    is a String type that consists of a name, and the start and end positions in the
    chromosome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the `variantID`, we should hunt for the `alternateCount`. In the
    genotype file, objects for which an allele reference is present, would be the
    genetic alternates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will construct a `SampleVariant` object. For this, we need to intern
    sample IDs as they will be repeated a lot in a VCF file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need is to prepare `variantsRDD`. First, we have to group the variants
    by sample ID so that we can process the variants sample by sample. Then, we can
    get the total number of samples to be used to find the variants that are missing
    for some samples. Finally, we have to group the variants by variant ID and filter
    out those variants that are missing from some samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s map `variantId` with the count of samples with an alternate count
    of greater than zero. Then, we filter out the variants that are not in our desired
    frequency range. The objective here is to reduce the number of dimensions in the
    dataset to make it easier to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The total number of samples (or individuals) has been determined. Now, before
    grouping them using their variant IDs, we can filter out less significant variants.
    Since we have more than 84 million genetic variants, filtering would help us deal
    with the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specified range is arbitrary and was chosen because it includes a reasonable
    number of variants, but not too many. To be more specific, for each variant, the
    frequency for alternate alleles has been calculated, and variants with fewer than
    12 alternate alleles have been excluded, leaving about 3,000,000 variants in the
    analysis (for 23 chromosome files):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have `filteredVariantsBySampleId`, we need to sort the variants for
    each sample ID. Each sample should now have the same number of sorted variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'All items in the RDD should now have the same variants in the same order. The
    final task is to use `sortedVariantsBySampleId` to construct an RDD of a row that
    contains the region and the alternate count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we can just use the first one to construct our header for the training
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Well done! We have our RDD and the `StructType` header. Now, we can play with
    the Spark machine learning algorithm with minimal adjustment/conversion.
  prefs: []
  type: TYPE_NORMAL
- en: Training the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have the `rowRDD` and the header, we need to construct the rows of
    our schema DataFrame from the variants using the header and `rowRDD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `show()` method should show a snapshot of the training dataset
    that contains the features and the `label` columns (that is, `Region`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b82f5318-3021-49a0-b8ed-ba8d1cdc66cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding DataFrame, only a few `feature` columns and the `label` column
    are shown so that it fits on the page. Since the training would be unsupervised,
    we need to drop the `label` column (that is, `Region`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `show()` method shows the following snapshot of the training
    dataset for k-means. Note that there is no `label` column (that is, `Region`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/084d8df1-14cb-450a-a607-23c68c8d0adc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In [Chapter 1](33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml), *Introduction to
    Machine Learning with Scala*, and [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml),
    *Scala for Regression Analysis*, we saw that Spark expects two columns (`features`
    and `label`) for supervised training. However, for the unsupervised training,
    only a single column containing the features is required. Since we dropped the
    `label` column, we now need to amalgamate the entire `variable` column into a
    single `features` column. For this, we will use the `VectorAssembler()` transformer.
    Let''s select the columns to be embedded into a vector space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will instantiate the `VectorAssembler()` transformer by specifying
    the input columns and the output column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see what the feature vectors for the k-means look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line shows the assembled vectors, which can be used as the feature
    vectors for the k-means model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e18458d-a0b4-4da2-8f8e-212bd5351897.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we are ready to train the k-means algorithm and evaluate the clustering
    by computing **WCSS**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the **WCSS** value for `k = 5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We managed to apply k-means to cluster genetic variants. However, we saw that
    the WCSS was high because k-means was unable to separate the non-linearity among
    different correlated and high-dimensional features. This is because genomic sequencing
    datasets are very high dimensional due to a huge number of genetic variants.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how we can use dimensionality-reduction techniques,
    such as PCA, to reduce the dimensionality of the input data before feeding it
    to k-means in order to get better clustering quality.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since humans are visual creatures, understanding a high dimensional dataset
    (even with more than three dimensions) is impossible. Even for a machine (or say,
    our machine learning algorithm), it's difficult to model the non-linearity from
    correlated and high-dimensional features. Here, the dimensionality reduction technique
    is a savior.
  prefs: []
  type: TYPE_NORMAL
- en: Statistically, dimensionality reduction is the process of reducing the number
    of random variables to find a low-dimensional representation of the data while
    preserving as much information as possible**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall step in PCA can be visualized naively in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02ea50cd-5589-46e8-8bf3-388c3ed9c326.png)'
  prefs: []
  type: TYPE_IMG
- en: PCA and **singular-value decomposition** (**SVD**) are the most popular algorithms
    for dimensionality reduction. Technically, PCA is a statistical technique that's
    used to emphasize variation and extract the most significant patterns (that is,
    features) from a dataset, which is not only useful for clustering but also for
    classification and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis with Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark-ML-based PCA can be used to project vectors to a low-dimensional space
    to reduce the dimensionality of genetic variant features before feeding them into
    the k-means model. The following example shows how to project 6D feature vectors
    into 4D principal components from the following feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a feature DataFrame with a 6D-feature vector, it can be fed
    into the PCA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0069126d-2e94-400f-a8e4-9bf63f6f3ab0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we have to instantiate the PCA model by setting the necessary parameters,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To distinguish the original features from the principal component-based features,
    we set the output column name as `pcaFeatures` using the `setOutputCol()` method.
    Then, we set the dimension of the PCA (that is, the number of principal components).
    Finally, we fit the DataFrame to make the transformation. A model can be loaded
    from older data but will have an empty vector for `explainedVariance`. Now, let''s
    show the resulting features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a feature DataFrame with 4D feature vectors as
    principal components using the PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b6c797c-957d-49ee-a048-f66fd3b659e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we can transform the assembled DataFrame (that is, `assembleDF`)
    in the previous step and the top five principle components. You can adjust the
    number of principal components, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to avoid any ambiguity, we renamed the `pcaFeatures` column to `features`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines of code show the embedded vectors, which can be used as
    the feature vectors for the k-means model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79031b6c-40ce-4ec1-b5cc-a8369cfc143c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding screenshot shows the top five principal components as the most
    important features. Excellent—everything went smoothly. Finally, we are ready
    to train the k-means algorithm and evaluate clustering by computing WCSS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the WCSS is slightly lower (compared to the previous value, which
    was `59.34564329865`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Normally, we set the number of `k` (that is, `5`) randomly and computed the
    WCSS. However, this way, we cannot always set the optimal number of clusters.
    In order to find an optimal value, researchers have come up with two techniques,
    called the elbow method and silhouette analysis, which we'll look at in the following
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the optimal number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, assuming the number of clusters naively and before starting the
    training may not be a good idea. If the assumption is too far from the optimal
    number of clusters, the model performs poorly because of the overfitting or underfitting
    issue that''s introduced. So, determining the number of optimal clusters is a
    separate optimization problem. There are two popular techniques to tackle this:'
  prefs: []
  type: TYPE_NORMAL
- en: The heuristic approach, called the **elbow method**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Silhouette analysis**, to observe the separation distance between predicted
    clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The elbow method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by setting the `k` value to `2` and running the k-means algorithm
    on the same dataset by increasing `k` and observing the value of WCSS. As expected,
    a drastic drop should occur in the cost function (that is, WCSS values) at some
    point. However, after the drastic fall, the value of WCSS becomes marginal with
    the increasing value of `k`. As suggested by the elbow method, we can pick the
    optimal value of `k` after the last big drop of WCSS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see the WCSS values for a different number of clusters, such as
    between `2` and `20`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we calculated the cost function, WCSS, as a
    function of a number of clusters for the k-means algorithm, and applied them to
    the Y chromosome genetic variants from the selected population groups. It can
    be observed that a big drop occurs when `k = 5` (which is not a drastic drop,
    though). Therefore, we chose a number of clusters to be 10.
  prefs: []
  type: TYPE_NORMAL
- en: The silhouette analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing the silhouette is carried out by observing the separation distance
    between predicted clusters. Drawing a silhouette plot will show the distance between
    a data point from its neighboring clusters, and then we can visually inspect a
    number of clusters so that similar data points get well-separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The silhouette score, which is used to measure the clustering quality, has
    a range of [-1, 1]. Evaluate the clustering quality by computing the silhouette
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the height value of the silhouette is generated
    with `k = 2`, which is `0.9175803927739566`. However, this suggests that genetic
    variants should be clustered in two groups. The elbow method suggested `k = 5`
    as the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s find out the silhouette using the squared Euclidean distance, as shown
    in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The silhouette with the squared Euclidean distance for `k = 2` is `0.9175803927739566`.
  prefs: []
  type: TYPE_NORMAL
- en: It has been found that the bisecting k-means algorithm can result in better
    cluster assignment for data points, converging to the global minima. On the other
    hand, k-means often gets stuck in the local minima. Please note that you might
    observe different values of the preceding parameters depending on your machine's
    hardware configuration and the random nature of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Interested readers should also refer to the Spark-MLlib-based clustering techniques
    at [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    for more insights.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed some clustering analysis techniques, such as k-means,
    bisecting k-means, and GMM. We saw a step-by-step example of how to cluster ethnic
    groups based on their genetic variants. In particular, we used the PCA for dimensionality
    reduction, k-means for clustering, and H2O and ADAM for handling large-scale genomics
    datasets. Finally, we learned about the elbow and silhouette methods for finding
    the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is the key to most data-driven applications. Readers can try to apply
    clustering algorithms on higher-dimensional datasets, such as gene expression
    or miRNA expression, in order to cluster similar and correlated genes. A great
    resource is the gene expression cancer RNA-Seq dataset, which is open source.
    This dataset can be downloaded from the UCI machine learning repository at [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss item-based collaborative filtering approaches
    for the recommender system. We'll learn how to develop a book recommendation system.
    Technically, it will be a model-based recommendation engine with Scala and Spark.
    We will see how we can interoperate between ALS and matrix factorization.
  prefs: []
  type: TYPE_NORMAL
