["```py\nJupyter==1.0.0\nnetworkx==2.5\nscikit-learn==0.24.0\npandas==1.1.3\nnode2vec==0.3.3\nnumpy==1.19.2\ncommunities==2.2.0\n```", "```py\nimport pandas as pd\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n```", "```py\ndf[\"is_fraud\"].value_counts()\n```", "```py\n0    257834\n1      7506\n```", "```py\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df = df_input.copy()\n    mapping = {x:node_id for node_id,x in enumerate(set(df[\"cc_num\"].values.tolist() + df[\"merchant\"].values.tolist()))}\n    df[\"from\"] = df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"to\"] = df[\"merchant\"].apply(lambda x: mapping[x])\n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from', 'to']).agg({\"is_fraud\": \"sum\", \"amt\": \"sum\"}).reset_index()\n    df[\"is_fraud\"] = df[\"is_fraud\"].apply(lambda x: 1 if x>0 else 0)\n    G = nx.from_edgelist(df[[\"from\", \"to\"]].values, create_using=graph_type)\n    nx.set_edge_attributes(G, {(int(x[\"from\"]), int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")\n    nx.set_edge_attributes(G,{(int(x[\"from\"]), int(x[\"to\"])):x[\"amt\"] for idx, x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\")\n    return G\n```", "```py\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))))\n```", "```py\nG_bd = build_graph_bipartite(df, nx.DiGraph(name=\"Bipartite Direct\"))))\n```", "```py\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df = df_input.copy()\n    mapping = {x:node_id for node_id,x in enumerate(set(df.index.values.tolist() + df[\"cc_num\"].values.tolist() + df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"] = df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"] = df[\"merchant\"].apply(lambda x: mapping[x])\n    G = nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] + [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    return G\n```", "```py\nG_tu = build_graph_tripartite(df, nx.Graph(name=\"Tripartite Undirect\"))\n```", "```py\nG_td = build_graph_tripartite(df, nx.DiGraph(name=\"Tripartite Direct\"))\n```", "```py\nfrom networkx.algorithms import bipartite\nall([bipartite.is_bipartite(G) for G in [G_bu,G_tu]]\n```", "```py\nfor G in [G_bu, G_tu]:\n print(nx.info(G))\n```", "```py\nName: Bipartite Undirect\nType: Graph\nNumber of nodes: 1676\nNumber of edges: 201725\nAverage degree: 240.7220\nName: Tripartite Undirect\nType: Graph\nNumber of nodes: 267016\nNumber of edges: 530680\nAverage degree:   3.9749\n```", "```py\nfor G in [G_bu, G_tu]:\n  plt.figure(figsize=(10,10))\n  degrees = pd.Series({k: v for k, v in nx.degree(G)})\n  degrees.plot.hist()\n  plt.yscale(\"log\")\n```", "```py\n    for G in [G_bu, G_tu]:\n      allEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in G.edges(data=True)})\n      np.quantile(allEdgesWeights.values,[0.10,0.50,0.70,0.9])\n    ```", "```py\n    array([  5.03 ,  58.25 ,  98.44 , 215.656])\n     array([  4.21,  48.51,  76.4 , 147.1 ])\n    ```", "```py\n    for G in [G_bu, G_tu]:\n      plt.figure(figsize=(10,10))\n      bc_distr = pd.Series(nx.betweenness_centrality(G))\n      bc_distr.plot.hist()\n      plt.yscale(\"log\")\n    ```", "```py\n    for G in [G_bu, G_tu]:\n       print(nx.degree_pearson_correlation_coefficient(G)) \n    ```", "```py\n    -0.1377432041049189\n    -0.8079472914876812\n    ```", "```py\n    import community\n    for G in [G_bu, G_tu]:\n       parts = community.best_partition(G, random_state=42, weight='weight')\n       communities = pd.Series(parts)   print(communities.value_counts().sort_values(ascending=False))\n    ```", "```py\n    5     546\n    0     335\n    7     139\n    2     136\n    4     123\n    3     111\n    8      83\n    9      59\n    10     57\n    6      48\n    11     26\n    1      13\n    ```", "```py\n    11     4828\n    3      4493\n    26     4313\n    94     4115\n    8      4036\n        ... 47     1160\n    103    1132\n    95      954\n    85      845\n    102     561\n    ```", "```py\n    communities.value_counts().plot.hist(bins=20)\n    ```", "```py\n    graphs = []\n    d = {}\n    for x in communities.unique():\n        tmp = nx.subgraph(G, communities[communities==x].index)\n        fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n        ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n        d[x] = ratio\n        graphs += [tmp]\n    print(pd.Series(d).sort_values(ascending=False))\n    ```", "```py\n    gId = 10\n    spring_pos = nx.spring_layout(graphs[gId])\n     edge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]\n    nx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, edge_color=edge_colors, with_labels=False, node_size=15)\n    ```", "```py\n    9     26.905830\n    10    25.482625\n    6     22.751323\n    2     21.993834\n    11    21.333333\n    3     20.470263\n    8     18.072289\n    4     16.218905\n    7      6.588580\n    0      4.963345\n    5      1.304983\n    1      0.000000\n    ```", "```py\n    6      6.857728\n    94     6.551151\n    8      5.966981\n    1      5.870918\n    89     5.760271\n          ...   \n    102    0.889680\n    72     0.836013\n    85     0.708383\n    60     0.503461\n    46     0.205170\n    ```", "```py\n    pd.Series(d).plot.hist(bins=20)\n    ```", "```py\n    from sklearn.utils import resample\n    df_majority = df[df.is_fraud==0]\n     df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n     G_down = build_graph_bipartite(df_downsampled, nx.Graph())\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    train_edges, val_edges, train_labels, val_labels = train_test_split(list(range(len(G_down.edges))), list(nx.get_edge_attributes(G_down, \"label\").values()), test_size=0.20, random_state=42)\n     edgs = list(G_down.edges)\n    train_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\n    train_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n    ```", "```py\n    from node2vec import Node2Vec\n    node2vec = Node2Vec(train_graph, weight_key='weight')\n     model = node2vec_train.fit(window=10)\n    ```", "```py\n    from sklearn import metrics\n    from sklearn.ensemble import RandomForestClassifier \n    from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n    classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n    for cl in classes:\n        embeddings = cl(keyed_vectors=model.wv)\n        train_embeddings = [embeddings[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n        val_embeddings = [embeddings[str(edgs[x][0]), str(edgs[x][1])] for x in val_edges]\n        rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n        rf.fit(train_embeddings, train_labels)\n        y_pred = rf.predict(val_embeddings)\n        print(cl)\n        print('Precision:', metrics.precision_score(val_labels, y_pred))\n        print('Recall:', metrics.recall_score(val_labels, y_pred))\n        print('F1-Score:', metrics.f1_score(val_labels, y_pred))\n    ```", "```py\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\n unsup_vals = nod2vec_unsup.fit(window=10)\n```", "```py\nfrom sklearn.cluster import KMeans\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n true_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv)\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n```"]