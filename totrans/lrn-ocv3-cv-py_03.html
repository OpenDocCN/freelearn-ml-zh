<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Processing Images with OpenCV 3" id="aid-LTSU1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Processing Images with OpenCV 3</h1></div></div></div><p>Sooner or later, when working with images, you will find yourself in need of altering images: be it applying artistic filters, extrapolating certain sections, cutting, pasting, or whatever else your mind can conjure. This chapter presents some techniques to alter images, and by the end of it, you should be able to perform tasks, such as detecting skin tone in an image, sharpening an image, mark contours of subjects, and detecting crosswalks using a line segment detector.</p><div class="section" title="Converting between different color spaces"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Converting between different color spaces</h1></div></div></div><p>There are literally hundreds of methods in OpenCV that pertain to the conversion of color spaces. In general, three color spaces are prevalent in modern day computer vision: gray, BGR, and <a id="id132" class="indexterm"/>
<span class="strong"><strong>Hue, Saturation, Value</strong></span> (<span class="strong"><strong>HSV</strong></span>).</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Gray is <a id="id133" class="indexterm"/>a color space that effectively eliminates color information translating to shades of gray: this color space is extremely useful for intermediate processing, such as face detection.</li><li class="listitem">BGR is the blue-green-red color space, in which each pixel is a three-element array, each value representing the blue, green, and red colors: web developers would be familiar with a similar definition of colors, except the order of colors is RGB.</li><li class="listitem">In HSV, hue is a color tone, saturation is the intensity of a color, and value represents its darkness (or brightness at the opposite end of the spectrum).</li></ul></div><div class="section" title="A quick note on BGR"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"/>A quick note on BGR</h2></div></div></div><p>When I first <a id="id134" class="indexterm"/>started dealing with the BGR color space, something wasn't adding up: the <code class="literal">[0 255 255]</code> value (no blue, full green, and full red) produces the yellow color. If you have an artistic background, you won't even need to pick up paints and brushes to witness green and red mix into a muddy shade of brown. That is because the color model used in <a id="id135" class="indexterm"/>computing is called an <span class="strong"><strong>additive</strong></span> and deals with lights. Lights behave differently from paints (which follow the <span class="strong"><strong>subtractive</strong></span> color model), and—as <a id="id136" class="indexterm"/>software runs on computers whose medium is a monitor <a id="id137" class="indexterm"/>that emits light—the color model of reference is the additive one.</p></div></div></div>
<div class="section" title="The Fourier Transform"><div class="titlepage" id="aid-MSDG2"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>The Fourier Transform</h1></div></div></div><p>Much of the <a id="id138" class="indexterm"/>processing you apply to images and videos in OpenCV involves the concept of Fourier Transform in some capacity. Joseph Fourier was an 18th century French mathematician who discovered and popularized many mathematical concepts, and concentrated his work on studying the laws governing heat, and in mathematics, all things waveform. In particular, he observed that all waveforms are just the sum of simple sinusoids of different frequencies.</p><p>In other words, the waveforms you observe all around you are the sum of other waveforms. This concept is incredibly useful when manipulating images, because it allows us to identify regions in images where a signal (such as image pixels) changes a lot, and regions where the change is less dramatic. We can then arbitrarily mark these regions as noise or regions of interests, background or foreground, and so on. These are the frequencies that make up the original image, and we have the power to separate them to make sense of the image and extrapolate interesting data.</p><div class="note" title="Note"><h3 class="title"><a id="note13"/>Note</h3><p>In an OpenCV context, there are a number of algorithms implemented that enable us to process images and make sense of the data contained in them, and these are also reimplemented in NumPy to make our life even easier. NumPy has a <a id="id139" class="indexterm"/>
<span class="strong"><strong>Fast Fourier Transform</strong></span> (<span class="strong"><strong>FFT</strong></span>) package, which contains the <code class="literal">fft2()</code> method. This method allows us to compute <a id="id140" class="indexterm"/>
<span class="strong"><strong>Discrete Fourier Transform</strong></span> (<span class="strong"><strong>DFT</strong></span>) of the image.</p></div><p>Let's examine the <a id="id141" class="indexterm"/>
<span class="strong"><strong>magnitude spectrum</strong></span> concept of an image using Fourier Transform. The magnitude spectrum of an image is another image, which gives a representation of the original image in terms of its changes: think of it as taking an image and dragging all the brightest pixels to the center. Then, you gradually work your way out to the border where all the darkest pixels have been pushed. Immediately, you will be able to see how many light and dark pixels are contained in your image and the percentage of their distribution.</p><p>The concept of Fourier Transform is the basis of many algorithms used for common image processing operations, such as edge detection or line and shape detection.</p><p>Before examining these in detail, let's take a look at two concepts that—in conjunction with the Fourier <a id="id142" class="indexterm"/>Transform—form the foundation of the aforementioned processing operations: high pass filters and low pass filters.</p><div class="section" title="High pass filter"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>High pass filter</h2></div></div></div><p>A <span class="strong"><strong>high pass filter</strong></span> (<span class="strong"><strong>HPF</strong></span>) is a filter that examines a region of an image and boosts the intensity of <a id="id143" class="indexterm"/>certain pixels based on the difference in the <a id="id144" class="indexterm"/>intensity with the surrounding pixels.</p><p>Take, for example, the following kernel:</p><div class="informalexample"><pre class="programlisting">[[0, -0.25, 0],
 [-0.25, 1, -0.25],
 [0, -0.25, 0]]</pre></div><div class="note" title="Note"><h3 class="title"><a id="note14"/>Note</h3><p>A <span class="strong"><strong>kernel</strong></span> is a set <a id="id145" class="indexterm"/>of weights that are applied to a region in a source image to generate a single pixel in the destination image. For example, a <code class="literal">ksize</code> of <code class="literal">7</code> implies that <code class="literal">49 (7 x 7)</code> source pixels are considered in generating each destination pixel. We can think of a kernel as a piece of frosted glass moving over the source image and letting through a diffused blend of the source's light.</p></div><p>After calculating the sum of differences of the intensities of the central pixel compared to all the immediate neighbors, the intensity of the central pixel will be boosted (or not) if a high level of changes are found. In other words, if a pixel stands out from the surrounding pixels, it will get boosted.</p><p>This is particularly effective in edge detection, where a common form of HPF called high boost filter is used.</p><p>Both high pass and low pass filters use a property called <code class="literal">radius</code>, which extends the area of the neighbors involved in the filter calculation.</p><p>Let's go through an example of an HPF:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np
from scipy import ndimage

kernel_3x3 = np.array([[-1, -1, -1],
                   [-1,  8, -1],
                   [-1, -1, -1]])

kernel_5x5 = np.array([[-1, -1, -1, -1, -1],
                       [-1,  1,  2,  1, -1],
                       [-1,  2,  4,  2, -1],
                       [-1,  1,  2,  1, -1],
                       [-1, -1, -1, -1, -1]])</pre></div><div class="note" title="Note"><h3 class="title"><a id="note15"/>Note</h3><p>Note that both filters sum up to <code class="literal">0</code>, the reason for this is explained in detail in the <span class="emphasis"><em>Edge detection</em></span> section.</p></div><div class="informalexample"><pre class="programlisting">img = cv2.imread("../images/color1_small.jpg", 0)

k3 = ndimage.convolve(img, kernel_3x3)
k5 = ndimage.convolve(img, kernel_5x5)

blurred = cv2.GaussianBlur(img, (11,11), 0)
g_hpf = img - blurred

cv2.imshow("3x3", k3)
cv2.imshow("5x5", k5)
cv2.imshow("g_hpf", g_hpf)
cv2.waitKey()
cv2.destroyAllWindows()</pre></div><p>After the initial imports, we define a <code class="literal">3x3</code> kernel and a <code class="literal">5x5</code> kernel, and then we load the image in grayscale. Normally, the majority of image processing is done with NumPy; however, in this particular case, we want to "convolve" an image with a given kernel and NumPy happens to only accept one-dimensional arrays.</p><p>This does not <a id="id146" class="indexterm"/>mean that the convolution of deep <a id="id147" class="indexterm"/>arrays can't be achieved with NumPy, just that it would be a bit complex. Instead, <code class="literal">ndimage</code> (which is a part of SciPy, so you should have it installed as per the instructions in <a class="link" title="Chapter 1. Setting Up OpenCV" href="part0014.xhtml#aid-DB7S2">Chapter 1</a>, <span class="emphasis"><em>Setting Up OpenCV</em></span>), makes this trivial, through its <code class="literal">convolve()</code> function, which supports the classic NumPy arrays that the <code class="literal">cv2</code> modules use to store images.</p><p>We apply two HPFs with the two convolution kernels we defined. Lastly, we also implement a differential method of obtaining a HPF by applying a low pass filter and calculating the difference with the original image. You will notice that the third method actually yields the best result, so let's also elaborate on low pass filters.</p></div><div class="section" title="Low pass filter"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Low pass filter</h2></div></div></div><p>If an HPF boosts <a id="id148" class="indexterm"/>the intensity of a pixel, given its difference <a id="id149" class="indexterm"/>with its neighbors, a <span class="strong"><strong>low pass filter</strong></span> (<span class="strong"><strong>LPF</strong></span>) will smoothen the pixel if the difference with the surrounding pixels is lower than a certain threshold. This is used in denoising and blurring. For example, one of the most popular blurring/smoothening <a id="id150" class="indexterm"/>filters, the Gaussian blur, is a low pass filter that attenuates the intensity of high frequency signals.</p></div></div>
<div class="section" title="Creating modules" id="aid-NQU21"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Creating modules</h1></div></div></div><p>As in the case of our <code class="literal">CaptureManager</code> and <code class="literal">WindowManager</code> classes, our filters should be reusable outside <a id="id151" class="indexterm"/>Cameo. Thus, we should separate the filters into their own Python module or file.</p><p>Let's create a file called <code class="literal">filters.py</code> in the same directory as <code class="literal">cameo.py</code>. We need the following <code class="literal">import</code> statements in <code class="literal">filters.py</code>:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy
import utils</pre></div><p>Let's also create a file called <code class="literal">utils.py</code> in the same directory. It should contain the following <code class="literal">import</code> statements:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy
import scipy.interpolate</pre></div><p>We will be adding filter functions and classes to <code class="literal">filters.py</code>, while more general-purpose math functions will go in <code class="literal">utils.py</code>.</p></div>
<div class="section" title="Edge detection" id="aid-OPEK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Edge detection</h1></div></div></div><p>Edges play a <a id="id152" class="indexterm"/>major role in both human and computer vision. We, as humans, can easily recognize many object types and their pose just by seeing a backlit silhouette or a rough sketch. Indeed, when art emphasizes edges and poses, it often seems to convey the idea of an archetype, such as Rodin's <span class="emphasis"><em>The Thinker</em></span> or Joe Shuster's <span class="emphasis"><em>Superman</em></span>. Software, too, can reason about edges, poses, and archetypes. We will discuss these kinds of reasonings in later chapters.</p><p>OpenCV provides many edge-finding filters, including <code class="literal">Laplacian()</code>, <code class="literal">Sobel()</code>, and <code class="literal">Scharr()</code>. These filters are supposed to turn non-edge regions to black while turning edge regions to white or saturated colors. However, they are prone to misidentifying noise as edges. This flaw can be mitigated by blurring an image before trying to find its edges. OpenCV also provides many blurring filters, including <code class="literal">blur()</code> (simple average), <code class="literal">medianBlur()</code>, and <code class="literal">GaussianBlur()</code>. The arguments for the edge-finding and blurring filters vary but always include <code class="literal">ksize</code>, an odd whole number that represents the width and height (in pixels) of a filter's kernel.</p><p>For blurring, let's use <code class="literal">medianBlur()</code>, which is effective in removing digital video noise, especially in color images. For edge-finding, let's use <code class="literal">Laplacian()</code>, which produces bold edge lines, especially in grayscale images. After applying <code class="literal">medianBlur()</code>, but before applying <code class="literal">Laplacian()</code>, we should convert the image from BGR to grayscale.</p><p>Once we have the result of <code class="literal">Laplacian()</code>, we can invert it to get black edges on a white background. Then, we can normalize it (so that its values range from 0 to 1) and multiply it with the source image to darken the edges. Let's implement this approach in <code class="literal">filters.py</code>:</p><div class="informalexample"><pre class="programlisting">def strokeEdges(src, dst, blurKsize = 7, edgeKsize = 5):
    if blurKsize &gt;= 3:
        blurredSrc = cv2.medianBlur(src, blurKsize)
        graySrc = cv2.cvtColor(blurredSrc, cv2.COLOR_BGR2GRAY)
    else:
        graySrc = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
    cv2.Laplacian(graySrc, cv2.CV_8U, graySrc, ksize = edgeKsize)
    normalizedInverseAlpha = (1.0 / 255) * (255 - graySrc)
    channels = cv2.split(src)
    for channel in channels:
        channel[:] = channel * normalizedInverseAlpha
    cv2.merge(channels, dst)</pre></div><p>Note that we allow kernel sizes to be specified as arguments for <code class="literal">strokeEdges()</code>. The <code class="literal">blurKsize</code> <a id="id153" class="indexterm"/>argument is used as <code class="literal">ksize</code> for <code class="literal">medianBlur()</code>, while <code class="literal">edgeKsize</code> is used as <code class="literal">ksize</code> for <code class="literal">Laplacian()</code>. With my webcams, I find that a <code class="literal">blurKsize</code> value of <code class="literal">7</code> and an <code class="literal">edgeKsize</code> value of <code class="literal">5</code> looks best. Unfortunately, <code class="literal">medianBlur()</code> is expensive with a large <code class="literal">ksize</code>, such as <code class="literal">7</code>.</p><div class="note" title="Note"><h3 class="title"><a id="tip06"/>Tip</h3><p>If you encounter performance problems when running <code class="literal">strokeEdges()</code>, try decreasing the <code class="literal">blurKsize</code> value. To turn off blur, set it to a value less than <code class="literal">3</code>.</p></div></div>
<div class="section" title="Custom kernels &#x2013; getting convoluted" id="aid-PNV61"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Custom kernels – getting convoluted</h1></div></div></div><p>As we have just seen, many of OpenCV's predefined filters use a kernel. Remember that a kernel is a set of weights, which determine how each output pixel is calculated from a neighborhood of input pixels. Another term for a kernel is a <span class="strong"><strong>convolution matrix</strong></span>. It mixes up or <a id="id154" class="indexterm"/>convolves the pixels in a region. Similarly, a kernel-based filter <a id="id155" class="indexterm"/>may be called a convolution filter.</p><p>OpenCV provides a very versatile <code class="literal">filter2D()</code> function, which applies any kernel or convolution matrix that we specify. To understand how to use this function, let's first learn the format of a convolution matrix. It is a 2D array with an odd number of rows and columns. The central element corresponds to a pixel of interest and the other elements correspond to the neighbors of this pixel. Each element contains an integer or floating point value, which is a weight that gets applied to an input pixel's value. Consider this example:</p><div class="informalexample"><pre class="programlisting">kernel = numpy.array([[-1, -1, -1],
                      [-1,  9, -1],
                      [-1, -1, -1]])</pre></div><p>Here, the pixel of interest has a weight of <code class="literal">9</code> and its immediate neighbors each have a weight of <code class="literal">-1</code>. For the pixel of interest, the output color will be nine times its input color minus the input colors of all eight adjacent pixels. If the pixel of interest is already a bit different from its neighbors, this difference becomes intensified. The effect is that the image looks <span class="emphasis"><em>sharper</em></span> as the contrast between the neighbors is increased.</p><p>Continuing our example, we can apply this convolution matrix to a source and destination image, respectively, as follows:</p><div class="informalexample"><pre class="programlisting">cv2.filter2D(src, -1, kernel, dst)</pre></div><p>The second argument specifies the per-channel depth of the destination image (such as <code class="literal">cv2.CV_8U</code> for 8 bits per channel). A negative value (as used here) means that the destination image has the same depth as the source image.</p><div class="note" title="Note"><h3 class="title"><a id="note16"/>Note</h3><p>For color images, note that <code class="literal">filter2D()</code> applies the kernel equally to each channel. To use different kernels on different channels, we would also have to use the <code class="literal">split()</code> and <code class="literal">merge()</code> functions.</p></div><p>Based on this simple example, let's add two classes to <code class="literal">filters.py</code>. One class, <code class="literal">VConvolutionFilter</code>, will represent a convolution filter in general. A subclass, <code class="literal">SharpenFilter</code>, will <a id="id156" class="indexterm"/>represent our sharpening filter specifically. Let's edit <code class="literal">filters.py</code> to implement these two new classes as follows:</p><div class="informalexample"><pre class="programlisting">class VConvolutionFilter(object):
    """A filter that applies a convolution to V (or all of BGR)."""
    
    def __init__(self, kernel):
        self._kernel = kernel
    
    def apply(self, src, dst):
        """Apply the filter with a BGR or gray source/destination."""
        cv2.filter2D(src, -1, self._kernel, dst)

class SharpenFilter(VConvolutionFilter):
    """A sharpen filter with a 1-pixel radius."""
    
    def __init__(self):
        kernel = numpy.array([[-1, -1, -1],
                              [-1,  9, -1],
                              [-1, -1, -1]])
        VConvolutionFilter.__init__(self, kernel)</pre></div><p>Note that the weights sum up to <code class="literal">1</code>. This should be the case whenever we want to leave the image's overall brightness unchanged. If we modify a sharpening kernel slightly so that its weights sum up to <code class="literal">0</code> instead, we have an edge detection kernel that turns edges white and non-edges black. For example, let's add the following edge detection filter to <code class="literal">filters.py</code>:</p><div class="informalexample"><pre class="programlisting">class FindEdgesFilter(VConvolutionFilter):
    """An edge-finding filter with a 1-pixel radius."""
    
    def __init__(self):
        kernel = numpy.array([[-1, -1, -1],
                              [-1,  8, -1],
                              [-1, -1, -1]])
        VConvolutionFilter.__init__(self, kernel)</pre></div><p>Next, let's make a blur filter. Generally, for a blur effect, the weights should sum up to <code class="literal">1</code> and should be positive throughout the neighborhood. For example, we can take a simple average of the neighborhood as follows:</p><div class="informalexample"><pre class="programlisting">class BlurFilter(VConvolutionFilter):
    """A blur filter with a 2-pixel radius."""
    
    def __init__(self):
        kernel = numpy.array([[0.04, 0.04, 0.04, 0.04, 0.04],
                              [0.04, 0.04, 0.04, 0.04, 0.04],
                              [0.04, 0.04, 0.04, 0.04, 0.04],
                              [0.04, 0.04, 0.04, 0.04, 0.04],
                              [0.04, 0.04, 0.04, 0.04, 0.04]])
        VConvolutionFilter.__init__(self, kernel)</pre></div><p>Our sharpening, edge <a id="id157" class="indexterm"/>detection, and blur filters use kernels that are highly symmetric. Sometimes, though, kernels with less symmetry produce an interesting effect. Let's consider a kernel that blurs on one side (with positive weights) and sharpens on the other (with negative weights). It will produce a ridged or <span class="emphasis"><em>embossed</em></span> effect. Here is an implementation that we can add to <code class="literal">filters.py</code>:</p><div class="informalexample"><pre class="programlisting">class EmbossFilter(VConvolutionFilter):
    """An emboss filter with a 1-pixel radius."""
    
    def __init__(self):
        kernel = numpy.array([[-2, -1, 0],
                              [-1,  1, 1],
                              [ 0,  1, 2]])
        VConvolutionFilter.__init__(self, kernel)</pre></div><p>This set of custom <a id="id158" class="indexterm"/>convolution filters is very basic. Indeed, it is more basic than OpenCV's ready-made set of filters. However, with a bit of experimentation, you should be able to write your own kernels that produce a unique look.</p></div>
<div class="section" title="Modifying the application" id="aid-QMFO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Modifying the application</h1></div></div></div><p>Now that we have <a id="id159" class="indexterm"/>high-level functions and classes for several filters, it is trivial to apply any of them to the captured frames in Cameo. Let's edit <code class="literal">cameo.py</code> and add the lines that appear in bold face in the following excerpt:</p><div class="informalexample"><pre class="programlisting">import cv2
<span class="strong"><strong>import filters</strong></span>
from managers import WindowManager, CaptureManager

class Cameo(object):
    
    def __init__(self):
        self._windowManager = WindowManager('Cameo',
                                            self.onKeypress)
        self._captureManager = CaptureManager(
            cv2.VideoCapture(0), self._windowManager, True)
<span class="strong"><strong>        self._curveFilter = filters.BGRPortraCurveFilter()</strong></span>
    
    def run(self):
        """Run the main loop."""
        self._windowManager.createWindow()
        while self._windowManager.isWindowCreated:
            self._captureManager.enterFrame()
            frame = self._captureManager.frame


            
<span class="strong"><strong>            filters.strokeEdges(frame, frame)</strong></span>
<span class="strong"><strong>            self._curveFilter.apply(frame, frame)</strong></span>
            
            self._captureManager.exitFrame()
            self._windowManager.processEvents()
    
    # ... The rest is the same as in Chapter 2.</pre></div><p>Here, I have chosen to apply two effects: stroking the edges and emulating Portra film colors. Feel free to modify the code to apply any filters you like.</p><p>Here is a screenshot <a id="id160" class="indexterm"/>from Cameo with stroked edges and Portra-like colors:</p><div class="mediaobject"><img src="../Images/image00194.jpeg" alt="Modifying the application"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Edge detection with Canny" id="aid-RL0A1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Edge detection with Canny</h1></div></div></div><p>OpenCV <a id="id161" class="indexterm"/>also offers a very handy function called Canny (after the <a id="id162" class="indexterm"/>algorithm's inventor, John F. Canny), which is very popular not only because of its effectiveness, but also the simplicity of its implementation in an OpenCV program, as it is a one-liner:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread("../images/statue_small.jpg", 0)
cv2.imwrite("canny.jpg", cv2.Canny(img, 200, 300))
cv2.imshow("canny", cv2.imread("canny.jpg"))
cv2.waitKey()
cv2.destroyAllWindows()</pre></div><p>The result is a very clear identification of the edges:</p><div class="mediaobject"><img src="../Images/image00195.jpeg" alt="Edge detection with Canny"/></div><p style="clear:both; height: 1em;"> </p><p>The Canny edge detection algorithm is quite complex but also interesting: it's a five-step process that denoises the image with a Gaussian filter, calculates gradients, applies <span class="strong"><strong>non maximum </strong></span>
<a id="id163" class="indexterm"/>
<span class="strong"><strong>suppression</strong></span> (<span class="strong"><strong>NMS</strong></span>) on edges, a double <a id="id164" class="indexterm"/>threshold on all the detected edges to eliminate false <a id="id165" class="indexterm"/>positives, and, lastly, analyzes all the edges and their connection to each other to keep the real edges and discard the weak ones.</p></div>
<div class="section" title="Contour detection" id="aid-SJGS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Contour detection</h1></div></div></div><p>Another <a id="id166" class="indexterm"/>vital task in computer vision is contour detection, not only because of the obvious aspect of detecting contours of subjects contained in an image or video frame, but because of the derivative operations connected with identifying contours.</p><p>These operations are, namely, computing bounding polygons, approximating shapes, and generally calculating regions of interest, which considerably simplify interaction with image data because a rectangular region with NumPy is easily defined with an array slice. We will be using this technique a lot when exploring the concept of object detection (including faces) and object tracking.</p><p>Let's go in order and familiarize ourselves with the API first with an example:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

img = np.zeros((200, 200), dtype=np.uint8)
img[50:150, 50:150] = 255

ret, thresh = cv2.threshold(img, 127, 255, 0)
image, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
img = cv2.drawContours(color, contours, -1, (0,255,0), 2)
cv2.imshow("contours", color)
cv2.waitKey()
cv2.destroyAllWindows()</pre></div><p>Firstly, we create an empty black image that is 200x200 pixels in size. Then, we place a white square in the center of it utilizing ndarray's ability to assign values on a slice.</p><p>We then threshold the image, and call the <code class="literal">findContours()</code> function. This function has three parameters: the input image, hierarchy type, and the contour approximation method. There are a number of aspects that are of particular interest in this function:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The function modifies the input image, so it would be advisable to use a copy of the original image (for example, by passing <code class="literal">img.copy()</code>).</li><li class="listitem">Secondly, the <a id="id167" class="indexterm"/>hierarchy tree returned by the function is quite important: <code class="literal">cv2.RETR_TREE</code> will retrieve the entire hierarchy of contours in the image, enabling you to establish "relationships" between contours. If you only want to retrieve the most external contours, use <code class="literal">cv2.RETR_EXTERNAL</code>. This is particularly useful when you want to eliminate contours that are entirely contained in other contours (for example, in a vast majority of cases, you won't need to detect an object within another object of the same type).</li></ul></div><p>The <code class="literal">findContours</code> function returns three elements: the modified image, contours, and their hierarchy. We use the contours to draw on the color version of the image (so that we can draw contours in green) and eventually display it.</p><p>The result is a white square with its contour drawn in green. Spartan, but effective in demonstrating the concept! Let's move on to more meaningful examples.</p></div>
<div class="section" title="Contours &#x2013; bounding box, minimum area rectangle, and minimum enclosing circle" id="aid-TI1E1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Contours – bounding box, minimum area rectangle, and minimum enclosing circle</h1></div></div></div><p>Finding the <a id="id168" class="indexterm"/>contours of a square is a simple task; irregular, skewed, and <a id="id169" class="indexterm"/>rotated shapes bring the best out <a id="id170" class="indexterm"/>of the <code class="literal">cv2.findContours</code> utility function of <a id="id171" class="indexterm"/>OpenCV. Let's take a look at the following image:</p><div class="mediaobject"><img src="../Images/image00196.jpeg" alt="Contours – bounding box, minimum area rectangle, and minimum enclosing circle"/></div><p style="clear:both; height: 1em;"> </p><p>In a real-life application, we would be most interested in determining the bounding box of the subject, its minimum enclosing rectangle, and its circle. The <code class="literal">cv2.findContours</code> function in <a id="id172" class="indexterm"/>conjunction with a few other OpenCV utilities <a id="id173" class="indexterm"/>makes this very easy to accomplish:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

img = cv2.pyrDown(cv2.imread("hammer.jpg", cv2.IMREAD_UNCHANGED))

ret, thresh = cv2.threshold(cv2.cvtColor(img.copy(), cv2.COLOR_BGR2GRAY) , 127, 255, cv2.THRESH_BINARY)
image, contours, hier = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

for c in contours:
  # find bounding box coordinates
  x,y,w,h = cv2.boundingRect(c)
  cv2.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 2)

  # find minimum area
  rect = cv2.minAreaRect(c)
  # calculate coordinates of the minimum area rectangle
  box = cv2.boxPoints(rect)
  # normalize coordinates to integers
  box = np.int0(box)
  # draw contours
  cv2.drawContours(img, [box], 0, (0,0, 255), 3)
  
  # calculate center and radius of minimum enclosing circle
  (x,y),radius = cv2.minEnclosingCircle(c)
  # cast to integers
  center = (int(x),int(y))
  radius = int(radius)
  # draw the circle
  img = cv2.circle(img,center,radius,(0,255,0),2)

cv2.drawContours(img, contours, -1, (255, 0, 0), 1)
cv2.imshow("contours", img)</pre></div><p>After the initial imports, we load the image, and then apply a binary threshold on a grayscale version of the original image. By doing this, we operate all find-contour calculations on a <a id="id174" class="indexterm"/>grayscale copy, but we draw on the original so that <a id="id175" class="indexterm"/>we can utilize color information.</p><p>Firstly, let's <a id="id176" class="indexterm"/>calculate a simple bounding box:</p><div class="informalexample"><pre class="programlisting">x,y,w,h = cv2.boundingRect(c)</pre></div><p>This is a pretty straightforward conversion of contour information to the <code class="literal">(x, y)</code> coordinates, plus the height and width of the rectangle. Drawing this rectangle is an easy task and can be done using this code:</p><div class="informalexample"><pre class="programlisting">  cv2.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 2)</pre></div><p>Secondly, let's calculate the minimum area enclosing the subject:</p><div class="informalexample"><pre class="programlisting">rect = cv2.minAreaRect(c)
box = cv2.boxPoints(rect)
  box = np.int0(box)</pre></div><p>The mechanism used here is particularly interesting: OpenCV does not have a function to calculate the coordinates of the minimum rectangle vertexes directly from the contour information. Instead, we calculate the minimum rectangle area, and then calculate the vertexes of this rectangle. Note that the calculated vertexes are floats, but pixels are accessed with integers (you can't access a "portion" of a pixel), so we need to operate this conversion. Next, we draw the box, which gives us the perfect opportunity to introduce the <code class="literal">cv2.drawContours</code> function:</p><div class="informalexample"><pre class="programlisting">cv2.drawContours(img, [box], 0, (0,0, 255), 3)</pre></div><p>Firstly, this function—like all drawing functions—modifies the original image. Secondly, it takes an array of contours in its second parameter, so you can draw a number of contours in a single operation. Therefore, if you have a single set of points representing a contour polygon, you need to wrap these points into an array, exactly like we did with our box in the preceding example. The third parameter of this function specifies the index of the contours array that we want to draw: a value of <code class="literal">-1</code> will draw all contours; otherwise, a contour at the specified index in the contours array (the second parameter) will be drawn.</p><p>Most drawing functions take the color of the drawing and its thickness as the last two parameters.</p><p>The last bounding <a id="id177" class="indexterm"/>contour we're going to examine is the minimum enclosing circle:</p><div class="informalexample"><pre class="programlisting">  (x,y),radius = cv2.minEnclosingCircle(c)
  center = (int(x),int(y))
  radius = int(radius)
  img = cv2.circle(img,center,radius,(0,255,0),2)</pre></div><p>The only <a id="id178" class="indexterm"/>peculiarity of the <code class="literal">cv2.minEnclosingCircle</code> function is that it returns a two-element tuple, of which the first element is a <a id="id179" class="indexterm"/>tuple itself, representing the coordinates of the circle's center, and the second element is the radius of this circle. After converting all these values to integers, drawing the circle is quite a trivial operation.</p><p>The final result on the original image looks like this:</p><div class="mediaobject"><img src="../Images/image00197.jpeg" alt="Contours – bounding box, minimum area rectangle, and minimum enclosing circle"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Contours &#x2013; convex contours and the Douglas-Peucker algorithm" id="aid-UGI01"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Contours – convex contours and the Douglas-Peucker algorithm</h1></div></div></div><p>Most of the <a id="id180" class="indexterm"/>time, when working with contours, subjects will <a id="id181" class="indexterm"/>have the most diverse shapes, including convex <a id="id182" class="indexterm"/>ones. A convex shape is one where there are two points <a id="id183" class="indexterm"/>within this shape whose connecting line goes outside the perimeter of the shape itself.</p><p>The first facility that OpenCV offers to calculate the approximate bounding polygon of a shape is <code class="literal">cv2.approxPolyDP</code>. This function takes three parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">A contour</li><li class="listitem">An epsilon value representing the maximum discrepancy between the original contour and the approximated polygon (the lower the value, the closer the approximated value will be to the original contour)</li><li class="listitem">A Boolean flag signifying that the polygon is closed</li></ul></div><p>The epsilon value is of vital importance to obtain a useful contour, so let's understand what it represents. An epsilon is the maximum difference between the approximated polygon's perimeter and the original contour's perimeter. The lower this difference is, the more the approximated polygon will be similar to the original contour.</p><p>You may ask yourself why we need an approximate polygon when we have a contour that is already a precise representation. The answer to this is that a polygon is a set of straight lines, and the importance of being able to define polygons in a region for further manipulation and processing is paramount in many computer vision tasks.</p><p>Now that we know <a id="id184" class="indexterm"/>what an epsilon is, we need to obtain contour <a id="id185" class="indexterm"/>perimeter information as a reference value. This <a id="id186" class="indexterm"/>is obtained with the <code class="literal">cv2.arcLength</code> function of OpenCV:</p><div class="informalexample"><pre class="programlisting">epsilon = 0.01 * cv2.arcLength(cnt, True)
approx = cv2.approxPolyDP(cnt, epsilon, True)</pre></div><p>Effectively, we're <a id="id187" class="indexterm"/>instructing OpenCV to calculate an approximated polygon whose perimeter can only differ from the original contour in an epsilon ratio.</p><p>OpenCV also offers a <code class="literal">cv2.convexHull</code> function to obtain processed contour information for convex shapes and this is a straightforward one-line expression:</p><div class="informalexample"><pre class="programlisting">hull = cv2.convexHull(cnt)</pre></div><p>Let's combine the original contour, approximated polygon contour, and the convex hull in one image to observe the difference between them. To simplify things, I've applied the contours to a black image so that the original subject is not visible but its contours are:</p><div class="mediaobject"><img src="../Images/image00198.jpeg" alt="Contours – convex contours and the Douglas-Peucker algorithm"/></div><p style="clear:both; height: 1em;"> </p><p>As you can see, the <a id="id188" class="indexterm"/>convex hull surrounds the entire subject, the <a id="id189" class="indexterm"/>approximated polygon is the innermost <a id="id190" class="indexterm"/>polygon shape, and in between the two is the original contour, mainly <a id="id191" class="indexterm"/>composed of arcs.</p></div>
<div class="section" title="Line and circle detection" id="aid-VF2I1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Line and circle detection</h1></div></div></div><p>Detecting edges <a id="id192" class="indexterm"/>and contours are not only common and important tasks, they also constitute the basis for other complex operations. Lines and shape detection go hand in hand with edge and contour detection, so let's examine how OpenCV implements these.</p><p>The theory behind <a id="id193" class="indexterm"/>lines and shape detection has its foundation in a technique called the Hough transform, invented by Richard Duda and Peter Hart, who extended (generalized) the work done by Paul Hough in the early 1960s.</p><p>Let's take a look at OpenCV's API for the Hough transforms.</p><div class="section" title="Line detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Line detection</h2></div></div></div><p>First of all, let's <a id="id194" class="indexterm"/>detect some lines, which is done with the <code class="literal">HoughLines</code> and <code class="literal">HoughLinesP</code> functions. The only difference between the two functions is that one uses the standard Hough transform, and the second uses the probabilistic Hough transform (hence <code class="literal">P</code> in the name).</p><p>The probabilistic version is so-called because it only analyzes a subset of points and estimates the probability of these points all belonging to the same line. This implementation is an optimized version of the standard Hough transform, and in this case, it's less computationally intensive and executes faster.</p><p>Let's take a look at a very simple example:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

img = cv2.imread('lines.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray,50,120)
minLineLength = 20
maxLineGap = 5
lines = cv2.HoughLinesP(edges,1,np.pi/180,100,minLineLength,maxLineGap)
for x1,y1,x2,y2 in lines[0]:
  cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)

cv2.imshow("edges", edges)
cv2.imshow("lines", img)
cv2.waitKey()
cv2.destroyAllWindows()</pre></div><p>The crucial point of this simple script—aside from the <code class="literal">HoughLines</code> function call—is the setting of minimum line length (shorter lines will be discarded) and the maximum line gap, which is the maximum size of a gap in a line before the two segments start being considered as separate lines.</p><p>Also note that the <a id="id195" class="indexterm"/>
<code class="literal">HoughLines</code> function takes a single channel binary image, processed through the Canny edge detection filter. Canny is not a strict requirement, however; an image that's been denoised and only represents edges, is the ideal source for a Hough transform, so you will find this to be a common practice.</p><p>The parameters of <code class="literal">HoughLinesP</code> are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The image we want to process.</li><li class="listitem">The geometrical representations of the lines, <code class="literal">rho</code> and <code class="literal">theta</code>, which are usually <code class="literal">1</code> and <code class="literal">np.pi/180</code>.</li><li class="listitem">The threshold, which represents the threshold below which a line is discarded. The Hough transform works with a system of bins and votes, with each bin representing a line, so any line with a minimum of the <code class="literal">&lt;threshold&gt;</code> votes is retained, the rest discarded.</li><li class="listitem"><code class="literal">MinLineLength</code> and <code class="literal">MaxLineGap</code>, which we mentioned previously.</li></ul></div></div><div class="section" title="Circle detection"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/>Circle detection</h2></div></div></div><p>OpenCV <a id="id196" class="indexterm"/>also has a function for detecting circles, called <code class="literal">HoughCircles</code>. It works in a very similar fashion to <code class="literal">HoughLines</code>, but where <code class="literal">minLineLength</code> and <code class="literal">maxLineGap</code> were the parameters to discard or retain lines, <code class="literal">HoughCircles</code> has a minimum distance between circles' centers, minimum, and maximum radius of the circles. Here's the obligatory example:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

planets = cv2.imread('planet_glow.jpg')
gray_img = cv2.cvtColor(planets, cv2.COLOR_BGR2GRAY)
img = cv2.medianBlur(gray_img, 5)
cimg = cv2.cvtColor(img,cv2.COLOR_GRAY2BGR)

circles = cv2.HoughCircles(img,cv2.HOUGH_GRADIENT,1,120,
                            param1=100,param2=30,minRadius=0,maxRadius=0)

circles = np.uint16(np.around(circles))

for i in circles[0,:]:
    # draw the outer circle
    cv2.circle(planets,(i[0],i[1]),i[2],(0,255,0),2)
    # draw the center of the circle
    cv2.circle(planets,(i[0],i[1]),2,(0,0,255),3)

cv2.imwrite("planets_circles.jpg", planets)
cv2.imshow("HoughCirlces", planets)
cv2.waitKey()
cv2.destroyAllWindows()</pre></div><p>Here's a visual representation of the result:</p><div class="mediaobject"><img src="../Images/image00199.jpeg" alt="Circle detection"/></div><p style="clear:both; height: 1em;"> </p></div></div>
<div class="section" title="Detecting shapes" id="aid-10DJ41"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Detecting shapes</h1></div></div></div><p>The detection of <a id="id197" class="indexterm"/>shapes with the Hough transform is limited to circles; however, we already implicitly explored detecting shapes of any kind, specifically when we talked about <code class="literal">approxPolyDP</code>. This function allows the approximation of polygons, so if your image contains polygons, they will be quite accurately detected, combining the usage of <code class="literal">cv2.findContours</code> and <code class="literal">cv2.approxPolyDP</code>.</p></div>
<div class="section" title="Summary" id="aid-11C3M1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Summary</h1></div></div></div><p>At this point, you should have gained a good understanding of color spaces, Fourier Transform, and the several kinds of filters made available by OpenCV to process images.</p><p>You should also be proficient in detecting edges, lines, circles, and shapes in general. Additionally, you should be able to find contours and exploit the information they provide about the subjects contained in an image. These concepts will serve as the ideal background to explore the topics in the next chapter.</p></div></body></html>