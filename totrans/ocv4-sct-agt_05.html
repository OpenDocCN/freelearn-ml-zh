<html><head></head><body>
        

                            
                    <h1 class="header-title">Training a Smart Alarm to Recognize the Villain and His Cat</h1>
                
            
            
                
<p>"The naming of cats is a difficult matter."<br/>
                                              – T. S. Eliot, Old Possum's Book of Practical Cats (1939)</p>
<p>"Blofeld: I've taught you to love chickens, to love their flesh, their voice."<br/>
                                                          – On Her Majesty's Secret Service (1969)</p>
<p>Imagine that the date is January 1, 2015. The balance of world power is shifting again. Lithuania is joining the eurozone. Russia, Belarus, Armenia, Kazakhstan, and Kyrgyzstan are forming the Eurasian Economic Union. The first edition of <em>OpenCV for Secret Agents</em> is going to the printers. On this day, if you saw Ernst Stavro Blofeld, would you recognize him?</p>
<p>Let me remind you that Blofeld, as the number one man in the <strong>Special Executive for Counterintelligence, Terrorism, Revenge, and Extortion</strong> (<strong>SPECTRE</strong>), is a super-villain who eludes James Bond countless times before being written out of the movies due to an intellectual property dispute. Blofeld last appears as an anonymous character in the intro sequence of <em>For Your Eyes Only</em> (1981), where we see him fall from a helicopter and down a factory's smokestack as he shouts, <em>Mr. Boooooooooond!</em></p>
<p>Despite this dramatic exit, the evidence of Blofeld's death is unclear. After all, Blofeld is a notoriously difficult man to recognize. His face is seldom caught on camera. As early as the 1960s, he was using plastic surgery to change his identity and to turn his henchmen into lookalikes of himself. Half a century later, we must ask, is Blofeld a dead man or is he just made-over, perhaps as a beautiful actress in a Colombian telenovela?</p>
<p>One thing is certain. If Blofeld is alive, he is accompanied by a blue-eyed, white Angora cat (preserved by a veterinary miracle or taxidermy). Patting this cat is Blofeld's telltale habit in every movie. His face may be different but his lap cat is the same. We last see the cat jumping out of Blofeld's lap just before the fateful helicopter ride.</p>
<p>Some commentators have noted a resemblance between Blofeld and Dr. Evil, the nemesis of Austin Powers. However, by comparing the respective lap cats, we can prove that these two villains are not the same.</p>
<p>The moral is that two approaches to recognition are better than one. Though we cannot see the man's face, we should not lose sight of his cat.</p>
<p>Of course, the suspense ended on October 26, 2015, when Blofeld made his comeback in <em>Spectre</em>. He and the cat looked remarkably unchanged after 34 years.</p>
<p>To automate the search for villains and their cats, we are going to develop a desktop or Raspberry Pi application called <kbd>Angora Blue</kbd> (an innocent-sounding code name that alludes to the blue eyes of Blofeld's cat). <kbd>Angora Blue</kbd> will send us an email alert when it recognizes a specified villain or a specified cat with a certain level of confidence. We will also develop a GUI app called <kbd>Interactive Recognizer</kbd>, which will train Angora Blue's recognition model based on camera images and names that we provide interactively. To distinguish faces from the background, <kbd>Interactive Recognizer</kbd> depends on a human face detection model that comes with OpenCV and a cat face detection model that we are going to train using an original script and third-party image databases.</p>
<p>Perhaps you have heard that OpenCV comes with a set of pretrained cat face detectors. This is true! I originally developed them for this book's first edition; I contributed them to OpenCV, and I have maintained them with improvements. This chapter covers the process that I used to train the latest version of these official OpenCV cat-face detectors.</p>
<p>This is a big chapter, but it is rewarding because you will learn a process that applies to detecting and recognizing any kind of animal face, and even any object!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter's project has the following software dependencies:</p>
<ul>
<li><strong>A Python environment with the following modules</strong>: OpenCV (including opencv_contrib), NumPy, SciPy, Requests, wxPython, and optionally PyInstaller</li>
</ul>
<p>Setup instructions are covered in <a href="e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml">Chapter 1</a>, <em>Preparing for the Mission</em>. Refer to the setup instructions for any version requirements. Basic instructions for running Python code are covered in <a href="c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml">Appendix C</a>, <em>Running with Snakes (or, First Steps with Python)</em>.</p>
<p>The completed project for this chapter can be found in this book's GitHub repository, <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition</a>, in the <kbd>Chapter003</kbd> folder.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding machine learning in general</h1>
                
            
            
                
<p>Our work throughout this chapter builds on the techniques of machine learning, meaning that the software makes predictions or decisions based on statistical models. Particularly, our approach is one of supervised learning, meaning that we (programmers and users) provide the software with examples of data and correct responses. The software creates the statistical model to extrapolate from these examples. The human provided examples are referred to as reference data or training data (or reference images or training images in the context of computer vision). Conversely, the software's extrapolations pertain to test data (or test images or scenes in the context of computer vision).</p>
<p>Supervised learning is much like the flashcard pedagogy used in early childhood education. The teacher shows the child a series of pictures (training images) and says,</p>
<p>"This is a cow. Moo! This is a horse. Neigh!"</p>
<p>Then, on a field trip to a farm (a scene), the child can hopefully distinguish between a horse and a cow. However, I must confess that I once mistook a horse for a cow, and I was teased about this misclassification for many years thereafter.</p>
<p>Apart from supervised learning, which is widely used in problems of vision and semantics, there are two other broad approaches to machine learning—unsupervised learning and reinforcement learning. <strong>Unsupervised learning</strong> requires the software to find some structure, such as clusters, in data where no meaning or correct examples are assigned by a human. Analyzing biological structures, such as genomes, is a common problem for unsupervised learning. On the other hand, <strong>reinforcement learning</strong> requires the software to experimentally optimize a solution to some sequence of problems, where a human assigns the final goal but the software must set intermediate goals. Piloting a vehicle and playing a game are common problems for reinforcement learning.</p>
<p>Besides being a computer vision library, OpenCV offers a general purpose machine learning module that can process any kind of data, not necessarily images. For more information on this module and the underlying machine learning concepts, see the <em>Machine Learning</em> section of the official OpenCV-Python tutorials at <a href="https://docs.opencv.org/4.0.0-beta/d6/de2/tutorial_py_table_of_contents_ml.html">https://docs.opencv.org/4.0.0-beta/d6/de2/tutorial_py_table_of_contents_ml.html</a>. Meanwhile, our chapter proceeds with more specialized machine learning functionality and concepts that OpenCV users often apply to face detection and recognition.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the Interactive Recognizer app</h1>
                
            
            
                
<p>Let's begin this project with the middle layer, the <kbd>Interactive Recognizer</kbd> app, in order to see how all layers connect. Like Luxocator from the <a href="4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml">Chapter 2</a>, <em>Searching for Luxury Accommodations Worldwide,</em> project, <kbd>Interactive Recognizer</kbd> is a GUI app built with wxPython. Refer to the following screenshot, featuring one of my colleagues, Chief Science Officer Sanibel San Delphinium Andromeda, high priestess of the Numm:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-195 image-border" src="img/c3363886-0e67-42c2-bdcd-a2a3289e91ed.png" style="width:92.08em;height:57.50em;"/></p>
<p>The app uses a face detection model, which is loaded from disk, and it maintains a face recognition model, which is saved to disk and later loaded back from disk. The user may specify the identity of any detected face, and this input is added to the face recognition model. A detection result is shown by outlining the face in the video feed, while a recognition result is shown by displaying the name of the face in the text below. To elaborate, we may say that the app has the following flow of execution:</p>
<ol>
<li>Load a face detection model from file. The role of the detection model is to distinguish faces from the background.</li>
</ol>
<ol start="2">
<li>Load a face recognition model from file if any such model was saved during a previous run of <kbd>Interactive Recognizer</kbd>. Otherwise, if there is no such model to load, create a new one. The role of the recognition model is to distinguish faces of different individuals from each other.</li>
<li>Capture and display a live video from a camera.</li>
<li>For each frame of video, detect the largest face, if any. If a face is detected:</li>
</ol>
<ol>
<li style="padding-left: 60px">Draw a rectangle around the face.</li>
<li style="padding-left: 60px">Permit the user to enter the face's identity as a short string (up to four characters), such as <kbd>Joe</kbd> or <kbd>Puss</kbd>. When the user hits the Add to Model button, train the model to recognize the face as whomever the user specified (<kbd>Joe</kbd>, <kbd>Puss</kbd>, or another identity).</li>
<li style="padding-left: 60px">If the recognition model is trained for at least one face, display the recognizer's prediction for the current face—that is, display the most probable identity of the current face, according to the recognizer. Also, display a measure of distance (non-confidence) for this prediction.</li>
</ol>
<p> </p>
<ol start="5">
<li>If the recognition model is trained for at least one face, permit the user to hit the Clear Model button to delete the model (including any version saved to file) and create a new one.</li>
<li>On exit, if the recognition model is trained for at least one face, save the model to file so that it can be loaded in subsequent runs of <kbd>Interactive Recognizer</kbd> and <kbd>Angora Blue</kbd>.</li>
</ol>
<div><div><p>We could generalize by using the term object instead of face. Depending on the models that it loads, <kbd>Interactive Recognizer</kbd> could detect and recognize any kind of object, not necessarily faces.</p>
</div>
</div>
<p>We use a type of detection model called a <strong>Haar cascade</strong> and a type of recognition model called <strong>Local Binary Patterns</strong> (<strong>LBP</strong>) or <strong>Local Binary Pattern Histograms</strong> (<strong>LBPH</strong>). Alternatively, we may use LBPH for both detection and recognition. As detection models, LBP cascades are faster but generally less reliable, compared to Haar cascades. OpenCV comes with some Haar cascade and LBP cascade files, including several face detection models. Command-line tools for generating such files are also included with OpenCV. The APIs offer high-level classes for loading and using Haar or LBP cascades and for loading, saving, training, and using LBPH recognition models. Let's look at the basic concepts of these models.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding Haar cascades and LBPH</h1>
                
            
            
                
<p>"Cookie Monster: Hey, you know what? A round cookie with one bite out of it looks like a C. A round donut with one bite out of it also looks like a C! But it is not as good as a cookie. Oh, and the moon sometimes looks like a C! But you can't eat that."<br/>
                                                                              – "C is for Cookie," Sesame Street</p>
<p>Think about cloud-watching. If you lie on the ground and look up at the clouds, maybe you imagine that one cloud is shaped like a mound of mashed potatoes on a plate. If you board an airplane and fly to this cloud, you will still see some resemblance between the cloud's surface and the fluffy, lumpy texture of hearty mashed potatoes. However, if you could slice off a piece of cloud and examine it under a microscope, you might see ice crystals that do not resemble the microscopic structure of mashed potatoes at all.</p>
<p>Similarly, in an image made up of pixels, a person or a computer vision algorithm can see many distinctive shapes or patterns, partly depending on the level of magnification. During the creation of a Haar cascade, various parts of the image are cropped and/or scaled so that we consider only a few pixels at a time (though these pixels may represent any level of magnification). This sample of the image is called a <strong>window</strong>. We subtract some of the grayscale pixel values from others in order to measure the window's similarity to certain common shapes where a dark region meets a light region. Examples include an edge, a corner, or a thin line, as shown in the following diagram. If a window has a high similarity to one of these archetypes, it may be selected as a <strong>feature</strong>. We expect to find similar features at similar positions and magnifications relative to each other, across all images of the same subject:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-196 image-border" src="img/7751bc20-2862-456c-a467-0e2a24720d46.png" style="width:18.42em;height:15.50em;"/></p>
<p>Not all features are equally significant. Across a set of images, we can see whether a feature is truly typical of images that include our subject (the <strong>positive training set</strong>) and atypical of images that exclude our subject (the <strong>negative training set</strong>). We give features a different rank or <strong>stage</strong> depending on how well they distinguish subjects from non-subjects. Together, a set of stages form a <strong>cascade</strong> or a series of comparison criteria. Every stage must be passed in order to reach a positive detection result. Conversely, a negative detection result can be reached in fewer stages, perhaps only a single stage (an important optimization). Like the training images, scenes are examined through various windows, and we may end up detecting multiple subjects in one scene.</p>
<div><p>For more information about Haar cascades in OpenCV, see the official documentation at <a href="https://docs.opencv.org/4.0.0-beta/d7/d8b/tutorial_py_face_detection.html">https://docs.opencv.org/4.0.0-beta/d7/d8b/tutorial_py_face_detection.html</a>.</p>
</div>
<p>An LBPH model, as the name suggests, is based on a kind of histogram. For each pixel in a window, we note whether each neighboring pixel in a certain radius is brighter or darker. Our histogram counts the darker pixels in each neighboring position. For example, suppose a window contains the following two neighborhoods of a one-pixel radius:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/30c8be1f-f92e-4085-9727-4b9f9c797077.png" style="width:13.83em;height:10.67em;"/></p>
<p>Counting these two neighborhoods (and not yet counting other neighborhoods in the window), our histogram can be visualized like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/cefbc6c1-db29-43dd-ad5f-cea6c565ee37.png" style="width:13.25em;height:4.33em;"/></p>
<p>If we compute the LBPH of multiple sets of reference images for multiple subjects, we can determine which set of LBPH references is least distant from the LBPH of a piece of a scene, such as a detected face. Based on the least distant set of references, we can predict the identity of the face (or other object) in the scene.</p>
<p>An LBPH model is good at capturing fine texture detail in any subject, not just faces. Moreover, it is good for applications where the model needs to be updated, such as <kbd>Interactive Recognizer</kbd>. The histograms for any two images are computed independently, so a new reference image can be added without recomputing the rest of the model.</p>
<p>OpenCV also implements other models that are popular for face recognition, namely, Eigenfaces and Fisherfaces. We use LBPH because it supports updates in real time, whereas Eigenfaces and Fisherfaces do not. For more information on these three recognition models, see the official documentation at <a href="https://docs.opencv.org/4.0.0-beta/da/d60/tutorial_face_main.html">https://docs.opencv.org/4.0.0-beta/da/d60/tutorial_face_main.html</a>.</p>
<p>Alternatively, for detection rather than recognition, we can organize LBPH models into a cascade of multiple tests, much like a Haar cascade. Unlike an LBPH recognition model, an LBP cascade cannot be updated in real time.</p>
<p>Haar cascades, LBP cascades, and LBPH recognition models are not robust with respect to rotation or flipping. For example, if we look at a face upside down, it will not be detected by a Haar cascade that was trained only with upright faces. Similarly, if we had an LBPH recognition model trained for a cat whose face is black on the cat's left-hand side and orange on the cat's right-hand side, the model might not recognize the same cat in a mirror. The exception is that we could include mirror images in the training set, but then we might get a false positive recognition for a different cat whose face is orange on the cat's left-hand side and black on the cat's right-hand side.</p>
<p>Unless otherwise noted, we may assume that a Haar cascade or LBPH model is trained for an <em>upright</em> subject. That is, the subject is not tilted or upside down in the image's coordinate space. If a man is standing on his head, we can take an upright photo of his face by turning the camera upside down or, equivalently, by applying a 180-degree rotation in software.</p>
<p>Some other directional terms are worth noting. A <em>frontal</em>, <em>rear</em>, or <em>profile</em> subject has its front, rear, or profile visible in the image. Most computer vision people, including the authors of OpenCV, express <em>left</em> and <em>right</em> in the image's coordinate space. For example, if we say <em>left eye</em>, for an upright, frontal, non-mirrored face, we mean the subject's right-hand eye, since left and right in image space are opposite from an upright, frontal, non-mirrored subject's left-hand and right-hand directions.</p>
<p>The following screenshot shows how we would label the <em>left eye</em> and <em>right eye</em> in a non-mirrored image (left) and mirrored image (right):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-197 image-border" src="img/851a49a3-9db4-4a43-b274-929ce15bacb8.png" style="width:32.17em;height:10.42em;"/></p>
<p>Our human and feline detectors deal with upright, frontal faces.</p>
<p>Of course, in a real-world photo, we cannot expect a face to be perfectly upright. The person's head or the camera might have been slightly tilted. Moreover, we cannot expect boundary regions, where a face meets a background, to be similar across images. We must take great care to preprocess the training images so that the face is rotated to a nearly perfect upright pose and boundary regions are cropped off. When cropping, we should place the major features of the face, such as eyes, in a consistent position. These considerations are addressed further in the <em>Planning the cat-detection model</em> section, later in this chapter.</p>
<p>If we must detect faces in various rotations, one option is to rotate the scene before sending it to the detector. For example, we can try to detect faces in the original scene, then in a version of the scene that has been rotated 15 degrees, then a version rotated—15 degrees (345 degrees), then a version rotated 30 degrees, and so on. Similarly, we can send mirrored versions of the scene to the detector. Depending on how many variations of the scene are tested, such an approach may be too slow for real-time use, and thus we do not use it in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the Interactive Recognizer app</h1>
                
            
            
                
<p>Let's create a new folder, where we will store this chapter's project, including the following subfolders and files that are relevant to <kbd>Interactive Recognizer</kbd>:</p>
<ul>
<li><kbd>cascades/haarcascade_frontalface_alt.xml</kbd>: A detection model for a frontal human face. It should be included with OpenCV at a path such as <kbd>&lt;opencv_unzip_destination&gt;/data/haarcascades/haarcascade_frontalface_alt.xml</kbd> or for a MacPorts installation at <kbd>/opt/local/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml</kbd>. Copy or link to that version. (Alternatively, get it from this book's GitHub repository).</li>
<li><kbd>cascades/lbpcascade_frontalface.xml</kbd>: An alternative (faster but less reliable) detection model for a frontal human face. It should be included with OpenCV at a path such as <kbd>&lt;opencv_unzip_destination&gt;/data/lbpcascades/lbpcascade_frontalface.xml</kbd> or for a MacPorts installation at <kbd>/opt/local/share/OpenCV/lbpcascades/lbpcascade_frontalface.xml</kbd>. Copy or link to that version. Alternatively, get it from this book's GitHub repository.</li>
<li><kbd>cascades/haarcascade_frontalcatface.xml</kbd>: A detection model for a frontal, feline face. We will build it later in this chapter. Alternatively, you may get a prebuilt version from this book's GitHub repository.</li>
<li><kbd>cascades/haarcascade_frontalcatface_extended.xml</kbd>: An alternative detection model for a frontal feline face. This version is sensitive to diagonal patterns, potentially including whiskers and ears. We will build it later in this chapter. (Alternatively, you may get a prebuilt version from this book's GitHub repository.)</li>
<li><kbd>cascades/lbpcascade_frontalcatface.xml</kbd>: Another alternative (faster but less reliable) detection model for a frontal feline face. We will build it later in this chapter. (Alternatively, you may get a prebuilt version from this book's GitHub repository.)</li>
<li><kbd>recognizers/lbph_human_faces.xml</kbd>: A recognition model for the faces of certain human individuals. It is generated by <kbd>InteractiveHumanFaceRecognizer.py</kbd>, as follows later.</li>
<li><kbd>recognizers/lbph_cat_faces.xml</kbd>: A recognition model for the faces of certain feline individuals. It is generated by <kbd>InteractiveCatFaceRecognizer.py</kbd>, as follows later.</li>
<li> <kbd>ResizeUtils.py</kbd>: Utility functions for resizing images. It copies or links to the previous chapter's version of <kbd>ResizeUtils.py</kbd>. We will add a function to resize the camera capture dimensions.</li>
<li><kbd>WxUtils.py</kbd>: Utility functions for wxPython GUI applications. It copies or links to the <a href="4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml">Chapter 2</a>, <em>Searching for Luxury Accommodations Worldwide</em>, version of <kbd>WxUtils.py</kbd>.</li>
<li><kbd>BinasciiUtils.py</kbd>: Utility functions for converting human-readable identifiers into numbers and back.</li>
<li><kbd>InteractiveRecognizer.py</kbd>: A class that encapsulates the <kbd>Interactive Recognizer</kbd> app and exposes certain variables for configuration. We will implement it in this section.</li>
<li><kbd>InteractiveHumanFaceRecognizer.py</kbd>: A script to launch a version of Interactive Recognizer that is configured for frontal human faces. We will implement it in this section.</li>
<li><kbd>InteractiveCatFaceRecognizer.py</kbd>: A script to launch a version of <kbd>Interactive Recognizer</kbd> that is configured for frontal feline faces. We will implement it in this section.</li>
</ul>
<p>Let's start with an addition to our existing <kbd>ResizeUtils</kbd> module. We want to be able to specify the resolution at which a camera captures images. Camera input is represented by an OpenCV class called <kbd>VideoCapture</kbd>, with the <kbd>get</kbd> and <kbd>set</kbd> methods that pertain to various camera parameters, including resolution. (Incidentally, <kbd>VideoCapture</kbd> can also represent a video file.) There is no guarantee that a given capture resolution is supported by a given camera. We need to check the success or failure of any attempt to set the capture resolution. Accordingly, let's add the following utility function to <kbd>ResizeUtils.py</kbd> to attempt to set a capture resolution and to return the actual capture resolution:</p>
<pre>def cvResizeCapture(capture, preferredSize):<br/><br/>    # Try to set the requested dimensions.<br/>    w, h = preferredSize<br/>    capture.set(cv2.CAP_PROP_FRAME_WIDTH, w)<br/>    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, h)<br/><br/>    # Sometimes the dimensions fluctuate at the start of capture.<br/>    # Discard two frames to allow for this.<br/>    capture.read()<br/>    capture.read()<br/><br/>    # Try to return the actual dimensions of the third frame.<br/>    success, image = capture.read()<br/>    if success and image is not None:<br/>        h, w = image.shape[:2]<br/>    return (w, h) </pre>
<p>Now, let's consider the requirements for our new <kbd>BinasciiUtils</kbd> module. OpenCV's recognizers use 32-bit integers as identifiers. For a GUI, asking the user to give a face a number, instead of a name, is not very friendly. We could keep a dictionary that maps numbers to names, and we could save this dictionary to disk, alongside the recognition model, but here is my lazier solution. Four or fewer ASCII characters can be cast to a 32-bit integer (and vice versa). For example, consider the name <em>Puss</em>, in which the letter ASCII codes are <em>80</em>, <em>117</em>, <em>115</em>, and <em>115</em>, respectively. Remembering that each letter is one byte or 8 bits, we can apply bitshift operations to the ASCII codes to get the following value:</p>
<p class="mce-root"><img class="fm-editor-equation" src="img/057dc3ba-57dd-475e-b121-7f32de7ac5eb.png" style="width:32.08em;height:1.42em;"/></p>
<p>We will let the user enter names of up to four characters, and, behind the scenes, we will convert to and from the 32-bit integers that the model stores. Let's create <kbd>BinasciiUtils.py</kbd>, and put the following imports and conversion functions in it:</p>
<pre>import binascii<br/><br/>def fourCharsToInt(s):<br/>    return int(binascii.hexlify(bytearray(s, 'ascii')), 16)<br/><br/>def intToFourChars(i):<br/>    return binascii.unhexlify(format(i, 'x')).decode('ascii')</pre>
<p>Now, let's proceed to write <kbd>InteractiveRecognizer.py</kbd>. It should start with the following <kbd>import</kbd> statements:</p>
<pre>import numpy<br/>import cv2<br/>import os<br/>import sys<br/>import threading<br/>import wx<br/><br/>import BinasciiUtils<br/>import ResizeUtils<br/>import WxUtils</pre>
<p>Our <kbd>InteractiveRecognizer</kbd> application class accepts several arguments that allow us to create different variants of the app with different titles, highlight colors, recognition models, detection models, and tweaks to the detection behavior. Let's look at the initializer's declaration:</p>
<pre>class InteractiveRecognizer(wx.Frame):<br/><br/>    def __init__(self, recognizerPath, cascadePath,<br/>                 scaleFactor=1.3, minNeighbors=4,<br/>                 minSizeProportional=(0.25, 0.25),<br/>                 rectColor=(0, 255, 0),<br/>                 cameraDeviceID=0, imageSize=(1280, 720),<br/>                 title='Interactive Recognizer'):</pre>
<p>The initializer's arguments are defined as follows:</p>
<ul>
<li><kbd>recognizerPath</kbd>: This is the file containing the recognition model. This file does not need to exist when the app starts. Rather, the recognition model (if any) is saved here when the app exits.</li>
<li><kbd>cascadePath</kbd>: This is the file containing the detection model. This file does need to exist when the app starts.</li>
<li><kbd>scaleFactor</kbd>: The detector searches for faces at several different scales. This argument specifies the ratio of each scale to the next smaller scale. A bigger ratio implies a faster search but fewer detections.</li>
<li><kbd>minNeighbors</kbd>: If the detector encounters two overlapping regions that both might pass detection as faces, they are called neighbors. The <kbd>minNeighbors</kbd> argument specifies the minimum number of neighbors that a face must have in order to pass detection. Where <kbd>minNeighbors&gt;0</kbd>, the rationale is that a true face could be cropped in several alternative places and still look like a face. A greater number of required neighbors implies fewer detections and a lower proportion of false positives.</li>
<li><kbd>minSizeProportional</kbd>: A face's minimum width and height are expressed as a proportion of the camera's vertical resolution or horizontal resolution, whichever is less. For example, if the camera resolution is <em>640 x 480</em> and <kbd>minSizeProportional=(0.25, 0.25)</kbd>, the face must measure at least <em>120 x 120</em> (in pixels) in order to pass detection. A bigger minimum size implies a faster search but fewer detections. The <kbd>(0.25, 0.25)</kbd> default value is appropriate for a face that is close to a webcam.</li>
<li><kbd>rectColor</kbd>: This is for the color of the rectangle outlining a detected face. Like most color tuples in OpenCV, it is specified in <strong>blue, green, and red</strong> (<strong>BGR</strong>) order (not RGB).</li>
<li><kbd>cameraDeviceID</kbd>: The is the device ID of the camera that should be used for input. Typically, webcams are numbered starting from <kbd>0</kbd>, with any connected external webcams coming before any internal webcams. Some camera drivers reserve fixed device IDs. For example, OpenNI reserves <kbd>900</kbd> for Kinect and <kbd>910</kbd> for Asus Xtion.</li>
<li><kbd>imageSize</kbd>: The preferred resolution for captured images. If the camera does not support this resolution, another resolution is used.</li>
<li><kbd>title</kbd>: The app title, as seen in the window title bar.</li>
</ul>
<p>We also provide a public Boolean variable to configure whether or not the camera feed is mirrored. By default, it is mirrored because users find a mirrored image of themselves to be more intuitive:</p>
<pre>        self.mirrored = True</pre>
<p>Another Boolean tracks whether the app should still be running or whether it is closing. This information is relevant to cleaning up a background thread:</p>
<pre>        self._running = True</pre>
<p>Using an OpenCV class called <kbd>cv2.VideoCapture</kbd>, we open a camera feed and get its resolution, as follows:</p>
<pre>        self._capture = cv2.VideoCapture(cameraDeviceID)<br/>        size = ResizeUtils.cvResizeCapture(<br/>                self._capture, imageSize)<br/>        self._imageWidth, self._imageHeight = size</pre>
<p>We define variables to store the images that we will capture, process, and display. Initially, these are <kbd>None</kbd>. In order to capture and process images on one thread, and then draw them to the screen on another thread, we will use a pattern known as <strong>double buffering</strong>. While one frame (the <strong>back buffer</strong>) is being prepared on one thread, another frame (the <strong>front buffer</strong>) will be being drawn on a second thread. When both threads have done a round of work, we will swap the buffers so that the old back buffer becomes the new front buffer and vice versa (by simply changing references, without copying data). To accomplish this in a thread-safe manner, we need to declare a <strong>mutual exclusion lock</strong> (also called a <strong>mutex</strong>), which represents a permission or resource (in this case, access to the front buffer) that only one thread can acquire at a time. We will see the lock in use later in this section, in the <kbd>_onVideoPanelPaint</kbd> and <kbd>_runCaptureLoop</kbd> methods. For now, here are the initial declarations of the images and lock:</p>
<pre>        self._image = None<br/>        self._grayImage = None<br/>        self._equalizedGrayImage = None<br/><br/>        self._imageFrontBuffer = None<br/>        self._imageFrontBufferLock = threading.Lock()</pre>
<p>Next, we set up variables related to detection and recognition. Many of these variables just store initialization arguments for later use. Also, we keep a reference to the currently detected face, which is initially <kbd>None</kbd>. We initialize an LBPH recognizer and load any recognition model that we may have saved on a previous run on the app. Likewise, we initialize a detector by loading a Haar cascade or LBP cascade from file. Here is the relevant code:</p>
<pre>        self._currDetectedObject = None<br/><br/>        self._recognizerPath = recognizerPath<br/>        self._recognizer = cv2.face.LBPHFaceRecognizer_create()<br/>        if os.path.isfile(recognizerPath):<br/>            self._recognizer.read(recognizerPath)<br/>            self._recognizerTrained = True<br/>        else:<br/>            self._recognizerTrained = False<br/><br/>        self._detector = cv2.CascadeClassifier(cascadePath)<br/>        self._scaleFactor = scaleFactor<br/>        self._minNeighbors = minNeighbors<br/>        minImageSize = min(self._imageWidth, self._imageHeight)<br/>        self._minSize = (int(minImageSize * minSizeProportional[0]),<br/>                         int(minImageSize * minSizeProportional[1]))<br/>        self._rectColor = rectColor</pre>
<p>Having set up the variables that are relevant to computer vision, we proceed to the GUI implementation, which is mostly boilerplate code. First, in the following snippet, we set up the window with a certain style, size, title, and background color, and we bind a handler for its close event:</p>
<pre>        style = wx.CLOSE_BOX | wx.MINIMIZE_BOX | wx.CAPTION | \<br/>            wx.SYSTEM_MENU | wx.CLIP_CHILDREN<br/>        wx.Frame.__init__(self, None, title=title,<br/>                          style=style, size=size)<br/>        self.SetBackgroundColour(wx.Colour(232, 232, 232))<br/><br/>        self.Bind(wx.EVT_CLOSE, self._onCloseWindow)</pre>
<p>Next, we set a callback for the <em>Escape</em> key. Since a key is not a GUI widget, there is no <kbd>Bind</kbd> method directly associated with a key, and we need to set up the callback a bit differently than we have previously seen with wxWidgets. We bind a new menu event and callback to the <kbd>InteractiveRecognizer</kbd> instance, and we map a keyboard shortcut to the menu event using a class called <kbd>wx.AcceleratorTable</kbd>. (Note, however, that our app actually has no menu, nor is an actual menu item required for the keyboard shortcut to work.) Here is the code:</p>
<pre>        quitCommandID = wx.NewId()<br/>        self.Bind(wx.EVT_MENU, self._onQuitCommand,<br/>                  id=quitCommandID)<br/>        acceleratorTable = wx.AcceleratorTable([<br/>            (wx.ACCEL_NORMAL, wx.WXK_ESCAPE, quitCommandID)<br/>        ])<br/>        self.SetAcceleratorTable(acceleratorTable)</pre>
<p>The following code initializes the GUI widgets (including a video panel, text field, buttons, and label) and sets their event callbacks:</p>
<pre>        self._videoPanel = wx.Panel(self, size=size)<br/>        self._videoPanel.Bind(<br/>                wx.EVT_ERASE_BACKGROUND,<br/>                self._onVideoPanelEraseBackground)<br/>        self._videoPanel.Bind(<br/>                wx.EVT_PAINT, self._onVideoPanelPaint)<br/><br/>        self._videoBitmap = None<br/><br/>        self._referenceTextCtrl = wx.TextCtrl(<br/>                self, style=wx.TE_PROCESS_ENTER)<br/>        self._referenceTextCtrl.SetMaxLength(4)<br/>        self._referenceTextCtrl.Bind(<br/>                wx.EVT_KEY_UP, self._onReferenceTextCtrlKeyUp)<br/><br/>        self._predictionStaticText = wx.StaticText(self)<br/>        # Insert an endline for consistent spacing.<br/>        self._predictionStaticText.SetLabel('\n')<br/><br/>        self._updateModelButton = wx.Button(<br/>                self, label='Add to Model')<br/>        self._updateModelButton.Bind(<br/>                wx.EVT_BUTTON, self._updateModel)<br/>        self._updateModelButton.Disable()<br/><br/>        self._clearModelButton = wx.Button(<br/>                self, label='Clear Model')<br/>        self._clearModelButton.Bind(<br/>                wx.EVT_BUTTON, self._clearModel)<br/>        if not self._recognizerTrained:<br/>            self._clearModelButton.Disable()</pre>
<p>Similar to Luxocator (the <a href="4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml">Chapter 2</a>, <em>Searching for Luxury Accommodations Worldwide,</em> project), <kbd>Interactive Recognizer</kbd> lays out the image on top and a row of controls on the bottom. Here is the layout code:</p>
<pre>        border = 12<br/><br/>        controlsSizer = wx.BoxSizer(wx.HORIZONTAL)<br/>        controlsSizer.Add(self._referenceTextCtrl, 0,<br/>                          wx.ALIGN_CENTER_VERTICAL | wx.RIGHT,<br/>                          border)<br/>        controlsSizer.Add(<br/>                self._updateModelButton, 0,<br/>                wx.ALIGN_CENTER_VERTICAL | wx.RIGHT, border)<br/>        controlsSizer.Add(self._predictionStaticText, 0,<br/>                          wx.ALIGN_CENTER_VERTICAL)<br/>        controlsSizer.Add((0, 0), 1) # Spacer<br/>        controlsSizer.Add(self._clearModelButton, 0,<br/>                          wx.ALIGN_CENTER_VERTICAL)<br/><br/>        rootSizer = wx.BoxSizer(wx.VERTICAL)<br/>        rootSizer.Add(self._videoPanel)<br/>        rootSizer.Add(controlsSizer, 0, wx.EXPAND | wx.ALL, border)<br/>        self.SetSizerAndFit(rootSizer)</pre>
<p>Finally, the initializer starts a background thread that performs image capture and image processing, including detection and recognition. It is important to perform the intensive computer vision work on a background thread so that it doesn't stall the handling of GUI events. Here is the code that starts the thread:</p>
<pre>        self._captureThread = threading.Thread(<br/>                target=self._runCaptureLoop)<br/>        self._captureThread.start()</pre>
<p>With a variety of input events and background work, <kbd>InteractiveRecognizer</kbd> has many methods that run in an indeterminate order. We will look at input event handlers first, before proceeding to the image pipeline (capture, processing, and display), which partly runs on the background thread.</p>
<p>When the window is closed, we ensure that the background thread stops. Then, if the recognition model is trained, we save it to file. Here is the implementation of the relevant callback:</p>
<pre>    def _onCloseWindow(self, event):<br/>        self._running = False<br/>        self._captureThread.join()<br/>        if self._recognizerTrained:<br/>            modelDir = os.path.dirname(self._recognizerPath)<br/>            if not os.path.isdir(modelDir):<br/>                os.makedirs(modelDir)<br/>            self._recognizer.write(self._recognizerPath)<br/>        self.Destroy()</pre>
<p>Besides closing the window when its standard <em>X</em> button is clicked, we also close it in the <kbd>_onQuitCommand</kbd> callback, which we linked to the <em>Esc</em> button. The callback's implementation is shown in the following code:</p>
<pre>    def _onQuitCommand(self, event):<br/>        self.Close()</pre>
<p>We handle the video panel's erase event by doing nothing because we simply want to draw over the old video frame instead of erasing it. We handle the video panel's draw event by acquiring the lock that gives us thread-safe access to the front image buffer, converting the image into a wxPython bitmap, and then drawing the bitmap to the panel. Here are the implementations of the two relevant callbacks in the following code:</p>
<pre>    def _onVideoPanelEraseBackground(self, event):<br/>        pass<br/><br/>    def _onVideoPanelPaint(self, event):<br/><br/>        self._imageFrontBufferLock.acquire()<br/><br/>        if self._imageFrontBuffer is None:<br/>            self._imageFrontBufferLock.release()<br/>            return<br/><br/>        # Convert the image to bitmap format.<br/>        self._videoBitmap = \<br/>                WxUtils.wxBitmapFromCvImage(self._imageFrontBuffer)<br/><br/>        self._imageFrontBufferLock.release()<br/><br/>        # Show the bitmap.<br/>        dc = wx.BufferedPaintDC(self._videoPanel)<br/>        dc.DrawBitmap(self._videoBitmap, 0, 0)</pre>
<p>When the user adds or deletes text in the text field, our <kbd>_onReferenceTextCtrlKeyUp</kbd> callback (as follows) calls a helper method to check whether the Add to Model button should be enabled or disabled:</p>
<pre>    def _onReferenceTextCtrlKeyUp(self, event):<br/>        self._enableOrDisableUpdateModelButton()</pre>
<p>When the Add to Model button is clicked, its callback provides new training data to the recognition model. If the LBPH model has no prior training data, we must use the recognizer's <kbd>train</kbd> method; otherwise, we must use its <kbd>update</kbd> method. Both methods accept two arguments—a list of images (the faces) and a NumPy array of integers (the face identifiers). We train or update the model with just one image at a time so that the user can interactively test the effect of each incremental change to the model. The image is the most recently detected face, and the identifier is converted from the text in the text field using our <kbd>BinasciiUtils.fourCharsToInt</kbd> function. Here is the implementation of the Add to Model button's callback:</p>
<pre>    def _updateModel(self, event):<br/>        labelAsStr = self._referenceTextCtrl.GetValue()<br/>        labelAsInt = BinasciiUtils.fourCharsToInt(labelAsStr)<br/>        src = [self._currDetectedObject]<br/>        labels = numpy.array([labelAsInt])<br/>        if self._recognizerTrained:<br/>            self._recognizer.update(src, labels)<br/>        else:<br/>            self._recognizer.train(src, labels)<br/>            self._recognizerTrained = True<br/>            self._clearModelButton.Enable()</pre>
<p>When the Clear Model button is clicked, its callback deletes the recognition model (including any version that has been saved to disk) and creates a new one. Also, we record that the model is untrained and we disable the Clear Model button until the model is retrained. Here is the implementation in the following code:</p>
<pre>    def _clearModel(self, event=None):<br/>        self._recognizerTrained = False<br/>        self._clearModelButton.Disable()<br/>        if os.path.isfile(self._recognizerPath):<br/>            os.remove(self._recognizerPath)<br/>        self._recognizer = cv2.face.LBPHFaceRecognizer_create()</pre>
<p>Our background thread runs a loop. On each iteration, we capture an image using the <kbd>VideoCapture</kbd> object's <kbd>read</kbd> method. Along with the image, the <kbd>read</kbd> method returns a <kbd>success</kbd> flag, which we do not need because instead we just check whether the image is <kbd>None</kbd>. If the image is not <kbd>None</kbd>, we call a helper method named <kbd>_detectAndRecognize</kbd>, and then we may mirror the image for display. We also acquire the lock to perform a thread-safe swap of the front and back image buffers. After the swap, we tell the video panel to refresh itself by drawing the bitmap from the new front buffer. Here is the implementation of the loop in the following code:</p>
<pre>    def _runCaptureLoop(self):<br/>        while self._running:<br/>            success, self._image = self._capture.read(<br/>                    self._image)<br/>            if self._image is not None:<br/>                self._detectAndRecognize()<br/>                if (self.mirrored):<br/>                    self._image[:] = numpy.fliplr(self._image)<br/><br/>                # Perform a thread-safe swap of the front and<br/>                # back image buffers.<br/>                self._imageFrontBufferLock.acquire()<br/>                self._imageFrontBuffer, self._image = \<br/>                        self._image, self._imageFrontBuffer<br/>                self._imageFrontBufferLock.release()<br/><br/>                # Send a refresh event to the video panel so<br/>                # that it will draw the image from the front<br/>                # buffer.<br/>                self._videoPanel.Refresh()</pre>
<p>By calling <kbd>self._capture.read(self._image)</kbd>, we are telling OpenCV to reuse the image buffer in <kbd>self._image</kbd> (if <kbd>self.image</kbd> is not <kbd>None</kbd> and is the right size) so that new memory doesn't have to be allocated every time we capture a new frame. Alternatively, it would be valid, but less efficient, to call <kbd>self._capture.read()</kbd> without arguments; in this case, new memory would be allocated every time we captured a new frame.</p>
<p>Recall that the loop ends after our <kbd>_onCloseWindow</kbd> callback sets <kbd>_running</kbd> to <kbd>False</kbd>.</p>
<p>The <kbd>_detectAndRecognize</kbd> helper method is also running on the background thread. It begins by creating an equalized grayscale version of the image. An <strong>equalized</strong> image has an approximately uniform histogram; that is to say, for some bin size, the number of pixels in each bin of gray values is approximately equal. It is a kind of contrast adjustment that makes a subject's appearance more predictable, despite different lighting conditions and exposure settings in different images; thus, it aids detection or recognition. We pass the equalized image to the classifier's <kbd>detectMultiScale</kbd> method, also using the <kbd>scaleFactor</kbd>, <kbd>minNeighbors</kbd>, and <kbd>minSize</kbd> arguments that were specified during initialization of <kbd>InteractiveRecognizer</kbd>. As the return value from <kbd>detectMultiScale</kbd>, we get a list of rectangle measurements, describing the bounds of the detected faces. For display, we draw green outlines around these faces. If at least one face is detected, we store an equalized grayscale version of the first face in the <kbd>_currDetectedObject</kbd> member variable. Here is the implementation of this first portion of the <kbd>_detectAndRecognize</kbd> method:</p>
<pre>    def _detectAndRecognize(self):<br/>        self._grayImage = cv2.cvtColor(<br/>                self._image, cv2.COLOR_BGR2GRAY,<br/>                self._grayImage)<br/>        self._equalizedGrayImage = cv2.equalizeHist(<br/>                self._grayImage, self._equalizedGrayImage)<br/>        rects = self._detector.detectMultiScale(<br/>                self._equalizedGrayImage,<br/>                scaleFactor=self._scaleFactor,<br/>                minNeighbors=self._minNeighbors,<br/>                minSize=self._minSize)<br/>        for x, y, w, h in rects:<br/>            cv2.rectangle(self._image, (x, y), (x+w, y+h),<br/>                          self._rectColor, 1)<br/>        if len(rects) &gt; 0:<br/>            x, y, w, h = rects[0]<br/>            self._currDetectedObject = cv2.equalizeHist(<br/>                    self._grayImage[y:y+h, x:x+w])</pre>
<p>Note that we perform equalization separately on the detected face region after we crop it. This enables us to get an equalization result that is better adapted to the local contrast of the face, instead of the global contrast of the whole image.</p>
<p>If a face is currently detected and the recognition model is trained for at least one individual, we can proceed to predict the identity of the face. We pass the equalized face to the <kbd>predict</kbd> method of the recognizer and get two return values—an integer identifier and a measure of distance (non-confidence). Using our <kbd>BinasciiUtils.intToFourChars</kbd> function, we convert the integer into a string (of at most four characters), which will be one of the face names that the user previously entered. We show the name and distance. If an error occurs (for example, if an invalid model was loaded from file), we delete and recreate the model. If the model is not yet trained, we show instructions about training the model. Here is the implementation of this middle portion of the <kbd>_detectAndRecognize</kbd> method:</p>
<pre>            if self._recognizerTrained:<br/>                try:<br/>                    labelAsInt, distance = self._recognizer.predict(<br/>                            self._currDetectedObject)<br/>                    labelAsStr = BinasciiUtils.intToFourChars(labelAsInt)<br/>                    self._showMessage(<br/>                            'This looks most like %s.\n'<br/>                            'The distance is %.0f.' % \<br/>                            (labelAsStr, distance))<br/>                except cv2.error:<br/>                    print &gt;&gt; sys.stderr, \<br/>                            'Recreating model due to error.'<br/>                    self._clearModel()<br/>            else:<br/>                self._showInstructions()</pre>
<p>If no face was detected, we set <kbd>_currDetectedObject</kbd> to <kbd>None</kbd> and show either the instructions (if the model hasn't been trained yet) or no descriptive text, otherwise. Under all conditions, we end the <kbd>_detectAndRecognize</kbd> method by ensuring that the Add to Model button is enabled or disabled, as appropriate. Here is this final portion of the method's implementation:</p>
<pre>        else:<br/>            self._currDetectedObject = None<br/>            if self._recognizerTrained:<br/>                self._clearMessage()<br/>            else:<br/>                self._showInstructions()<br/><br/>        self._enableOrDisableUpdateModelButton()</pre>
<p>The Add to Model button should be enabled only when a face is detected and the text field is not empty. We can implement this logic in the following manner:</p>
<pre>    def _enableOrDisableUpdateModelButton(self):<br/>        labelAsStr = self._referenceTextCtrl.GetValue()<br/>        if len(labelAsStr) &lt; 1 or \<br/>                    self._currDetectedObject is None:<br/>            self._updateModelButton.Disable()<br/>        else:<br/>            self._updateModelButton.Enable()</pre>
<p>Since we set the label's text under several different conditions, we use the following helper functions to reduce repetition of code, as shown in the following code:</p>
<pre>    def _showInstructions(self):<br/>        self._showMessage(<br/>                'When an object is highlighted, type its name\n'<br/>                '(max 4 chars) and click "Add to Model".')<br/><br/>    def _clearMessage(self):<br/>        # Insert an endline for consistent spacing.<br/>        self._showMessage('\n')<br/><br/>    def _showMessage(self, message):<br/>        wx.CallAfter(self._predictionStaticText.SetLabel, message)</pre>
<p>Note the use of the <kbd>wx.CallAfter</kbd> function to ensure that the label is updated on the main thread.</p>
<p>That is all the functionality of <kbd>Interactive Recognizer</kbd>. Now, we just need to write the <kbd>main</kbd> functions for the two variants of the app, starting with the <kbd>Interactive Human Face Recognizer</kbd>. As arguments to the initializer of <kbd>InteractiveRecognizer</kbd>, we provide the app's title and PyInstaller-compatible paths to the relevant detection model and recognition model. We run the app. Here is the implementation, which we may put in <kbd>InteractiveHumanFaceRecognizer.py</kbd>:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import wx<br/><br/>from InteractiveRecognizer import InteractiveRecognizer<br/>import PyInstallerUtils<br/><br/><br/>def main():<br/>    app = wx.App()<br/>    recognizerPath = PyInstallerUtils.resourcePath(<br/>            'recognizers/lbph_human_faces.xml')<br/>    cascadePath = PyInstallerUtils.resourcePath(<br/>            # Uncomment the next argument for LBP.<br/>            #'cascades/lbpcascade_frontalface.xml')<br/>            # Uncomment the next argument for Haar.<br/>            'cascades/haarcascade_frontalface_alt.xml')<br/>    interactiveRecognizer = InteractiveRecognizer(<br/>            recognizerPath, cascadePath,<br/>            title='Interactive Human Face Recognizer')<br/>    interactiveRecognizer.Show()<br/>    app.MainLoop()<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Remember that <kbd>cascades/haarcascade_frontalface_alt.xml</kbd> or <kbd>cascades/lpbcascade_frontalface.xml</kbd> needs to be obtained from the OpenCV samples or from this book's GitHub repository. Feel free to test <kbd>Interactive Human Face Recognizer</kbd> now!</p>
<p>Our second variant of the app, <kbd>Interactive Cat Face Recognizer</kbd>, uses very similar code. We change the app's title and the paths of the detection and recognition models. Also, we lower the <kbd>scaleFactor</kbd> value to <kbd>1.2</kbd>, the <kbd>minNeighbors</kbd> value to <kbd>1</kbd>, and the <kbd>minSizeProportional</kbd> value to (<kbd>0.125</kbd>, <kbd>0.125</kbd>) to make the detector a little more sensitive. (A cat face is smaller than a human face, and our cat face detection model turns out to be less prone to false positives than our human face detection model, so these adjustments are appropriate.) Here is the implementation, which we may put in <kbd>InteractiveCatFaceRecognizer.py</kbd>:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import wx<br/><br/>from InteractiveRecognizer import InteractiveRecognizer<br/>import PyInstallerUtils<br/><br/><br/>def main():<br/>    app = wx.App()<br/>    recognizerPath = PyInstallerUtils.resourcePath(<br/>            'recognizers/lbph_cat_faces.xml')<br/>    cascadePath = PyInstallerUtils.resourcePath(<br/>            # Uncomment the next argument for LBP.<br/>            #'cascades/lbpcascade_frontalcatface.xml')<br/>            # Uncomment the next argument for Haar with basic<br/>            # features.<br/>            #'cascades/haarcascade_frontalcatface.xml')<br/>            # Uncomment the next argument for Haar with extended<br/>            # features.<br/>            'cascades/haarcascade_frontalcatface_extended.xml')<br/>    interactiveRecognizer = InteractiveRecognizer(<br/>            recognizerPath, cascadePath,<br/>            scaleFactor=1.2, minNeighbors=1,<br/>            minSizeProportional=(0.125, 0.125),<br/>            title='Interactive Cat Face Recognizer')<br/>    interactiveRecognizer.Show()<br/>    app.MainLoop()<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>At this stage, <kbd>Interactive Cat Face Recognizer</kbd> will not run properly because <kbd>cascades/haarcascade_frontalcatface.xml</kbd>, <kbd>cascades/haarcascade_frontalcatface_extended.xml</kbd>, or <kbd>cascades/lpbcascade_frontalcatface.xml</kbd> does not exist (unless you copied the prebuilt version from this book's GitHub repository). Soon, we will create it!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the cat-detection model</h1>
                
            
            
                
<p>When I said <em>soon</em>, I meant in a day or two. Training a Haar cascade takes a lot of processing time. Training an LBP cascade is relatively quick. However, in either case, we need to download some big collections of images before we even start. Settle down with a reliable internet connection, a power outlet, at least 4 GB of free disk space, and the fastest CPU and biggest RAM you can find. Do not attempt this segment of the project on a Raspberry Pi. Keep the computer away from external heat sources or things that might block its fans. My processing time for Haar cascade training was 24 hours (or more for the whisker-friendly version that is sensitive to diagonal patterns), with 100% usage on four cores, on a MacBook Pro with a 2.6 GHz Intel Core i7 CPU and 16 GB RAM.</p>
<p>We use the following sets of images, which are freely available for research purposes:</p>
<ul>
<li>The PASCAL <strong>Visual Object Classes Challenge 2007</strong> (<strong>VOC2007</strong>) dataset. VOC2007 contains 10,000 images of diverse subjects against diverse backgrounds, under diverse lighting conditions, so it is suitable as the basis of our negative training set. The images come with annotation data, including a count of cats for each image (most often 0). Thus, in building our negative training set, we can easily omit images that contain cats.</li>
<li>The frontal face dataset from the <strong>California Institute of Technology</strong> (<strong>Caltech</strong>) Faces 1999. This set contains 450 images of frontal human faces under diverse lighting conditions and against diverse backgrounds. These images make a useful addition to our negative training set because our frontal cat face detector may be deployed in places where frontal human faces are also likely to be present. None of the images contain cats.</li>
<li>The Urtho negative training set, which was originally part of a face- and eye-detection project called <strong>Urtho</strong>. This set contains 3,000 images of diverse backgrounds. None of the images contain cats.</li>
<li>The cat head dataset from Microsoft Research (<em>Microsoft Cat Dataset 2008</em>) has 10,000 images of cats against diverse backgrounds and under diverse lighting conditions. The rotation of the cat's head varies, but in all cases the nose, mouth, both eyes, and both ears are clearly visible. Thus, we may say that all the images include frontal faces and are suitable for use as our positive training set. Each image comes with annotation data, indicating coordinates of the center of the mouth, center of the eyes, and corners of the hollow of the ear (three corners per ear). Based on the annotation data, we can straighten and crop the cat's face in order to make the positive training images more similar to each other, as shown in the following screenshot:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-198 image-border" src="img/4ea6b541-c4f7-4b19-b7ed-f2887149e46e.png" style="width:35.17em;height:25.92em;"/></p>
<div><p>The author of the Urtho negative training set is unknown. The other annotated datasets are generously provided by the following authors, as part of the following publications:</p>
<ul>
<li>Everingham, M. and Van Gool, L. and Williams, C. K. I. and Winn, J., and Zisserman, A. <em>The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results</em>.</li>
<li>Weber, Markus. <em>Frontal face dataset</em>. California Institute of Technology, 1999.</li>
<li>Weiwei Zhang, Jian Sun, and Xiaoou Tang. <em>Cat Head Detection - How to Effectively Exploit Shape and Texture Features</em>, <em>Proc. of European Conf. Computer Vision</em>, vol. 4, pp. 802-816, 2008.</li>
</ul>
</div>
<p>We will preprocess the images and generate files describing the positive and negative training sets. After preprocessing, all the training images are in equalized grayscale format, and the positive training images are upright and cropped. The description files conform to certain formats expected by the OpenCV training tools. With the training sets prepared, we will run the OpenCV training tools with the appropriate parameters. The output will be a Haar cascade file for detecting upright frontal cat faces.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the training script for the cat-detection model</h1>
                
            
            
                
<p>"Praline: I've never seen so many aerials in me life. The man told me, their equipment could pinpoint a purr at 400 yards and Eric, being such a happy cat, was a piece of cake."<br/>
                        – Fish License sketch, Monty Python's Flying Circus, Episode 23 (1970)</p>
<p>This segment of the project uses tens of thousands of files, including images, annotation files, scripts, and intermediate and final outputs of the training process. Let's organize all of this new material by giving our project a subfolder, <kbd>cascade_training</kbd>, which will ultimately have the following contents:</p>
<ul>
<li><kbd>cascade_training/CAT_DATASET_01</kbd>: This is the first half of the Microsoft Cat Dataset 2008.</li>
<li><kbd>cascade_training/CAT_DATASET_02</kbd>: This is the second half of the Microsoft Cat Dataset 2008.</li>
<li><kbd>cascade_training/faces</kbd>: This is the Caltech Faces 1999 dataset.</li>
<li><kbd>cascade_training/urtho_negatives</kbd>: This is the Urtho negatives dataset.</li>
<li><kbd>cascade_training/VOC2007</kbd>: This is the VOC2007 dataset.</li>
<li><kbd>cascade_training/describe.py</kbd>: A script to preprocess and describe the positive and negative training sets. As outputs, it creates new images in the previous dataset directories and in the following text description files.</li>
<li><kbd>cascade_training/negative_description.txt</kbd>: This is a generated text file describing the negative training set.</li>
<li><kbd>cascade_training/positive_description.txt</kbd>: This is a generated text file describing the positive training set.</li>
<li><kbd>cascade_training/train.bat</kbd> (Windows) or <kbd>cascade_training/train.sh</kbd> (Mac or Linux): This is a script to run the OpenCV cascade training tools with appropriate parameters. As input, it uses the previous text description files. As output, it generates a not-yet mentioned binary description file and cascade files.</li>
<li><kbd>cascade_training/binary_description</kbd>: This is a generated binary file describing the positive training set.</li>
<li><kbd>cascade_training/lbpcascade_frontalcatface/*.xml</kbd>: This gives the intermediate and final results of the LBP cascade training.</li>
<li><kbd>cascades/lbpcascade_frontalcatface.xml</kbd>: This is a copy of the final result of the LBP cascade training, in a location where our apps expect it.</li>
<li><kbd>cascade_training/haarcascade_frontalcatface/*.xml</kbd>: This shows the intermediate and final results of the Haar cascade training.</li>
<li><kbd>cascades/haarcascade_frontalcatface.xml</kbd>: This is a copy of the final result of the Haar cascade training, in a location where our apps expect it.</li>
</ul>
<p>For up-to-date instructions on obtaining and extracting the Microsoft Cat Dataset 2008, the Caltech Faces 1999 dataset, the Urtho negatives dataset, and the VOC2007 dataset, refer to the README on this book's GitHub web page at <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/</a>. Over time, some of the datasets' original websites and mirrors have gone down permanently, yet other mirrors continue to come online.</p>
<p>Once the datasets are downloaded and decompressed to the proper locations, let's write <kbd>describe.py</kbd>. It needs to start with the following shebang line and imports:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>from __future__ import print_function<br/><br/>import cv2<br/>import glob<br/>import math<br/>import sys</pre>
<p>All our source images need some preprocessing to optimize them as training images. We need to save the preprocessed versions, so let's globally define an extension that we will use for these files:</p>
<pre>outputImageExtension = '.out.jpg'</pre>
<p>To give our training images a more predictable appearance despite differences in lighting conditions and exposure settings, we need to create equalized grayscale images at several points in this script. Let's write the following helper function for this purpose:</p>
<pre>def equalizedGray(image):<br/>    return cv2.equalizeHist(cv2.cvtColor(<br/>            image, cv2.COLOR_BGR2GRAY))</pre>
<p>Similarly, we need to append to the negative description file at more than one point in the script. Each line in the negative description is just an image path. Let's add the following helper method, which accepts an image path and a file object for the negative description, loads the image and saves an equalized version, and appends the equalized version's path to the description file:</p>
<pre>def describeNegativeHelper(imagePath, output):<br/>    outputImagePath = '%s%s' % (imagePath, outputImageExtension)<br/>    image = cv2.imread(imagePath)<br/>    # Save an equalized version of the image.<br/>    cv2.imwrite(outputImagePath, equalizedGray(image))<br/>    # Append the equalized image to the negative description.<br/>    print(outputImagePath, file=output)</pre>
<p>Now, let's write the <kbd>describeNegative</kbd> function that calls <kbd>describeNegativeHelper</kbd>. It begins by opening a file in write mode so that we can write the negative description. Then, we iterate over all the image paths in the Caltech Faces 1999 set, which contains no cats. We skip any paths to output images that were written on a previous call of this function. We pass the remaining image paths, along with the newly opened negative description file, to <kbd>describeNegativeHelper</kbd>, as follows:</p>
<pre>def describeNegative():<br/>    output = open('negative_description.txt', 'w')<br/>    # Append all images from Caltech Faces 1999, since all are<br/>    # non-cats.<br/>    for imagePath in glob.glob('faces/*.jpg'):<br/>        if imagePath.endswith(outputImageExtension):<br/>            # This file is equalized, saved on a previous run.<br/>            # Skip it.<br/>            continue<br/>        describeNegativeHelper(imagePath, output)</pre>
<p>For every image in the Urtho negative training set, we pass the file path to <kbd>describeNegativeHelper</kbd>, as follows: </p>
<pre>    # Append all images from the Urtho negative training set,<br/>    # since all are non-cats.<br/>    for imagePath in glob.glob('urtho_negatives/*.jpg'):<br/>        if imagePath.endswith(outputImageExtension):<br/>            # This file is equalized, saved on a previous run.<br/>            # Skip it.<br/>            continue<br/>        describeNegativeHelper(imagePath, output)</pre>
<p>The remainder of the <kbd>describeNegative</kbd> function is responsible for passing relevant file paths from the VOC2007 image set to <kbd>describeNegativeHelper</kbd>. Some images in VOC2007 do contain cats. An annotation file, <kbd>VOC2007/ImageSets/Main/cat_test.txt</kbd>, lists image IDs and a flag indicating whether any cats are present in the image. The flag may be—<kbd>1</kbd> (no cats), <kbd>0</kbd> (one or more cats as background or secondary subjects of the image), or <kbd>1</kbd> (one or more cats as foreground or foreground subjects of the image). We parse this annotation data and, if an image contains no cats, we pass its path and the description file to <kbd>describeNegativeHelper</kbd>, as follows:</p>
<pre>    # Append non-cat images from VOC2007.<br/>    input = open('VOC2007/ImageSets/Main/cat_test.txt', 'r')<br/>    while True:<br/>        line = input.readline().rstrip()<br/>        if not line:<br/>            break<br/>        imageNumber, flag = line.split()<br/>        if int(flag) &lt; 0:<br/>            # There is no cat in this image.<br/>            imagePath = 'VOC2007/JPEGImages/%s.jpg' % imageNumber<br/>            describeNegativeHelper(imagePath, output)</pre>
<p>Now, let's move on to helper functions for generating the positive description. When rotating a face to straighten it, we also need to rotate a list of coordinate pairs representing features of the face. The following helper function accepts such a list, along with a center of rotation and angle of rotation, and returns a new list of the rotated coordinate pairs:</p>
<pre>def rotateCoords(coords, center, angleRadians):<br/>    # Positive y is down so reverse the angle, too.<br/>    angleRadians = -angleRadians<br/>    xs, ys = coords[::2], coords[1::2]<br/>    newCoords = []<br/>    n = min(len(xs), len(ys))<br/>    i = 0<br/>    centerX = center[0]<br/>    centerY = center[1]<br/>    cosAngle = math.cos(angleRadians)<br/>    sinAngle = math.sin(angleRadians)<br/>    while i &lt; n:<br/>        xOffset = xs[i] - centerX<br/>        yOffset = ys[i] - centerY<br/>        newX = xOffset * cosAngle - yOffset * sinAngle + centerX<br/>        newY = xOffset * sinAngle + yOffset * cosAngle + centerY<br/>        newCoords += [newX, newY]<br/>        i += 1<br/>    return newCoords</pre>
<p>Next, let's write a long helper function to preprocess a single positive training image. This function accepts two arguments—a list of coordinate pairs (which is named <kbd>coords</kbd>) and an OpenCV image. Refer to the diagram of feature points on a cat face. The numbering of the points signifies their order in a line of annotation data and in <kbd>coords</kbd>. To begin the function, we get the coordinates for the eyes and mouth. If the face is upside down (not an uncommon pose in playful or sleepy cats), we swap our definitions of left and right eyes to be consistent with an upright pose. (In determining whether the face is upside down, we rely in part on the position of the mouth relative to the eyes.) Then, we find the angle between the eyes and we rotate the image so that the face becomes upright. An OpenCV function called <kbd>cv2.getRotationMatrix2D</kbd> is used to define the rotation, and another function called <kbd>cv2.warpAffine</kbd> is used to apply it. As a result of rotating border regions, some blank regions are introduced into the image. We may specify a fill color for these regions as an argument to <kbd>cv2.warpAffine</kbd>. We use 50% gray, since it has the least tendency to bias the equalization of the image. Here is the implementation of this first part of the <kbd>preprocessCatFace</kbd> function:</p>
<pre>def preprocessCatFace(coords, image):<br/>    <br/>    leftEyeX, leftEyeY = coords[0], coords[1]<br/>    rightEyeX, rightEyeY = coords[2], coords[3]<br/>    mouthX = coords[4]<br/>    if leftEyeX &gt; rightEyeX and leftEyeY &lt; rightEyeY and \<br/>            mouthX &gt; rightEyeX:<br/>        # The "right eye" is in the second quadrant of the face,<br/>        # while the "left eye" is in the fourth quadrant (from the<br/>        # viewer's perspective.) Swap the eyes' labels in order to<br/>        # simplify the rotation logic.<br/>        leftEyeX, rightEyeX = rightEyeX, leftEyeX<br/>        leftEyeY, rightEyeY = rightEyeY, leftEyeY<br/><br/>    eyesCenter = (0.5 * (leftEyeX + rightEyeX),<br/>                  0.5 * (leftEyeY + rightEyeY))<br/>    <br/>    eyesDeltaX = rightEyeX - leftEyeX<br/>    eyesDeltaY = rightEyeY - leftEyeY<br/>    eyesAngleRadians = math.atan2(eyesDeltaY, eyesDeltaX)<br/>    eyesAngleDegrees = eyesAngleRadians * 180.0 / math.pi<br/>    <br/>    # Straighten the image and fill in gray for blank borders.<br/>    rotation = cv2.getRotationMatrix2D(<br/>            eyesCenter, eyesAngleDegrees, 1.0)<br/>    imageSize = image.shape[1::-1]<br/>    straight = cv2.warpAffine(image, rotation, imageSize,<br/>                              borderValue=(128, 128, 128))</pre>
<p>As well as straightening the image, we call <kbd>rotateCoords</kbd> to make feature coordinates that match the straightened image. Here is the code for this function call:</p>
<pre>    # Straighten the coordinates of the features.<br/>    newCoords = rotateCoords(<br/>            coords, eyesCenter, eyesAngleRadians)</pre>
<p>At this stage, the image and feature coordinates are transformed so that the cat's eyes are level and upright. Next, let's crop the image to eliminate most of the background and to standardize the eyes' position relative to the bounds. Arbitrarily, we define the cropped face to be a square region, as wide as the distance between the outer base points of the cat's ears. This square is positioned so that half its area lies to the left of the midpoint between the cat's eyes, half lies to the right, 40% lies above, and 60% lies below. For an ideal frontal cat face, this crop excludes all background regions, but includes the eyes, chin, and several fleshy regions—the nose, mouth, and part of the inside of the ears. We equalize and return the cropped image. Accordingly, the implementation of <kbd>preprocessCatFace</kbd> proceeds as follows:</p>
<pre>    # Make the face as wide as the space between the ear bases.<br/>    # (The ear base positions are specified in the reference<br/>    # coordinates.)<br/>    w = abs(newCoords[16] - newCoords[6])<br/>    # Make the face square.<br/>    h = w<br/>    # Put the center point between the eyes at (0.5, 0.4) in<br/>    # proportion to the entire face.<br/>    minX = eyesCenter[0] - w/2<br/>    if minX &lt; 0:<br/>        w += minX<br/>        minX = 0<br/>    minY = eyesCenter[1] - h*2/5<br/>    if minY &lt; 0:<br/>        h += minY<br/>        minY = 0<br/>    <br/>    # Crop the face.<br/>    crop = straight[int(minY):int(minY+h), int(minX):int(minX+w)]<br/>    # Convert the crop to equalized grayscale.<br/>    crop = equalizedGray(crop)<br/>    # Return the crop.<br/>    return crop</pre>
<p>During cropping, we usually eliminate the blank border region that was introduced during rotation. However, if the cat face was close to the border of the original image, some of the rotated gray border region may remain.</p>
<p>The following pair of screenshots is an example of input and output for the <kbd>processCatFace</kbd> function. First, there's the input:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-199 image-border" src="img/f23403bc-aabd-42a5-82d9-1dcd5a023e9b.png" style="width:17.75em;height:14.58em;"/></p>
<p>The output is displayed in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-200 image-border" src="img/943b2738-7ab0-45be-b71c-419354f919b5.png" style="width:17.67em;height:17.58em;"/></p>
<p>To generate the positive description file, we iterate over all the images in the Microsoft Cat Dataset 2008. For each image, we parse the cat feature coordinates from the corresponding <kbd>.cat</kbd> file and we generate the straightened, cropped, and equalized image by passing the coordinates and original image to our <kbd>processCatFace</kbd> function. We append each processed image path and measurements to the positive description file. Here is the implementation:</p>
<pre>def describePositive():<br/>    output = open('positive_description.txt', 'w')<br/>    dirs = ['CAT_DATASET_01/CAT_00',<br/>            'CAT_DATASET_01/CAT_01',<br/>            'CAT_DATASET_01/CAT_02',<br/>            'CAT_DATASET_02/CAT_03',<br/>            'CAT_DATASET_02/CAT_04',<br/>            'CAT_DATASET_02/CAT_05',<br/>            'CAT_DATASET_02/CAT_06']<br/>    for dir in dirs:<br/>        for imagePath in glob.glob('%s/*.jpg' % dir):<br/>            if imagePath.endswith(outputImageExtension):<br/>                # This file is a crop, saved on a previous run.<br/>                # Skip it.<br/>                continue<br/>            # Open the '.cat' annotation file associated with this<br/>            # image.<br/>            input = open('%s.cat' % imagePath, 'r')<br/>            # Read the coordinates of the cat features from the<br/>            # file. Discard the first number, which is the number<br/>            # of features.<br/>            coords = [int(i) for i in input.readline().split()[1:]]<br/>            # Read the image.<br/>            image = cv2.imread(imagePath)<br/>            # Straighten and crop the cat face.<br/>            crop = preprocessCatFace(coords, image)<br/>            if crop is None:<br/>                sys.stderr.write(<br/>                        'Failed to preprocess image at %s.\n' % \<br/>                        imagePath)<br/>                continue<br/>            # Save the crop.<br/>            cropPath = '%s%s' % (imagePath, outputImageExtension)<br/>            cv2.imwrite(cropPath, crop)<br/>            # Append the cropped face and its bounds to the<br/>            # positive description.<br/>            h, w = crop.shape[:2]<br/>            print('%s 1 0 0 %d %d' % (cropPath, w, h), file=output)</pre>
<p>Here, let's take note of the format of a positive description file. Each line contains a path to a training image, followed by a series of numbers indicating the count of positive objects in the image and the measurements (x, y, width, and height) of rectangles containing those objects. In our case, there is always one cat face filling the entire cropped image, so we get lines such as the following, which is for a <em>64 x 64</em> image:</p>
<pre>CAT_DATASET_02/CAT_06/00001493_005.jpg.out.jpg 1 0 0 64 64</pre>
<p>Hypothetically, if the image had two <em>8 x 8</em> pixel cat faces in opposite corners, its line in the description file would look like this:</p>
<pre>CAT_DATASET_02/CAT_06/00001493_005.jpg.out.jpg 2 0 0 8 8 56 56 8 8</pre>
<p>The main function of <kbd>describe.py</kbd> simply calls our <kbd>describeNegative</kbd> and <kbd>describePositive</kbd> functions, as follows:</p>
<pre>def main(): <br/>    describeNegative()<br/>    describePositive()<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Run <kbd>describe.py</kbd> and then feel free to have a look at the generated files, including <kbd>negative_description.txt</kbd>, <kbd>positive_description.txt</kbd>, and the cropped cat faces whose filenames follow the <kbd>CAT_DATASET_*/CAT_*/*.out.jpg</kbd> pattern.</p>
<p>Next, we will use two of OpenCV's command-line tools. We will refer to them as <kbd>&lt;opencv_createsamples&gt;</kbd> and <kbd>&lt;opencv_traincascade&gt;</kbd>. They are responsible for converting the positive description to a binary format and generating the Haar cascade in an XML format, respectively. On Windows, these executables are named <kbd>opencv_createsamples.exe</kbd> and <kbd>opencv_traincascade.exe</kbd>. On Mac or Linux, the executables are named <kbd>opencv_createsamples</kbd> and <kbd>opencv_traincascade</kbd>.</p>
<p>For up-to-date instructions on obtaining <kbd>&lt;opencv_createsamples&gt;</kbd> and <kbd>&lt;opencv_traincascade&gt;</kbd>, refer to the README on this book's GitHub web page at <a href="https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/">https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition/</a>. At the time of writing, there is not yet an OpenCV 4.x version of these two command-line tools, but the OpenCV 3.4 version of them is forward-compatible, and work on a 4.x version has been proposed for summer 2019.</p>
<p>Many flags can be used to provide arguments to <kbd>&lt;opencv_createsamples&gt;</kbd> and <kbd>&lt;opencv_traincascade&gt;</kbd>, as described in the official documentation at <a href="https://docs.opencv.org/master/dc/d88/tutorial_traincascade.html">https://docs.opencv.org/master/dc/d88/tutorial_traincascade.html</a>. We use the following flags and values:</p>
<ul>
<li><kbd>vec</kbd>: This is the path to a binary description of the positive training images. This file is generated by <kbd>&lt;opencv_createsamples&gt;</kbd>.</li>
<li><kbd>info</kbd>: This is the path to a text description of the positive training images. We generated this file using <kbd>describe.py</kbd>.</li>
<li><kbd>bg</kbd>: The path to a text description of the negative training images. We generated this file using <kbd>describe.py</kbd>.</li>
<li><kbd>num</kbd>: The number of positive training images in <kbd>info</kbd>.</li>
<li><kbd>numStages</kbd>: The number of stages in the cascade. As we discussed earlier in <em>Conceptualizing Haar cascades and LBPH</em>, each stage is a test that is applied to an image region. If the region passes all tests, it is classified as a frontal cat face (or whatever class of object the positive training set represents). We use <kbd>20</kbd>.</li>
<li><kbd>numPos</kbd>: The number of positive training images used in each stage. It should be significantly smaller than <kbd>num</kbd>. (Otherwise, the trainer will fail, complaining that it has run out of new images to use in new stages.) We use 90% of <kbd>num</kbd>.</li>
<li><kbd>numNeg</kbd>: The number of negative training images used in each stage. We use 90% of the number of negative training images in <kbd>bg</kbd>.</li>
<li><kbd>minHitRate</kbd>: The <strong>hit rate</strong> is also called the <strong>sensitivity</strong>, <strong>recall</strong>, or <strong>true positive rate</strong>. In our case, it is the proportion of cat faces that are correctly classified as such. The <kbd>minHitRate</kbd> parameter specifies the minimum hit rate that <em>each</em> stage must achieve. A higher proportion implies a longer training time but a better fit between the model and the training data. (A better fit is normally a good thing, though it is possible to <strong>overfit</strong> so that the model does not make correct extrapolations beyond the training data.) We use <kbd>0.995</kbd>. With 20 stages, this implies an overall hit rate of <em>0.995 ^ 20</em> or approximately 99%.</li>
<li><kbd>maxFalseAlarmRate</kbd>: The <strong>false alarm rate</strong> is also called the <strong>miss rate</strong> or <strong>false positive rate</strong>. In our case, it is the proportion of backgrounds or non-cat faces that are misclassified as cat faces. The <kbd>maxFalseAlarmRate</kbd> parameter specifies the maximum false alarm rate for <em>each</em> stage. We use <kbd>0.5</kbd>. With 20 stages, this implies an overall false alarm rate of <em>0.5 ^ 20</em> or approximately one in a million.</li>
<li><kbd>featureType</kbd>: The type of features used, either <kbd>HAAR</kbd> (the default) or <kbd>LBP</kbd>. As we discussed previously, Haar cascades tend to be more reliable but are much slower to train and somewhat slower at runtime.</li>
<li><kbd>mode</kbd>: This is the subset of Haar features used. (For LBP, this flag has no effect.) The valid options are <kbd>BASIC</kbd> (the default), <kbd>CORE</kbd>, and <kbd>ALL</kbd>. The <kbd>CORE</kbd> option makes the model slower to train and run, but the benefit is to make the model sensitive to little dots and thick lines. The <kbd>ALL</kbd> option goes further, making the model even slower to train and run but adding sensitivity to diagonal patterns (whereas <kbd>BASIC</kbd> and <kbd>CORE</kbd> are only sensitive to horizontal and vertical patterns). The <kbd>ALL</kbd> option has nothing to do with detecting non-upright subjects. Rather, it relates to detecting subjects that contain diagonal patterns. For example, a cat's whiskers and ears might qualify as diagonal patterns.</li>
</ul>
<p>Let's write a shell script to run <kbd>&lt;opencv_createsamples&gt;</kbd> and <kbd>&lt;opencv_traincascade&gt;</kbd> with the appropriate flags and to copy the resulting Haar cascade to the path where <kbd>Interactive Cat Face Recognizer</kbd> expects it. On Windows, let's call our script <kbd>train.bat</kbd> and implement it as follows:</p>
<pre>REM On Windows, opencv_createsamples and opencv_traincascades expect<br/>REM absolute paths.<br/>REM Set baseDir to be the absolute path to this script's directory.<br/>set baseDir=%~dp0<br/><br/>REM Use baseDir to construct other absolute paths.<br/><br/>set vec=%baseDir%\binary_description<br/>set info=%baseDir%\positive_description.txt<br/>set bg=%baseDir%\negative_description.txt<br/><br/>REM Uncomment the next 4 variables for LBP training.<br/>REM set featureType=LBP<br/>REM set data=%baseDir%\lbpcascade_frontalcatface\\<br/>REM set dst=%baseDir%\..\\cascades\\lbpcascade_frontalcatface.xml<br/>REM set mode=BASIC<br/><br/>REM Uncomment the next 4 variables for Haar training with basic<br/>REM features.<br/>set featureType=HAAR<br/>set data=%baseDir%\haarcascade_frontalcatface\\<br/>set dst=%baseDir%\..\\cascades\\haarcascade_frontalcatface.xml<br/>set mode=BASIC<br/><br/>REM Uncomment the next 4 variables for Haar training with<br/>REM extended features.<br/>REM set featureType=HAAR<br/>REM set data=%baseDir%\haarcascade_frontalcatface_extended\\<br/>REM set dst=%baseDir%\..\\cascades\\haarcascade_frontalcatface_extended.xml<br/>REM set mode=ALL<br/><br/>REM Set numPosTotal to be the line count of info.<br/>for /f %%c in ('find /c /v "" ^&lt; "%info%"') do set numPosTotal=%%c<br/><br/>REM Set numNegTotal to be the line count of bg.<br/>for /f %%c in ('find /c /v "" ^&lt; "%bg%"') do set numNegTotal=%%c<br/><br/>set /a numPosPerStage=%numPosTotal%*9/10<br/>set /a numNegPerStage=%numNegTotal%*9/10<br/>set numStages=20<br/>set minHitRate=0.995<br/>set maxFalseAlarmRate=0.5<br/><br/>REM Ensure that the data directory exists and is empty.<br/>if not exist "%data%" (mkdir "%data%") else del /f /q "%data%\*.xml"<br/><br/>opencv_createsamples -vec "%vec%" -info "%info%" -bg "%bg%" ^<br/>        -num "%numPosTotal%"<br/>opencv_traincascade -data "%data%" -vec "%vec%" -bg "%bg%" ^<br/>        -numPos "%numPosPerStage%" -numNeg "%numNegPerStage%" ^<br/>        -numStages "%numStages%" -minHitRate "%minHitRate%" ^<br/>        -maxFalseAlarmRate "%maxFalseAlarmRate%" ^<br/>        -featureType "%featureType%" -mode "%mode%"<br/><br/>copy /Y "%data%\cascade.xml" "%dst%"</pre>
<p>On Mac or Linux, let's instead call our script <kbd>train.sh</kbd> and implement it as follows:</p>
<pre>#!/bin/sh<br/><br/>vec=binary_description<br/>info=positive_description.txt<br/>bg=negative_description.txt<br/><br/># Uncomment the next 4 variables for LBP training.<br/>#featureType=LBP<br/>#data=lbpcascade_frontalcatface/<br/>#dst=../cascades/lbpcascade_frontalcatface.xml<br/>#mode=BASIC<br/><br/># Uncomment the next 4 variables for Haar training with basic<br/># features.<br/>featureType=HAAR<br/>data=haarcascade_frontalcatface/<br/>dst=../cascades/haarcascade_frontalcatface.xml<br/>mode=BASIC<br/><br/># Uncomment the next 4 variables for Haar training with<br/># extended features.<br/>#featureType=HAAR<br/>#data=haarcascade_frontalcatface_extended/<br/>#dst=../cascades/haarcascade_frontalcatface_extended.xml<br/>#mode=ALL<br/><br/># Set numPosTotal to be the line count of info.<br/>numPosTotal=`wc -l &lt; $info`<br/><br/># Set numNegTotal to be the line count of bg.<br/>numNegTotal=`wc -l &lt; $bg`<br/><br/>numPosPerStage=$(($numPosTotal*9/10))<br/>numNegPerStage=$(($numNegTotal*9/10))<br/>numStages=20<br/>minHitRate=0.995<br/>maxFalseAlarmRate=0.5<br/><br/># Ensure that the data directory exists and is empty.<br/>if [ ! -d "$data" ]; then<br/>    mkdir "$data"<br/>else<br/>    rm "$data/*.xml"<br/>fi<br/><br/>opencv_createsamples -vec "$vec" -info "$info" -bg "$bg" \<br/>        -num "$numPosTotal"<br/>opencv_traincascade -data "$data" -vec "$vec" -bg "$bg" \<br/>        -numPos "$numPosPerStage" -numNeg "$numNegPerStage" \<br/>        -numStages "$numStages" -minHitRate "$minHitRate" \<br/>        -maxFalseAlarmRate "$maxFalseAlarmRate" \<br/>        -featureType "$featureType" -mode "$mode"<br/><br/>cp "$data/cascade.xml" "$dst"</pre>
<p>The preceding versions of the training script are configured to use basic Haar features and will take a long, long time to run, perhaps more than a day. By commenting out the variables related to a basic Haar configuration and uncommenting the variables related to an LBP configuration, we can cut the training time down to several minutes. As a third alternative, variables for an extended Haar configuration (sensitive to diagonal patterns) are also present but currently commented out.</p>
<p>When the training is done, feel free to have a look at the generated files, including the following:</p>
<ul>
<li>For basic Haar features, <kbd>cascades/haarcascade_frontalcatface.xml</kbd> and <kbd>cascade_training/haarcascade_frontalcatface/*</kbd></li>
<li>For extended Haar features, <kbd>cascades/haarcascade_frontalcatface_extended.xml</kbd> and <kbd>cascade_training/haarcascade_frontalcatface_extended/*</kbd></li>
<li>For LBP, <kbd>cascades/lbpcascade_frontalcatface.xml</kbd> and <kbd>cascade_training/lbpcascade_frontalcatface/*</kbd></li>
</ul>
<p>Finally, let's run <kbd>InteractiveCatFaceRecognizer.py</kbd> to test our cascade!</p>
<div><p>Remember that our detector is designed for frontal upright cat faces. The cat should be facing the camera and might need some incentive to hold that pose. For example, you could ask the cat to settle on a blanket or in your lap, and you could pat or comb the cat. See the following screenshot of my colleague, Chancellor Josephine (<em>Little Jo</em>) Antoinette Puddingcat, GRL (Grand Rock of Lambda), sitting for a test.</p>
<p>If you do not have a cat (or even a human) who is willing to participate, then you can simply print a few images of a given cat (or human) from the web. Use heavy, matte paper and hold the print so that it faces the camera. Use prints of some images for training the recognizer and prints of other images for testing it:</p>
</div>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-201 image-border" src="img/1496173d-fee5-46a8-ae44-f33cdfda9fe4.png" style="width:44.92em;height:28.00em;"/></p>
<p>Our detector is pretty good at finding frontal cat faces. However, I encourage you to experiment further, make it better, and share your results! The current version sometimes mistakes the center of a frontal human face for a frontal cat face. Perhaps we should have used more databases of human faces as negative training images. Alternatively, if we had used faces of several mammal species as positive training images, could we have created a more general mammal face detector? Let me know what you discover!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the Angora Blue app</h1>
                
            
            
                
<p><kbd>Angora Blue</kbd> reuses the same detection and recognition models that we created earlier. It is a relatively linear and simple app because it has no GUI and does not modify any models. It just loads the detection and recognition models from file and then silently runs a camera until a face is recognized with a certain level of confidence. After recognizing a face, the app sends an email alert and exits. To elaborate, we may say the app has the following flow of execution:</p>
<ol>
<li>Load face detection and face recognition models from file for both human and feline subjects.</li>
<li>Capture a live video from a camera. For each frame of video, it can do the following:</li>
</ol>
<ul>
<li style="padding-left: 60px">Detect all human faces in the frame. Perform recognition on each human face. If a face is recognized with a certain level of confidence, it sends an email alert and exits the app.</li>
<li style="padding-left: 60px">Detect all cat faces in the frame. Discard any cat faces that intersect with human faces. (We assume that such cat faces are false positives, since our cat detector sometimes mistakes human faces for cat faces.) For each remaining cat face, it performs recognition. If a face is recognized with a certain level of confidence, it sends an email alert and exits the app.</li>
</ul>
<p><kbd>Angora Blue</kbd> is capable of running on a Raspberry Pi. The Pi's small size makes it a nice platform for a hidden alarm system! Make sure that the Pi or other machine is connected to the internet in order to send email messages.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the Angora Blue app</h1>
                
            
            
                
<p>The <kbd>Angora Blue</kbd> app uses three new files—<kbd>GeomUtils.py</kbd>, <kbd>MailUtils.py</kbd>, and <kbd>AngoraBlue.py</kbd>, which should all be in our project's top folder. Given the app's dependencies on our previous work, the following files are relevant to <kbd>Angora Blue</kbd>:</p>
<ul>
<li><kbd>cascades/haarcascade_frontalface_alt.xml</kbd></li>
<li><kbd>cascades/haarcascade_frontalcatface.xml</kbd></li>
<li><kbd>recognizers/lbph_human_faces.xml</kbd></li>
<li><kbd>recognizers/lbph_cat_faces.xml</kbd></li>
<li><kbd>ResizeUtils.py</kbd>: A utility function for resizing images, including camera capture dimensions</li>
<li><kbd>GeomUtils.py</kbd>: A utility function for geometric operations</li>
<li><kbd>MailUtils.py</kbd>: A utility function for sending emails</li>
<li><kbd>AngoraBlue.py</kbd>: The application that sends an email alert when a person or cat is recognized</li>
</ul>
<p>First, let's create <kbd>GeomUtils.py</kbd>. It doesn't need any import statements. Let's add the following <kbd>intersects</kbd> function, which accepts two rectangles as arguments and returns either <kbd>True</kbd> (if they intersect) or <kbd>False</kbd> (otherwise), as shown in the following code:</p>
<pre>def intersects(rect0, rect1):<br/>    x0, y0, w0, h0 = rect0<br/>    x1, y1, w1, h1 = rect1<br/>    if x0 &gt; x1 + w1: # rect0 is wholly to right of rect1<br/>        return False<br/>    if x1 &gt; x0 + w0: # rect1 is wholly to right of rect0<br/>        return False<br/>    if y0 &gt; y1 + h1: # rect0 is wholly below rect1<br/>        return False<br/>    if y1 &gt; y0 + h0: # rect1 is wholly below rect0<br/>        return False<br/>    return True</pre>
<p>Using the <kbd>intersects</kbd> function, let's write the following <kbd>difference</kbd> function, which accepts two lists of rectangles, <kbd>rects0</kbd> and <kbd>rects1</kbd>, and returns a new list containing the rectangles in <kbd>rects0</kbd> that don't intersect with any rectangle in <kbd>rects1</kbd>:</p>
<pre>def difference(rects0, rects1):<br/>    result = []<br/>    for rect0 in rects0:<br/>        anyIntersects = False<br/>        for rect1 in rects1:<br/>            if intersects(rect0, rect1):<br/>                anyIntersects = True<br/>                break<br/>        if not anyIntersects:<br/>            result += [rect0]<br/>    return result</pre>
<p>Later, we will use the <kbd>difference</kbd> function to filter out cat faces that intersect with human faces.</p>
<p>Now, let's create <kbd>MailUtils.py</kbd>. It needs the following <kbd>import</kbd> statement:</p>
<pre>import smtplib</pre>
<p>For the task of sending an email, let's copy the following function from Rosetta Code, a free wiki that offers utility functions in many programming languages, as shown in the following code:</p>
<pre>def sendEmail(fromAddr, toAddrList, ccAddrList, subject, message,<br/>              login, password, smtpServer='smtp.gmail.com:587'):<br/><br/>    # Taken from http://rosettacode.org/wiki/Send_an_email#Python<br/><br/>    header = 'From: %s\n' % fromAddr<br/>    header += 'To: %s\n' % ','.join(toAddrList)<br/>    header += 'Cc: %s\n' % ','.join(ccAddrList)<br/>    header += 'Subject: %s\n\n' % subject<br/>    message = header + message<br/><br/>    server = smtplib.SMTP(smtpServer)<br/>    server.starttls()<br/>    server.login(login,password)<br/>    problems = server.sendmail(fromAddr, toAddrList, message)<br/>    server.quit()<br/>    return problems</pre>
<p>By default, the <kbd>sendEmail</kbd> function uses Gmail. By specifying the optional <kbd>smtpServer</kbd> argument, we can use a different service.</p>
<p>Since July 2014, the default security settings on Google accounts require apps to use not only SMTP authentication but also OAuth authentication in order to send an email through Gmail. Our <kbd>sendEmail</kbd> function uses a secure TLS connection but handles SMTP authentication only (as this is sufficient for most email services other than Gmail). To reconfigure your Google account for compatibility with our function, log in to your account, go to <a href="https://www.google.com/settings/security/lesssecureapps">https://www.google.com/settings/security/lesssecureapps</a>, select the Enable option, and click Done. For best security, you might wish to create a dummy Google account for this project and apply the custom security setting to this dummy account only. Alternatively, most email services besides Gmail should not require special configuration.</p>
<p>Now, we are ready to implement <kbd>AngoraBlue.py</kbd>. It starts with the following shebang line and imports:</p>
<pre>#!/usr/bin/env python<br/><br/><br/>import numpy # Hint to PyInstaller<br/>import cv2<br/>import getpass<br/>import os<br/>import socket<br/>import sys<br/><br/>import BinasciiUtils<br/>import GeomUtils<br/>import MailUtils<br/>import PyInstallerUtils<br/>import ResizeUtils</pre>
<p><kbd>Angora Blue</kbd> simply uses a <kbd>main</kbd> function and one helper function, <kbd>recognizeAndReport</kbd>. This helper function begins as follows, by iterating over a given list of face rectangles and using a given recognizer (be it a human recognizer or a cat recognizer) to get a label and distance (non-confidence) for each face, as shown in the following code:</p>
<pre>def recognizeAndReport(recognizer, grayImage, rects, maxDistance,<br/>                       noun, smtpServer, login, password, fromAddr,<br/>                       toAddrList, ccAddrList):<br/>    for x, y, w, h in rects:<br/>        crop = cv2.equalizeHist(grayImage[y:y+h, x:x+w])<br/>        labelAsInt, distance = recognizer.predict(crop)<br/>        labelAsStr = BinasciiUtils.intToFourChars(labelAsInt)</pre>
<p>For testing, it is useful to log the recognition results here. However, we comment out the logging in the final version, as follows:</p>
<pre>        #print('%s %s %d' % (noun, labelAsStr, distance))</pre>
<p>If any of the faces is recognized with a certain level of confidence (based on a <kbd>maxDistance</kbd> argument), we attempt to send an email alert. If the alert is sent successfully, the function returns <kbd>True</kbd>, meaning it did recognize and report a face. Otherwise, it returns <kbd>False</kbd>. Here is the remainder of the implementation:</p>
<pre>        if distance &lt;= maxDistance:<br/>            subject = 'Angora Blue'<br/>            message = 'We have sighted the %s known as %s.' % \<br/>                    (noun, labelAsStr)<br/>            try:<br/>                problems = MailUtils.sendEmail(<br/>                        fromAddr, toAddrList, ccAddrList, subject,<br/>                        message, login, password, smtpServer)<br/>                if problems:<br/>                    sys.stderr.write(<br/>                            'Email problems: {0}\n'.format(problems))<br/>                else:<br/>                    return True<br/>            except socket.gaierror:<br/>                sys.stderr.write('Unable to reach email server\n')<br/>    return False</pre>
<p>The <kbd>main</kbd> function starts by defining paths to the detection and recognition models. If either recognition model does not exist (because it has not been trained), we print an error and exit, as follows:</p>
<pre>def main():<br/><br/>    humanCascadePath = PyInstallerUtils.resourcePath(<br/>            # Uncomment the next argument for LBP.<br/>            #'cascades/lbpcascade_frontalface.xml')<br/>            # Uncomment the next argument for Haar.<br/>            'cascades/haarcascade_frontalface_alt.xml')<br/>    humanRecognizerPath = PyInstallerUtils.resourcePath(<br/>            'recognizers/lbph_human_faces.xml')<br/>    if not os.path.isfile(humanRecognizerPath):<br/>        sys.stderr.write(<br/>                'Human face recognizer not trained. Exiting.\n')<br/>        return<br/><br/>    catCascadePath = PyInstallerUtils.resourcePath(<br/>            # Uncomment the next argument for LBP.<br/>            #'cascades/lbpcascade_frontalcatface.xml')<br/>            # Uncomment the next argument for Haar with basic<br/>            # features.<br/>            #'cascades/haarcascade_frontalcatface.xml')<br/>            # Uncomment the next argument for Haar with extended<br/>            # features.<br/>            'cascades/haarcascade_frontalcatface_extended.xml')<br/>    catRecognizerPath = PyInstallerUtils.resourcePath(<br/>            'recognizers/lbph_cat_faces.xml')<br/>    if not os.path.isfile(catRecognizerPath):<br/>        sys.stderr.write(<br/>                'Cat face recognizer not trained. Exiting.\n')<br/>        return</pre>
<p>We prompt the user to enter email credentials and recipients, and we store the user responses in local variables, as shown in the following code:</p>
<pre>    print('What email settings shall we use to send alerts?')<br/><br/>    defaultSMTPServer = 'smtp.gmail.com:587'<br/>    print('Enter SMTP server (default: %s):' % defaultSMTPServer)<br/>    smtpServer = sys.stdin.readline().rstrip()<br/>    if not smtpServer:<br/>        smtpServer = defaultSMTPServer<br/><br/>    print('Enter username:')<br/>    login = sys.stdin.readline().rstrip()<br/><br/>    print('Enter password:')<br/>    password = getpass.getpass('')<br/><br/>    defaultAddr = '%s@gmail.com' % login<br/>    print('Enter "from" email address (default: %s):' % defaultAddr)<br/>    fromAddr = sys.stdin.readline().rstrip()<br/>    if not fromAddr:<br/>        fromAddr = defaultAddr<br/><br/>    print('Enter comma-separated "to" email addresses (default: '<br/>          '%s):' % defaultAddr)<br/>    toAddrList = sys.stdin.readline().rstrip().split(',')<br/>    if toAddrList == ['']:<br/>        toAddrList = [defaultAddr]<br/><br/>    print('Enter comma-separated "c.c." email addresses:')<br/>    ccAddrList = sys.stdin.readline().rstrip().split(',')</pre>
<p>As in <kbd>Interactive Recognizer</kbd>, we start capturing video from a camera and we store the video's resolution in order to calculate the relative, minimum size of a face. Here is the relevant code:</p>
<pre>    capture = cv2.VideoCapture(0)<br/>    imageWidth, imageHeight = \<br/>            ResizeUtils.cvResizeCapture(capture, (1280, 720))<br/>    minImageSize = min(imageWidth, imageHeight)</pre>
<p>We load detectors and recognizers from file and set a minimum face size for detection and maximum distance (non-confidence) for recognition. We specify the values separately for human and feline subjects. You may need to tweak the values based on your particular camera setup and models. The code proceeds as follows:</p>
<pre>    humanDetector = cv2.CascadeClassifier(humanCascadePath)<br/>    humanRecognizer = cv2.face.LBPHFaceRecognizer_create()<br/>    humanRecognizer.read(humanRecognizerPath)<br/>    humanMinSize = (int(minImageSize * 0.25),<br/>                    int(minImageSize * 0.25))<br/>    humanMaxDistance = 25<br/><br/>    catDetector = cv2.CascadeClassifier(catCascadePath)<br/>    catRecognizer = cv2.face.LBPHFaceRecognizer_create()<br/>    catRecognizer.read(catRecognizerPath)<br/>    catMinSize = (int(minImageSize * 0.125),<br/>                  int(minImageSize * 0.125))<br/>    catMaxDistance = 25</pre>
<p>We read frames from the camera continuously until an email alert is sent as a result of face recognition. Each frame is converted into grayscale and equalized. Next, we detect and recognize human faces and possibly send an alert, as follows:</p>
<pre>    while True:<br/>        success, image = capture.read()<br/>        if image is not None:<br/>            grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br/>            equalizedGrayImage = cv2.equalizeHist(grayImage)<br/><br/>            humanRects = humanDetector.detectMultiScale(<br/>                    equalizedGrayImage, scaleFactor=1.3,<br/>                    minNeighbors=4, minSize=humanMinSize)<br/>            if recognizeAndReport(<br/>                    humanRecognizer, grayImage, humanRects,<br/>                    humanMaxDistance, 'human', smtpServer, login,<br/>                    password, fromAddr, toAddrList, ccAddrList):<br/>                break</pre>
<p>If no alert has been sent, we continue to cat-detection and recognition. For cat-detection, we make extra efforts to eliminate false positives by specifying a higher <kbd>minNeighbors</kbd> value and by filtering out any cat faces that intersect human faces. Here is this final part of Angora Blue's implementation:</p>
<pre>            catRects = catDetector.detectMultiScale(<br/>                    equalizedGrayImage, scaleFactor=1.2,<br/>                    minNeighbors=1, minSize=catMinSize)<br/>            # Reject any cat faces that overlap with human faces.<br/>            catRects = GeomUtils.difference(catRects, humanRects)<br/>            if recognizeAndReport(<br/>                    catRecognizer, grayImage, catRects,<br/>                    catMaxDistance, 'cat', smtpServer, login,<br/>                    password, fromAddr, toAddrList, ccAddrList):<br/>                break<br/><br/>if __name__ == '__main__':<br/>    main()</pre>
<p>Before testing <kbd>Angora Blue</kbd>, ensure that the two recognition models are trained using <kbd>Interactive Human Face Recognizer</kbd> and <kbd>Interactive Cat Face Recognizer</kbd>. Preferably, each model should contain two or more individuals. Then, set up a computer and webcam in a place where frontal human faces and frontal cat faces will be encountered. Try to get your friends and pets to participate in the following test cases:</p>
<ul>
<li>A human, who is unknown to the model, looks into the camera. Nothing should happen. If you get an email alert, increase <kbd>humanMaxDistance</kbd> and try again.</li>
<li>A cat, who is unknown to the model, looks into the camera. Nothing should happen. If you get an email alert, increase <kbd>catMaxDistance</kbd> and try again.</li>
<li>A human, who is known to the model, looks into the camera. You should get an email alert. If not, decrease <kbd>humanMaxDistance</kbd> or rerun <kbd>Interactive Human Face Recognizer</kbd> to add more samples of the given human face. Try <kbd>Angora Blue</kbd> again.</li>
<li>A cat, who is known to the model, looks into the camera. You should get an email alert. If not, decrease <kbd>catMaxDistance</kbd> or rerun <kbd>Interactive Cat Face Recognizer</kbd> to add more samples of the given cat face. Try <kbd>Angora Blue</kbd> again.</li>
</ul>
<p>Again, if you don't have enough human or feline volunteers, just get some heavy, matte paper and print faces from the web. Hold a print so that it is visible (and upright) from the camera's perspective, but ensure that you stay out of view so that the recognizer runs only on the print, not on you.</p>
<p>Once the recognition model and <kbd>Angora Blue</kbd> are tweaked, we are ready to deploy our alarm system to a vast network of webcam-enabled computers! Let the search for the blue-eyed Angora begin!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building Angora Blue for distribution</h1>
                
            
            
                
<p>We can use PyInstaller to bundle <kbd>Angora Blue</kbd>, along with detection and recognition models, for distribution. Since the build scripts should be quite similar to the ones we used for Luxocator (the <a href="4ec4e82a-b63d-4fc1-bf3b-47c653c25a79.xhtml">Chapter 2</a>, <em>Searching for Luxury Accommodations Worldwide,</em> project), we will not discuss their implementation here. However, they are included in this book's GitHub repository.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Further fun with finding felines</h1>
                
            
            
                
<p><strong>Kittydar</strong> (short for <strong>kitty radar</strong>), by Heather Arthur, is an open source, JavaScript library for detecting upright frontal cat faces. You can find its demo application at <a href="http://harthur.github.io/kittydar/">http://harthur.github.io/kittydar/</a> and its source code at <a href="https://github.com/harthur/kittydar">https://github.com/harthur/kittydar</a>.</p>
<p>Another detector for upright frontal cat faces was developed by Microsoft Research using the Microsoft Cat Dataset 2008. The detector is described in the following research paper, but no demo application or source code has been released:</p>
<p>Weiwei Zhang, Jian Sun, and Xiaoou Tang. Cat Head Detection - How to Effectively Exploit Shape and Texture Features, <em>Proc. of European Conf. Computer Vision</em>, vol. 4, pp. 802-816, 2008.</p>
<p>If you know of other work on cat detectors, recognizers, or datasets, please write to tell me about it!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Like the previous chapter, this chapter has dealt with classification tasks, as well as interfaces among OpenCV, a source of images, and a GUI. This time, our classification labels have more objective meanings (a species or an individual's identity), so the classifier's success or failure is more obvious. To meet the challenge, we used much bigger sets of training images, we preprocessed the training images for greater consistency, and we applied two tried-and-true classification techniques in sequence (either Haar cascades or LBP cascades for detection and then LBPH for recognition).</p>
<p>The methodology presented in this chapter, as well as the entire <kbd>Interactive Recognizer</kbd> app and some of the other code, generalizes well to other original work in detection and recognition. With the right training images, you could detect and recognize many more animals in many poses. You could even detect an object such as a car and recognize the Batmobile!</p>
<p>For our next project, we turn our attention to a moving target, literally. We will try to detect a person who is in motion and then recognize particular gestures.</p>


            

            
        
    </body></html>