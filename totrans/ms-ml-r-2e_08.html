<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Cluster Analysis</h1>
            </header>

            <article>
                
<div class="packt_quote">"Quickly bring me a beaker of wine, so that I may wet my mind and say something clever."<br/>
                                                                                - Aristophanes, Athenian Playwright</div>
<p>In the earlier chapters, we focused on trying to learn the best algorithm in order to solve an outcome or response, for example, a breast cancer diagnosis or level of Prostate Specific Antigen. In all these cases, we had <em>y,</em> and that <em>y</em> is a function of <em>x,</em> or <em>y = f(x)</em>. In our data, we had the actual <em>y</em> values and we could train the <em>x</em> accordingly. This is referred to as <strong>supervised learning</strong>. However, there are many situations where we try to learn something from our data and either we do not have the <em>y</em> or we actually choose to ignore it. If so, we enter the world of <strong>unsupervised learning</strong>. In this world, we build and select our algorithm based on how well it addresses our business needs versus how accurate it is.</p>
<p>Why would we try and learn without supervision? First of all, unsupervised learning can help you understand and identify patterns in your data, which may be valuable. Second, you can use it to transform your data in order to improve your supervised learning techniques.</p>
<p>This chapter will focus on the former and the next chapter on the latter.</p>
<p>So, let's begin by tackling a popular and powerful technique known as <strong>cluster analysis</strong>. With cluster analysis, the goal is to group the observations into a number of groups (k-groups), where the members in a group are as similar as possible while the members between groups are as different as possible. There are many examples of how this can help an organization; here are just a few:</p>
<ul>
<li>The creation of customer types or segments</li>
<li>The detection of high-crime areas in a geography</li>
<li>Image and facial recognition</li>
</ul>
<ul>
<li>Genetic sequencing and transcription</li>
<li>Petroleum and geological exploration</li>
</ul>
<p>There are many uses of cluster analysis but there are also many techniques. We will focus on the two most common: <strong>hierarchical</strong> and <strong>k-means</strong>. They are both effective clustering methods, but may not always be appropriate for the large and varied datasets that you may be called upon to analyze. Therefore, we will also examine <strong>Partitioning Around Medoids</strong> (<strong>PAM</strong>) using a <strong>Gower-based</strong> metric dissimilarity matrix as the input.  Finally, we will examine a new methodology I recently learned and applied using <strong>Random Forest</strong> to transform your data.  The transformed data can then be used as an input to unsupervised learning.</p>
<p>A final comment before moving on. You may be asked if these techniques are more art than science as the learning is unsupervised. I think the clear answer is, <em>it depends</em>. In early 2016, I presented the methods here at a meeting of the Indianapolis, Indiana R-User Group. To a person, we all agreed that it is the judgment of the analysts and the business users that makes unsupervised learning meaningful and determines whether you have, say, three versus four clusters in your final algorithm.  This quote sums it up nicely:</p>
<div class="packt_quote">"The major obstacle is the difficulty in evaluating a clustering algorithm without taking into account the context: why does the user cluster his data in the first place, and what does he want to do with the clustering afterwards? We argue that clustering should not be treated as an application-independent mathematical problem, but should always be studied in the context of its end-use."<br/>
                                                                                                   - Luxburg et al. (2012)</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Hierarchical clustering</h1>
            </header>

            <article>
                
<p>The hierarchical clustering algorithm is based on a dissimilarity measure between observations. A common measure, and what we will use, is <strong>Euclidean distance</strong>. Other distance measures are also available.</p>
<div class="packt_infobox">Hierarchical clustering is an agglomerative or bottom-up technique. By this, we mean that all observations are their own cluster. From there, the algorithm proceeds iteratively by searching all the pairwise points and finding the two clusters that are the most similar. So, after the first iteration, there are <em>n-1</em> clusters, and after the second iteration, there are <em>n-2</em> clusters, and so forth.</div>
<p>As the iterations continue, it is important to understand that in addition to the distance measure, we need to specify the linkage between the groups of observations. Different types of data will demand that you use different cluster linkages. As you experiment with the linkages, you may find that some may create highly unbalanced numbers of observations in one or more clusters. For example, if you have 30 observations, one technique may create a cluster of just one observation, regardless of how many total clusters that you specify. In this situation, your judgment will likely be needed to select the most appropriate linkage as it relates to the data and business case.</p>
<p>The following table lists the types of common linkages, but note that there are others:</p>
<table class="table">
<tbody>
<tr>
<td><strong>Linkage</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>Ward</td>
<td>This minimizes the total within-cluster variance as measured by the sum of squared errors from the cluster points to its centroid</td>
</tr>
<tr>
<td>Complete</td>
<td>The distance between two clusters is the maximum distance between an observation in one cluster and an observation in the other cluster</td>
</tr>
<tr>
<td>Single</td>
<td>The distance between two clusters is the minimum distance between an observation in one cluster and an observation in the other cluster</td>
</tr>
<tr>
<td>Average</td>
<td>The distance between two clusters is the mean distance between an observation in one cluster and an observation in the other cluster</td>
</tr>
<tr>
<td>Centroid</td>
<td>The distance between two clusters is the distance between the cluster centroids</td>
</tr>
</tbody>
</table>
<div class="packt_figure">The output of hierarchical clustering will be a <strong>dendrogram</strong>, which is a tree-like diagram that shows the arrangement of the various clusters.</div>
<p>As we will see, it can often be difficult to identify a clear-cut breakpoint in the selection of the number of clusters. Once again, your decision should be iterative in nature and focused on the context of the business decision.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Distance calculations</h1>
            </header>

            <article>
                
<p>As mentioned previously, Euclidean distance is commonly used to build the input for hierarchical clustering. Let's look at a simple example of how to calculate it with two observations and two variables/features.</p>
<p>Let's say that observation <em>A</em> costs $5.00 and weighs 3 pounds. Further, observation <em>B</em> costs $3.00 and weighs 5 pounds. We can place these values in the distance formula: <em>distance between A and B is equal to the square root of the sum of the squared differences</em>, which in our example would be as follows:</p>
<div class="packt_figure"><em>d(A, B) = square root((5 - 3)<sup>2</sup> + (3 - 5)<sup>2</sup>) </em>, which is equal to <em>2.83</em></div>
<p>The value of 2.83 is not a meaningful value in and of itself, but is important in the context of the other pairwise distances. This calculation is the default in R for the <kbd>dist()</kbd> function. You can specify other distance calculations (maximum, manhattan, canberra, binary, and minkowski) in the function. We will avoid going in to detail on why or where you would choose these over Euclidean distance. This can get rather domain-specific, for example, a situation where Euclidean distance may be inadequate is where your data suffers from high-dimensionality, such as in a genomic study. It will take domain knowledge and/or trial and error on your part to determine the proper distance measure.</p>
<div class="packt_infobox">One final note is to scale your data with a mean of zero and standard deviation of one so that the distance calculations are comparable. If not, any variable with a larger scale will have a larger effect on distances.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">K-means clustering</h1>
            </header>

            <article>
                
<p>With k-means, we will need to specify the exact number of clusters that we want. The algorithm will then iterate until each observation belongs to just one of the k-clusters. The algorithm's goal is to minimize the within-cluster variation as defined by the squared Euclidean distances. So, the kth-cluster variation is the sum of the squared Euclidean distances for all the pairwise observations divided by the number of observations in the cluster.</p>
<p>Due to the iteration process that is involved, one k-means result can differ greatly from another result even if you specify the same number of clusters. Let's see how this algorithm plays out:</p>
<ol>
<li><strong>Specify</strong> the exact number of clusters you desire (k).</li>
<li><strong>Initialize</strong> K observations are randomly selected as the initial <em>means.</em></li>
</ol>
<ol start="3">
<li><strong>Iterate</strong>:
<ul>
<li>K clusters are created by assigning each observation to its closest cluster center (minimizing within-cluster sum of squares)</li>
<li>The centroid of each cluster becomes the new <em>mean</em></li>
<li>This is repeated until convergence, that is the cluster centroids do not change</li>
</ul>
</li>
</ol>
<p>As you can see, the final result will vary because of the initial assignment in step 1. Therefore, it is important to run multiple initial starts and let the software identify the best solution. In R, this can be a simple process as we will see.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Gower and partitioning around medoids</h1>
            </header>

            <article>
                
<p>As you conduct clustering analysis in real life, one of the things that can quickly become apparent is the fact that neither hierarchical nor k-means is specifically designed to handle mixed datasets. By mixed data, I mean both quantitative and qualitative or, more specifically, nominal, ordinal, and interval/ratio data.</p>
<p>The reality of most datasets that you will use is that they will probably contain mixed data. There are a number of ways to handle this, such as doing <strong>Principal Components Analysis</strong> (<strong>PCA</strong>) first in order to create latent variables, then using them as input in clustering or using different dissimilarity calculations. We will discuss PCA in the next chapter.</p>
<p>With the power and simplicity of R, you can use the <strong>Gower</strong> <strong>dissimilarity coefficient</strong> to turn mixed data to the proper feature space. In this method, you can even include factors as input variables. Additionally, instead of k-means, I recommend using the <strong>PAM clustering algorithm</strong>.</p>
<p>PAM is very similar to k-means but offers a couple of advantages. They are listed as follows:</p>
<ul>
<li>First, PAM accepts a dissimilarity matrix, which allows the inclusion of mixed data</li>
<li>Second, it is more robust to outliers and skewed data because it minimizes a sum of dissimilarities instead of a sum of squared Euclidean distances (Reynolds, 1992)</li>
</ul>
<p>This is not to say that you must use Gower and PAM together. If you choose, you can use the Gower coefficients with hierarchical and I've seen arguments for and against using it in the context of k-means. Additionally, PAM can accept other linkages. However, when paired they make an effective method to handle mixed data. Let's take a quick look at both of these concepts before moving on.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Gower</h1>
            </header>

            <article>
                
<p>The Gower coefficient compares cases pairwise and calculates a dissimilarity between them, which is essentially the weighted mean of the contributions of each variable. It is defined for two cases called <em>i</em> and <em>j</em> as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="36" width="268" src="assets/B06473_08_01.png"/></div>
<p>Here, <em>S<sub>ijk</sub></em> is the contribution provided by the <em>k</em><sub>th</sub> variable, and <em>W<sub>ijk</sub></em> is 1 if the <em>k</em><sub>th</sub> variable is valid, or else <em>0</em>.</p>
<p>For ordinal and continuous variables, <em>S<sub>ijk</sub> = 1 - (absolute value of x<sub>ij</sub> - x<sub>ik</sub>) / r<sub>k</sub></em>, where <em>r<sub>k</sub></em> is the range of values for the <em>k</em><sub>th</sub> variable.</p>
<p>For nominal variables, <em>S<sub>ijk</sub> = 1</em> if <em>x<sub>ij</sub> = x<sub>jk</sub></em>, or else <em>0</em>.</p>
<p>For binary variables, <em>S<sub>ijk</sub></em> is calculated based on whether an attribute is present (+) or not present (-), as shown in the following table:</p>
<table class="table">
<tbody>
<tr>
<td><strong>Variables</strong></td>
<td><strong>Value of attribute <em>k</em></strong></td>
</tr>
<tr>
<td>Case <em>i</em></td>
<td>+</td>
<td>+</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Case <em>j</em></td>
<td>+</td>
<td>-</td>
<td>+</td>
<td>-</td>
</tr>
<tr>
<td>Sijk</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Wijk</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<div class="packt_figure"/>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">PAM</h1>
            </header>

            <article>
                
<p>For <strong>Partitioning Around Medoids</strong>, let's first define a <strong>medoid</strong>.</p>
<div class="packt_infobox">A medoid is an observation of a cluster that minimizes the dissimilarity (in our case, calculated using the Gower metric) between the other observations in that cluster. So, similar to k-means, if you specify five clusters, you will have five partitions of the data.</div>
<p>With the objective of minimizing the dissimilarity of all the observations to the nearest medoid, the PAM algorithm iterates over the following steps:</p>
<ol>
<li>Randomly select k observations as the initial medoid.</li>
<li>Assign each observation to the closest medoid.</li>
<li>Swap each medoid and non-medoid observation, computing the dissimilarity cost.</li>
<li>Select the configuration that minimizes the total dissimilarity.</li>
<li>Repeat steps 2 through 4 until there is no change in the medoids.</li>
</ol>
<p>Both Gower and PAM can be called using the <kbd>cluster</kbd> package in R. For Gower, we will use the <kbd>daisy()</kbd> function in order to calculate the dissimilarity matrix and the <kbd>pam()</kbd> function for the actual partitioning. With this, let's get started with putting these methods to the test.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random forest</h1>
            </header>

            <article>
                
<p>Like our motivation with the use of the Gower metric in handling mixed, in fact, <em>messy</em> data, we can apply random forest in an unsupervised fashion.  Selection of this method has some advantages:</p>
<ul>
<li>Robust against outliers and highly skewed variables</li>
<li>No need to transform or scale the data</li>
<li>Handles mixed data (numeric and factors)</li>
<li>Can accommodate missing data</li>
<li>Can be used on data with a large number of variables, in fact, it can be used to eliminate useless features by examining variable importance</li>
<li>The dissimilarity matrix produced serves as an input to the other techniques discussed earlier (hierarchical, k-means, and PAM) </li>
</ul>
<p>A couple words of caution.  It may take some trial and error to properly tune the Random Forest with respect to the number of variables sampled at each tree split (<kbd>mtry = ?</kbd> in the function) and the number of trees grown.  Studies done show that the more trees grown, up to a point, provide better results, and a good starting point is to grow 2,000 trees (Shi, T. &amp; Horvath, S., 2006).</p>
<p>This is how the algorithm works, given a data set with no labels:</p>
<ul>
<li>The current observed data is labeled as class 1</li>
<li>A second (synthetic) set of observations are created of the same size as the observed data; this is created by randomly sampling from each of the features from the observed data, so if you have 20 observed features, you will have 20 synthetic features</li>
<li>The synthetic portion of the data is labeled as class 2, which facilitates using Random Forest as an artificial classification problem</li>
<li>Create a Random Forest model to distinguish between the two classes</li>
<li>Turn the model's proximity measures of just the observed data (the synthetic data is now discarded) into a dissimilarity matrix</li>
<li>Utilize the dissimilarity matrix as the clustering input features</li>
</ul>
<p>So what exactly are these proximity measures?  </p>
<div class="packt_infobox">Proximity measure is a pairwise measure between all the observations. If two observations end up in the same terminal node of a tree, their proximity score is equal to one, otherwise zero.</div>
<p> At the termination of the Random Forest run, the proximity scores for the observed data are normalized by dividing by the total number of trees.  The resulting NxN matrix contains scores between zero and one, naturally with the diagonal values all being one.  That's all there is to it.  An effective technique that I believe is underutilized and one that I wish I had learned years ago.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>Until a couple of weeks ago, I was unaware that there were less than 300 certified Master Sommeliers in the entire world. The exam, administered by the Court of Master Sommeliers, is notorious for its demands and high failure rate.<br/>
The trials, tribulations, and rewards of several individuals pursuing the certification are detailed in the critically-acclaimed documentary, <strong>Somm</strong>. So, for this exercise, we will try and help a hypothetical individual struggling to become a Master Sommelier find a latent structure in Italian wines.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>Let's start with loading the R packages that we will need for this chapter. As always, make sure that you have installed them first:</p>
<pre>
    <strong>&gt; library(cluster) #conduct cluster analysis</strong><br/>    <strong>&gt; library(compareGroups) #build descriptive statistic tables</strong><br/>    <strong>&gt; library(HDclassif) #contains the dataset</strong><br/>    <strong>&gt; library(NbClust) #cluster validity measures</strong><br/>    <strong>&gt; library(sparcl) #colored dendrogram</strong>
</pre>
<p>The dataset is in the <kbd>HDclassif</kbd> package, which we installed. So, we can load the data and examine the structure with the <kbd>str()</kbd> function:</p>
<pre>
    <strong>&gt; data(wine)</strong><br/>    <br/>    <strong>&gt; str(wine)</strong><br/>    <strong>'data.frame':178 obs. of  14 variables:</strong><br/>    <strong> $ class: int  1 1 1 1 1 1 1 1 1 1 ...</strong><br/>    <strong> $ V1   : num  14.2 13.2 13.2 14.4 13.2 ...</strong><br/>    <strong> $ V2   : num  1.71 1.78 2.36 1.95 2.59 1.76 1.87 2.15 1.64 1.35 <br/>       ...</strong><br/>    <strong> $ V3   : num  2.43 2.14 2.67 2.5 2.87 2.45 2.45 2.61 2.17 2.27 ...</strong><br/>    <strong> $ V4   : num  15.6 11.2 18.6 16.8 21 15.2 14.6 17.6 14 16 ...</strong><br/>    <strong> $ V5   : int  127 100 101 113 118 112 96 121 97 98 ...</strong><br/>    <strong> $ V6   : num  2.8 2.65 2.8 3.85 2.8 3.27 2.5 2.6 2.8 2.98 ...</strong><br/>    <strong> $ V7   : num  3.06 2.76 3.24 3.49 2.69 3.39 2.52 2.51 2.98 3.15 <br/>       ...</strong><br/>    <strong> $ V8   : num  0.28 0.26 0.3 0.24 0.39 0.34 0.3 0.31 0.29 0.22 ...</strong><br/>    <strong> $ V9   : num  2.29 1.28 2.81 2.18 1.82 1.97 1.98 1.25 1.98 1.85 <br/>       ...</strong><br/>    <strong> $ V10  : num  5.64 4.38 5.68 7.8 4.32 6.75 5.25 5.05 5.2 7.22 ...</strong><br/>    <strong> $ V11  : num  1.04 1.05 1.03 0.86 1.04 1.05 1.02 1.06 1.08 1.01 <br/>       ...</strong><br/>    <strong> $ V12  : num  3.92 3.4 3.17 3.45 2.93 2.85 3.58 3.58 2.85 3.55 ...</strong><br/>    <strong> $ V13  : int  1065 1050 1185 1480 735 1450 1290 1295 1045 1045 ...</strong>
</pre>
<p>The data consists of <kbd>178</kbd> wines with 13 variables of the chemical composition and one variable <kbd>Class</kbd>, the label, for the cultivar or plant variety. We won't use this in the clustering but as a test of model performance. The variables, <kbd>V1</kbd> through <kbd>V13</kbd>, are the measures of the chemical composition as follows:</p>
<ul>
<li><kbd>V1</kbd>: alcohol</li>
<li><kbd>V2</kbd>: malic acid</li>
<li><kbd>V3</kbd>: ash</li>
<li><kbd>V4</kbd>: alkalinity of ash</li>
<li><kbd>V5</kbd>: magnesium</li>
<li><kbd>V6</kbd>: total phenols</li>
<li><kbd>V7</kbd>: flavonoids</li>
<li><kbd>V8</kbd>: non-flavonoid phenols</li>
<li><kbd>V9</kbd>: proanthocyanins</li>
<li><kbd>V10</kbd>: color intensity</li>
<li><kbd>V11</kbd>: hue</li>
<li><kbd>V12</kbd>: OD280/OD315</li>
<li><kbd>V13</kbd>: proline</li>
</ul>
<p>The variables are all quantitative. We should rename them to something meaningful for our analysis. This is easily done with the <kbd>names()</kbd> function:</p>
<pre>
    <strong>&gt; names(wine) &lt;- c("Class", "Alcohol", "MalicAcid", "Ash", <br/>      "Alk_ash", "magnesium", "T_phenols", "Flavanoids", "Non_flav", <br/></strong><strong>        "Proantho", "C_Intensity", "Hue", "OD280_315", "Proline")</strong><br/>    <br/>    <strong>&gt; names(wine)</strong><br/>    <strong> [1] "Class"       "Alcohol"     "MalicAcid"   "Ash"        </strong><br/>    <strong> [5] "Alk_ash"     "magnesium"   "T_phenols"   "Flavanoids" </strong><br/>    <strong> [9] "Non_flav"    "Proantho"    "C_Intensity" "Hue"        </strong><br/>    <strong>[13] "OD280_315"   "Proline"    </strong>
</pre>
<p>As the variables are not scaled, we will need to do this using the <kbd>scale()</kbd> function. This will first center the data where the column mean is subtracted from each individual in the column. Then the centered values will be divided by the corresponding column's standard deviation. We can also use this transformation to make sure that we only include columns 2 through 14, dropping class and putting it in a data frame. This can all be done with one line of code:</p>
<pre>
    <strong>&gt; df &lt;- as.data.frame(scale(wine[, -1]))</strong>
</pre>
<p>Now, check the structure to make sure that it all worked according to plan:</p>
<pre>
    <strong>&gt; str(df)</strong><br/>    <strong>'data.frame':178 obs. of  13 variables:</strong><br/>    <strong> $ Alcohol    : num  1.514 0.246 0.196 1.687 0.295 ...</strong><br/>    <strong> $ MalicAcid  : num  -0.5607 -0.498 0.0212 -0.3458 0.2271 ...</strong><br/>    <strong> $ Ash        : num  0.231 -0.826 1.106 0.487 1.835 ...</strong><br/>    <strong> $ Alk_ash    : num  -1.166 -2.484 -0.268 -0.807 0.451 ...</strong><br/>    <strong> $ magnesium  : num  1.9085 0.0181 0.0881 0.9283 1.2784 ...</strong><br/>    <strong> $ T_phenols  : num  0.807 0.567 0.807 2.484 0.807 ...</strong><br/>    <strong> $ Flavanoids : num  1.032 0.732 1.212 1.462 0.661 ...</strong><br/>    <strong> $ Non_flav   : num  -0.658 -0.818 -0.497 -0.979 0.226 ...</strong><br/>    <strong> $ Proantho   : num  1.221 -0.543 2.13 1.029 0.4 ...</strong><br/>    <strong> $ C_Intensity: num  0.251 -0.292 0.268 1.183 -0.318 ...</strong><br/>    <strong> $ Hue        : num  0.361 0.405 0.317 -0.426 0.361 ...</strong><br/>    <strong> $ OD280_315  : num  1.843 1.11 0.786 1.181 0.448 ...</strong><br/>    <strong> $ Proline    : num  1.0102 0.9625 1.3912 2.328 -0.0378 ...</strong>
</pre>
<p>Before moving on, let's do a quick table to see the distribution of the cultivars or <kbd>Class</kbd>:</p>
<pre>
    <strong>&gt; table(wine$Class)</strong><br/>    <br/>    <strong> 1  2  3 </strong><br/>    <strong>59 71 48</strong>
</pre>
<p>We can now move on to the modeling step of the process.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>Having created our data frame, <kbd>df</kbd>, we can begin to develop the clustering algorithms. We will start with hierarchical and then try our hand at k-means. After this, we will need to manipulate our data a little bit to demonstrate how to incorporate mixed data with Gower and Random Forest.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Hierarchical clustering</h1>
            </header>

            <article>
                
<p>To build a hierarchical cluster model in R, you can utilize the <kbd>hclust()</kbd> function in the base <kbd>stats</kbd> package. The two primary inputs needed for the function are a distance matrix and the clustering method. The distance matrix is easily done with the <kbd>dist()</kbd> function. For the distance, we will use Euclidean distance. A number of clustering methods are available, and the default for <kbd>hclust()</kbd> is the complete linkage.<br/>
We will try this, but I also recommend Ward's linkage method. Ward's method tends to produce clusters with a similar number of observations.</p>
<p>The complete linkage method results in the distance between any two clusters that is the maximum distance between any one observation in a cluster and any one observation in the other cluster. Ward's linkage method seeks to cluster the observations in order to minimize the within-cluster sum of squares.</p>
<p>It is noteworthy that the R method <kbd>ward.D2</kbd> uses the squared Euclidean distance, which is indeed Ward's linkage method. In R, <kbd>ward.D</kbd> is available but requires your distance matrix to be squared values. As we will be building a distance matrix of non-squared values, we will require <kbd>ward.D2</kbd>.</p>
<p>Now, the big question is how many clusters should we create? As stated in the introduction, the short, and probably not very satisfying answer is that it depends. Even though there are cluster validity measures to help with this dilemma--which we will look at--it really requires an intimate knowledge of the business context, underlying data, and, quite frankly, trial and error. As our sommelier partner is fictional, we will have to rely on the validity measures. However, that is no panacea to selecting the numbers of clusters as there are several dozen validity measures.</p>
<p>As exploring the positives and negatives of the vast array of cluster validity measures is way outside the scope of this chapter, we can turn to a couple of papers and even R itself to simplify this problem for us. A paper by Miligan and Cooper, 1985, explored the performance of 30 different measures/indices on simulated data. The top five performers were CH index, Duda Index, Cindex, Gamma, and Beale Index. Another well-known method to determine the number of clusters is the <strong>gap statistic</strong> (Tibshirani, Walther, and Hastie, 2001). These are two good papers for you to explore if your cluster validity curiosity gets the better of you.</p>
<p>With R, one can use the <kbd>NbClust()</kbd> function in the <kbd>NbClust</kbd> package to pull results on 23 indices, including the top five from Miligan and Cooper and the gap statistic. You can see a list of all the available indices in the help file for the package. There are two ways to approach this process: one is to pick your favorite index or indices and call them with R, the other way is to include all of them in the analysis and go with the majority rules method, which the function summarizes for you nicely. The function will also produce a couple of plots as well.</p>
<p>With the stage set, let's walk through the example of using the complete linkage method. When using the function, you will need to specify the minimum and maximum number of clusters, distance measures, and indices in addition to the linkage. As you can see in the following code, we will create an object called <kbd>numComplete</kbd>. The function specifications are for Euclidean distance, minimum number of clusters two, maximum number of clusters six, complete linkage, and all indices. When you run the command, the function will automatically produce an output similar to what you can see here--a discussion on both the graphical methods and majority rules conclusion:</p>
<pre>
    <strong>&gt; numComplete &lt;- NbClust(df, distance = "euclidean", min.nc = 2, <br/>       max.nc=6, method = "complete", index = "all")</strong><br/>    <strong>*** : The Hubert index is a graphical method of determining the <br/>       number of clusters.</strong><br/>    <strong>In the plot of Hubert index, we seek a significant knee that <br/>       corresponds to a </strong><strong>significant increase of the value of the <br/>         measure that is the significant peak in Hubert index second <br/>           differences plot. </strong><br/>    <br/>    <strong>*** : The D index is a graphical method of determining the number <br/>       of clusters. </strong><br/>    <strong>In the plot of D index, we seek a significant knee (the significant peak in Dindex second differences plot) that corresponds to a significant increase of the value of the measure. </strong><br/>    <br/>    <strong>******************************************************************* </strong><br/>    <strong>* Among all indices:                                                </strong><br/>    <strong>* 1 proposed 2 as the best number of clusters </strong><br/>    <strong>* 11 proposed 3 as the best number of clusters </strong><br/>    <strong>* 6 proposed 5 as the best number of clusters </strong><br/>    <strong>* 5 proposed 6 as the best number of clusters </strong><br/>    <br/>    <strong>                   ***** Conclusion *****                            </strong><br/>    <br/>    <strong>* According to the majority rule, the best number of clusters is 3</strong>
</pre>
<p>Going with the majority rules method, we would select three clusters as the optimal solution, at least for hierarchical clustering. The two plots that are produced contain two graphs each. As the preceding output states, you are looking for a significant knee in the plot (the graph on the left-hand side) and the peak of the graph on the right-hand side. This is the <strong>Hubert Index</strong> plot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="271" width="461" class="image-border" src="assets/image_08_03.png"/></div>
<p>You can see that the bend or knee is at three clusters in the graph on the left-hand side. Additionally, the graph on the right-hand side has its peak at three clusters. The following <strong>Dindex plot</strong> provides the same information:</p>
<div class="CDPAlignCenter CDPAlign"><img height="236" width="402" class="image-border" src="assets/image_08_04.png"/></div>
<p>There are a number of values that you can call with the function and there is one that I would like to show. This output is the best number of clusters for each index and the index value for that corresponding number of clusters. This is done with <kbd>$Best.nc</kbd>. I've abbreviated the output to the first nine indices:</p>
<pre>
    <strong>&gt; numComplete$Best.nc</strong><br/>    <strong>                     KL      CH Hartigan   CCC    Scott</strong><br/>    <strong>Number_clusters  5.0000  3.0000   3.0000 5.000   3.0000</strong><br/>    <strong>Value_Index     14.2227 48.9898  27.8971 1.148 340.9634</strong><br/>    <strong>                     Marriot   TrCovW   TraceW Friedman</strong><br/>    <strong>Number_clusters 3.000000e+00     3.00   3.0000   3.0000</strong><br/>    <strong>Value_Index     6.872632e+25 22389.83 256.4861  10.6941</strong>
</pre>
<p>You can see that the first index, (<kbd>KL</kbd>), has the optimal number of clusters as five and the next index, (<kbd>CH</kbd>), has it as three.</p>
<p>With three clusters as the recommended selection, we will now compute the distance matrix and build our hierarchical cluster object. This code will build the distance matrix:</p>
<pre>
    <strong>&gt; dis &lt;- dist(df, method = "euclidean")</strong>
</pre>
<p>Then, we will use this matrix as the input for the actual clustering with <kbd>hclust()</kbd>:</p>
<pre>
    <strong>&gt; hc &lt;- hclust(dis, method = "complete")</strong>
</pre>
<p>The common way to visualize hierarchical clustering is to plot a <strong>dendrogram</strong>. We will do this with the plot function. Note that <kbd>hang = -1</kbd> puts the observations across the bottom of the diagram:</p>
<pre>
    <strong>&gt; plot(hc, hang = -1, labels = FALSE, main = "Complete-Linkage")</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="224" width="389" class="image-border" src="assets/image_08_05.png"/></div>
<p>The dendrogram is a tree diagram that shows you how the individual observations are clustered together. The arrangement of the connections (branches, if you will) tells us which observations are similar. The height of the branches indicates how much the observations are similar or dissimilar to each other from the distance matrix. Note that I specified <kbd>labels = FALSE</kbd>. This was done to aid in the interpretation because of the number of observations. In a smaller dataset of, say, no more than 40 observations, the row names can be displayed.</p>
<p>To aid in visualizing the clusters, you can produce a colored dendrogram using the <kbd>sparcl</kbd> package. To color the appropriate number of clusters, you need to cut the dendrogram tree to the proper number of clusters using the <kbd>cutree()</kbd> function. This will also create the cluster label for each of the observations:</p>
<pre>
    <strong>&gt; comp3 &lt;- cutree(hc, 3)</strong>
</pre>
<p>Now, the <kbd>comp3</kbd> object is used in the function to build the colored dendrogram:</p>
<pre>
    <strong>&gt; ColorDendrogram(hc, y = comp3, main = "Complete", branchlength = 50)</strong>
</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="265" width="461" class="image-border" src="assets/image_08_06.png"/></div>
<p>Note that I used <kbd>branchlength = 50</kbd>. This value will vary based on your own data. As we have the cluster labels, let's build a table that shows the count per cluster:</p>
<pre>
    <strong>&gt; table(comp3)</strong><br/>    <strong>comp3</strong><br/>    <strong>  1  2  3  </strong><br/>     <strong>69 58 51</strong>
</pre>
<p>Out of curiosity, let's go ahead and compare how this clustering algorithm compares to the <strong>cultivar</strong> labels:</p>
<pre>
    <strong>&gt; table(comp3,wine$Class)</strong><br/>    <br/>    <strong>comp3  1  2  3</strong><br/>    <strong>    1 51 18  0</strong><br/>    <strong>    2  8 50  0</strong><br/>    <strong>    3  0  3 48</strong>
</pre>
<p>In this table, the rows are the clusters and columns are the cultivars. This method matched the cultivar labels at an 84 percent rate. Note that we are not trying to use the clusters to predict a cultivar, and in this example, we have no a priori reason to match clusters to the cultivars.</p>
<p>We will now try Ward's linkage. This is the same code as before; it first starts with trying to identify the number of clusters, which means that we will need to change the method to <kbd>Ward.D2</kbd>:</p>
<pre>
    <strong>&gt; numWard &lt;- NbClust(df, diss = NULL, distance = "euclidean", <br/>      min.nc = 2, max.nc = 6, method = "ward.D2", index = "all")<br/><br/></strong><br/>    <strong>*** : The Hubert index is a graphical method of determining the number of clusters.</strong><br/>    <strong>In the plot of Hubert index, we seek a significant knee that corresponds to a significant increase of the value of the measure i.e the significant peak in Hubert index second differences plot. </strong><br/>    <br/>    <strong>*** : The D index is a graphical method of determining the number of clusters. </strong><br/>    <strong>In the plot of D index, we seek a significant knee (the significant peak in Dindex second differences plot) that corresponds to a significant increase of the value of the measure. </strong><br/>    <br/>    <strong>******************************************************************* </strong><br/>    <strong>* Among all indices:                                                </strong><br/>    <strong>* 2 proposed 2 as the best number of clusters </strong><br/>    <strong>* 18 proposed 3 as the best number of clusters </strong><br/>    <strong>* 2 proposed 6 as the best number of clusters </strong><br/>    <br/>    <strong>                   ***** Conclusion *****                            </strong><br/>    <br/>    <strong>* According to the majority rule, the best number of clusters is 3</strong>
</pre>
<p>This time around also, the majority rules were for a three cluster solution. Looking at the Hubert Index, the best solution is a three cluster as well:</p>
<div class="CDPAlignCenter CDPAlign"><img height="295" width="513" class="image-border" src="assets/image_08_07.png"/></div>
<p>The Dindex adds further support to the three cluster solution:</p>
<div class="CDPAlignCenter"><img height="285" width="496" class="image-border" src="assets/image_08_08.png"/></div>
<p>Let's move on to the actual clustering and production of the dendrogram for Ward's linkage:</p>
<pre>
    <strong>&gt; hcWard &lt;- hclust(dis, method = "ward.D2")</strong><br/>    <br/>    <strong>&gt; plot(hcWard, labels = FALSE, main = "Ward's-Linkage")</strong>
</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="219" width="381" class="image-border" src="assets/image_08_09.png"/></div>
<p>The plot shows three pretty distinct clusters that are roughly equal in size. Let's get a count of the cluster size and show it in relation to the cultivar labels:</p>
<pre>
    <strong>&gt; ward3 &lt;- cutree(hcWard, 3)</strong><br/>    <strong>&gt; table(ward3, wine$Class)</strong><br/>    <br/>    <strong>ward3  1  2  3</strong><br/>    <strong>    1 59  5  0</strong><br/>    <strong>    2  0 58  0</strong><br/>    <strong>    3  0  8 48</strong>
</pre>
<p>So, cluster one has 64 observations, cluster two has <kbd>58</kbd>, and cluster three has 56. This method matches the cultivar categories closer than using complete linkage.</p>
<p>With another table, we can compare how the two methods match observations:</p>
<pre>
    <strong>&gt; table(comp3, ward3)</strong><br/>    <strong>     ward3</strong><br/>    <strong>comp3  1  2  3</strong><br/>    <strong>    1 53 11  5</strong><br/>    <strong>    2 11 47  0</strong><br/>    <strong>    3  0  0 51</strong>
</pre>
<p>While cluster three for each method is pretty close, the other two are not. The question now is how do we identify what the differences are for the interpretation? In many examples, the datasets are very small and you can look at the labels for each cluster. In the real world, this is often impossible. A good way to compare is to use the <kbd>aggregate()</kbd> function, summarizing on a statistic such as the <kbd>mean</kbd> or median. Additionally, instead of doing it on the scaled data, let's try it on the original data. In the function, you will need to specify the dataset, what you are aggregating it by, and the summary statistic:</p>
<pre>
    <strong>&gt; aggregate(wine[, -1], list(comp3), mean)</strong><br/>    <strong>  Group.1  Alcohol MalicAcid      Ash  Alk_ash magnesium T_phenols</strong><br/>    <strong>1       1 13.40609  1.898986 2.305797 16.77246 105.00000  2.643913</strong><br/>    <strong>2       2 12.41517  1.989828 2.381379 21.11724  93.84483  2.424828</strong><br/>    <strong>3       3 13.11784  3.322157 2.431765 21.33333  99.33333  1.675686</strong><br/>    <strong>  Flavanoids  Non_flav Proantho C_Intensity       Hue OD280_315  Proline</strong><br/>    <strong>1  2.6689855 0.2966667 1.832899    4.990725 1.0696522  2.970000 984.6957</strong><br/>    <strong>2  2.3398276 0.3668966 1.678103    3.280345 1.0579310  2.978448 573.3793</strong><br/>    <strong>3  0.8105882 0.4443137 1.164314    7.170980 0.6913725  1.709804 622.4902</strong>
</pre>
<p>This gives us the <kbd>mean</kbd> by the cluster for each of the 13 variables in the data. With complete linkage done, let's give Ward a try:</p>
<pre>
    <strong>&gt; aggregate(wine[, -1], list(ward3), mean)</strong><br/>    <strong>  Group.1  Alcohol MalicAcid      Ash  Alk_ash magnesium T_phenols</strong><br/>    <strong>1       1 13.66922  1.970000 2.463125 17.52812 106.15625  2.850000</strong><br/>    <strong>2       2 12.20397  1.938966 2.215172 20.20862  92.55172  2.262931</strong><br/>    <strong>3       3 13.06161  3.166607 2.412857 21.00357  99.85714  1.694286</strong><br/>    <strong>  Flavanoids  Non_flav Proantho C_Intensity      Hue OD280_315   Proline</strong><br/>    <strong>1  3.0096875 0.2910937 1.908125    5.450000 1.071406  3.158437 1076.0469</strong><br/>    <strong>2  2.0881034 0.3553448 1.686552    2.895345 1.060000  2.862241  501.4310</strong><br/>    <strong>3  0.8478571 0.4494643 1.129286    6.850179 0.721000  1.727321  624.9464</strong>
</pre>
<p>The numbers are very close. The cluster one for Ward's method does have slightly higher values for all the variables. For cluster two of Ward's method, the mean values are smaller except for Hue. This would be something to share with someone who has the domain expertise to assist in the interpretation. We can help this effort by plotting the values for the variables by the cluster for the two methods.</p>
<p>A nice plot to compare distributions is the <strong>boxplot</strong>. The boxplot will show us the minimum, first quartile, median, third quartile, maximum, and potential outliers.</p>
<p>Let's build a comparison plot with two boxplot graphs with the assumption that we are curious about the <kbd>Proline</kbd> values for each clustering method. The first thing to do is to prepare our plot area in order to display the graphs side by side. This is done with the <kbd>par()</kbd> function:</p>
<pre>
    <strong>&gt; par(mfrow =c (1, 2))</strong>
</pre>
<p>Here, we specified that we wanted one row and two columns with <kbd>mfrow = c(1, 2))</kbd>. If you want it as two rows and one column, then it would have been <kbd>mfrow = c(2, 1))</kbd>. In the <kbd>boxplot()</kbd> function, we will need to specify that the <em>y</em> axis values are a function of the <em>x</em> axis values with the tilde <kbd>~</kbd> symbol:</p>
<pre>
    <strong>&gt; boxplot(wine$Proline ~ comp3, data = wine, <br/>              main="Proline by Complete Linkage")</strong><br/>    <br/>    <strong>&gt; boxplot(wine$Proline ~ ward3, data = wine, <br/>              main = "Proline by Ward's Linkage")</strong>
</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="218" width="379" class="image-border" src="assets/image_08_10.png"/></div>
<p>Looking at the boxplot, the thick boxes represent the first quartile, median (the thick horizontal line in the box), and the third quartile, which is the <strong>interquartile range</strong>. The ends of the dotted lines, commonly referred to as <strong>whiskers</strong> represent the minimum and maximum values. You can see that cluster two in complete linkage has five small circles above the maximum. These are known as <strong>suspected outliers</strong> and are calculated as greater than plus or minus 1.5 times the interquartile range.</p>
<p>Any value that is greater than plus or minus three times the interquartile range are deemed outliers and are represented as solid black circles. For what it's worth, clusters one and two of Ward's linkage have tighter interquartile ranges with no suspected outliers.</p>
<div class="packt_tip">Looking at the boxplots for each of the variables could help you, and a domain expert can determine the best hierarchical clustering method to accept. With this in mind, let's move on to k-means clustering.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">K-means clustering</h1>
            </header>

            <article>
                
<p>As we did with hierarchical clustering, we can also use <kbd>NbClust()</kbd> to determine the optimum number of clusters for k-means. All you need to do is specify <kbd>kmeans</kbd> as the method in the function. Let's also loosen up the maximum number of clusters to <kbd>15</kbd>. I've abbreviated the following output to just the majority rules portion:</p>
<pre>
    <strong>&gt; numKMeans &lt;- NbClust(df, min.nc = 2, max.nc = 15, method = <br/>      "kmeans")</strong><br/>    <strong>* Among all indices:                                                </strong><br/>    <strong>* 4 proposed 2 as the best number of clusters </strong><br/>    <strong>* 15 proposed 3 as the best number of clusters </strong><br/>    <strong>* 1 proposed 10 as the best number of clusters </strong><br/>    <strong>* 1 proposed 12 as the best number of clusters </strong><br/>    <strong>* 1 proposed 14 as the best number of clusters </strong><br/>    <strong>* 1 proposed 15 as the best number of clusters </strong><br/>    <br/>    <strong>                   ***** Conclusion *****                            </strong><br/>    <br/>    <strong>* According to the majority rule, the best number of clusters is 3</strong>
</pre>
<p>Once again, three clusters appear to be the optimum solution. Here is the Hubert plot, which confirms this:</p>
<div class="packt_figure CDPAlignCenter"><img height="274" width="477" class="image-border" src="assets/image_08_11.png"/></div>
<p>In R, we can use the <kbd>kmeans()</kbd> function to do this analysis. In addition to the input data, we have to specify the number of clusters we are solving for and a value for random assignments, the <kbd>nstart</kbd> argument. We will also need to specify a random seed:</p>
<pre>
    <strong>&gt; set.seed(1234)</strong><br/>    <br/>    <strong>&gt; km &lt;- kmeans(df, 3, nstart = 25)</strong>
</pre>
<p>Creating a table of the clusters gives us a sense of the distribution of the observations between them:</p>
<pre>
    <strong>&gt; table(km$cluster)</strong><br/>    <br/>    <strong> 1  2  3 </strong><br/>    <strong>62 65 51</strong>
</pre>
<p>The number of observations per cluster is well-balanced. I have seen on a number of occasions with larger datasets and many more variables that no number of k-means yields a promising and compelling result. Another way to analyze the clustering is to look at a matrix of the cluster centers for each variable in each cluster:</p>
<pre>
    <strong>&gt; km$centers</strong><br/>    <strong>     Alcohol  MalicAcid        Ash    Alk_ash   magnesium   T_phenols</strong><br/>    <strong>1  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724</strong><br/>    <strong>2 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891</strong><br/>    <strong>3  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548</strong><br/>    <strong>   Flavanoids    Non_flav    Proantho C_Intensity        Hue  OD280_315</strong><br/>    <strong>1  0.97506900 -0.56050853  0.57865427   0.1705823  0.4726504  0.7770551</strong><br/>    <strong>2  0.02075402 -0.03343924  0.05810161  -0.8993770  0.4605046  0.2700025</strong><br/>    <strong>3 -1.21182921  0.72402116 -0.77751312   0.9388902 -1.1615122 -1.2887761</strong><br/>    <strong>     Proline</strong><br/>    <strong>1  1.1220202</strong><br/>    <strong>2 -0.7517257</strong><br/>    <strong>3 -0.4059428</strong>
</pre>
<p>Note that cluster one has, on average, a higher alcohol content. Let's produce a boxplot to look at the distribution of alcohol content in the same manner as we did before and also compare it to Ward's:</p>
<pre>
    <br/>    <strong>&gt; boxplot(wine$Alcohol ~ km$cluster, data = wine, <br/>              main = "Alcohol Content, K-Means")</strong><br/>    <br/>    <strong>&gt; boxplot(wine$Alcohol ~ ward3, data = wine, <br/>              main = "Alcohol Content, Ward's")</strong>
</pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="287" width="499" class="image-border" src="assets/image_08_12.png"/></div>
<p>The alcohol content for each cluster is almost exactly the same. On the surface, this tells me that three clusters is the proper latent structure for the wines and there is little difference between using k-means or hierarchical clustering. Finally, let's do the comparison of the k-means clusters versus the cultivars:</p>
<pre>
    <strong>&gt; table(km$cluster, wine$Class)</strong><br/>    <br/>    <strong>     1  2  3</strong><br/>    <strong>  1 59  3  0</strong><br/>    <strong>  2  0 65  0</strong><br/>    <strong>  3  0  3 48</strong>
</pre>
<p>This is very similar to the distribution produced by Ward's method, and either one would probably be acceptable to our hypothetical sommelier.</p>
<p>However, to demonstrate how you can cluster on data with both numeric and non-numeric values, let's work through some more examples.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Gower and PAM</h1>
            </header>

            <article>
                
<p>To begin this step, we will need to wrangle our data a little bit. As this method can take variables that are factors, we will convert alcohol to either high or low content. It also takes only one line of code utilizing the <kbd>ifelse()</kbd> function to change the variable to a factor. What this will accomplish is if alcohol is greater than zero, it will be <kbd>High</kbd>, otherwise, it will be <kbd>Low</kbd>:</p>
<pre>
    <strong>&gt; wine$Alcohol &lt;- as.factor(ifelse(df$Alcohol &gt; 0, "High", "Low"))</strong>
</pre>
<p>We are now ready to create the dissimilarity matrix using the <kbd>daisy()</kbd> function from the <kbd>cluster</kbd> package and specifying the method as <kbd>gower</kbd>:</p>
<pre>
    <strong>&gt; disMatrix &lt;- daisy(wine[, -1], metric = "gower")</strong>
</pre>
<p>The creation of the cluster object--let's call it <kbd>pamFit</kbd>--is done with the <kbd>pam()</kbd> function, which is a part of the <kbd>cluster</kbd> package. We will create three clusters in this example and create a table of the cluster size:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <br/>    <strong>&gt; pamFit &lt;- pam(disMatrix, k = 3)</strong><br/>    <br/>    <strong>&gt; table(pamFit$clustering)</strong><br/>    <br/>    <strong> 1  2  3 </strong><br/>    <strong>63 67 48</strong>
</pre>
<p>Now, let's see how it does compared to the cultivar labels:</p>
<pre>
    <strong>&gt; table(pamFit$clustering, wine$Class)</strong><br/>    <br/>    <strong>     1  2  3</strong><br/>    <strong>  1 57  6  0</strong><br/>    <strong>  2  2 64  1</strong><br/>    <strong>  3  0  1 47</strong>
</pre>
<p> Let's take this solution and build a descriptive statistics table using the power of the <kbd>compareGroups</kbd> package. In base R, creating presentation-worthy tables can be quite difficult and this package offers an excellent solution. The first step is to create an object of the descriptive statistics by the cluster with the <kbd>compareGroups()</kbd> function of the package. Then, using <kbd>createTable()</kbd>, we will turn the statistics to an easy-to-export table, which we will do as a .csv. If you want, you can also export the table as a PDF, HTML, or the LaTeX format:</p>
<pre>
    <strong>&gt; wine$cluster &lt;- pamFit$clustering</strong><br/>    <br/>    <strong>&gt; group &lt;- compareGroups(cluster ~ ., data = wine)</strong><br/>    <br/>    <strong>&gt; clustab &lt;- createTable(group)</strong><br/>    <br/>    <strong>&gt; clustab</strong><br/>    <br/>    <strong>--------Summary descriptives table by 'cluster'---------</strong><br/>    <br/><strong>    __________________________________________________________ </strong><br/><strong>                      1           2            3      p.overall </strong><br/><strong>                    N=63         N=67         N=48 </strong><br/><strong>   ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ </strong><br/><strong>   Class         1.10 (0.30)  1.99 (0.21)  2.98 (0.14) &lt;0.001 </strong><br/><strong>   Alcohol:                                            &lt;0.001 </strong><br/><strong>          High     63 (100%)    1 (1.49%)   28 (58.3%) </strong><br/><strong>           Low     0 (0.00%)   66 (98.5%)   20 (41.7%) </strong><br/><strong>   MalicAcid     1.98 (0.83)   1.92 (0.90) 3.39 (1.05) &lt;0.001 </strong><br/><strong>   Ash           2.42 (0.27)   2.27 (0.31) 2.44 (0.18)  0.001 </strong><br/><strong>   Alk_ash       17.2 (2.73)   20.2 (3.28) 21.5 (2.21) &lt;0.001 </strong><br/><strong>   magnesium     105  (11.6)   95.6 (17.2) 98.5 (10.6)  0.001 </strong><br/><strong>   T_phenols     2.82 (0.36)   2.24 (0.55) 1.68 (0.36) &lt;0.001 </strong><br/><strong>   Flavanoids    2.94 (0.47)   2.07 (0.70) 0.79 (0.31) &lt;0.001 </strong><br/><strong>   Non_flav      0.29 (0.08)   0.36 (0.12) 0.46 (0.12) &lt;0.001 </strong><br/><strong>   Proantho      1.86 (0.47)   1.64 (0.59) 1.17 (0.41) &lt;0.001 </strong><br/><strong>   C_Intensity   5.41 (1.31)   3.05 (0.89) 7.41 (2.29) &lt;0.001 </strong><br/><strong>   Hue           1.07 (0.13)   1.05 (0.20) 0.68 (0.12) &lt;0.001 </strong><br/><strong>   OD280_315     3.10 (0.39)   2.80 (0.53) 1.70 (0.27) &lt;0.001 </strong><br/><strong>   Proline       1065 (280)     533 (171)   628 (116)  &lt;0.001 </strong><br/><strong>   comp_cluster  1.16 (0.37)   1.81 (0.50) 3.00 (0.00) &lt;0.001 </strong><br/><strong>¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯</strong>
</pre>
<p>This table shows the proportion of the factor levels by the cluster, and for the numeric variables, the mean and standard deviation are displayed in parentheses. To export the table to a <kbd>.csv</kbd> file, just use the <kbd>export2csv()</kbd> function:</p>
<pre>
    <strong>&gt; export2csv(clustab,file = "wine_clusters.csv")</strong>
</pre>
<p>If you open this file, you will get this table, which is conducive to further analysis and can be easily manipulated for presentation purposes:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/image_08_001.jpg"/></div>
<p>Finally, we'll create a dissimilarity matrix with Random Forest and create three clusters with PAM.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random Forest and PAM</h1>
            </header>

            <article>
                
<p>To perform this method in R, you can use the <kbd>randomForest()</kbd> function.  After seeding the random seed, simply create the model object.  In the following code, I specify the number of trees as <kbd>2000</kbd> and set proximity measure to <kbd>TRUE</kbd>:</p>
<pre>
<strong>    &gt; set.seed(1)<br/></strong><br/><strong>    &gt; rf &lt;- randomForest(x = wine[, -1], ntree = 2000, proximity = T)<br/></strong><br/><strong>    &gt; rf</strong><br/><br/><strong>    Call:</strong><br/><strong>      randomForest(x = wine[, -1], ntree = 2000, proximity = T) </strong><br/><strong>            Type of random forest: unsupervised</strong><br/><strong>            Number of trees: 2000</strong><br/><strong>      No. of variables tried at each split: 3</strong>
</pre>
<p>As you can see, placing a call to <kbd>rf</kbd> did not provide any meaningful output other than the variables sampled at each split (<kbd>mtry</kbd>).  Let's examine the first five rows and first five columns of the <em>N x N</em> matrix:</p>
<pre>
<strong>&gt; dim(rf$proximity)</strong><br/><strong>[1] 178 178</strong><br/><br/><strong>&gt; rf$proximity[1:5, 1:5]</strong><br/><strong>          1         2         3          4          5</strong><br/><strong>1 1.0000000 0.2593985 0.2953586 0.36013986 0.17054264</strong><br/><strong>2 0.2593985 1.0000000 0.1307420 0.16438356 0.11029412</strong><br/><strong>3 0.2953586 0.1307420 1.0000000 0.29692833 0.23735409</strong><br/><strong>4 0.3601399 0.1643836 0.2969283 1.00000000 0.08076923</strong><br/><strong>5 0.1705426 0.1102941 0.2373541 0.08076923 1.00000000</strong>
</pre>
<p>One way to think of the values is that they are the percentage of times those two observations show up in the same terminal nodes! Looking at variable importance we see that the transformed Alcohol input could be dropped. We will keep it for simplicity:</p>
<pre>
<strong>    &gt; importance(rf)</strong><br/><strong>                 MeanDecreaseGini</strong><br/><strong>    Alcohol      0.5614071</strong><br/><strong>    MalicAcid    6.8422540</strong><br/><strong>    Ash          6.4693717</strong><br/><strong>    Alk_ash      5.9103567</strong><br/><strong>    magnesium    5.9426505</strong><br/><strong>    T_phenols    6.2928709</strong><br/><strong>    Flavanoids   6.2902370</strong><br/><strong>    Non_flav     5.7312940</strong><br/><strong>    Proantho     6.2657613</strong><br/><strong>    C_Intensity  6.5375605</strong><br/><strong>    Hue          6.3297808</strong><br/><strong>    OD280_315    6.4894731</strong><br/><strong>    Proline      6.6105274</strong>
</pre>
<p>It is now just a matter of creating the dissimilarity matrix, which transforms the proximity values (<em>square root(1 - proximity)</em>) as follows:</p>
<pre>
<strong>    &gt; dissMat &lt;- sqrt(1 - rf$proximity)</strong><br/><br/><strong>    &gt; dissMat[1:2, 1:2]</strong><br/><strong>              1         2</strong><br/><strong>    1 0.0000000 0.8605821</strong><br/><strong>    2 0.8605821 0.0000000</strong>
</pre>
<p>We now have our input features, so let's run a PAM clustering as we did earlier:</p>
<pre>
<strong>    &gt; set.seed(123)</strong><br/><br/><strong>    &gt; pamRF &lt;- pam(dissMat, k = 3)</strong><br/><br/><strong>    &gt; table(pamRF$clustering)</strong><br/><br/><strong>     1  2  3 </strong><br/><strong>    62 68 48 </strong><br/><br/><strong>    &gt; table(pamRF$clustering, wine$Class)</strong><br/> <br/><strong>      1 2 3</strong><br/><strong>     1 57  5  0</strong><br/><strong>     2  2 64  2</strong><br/><strong>     3  0  2 46</strong>
</pre>
<p>These results are comparable to the other techniques applied. Can you improve the results by tuning the Random Forest?</p>
<p>If you have messy data for a clustering problem, consider using Random Forest.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we started exploring unsupervised learning techniques. We focused on cluster analysis to both provide data reduction and data understanding of the observations.</p>
<p>Four methods were introduced: the traditional hierarchical and k-means clustering algorithms, along with PAM, incorporating two different inputs (Gower and Random Forest). We applied these four methods to find a structure in Italian wines coming from three different cultivars and examined the results.</p>
<p>In the next chapter, we will continue exploring unsupervised learning, but instead of finding structure among the observations, we will focus on finding structure among the variables in order to create new features that can be used in a supervised learning problem.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>