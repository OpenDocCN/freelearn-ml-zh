<html><head></head><body>
		<div id="_idContainer074">
			<h1 id="_idParaDest-78"><em class="italic"><a id="_idTextAnchor077"/>Chapter 5</em>: Building and Training ML Models with SageMaker Studio IDE</h1>
			<p>Building and training a <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model can be easy with SageMaker Studio. It is an <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) designed for ML developers for building and training ML models at scale and efficiently. In order to train an ML model, you may previously have dealt with the cumbersome overhead of managing compute infrastructure for yourself or for your team to train ML models properly. You may also have experienced compute resource constraints, either on desktop machines or with cloud resources, where you are given a fixed-size instance. When you develop in SageMaker Studio, there is no more frustration with provisioning and managing compute infrastructure because you can easily make use of elastic compute in SageMaker Studio and its wide support of sophisticated ML algorithms and frameworks for your ML use case.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Training models with SageMaker's built-in algorithms</li>
				<li>Training with code written in popular frameworks </li>
				<li>Developing and collaborating using SageMaker Notebook</li>
			</ul>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Technical requirements</h1>
			<p>For this chapter, you need to access the code provided at <a href="https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter05">https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter05</a>.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Training models with SageMaker's built-in algorithms</h1>
			<p>When you want to build an ML model from a notebook in SageMaker Studio for your ML use <a id="_idIndexMarker292"/>case and data, one of <a id="_idIndexMarker293"/>the easiest approaches is to use one of SageMaker's built-in algorithms. There are two advantages of using <a id="_idIndexMarker294"/>built-in algorithms:</p>
			<ul>
				<li>The built-in algorithms do not require you to write any sophisticated ML code. You only need to provide your data, make sure the data format matches the algorithms' requirements, and specify the hyperparameters and compute resources.</li>
				<li>The built-in algorithms are optimized for AWS compute infrastructure and are scalable out of the box. It is easy to perform distributed training across multiple compute instances and/or enable GPU support to speed up training time.</li>
			</ul>
			<p>SageMaker's built-in algorithm suite offers algorithms that are suitable for the most common <a id="_idIndexMarker295"/>ML use cases. There <a id="_idIndexMarker296"/>are algorithms for the following <a id="_idIndexMarker297"/>categories: <strong class="bold">supervised learning</strong>, <strong class="bold">unsupervised learning</strong>, <strong class="bold">image analysis</strong>, and <strong class="bold">textual analysis</strong>. Most notably, there <a id="_idIndexMarker298"/>is <strong class="bold">XGBoost</strong> and <strong class="bold">k-means</strong> for tabular data for supervised <a id="_idIndexMarker299"/>learning and <a id="_idIndexMarker300"/>unsupervised learning, respectively, as well as <a id="_idIndexMarker301"/><strong class="bold">image classification</strong>, <strong class="bold">object detection</strong>, and <strong class="bold">semantic segmentation</strong> for <a id="_idIndexMarker302"/>image analysis. For textual <a id="_idIndexMarker303"/>analysis, <a id="_idIndexMarker304"/>we have the <strong class="bold">word2vec</strong>, <strong class="bold">text classification</strong>, and <strong class="bold">sequence-to-sequence</strong> algorithms. These are just <a id="_idIndexMarker305"/>example algorithms for each category we've mentioned. There are <a id="_idIndexMarker306"/>more useful algorithms available but I am not listing them exhaustively. You can visit <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a> to see a full list and further details.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">GPU support and distributed training capability for algorithms vary. Please visit <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/common-info-all-im-models.html">https://docs.aws.amazon.com/sagemaker/latest/dg/common-info-all-im-models.html</a> for GPU and distributed training support for each algorithm. </p>
			<p>Let's take a use case and an algorithm to demonstrate how to use SageMaker's built-in algorithms.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Training an NLP model easily</h2>
			<p>Training an ML <a id="_idIndexMarker307"/>model does not require writing any ML codes with SageMaker's built-in algorithms. We will look at an NLP use <a id="_idIndexMarker308"/>case to classify sentences into categories using the DBpedia Ontology Dataset from <em class="italic">DBpedia</em> (<a href="https://www.dbpedia.org/">https://www.dbpedia.org/</a>), which consists of 560,000 training samples and 70,000 testing samples of the titles and abstracts of Wikipedia articles. Please open the notebook in <strong class="source-inline">chapter05/01-built_in_algorithm_text_classification.ipynb</strong> from the repository <a id="_idIndexMarker309"/>using the <strong class="bold">Python 3 (Data Science)</strong> kernel and the <strong class="bold">ml.t3.medium</strong> instance.</p>
			<p>In the notebook, we first download the dataset and inspect it to understand how we need to process the data, as shown in the following snippet:</p>
			<p class="source-code">!wget -q https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz</p>
			<p class="source-code">!tar -xzf dbpedia_csv.tar.gz</p>
			<p class="source-code">!head dbpedia_csv/train.csv -n 3</p>
			<p class="source-code">!cat dbpedia_csv/classes.txt</p>
			<p>We see that the data, <strong class="source-inline">dbpedia_csv/train.csv</strong>, is formatted as <strong class="source-inline">&lt;class index&gt;,&lt;title&gt;,&lt;abstract&gt;</strong>. There is also a file called <strong class="source-inline">dbpedia_csv/classes.txt</strong> documenting the classes in an order that corresponds to the class index seen in <strong class="source-inline">dbpedia_csv/train.csv</strong>.</p>
			<p>This is a text classification problem: given the abstract of an article, we want to build a model to predict and classify the classes this abstract belong to. This is a common use case when working with a large number of text documents, such as articles on the Wikipedia site from which this dataset is sourced. It is almost impossible to use human review to organize all the documents.</p>
			<p>One of the built-in algorithms that is suitable for this use case is <strong class="bold">BlazingText</strong>. BlazingText has <a id="_idIndexMarker310"/>highly optimized implementations for both Word2vec (unsupervised) and text classification (supervised). The Word2vec algorithm can convert <a id="_idIndexMarker311"/>text into a vector representation, or <strong class="bold">word embedding</strong>, for any downstream NLP usage, such as sentiment analysis or named entity recognition. Text classification can classify documents into categories. This is perfect for our use case and dataset.</p>
			<p>Getting <a id="_idIndexMarker312"/>the data ready for training is key when using SageMaker's built-in algorithm. Using BlazingText for text classification <a id="_idIndexMarker313"/>requires each data point to be formatted as <strong class="source-inline">__label__&lt;class&gt; textâ€¦</strong>. Here's an example:</p>
			<p class="source-code">__label__latin Lorem ipsum dolor sit amet , consectetur adipiscing elit , sed do eiusmod tempor incididunt ut labore et dolore magna aliqua . </p>
			<p>We use a <strong class="source-inline">preprocess</strong> function, which calls the <strong class="source-inline">transform_text</strong> function to tokenize each row of the abstract. We use a sentence tokenizer, <strong class="source-inline">punkt</strong>, from the <strong class="source-inline">nltk</strong> library inside the <strong class="source-inline">transform_text</strong> function. We preprocess both train and test files. To keep the processing time manageable, we use only 20% of the training data, as shown in the following code snippet:</p>
			<p class="source-code">preprocess("dbpedia_csv/train.csv", "dbpedia.train", keep=0.2)</p>
			<p class="source-code">preprocess("dbpedia_csv/test.csv", "dbpedia.validation")</p>
			<p class="source-code">!head -n 1 dbpedia.train </p>
			<p class="source-code"><strong class="bold">__label__Company automatic electric automatic electric company ( ae ) was the largest of the manufacturing units of the automatic electric group . it was a telephone equipment supplier for independent telephone companies in north america and also had a world-wide presence . with its line of automatic telephone exchanges it was also a long-term supplier of switching equipment to the bell system starting in 1919.</strong></p>
			<p>We can see that now we have the data in the expected format. Feel free to expand the training set to a higher percentage using the <strong class="source-inline">keep</strong> argument in <strong class="source-inline">preprocess</strong>. After preprocessing, we are ready to invoke the built-in algorithm.</p>
			<p>SageMaker's built-in algorithms are fully managed containers that can be accessed with a simple SDK call. The following code allows us to use the BlazingText algorithm for text classification:</p>
			<p class="source-code">image=sagemaker.image_uris.retrieve(framework='blazingtext', </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â region=region, </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â version='1')</p>
			<p class="source-code">print(image)</p>
			<p class="source-code"><strong class="bold">433757028032.dkr.ecr.us-west-2.amazonaws.com/blazingtext:1</strong></p>
			<p>After <a id="_idIndexMarker314"/>execution, we get a string in <a id="_idIndexMarker315"/>a variable called <strong class="source-inline">image</strong>. You may be wondering, what is this string that looks like a URL path? How is this an algorithm for model training?</p>
			<p><strong class="bold">Container technology</strong> is the core of SageMaker managed training. Container technology <a id="_idIndexMarker316"/>allows SageMaker the flexibility to work with algorithms from any framework and any runtime requirements. Instead of using the runtime setup in the notebook and using the compute resource behind the notebook for model training, SageMaker takes the data you supply and a container image that has the runtime setup and the code base to a separate SageMaker-managed compute infrastructure to conduct model training.</p>
			<p>The path in <strong class="source-inline">image</strong> points <a id="_idIndexMarker317"/>to a container image stored in <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">ECR</strong>) that has the BlazingText ML algorithm. We can use it to start a model training job with a SageMaker estimator.</p>
			<p>SageMaker estimator is a key construct for the fully managed model training that enables us to command various aspects of a model training job with a simple API. The following snippet is how we set up a training job with SageMaker's BlazingText algorithm:</p>
			<p class="source-code">estimator = sagemaker.estimator.estimator(</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â image,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â role,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â instance_count=1,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â instance_type='ml.c5.2xlarge',</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â volume_size=30,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â max_run=360000,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â input_mode='File',</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â enable_sagemaker_metrics=True,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â output_path=s3_output_location,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â hyperparameters={</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'mode': 'supervised',</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'epochs': 20,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'min_count': 2,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'learning_rate': 0.05,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'vector_dim': 10,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'early_stopping': True,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'patience': 4,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'min_epochs': 5,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'word_ngrams': 2,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â },</p>
			<p class="source-code">)</p>
			<p>Most <a id="_idIndexMarker318"/>notably, the arguments that <a id="_idIndexMarker319"/>go into the estimator are as follows:</p>
			<ul>
				<li>The algorithm as a container, <strong class="source-inline">image</strong></li>
				<li><strong class="source-inline">hyperparameters</strong> for the training job</li>
				<li>The compute resources needed for the job, <strong class="source-inline">instance_type</strong>, <strong class="source-inline">instance_count</strong>, and <strong class="source-inline">volume_size</strong></li>
				<li>The IAM execution role, <strong class="source-inline">role</strong></li>
			</ul>
			<p>As you can see, not only do we specify algorithmic options, but also instruct SageMaker what cloud compute resources we need for this model training run. We request one <strong class="source-inline">ml.c5.2xlarge</strong> instance, a compute-optimized instance that has high-performance processors, with 30 GB storage for this training job. It allows us to use a lightweight, cheap instance type (<strong class="source-inline">ml.t3.medium</strong>) for the notebook environment during prototyping and do full-scale training on a more powerful instance type to get the job done faster.</p>
			<p>We have set up the algorithm and the compute resource; next, we need to associate the estimator <a id="_idIndexMarker320"/>with the training <a id="_idIndexMarker321"/>data. After we have prepared the data, we need to upload the data into an S3 bucket so that the SageMaker training job can access the <strong class="source-inline">ml.c5.4xlarge</strong> instance. We start the training by simply calling <strong class="source-inline">estimator.fit()</strong> with the data:</p>
			<p class="source-code">train_channel = prefix + '/train'</p>
			<p class="source-code">validation_channel = prefix + '/validation'</p>
			<p class="source-code">sess.upload_data(path='dbpedia_csv/dbpedia.train', bucket=bucket, key_prefix=train_channel)</p>
			<p class="source-code">sess.upload_data(path='dbpedia_csv/dbpedia.validation', bucket=bucket, key_prefix=validation_channel)</p>
			<p class="source-code">s3_train_data = f's3://{bucket}/{train_channel}'</p>
			<p class="source-code">s3_validation_data = f's3://{bucket}/{validation_channel}'</p>
			<p class="source-code">print(s3_train_data)</p>
			<p class="source-code">print(s3_validation_data)</p>
			<p class="source-code">data_channels = {'train': s3_train_data, </p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'validation': s3_validation_data}</p>
			<p class="source-code">exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())</p>
			<p class="source-code">jobname = f'dbpedia-blazingtext-{exp_datetime}'</p>
			<p class="source-code">estimator.fit(inputs=data_channels,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â job_name=jobname,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â logs=True)</p>
			<p>You can see the job log in the notebook and observe the following: </p>
			<ul>
				<li>SageMaker spins up one <strong class="source-inline">ml.c5.2xlarge</strong> instance for this training job.</li>
				<li>SageMaker downloads the data from S3 and the BlazingText container image from ECR.</li>
				<li>SageMaker runs the model training and logs the training and validation accuracy in the cell output shown here:<p class="source-code"><strong class="bold">#train_accuracy: 0.9961</strong></p><p class="source-code"><strong class="bold">Number of train examples: 112000</strong></p><p class="source-code"><strong class="bold">#validation_accuracy: 0.9766</strong></p><p class="source-code"><strong class="bold">Number of validation examples: 70000</strong></p></li>
			</ul>
			<p>The cell <a id="_idIndexMarker322"/>output from the training <a id="_idIndexMarker323"/>job is also available <a id="_idIndexMarker324"/>in <strong class="bold">Amazon CloudWatch Logs</strong>. The metrics of the training job, such as CPU utilization and accuracy measures, which we enabled in <strong class="source-inline">estimator(â€¦, enable_sagemaker_metrics=True)</strong>, are sent to <strong class="bold">Amazon CloudWatch Metrics</strong> automatically. This <a id="_idIndexMarker325"/>gives us governance of the training jobs even if the notebooks are accidentally deleted.</p>
			<p>Once the training job finishes, you can access the trained model in <strong class="source-inline">estimator.model_data</strong>, which can later be used for hosting and inferencing either in the cloud, which is a topic we will explore in depth in the next chapter, or on a computer with the <strong class="source-inline">fastText</strong> program. You can access the model with the following code block:</p>
			<p class="source-code">!aws s3 cp {estimator.model_data} ./dbpedia_csv/</p>
			<p class="source-code">%%sh</p>
			<p class="source-code">cd dbpedia_csv/</p>
			<p class="source-code">tar -zxf model.tar.gz</p>
			<p class="source-code"># Use the model archive with fastText</p>
			<p class="source-code"># eg. fasttext predict ./model.bin test.txt</p>
			<p class="callout-heading">Note</p>
			<p class="callout">BlazingText is a GPU-accelerated version of FastText. FastText (<a href="https://fasttext.cc/">https://fasttext.cc/</a>) is an open source library that can perform both word embedding generation (unsupervised) and text classification (supervised). The models created by BlazingText and FastText are compatible with each other.</p>
			<p>We have <a id="_idIndexMarker326"/>just created a sophisticated <a id="_idIndexMarker327"/>text classification model that is capable of classifying the category of documents from DBpedia at an accuracy of 0.9766 on the validation data with minimal ML code.</p>
			<p>Let's also set up an ML experiment management framework, <strong class="bold">SageMaker Experiments</strong>, to keep track of jobs we launch in this chapter.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Managing training jobs with SageMaker Experiments</h2>
			<p>As data scientists, we might have all encountered a tricky situation where the number of model <a id="_idIndexMarker328"/>training runs can grow very <a id="_idIndexMarker329"/>quickly to such a degree that it becomes difficult to track the best model in various experiment settings, such as dataset versions, hyperparameters, and algorithms. In SageMaker Studio, you can easily track the experiments among the training runs with <strong class="bold">SageMaker Experiments</strong> and visualize them in the experiments and trials component UI. SageMaker Experiments is an open source project (<a href="https://github.com/aws/sagemaker-experiments">https://github.com/aws/sagemaker-experiments</a>) and can be accessed programmatically through the Python SDK.</p>
			<p>In SageMaker Experiments, an <strong class="bold">Experiment</strong> is a collection of <strong class="bold">trial</strong> runs that are executions of an ML workflow that can contain <strong class="bold">trial components</strong> such as data processing and model training.</p>
			<p>Let's continue with the <strong class="source-inline">chapter05/01-built_in_algorithm_text_classification.ipynb</strong> notebook and see how we can set up an experiment and trial with SageMaker Experiments to track training jobs with different learning rates in the following snippet so that we can compare the performance from the trials easily in SageMaker Studio:</p>
			<ol>
				<li>First, we install the <strong class="source-inline">sagemaker-experiments</strong> SDK in the notebook kernel:<p class="source-code">!pip install -q sagemaker-experiments</p></li>
				<li>We then create an experiment named <strong class="source-inline">dbpedia-text-classification</strong> that we can use to store all the jobs related to this model training use case using the <strong class="source-inline">smexperiments</strong> library:<p class="source-code">from smexperiments.experiment import Experiment</p><p class="source-code">from smexperiments.trial import Trial</p><p class="source-code">from botocore.exceptions import ClientError</p><p class="source-code">from time import gmtime, strftime</p><p class="source-code">import time</p><p class="source-code">experiment_name = 'dbpedia-text-classification'</p><p class="source-code">try:</p><p class="source-code">Â Â Â Â experiment = Experiment.create(</p><p class="source-code">Â Â Â Â Â Â Â Â experiment_name=experiment_name, </p><p class="source-code">Â Â Â Â Â Â Â Â description='Training a text classification model using dbpedia dataset.')</p><p class="source-code">except ClientError as e:</p><p class="source-code">Â Â Â Â print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')</p></li>
				<li>Then <a id="_idIndexMarker330"/>we create a utility <a id="_idIndexMarker331"/>function, <strong class="source-inline">create_estimator()</strong>, with an input argument, <strong class="source-inline">learning_rate</strong>, for ease of use later when we iterate over various learning rates:<p class="source-code">def create_estimator(learning_rate):</p><p class="source-code">Â Â Â Â hyperparameters={'mode': 'supervised',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'epochs': 40,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'min_count': 2,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'learning_rate': learning_rate,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'vector_dim': 10,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'early_stopping': True,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'patience': 4,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'min_epochs': 5,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 'word_ngrams': 2}</p><p class="source-code">Â Â Â Â estimator = sagemaker.estimator.estimator(</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â image,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â role,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â instance_count=1,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â instance_type='ml.c4.4xlarge',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â volume_size=30,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â max_run=360000,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â input_mode='File',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â enable_sagemaker_metrics=True,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â output_path=s3_output_location,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â hyperparameters=hyperparameters)</p><p class="source-code">Â Â Â Â return estimator</p></li>
				<li>Let's <a id="_idIndexMarker332"/>run three training <a id="_idIndexMarker333"/>jobs in a <strong class="source-inline">for</strong> loop with varying learning rates in order to understand how the accuracy changes:<p class="source-code">for lr in [0.1, 0.01, 0.001]:</p><p class="source-code">Â Â Â Â exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())</p><p class="source-code">Â Â Â Â jobname = f'dbpedia-blazingtext-{exp_datetime}'</p><p class="source-code">Â Â Â Â exp_trial = Trial.create(</p><p class="source-code">Â Â Â Â Â Â Â Â experiment_name=experiment_name, </p><p class="source-code">Â Â Â Â Â Â Â Â trial_name=jobname)</p><p class="source-code">Â Â Â Â experiment_config={</p><p class="source-code">Â Â Â Â Â Â Â Â 'ExperimentName': experiment_name,</p><p class="source-code">Â Â Â Â Â Â Â Â 'TrialName': exp_trial.trial_name,</p><p class="source-code">Â Â Â Â Â Â Â Â 'TrialComponentDisplayName': 'Training'}</p><p class="source-code">Â Â Â Â estimator = create_estimator(learning_rate=lr)Â Â Â Â </p><p class="source-code">Â Â Â Â estimator.fit(inputs=data_channels,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â job_name=jobname,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â experiment_config=experiment_config,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â wait=False)</p></li>
			</ol>
			<p>In the <strong class="source-inline">for</strong> loop, we <a id="_idIndexMarker334"/>create unique training job names, <strong class="source-inline">dbpedia-blazingtext-{exp_datetime}</strong>, to be <a id="_idIndexMarker335"/>associated with a trial, <strong class="source-inline">exp_trial</strong>, and an experiment configuration, <strong class="source-inline">experiment_config</strong>, to store information. Then we pass <strong class="source-inline">experiment_config</strong> into the <strong class="source-inline">estimator.fit()</strong> function and SageMaker will track the experiments for us automatically.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We put <strong class="source-inline">wait=False</strong> in the <strong class="source-inline">estimator.fit()</strong> call. This allows the training job to run asynchronously, meaning that the cell is returned immediately as opposed to being held by the process until the training is completed. In effect, our jobs with different learning rates are run in parallel, each using its own separate SageMaker-managed instances for training.</p>
			<p>In SageMaker Studio, you can easily compare the results of these training jobs with SageMaker Experiments. We can create a chart to compare the accuracies of the three jobs with varying learning rates in the SageMaker Studio UI:</p>
			<ol>
				<li value="1">Click on the <strong class="bold">SageMaker Components and registries</strong> in the left sidebar, as shown in <em class="italic">Figure 5.1</em>:</li>
			</ol>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B17447_06_01.jpg" alt="Figure 5.1 â€“ Viewing experiments and trials from the left sidebar&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 â€“ Viewing experiments and trials from the left sidebar</p>
			<ol>
				<li value="2">Select <strong class="bold">Experiments and trials</strong> in the <a id="_idIndexMarker336"/>drop-down menu, as shown in <em class="italic">Figure 5.1</em>.</li>
				<li>Right-click <a id="_idIndexMarker337"/>on the <strong class="bold">dbpedia-text-classification</strong> experiment entry and choose <strong class="bold">Open in trial component list</strong>.</li>
				<li>A new view in the main working area will pop up. You can configure the columns to show the accuracies and learning rates as shown in <em class="italic">Figure 5.2</em>. We can see <strong class="bold">validation:accuracy</strong>, and <strong class="bold">train:accuracy</strong> with respect to the three <strong class="bold">learning_rate</strong> settings. With <strong class="bold">learning_rate</strong> set to <strong class="bold">0.01</strong>, we have the most balanced training and validation accuracies. A learning rate of 0.1 is overfitted, while a learning rate of 0.001 is underfitted.</li>
			</ol>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B17447_06_02.jpg" alt="Figure 5.2 â€“ Viewing and comparing training jobs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 â€“ Viewing and comparing training jobs</p>
			<ol>
				<li value="5">We can <a id="_idIndexMarker338"/>create a line <a id="_idIndexMarker339"/>chart of <strong class="bold">validation:accuracy</strong> versus <strong class="bold">learning_rate</strong>. Multi-select the three trial components and click <strong class="bold">Add chart</strong> in the top right. A new view will pop up. Configure the chart properties as shown in <em class="italic">Figure 5.3</em>. You will get a chart that shows the relationship between <strong class="bold">validation:accuracy</strong> and <strong class="bold">learning_rate</strong>.</li>
			</ol>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B17447_06_03.jpg" alt="Figure 5.3 â€“ Comparing and charting validation accuracy versus learning rate&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 â€“ Comparing and charting validation accuracy versus learning rate</p>
			<p>SageMaker <a id="_idIndexMarker340"/>Experiments is useful <a id="_idIndexMarker341"/>for managing jobs and resources and comparing performance as you start building an ML project at scale in SageMaker Studio.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Training and processing jobs that do not have <strong class="source-inline">experiment_config</strong> will be placed in <strong class="bold">Unassigned trial components</strong>. </p>
			<p>More often than not, you already have some ML projects that use popular frameworks such as TensorFlow and PyTorch to train models. You can also run them with SageMaker's fully managed training capability.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Training with code written in popular frameworks </h1>
			<p>SageMaker's <a id="_idIndexMarker342"/>fully managed training works <a id="_idIndexMarker343"/>with your favorite ML frameworks too, thanks to the container technology we mentioned previously. You may have been working with <strong class="source-inline">Tensorflow</strong>, <strong class="source-inline">PyTorch</strong>, <strong class="source-inline">Hugging</strong> <strong class="source-inline">Face</strong>, <strong class="source-inline">MXNet</strong>, <strong class="source-inline">scikit-learn</strong>, and many more. You can easily use them with SageMaker so that you can use its fully managed training capabilities and benefit from the ease of provisioning right-sized compute <a id="_idIndexMarker344"/>infrastructure. SageMaker enables <a id="_idIndexMarker345"/>you to use your own training scripts for custom models and run them on prebuilt containers for popular frameworks. This <a id="_idIndexMarker346"/>is known as <strong class="bold">Script Mode</strong>. For frameworks not covered by the prebuilt containers, you also can use your own container for virtually any framework of your choice.</p>
			<p>Let's look at training a sentiment analysis model written in TensorFlow as an example to show you how to use your own script in SageMaker to run with SageMaker's prebuilt TensorFlow container. Then we will describe a similar process for other frameworks.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>TensorFlow</h2>
			<p>TensorFlow is an open source framework for ML, specifically for deep neural networks. You can <a id="_idIndexMarker347"/>run TensorFlow code using SageMaker's <a id="_idIndexMarker348"/>prebuilt TensorFlow training and inference containers, available through the SageMaker SDK's <strong class="source-inline">sagemaker.tensorflow</strong>. Please open the notebook in <strong class="source-inline">chapter05/02-tensorflow_sentiment_analysis.ipynb</strong> from the repository using the <strong class="bold">Python 3 </strong>(<strong class="bold">TensorFlow 2.3 Python 3.7 CPU Optimized</strong>) kernel and the <strong class="source-inline">ml.t3.medium</strong> instance. The objective in this example is to train and predict the sentiment (positive/negative) from movie reviews from the IMDb movie database using a neural network built with TensorFlow layers. You could run the neural network training inside a notebook, but this will require you to have a compute instance that is capable of training a deep neural network with a large amount of data at all times, even when you are just exploring data and writing code. But with SageMaker, you can optimize the compute usage by using a smaller instance for code building and only using a GPU instance for full-scale training.</p>
			<p>In <strong class="source-inline">chapter06/02-tensorflow_sentiment_analysis.ipynb</strong>, we first install the library we need and get the Sagemaker session set up. Then we load the IMDb dataset from <strong class="source-inline">tensorflow.python.keras.datasets</strong>, run minimal data preprocessing, and save the training and test splits to the local filesystem and then to an S3 bucket.</p>
			<p>Assuming we have previously developed a neural network architecture that works on this IMDb dataset, as shown in the following code block, we can easily take it into SageMaker to train.</p>
			<p class="source-code">embedding_layer = tf.keras.layers.Embedding(max_features,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â embedding_dims,</p>
			<p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â input_length=maxlen)</p>
			<p class="source-code">sequence_input = tf.keras.Input(shape=(maxlen,), dtype='int32')</p>
			<p class="source-code">embedded_sequences = embedding_layer(sequence_input)</p>
			<p class="source-code">x = tf.keras.layers.Dropout(args.drop_out_rate)(embedded_sequences)</p>
			<p class="source-code">x = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(x)</p>
			<p class="source-code">x = tf.keras.layers.MaxPooling1D()(x)</p>
			<p class="source-code">x = tf.keras.layers.GlobalMaxPooling1D()(x)</p>
			<p class="source-code">x = tf.keras.layers.Dense(hidden_dims, activation='relu')(x)</p>
			<p class="source-code">x = tf.keras.layers.Dropout(drop_out_rate)(x)</p>
			<p class="source-code">preds = tf.keras.layers.Dense(1, activation='sigmoid')(x)</p>
			<p class="source-code">model = tf.keras.Model(sequence_input, preds)</p>
			<p class="source-code">optimizer = tf.keras.optimizers.Adam(learning_rate)</p>
			<p class="source-code">model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])</p>
			<p>SageMaker can take a TensorFlow <a id="_idIndexMarker349"/>script into a Docker container and train the script with the data. To do so, SageMaker requires the script to be aware of environmental variables set in the container, the compute infrastructure, and, optionally, the script needs to be able to take inputs from the execution, such as hyperparameters. Here are the steps:</p>
			<ol>
				<li value="1">Create a script to put in the model architecture and data loading functions (<strong class="source-inline">get_model</strong>, <strong class="source-inline">get_train_data</strong>, <strong class="source-inline">get_test_data</strong>, and so on).</li>
				<li>Create an argument parser that takes in parameters such as hyperparameters and training data location from script execution. SageMaker is going to run the script as an executable in the container with arguments specified from a SDK call. The training data location is passed into the script with a default from environmental variable SageMaker set up in the container (<strong class="source-inline">SM_CHANNEL_*</strong>). The argument parser is defined in a <strong class="source-inline">parse_arg()</strong> function, shown as follows: <p class="source-code">def parse_args():</p><p class="source-code">Â Â Â Â parser = argparse.ArgumentParser()</p><p class="source-code">Â Â Â Â # hyperparameters sent by the client are passed as command-line arguments to the script</p><p class="source-code">Â Â Â Â parser.add_argument('--epochs', type=int, default=1)</p><p class="source-code">Â Â Â Â parser.add_argument('--batch_size', type=int, default=64)</p><p class="source-code">Â Â Â Â parser.add_argument('--learning_rate', type=float, default=0.01)</p><p class="source-code">Â Â Â Â parser.add_argument('--drop_out_rate', type=float, default=0.2)</p><p class="source-code">Â Â Â Â # data directories</p><p class="source-code">Â Â Â Â parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))</p><p class="source-code">Â Â Â Â parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))</p><p class="source-code">Â Â Â Â # model directory /opt/ml/model default set by SageMaker</p><p class="source-code">Â Â Â Â parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))</p><p class="source-code">Â Â Â Â return parser.parse_known_args()</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">TRAIN</strong> or <strong class="source-inline">TEST</strong> suffix in the <strong class="source-inline">SM_CHANNEL_*</strong> environmental variable has to match <a id="_idIndexMarker350"/>that of the dictionary key provided in the input data channel in <a id="_idIndexMarker351"/>the <strong class="source-inline">estimator.fit()</strong> call. So, later, when we specify the data channel, we need to create a dictionary whose keys are <strong class="source-inline">TRAIN</strong> and <strong class="source-inline">TEST</strong>, case-insensitive.</p></li>
				<li>Put in the training steps as part of <strong class="source-inline">if __name__ == "__main__":</strong>:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">Â Â Â Â args, _ = parse_args()</p><p class="source-code">Â Â Â Â x_train, y_train = get_train_data(args.train)</p><p class="source-code">Â Â Â Â x_test, y_test = get_test_data(args.test)</p><p class="source-code">Â Â Â Â model = get_model(args)</p><p class="source-code">Â Â Â Â history = model.fit(x_train, y_train,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â batch_size=args.batch_size,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â epochs=args.epochs,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â validation_data=(x_test, y_test))</p><p class="source-code">Â Â Â Â save_history(args.model_dir + "/history.p", history)</p><p class="source-code">Â Â Â Â # create a TensorFlow SavedModel for deployment to a SageMaker endpoint with TensorFlow Serving</p><p class="source-code">Â Â Â Â model.save(args.model_dir + '/1')</p></li>
				<li>Make <a id="_idIndexMarker352"/>sure to replace the variables in the network with that from the argument parser. For example, change <strong class="source-inline">tf.keras.optimizers.Adam(learning_rate)</strong> to <strong class="source-inline">tf.keras.optimizers.Adam(</strong>args.<strong class="source-inline">learning_rate)</strong>.</li>
				<li>In our <a id="_idIndexMarker353"/>notebook, we write out the script to <strong class="source-inline">code/tensorflow_sentiment.py</strong>.</li>
				<li>Create a TensorFlow estimator using <strong class="source-inline">sagemaker.tensorflow.TensorFlow</strong>, which is an extension of the <strong class="source-inline">estimator</strong> class we used previously to work exclusively with ML training written in TensorFlow:<p class="source-code">from sagemaker.tensorflow import TensorFlow</p><p class="source-code">exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())</p><p class="source-code">jobname = f'imdb-tf-{exp_datetime}'</p><p class="source-code">model_dir = f's3://{bucket}/{prefix}/{jobname}'</p><p class="source-code">code_dir = f's3://{bucket}/{prefix}/{jobname}'</p><p class="source-code">train_instance_type = 'ml.p3.2xlarge'</p><p class="source-code">hyperparameters = {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01 , 'drop_out_rate': 0.2 }</p><p class="source-code">estimator = TensorFlow(source_dir='code',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â entry_point='tensorflow_sentiment.py',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â model_dir=model_dir,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â code_location=code_dir,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â instance_type=train_instance_type,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â instance_count=1,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â enable_sagemaker_metrics=True,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â hyperparameters=hyperparameters,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â role=role,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â framework_version='2.1',</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â py_version='py3')</p></li>
			</ol>
			<p>Some <a id="_idIndexMarker354"/>of the key arguments here in TensorFlow estimator are <strong class="source-inline">source_dir</strong>, <strong class="source-inline">entry_point</strong>, <strong class="source-inline">code_location</strong>, <strong class="source-inline">framework_version</strong>, and <strong class="source-inline">py_version</strong>. <strong class="source-inline">source_dir</strong>, and <strong class="source-inline">entry_point</strong> is where we specify where the training <a id="_idIndexMarker355"/>script is located on the EFS filesystem (<strong class="source-inline">code/tensorflow_sentiment.py</strong>). If you need to use any additional Python libraries, you can include the libraries in a <strong class="source-inline">requirements.txt</strong> file, and place the text file in a directory specified in <strong class="source-inline">source_dir</strong> argument. SageMaker will first install libraries listed in the <strong class="source-inline">requirements.txt</strong> before executing the training script. <strong class="source-inline">code_location</strong> is where the script will be staged in S3. <strong class="source-inline">framework_version</strong> and <strong class="source-inline">py_version</strong> allow us to specify the TensorFlow version and Python version that the training script is developed in.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find supported versions of TensorFlow at <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>. You <a id="_idIndexMarker356"/>can find the TensorFlow estimator API at <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html">https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html</a>.</p>
			<ol>
				<li value="7">Create <a id="_idIndexMarker357"/>a data channel dictionary:<p class="source-code">data_channels = {'train':train_s3, 'test': test_s3}</p></li>
				<li>Create <a id="_idIndexMarker358"/>a new experiment in SageMaker Experiments:<p class="source-code">experiment_name = 'imdb-sentiment-analysis'</p><p class="source-code">try:</p><p class="source-code">Â Â Â Â experiment = Experiment.create(</p><p class="source-code">Â Â Â Â Â Â Â Â experiment_name=experiment_name, </p><p class="source-code">Â Â Â Â Â Â Â Â description='Training a sentiment classification model using imdb dataset.')</p><p class="source-code">except ClientError as e:</p><p class="source-code">Â Â Â Â print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')</p><p class="source-code"># Creating a new trial for the experiment</p><p class="source-code">exp_trial = Trial.create(</p><p class="source-code">Â Â Â Â experiment_name=experiment_name, </p><p class="source-code">Â Â Â Â trial_name=jobname)</p><p class="source-code">experiment_config={</p><p class="source-code">Â Â Â Â 'ExperimentName': experiment_name,</p><p class="source-code">Â Â Â Â 'TrialName': exp_trial.trial_name,</p><p class="source-code">Â Â Â Â 'TrialComponentDisplayName': 'Training'}</p></li>
				<li>Call the <strong class="source-inline">estimator.fit()</strong> function with data and experiment configurations:<p class="source-code">estimator.fit(inputs=data_channels,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â job_name=jobname,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â experiment_config=experiment_config,</p><p class="source-code">Â Â Â Â Â Â Â Â Â Â Â Â Â Â logs=True)</p></li>
			</ol>
			<p>The training <a id="_idIndexMarker359"/>on one ml.p3.2xlarge instance, which has one high-performance NVIDIAÂ® V100 Tensor Core GPU, takes about 3 minutes. Once the training job finishes, you can access the trained <a id="_idIndexMarker360"/>model from <strong class="source-inline">model_dir</strong> on S3. This model is a Keras model and can be loaded in by Keras' <strong class="source-inline">load_model</strong> API. You can then evaluate the model the same way you would in TensorFlow:</p>
			<p class="source-code">!mkdir ./imdb_data/model -p</p>
			<p class="source-code">!aws s3 cp {estimator.model_data} ./imdb_data/model.tar.gz</p>
			<p class="source-code">!tar -xzf ./imdb_data/model.tar.gz -C ./imdb_data/model/</p>
			<p class="source-code">my_model=tf.keras.models.load_model('./imdb_data/model/1/')</p>
			<p class="source-code">my_model.summary()</p>
			<p class="source-code">loss, acc=my_model.evaluate(x_test, y_test, verbose=2)</p>
			<p class="source-code">print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))</p>
			<p class="source-code"><strong class="bold">782/782 - 55s - loss: 0.7448 - accuracy: 0.8713</strong></p>
			<p class="source-code"><strong class="bold">Restored model, accuracy: 87.13%</strong></p>
			<p>We have successfully trained a custom TensorFlow model to predict IMDb review sentiment using SageMaker's fully managed training infrastructure. For other frameworks, it is rather a similar process to adopt a custom script to SageMaker. We will take a look at the estimator API for PyTorch, Hugging Face, MXNet, and scikit-learn, which share the same base class: <strong class="source-inline">sagemaker.estimator.Framework</strong>.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>PyTorch</h2>
			<p>PyTorch is a <a id="_idIndexMarker361"/>popular open source deep learning <a id="_idIndexMarker362"/>framework that is analogous to TensorFlow. Similar to how SageMaker supports TensorFlow, SageMaker has an estimator dedicated to PyTorch. You can access it with the <strong class="source-inline">sagemaker.pytorch.PyTorch</strong> class. The API's documentation is available at <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html">https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html</a>. Follow <em class="italic">steps</em> <em class="italic">1-9</em> in the <em class="italic">TensorFlow</em> section to use your PyTorch training script, but instead of <strong class="source-inline">framework_version</strong>, you would specify the PyTorch version to access the specific SageMaker-managed PyTorch training container image.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Hugging Face</h2>
			<p>Hugging Face is an ML framework dedicated to natural language processing use cases. It helps you <a id="_idIndexMarker363"/>train complex NLP models easily with pre-built architecture <a id="_idIndexMarker364"/>and pre-trained models. It is compatible with both TensorFlow and PyTorch, so you can train with the framework that you are most familiar with. You can access the estimator with the <strong class="source-inline">sagemaker.huggingface.HuggingFace</strong> class. The API's documentation is available at <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html">https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html</a>. Follow <em class="italic">steps</em> <em class="italic">1-9</em> in the <em class="italic">TensorFlow</em> section to use your scripts. The major difference compared with TensorFlow/PyTorch estimators is that there is an additional argument, <strong class="source-inline">transformers_version</strong>, for the <strong class="bold">transformer</strong> library from Hugging Face. Another difference is that depending on your choice of underlying framework, you need to specify <strong class="source-inline">pytorch_version</strong> or <strong class="source-inline">tensorflow_version</strong> instead of <strong class="source-inline">framework_version</strong>.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>MXNet</h2>
			<p>MXNet is a <a id="_idIndexMarker365"/>popular open source deep learning <a id="_idIndexMarker366"/>framework that is analogous to TensorFlow. You can access the MXNet estimator <a id="_idIndexMarker367"/>with the <strong class="source-inline">sagemaker.mxnet.MXNet</strong> class. The API documentation is available at <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/sagemaker.mxnet.html">https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/sagemaker.mxnet.html</a>. Follow <em class="italic">steps</em> <em class="italic">1-9</em> in the <em class="italic">TensorFlow</em> section to use your MXNet training script, but instead of <strong class="source-inline">framework_version</strong>, you need to specify the MXNet version to access the specific SageMaker-managed container image.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>Scikit-learn</h2>
			<p><strong class="bold">Scikit-learn</strong> (<strong class="bold">sklearn</strong>) is a <a id="_idIndexMarker368"/>popular open source ML framework that is tightly integrated with NumPy, SciPy, and matplotlib. You can access the <a id="_idIndexMarker369"/>sklearn estimator with the <strong class="source-inline">sagemaker.sklearn.SKLearn</strong> class. The API's documentation is available at https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html. Follow <em class="italic">steps</em> <em class="italic">1-9</em> in the <em class="italic">TensorFlow</em> section to use your sklearn training script, but <a id="_idIndexMarker370"/>instead of <strong class="source-inline">framework_version</strong>, you need to specify the sklearn version to access the specific SageMaker-managed container image.</p>
			<p>While developing in SageMaker Studio, it is common that you need to be able to collaborate with your colleague and be able to run ML and data science code with diverse Python libraries. Let's see how we can enrich our model-building experience in SageMaker Studio.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Developing and collaborating using SageMaker Notebook</h1>
			<p>The SageMaker Studio IDE makes collaboration and customization easy. Besides the freedom of <a id="_idIndexMarker371"/>choosing the kernel and instance backing a SageMaker notebook, you could also manage Git repositories, compare <a id="_idIndexMarker372"/>notebooks, and share notebooks.</p>
			<p>Users can interact with a Git repository easily in SageMaker Studio, and you may have already done so to clone the sample repository from GitHub for this book. Not only can you clone a repository from a system terminal, you can also use the Git integration in the left sidebar in the UI to graphically interact with your code base, as shown in <em class="italic">Figure 5.4</em>. You can conduct actions you would normally do in Git with the UI: switching branches, pull, commit, and push.</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B17447_06_04.jpg" alt="Figure 5.4 â€“ Graphical interface of Git integration in the SageMaker Studio IDE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 â€“ Graphical interface of Git integration in the SageMaker Studio IDE</p>
			<p>You can <a id="_idIndexMarker373"/>also perform <em class="italic">notebook diff</em> on a changed <a id="_idIndexMarker374"/>file by right-clicking on the changed file and selecting <strong class="bold">Diff</strong>, as shown in <em class="italic">Figure 5.5</em>. A new view will appear in the main working area to display the changes in the cell in the notebook. This is more powerful than the command-line tool <strong class="source-inline">$ git diff</strong>. For example, in <em class="italic">Figure 5.5</em>, we can see clearly that <strong class="source-inline">instance_type</strong> has been changed since the last commit:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B17447_06_05.jpg" alt="Figure 5.5 â€“ Visualizing changes in a notebook in Git&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 â€“ Visualizing changes in a notebook in Git</p>
			<p>Another <a id="_idIndexMarker375"/>powerful collaboration feature in SageMaker Studio is sharing a notebook with your colleagues so that they can directly work on <a id="_idIndexMarker376"/>the notebook you created. You can share a notebook with output and Git repository information with a click of the <strong class="bold">Share</strong> button in the top right of a notebook, as shown in <em class="italic">Figure 5.6</em>:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17447_06_06.jpg" alt="Figure 5.6 â€“ Sharing a notebook in SageMaker Studio with another user&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 â€“ Sharing a notebook in SageMaker Studio with another user</p>
			<p>You will be prompted to choose the level of information to be included and will be provided with a URL such as https://&lt;sm-domain-id&gt;.studio.&lt;region&gt;.sagemaker.aws/jupyter/default/lab?sagemaker-share-id=xxxxxxxxxxxxxxxxxxx for anyone who has a user profile in the same SageMaker Studio domain. Once your colleague opens the URL, they will see the read-only notebook, snapshot details, and an option to create a copy to be able to edit the notebook, as shown in <em class="italic">Figure 5.7</em>:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B17447_06_07.jpg" alt="Figure 5.7 â€“ Another user's view of the shared notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 â€“ Another user's view of the shared notebook</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <a id="_idIndexMarker377"/>notebook-sharing feature requires configuration <a id="_idIndexMarker378"/>when the domain is created. Notebook sharing is <a id="_idIndexMarker379"/>enabled if you set up the domain using <strong class="bold">Quickstart</strong>, as described in <a href="B17447_02_ePub_RK.xhtml#_idTextAnchor025"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon SageMaker Studio</em>. If you use the Standard setup, you need to explicitly enable notebook sharing.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Summary</h1>
			<p>In this chapter, we explained how you can train a ML model in a notebook in SageMaker Studio. We ran two examples, one using SageMaker's built-in BlazingText algorithm to train a text classification model, and another one using TensorFlow as a deep learning framework to build a network architecture to train a sentiment analysis model to predict the sentiment in movie review data. We learned how SageMaker's fully managed training feature works and how to provision the right amount of compute resources from the SageMaker SDK for your training script.</p>
			<p>We demonstrated SageMaker Experiments' ability to manage and compare ML training runs in SageMaker Studio's UI. Besides training with TensorFlow scripts, we also explained how flexible SageMaker training is when working with various ML frameworks, such as PyTorch, MXNet, Hugging Face, and scikit-learn. Last but not least, we showed you how SageMaker's Git integration and notebook-sharing features can help boost your productivity.</p>
			<p>In the next chapter, we will learn about <strong class="bold">SageMaker Clarify</strong> and how to apply SageMaker Clarify to detect bias in your data and ML models and to explain how models make decisions. Understanding bias and model explainability is essential to creating a fair ML model. We will dive deep into the approaches, metrics SageMaker Clarify uses to measure the bias and how Clarify explains the model. </p>
		</div>
	</body></html>