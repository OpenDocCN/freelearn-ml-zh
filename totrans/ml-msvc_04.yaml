- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Key Machine Learning Algorithms and Concepts
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键机器学习算法和概念
- en: In the previous chapters, we explored the different concepts of MSA and the
    role it plays when creating enterprise systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了MSA的不同概念及其在创建企业系统时所起的作用。
- en: In the coming chapters, we will begin to shift our focus from learning about
    MSA concepts to learning about key machine learning concepts. We will also learn
    about the different libraries and packages being used in machine learning models
    using Python.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将开始将我们的关注点从学习MSA概念转移到学习关键机器学习概念。我们还将学习使用Python在机器学习模型中使用到的不同库和包。
- en: 'We will cover the following areas in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: The differences between **artificial intelligence**, **machine learning**, and
    **deep learning**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能、机器学习和深度学习之间的区别
- en: Common deep learning packages and libraries used in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中常用的常见深度学习包和库
- en: Building **regression** models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建回归**模型**
- en: Building multiclass **classification**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建多类**分类**
- en: Text sentiment analysis and topic modeling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本情感分析和主题建模
- en: Pattern analysis and forecasting using machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习进行模式分析和预测
- en: Building enhanced models using deep learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习构建增强模型
- en: The differences between artificial intelligence, machine learning, and deep
    learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能、机器学习和深度学习之间的区别
- en: Despite the recent rise in popularity of artificial intelligence and machine
    learning, the field of artificial intelligence has been around since the 1960s.
    With different sub-fields emerging, it is important to be able to differentiate
    between them and understand them and what they entail.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工智能和机器学习在最近几年越来越受欢迎，但人工智能领域自20世纪60年代以来就已经存在。随着不同子领域的出现，能够区分它们并理解它们及其所包含的内容变得很重要。
- en: To start, artificial intelligence is the overarching field that encompasses
    all the sub-fields we see today, such as machine learning, deep learning, and
    more. Any system that perceives or receives information from its environment and
    carries out an action to maximize the reward or achieve its goal is considered
    to be an artificially intelligent machine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，人工智能是涵盖我们今天看到的所有子领域的总领域，例如机器学习、深度学习等。任何从其环境中感知或接收信息并执行动作以最大化奖励或实现其目标的系统都被认为是人工智能机器。
- en: This is commonly used today when it comes to robotics. Most of our machines
    are designed so that they can capture data using their sensors, such as cameras,
    sonars, or gyroscopes, and use the data captured to respond to a particular task
    most efficiently. This concept is very similar to how humans function. We use
    our senses to “capture” information from our environment and based on the information
    we receive, we carry out certain actions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这在今天与机器人技术相关时非常常见。我们的大多数机器被设计成能够使用它们的传感器，如摄像头、声纳或陀螺仪来捕获数据，并使用捕获到的数据以最高效的方式响应特定任务。这个概念与人类的功能非常相似。我们使用我们的感官来“捕获”来自环境的信息，并根据我们接收到的信息执行某些动作。
- en: Artificial intelligence is an expansive field, but it can be broken into different
    sub-fields, one we commonly know today as machine learning. What makes machine
    learning unique is that this field works on creating systems or machines that
    can continually learn and improve their model without explicitly being programmed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是一个广泛的领域，但它可以被划分为不同的子领域，我们今天普遍知道的一个子领域是机器学习。使机器学习独特的是，这个领域致力于创建可以持续学习和改进其模型而无需明确编程的系统或机器。
- en: Machine learning does this by collecting data, also known as training data,
    and trying to find patterns in the data to make accurate predictions without being
    programmed to do so. There are many different methods used in machine learning
    to learn the data and the methods are tailored to the different problems we encounter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通过收集数据，也称为训练数据，并试图在数据中找到模式以进行准确预测，而不需要被编程来做这样的事情来实现这一点。机器学习中使用了许多不同的方法来学习数据，这些方法针对我们遇到的不同问题进行了定制。
- en: '![Figure 4.1: Different fields in artificial intelligence](img/B18934_04_1.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1：人工智能的不同领域](img/B18934_04_1.jpg)'
- en: 'Figure 4.1: Different fields in artificial intelligence'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：人工智能的不同领域
- en: 'Machine learning problems can be broken down into three different tasks: **supervised
    learning**, **unsupervised learning**, and **reinforcement learning**. For now,
    we will focus on supervised and unsupervised learning. This distinction is based
    on the training data that we have. Supervised learning is when we have the input
    data and the expected output for the particular set of data, which is also called
    the label. Unsupervised learning, on the other hand, only consists of the input
    without an expected output.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题可以分为三个不同的任务：**监督学习**、**无监督学习**和**强化学习**。现在，我们将专注于监督学习和无监督学习。这种区分基于我们拥有的训练数据。监督学习是我们有特定数据集的输入数据和预期输出时的情况，这也被称为标签。另一方面，无监督学习只包含输入而没有预期输出。
- en: Supervised learning works by understanding the relationship between the input
    and output data. One common example of supervised learning is predicting the price
    of a home in a certain city. We can collect data on existing homes by capturing
    their specifications and their current prices and then learn the pattern between
    the characteristics of these homes and their prices. We can then take a home,
    not in our training set, and test our model by inputting the features of the house
    into our program and have the model predict the price of the home.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通过理解输入和输出数据之间的关系来工作。监督学习的一个常见例子是预测某个城市的房价。我们可以通过捕捉现有房屋的规格和它们当前的价格来收集现有房屋的数据，然后学习这些房屋的特征和它们价格之间的模式。然后我们可以取一个不在我们的训练集中的房屋，通过将房屋的特征输入到我们的程序中来测试我们的模型，并让模型预测该房屋的价格。
- en: Unsupervised learning works by learning about the structure of the data either
    using grouping or clustering methods. This method is commonly used for marketing
    purposes. For example, a store wants to cluster its customers into different groups
    so that it can efficiently tailor its products to different demographics. It can
    capture the purchase history of its customers, use that data to learn about purchasing
    patterns, and suggest certain items or goods that would interest them, thus maximizing
    its revenue.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习通过使用分组或聚类方法来学习数据的结构。这种方法通常用于营销目的。例如，一家商店想要将其客户聚类到不同的群体中，以便它可以有效地针对不同的细分市场调整其产品。它可以捕捉其客户的购买历史，使用这些数据来学习购买模式，并建议某些可能会引起他们兴趣的项目或商品，从而最大化其收入。
- en: Before we can understand deep learning, which is a sub-field of machine learning,
    we must first understand what **Artificial Neural Networks** (**ANNs**) are. Taking
    inspiration from neurons in a brain, ANNs are models that comprise a network of
    fully connected nodes, also known as artificial neurons. They contain a set of
    inputs, hidden layers connecting the neurons, and also an output node. Each neuron
    has an input and output, which can be propagated throughout the network. In order
    to calculate the output of a neuron, we take the weighted sum of all the inputs,
    multiply it by the weight of the neuron, and then usually add a bias term.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够理解深度学习——它是机器学习的一个子领域——之前，我们首先必须了解什么是**人工神经网络**（**ANNs**）。ANNs是从大脑中的神经元得到灵感，由一组完全连接的节点组成的模型，也称为人工神经元。它们包含一组输入、连接神经元的隐藏层，以及一个输出节点。每个神经元都有一个输入和输出，这些可以通过整个网络传播。为了计算神经元的输出，我们取所有输入的加权和，乘以神经元的权重，然后通常加上一个偏差项。
- en: We continue to perform these actions until we reach the last layer, which is
    the output neuron. We perform a nonlinear activation function, such as a sigmoid
    function, to give us the final prediction. We then take the predicted output value
    and input it in a **cost function**. This function tells us how well our network
    is learning. We take this value and backpropagate through our layers back to the
    first layer, adjusting the weights of the neurons depending on how our network
    is performing. With this, we can create strong models that can perform tasks such
    as handwriting recognition, game-playing AI, and much more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续执行这些操作，直到我们达到最后一层，即输出神经元。我们执行一个非线性激活函数，例如Sigmoid函数，以给出最终的预测。然后我们将预测的输出值输入到一个**成本函数**中。这个函数告诉我们我们的网络学习得有多好。我们取这个值，并通过反向传播通过我们的层回到第一层，根据我们的网络表现调整神经元的权重。通过这种方式，我们可以创建强大的模型，可以执行诸如手写识别、游戏AI等任务。
- en: Important Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A program is considered to be a machine learning model if it can take input
    data and learn the patterns to make predictions without being explicitly programmed
    to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个程序能够接受输入数据并学习模式以进行预测，而不需要明确编程，那么它被认为是一个机器学习模型。
- en: '![Figure 4.2: An ANN](img/B18934_04_2.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2：一个ANN](img/B18934_04_2.jpg)'
- en: 'Figure 4.2: An ANN'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：一个ANN
- en: 'While ANNs are capable of performing many tasks, there are significant downsides
    that limit their use in today’s market:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ANNs能够执行许多任务，但它们在当今市场的使用中存在显著的缺点：
- en: It can be difficult to understand how the model performs. As you add more hidden
    layers to the network, it becomes complicated to try and debug the network.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型的性能可能很困难。随着你在网络中添加更多的隐藏层，尝试调试网络变得复杂。
- en: Training the model takes a long time, especially with copious amounts of training
    data, and can drain hardware resources, as it is difficult to perform all these
    mathematical operations on a CPU.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型需要很长时间，特别是当有大量的训练数据时，并且可能会耗尽硬件资源，因为在CPU上执行所有这些数学运算很困难。
- en: The biggest issue with ANNs is overfitting. As we add more hidden layers, there
    is a point at which the weights assigned to the neurons will be heavily tailored
    to our training data. This makes our network perform very poorly when we try to
    test it with data it has not seen before.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANNs最大的问题是过拟合。随着我们添加更多的隐藏层，存在一个点，此时分配给神经元的权重将高度定制于我们的训练数据。这使得当尝试用之前未见过的数据测试网络时，我们的网络表现非常糟糕。
- en: 'This is where deep learning comes into play. Deep learning can be categorized
    by these key features:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习发挥作用的地方。深度学习可以根据以下关键特征进行分类：
- en: '**The hierarchical composition of layers**: Rather than having only fully connected
    layers in a network, we can create and combine multiple different layers, consisting
    of non-linear and linear transformations. These different layers play a role in
    extracting key features in the data that would be otherwise difficult to find
    in an ANN.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层的层次结构组成**：而不是在网络中只有全连接层，我们可以创建和组合多个不同的层，包括非线性变换和线性变换。这些不同的层在数据中提取关键特征中发挥作用，这些特征在ANN中可能很难找到。'
- en: '**End-to-end learning**: The network starts with a method called feature extraction.
    It looks at the data and finds a way to group redundant information and identify
    the important features of the data. The network then uses these features to train
    and predict or classify using fully connected layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端学习**：网络从一种称为特征提取的方法开始。它查看数据，并找到一种方法来分组冗余信息并识别数据的重要特征。然后，网络使用这些特征通过全连接层进行训练和预测或分类。'
- en: '**A distributed representation of neurons**: With feature extraction, the network
    can group neurons to encode a bigger feature of the data. Unlike in an ANN, no
    single neuron encodes everything. This allows the model to reduce the number of
    parameters it has to learn while still retaining the key elements in the data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元的分布式表示**：通过特征提取，网络可以将神经元分组以编码数据的一个更大特征。与ANN不同，没有单个神经元编码所有内容。这允许模型在保留数据中的关键元素的同时，减少它必须学习的参数数量。'
- en: Deep learning is prevalent in computer vision. Due to the advances in the technology
    of capturing photos and videos, it has become very difficult for ANNs to learn
    and perform well when it comes to image detection. For starters, when we use an
    image to train our model, we have to look at every pixel in an image as an input
    to the model. So, for an image of resolution 256x256, we would be looking at over
    65,000 input parameters. Depending on the number of neurons in your fully connected
    layer, you could be looking at millions of parameters. With the sheer number of
    parameters, this will be bound to cause overfitting and could take days of training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉中非常普遍。由于捕捉照片和视频技术的进步，当涉及到图像检测时，人工神经网络（ANNs）学习和表现良好变得非常困难。首先，当我们使用图像来训练我们的模型时，我们必须将图像中的每一个像素都视为模型的一个输入。因此，对于一个256x256分辨率的图像，我们将会查看超过65,000个输入参数。根据你全连接层中神经元的数量，你可能需要查看数百万个参数。由于参数数量庞大，这必然会导致过拟合，并且可能需要数天的训练时间。
- en: With deep learning, we can create a group of layers called **Convolutional Neural
    Networks** (**CNNs**). These layers are responsible for reducing the number of
    parameters that we have to learn in our model while still retaining the key features
    in our data. With these additions, we can learn how to extract certain features
    and use those to train our model to predict with efficiency and accuracy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深度学习，我们可以创建一组称为**卷积神经网络**（**CNNs**）的层。这些层负责减少我们在模型中需要学习的参数数量，同时仍然保留数据中的关键特征。有了这些添加，我们可以学习如何提取某些特征，并使用这些特征来训练我们的模型，以高效和准确地进行预测。
- en: '![Figure 4.3: A CNN](img/B18934_04_3.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：一个 CNN](img/B18934_04_3.jpg)'
- en: 'Figure 4.3: A CNN'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：一个 CNN
- en: In the next section, we will be looking at the different Python libraries used
    for machine learning and deep learning and their different use cases.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨用于机器学习和深度学习的不同 Python 库及其不同的用例。
- en: Common deep learning and machine learning libraries used in Python
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 中常用的深度学习和机器学习库
- en: Now that we have gone over the concepts of artificial intelligence and machine
    learning, we can start looking at the programming aspect of implementing these
    concepts. Many programming languages are used today when it comes to creating
    machine learning models. Commonly used are MATLAB, R, and Python. Among them,
    Python has grown to be the most popular programming language in machine learning
    due to its versatility as a programming language and the extensive number of libraries,
    which makes creating machine learning models easier. In this section, we will
    be going over the most commonly used libraries today.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了人工智能和机器学习的基本概念，我们可以开始探讨实现这些概念编程方面的内容。在创建机器学习模型时，今天使用的编程语言有很多。常用的有 MATLAB、R
    和 Python。其中，Python 由于其作为编程语言的灵活性和丰富的库，已经成为机器学习中最受欢迎的编程语言。在本节中，我们将介绍今天最常用的库。
- en: NumPy
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NumPy
- en: NumPy is an essential package when it comes to building machine learning models
    in Python. You will be mostly working with large, multi-dimensional matrices when
    building your models. Most of the effort is spent on transforming, splicing, and
    performing advanced mathematical operations on matrices, and NumPy provides the
    tools need to perform these actions while retaining speed and efficiency.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 是在 Python 中构建机器学习模型时必不可少的包。在构建模型时，你将主要与大型、多维矩阵打交道。大部分的工作都花在了对矩阵进行转换、拼接以及执行高级数学运算上，而
    NumPy 提供了执行这些操作所需的同时保持速度和效率的工具。
- en: 'For more information on the different APIs that NumPy offers, you can visit
    the documentation on its website: https://numpy.org/doc/stable/reference/index.html.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多关于 NumPy 提供的不同 API 的信息，您可以访问其网站上的文档：https://numpy.org/doc/stable/reference/index.html。
- en: 'Here, we will look at the example code. This section shows us how we can initialize
    a NumPy array. In this example, we will create a 3x3 matrix with initialized values
    of 1 through 9:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将查看示例代码。本节展示了我们如何初始化一个 NumPy 数组。在这个例子中，我们将创建一个 3x3 矩阵，其初始化值为 1 到 9：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we will print out the results:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将打印出结果：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, we can show how we can splice and extract certain elements from our array.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以展示如何从我们的数组中拼接和提取某些元素。
- en: 'This line of code allows us to pull all the values that are in the second column
    of our array. Keep in mind that in NumPy our arrays and lists are zero-indexed,
    meaning that the zero index refers to the first element in the array or list:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码允许我们从数组中提取第二列的所有值。请注意，在 NumPy 中，我们的数组和列表是零索引的，这意味着零索引指的是数组或列表中的第一个元素：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, we extract all of the values in the row of `2` in our array:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们提取了我们数组中第`2`行的所有值：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Another useful aspect of NumPy arrays is that we can apply mathematical functions
    to our matrices without having to implement code to perform basic functions. Not
    only is this much easier but it also is much faster and more efficient.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组的另一个有用之处在于，我们可以对矩阵应用数学函数，而无需编写执行基本函数的代码。这不仅更容易，而且更快、更高效。
- en: 'In this example, we simply perform a multiplication between our matrix and
    a scalar value of `-1`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们只是将我们的矩阵与标量值`-1`进行乘法运算：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Matplotlib
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Matplotlib
- en: In order to see how your model is learning and performing, it is important to
    be able to visualize your results and your data. Matplotlib offers a simple way
    to graph your data, from something as simple as a line plot to more advanced plots,
    such as contour plots and 3D plots. What makes this library so popular is its
    seamlessness when working with NumPy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到您的模型是如何学习和表现的，能够可视化您的结果和数据非常重要。Matplotlib提供了一个简单的方法来绘制您的数据，从简单的线图到更高级的图表，如等高线图和3D图。使这个库如此受欢迎的是它与NumPy工作的无缝性。
- en: 'For more information on their different functions, you can visit their website:
    [https://matplotlib.org/stable/index.html](https://matplotlib.org/stable/index.html).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于它们不同功能的更多信息，您可以访问它们的网站：[https://matplotlib.org/stable/index.html](https://matplotlib.org/stable/index.html)。
- en: 'In this example, we will create a simple line graph. We first initialize two
    arrays, `x` and `y`, and both arrays will contain values from 0 to 9\. Then, using
    Matplotlib’s APIs, we can plot and show our simple graph:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个简单的线图。我们首先初始化两个数组，`x`和`y`，这两个数组都将包含从0到9的值。然后，使用Matplotlib的API，我们可以绘制并显示我们的简单图表：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 4.4: A simple line graph using Matplotlib](img/B18934_04_4.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4：使用Matplotlib的简单线图](img/B18934_04_4.jpg)'
- en: 'Figure 4.4: A simple line graph using Matplotlib'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：使用Matplotlib的简单线图
- en: Pandas
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas
- en: With the recent trend of storing data in CSV files, Pandas has become a staple
    in the Python community due to its ease and versatility. Pandas is commonly used
    for data analysis. It stores the data in a tabular format, and it provides users
    with simple functions to pre-process and manipulate the data to fit their needs.
    It has also become useful when dealing with time-series data, which is helpful
    when building forecasting models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最近将数据存储在CSV文件中的趋势，Pandas因其易用性和多功能性而成为Python社区中的必备工具。Pandas通常用于数据分析。它以表格格式存储数据，并为用户提供简单的函数来预处理和操作数据以满足他们的需求。在处理时间序列数据时，它也变得非常有用，这对于构建预测模型很有帮助。
- en: 'For more information on the different functions, you can view the documentation
    on its website: [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同函数的更多信息，您可以在其网站上查看文档：[https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)。
- en: 'In this example, first, we will simply initialize a DataFrame. This is the
    data structure used to store our data in a two-dimensional tabular format in Pandas.
    Usually, we store the data from the files we read from, but it is also possible
    to create a DataFrame with your own data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，首先，我们将简单地初始化一个DataFrame。这是在Pandas中以二维表格格式存储我们数据的数据结构。通常，我们存储从读取的文件中读取的数据，但也可以使用自己的数据创建DataFrame：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 4.5: Output of our DataFrame](img/B18934_04_5.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5：我们的DataFrame的输出](img/B18934_04_5.jpg)'
- en: 'Figure 4.5: Output of our DataFrame'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：我们的DataFrame的输出
- en: 'As with NumPy, we can extract certain columns and rows of our DataFrame. In
    this code, we can view the first row of our DataFrame:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与NumPy一样，我们可以从我们的DataFrame中提取某些列和行。在这段代码中，我们可以查看我们的DataFrame的第一行：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 4.6: Output of the first row of our DataFrame](img/B18934_04_6.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6：我们的DataFrame的第一行输出](img/B18934_04_6.jpg)'
- en: 'Figure 4.6: Output of the first row of our DataFrame'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：我们的DataFrame的第一行输出
- en: 'With Pandas, we can also extract certain columns from our DataFrame by using
    the name of the column rather than the index:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas，我们还可以通过使用列名而不是索引来从我们的DataFrame中提取某些列：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 4.7: Output of all the values in the Price column](img/B18934_04_7.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7：价格列中所有值的输出](img/B18934_04_7.jpg)'
- en: 'Figure 4.7: Output of all the values in the Price column'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：价格列中所有值的输出
- en: TensorFlow and Keras
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow和Keras
- en: TensorFlow and Keras are the foundation when it comes to building deep learning
    models. While both can be used individually, Keras is used as an interface for
    the TensorFlow framework, allowing users to easily create powerful deep learning
    models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和Keras是构建深度学习模型的基础。虽然两者都可以单独使用，但Keras被用作TensorFlow框架的接口，使用户能够轻松创建强大的深度学习模型。
- en: TensorFlow, created by Google, functions as the backend when creating machine
    learning models. It works by creating static data flow graphs that specify how
    the data moves through the deep learning pipeline. The graph contains nodes and
    edges, where the nodes represent mathematical operations. It passes this data
    using multidimensional arrays known as Tensors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google创建的TensorFlow在创建机器学习模型时充当后端。它通过创建静态数据流图来工作，这些图指定了数据在深度学习管道中的流动方式。图中包含节点和边，其中节点代表数学运算。它使用称为张量的多维数组传递这些数据。
- en: Keras, later to be integrated with TensorFlow, can be viewed as the frontend
    for designing deep learning models. It was implemented to be user-friendly by
    allowing users to focus on designing their neural network models without having
    to deal with a complicated backend. It is similar to object-oriented programming,
    as it replicates the style of creating objects. Users can freely add different
    types of layers, activation functions, and more. They can even use prebuilt neural
    networks for easy training and testing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Keras，后来与TensorFlow集成，可以看作是设计深度学习模型的界面。它被设计成用户友好，允许用户专注于设计他们的神经网络模型，无需处理复杂的后端。它类似于面向对象编程，因为它复制了创建对象的方式。用户可以自由添加不同类型的层、激活函数等。他们甚至可以使用预构建的神经网络进行简单的训练和测试。
- en: 'In the following example code, we can see how we can create a simple, hidden
    two-layer neural network. This block of code allows us to initialize a `Sequential`
    model, which consists of a simple stack of layers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例代码中，我们可以看到如何创建一个简单的、隐藏的两层神经网络。这段代码允许我们初始化一个`Sequential`模型，它由简单的层堆叠组成：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Depending on our application, we can add multiple layers with different configurations,
    such as the number of nodes, the activation functions, and the kernel regularizer:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的应用，我们可以添加具有不同配置的多个层，例如节点数、激活函数和核正则化器：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we can compile our model, which essentially gathers all the different
    layers and combines them into one simple neural network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以编译我们的模型，这实际上是将所有不同的层聚集在一起，并将它们组合成一个简单的神经网络：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: PyTorch
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: 'PyTorch is another machine learning framework created by Meta, formally known
    as Facebook. Much like Keras/TensorFlow, it allows the users to create machine
    learning models. The framework is well suited to **Natural Language Processing**
    (**NLP**) and computer vision problems but can be tailored to most applications.
    What makes PyTorch unique is its dynamic computational graph. It has a module
    called Autograd, which allows you to perform automatic differentiation dynamically,
    compared to TensorFlow, in which it is static. Also, PyTorch is more in line with
    the Python language, which makes it easier to understand and takes advantage of
    useful features of Python such as parallel programming. For more information,
    visit the documentation on their website: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是由Meta（前Facebook）创建的另一个机器学习框架。与Keras/TensorFlow类似，它允许用户创建机器学习模型。该框架非常适合自然语言处理（NLP）和计算机视觉问题，但可以定制以适应大多数应用。PyTorch的独特之处在于其动态计算图。它有一个名为Autograd的模块，允许你动态地进行自动微分，与TensorFlow中的静态微分相比。此外，PyTorch更符合Python语言，这使得它更容易理解，并利用Python的有用特性，如并行编程。有关更多信息，请访问他们网站上的文档：[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)。
- en: 'In this section of code, we can create a simple single-layer neural network.
    Similar to Keras, we can initialize a `Sequential` model and add layers depending
    on our needs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们可以创建一个简单的单层神经网络。类似于Keras，我们可以初始化一个`Sequential`模型，并根据需要添加层：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: SciPy
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SciPy
- en: 'This library is designed for scientific computing. There are many built-in
    functions and methods used for linear algebra, optimization, and integration,
    which are commonly used in machine learning. This library is useful when trying
    to compute certain statistics and transformations as you build your machine learning
    model. For more information on the different functions it provides, view the documentation
    on its website: [https://docs.scipy.org/doc/scipy/](https://docs.scipy.org/doc/scipy/).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库是为科学计算设计的。它包含许多用于线性代数、优化和积分的内置函数和方法，这些在机器学习中常用。当你构建机器学习模型时，尝试计算某些统计和转换时，这个库很有用。有关它提供的不同函数的更多信息，请查看其网站上的文档：[https://docs.scipy.org/doc/scipy/](https://docs.scipy.org/doc/scipy/)。
- en: 'In this example code, we can create a 3x3 array using NumPy and then we can
    use SciPy to calculate the determinate:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例代码中，我们可以使用NumPy创建一个3x3的数组，然后我们可以使用SciPy来计算行列式：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: scikit-learn
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn
- en: 'scikit-learn is a machine learning library that is an extension of SciPy and
    is built using NumPy and Matplotlib. It contains many prebuilt machine learning
    models, such as random forests, K-means, and support vector machines. For more
    information on the different APIs it provides, view the documentation by visiting
    its website: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn是一个机器学习库，它是SciPy的扩展，使用NumPy和Matplotlib构建。它包含许多预构建的机器学习模型，如随机森林、K-means和支持向量机。有关它提供的不同API的更多信息，请访问其网站：[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)。
- en: 'In the following example, we will use an example dataset provided by scikit-learn
    and build a simple logistic regression model. First, we import all the required
    libraries and then load the Iris dataset provided by scikit-learn. We can use
    a handy API from scikit-learn to split our data into training and test datasets:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们将使用scikit-learn提供的示例数据集来构建一个简单的逻辑回归模型。首先，我们导入所有必需的库，然后加载scikit-learn提供的Iris数据集。我们可以使用scikit-learn的一个便捷API将我们的数据分成训练集和测试集：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then initialize our logistic regression model and simply run the `fit`
    function with our training data to train the model. Once we train our model, we
    can use it to make predictions and then measure its accuracy:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以初始化我们的逻辑回归模型，并简单地使用我们的训练数据运行`fit`函数来训练模型。一旦我们训练了我们的模型，我们就可以用它来进行预测，然后测量其准确度：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next few sections, we will start looking at the different models we can
    build using these libraries. We will understand what makes these models unique,
    how they are structured, and for what purposes and applications they can best
    serve our needs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将开始探讨我们可以使用这些库构建的不同模型。我们将了解是什么使这些模型独特，它们的结构如何，以及它们可以最好地服务于哪些目的和应用程序来满足我们的需求。
- en: Building regression models
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建回归模型
- en: First, we will look at regression models. Regression models or regression analysis
    are modeling techniques used to find the relationship between independent and
    dependent variables. The output of a regression model is typically a continuous
    value, also known as a quantitative variable. Some common examples are predicting
    the price of a home based on its features or predicting the sales of a certain
    product in a new store based on previous sales information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨回归模型。回归模型或回归分析是用于寻找独立变量和依赖变量之间关系的建模技术。回归模型的输出通常是连续值，也称为定量变量。一些常见的例子是根据房屋的特征预测房价，或者根据之前的销售信息预测新商店中某种产品的销售情况。
- en: 'Before building a regression model, we must first understand the data and how
    it is structured. The majority of regression models involve supervised learning.
    This consists of features and an output variable, known as a label. This will
    help the model by adjusting the weights to better fit the data we have observed
    so far. We usually denote our features as X and our labels as Y to help us understand
    the mathematical models used to solve regression models:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建回归模型之前，我们首先必须理解数据和它的结构。大多数回归模型涉及监督学习。这包括特征和一个输出变量，称为标签。这将通过调整权重来帮助模型更好地拟合我们迄今为止观察到的数据。我们通常将我们的特征表示为X，将我们的标签表示为Y，以帮助我们理解用于解决回归模型的数学模型：
- en: '![Figure 4.8: Example of a supervised learning data structure](img/B18934_04_8.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8：监督学习数据结构的示例](img/B18934_04_8.jpg)'
- en: 'Figure 4.8: Example of a supervised learning data structure'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：监督学习数据结构的示例
- en: Typically, our data is split into two subsets, **training** and **testing**
    sets. The training dataset usually consists of between 70-80% of the original
    data and the testing dataset contains the rest. This is to allow the model to
    learn on the training dataset and validate its result on the testing dataset to
    show its performance. From the results, we can infer how our model is performing
    on the dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们的数据被分成两个子集，**训练集**和**测试集**。训练集通常包含原始数据的70-80%，而测试集包含剩余的部分。这是为了让模型在训练集上学习，并在测试集上验证其结果以展示其性能。从结果中，我们可以推断出我们的模型在数据集上的表现。
- en: 'For a linear regression model to perform effectively, our data must be structured
    linearly. The model uses this formula to train on and learn about the data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了线性回归模型能够有效运行，我们的数据必须以线性方式组织。模型使用以下公式在数据上进行训练和学习：
- en: '![](img/Formula_04_001.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![公式4.1](img/Formula_04_001.jpg)'
- en: 'In this equation, ![](img/Formula_04_002.png) represents the output of the
    model, or what we usually call the prediction. The prediction is calculated by
    taking the intercept ![](img/Formula_04_003.png) and the slope ![](img/Formula_04_004.png).
    The slope, which is also referred to as the weight, is applied to all the features
    in the data, which represents ![](img/Formula_04_005.png). When working with the
    data, we usually represent it as a matrix, which makes it easy to understand and
    easy to work with when using Python:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![公式 4.002](img/Formula_04_002.png)代表模型的输出，或者我们通常所说的预测。预测是通过取截距![公式 4.003](img/Formula_04_003.png)和斜率![公式
    4.004](img/Formula_04_004.png)来计算的。斜率，也称为权重，应用于数据中的所有特征，代表![公式 4.005](img/Formula_04_005.png)。在处理数据时，我们通常将其表示为矩阵，这使得它易于理解，并且在使用Python时易于操作：
- en: '![](img/Formula_04_006.jpg)![](img/Formula_04_007.jpg)![](img/Formula_04_008.jpg)![](img/Formula_04_009.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![公式 4.06](img/Formula_04_006.jpg)![公式 4.07](img/Formula_04_007.jpg)![公式 4.08](img/Formula_04_008.jpg)![公式
    4.09](img/Formula_04_009.jpg)'
- en: The number of features describes what type of problem you are solving. If your
    data only has one feature, it is considered a simple linear regression model.
    While it can solve straightforward problems, for more advanced data and problems,
    it can be difficult to map relationships. Therefore, you can create a multiple
    linear regression model by adding more features (![](img/Formula_04_010.png)).
    This allows the model to be more robust and find deeper relationships.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特征数量描述了你正在解决什么类型的问题。如果你的数据只有一个特征，它被认为是一个简单的线性回归模型。虽然它可以解决直接的问题，但对于更复杂的数据和问题，映射关系可能会很困难。因此，你可以通过添加更多特征来创建一个多元线性回归模型（![公式
    4.10](img/Formula_04_010.png)）。这使得模型更加稳健，并能找到更深层次的关系。
- en: '![Figure 4.9: A simple linear regression model](img/B18934_04_9.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9：一个简单的线性回归模型](img/B18934_04_9.jpg)'
- en: 'Figure 4.9: A simple linear regression model'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：一个简单的线性回归模型
- en: Once we train our model, we need to learn how to evaluate our model and understand
    how it performs against the test data. When it comes to linear regression, the
    two common metrics we use to assess our model are the **Root Mean Square Error**
    (**RMSE**) and the **R**2 metrics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了我们的模型，我们需要学习如何评估我们的模型并理解它在测试数据上的表现。当涉及到线性回归时，我们用来评估模型的两个常用指标是**均方根误差**（**RMSE**）和**R**2指标。
- en: 'The RMSE is the standard deviation of the **residual** errors across the predictions.
    The residual is the measure of the distance from the actual data points to the
    regression line. The further the average distance of all the points is from the
    line, the higher the error is. This indicates a weak model, as it’s unable to
    find the correlation between the data points. This metric can be calculated by
    using this formula where ![](img/Formula_04_011.png) is the actual value, ![](img/Formula_04_012.png)
    is the predicted value, and ![](img/Formula_04_013.png) is the number of data
    points:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE是预测中残差误差的标准差。残差是实际数据点到回归线的距离的度量。所有点的平均距离越远，误差就越高。这表明模型较弱，因为它无法找到数据点之间的相关性。这个指标可以通过以下公式计算，其中![公式
    4.11](img/Formula_04_011.png)是实际值，![公式 4.12](img/Formula_04_012.png)是预测值，![公式
    4.13](img/Formula_04_013.png)是数据点的数量：
- en: '![](img/Formula_04_014.jpg)![Figure 4.10: Calculating the residual of a linear
    regression model](img/B18934_04_10.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![公式 4.14](img/Formula_04_014.jpg)![图 4.10：计算线性回归模型的残差](img/B18934_04_10.jpg)'
- en: 'Figure 4.10: Calculating the residual of a linear regression model'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：计算线性回归模型的残差
- en: 'R2, also known as the coefficient of determination, measures the proportion
    of variance in the dependent variables (Y) that can be explained by the independent
    variables (X). It essentially tells us how well the data fits the model. Unlike
    the RMSE, which can be an arbitrary number, R2 is given as a percentage, which
    can be easier to understand. The higher the percentage, the better the correlation
    of data. Although useful, a higher percentage is not always indicative of a strong
    model. What determines a good R2 value depends on the application and how the
    user understands the data. R2 can be calculated by using this formula:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: R2，也称为确定系数，衡量了因变量（Y）的方差中可以被自变量（X）解释的比例。它本质上告诉我们数据与模型拟合得有多好。与RMSE不同，RMSE可以是一个任意数，R2以百分比的形式给出，这可能更容易理解。百分比越高，数据的关联性越好。尽管很有用，但高百分比并不总是意味着模型强大。决定一个好的R2值取决于应用和用户如何理解数据。R2可以通过以下公式计算：
- en: '![](img/Formula_04_015.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![公式 4.15](img/Formula_04_015.jpg)'
- en: Many more metrics can evaluate the effectiveness of your regression model, but
    these two are more than enough to get an understanding of how your model is performing.
    When building and evaluating your model, it is important to plot and visualize
    your data and model, as this can identify key points. The plots can help you determine
    whether your model is **overfitting** or **underfitting**.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多更多的指标可以评估你的回归模型的有效性，但这两个指标已经足够让你了解你的模型是如何表现的。在构建和评估你的模型时，重要的是绘制和可视化你的数据和模型，因为这可以识别关键点。图表可以帮助你确定你的模型是**过度拟合**还是**欠拟合**。
- en: 'Overfitting occurs when your model is too suited to your training data. Your
    RMSE will be really low, and you will have a training accuracy of almost 100%.
    While this seems tempting, it is an indication of a poor model. This can be caused
    by one of two things: not enough data or too many parameters. As a result, when
    you test your model on new data it has not seen before, it will perform very poorly
    due to it not being able to generalize the data.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型过于适合你的训练数据时，会发生过度拟合。你的RMSE将会非常低，并且你的训练准确率几乎达到100%。虽然这看起来很有吸引力，但这表明模型不好。这可能是由于以下两个原因之一：数据不足或参数过多。因此，当你用之前未见过的新数据测试你的模型时，由于它无法泛化数据，它的表现会非常差。
- en: '![Figure 4.11: An overfitted linear regression model](img/B18934_04_11.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11：过度拟合的线性回归模型](img/B18934_04_11.jpg)'
- en: 'Figure 4.11: An overfitted linear regression model'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：过度拟合的线性回归模型
- en: 'To address overfitting, you can try to increase the amount of training data,
    or make the model less complex. It also helps to randomly shuffle your data before
    you split it into the training and testing set. Another important technique is
    called **regularization**. While there are many different regularization techniques
    (L1 or L2 regularization) depending on the model, they all work similarly in that
    they add **bias** or noise into the model to prevent overfitting. In the regression
    equation we previously saw, we can add another term, ![](img/Formula_04_016.png),
    to show that regularization is being applied to our model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决过度拟合问题，你可以尝试增加训练数据量，或者使模型更简单。在将数据分成训练集和测试集之前随机打乱数据也有帮助。另一个重要的技术称为**正则化**。虽然根据模型的不同，有许多不同的正则化技术（L1或L2正则化），但它们在防止过度拟合方面的工作方式相似，即它们向模型中添加**偏差**或噪声。在我们之前看到的回归方程中，我们可以添加另一个项，![](img/Formula_04_016.png)，以表明正则化正在应用于我们的模型：
- en: '![](img/Formula_04_017.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_017.jpg)'
- en: 'On the other end, underfitting occurs when your model is unable to find any
    meaningful correlation within the data. This is not as common as overfitting since
    it is easy to find patterns in most data. If this occurs, either your data has
    too much noise and is severely uncorrelated, your model is too simple and doesn’t
    have enough parameters, or the model is not effective for the application at hand.
    It is also useful to debug your code and make sure there are no bugs when it comes
    to preprocessing your data or setting up your model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一端，当你的模型无法在数据中找到任何有意义的相关性时，会发生欠拟合。由于在大多数数据中很容易找到模式，所以这不如过度拟合常见。如果发生这种情况，要么是因为你的数据噪声太多且严重不相关，要么是因为你的模型太简单且参数不足，或者模型对当前的应用不有效。在调试代码并确保在预处理数据或设置模型时没有错误也是很有用的：
- en: '![Figure 4.12: An under-fitted linear regression model](img/B18934_04_12.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12：欠拟合的线性回归模型](img/B18934_04_12.jpg)'
- en: 'Figure 4.12: An under-fitted linear regression model'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：欠拟合的线性回归模型
- en: 'Therefore, the goal is to find the best-fitting model, between an overfit and
    an underfit. It takes time and experimentation to find a model that works for
    your needs, but using the key indicators and metrics discussed here can help guide
    you in the right direction:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标是找到一个最佳拟合模型，介于过度拟合和欠拟合之间。找到适合你需求的模型需要时间和实验，但使用这里讨论的关键指标和度量可以帮助你指引正确的方向：
- en: '![Figure 4.13: A best-fitting linear regression model](img/B18934_04_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图4.13：最佳拟合的线性回归模型](img/B18934_04_13.jpg)'
- en: 'Figure 4.13: A best-fitting linear regression model'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：最佳拟合的线性回归模型
- en: Important Note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Feature engineering is a critical part of building a comprehensive model. Understanding
    your data can help determine which features or parameters to include in your model
    so that you can capture the relationship between the independent and dependent
    variables without causing overfitting.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是构建全面模型的关键部分。理解你的数据可以帮助确定在模型中包含哪些特征或参数，这样你就可以捕捉独立变量和因变量之间的关系，而不会导致过度拟合。
- en: 'There are some key notes to keep in mind when collecting and working with the
    data for your model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集和处理模型数据时，有一些关键点需要注意：
- en: '**Normalize your data**: It is possible to have features with very high or
    low numbers, so to prevent them from overwhelming the model and creating biases,
    it is imperative to normalize all your data to make it uniform across the features.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化数据**：可能存在具有非常高的或很低的数值的特征，为了防止它们压倒模型并产生偏差，将所有数据归一化以使它们在特征上保持一致是至关重要的。'
- en: '**Clean your data**: In the real world, the data we collect isn’t always perfect
    and can contain missing or egregious data. It is important to deal with these
    issues because they can cause outliers and impact the model negatively.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理数据**：在现实世界中，我们收集的数据并不总是完美的，可能包含缺失或严重的错误数据。处理这些问题很重要，因为它们可能导致异常值并负面影响模型。'
- en: '**Understand the data**: It is a common practice to perform statistical analysis,
    also known as **Exploratory Data Analysis** (**EDA**), on your data to get a better
    understanding of how the data can impact your model. This can include plotting
    graphs, running statical methods, and even using machine learning techniques to
    reduce the dimensionality of the data, which will be discussed later in the chapter.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解数据**：对数据进行统计分析，也称为**探索性数据分析**（**EDA**），是常见的做法，以更好地了解数据如何影响模型。这可以包括绘制图表、运行统计方法，甚至使用机器学习技术来降低数据的维度，这些将在本章后面讨论。'
- en: In the next section, we will discuss classification models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论分类模型。
- en: Building multiclass classification
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建多类分类
- en: Unlike regression models that produce a continuous output, models are considered
    classification models when they produce a finite output. Some examples include
    email spam detection, image classification, and speech recognition.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与产生连续输出的回归模型不同，当模型产生有限输出时，它们被认为是分类模型。一些例子包括垃圾邮件检测、图像分类和语音识别。
- en: Classification models are considered versatile since they can apply to both
    supervised and unsupervised learning while regression models are mostly used for
    supervised learning. There are some regression models (such as logistic regression
    and support vector machine) that are also considered classification models since
    they use a threshold to split the output of continuous values into different categories.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型被认为是多才多艺的，因为它们可以应用于监督学习和无监督学习，而回归模型主要用于监督学习。有一些回归模型（如逻辑回归和支持向量机）也被认为是分类模型，因为它们使用阈值将连续值的输出分割成不同的类别。
- en: Unsupervised learning is a common application used in today’s market. Although
    supervised learning usually performs better and provides meaningful results since
    we know the expected output, the majority of the data we collect is unlabeled.
    It costs companies time and money for human experts to sift through the data and
    label it. Unsupervised learning helps reduce the cost and time by getting the
    model to try and determine the labels for the data and extract meaningful information.
    They can even perform better than humans sometimes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是当今市场上常用的应用。尽管监督学习通常表现更好，并提供了有意义的成果，因为我们知道预期的输出，但我们收集的大多数数据都是未标记的。对于公司来说，让人类专家筛选数据并进行标记既耗时又费钱。无监督学习通过让模型尝试为数据确定标签并提取有意义的信息，有助于降低成本和时间。有时它们甚至可以比人类表现得更好。
- en: 'The number of categories in the output of a classification model determines
    what type of model it is. For models with only two outputs (i.e., spam and not
    spam), this is called a binary classifier, while models with more than two outputs
    are called multiclass classifiers:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型输出的类别数量决定了它的类型。对于只有两个输出（即垃圾邮件和非垃圾邮件）的模型，这被称为二元分类器，而具有两个以上输出的模型被称为多类分类器：
- en: '![Figure 4.14: Binary and multiclass classifiers](img/B18934_04_14.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图4.14：二元和多类分类器](img/B18934_04_14.jpg)'
- en: 'Figure 4.14: Binary and multiclass classifiers'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14：二元和多类分类器
- en: 'From those classifiers, there are two types of learners: **lazy learners**
    and **eager learners**.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从那些分类器中，有两种类型的学习者：**懒惰学习者**和**急切学习者**。
- en: Lazy learners essentially store the training data and wait until they receive
    new test data. Once they get the test data, the model classifies the new data
    based on the already existing data. These types of learners take less time when
    training since you can continuously add new data without having to retrain the
    entire model, but take longer when performing classification since they have to
    go through all the data points. One common type of lazy learner is the **K-Nearest
    Neighbors** (**KNN**) algorithm.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 懒惰学习者基本上存储训练数据，并等待接收到新的测试数据。一旦他们获得测试数据，模型就会根据已有的数据对新数据进行分类。这些类型的学习者训练时所需时间较少，因为你可以连续添加新数据，而无需重新训练整个模型，但在执行分类时需要更多时间，因为他们必须通过所有数据点。一种常见的懒惰学习者类型是**K-最近邻**（**KNN**）算法。
- en: On the other hand, eager learners work in the opposite way. Whenever new data
    is added to the model, they have to retrain the model again. Although this takes
    more time compared to lazy learners, querying the model is much faster since they
    don’t have to go through all the data points. Some examples of eager learners
    are decision trees, naïve Bayes, and ANNs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，急切的学习者以相反的方式工作。每当有新数据添加到模型中时，他们必须重新训练模型。虽然与懒惰学习者相比，这需要更多的时间，但查询模型的速度要快得多，因为他们不必通过所有数据点。一些急切学习者的例子是决策树、朴素贝叶斯和人工神经网络。
- en: Important Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Supervised learning will generally perform better than unsupervised learning
    since we know what the expected output should be during training, but it is costly
    to have to collect and label the data, so unsupervised learning excels in this
    area of training on unlabeled data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通常比无监督学习表现更好，因为我们知道在训练过程中预期的输出应该是什么，但是收集和标记数据成本高昂，因此无监督学习在训练未标记数据方面表现出色。
- en: In the next few sections, we will be looking at a few niche models that can
    be used for unique problems that most basic classification or regression models
    can’t solve.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探讨一些用于解决大多数基本分类或回归模型无法解决的独特问题的利基模型。
- en: Text sentiment analysis and topic modeling
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本情感分析和主题建模
- en: A popular field in the machine learning field is topic modeling and text analysis.
    With a plethora of text on the internet, being able to understand that data and
    create complex models such as chatbots and translation services has become a hot
    topic. Interacting with human language using software is called **NLP**.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域的一个热门领域是主题建模和文本分析。随着互联网上文本的激增，能够理解这些数据并创建复杂的模型，如聊天机器人和翻译服务，已经成为一个热门话题。使用软件与人类语言交互被称为**NLP**。
- en: Despite the amount of data we can use to train our models, it is a difficult
    task to create meaningful models. Language itself is complex and contains many
    grammar rules, especially when trying to translate between languages. Certain
    powerful techniques can help us when creating NLP models though.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以使用大量数据来训练我们的模型，但创建有意义的模型是一项困难的任务。语言本身很复杂，包含许多语法规则，尤其是在尝试翻译不同语言时。某些强大的技术可以帮助我们在创建NLP模型时。
- en: Important Note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before implementing any NLP models, it is imperative to preprocess the data
    in some way. Documents and text tend to contain extraneous data, such as stopping
    words (the/a/and) or random characters, which can affect the model and produce
    flawed results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施任何NLP模型之前，以某种方式预处理数据至关重要。文档和文本往往包含一些无关数据，如停用词（the/a/and）或随机字符，这些可能会影响模型并产生错误的结果。
- en: 'The first idea we will discuss is **topic modeling**. This is the process of
    grouping text or words from documents into different topics or fields. This is
    useful when you have a document or text and want to classify and group it into
    a certain genre without having to go through the tedious process of reading documents
    one by one. There are many different models used for topic modeling:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的第一个想法是**主题建模**。这是一个将文档或文本中的文本或单词分组到不同主题或领域的进程。当你有一份文档或文本，并希望将其分类和分组到某个特定类型，而不必逐个阅读文档时，这非常有用。用于主题建模有许多不同的模型：
- en: '**Latent Semantic** **Analysis** (**LSA**)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在语义分析**（**LSA**）'
- en: '**Probabilistic Latent Semantic** **Analysis** (**PLSA**)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率潜在语义分析**（**PLSA**）'
- en: '**Latent Dirichlet** **Allocation** (**LDA**)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）'
- en: 'We will focus on LDA. LDA uses statistics to find patterns and repeated occurrences
    of words or phrases and groups them into their topics. It assumes that each document
    contains a mixture of topics and that each topic contains a mixture of words.
    LDA first starts the process of going through the documents and keeping a word
    matrix, where it contains the count of each word in each document:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注 LDA。LDA 使用统计学来寻找单词或短语的重复出现模式，并将它们分组到相应的主题中。它假设每个文档包含主题的混合，每个主题包含单词的混合。LDA
    首先开始处理文档的过程，并保持一个词矩阵，其中包含每个文档中每个单词的计数：
- en: '![Figure 4.15: A word matrix](img/B18934_04_15.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15：词矩阵](img/B18934_04_15.jpg)'
- en: 'Figure 4.15: A word matrix'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15：词矩阵
- en: 'After creating a word matrix, we determine the number of topics, ![](img/Formula_04_018.png),
    that we want to split up the words into and use statistics to find the probability
    of the words belonging to a certain topic. Using Bayesian statistics, we can then
    calculate the probability and use that to cluster the words into different topics:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建词矩阵后，我们确定要分割单词的主题数量，![](img/Formula_04_018.png)，并使用统计学来找到单词属于某个主题的概率。使用贝叶斯统计学，我们可以计算出概率，并使用该概率将单词聚类到不同的主题中：
- en: '![Figure 4.16: An LDA model](img/B18934_04_16.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16：LDA 模型](img/B18934_04_16.jpg)'
- en: 'Figure 4.16: An LDA model'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16：LDA 模型
- en: Another rising application in NLP is **sentiment analysis**. This involves the
    process of taking words or text and understanding the user’s intent or emotion.
    This is common today when dealing with online reviews or social media posts. It
    determines whether a piece of text contains positive, neutral, or negative emotions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在自然语言处理（NLP）中日益增长的应用是**情感分析**。这涉及对单词或文本进行理解，以了解用户的意图或情感。这在处理在线评论或社交媒体帖子时很常见。它决定了文本是否包含积极、中性或消极的情感。
- en: 'Many different methods and models can solve this problem. The simplest approach
    is through statistics by using Bayes’ theorem. This formula is used for predictive
    analysis, as it uses previous words in a text to update the model. The probability
    can be calculated using this formula:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 许多不同的方法和模型可以解决这个问题。最简单的方法是通过使用贝叶斯定理进行统计学。这个公式用于预测分析，因为它使用文本中的先前单词来更新模型。可以使用此公式计算概率：
- en: '![](img/Formula_04_019.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_019.jpg)'
- en: Deep learning has become a powerful tool for NLP and can be useful for sentiment
    analysis. CNNs and **Recurrent Neural Networks** (**RNNs**) are two types of deep
    learning models that can drastically improve models for NLP, especially for sentiment
    analysis. We will discuss these neural networks and how they perform more later
    in this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已成为 NLP 的强大工具，对情感分析很有用。卷积神经网络（CNNs）和**循环神经网络**（**RNNs**）是两种可以显著提高 NLP 模型，特别是情感分析模型的深度学习模型。我们将在本章后面讨论这些神经网络及其性能。
- en: Pattern analysis and forecasting in machine learning
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的模式分析和预测
- en: With the uncertainty of time, being able to predict certain trends and patterns
    has become a hot topic in today’s industry. Most regression models, while powerful,
    are not able to make confident time predictions. As a result, some researchers
    have devised models that take time into consideration when making certain predictions,
    such as gas prices, stock market, and sales forecasting. Before we go into the
    different models, we must first understand the different concepts in time-series
    analysis.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的不确定性，能够预测某些趋势和模式已成为当今行业的热门话题。尽管大多数回归模型功能强大，但它们无法做出自信的时间预测。因此，一些研究人员设计了在做出某些预测时考虑时间的模型，例如油价、股市和销售预测。在我们深入探讨不同的模型之前，我们首先必须了解时间序列分析中的不同概念。
- en: 'The first step when dealing with time-series problems is familiarizing yourself
    with the data. The data usually contains one of four data components:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间序列问题的第一步是熟悉数据。数据通常包含以下四种数据组件之一：
- en: '**Trend** – The data follows an increasing or decreasing continuous timeline
    and there are no periodic changes'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**趋势** – 数据遵循增加或减少的连续时间线，没有周期性变化'
- en: '**Seasonality** – The data changes in a set periodic timeline'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**季节性** – 数据在一定的周期性时间线上发生变化'
- en: '**Cyclical** – The data changes but there is no set periodic timeline'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期性** – 数据发生变化，但没有固定的周期性时间线'
- en: '**Irregular** – The data changes randomly with no pattern'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不规则** – 数据随机变化，没有模式'
- en: '![Figure 4.17: Different components of time-series data](img/B18934_04_17.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.17：时间序列数据的不同组成部分](img/B18934_04_17.jpg)'
- en: 'Figure 4.17: Different components of time-series data'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17：时间序列数据的不同组件
- en: 'These different trends can be split into two different data types:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的趋势可以分为两种不同的数据类型：
- en: '**Stationary** – Certain attributes of the data, such as mean, variance, and
    covariance, do not change over time'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平稳** – 数据的某些属性，如均值、方差和协方差，随时间不变'
- en: '**Non-stationary** – Attributes of the data change over time'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非平稳** – 数据的属性随时间变化'
- en: 'Often, you will work with non-stationary data, and creating machine learning
    models using this type of data will generate unreliable results. To resolve this
    issue, we use certain techniques to change our data into stationary data. They
    include the following methods:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你将处理非平稳数据，使用这类数据创建机器学习模型将产生不可靠的结果。为了解决这个问题，我们使用某些技术将我们的数据转换为平稳数据。它们包括以下方法：
- en: '**Differencing** – A mathematical method used to normalize the mean and remove
    the variance. It can be calculated by using this formula:![](img/Formula_04_020.jpg)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**差分** – 一种用于标准化均值并消除方差数学方法。它可以通过以下公式计算：![](img/Formula_04_020.jpg)'
- en: '**Transformation** – Mathematical methods are used to remove the change in
    variance. Among the transformations, these are three commonly used:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换** – 使用数学方法来消除方差的变化。在转换方法中，以下三种是常用的：'
- en: Log transform
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数变换
- en: Square root
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方根
- en: Power transform
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幂变换
- en: '![Figure 4.18: Differencing non-stationary data](img/B18934_04_18.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图4.18：差分非平稳数据](img/B18934_04_18.jpg)'
- en: 'Figure 4.18: Differencing non-stationary data'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18：差分非平稳数据
- en: Important Note
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Time is already uncertain, and this makes it almost impossible to create a model
    that can confidently predict future trends. The more we can remove uncertainty
    in our data, the better our model can find the relationships in our data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 时间本身就是不确定的，这使得创建一个可以自信预测未来趋势的模型几乎不可能。我们能在数据中消除的不确定性越多，我们的模型就能更好地找到数据中的关系。
- en: 'Once we can transform our data, we can start looking at models to help us with
    forecasting. Of the different models, the most popular model for time-series analysis
    is an **Auto-Regressive Integrated Moving Average** (**ARIMA**) model. This linear
    regression model consists of three subcomponents:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们可以转换我们的数据，我们就可以开始考虑使用模型来进行预测。在众多模型中，用于时间序列分析最流行的模型是**自回归积分移动平均**（**ARIMA**）模型。这个线性回归模型由三个子组件组成：
- en: '**Auto-Regression** (**AR**) – A regression model that uses the dependencies
    of the current time and previous time to make predictions'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自回归**（**AR**） – 使用当前时间和前一时间依赖性进行预测的回归模型'
- en: '**Integrated** (**I**) – The process of differencing in order to make the data
    stationary'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**积分**（**I**） – 差分的过程，以便使数据平稳'
- en: '**Moving Average** (**MA**) – Models between the expected data and the residual
    error by calculating the MA of the lagged observed data'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动平均**（**MA**） – 通过计算滞后观测数据的移动平均来模型预期数据和残差误差'
- en: Along with ARIMA, other machine learning models can be used for time-series
    problems. Another well-known model is the RNN model. This is a type of deep learning
    model used for data that has some sort of sequence. We will be going into more
    detail on how they work in the next section.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ARIMA，其他机器学习模型也可以用于时间序列问题。另一个著名的模型是RNN模型。这是一种用于具有某种序列数据的深度学习模型。我们将在下一节中更详细地介绍它们是如何工作的。
- en: Enhancing models using deep learning
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习增强模型
- en: Earlier in the chapter, we briefly discussed deep learning and the advantages
    it brings when enhancing simple machine learning models. In this section, we will
    go into more information on the different deep learning models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们简要讨论了深度学习及其在增强简单机器学习模型时带来的优势。在本节中，我们将更深入地介绍不同的深度学习模型。
- en: Before we can build our model, we will briefly go over the structure of the
    deep learning models. A simple ANN model usually contains about two to three fully
    connected layers and is usually strong enough to model most complex linear functions,
    but as we add more layers, the improvement to the model significantly diminishes,
    and it is unable to perform more complex applications due to overfitting.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建模型之前，我们将简要介绍深度学习模型的结构。一个简单的ANN模型通常包含大约两到三个全连接层，并且通常足够强大，可以模拟大多数复杂的线性函数，但随着我们添加更多层，模型改进的幅度显著减小，并且由于过拟合，它无法执行更复杂的应用。
- en: Deep learning allows us to add multiple hidden layers to our ANN while reducing
    our time to train the model and increasing the performance. We can do this by
    adding one or both types of hidden layers common in deep learning – a CNN or RNN.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使我们能够在我们的ANN中添加多个隐藏层，同时减少训练模型的时间并提高性能。我们可以通过添加深度学习中常见的隐藏层类型之一或两种来实现这一点——CNN或RNN。
- en: 'A CNN is mostly applied in the image detection and video recognition field
    due to how the neural network is structured. The CNN architecture comprises the
    following key features:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络的结构，CNN主要应用于图像检测和视频识别领域。CNN架构包括以下关键特性：
- en: Convolutional layers
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Activation layers
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活层
- en: Pooling layers
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: 'The convolutional layer is the core element in the CNN model. Its primary task
    is to convolve or group sections of the data using a **kernel** and produces an
    output called a **feature map**. This map contains all the key features extracted
    from the data, which can then be used for training in the fully connected layer.
    Each element in the feature map indicates a receptive field, which is used to
    denote which part of the input is used to map to the output. As you add more convolutional
    layers, you can extract more features, and this allows your model to adapt to
    more complex models:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是CNN模型的核心元素。其主要任务是使用**核**对数据进行卷积或分组，并产生一个称为**特征图**的输出。这个图包含了从数据中提取的所有关键特征，这些特征随后可以在全连接层中进行训练。特征图中的每个元素都表示一个感受野，用于表示输入的哪一部分被用来映射到输出。随着你添加更多的卷积层，你可以提取更多的特征，这允许你的模型适应更复杂的模型：
- en: '![Figure 4.19: Convolutional layer output](img/B18934_04_19.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图4.19：卷积层输出](img/B18934_04_19.jpg)'
- en: 'Figure 4.19: Convolutional layer output'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19：卷积层输出
- en: In *Figure 4**.19*, we can see how the feature map is created in the convolutional
    layer. The layer slides the kernel across the input data, performs the dot operation,
    and produces an output matrix, which is the result of the convolution function.
    The size of the kernel and the step size can dictate the output size of the feature
    map. In this example, we use a kernel size of 2x2 and a stride or step size of
    2, which gives us a feature map of size 2x2 based on our input size. The output
    of the feature map can then be used for more future convolutional layers depending
    on the requirements of the user.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图4.19**中，我们可以看到特征图是如何在卷积层中创建的。该层将核在输入数据上滑动，执行点积操作，并产生一个输出矩阵，这是卷积函数的结果。核的大小和步长可以决定特征图的输出大小。在这个例子中，我们使用2x2的核大小和2的步长或步长大小，根据我们的输入大小得到一个2x2大小的特征图。特征图的输出可以根据用户的需求用于更多的未来卷积层。
- en: 'Before we use our newly created feature map as an input for another convolutional
    or fully connected layer, we pass the feature map through an activation layer.
    It is important to pass your data through some type of nonlinear function during
    the training process, as this allows your model to map to more complex functions.
    Many different types of activation functions can be used throughout your model
    and have their benefits depending on the type of model you are planning to build.
    Among the many activation functions, these are the most commonly used:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将新创建的特征图作为另一个卷积层或全连接层的输入之前，我们需要将特征图通过一个激活层。在训练过程中，通过某种类型的非线性函数传递数据非常重要，因为这允许模型映射到更复杂的函数。在整个模型中可以使用许多不同类型的激活函数，并且根据你计划构建的模型类型具有各自的优势。在众多激活函数中，以下是最常用的：
- en: '**Rectified Linear** **Activation** (**ReLU**)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性** **修正** **激活** （**ReLU**）'
- en: '**Logistic (sigmoid)**'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑（sigmoid**）'
- en: The **Hyperbolic** **Tangent** (**Tanh**)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切（**Tanh**）
- en: 'The ReLU function is the most popular activation function used today. It is
    very simple and helps the model learn and converge more quickly than most other
    activation functions. It is calculated using this function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数是目前最流行的激活函数。它非常简单，有助于模型比大多数其他激活函数更快地学习和收敛。它是通过以下函数计算的：
- en: '![](img/Formula_04_021.jpg)![](img/B18934_04_20.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_021.jpg)![](img/B18934_04_20.jpg)'
- en: 'Figure 4.20: ReLU function'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20：ReLU函数
- en: 'The logistic function is another commonly used activation function. This is
    the same function used in logistical regression models. This function helps bound
    the output of the feature map between 0 and 1\. While useful, this function is
    computationally heavy and may slow down the training process. It is calculated
    using this function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数是另一种常用的激活函数。这是在逻辑回归模型中使用的相同函数。这个函数有助于将特征图的输出限制在0到1之间。虽然有用，但这个函数计算量大，可能会减慢训练过程。它使用以下函数计算：
- en: '![](img/Formula_04_022.jpg)![](img/B18934_04_21.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_022.jpg)![](img/B18934_04_21.jpg)'
- en: 'Figure 4.21: Sigmoid function'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.21：Sigmoid函数
- en: 'The Tanh function is similar to the sigmoid function in that it bounds the
    values from the feature map. Rather than bounding it from 0 to 1, it bounds the
    values from -1 to 1, and it usually performs better than a sigmoid function. The
    function is calculated using this formula:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh函数与sigmoid函数类似，它限制了特征图的值。而不是从0到1限制，它从-1到1限制值，并且通常比sigmoid函数表现更好。该函数使用以下公式计算：
- en: '![](img/Formula_04_023.jpg)![](img/B18934_04_22.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_023.jpg)![](img/B18934_04_22.jpg)'
- en: 'Figure 4.22: Tanh function'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.22：Tanh函数
- en: Each activation has its uses and benefits depending on the task or model at
    hand. The ReLU function is commonly used in CNN models while sigmoid and Tanh
    are mostly found in RNN models, but they can be used interchangeably and bring
    different results.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 每个激活函数都有其用途和好处，这取决于手头的任务或模型。ReLU函数在CNN模型中常用，而sigmoid和Tanh主要在RNN模型中找到，但它们可以互换使用并产生不同的结果。
- en: 'After we run our feature map through an activation layer, we come to the final
    piece – the pooling layer. As mentioned before, a key element in deep learning
    is the reduction of parameters. This allows the model to train on fewer parameters
    while still retaining the important features extracted from our convolutional
    layers. The pooling layer is responsible for this step of downsizing our parameter
    size. There are many common pooling functions but the most commonly used is max
    pooling. This is similar to the convolutional layer, where we use a kernel or
    filter to slide through our input data and only take the maximum value from each
    window:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将特征图通过激活层处理后，我们来到了最后一部分——池化层。正如之前提到的，深度学习中的一个关键元素是参数的减少。这允许模型在保持从卷积层提取的重要特征的同时，使用更少的参数进行训练。池化层负责这一步缩小我们的参数大小。有许多常见的池化函数，但最常用的是最大池化。这与卷积层类似，我们在输入数据上滑动一个核或过滤器，并只从每个窗口中取最大值：
- en: '![Figure 4.23: Max pooling layer output](img/B18934_04_23.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 4.23: Max pooling layer output](img/B18934_04_23.jpg)'
- en: 'Figure 4.23: Max pooling layer output'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.23：最大池化层输出
- en: In *Figure 4**.23*, we are using a kernel size of 2x2 with a stride size of
    2\. Here, we can see our output where only the maximum value from each window
    is selected.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4**.23*中，我们使用2x2的核大小和2的步长大小。在这里，我们可以看到我们的输出，其中只选择了每个窗口的最大值。
- en: Other layers and functions can be added to the model to help address certain
    issues and applications, such as a batch normalization layer, but with these three
    foundational layers, we can add multiple layers of different sizes and still build
    a powerful model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 可以向模型添加其他层和函数来帮助解决某些问题或应用，例如批量归一化层，但有了这三个基础层，我们仍然可以添加不同大小的多层，并构建一个强大的模型。
- en: In the final layer, we feed our output into the fully connected layer. By that
    time, we are able to extract the important features of the data and still learn
    more complex models with less time to train as compared to a simple ANN model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一层，我们将输出输入到全连接层。到那时，我们能够提取数据的重要特征，并且与简单的ANN模型相比，在更短的时间内学习更复杂的模型。
- en: Next, we will go over the RNN architecture. Due to the nature of how the RNN
    model is structured, it is designed for tasks that need to take into consideration
    a set of sequence data, in which the data later in the sequence is dependent on
    earlier data. This model is commonly used for certain fields such as NLP and signal
    processing.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍RNN架构。由于RNN模型的结构特性，它被设计用于需要考虑一系列序列数据的任务，其中序列中的数据后依赖于前面的数据。这个模型通常用于某些领域，如NLP和信号处理。
- en: The basics of an RNN are built by having a hidden layer in which the output
    of the layer is fed back into the same hidden layer. This way, the model is able
    to learn based on previous data and adjust the weights accordingly.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的基本结构是通过在隐藏层中有一个输出，该输出被送回到同一个隐藏层来构建的。这样，模型能够根据先前数据学习，并相应地调整权重。
- en: '![Figure 4.24: RNN architecture](img/B18934_04_24.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图4.24：RNN架构](img/B18934_04_24.jpg)'
- en: 'Figure 4.24: RNN architecture'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.24：RNN架构
- en: To better understand how the model works, you can envision a single layer for
    each data point. Each layer takes in the data point as an input ![](img/Formula_04_024.png)
    and produces an output ![](img/Formula_04_025.png). We then transfer the weights
    between the layers and then take the total average of all the cost functions from
    all the layers in the model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解模型的工作原理，你可以想象每个数据点都有一个单独的层。每一层将数据点作为输入 ![](img/Formula_04_024.png) 并产生一个输出
    ![](img/Formula_04_025.png)。然后我们在层之间传递权重，并取模型中所有层的总成本函数的平均值。
- en: '![Figure 4.25: An unraveled RNN](img/B18934_04_25.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图4.25：展开的RNN](img/B18934_04_25.jpg)'
- en: 'Figure 4.25: An unraveled RNN'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.25：展开的RNN
- en: 'This model works for most simple problems. As the sequence increases, it encounters
    a few issues:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型适用于大多数简单问题。随着序列的增加，它遇到了一些问题：
- en: '**Vanishing gradient**: This occurs during the training process when the gradient
    approaches zero. The weights aren’t updated properly as a result and the model
    performs poorly.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失**：在训练过程中，当梯度接近零时发生这种情况。结果权重没有得到适当的更新，模型表现不佳。'
- en: '**Lack of context**: The model is unidirectional and cannot look further or
    previously into the data. Therefore, the model is only able to predict based on
    data around the current sequence point and is more likely to make a poor prediction
    based on incorrect context.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏上下文**：该模型是单向的，不能进一步或先前查看数据。因此，模型只能根据当前序列点周围的数据进行预测，并且更有可能基于错误上下文做出较差的预测。'
- en: 'There are different variations of RNNs created to address some of the issues
    mentioned here. Among them, the most common one used today is the **Long Short-Term
    Memory** (**LSTM**) model. The LSTM model comprise three components:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这里提到的一些问题，已经创建了不同版本的循环神经网络（RNNs）。其中，今天最常用的一个是**长短期记忆**（**LSTM**）模型。LSTM模型由三个组件组成：
- en: An input gate
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门
- en: An output gate
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出门
- en: A forget gate
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个忘记门
- en: '![Figure 4.26: An LSTM neural network](img/B18934_04_26.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图4.26：LSTM神经网络](img/B18934_04_26.jpg)'
- en: 'Figure 4.26: An LSTM neural network'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.26：LSTM神经网络
- en: These gates work by regulating which data points are needed to contextualize
    the sequence. That way, the model can predict more accurately without being easily
    manipulated.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门通过调节哪些数据点需要上下文化序列来工作。这样，模型可以更准确地预测，而不容易被操纵。
- en: The forget gate is specifically responsible for removing previous data or context
    that is no longer needed. This gate uses the sigmoid function to determine whether
    it uses or “forgets” the data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 忘记门专门负责移除不再需要的前置数据或上下文。这个门使用sigmoid函数来确定是否使用或“忘记”数据。
- en: The input gate is used to determine whether the new data is relevant to the
    current sequence or not. This is so that only important data is being used to
    train the model and not redundant or irrelevant information.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门用于确定新数据是否与当前序列相关。这样，只有重要的数据被用来训练模型，而不是冗余或不相关信息。
- en: Lastly, the output gate’s primary function is to filter the current state’s
    information and only send relevant information to the next state. As with the
    other gates, it uses the context from previous states to apply a filter, which
    helps the model properly contextualize the data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出门的主要功能是过滤当前状态的信息，并且只将相关信息发送到下一个状态。与其他门一样，它使用先前状态中的上下文来应用过滤器，这有助于模型正确地上下文化数据。
- en: CNN and RNN models are mostly designed for supervised learning problems. When
    it comes to unsupervised learning, different models are needed to solve certain
    problems. Let’s discuss **autoencoders**.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）和循环神经网络（RNN）模型主要设计用于监督学习问题。当涉及到无监督学习时，需要不同的模型来解决某些问题。让我们讨论**自编码器**。
- en: Autoencoders work by taking the input data, compressing it, and then reconstructing
    it by decompressing it. While straightforward, it can be used for some advanced
    applications, such as generating audio or images, or it can be used as an anomaly
    detector.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通过接收输入数据，压缩它，然后通过解压缩它来重建它。虽然简单，但它可以用于一些高级应用，例如生成音频或图像，或者它可以用作异常检测器。
- en: 'The autoencoder comprises two parts:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器由两部分组成：
- en: An encoder
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器
- en: A decoder
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器
- en: '![Figure 4.27: The components of an autoencoder](img/B18934_04_27.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图4.27：自动编码器的组件](img/B18934_04_27.jpg)'
- en: 'Figure 4.27: The components of an autoencoder'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.27：自动编码器的组件
- en: The encoder and decoder are usually built with a one-layer ANN. The encoder
    is responsible for taking the data and compressing or flattening the data. Then,
    the decoder works on taking the flattened data and trying to reconstruct the input
    data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器通常使用单层人工神经网络（ANN）构建。编码器负责接收数据并压缩或展平数据。然后，解码器处理展平的数据，并尝试重建输入数据。
- en: The hidden layer in the middle of the encode and decoder is usually referred
    to as the bottleneck. The number of nodes in the hidden layer must be less than
    those in the encoder and decoder. This forces the model to try and find the pattern
    or representation in the input data so that it can reconstruct the data with little
    information. Thus, the cost function is there to calculate and minimize the difference
    between the input and output data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器中间的隐藏层通常被称为瓶颈。隐藏层中的节点数必须少于编码器和解码器中的节点数。这迫使模型尝试在输入数据中找到模式或表示，以便用少量信息重建数据。因此，成本函数用于计算并最小化输入和输出数据之间的差异。
- en: One aspect of autoencoders that is an integral part of deep learning is **dimensionality
    reduction**. This is the process of reducing the number of parameters or features
    used when training your model. As mentioned earlier in this chapter, to build
    a complex model that can build a deeper representation of the data, it is important
    to include more features. However, adding too many features can lead to overfitting,
    so how do we find the best number of features to use in our model?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的一个方面是**降维**，这是深度学习的一个基本组成部分。这是在训练模型时减少参数或特征数量的过程。正如本章前面提到的，为了构建一个能够构建数据更深层次表示的复杂模型，包括更多特征是很重要的。然而，添加过多的特征可能导致过拟合，那么我们如何找到在模型中使用最佳特征数量呢？
- en: There are many models and techniques, such as autoencoders, that can perform
    dimensionality reduction to help us find the best features to use in our model.
    Among the different techniques, **Principle Component Analysis** (**PCA**) is
    the most popular. This technique can take an N-dimensional dataset and reduce
    the number of dimensions in the data using linear algebra. It is a common practice
    to use a dimensionality reduction technique before using your data to train your
    model, as this can help to remove noise in the data and avoid overfitting.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多模型和技术，例如自动编码器，可以执行降维以帮助我们找到在模型中使用最佳特征。在众多技术中，**主成分分析**（**PCA**）是最受欢迎的。这项技术可以将N维数据集通过线性代数减少数据的维度。在用数据训练模型之前使用降维技术是一种常见做法，因为这有助于去除数据中的噪声并避免过拟合。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed what is considered artificial intelligence and
    the different sub-fields that it contains.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了什么是人工智能以及它包含的不同子领域。
- en: We discussed the different characteristics of regression and classification
    models. From there, we also went over the structure of our data and how the model
    performs when training over our data. We then discussed the different ways of
    analyzing our model’s performance and how to address the different issues that
    we can come across when training our model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了回归和分类模型的不同特性。从那里，我们还了解了数据结构以及模型在数据上训练时的表现。然后，我们讨论了分析模型性能的不同方法以及解决在训练模型时可能遇到的不同问题的方法。
- en: We briefly viewed the different packages and libraries that are used today in
    machine learning models and their different use cases.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要地了解了今天在机器学习模型中使用的不同包和库及其不同的用例。
- en: We also analyzed different topics such as topic modeling and time-series analysis
    and what they entail. With that, we were able to look at the different methods
    and techniques used to solve those types of problems.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分析了不同主题，如主题建模和时间序列分析及其包含的内容。有了这些，我们能够查看解决这些类型问题的不同方法和技术。
- en: Lastly, we went into deep learning and the different ways it improves on machine
    learning. We went over the two different types of neural networks – CNNs and RNNs
    – how they are structured, and their benefits and use cases.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们深入探讨了深度学习及其如何改进机器学习。我们讨论了两种不同的神经网络类型——卷积神经网络（CNNs）和循环神经网络（RNNs）——它们的结构以及它们的优点和用例。
- en: In the next chapter, we will take what we have learned and start looking into
    how we can design and build an end-to-end machine learning system and the different
    components that it contains.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将运用所学知识，开始探讨如何设计和构建一个端到端的机器学习系统及其包含的不同组件。
