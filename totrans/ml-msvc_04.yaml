- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Key Machine Learning Algorithms and Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we explored the different concepts of MSA and the
    role it plays when creating enterprise systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, we will begin to shift our focus from learning about
    MSA concepts to learning about key machine learning concepts. We will also learn
    about the different libraries and packages being used in machine learning models
    using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following areas in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The differences between **artificial intelligence**, **machine learning**, and
    **deep learning**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common deep learning packages and libraries used in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building **regression** models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building multiclass **classification**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text sentiment analysis and topic modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pattern analysis and forecasting using machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building enhanced models using deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The differences between artificial intelligence, machine learning, and deep
    learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the recent rise in popularity of artificial intelligence and machine
    learning, the field of artificial intelligence has been around since the 1960s.
    With different sub-fields emerging, it is important to be able to differentiate
    between them and understand them and what they entail.
  prefs: []
  type: TYPE_NORMAL
- en: To start, artificial intelligence is the overarching field that encompasses
    all the sub-fields we see today, such as machine learning, deep learning, and
    more. Any system that perceives or receives information from its environment and
    carries out an action to maximize the reward or achieve its goal is considered
    to be an artificially intelligent machine.
  prefs: []
  type: TYPE_NORMAL
- en: This is commonly used today when it comes to robotics. Most of our machines
    are designed so that they can capture data using their sensors, such as cameras,
    sonars, or gyroscopes, and use the data captured to respond to a particular task
    most efficiently. This concept is very similar to how humans function. We use
    our senses to “capture” information from our environment and based on the information
    we receive, we carry out certain actions.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence is an expansive field, but it can be broken into different
    sub-fields, one we commonly know today as machine learning. What makes machine
    learning unique is that this field works on creating systems or machines that
    can continually learn and improve their model without explicitly being programmed.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning does this by collecting data, also known as training data,
    and trying to find patterns in the data to make accurate predictions without being
    programmed to do so. There are many different methods used in machine learning
    to learn the data and the methods are tailored to the different problems we encounter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Different fields in artificial intelligence](img/B18934_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Different fields in artificial intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning problems can be broken down into three different tasks: **supervised
    learning**, **unsupervised learning**, and **reinforcement learning**. For now,
    we will focus on supervised and unsupervised learning. This distinction is based
    on the training data that we have. Supervised learning is when we have the input
    data and the expected output for the particular set of data, which is also called
    the label. Unsupervised learning, on the other hand, only consists of the input
    without an expected output.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning works by understanding the relationship between the input
    and output data. One common example of supervised learning is predicting the price
    of a home in a certain city. We can collect data on existing homes by capturing
    their specifications and their current prices and then learn the pattern between
    the characteristics of these homes and their prices. We can then take a home,
    not in our training set, and test our model by inputting the features of the house
    into our program and have the model predict the price of the home.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning works by learning about the structure of the data either
    using grouping or clustering methods. This method is commonly used for marketing
    purposes. For example, a store wants to cluster its customers into different groups
    so that it can efficiently tailor its products to different demographics. It can
    capture the purchase history of its customers, use that data to learn about purchasing
    patterns, and suggest certain items or goods that would interest them, thus maximizing
    its revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can understand deep learning, which is a sub-field of machine learning,
    we must first understand what **Artificial Neural Networks** (**ANNs**) are. Taking
    inspiration from neurons in a brain, ANNs are models that comprise a network of
    fully connected nodes, also known as artificial neurons. They contain a set of
    inputs, hidden layers connecting the neurons, and also an output node. Each neuron
    has an input and output, which can be propagated throughout the network. In order
    to calculate the output of a neuron, we take the weighted sum of all the inputs,
    multiply it by the weight of the neuron, and then usually add a bias term.
  prefs: []
  type: TYPE_NORMAL
- en: We continue to perform these actions until we reach the last layer, which is
    the output neuron. We perform a nonlinear activation function, such as a sigmoid
    function, to give us the final prediction. We then take the predicted output value
    and input it in a **cost function**. This function tells us how well our network
    is learning. We take this value and backpropagate through our layers back to the
    first layer, adjusting the weights of the neurons depending on how our network
    is performing. With this, we can create strong models that can perform tasks such
    as handwriting recognition, game-playing AI, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: A program is considered to be a machine learning model if it can take input
    data and learn the patterns to make predictions without being explicitly programmed
    to.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: An ANN](img/B18934_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: An ANN'
  prefs: []
  type: TYPE_NORMAL
- en: 'While ANNs are capable of performing many tasks, there are significant downsides
    that limit their use in today’s market:'
  prefs: []
  type: TYPE_NORMAL
- en: It can be difficult to understand how the model performs. As you add more hidden
    layers to the network, it becomes complicated to try and debug the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model takes a long time, especially with copious amounts of training
    data, and can drain hardware resources, as it is difficult to perform all these
    mathematical operations on a CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest issue with ANNs is overfitting. As we add more hidden layers, there
    is a point at which the weights assigned to the neurons will be heavily tailored
    to our training data. This makes our network perform very poorly when we try to
    test it with data it has not seen before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is where deep learning comes into play. Deep learning can be categorized
    by these key features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The hierarchical composition of layers**: Rather than having only fully connected
    layers in a network, we can create and combine multiple different layers, consisting
    of non-linear and linear transformations. These different layers play a role in
    extracting key features in the data that would be otherwise difficult to find
    in an ANN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**End-to-end learning**: The network starts with a method called feature extraction.
    It looks at the data and finds a way to group redundant information and identify
    the important features of the data. The network then uses these features to train
    and predict or classify using fully connected layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A distributed representation of neurons**: With feature extraction, the network
    can group neurons to encode a bigger feature of the data. Unlike in an ANN, no
    single neuron encodes everything. This allows the model to reduce the number of
    parameters it has to learn while still retaining the key elements in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning is prevalent in computer vision. Due to the advances in the technology
    of capturing photos and videos, it has become very difficult for ANNs to learn
    and perform well when it comes to image detection. For starters, when we use an
    image to train our model, we have to look at every pixel in an image as an input
    to the model. So, for an image of resolution 256x256, we would be looking at over
    65,000 input parameters. Depending on the number of neurons in your fully connected
    layer, you could be looking at millions of parameters. With the sheer number of
    parameters, this will be bound to cause overfitting and could take days of training.
  prefs: []
  type: TYPE_NORMAL
- en: With deep learning, we can create a group of layers called **Convolutional Neural
    Networks** (**CNNs**). These layers are responsible for reducing the number of
    parameters that we have to learn in our model while still retaining the key features
    in our data. With these additions, we can learn how to extract certain features
    and use those to train our model to predict with efficiency and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: A CNN](img/B18934_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: A CNN'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be looking at the different Python libraries used
    for machine learning and deep learning and their different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Common deep learning and machine learning libraries used in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gone over the concepts of artificial intelligence and machine
    learning, we can start looking at the programming aspect of implementing these
    concepts. Many programming languages are used today when it comes to creating
    machine learning models. Commonly used are MATLAB, R, and Python. Among them,
    Python has grown to be the most popular programming language in machine learning
    due to its versatility as a programming language and the extensive number of libraries,
    which makes creating machine learning models easier. In this section, we will
    be going over the most commonly used libraries today.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NumPy is an essential package when it comes to building machine learning models
    in Python. You will be mostly working with large, multi-dimensional matrices when
    building your models. Most of the effort is spent on transforming, splicing, and
    performing advanced mathematical operations on matrices, and NumPy provides the
    tools need to perform these actions while retaining speed and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the different APIs that NumPy offers, you can visit
    the documentation on its website: https://numpy.org/doc/stable/reference/index.html.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will look at the example code. This section shows us how we can initialize
    a NumPy array. In this example, we will create a 3x3 matrix with initialized values
    of 1 through 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will print out the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can show how we can splice and extract certain elements from our array.
  prefs: []
  type: TYPE_NORMAL
- en: 'This line of code allows us to pull all the values that are in the second column
    of our array. Keep in mind that in NumPy our arrays and lists are zero-indexed,
    meaning that the zero index refers to the first element in the array or list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we extract all of the values in the row of `2` in our array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Another useful aspect of NumPy arrays is that we can apply mathematical functions
    to our matrices without having to implement code to perform basic functions. Not
    only is this much easier but it also is much faster and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we simply perform a multiplication between our matrix and
    a scalar value of `-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Matplotlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to see how your model is learning and performing, it is important to
    be able to visualize your results and your data. Matplotlib offers a simple way
    to graph your data, from something as simple as a line plot to more advanced plots,
    such as contour plots and 3D plots. What makes this library so popular is its
    seamlessness when working with NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on their different functions, you can visit their website:
    [https://matplotlib.org/stable/index.html](https://matplotlib.org/stable/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will create a simple line graph. We first initialize two
    arrays, `x` and `y`, and both arrays will contain values from 0 to 9\. Then, using
    Matplotlib’s APIs, we can plot and show our simple graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.4: A simple line graph using Matplotlib](img/B18934_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: A simple line graph using Matplotlib'
  prefs: []
  type: TYPE_NORMAL
- en: Pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the recent trend of storing data in CSV files, Pandas has become a staple
    in the Python community due to its ease and versatility. Pandas is commonly used
    for data analysis. It stores the data in a tabular format, and it provides users
    with simple functions to pre-process and manipulate the data to fit their needs.
    It has also become useful when dealing with time-series data, which is helpful
    when building forecasting models.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the different functions, you can view the documentation
    on its website: [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, first, we will simply initialize a DataFrame. This is the
    data structure used to store our data in a two-dimensional tabular format in Pandas.
    Usually, we store the data from the files we read from, but it is also possible
    to create a DataFrame with your own data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.5: Output of our DataFrame](img/B18934_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Output of our DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with NumPy, we can extract certain columns and rows of our DataFrame. In
    this code, we can view the first row of our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.6: Output of the first row of our DataFrame](img/B18934_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Output of the first row of our DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'With Pandas, we can also extract certain columns from our DataFrame by using
    the name of the column rather than the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.7: Output of all the values in the Price column](img/B18934_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Output of all the values in the Price column'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorFlow and Keras are the foundation when it comes to building deep learning
    models. While both can be used individually, Keras is used as an interface for
    the TensorFlow framework, allowing users to easily create powerful deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow, created by Google, functions as the backend when creating machine
    learning models. It works by creating static data flow graphs that specify how
    the data moves through the deep learning pipeline. The graph contains nodes and
    edges, where the nodes represent mathematical operations. It passes this data
    using multidimensional arrays known as Tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Keras, later to be integrated with TensorFlow, can be viewed as the frontend
    for designing deep learning models. It was implemented to be user-friendly by
    allowing users to focus on designing their neural network models without having
    to deal with a complicated backend. It is similar to object-oriented programming,
    as it replicates the style of creating objects. Users can freely add different
    types of layers, activation functions, and more. They can even use prebuilt neural
    networks for easy training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example code, we can see how we can create a simple, hidden
    two-layer neural network. This block of code allows us to initialize a `Sequential`
    model, which consists of a simple stack of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on our application, we can add multiple layers with different configurations,
    such as the number of nodes, the activation functions, and the kernel regularizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can compile our model, which essentially gathers all the different
    layers and combines them into one simple neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PyTorch is another machine learning framework created by Meta, formally known
    as Facebook. Much like Keras/TensorFlow, it allows the users to create machine
    learning models. The framework is well suited to **Natural Language Processing**
    (**NLP**) and computer vision problems but can be tailored to most applications.
    What makes PyTorch unique is its dynamic computational graph. It has a module
    called Autograd, which allows you to perform automatic differentiation dynamically,
    compared to TensorFlow, in which it is static. Also, PyTorch is more in line with
    the Python language, which makes it easier to understand and takes advantage of
    useful features of Python such as parallel programming. For more information,
    visit the documentation on their website: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section of code, we can create a simple single-layer neural network.
    Similar to Keras, we can initialize a `Sequential` model and add layers depending
    on our needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: SciPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This library is designed for scientific computing. There are many built-in
    functions and methods used for linear algebra, optimization, and integration,
    which are commonly used in machine learning. This library is useful when trying
    to compute certain statistics and transformations as you build your machine learning
    model. For more information on the different functions it provides, view the documentation
    on its website: [https://docs.scipy.org/doc/scipy/](https://docs.scipy.org/doc/scipy/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example code, we can create a 3x3 array using NumPy and then we can
    use SciPy to calculate the determinate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'scikit-learn is a machine learning library that is an extension of SciPy and
    is built using NumPy and Matplotlib. It contains many prebuilt machine learning
    models, such as random forests, K-means, and support vector machines. For more
    information on the different APIs it provides, view the documentation by visiting
    its website: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will use an example dataset provided by scikit-learn
    and build a simple logistic regression model. First, we import all the required
    libraries and then load the Iris dataset provided by scikit-learn. We can use
    a handy API from scikit-learn to split our data into training and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then initialize our logistic regression model and simply run the `fit`
    function with our training data to train the model. Once we train our model, we
    can use it to make predictions and then measure its accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the next few sections, we will start looking at the different models we can
    build using these libraries. We will understand what makes these models unique,
    how they are structured, and for what purposes and applications they can best
    serve our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Building regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will look at regression models. Regression models or regression analysis
    are modeling techniques used to find the relationship between independent and
    dependent variables. The output of a regression model is typically a continuous
    value, also known as a quantitative variable. Some common examples are predicting
    the price of a home based on its features or predicting the sales of a certain
    product in a new store based on previous sales information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before building a regression model, we must first understand the data and how
    it is structured. The majority of regression models involve supervised learning.
    This consists of features and an output variable, known as a label. This will
    help the model by adjusting the weights to better fit the data we have observed
    so far. We usually denote our features as X and our labels as Y to help us understand
    the mathematical models used to solve regression models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Example of a supervised learning data structure](img/B18934_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Example of a supervised learning data structure'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, our data is split into two subsets, **training** and **testing**
    sets. The training dataset usually consists of between 70-80% of the original
    data and the testing dataset contains the rest. This is to allow the model to
    learn on the training dataset and validate its result on the testing dataset to
    show its performance. From the results, we can infer how our model is performing
    on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a linear regression model to perform effectively, our data must be structured
    linearly. The model uses this formula to train on and learn about the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, ![](img/Formula_04_002.png) represents the output of the
    model, or what we usually call the prediction. The prediction is calculated by
    taking the intercept ![](img/Formula_04_003.png) and the slope ![](img/Formula_04_004.png).
    The slope, which is also referred to as the weight, is applied to all the features
    in the data, which represents ![](img/Formula_04_005.png). When working with the
    data, we usually represent it as a matrix, which makes it easy to understand and
    easy to work with when using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_006.jpg)![](img/Formula_04_007.jpg)![](img/Formula_04_008.jpg)![](img/Formula_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of features describes what type of problem you are solving. If your
    data only has one feature, it is considered a simple linear regression model.
    While it can solve straightforward problems, for more advanced data and problems,
    it can be difficult to map relationships. Therefore, you can create a multiple
    linear regression model by adding more features (![](img/Formula_04_010.png)).
    This allows the model to be more robust and find deeper relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: A simple linear regression model](img/B18934_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: A simple linear regression model'
  prefs: []
  type: TYPE_NORMAL
- en: Once we train our model, we need to learn how to evaluate our model and understand
    how it performs against the test data. When it comes to linear regression, the
    two common metrics we use to assess our model are the **Root Mean Square Error**
    (**RMSE**) and the **R**2 metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RMSE is the standard deviation of the **residual** errors across the predictions.
    The residual is the measure of the distance from the actual data points to the
    regression line. The further the average distance of all the points is from the
    line, the higher the error is. This indicates a weak model, as it’s unable to
    find the correlation between the data points. This metric can be calculated by
    using this formula where ![](img/Formula_04_011.png) is the actual value, ![](img/Formula_04_012.png)
    is the predicted value, and ![](img/Formula_04_013.png) is the number of data
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_014.jpg)![Figure 4.10: Calculating the residual of a linear
    regression model](img/B18934_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Calculating the residual of a linear regression model'
  prefs: []
  type: TYPE_NORMAL
- en: 'R2, also known as the coefficient of determination, measures the proportion
    of variance in the dependent variables (Y) that can be explained by the independent
    variables (X). It essentially tells us how well the data fits the model. Unlike
    the RMSE, which can be an arbitrary number, R2 is given as a percentage, which
    can be easier to understand. The higher the percentage, the better the correlation
    of data. Although useful, a higher percentage is not always indicative of a strong
    model. What determines a good R2 value depends on the application and how the
    user understands the data. R2 can be calculated by using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Many more metrics can evaluate the effectiveness of your regression model, but
    these two are more than enough to get an understanding of how your model is performing.
    When building and evaluating your model, it is important to plot and visualize
    your data and model, as this can identify key points. The plots can help you determine
    whether your model is **overfitting** or **underfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting occurs when your model is too suited to your training data. Your
    RMSE will be really low, and you will have a training accuracy of almost 100%.
    While this seems tempting, it is an indication of a poor model. This can be caused
    by one of two things: not enough data or too many parameters. As a result, when
    you test your model on new data it has not seen before, it will perform very poorly
    due to it not being able to generalize the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: An overfitted linear regression model](img/B18934_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: An overfitted linear regression model'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address overfitting, you can try to increase the amount of training data,
    or make the model less complex. It also helps to randomly shuffle your data before
    you split it into the training and testing set. Another important technique is
    called **regularization**. While there are many different regularization techniques
    (L1 or L2 regularization) depending on the model, they all work similarly in that
    they add **bias** or noise into the model to prevent overfitting. In the regression
    equation we previously saw, we can add another term, ![](img/Formula_04_016.png),
    to show that regularization is being applied to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other end, underfitting occurs when your model is unable to find any
    meaningful correlation within the data. This is not as common as overfitting since
    it is easy to find patterns in most data. If this occurs, either your data has
    too much noise and is severely uncorrelated, your model is too simple and doesn’t
    have enough parameters, or the model is not effective for the application at hand.
    It is also useful to debug your code and make sure there are no bugs when it comes
    to preprocessing your data or setting up your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: An under-fitted linear regression model](img/B18934_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: An under-fitted linear regression model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the goal is to find the best-fitting model, between an overfit and
    an underfit. It takes time and experimentation to find a model that works for
    your needs, but using the key indicators and metrics discussed here can help guide
    you in the right direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: A best-fitting linear regression model](img/B18934_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: A best-fitting linear regression model'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is a critical part of building a comprehensive model. Understanding
    your data can help determine which features or parameters to include in your model
    so that you can capture the relationship between the independent and dependent
    variables without causing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some key notes to keep in mind when collecting and working with the
    data for your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalize your data**: It is possible to have features with very high or
    low numbers, so to prevent them from overwhelming the model and creating biases,
    it is imperative to normalize all your data to make it uniform across the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clean your data**: In the real world, the data we collect isn’t always perfect
    and can contain missing or egregious data. It is important to deal with these
    issues because they can cause outliers and impact the model negatively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understand the data**: It is a common practice to perform statistical analysis,
    also known as **Exploratory Data Analysis** (**EDA**), on your data to get a better
    understanding of how the data can impact your model. This can include plotting
    graphs, running statical methods, and even using machine learning techniques to
    reduce the dimensionality of the data, which will be discussed later in the chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will discuss classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Building multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike regression models that produce a continuous output, models are considered
    classification models when they produce a finite output. Some examples include
    email spam detection, image classification, and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models are considered versatile since they can apply to both
    supervised and unsupervised learning while regression models are mostly used for
    supervised learning. There are some regression models (such as logistic regression
    and support vector machine) that are also considered classification models since
    they use a threshold to split the output of continuous values into different categories.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is a common application used in today’s market. Although
    supervised learning usually performs better and provides meaningful results since
    we know the expected output, the majority of the data we collect is unlabeled.
    It costs companies time and money for human experts to sift through the data and
    label it. Unsupervised learning helps reduce the cost and time by getting the
    model to try and determine the labels for the data and extract meaningful information.
    They can even perform better than humans sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of categories in the output of a classification model determines
    what type of model it is. For models with only two outputs (i.e., spam and not
    spam), this is called a binary classifier, while models with more than two outputs
    are called multiclass classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14: Binary and multiclass classifiers](img/B18934_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Binary and multiclass classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: 'From those classifiers, there are two types of learners: **lazy learners**
    and **eager learners**.'
  prefs: []
  type: TYPE_NORMAL
- en: Lazy learners essentially store the training data and wait until they receive
    new test data. Once they get the test data, the model classifies the new data
    based on the already existing data. These types of learners take less time when
    training since you can continuously add new data without having to retrain the
    entire model, but take longer when performing classification since they have to
    go through all the data points. One common type of lazy learner is the **K-Nearest
    Neighbors** (**KNN**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, eager learners work in the opposite way. Whenever new data
    is added to the model, they have to retrain the model again. Although this takes
    more time compared to lazy learners, querying the model is much faster since they
    don’t have to go through all the data points. Some examples of eager learners
    are decision trees, naïve Bayes, and ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning will generally perform better than unsupervised learning
    since we know what the expected output should be during training, but it is costly
    to have to collect and label the data, so unsupervised learning excels in this
    area of training on unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will be looking at a few niche models that can
    be used for unique problems that most basic classification or regression models
    can’t solve.
  prefs: []
  type: TYPE_NORMAL
- en: Text sentiment analysis and topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular field in the machine learning field is topic modeling and text analysis.
    With a plethora of text on the internet, being able to understand that data and
    create complex models such as chatbots and translation services has become a hot
    topic. Interacting with human language using software is called **NLP**.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the amount of data we can use to train our models, it is a difficult
    task to create meaningful models. Language itself is complex and contains many
    grammar rules, especially when trying to translate between languages. Certain
    powerful techniques can help us when creating NLP models though.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before implementing any NLP models, it is imperative to preprocess the data
    in some way. Documents and text tend to contain extraneous data, such as stopping
    words (the/a/and) or random characters, which can affect the model and produce
    flawed results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first idea we will discuss is **topic modeling**. This is the process of
    grouping text or words from documents into different topics or fields. This is
    useful when you have a document or text and want to classify and group it into
    a certain genre without having to go through the tedious process of reading documents
    one by one. There are many different models used for topic modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Semantic** **Analysis** (**LSA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probabilistic Latent Semantic** **Analysis** (**PLSA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet** **Allocation** (**LDA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will focus on LDA. LDA uses statistics to find patterns and repeated occurrences
    of words or phrases and groups them into their topics. It assumes that each document
    contains a mixture of topics and that each topic contains a mixture of words.
    LDA first starts the process of going through the documents and keeping a word
    matrix, where it contains the count of each word in each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: A word matrix](img/B18934_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: A word matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a word matrix, we determine the number of topics, ![](img/Formula_04_018.png),
    that we want to split up the words into and use statistics to find the probability
    of the words belonging to a certain topic. Using Bayesian statistics, we can then
    calculate the probability and use that to cluster the words into different topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16: An LDA model](img/B18934_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: An LDA model'
  prefs: []
  type: TYPE_NORMAL
- en: Another rising application in NLP is **sentiment analysis**. This involves the
    process of taking words or text and understanding the user’s intent or emotion.
    This is common today when dealing with online reviews or social media posts. It
    determines whether a piece of text contains positive, neutral, or negative emotions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many different methods and models can solve this problem. The simplest approach
    is through statistics by using Bayes’ theorem. This formula is used for predictive
    analysis, as it uses previous words in a text to update the model. The probability
    can be calculated using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning has become a powerful tool for NLP and can be useful for sentiment
    analysis. CNNs and **Recurrent Neural Networks** (**RNNs**) are two types of deep
    learning models that can drastically improve models for NLP, especially for sentiment
    analysis. We will discuss these neural networks and how they perform more later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pattern analysis and forecasting in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the uncertainty of time, being able to predict certain trends and patterns
    has become a hot topic in today’s industry. Most regression models, while powerful,
    are not able to make confident time predictions. As a result, some researchers
    have devised models that take time into consideration when making certain predictions,
    such as gas prices, stock market, and sales forecasting. Before we go into the
    different models, we must first understand the different concepts in time-series
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step when dealing with time-series problems is familiarizing yourself
    with the data. The data usually contains one of four data components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trend** – The data follows an increasing or decreasing continuous timeline
    and there are no periodic changes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality** – The data changes in a set periodic timeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cyclical** – The data changes but there is no set periodic timeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irregular** – The data changes randomly with no pattern'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.17: Different components of time-series data](img/B18934_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Different components of time-series data'
  prefs: []
  type: TYPE_NORMAL
- en: 'These different trends can be split into two different data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationary** – Certain attributes of the data, such as mean, variance, and
    covariance, do not change over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-stationary** – Attributes of the data change over time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Often, you will work with non-stationary data, and creating machine learning
    models using this type of data will generate unreliable results. To resolve this
    issue, we use certain techniques to change our data into stationary data. They
    include the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Differencing** – A mathematical method used to normalize the mean and remove
    the variance. It can be calculated by using this formula:![](img/Formula_04_020.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation** – Mathematical methods are used to remove the change in
    variance. Among the transformations, these are three commonly used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log transform
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Square root
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Power transform
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.18: Differencing non-stationary data](img/B18934_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Differencing non-stationary data'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Time is already uncertain, and this makes it almost impossible to create a model
    that can confidently predict future trends. The more we can remove uncertainty
    in our data, the better our model can find the relationships in our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we can transform our data, we can start looking at models to help us with
    forecasting. Of the different models, the most popular model for time-series analysis
    is an **Auto-Regressive Integrated Moving Average** (**ARIMA**) model. This linear
    regression model consists of three subcomponents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Auto-Regression** (**AR**) – A regression model that uses the dependencies
    of the current time and previous time to make predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated** (**I**) – The process of differencing in order to make the data
    stationary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving Average** (**MA**) – Models between the expected data and the residual
    error by calculating the MA of the lagged observed data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with ARIMA, other machine learning models can be used for time-series
    problems. Another well-known model is the RNN model. This is a type of deep learning
    model used for data that has some sort of sequence. We will be going into more
    detail on how they work in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing models using deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the chapter, we briefly discussed deep learning and the advantages
    it brings when enhancing simple machine learning models. In this section, we will
    go into more information on the different deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can build our model, we will briefly go over the structure of the
    deep learning models. A simple ANN model usually contains about two to three fully
    connected layers and is usually strong enough to model most complex linear functions,
    but as we add more layers, the improvement to the model significantly diminishes,
    and it is unable to perform more complex applications due to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning allows us to add multiple hidden layers to our ANN while reducing
    our time to train the model and increasing the performance. We can do this by
    adding one or both types of hidden layers common in deep learning – a CNN or RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN is mostly applied in the image detection and video recognition field
    due to how the neural network is structured. The CNN architecture comprises the
    following key features:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The convolutional layer is the core element in the CNN model. Its primary task
    is to convolve or group sections of the data using a **kernel** and produces an
    output called a **feature map**. This map contains all the key features extracted
    from the data, which can then be used for training in the fully connected layer.
    Each element in the feature map indicates a receptive field, which is used to
    denote which part of the input is used to map to the output. As you add more convolutional
    layers, you can extract more features, and this allows your model to adapt to
    more complex models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19: Convolutional layer output](img/B18934_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Convolutional layer output'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.19*, we can see how the feature map is created in the convolutional
    layer. The layer slides the kernel across the input data, performs the dot operation,
    and produces an output matrix, which is the result of the convolution function.
    The size of the kernel and the step size can dictate the output size of the feature
    map. In this example, we use a kernel size of 2x2 and a stride or step size of
    2, which gives us a feature map of size 2x2 based on our input size. The output
    of the feature map can then be used for more future convolutional layers depending
    on the requirements of the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we use our newly created feature map as an input for another convolutional
    or fully connected layer, we pass the feature map through an activation layer.
    It is important to pass your data through some type of nonlinear function during
    the training process, as this allows your model to map to more complex functions.
    Many different types of activation functions can be used throughout your model
    and have their benefits depending on the type of model you are planning to build.
    Among the many activation functions, these are the most commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear** **Activation** (**ReLU**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic (sigmoid)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Hyperbolic** **Tangent** (**Tanh**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ReLU function is the most popular activation function used today. It is
    very simple and helps the model learn and converge more quickly than most other
    activation functions. It is calculated using this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_021.jpg)![](img/B18934_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic function is another commonly used activation function. This is
    the same function used in logistical regression models. This function helps bound
    the output of the feature map between 0 and 1\. While useful, this function is
    computationally heavy and may slow down the training process. It is calculated
    using this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_022.jpg)![](img/B18934_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Tanh function is similar to the sigmoid function in that it bounds the
    values from the feature map. Rather than bounding it from 0 to 1, it bounds the
    values from -1 to 1, and it usually performs better than a sigmoid function. The
    function is calculated using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_023.jpg)![](img/B18934_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Tanh function'
  prefs: []
  type: TYPE_NORMAL
- en: Each activation has its uses and benefits depending on the task or model at
    hand. The ReLU function is commonly used in CNN models while sigmoid and Tanh
    are mostly found in RNN models, but they can be used interchangeably and bring
    different results.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we run our feature map through an activation layer, we come to the final
    piece – the pooling layer. As mentioned before, a key element in deep learning
    is the reduction of parameters. This allows the model to train on fewer parameters
    while still retaining the important features extracted from our convolutional
    layers. The pooling layer is responsible for this step of downsizing our parameter
    size. There are many common pooling functions but the most commonly used is max
    pooling. This is similar to the convolutional layer, where we use a kernel or
    filter to slide through our input data and only take the maximum value from each
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23: Max pooling layer output](img/B18934_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: Max pooling layer output'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.23*, we are using a kernel size of 2x2 with a stride size of
    2\. Here, we can see our output where only the maximum value from each window
    is selected.
  prefs: []
  type: TYPE_NORMAL
- en: Other layers and functions can be added to the model to help address certain
    issues and applications, such as a batch normalization layer, but with these three
    foundational layers, we can add multiple layers of different sizes and still build
    a powerful model.
  prefs: []
  type: TYPE_NORMAL
- en: In the final layer, we feed our output into the fully connected layer. By that
    time, we are able to extract the important features of the data and still learn
    more complex models with less time to train as compared to a simple ANN model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will go over the RNN architecture. Due to the nature of how the RNN
    model is structured, it is designed for tasks that need to take into consideration
    a set of sequence data, in which the data later in the sequence is dependent on
    earlier data. This model is commonly used for certain fields such as NLP and signal
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of an RNN are built by having a hidden layer in which the output
    of the layer is fed back into the same hidden layer. This way, the model is able
    to learn based on previous data and adjust the weights accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24: RNN architecture](img/B18934_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: RNN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how the model works, you can envision a single layer for
    each data point. Each layer takes in the data point as an input ![](img/Formula_04_024.png)
    and produces an output ![](img/Formula_04_025.png). We then transfer the weights
    between the layers and then take the total average of all the cost functions from
    all the layers in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25: An unraveled RNN](img/B18934_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: An unraveled RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'This model works for most simple problems. As the sequence increases, it encounters
    a few issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing gradient**: This occurs during the training process when the gradient
    approaches zero. The weights aren’t updated properly as a result and the model
    performs poorly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of context**: The model is unidirectional and cannot look further or
    previously into the data. Therefore, the model is only able to predict based on
    data around the current sequence point and is more likely to make a poor prediction
    based on incorrect context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are different variations of RNNs created to address some of the issues
    mentioned here. Among them, the most common one used today is the **Long Short-Term
    Memory** (**LSTM**) model. The LSTM model comprise three components:'
  prefs: []
  type: TYPE_NORMAL
- en: An input gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A forget gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.26: An LSTM neural network](img/B18934_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.26: An LSTM neural network'
  prefs: []
  type: TYPE_NORMAL
- en: These gates work by regulating which data points are needed to contextualize
    the sequence. That way, the model can predict more accurately without being easily
    manipulated.
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate is specifically responsible for removing previous data or context
    that is no longer needed. This gate uses the sigmoid function to determine whether
    it uses or “forgets” the data.
  prefs: []
  type: TYPE_NORMAL
- en: The input gate is used to determine whether the new data is relevant to the
    current sequence or not. This is so that only important data is being used to
    train the model and not redundant or irrelevant information.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the output gate’s primary function is to filter the current state’s
    information and only send relevant information to the next state. As with the
    other gates, it uses the context from previous states to apply a filter, which
    helps the model properly contextualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: CNN and RNN models are mostly designed for supervised learning problems. When
    it comes to unsupervised learning, different models are needed to solve certain
    problems. Let’s discuss **autoencoders**.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders work by taking the input data, compressing it, and then reconstructing
    it by decompressing it. While straightforward, it can be used for some advanced
    applications, such as generating audio or images, or it can be used as an anomaly
    detector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The autoencoder comprises two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: An encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.27: The components of an autoencoder](img/B18934_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.27: The components of an autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder and decoder are usually built with a one-layer ANN. The encoder
    is responsible for taking the data and compressing or flattening the data. Then,
    the decoder works on taking the flattened data and trying to reconstruct the input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer in the middle of the encode and decoder is usually referred
    to as the bottleneck. The number of nodes in the hidden layer must be less than
    those in the encoder and decoder. This forces the model to try and find the pattern
    or representation in the input data so that it can reconstruct the data with little
    information. Thus, the cost function is there to calculate and minimize the difference
    between the input and output data.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect of autoencoders that is an integral part of deep learning is **dimensionality
    reduction**. This is the process of reducing the number of parameters or features
    used when training your model. As mentioned earlier in this chapter, to build
    a complex model that can build a deeper representation of the data, it is important
    to include more features. However, adding too many features can lead to overfitting,
    so how do we find the best number of features to use in our model?
  prefs: []
  type: TYPE_NORMAL
- en: There are many models and techniques, such as autoencoders, that can perform
    dimensionality reduction to help us find the best features to use in our model.
    Among the different techniques, **Principle Component Analysis** (**PCA**) is
    the most popular. This technique can take an N-dimensional dataset and reduce
    the number of dimensions in the data using linear algebra. It is a common practice
    to use a dimensionality reduction technique before using your data to train your
    model, as this can help to remove noise in the data and avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed what is considered artificial intelligence and
    the different sub-fields that it contains.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the different characteristics of regression and classification
    models. From there, we also went over the structure of our data and how the model
    performs when training over our data. We then discussed the different ways of
    analyzing our model’s performance and how to address the different issues that
    we can come across when training our model.
  prefs: []
  type: TYPE_NORMAL
- en: We briefly viewed the different packages and libraries that are used today in
    machine learning models and their different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We also analyzed different topics such as topic modeling and time-series analysis
    and what they entail. With that, we were able to look at the different methods
    and techniques used to solve those types of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we went into deep learning and the different ways it improves on machine
    learning. We went over the two different types of neural networks – CNNs and RNNs
    – how they are structured, and their benefits and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take what we have learned and start looking into
    how we can design and build an end-to-end machine learning system and the different
    components that it contains.
  prefs: []
  type: TYPE_NORMAL
