- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Key Machine Learning Algorithms and Concepts
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键机器学习算法和概念
- en: In the previous chapters, we explored the different concepts of MSA and the
    role it plays when creating enterprise systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了MSA的不同概念及其在创建企业系统时所起的作用。
- en: In the coming chapters, we will begin to shift our focus from learning about
    MSA concepts to learning about key machine learning concepts. We will also learn
    about the different libraries and packages being used in machine learning models
    using Python.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将开始将我们的关注点从学习MSA概念转移到学习关键机器学习概念。我们还将学习使用Python在机器学习模型中使用到的不同库和包。
- en: 'We will cover the following areas in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: The differences between **artificial intelligence**, **machine learning**, and
    **deep learning**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能、机器学习和深度学习之间的区别
- en: Common deep learning packages and libraries used in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中常用的常见深度学习包和库
- en: Building **regression** models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建回归**模型**
- en: Building multiclass **classification**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建多类**分类**
- en: Text sentiment analysis and topic modeling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本情感分析和主题建模
- en: Pattern analysis and forecasting using machine learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习进行模式分析和预测
- en: Building enhanced models using deep learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习构建增强模型
- en: The differences between artificial intelligence, machine learning, and deep
    learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能、机器学习和深度学习之间的区别
- en: Despite the recent rise in popularity of artificial intelligence and machine
    learning, the field of artificial intelligence has been around since the 1960s.
    With different sub-fields emerging, it is important to be able to differentiate
    between them and understand them and what they entail.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工智能和机器学习在最近几年越来越受欢迎，但人工智能领域自20世纪60年代以来就已经存在。随着不同子领域的出现，能够区分它们并理解它们及其所包含的内容变得很重要。
- en: To start, artificial intelligence is the overarching field that encompasses
    all the sub-fields we see today, such as machine learning, deep learning, and
    more. Any system that perceives or receives information from its environment and
    carries out an action to maximize the reward or achieve its goal is considered
    to be an artificially intelligent machine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，人工智能是涵盖我们今天看到的所有子领域的总领域，例如机器学习、深度学习等。任何从其环境中感知或接收信息并执行动作以最大化奖励或实现其目标的系统都被认为是人工智能机器。
- en: This is commonly used today when it comes to robotics. Most of our machines
    are designed so that they can capture data using their sensors, such as cameras,
    sonars, or gyroscopes, and use the data captured to respond to a particular task
    most efficiently. This concept is very similar to how humans function. We use
    our senses to “capture” information from our environment and based on the information
    we receive, we carry out certain actions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这在今天与机器人技术相关时非常常见。我们的大多数机器被设计成能够使用它们的传感器，如摄像头、声纳或陀螺仪来捕获数据，并使用捕获到的数据以最高效的方式响应特定任务。这个概念与人类的功能非常相似。我们使用我们的感官来“捕获”来自环境的信息，并根据我们接收到的信息执行某些动作。
- en: Artificial intelligence is an expansive field, but it can be broken into different
    sub-fields, one we commonly know today as machine learning. What makes machine
    learning unique is that this field works on creating systems or machines that
    can continually learn and improve their model without explicitly being programmed.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是一个广泛的领域，但它可以被划分为不同的子领域，我们今天普遍知道的一个子领域是机器学习。使机器学习独特的是，这个领域致力于创建可以持续学习和改进其模型而无需明确编程的系统或机器。
- en: Machine learning does this by collecting data, also known as training data,
    and trying to find patterns in the data to make accurate predictions without being
    programmed to do so. There are many different methods used in machine learning
    to learn the data and the methods are tailored to the different problems we encounter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通过收集数据，也称为训练数据，并试图在数据中找到模式以进行准确预测，而不需要被编程来做这样的事情来实现这一点。机器学习中使用了许多不同的方法来学习数据，这些方法针对我们遇到的不同问题进行了定制。
- en: '![Figure 4.1: Different fields in artificial intelligence](img/B18934_04_1.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1：人工智能的不同领域](img/B18934_04_1.jpg)'
- en: 'Figure 4.1: Different fields in artificial intelligence'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：人工智能的不同领域
- en: 'Machine learning problems can be broken down into three different tasks: **supervised
    learning**, **unsupervised learning**, and **reinforcement learning**. For now,
    we will focus on supervised and unsupervised learning. This distinction is based
    on the training data that we have. Supervised learning is when we have the input
    data and the expected output for the particular set of data, which is also called
    the label. Unsupervised learning, on the other hand, only consists of the input
    without an expected output.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题可以分为三个不同的任务：**监督学习**、**无监督学习**和**强化学习**。现在，我们将专注于监督学习和无监督学习。这种区分基于我们拥有的训练数据。监督学习是我们有特定数据集的输入数据和预期输出时的情况，这也被称为标签。另一方面，无监督学习只包含输入而没有预期输出。
- en: Supervised learning works by understanding the relationship between the input
    and output data. One common example of supervised learning is predicting the price
    of a home in a certain city. We can collect data on existing homes by capturing
    their specifications and their current prices and then learn the pattern between
    the characteristics of these homes and their prices. We can then take a home,
    not in our training set, and test our model by inputting the features of the house
    into our program and have the model predict the price of the home.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通过理解输入和输出数据之间的关系来工作。监督学习的一个常见例子是预测某个城市的房价。我们可以通过捕捉现有房屋的规格和它们当前的价格来收集现有房屋的数据，然后学习这些房屋的特征和它们价格之间的模式。然后我们可以取一个不在我们的训练集中的房屋，通过将房屋的特征输入到我们的程序中来测试我们的模型，并让模型预测该房屋的价格。
- en: Unsupervised learning works by learning about the structure of the data either
    using grouping or clustering methods. This method is commonly used for marketing
    purposes. For example, a store wants to cluster its customers into different groups
    so that it can efficiently tailor its products to different demographics. It can
    capture the purchase history of its customers, use that data to learn about purchasing
    patterns, and suggest certain items or goods that would interest them, thus maximizing
    its revenue.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习通过使用分组或聚类方法来学习数据的结构。这种方法通常用于营销目的。例如，一家商店想要将其客户聚类到不同的群体中，以便它可以有效地针对不同的细分市场调整其产品。它可以捕捉其客户的购买历史，使用这些数据来学习购买模式，并建议某些可能会引起他们兴趣的项目或商品，从而最大化其收入。
- en: Before we can understand deep learning, which is a sub-field of machine learning,
    we must first understand what **Artificial Neural Networks** (**ANNs**) are. Taking
    inspiration from neurons in a brain, ANNs are models that comprise a network of
    fully connected nodes, also known as artificial neurons. They contain a set of
    inputs, hidden layers connecting the neurons, and also an output node. Each neuron
    has an input and output, which can be propagated throughout the network. In order
    to calculate the output of a neuron, we take the weighted sum of all the inputs,
    multiply it by the weight of the neuron, and then usually add a bias term.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够理解深度学习——它是机器学习的一个子领域——之前，我们首先必须了解什么是**人工神经网络**（**ANNs**）。ANNs是从大脑中的神经元得到灵感，由一组完全连接的节点组成的模型，也称为人工神经元。它们包含一组输入、连接神经元的隐藏层，以及一个输出节点。每个神经元都有一个输入和输出，这些可以通过整个网络传播。为了计算神经元的输出，我们取所有输入的加权和，乘以神经元的权重，然后通常加上一个偏差项。
- en: We continue to perform these actions until we reach the last layer, which is
    the output neuron. We perform a nonlinear activation function, such as a sigmoid
    function, to give us the final prediction. We then take the predicted output value
    and input it in a **cost function**. This function tells us how well our network
    is learning. We take this value and backpropagate through our layers back to the
    first layer, adjusting the weights of the neurons depending on how our network
    is performing. With this, we can create strong models that can perform tasks such
    as handwriting recognition, game-playing AI, and much more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续执行这些操作，直到我们达到最后一层，即输出神经元。我们执行一个非线性激活函数，例如Sigmoid函数，以给出最终的预测。然后我们将预测的输出值输入到一个**成本函数**中。这个函数告诉我们我们的网络学习得有多好。我们取这个值，并通过反向传播通过我们的层回到第一层，根据我们的网络表现调整神经元的权重。通过这种方式，我们可以创建强大的模型，可以执行诸如手写识别、游戏AI等任务。
- en: Important Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A program is considered to be a machine learning model if it can take input
    data and learn the patterns to make predictions without being explicitly programmed
    to.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个程序能够接受输入数据并学习模式以进行预测，而不需要明确编程，那么它被认为是一个机器学习模型。
- en: '![Figure 4.2: An ANN](img/B18934_04_2.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2：一个ANN](img/B18934_04_2.jpg)'
- en: 'Figure 4.2: An ANN'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：一个ANN
- en: 'While ANNs are capable of performing many tasks, there are significant downsides
    that limit their use in today’s market:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然ANNs能够执行许多任务，但它们在当今市场的使用中存在显著的缺点：
- en: It can be difficult to understand how the model performs. As you add more hidden
    layers to the network, it becomes complicated to try and debug the network.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型的性能可能很困难。随着你在网络中添加更多的隐藏层，尝试调试网络变得复杂。
- en: Training the model takes a long time, especially with copious amounts of training
    data, and can drain hardware resources, as it is difficult to perform all these
    mathematical operations on a CPU.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型需要很长时间，特别是当有大量的训练数据时，并且可能会耗尽硬件资源，因为在CPU上执行所有这些数学运算很困难。
- en: The biggest issue with ANNs is overfitting. As we add more hidden layers, there
    is a point at which the weights assigned to the neurons will be heavily tailored
    to our training data. This makes our network perform very poorly when we try to
    test it with data it has not seen before.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ANNs最大的问题是过拟合。随着我们添加更多的隐藏层，存在一个点，此时分配给神经元的权重将高度定制于我们的训练数据。这使得当尝试用之前未见过的数据测试网络时，我们的网络表现非常糟糕。
- en: 'This is where deep learning comes into play. Deep learning can be categorized
    by these key features:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度学习发挥作用的地方。深度学习可以根据以下关键特征进行分类：
- en: '**The hierarchical composition of layers**: Rather than having only fully connected
    layers in a network, we can create and combine multiple different layers, consisting
    of non-linear and linear transformations. These different layers play a role in
    extracting key features in the data that would be otherwise difficult to find
    in an ANN.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层的层次结构组成**：而不是在网络中只有全连接层，我们可以创建和组合多个不同的层，包括非线性变换和线性变换。这些不同的层在数据中提取关键特征中发挥作用，这些特征在ANN中可能很难找到。'
- en: '**End-to-end learning**: The network starts with a method called feature extraction.
    It looks at the data and finds a way to group redundant information and identify
    the important features of the data. The network then uses these features to train
    and predict or classify using fully connected layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**端到端学习**：网络从一种称为特征提取的方法开始。它查看数据，并找到一种方法来分组冗余信息并识别数据的重要特征。然后，网络使用这些特征通过全连接层进行训练和预测或分类。'
- en: '**A distributed representation of neurons**: With feature extraction, the network
    can group neurons to encode a bigger feature of the data. Unlike in an ANN, no
    single neuron encodes everything. This allows the model to reduce the number of
    parameters it has to learn while still retaining the key elements in the data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经元的分布式表示**：通过特征提取，网络可以将神经元分组以编码数据的一个更大特征。与ANN不同，没有单个神经元编码所有内容。这允许模型在保留数据中的关键元素的同时，减少它必须学习的参数数量。'
- en: Deep learning is prevalent in computer vision. Due to the advances in the technology
    of capturing photos and videos, it has become very difficult for ANNs to learn
    and perform well when it comes to image detection. For starters, when we use an
    image to train our model, we have to look at every pixel in an image as an input
    to the model. So, for an image of resolution 256x256, we would be looking at over
    65,000 input parameters. Depending on the number of neurons in your fully connected
    layer, you could be looking at millions of parameters. With the sheer number of
    parameters, this will be bound to cause overfitting and could take days of training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算机视觉中非常普遍。由于捕捉照片和视频技术的进步，当涉及到图像检测时，人工神经网络（ANNs）学习和表现良好变得非常困难。首先，当我们使用图像来训练我们的模型时，我们必须将图像中的每一个像素都视为模型的一个输入。因此，对于一个256x256分辨率的图像，我们将会查看超过65,000个输入参数。根据你全连接层中神经元的数量，你可能需要查看数百万个参数。由于参数数量庞大，这必然会导致过拟合，并且可能需要数天的训练时间。
- en: With deep learning, we can create a group of layers called **Convolutional Neural
    Networks** (**CNNs**). These layers are responsible for reducing the number of
    parameters that we have to learn in our model while still retaining the key features
    in our data. With these additions, we can learn how to extract certain features
    and use those to train our model to predict with efficiency and accuracy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: A CNN](img/B18934_04_3.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: A CNN'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be looking at the different Python libraries used
    for machine learning and deep learning and their different use cases.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Common deep learning and machine learning libraries used in Python
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gone over the concepts of artificial intelligence and machine
    learning, we can start looking at the programming aspect of implementing these
    concepts. Many programming languages are used today when it comes to creating
    machine learning models. Commonly used are MATLAB, R, and Python. Among them,
    Python has grown to be the most popular programming language in machine learning
    due to its versatility as a programming language and the extensive number of libraries,
    which makes creating machine learning models easier. In this section, we will
    be going over the most commonly used libraries today.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NumPy is an essential package when it comes to building machine learning models
    in Python. You will be mostly working with large, multi-dimensional matrices when
    building your models. Most of the effort is spent on transforming, splicing, and
    performing advanced mathematical operations on matrices, and NumPy provides the
    tools need to perform these actions while retaining speed and efficiency.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the different APIs that NumPy offers, you can visit
    the documentation on its website: https://numpy.org/doc/stable/reference/index.html.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will look at the example code. This section shows us how we can initialize
    a NumPy array. In this example, we will create a 3x3 matrix with initialized values
    of 1 through 9:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we will print out the results:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, we can show how we can splice and extract certain elements from our array.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'This line of code allows us to pull all the values that are in the second column
    of our array. Keep in mind that in NumPy our arrays and lists are zero-indexed,
    meaning that the zero index refers to the first element in the array or list:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this example, we extract all of the values in the row of `2` in our array:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Another useful aspect of NumPy arrays is that we can apply mathematical functions
    to our matrices without having to implement code to perform basic functions. Not
    only is this much easier but it also is much faster and more efficient.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we simply perform a multiplication between our matrix and
    a scalar value of `-1`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Matplotlib
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to see how your model is learning and performing, it is important to
    be able to visualize your results and your data. Matplotlib offers a simple way
    to graph your data, from something as simple as a line plot to more advanced plots,
    such as contour plots and 3D plots. What makes this library so popular is its
    seamlessness when working with NumPy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到您的模型是如何学习和表现的，能够可视化您的结果和数据非常重要。Matplotlib提供了一个简单的方法来绘制您的数据，从简单的线图到更高级的图表，如等高线图和3D图。使这个库如此受欢迎的是它与NumPy工作的无缝性。
- en: 'For more information on their different functions, you can visit their website:
    [https://matplotlib.org/stable/index.html](https://matplotlib.org/stable/index.html).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于它们不同功能的更多信息，您可以访问它们的网站：[https://matplotlib.org/stable/index.html](https://matplotlib.org/stable/index.html)。
- en: 'In this example, we will create a simple line graph. We first initialize two
    arrays, `x` and `y`, and both arrays will contain values from 0 to 9\. Then, using
    Matplotlib’s APIs, we can plot and show our simple graph:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建一个简单的线图。我们首先初始化两个数组，`x`和`y`，这两个数组都将包含从0到9的值。然后，使用Matplotlib的API，我们可以绘制并显示我们的简单图表：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 4.4: A simple line graph using Matplotlib](img/B18934_04_4.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4：使用Matplotlib的简单线图](img/B18934_04_4.jpg)'
- en: 'Figure 4.4: A simple line graph using Matplotlib'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：使用Matplotlib的简单线图
- en: Pandas
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas
- en: With the recent trend of storing data in CSV files, Pandas has become a staple
    in the Python community due to its ease and versatility. Pandas is commonly used
    for data analysis. It stores the data in a tabular format, and it provides users
    with simple functions to pre-process and manipulate the data to fit their needs.
    It has also become useful when dealing with time-series data, which is helpful
    when building forecasting models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最近将数据存储在CSV文件中的趋势，Pandas因其易用性和多功能性而成为Python社区中的必备工具。Pandas通常用于数据分析。它以表格格式存储数据，并为用户提供简单的函数来预处理和操作数据以满足他们的需求。在处理时间序列数据时，它也变得非常有用，这对于构建预测模型很有帮助。
- en: 'For more information on the different functions, you can view the documentation
    on its website: [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同函数的更多信息，您可以在其网站上查看文档：[https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)。
- en: 'In this example, first, we will simply initialize a DataFrame. This is the
    data structure used to store our data in a two-dimensional tabular format in Pandas.
    Usually, we store the data from the files we read from, but it is also possible
    to create a DataFrame with your own data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，首先，我们将简单地初始化一个DataFrame。这是在Pandas中以二维表格格式存储我们数据的数据结构。通常，我们存储从读取的文件中读取的数据，但也可以使用自己的数据创建DataFrame：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 4.5: Output of our DataFrame](img/B18934_04_5.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5：我们的DataFrame的输出](img/B18934_04_5.jpg)'
- en: 'Figure 4.5: Output of our DataFrame'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：我们的DataFrame的输出
- en: 'As with NumPy, we can extract certain columns and rows of our DataFrame. In
    this code, we can view the first row of our DataFrame:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与NumPy一样，我们可以从我们的DataFrame中提取某些列和行。在这段代码中，我们可以查看我们的DataFrame的第一行：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 4.6: Output of the first row of our DataFrame](img/B18934_04_6.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6：我们的DataFrame的第一行输出](img/B18934_04_6.jpg)'
- en: 'Figure 4.6: Output of the first row of our DataFrame'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：我们的DataFrame的第一行输出
- en: 'With Pandas, we can also extract certain columns from our DataFrame by using
    the name of the column rather than the index:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas，我们还可以通过使用列名而不是索引来从我们的DataFrame中提取某些列：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 4.7: Output of all the values in the Price column](img/B18934_04_7.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7：价格列中所有值的输出](img/B18934_04_7.jpg)'
- en: 'Figure 4.7: Output of all the values in the Price column'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：价格列中所有值的输出
- en: TensorFlow and Keras
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow和Keras
- en: TensorFlow and Keras are the foundation when it comes to building deep learning
    models. While both can be used individually, Keras is used as an interface for
    the TensorFlow framework, allowing users to easily create powerful deep learning
    models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和Keras是构建深度学习模型的基础。虽然两者都可以单独使用，但Keras被用作TensorFlow框架的接口，使用户能够轻松创建强大的深度学习模型。
- en: TensorFlow, created by Google, functions as the backend when creating machine
    learning models. It works by creating static data flow graphs that specify how
    the data moves through the deep learning pipeline. The graph contains nodes and
    edges, where the nodes represent mathematical operations. It passes this data
    using multidimensional arrays known as Tensors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由Google创建的TensorFlow在创建机器学习模型时充当后端。它通过创建静态数据流图来工作，这些图指定了数据在深度学习管道中的流动方式。图中包含节点和边，其中节点代表数学运算。它使用称为张量的多维数组传递这些数据。
- en: Keras, later to be integrated with TensorFlow, can be viewed as the frontend
    for designing deep learning models. It was implemented to be user-friendly by
    allowing users to focus on designing their neural network models without having
    to deal with a complicated backend. It is similar to object-oriented programming,
    as it replicates the style of creating objects. Users can freely add different
    types of layers, activation functions, and more. They can even use prebuilt neural
    networks for easy training and testing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Keras，后来与TensorFlow集成，可以看作是设计深度学习模型的界面。它被设计成用户友好，允许用户专注于设计他们的神经网络模型，无需处理复杂的后端。它类似于面向对象编程，因为它复制了创建对象的方式。用户可以自由添加不同类型的层、激活函数等。他们甚至可以使用预构建的神经网络进行简单的训练和测试。
- en: 'In the following example code, we can see how we can create a simple, hidden
    two-layer neural network. This block of code allows us to initialize a `Sequential`
    model, which consists of a simple stack of layers:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例代码中，我们可以看到如何创建一个简单的、隐藏的两层神经网络。这段代码允许我们初始化一个`Sequential`模型，它由简单的层堆叠组成：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Depending on our application, we can add multiple layers with different configurations,
    such as the number of nodes, the activation functions, and the kernel regularizer:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的应用，我们可以添加具有不同配置的多个层，例如节点数、激活函数和核正则化器：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we can compile our model, which essentially gathers all the different
    layers and combines them into one simple neural network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以编译我们的模型，这实际上是将所有不同的层聚集在一起，并将它们组合成一个简单的神经网络：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: PyTorch
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch
- en: 'PyTorch is another machine learning framework created by Meta, formally known
    as Facebook. Much like Keras/TensorFlow, it allows the users to create machine
    learning models. The framework is well suited to **Natural Language Processing**
    (**NLP**) and computer vision problems but can be tailored to most applications.
    What makes PyTorch unique is its dynamic computational graph. It has a module
    called Autograd, which allows you to perform automatic differentiation dynamically,
    compared to TensorFlow, in which it is static. Also, PyTorch is more in line with
    the Python language, which makes it easier to understand and takes advantage of
    useful features of Python such as parallel programming. For more information,
    visit the documentation on their website: [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是由Meta（前Facebook）创建的另一个机器学习框架。与Keras/TensorFlow类似，它允许用户创建机器学习模型。该框架非常适合自然语言处理（NLP）和计算机视觉问题，但可以定制以适应大多数应用。PyTorch的独特之处在于其动态计算图。它有一个名为Autograd的模块，允许你动态地进行自动微分，与TensorFlow中的静态微分相比。此外，PyTorch更符合Python语言，这使得它更容易理解，并利用Python的有用特性，如并行编程。有关更多信息，请访问他们网站上的文档：[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)。
- en: 'In this section of code, we can create a simple single-layer neural network.
    Similar to Keras, we can initialize a `Sequential` model and add layers depending
    on our needs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们可以创建一个简单的单层神经网络。类似于Keras，我们可以初始化一个`Sequential`模型，并根据需要添加层：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: SciPy
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SciPy
- en: 'This library is designed for scientific computing. There are many built-in
    functions and methods used for linear algebra, optimization, and integration,
    which are commonly used in machine learning. This library is useful when trying
    to compute certain statistics and transformations as you build your machine learning
    model. For more information on the different functions it provides, view the documentation
    on its website: [https://docs.scipy.org/doc/scipy/](https://docs.scipy.org/doc/scipy/).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库是为科学计算设计的。它包含许多用于线性代数、优化和积分的内置函数和方法，这些在机器学习中常用。当你构建机器学习模型时，尝试计算某些统计和转换时，这个库很有用。有关它提供的不同函数的更多信息，请查看其网站上的文档：[https://docs.scipy.org/doc/scipy/](https://docs.scipy.org/doc/scipy/)。
- en: 'In this example code, we can create a 3x3 array using NumPy and then we can
    use SciPy to calculate the determinate:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例代码中，我们可以使用NumPy创建一个3x3的数组，然后我们可以使用SciPy来计算行列式：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: scikit-learn
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn
- en: 'scikit-learn is a machine learning library that is an extension of SciPy and
    is built using NumPy and Matplotlib. It contains many prebuilt machine learning
    models, such as random forests, K-means, and support vector machines. For more
    information on the different APIs it provides, view the documentation by visiting
    its website: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn是一个机器学习库，它是SciPy的扩展，使用NumPy和Matplotlib构建。它包含许多预构建的机器学习模型，如随机森林、K-means和支持向量机。有关它提供的不同API的更多信息，请访问其网站：[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)。
- en: 'In the following example, we will use an example dataset provided by scikit-learn
    and build a simple logistic regression model. First, we import all the required
    libraries and then load the Iris dataset provided by scikit-learn. We can use
    a handy API from scikit-learn to split our data into training and test datasets:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们将使用scikit-learn提供的示例数据集来构建一个简单的逻辑回归模型。首先，我们导入所有必需的库，然后加载scikit-learn提供的Iris数据集。我们可以使用scikit-learn的一个便捷API将我们的数据分成训练集和测试集：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then initialize our logistic regression model and simply run the `fit`
    function with our training data to train the model. Once we train our model, we
    can use it to make predictions and then measure its accuracy:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以初始化我们的逻辑回归模型，并简单地使用我们的训练数据运行`fit`函数来训练模型。一旦我们训练了我们的模型，我们就可以用它来进行预测，然后测量其准确度：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next few sections, we will start looking at the different models we can
    build using these libraries. We will understand what makes these models unique,
    how they are structured, and for what purposes and applications they can best
    serve our needs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将开始探讨我们可以使用这些库构建的不同模型。我们将了解是什么使这些模型独特，它们的结构如何，以及它们可以最好地服务于哪些目的和应用程序来满足我们的需求。
- en: Building regression models
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建回归模型
- en: First, we will look at regression models. Regression models or regression analysis
    are modeling techniques used to find the relationship between independent and
    dependent variables. The output of a regression model is typically a continuous
    value, also known as a quantitative variable. Some common examples are predicting
    the price of a home based on its features or predicting the sales of a certain
    product in a new store based on previous sales information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨回归模型。回归模型或回归分析是用于寻找独立变量和依赖变量之间关系的建模技术。回归模型的输出通常是连续值，也称为定量变量。一些常见的例子是根据房屋的特征预测房价，或者根据之前的销售信息预测新商店中某种产品的销售情况。
- en: 'Before building a regression model, we must first understand the data and how
    it is structured. The majority of regression models involve supervised learning.
    This consists of features and an output variable, known as a label. This will
    help the model by adjusting the weights to better fit the data we have observed
    so far. We usually denote our features as X and our labels as Y to help us understand
    the mathematical models used to solve regression models:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建回归模型之前，我们首先必须理解数据和它的结构。大多数回归模型涉及监督学习。这包括特征和一个输出变量，称为标签。这将通过调整权重来帮助模型更好地拟合我们迄今为止观察到的数据。我们通常将我们的特征表示为X，将我们的标签表示为Y，以帮助我们理解用于解决回归模型的数学模型：
- en: '![Figure 4.8: Example of a supervised learning data structure](img/B18934_04_8.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8：监督学习数据结构的示例](img/B18934_04_8.jpg)'
- en: 'Figure 4.8: Example of a supervised learning data structure'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：监督学习数据结构的示例
- en: Typically, our data is split into two subsets, **training** and **testing**
    sets. The training dataset usually consists of between 70-80% of the original
    data and the testing dataset contains the rest. This is to allow the model to
    learn on the training dataset and validate its result on the testing dataset to
    show its performance. From the results, we can infer how our model is performing
    on the dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们的数据被分成两个子集，**训练集**和**测试集**。训练集通常包含原始数据的70-80%，而测试集包含剩余的部分。这是为了让模型在训练集上学习，并在测试集上验证其结果以展示其性能。从结果中，我们可以推断出我们的模型在数据集上的表现。
- en: 'For a linear regression model to perform effectively, our data must be structured
    linearly. The model uses this formula to train on and learn about the data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了线性回归模型能够有效运行，我们的数据必须以线性方式组织。模型使用以下公式在数据上进行训练和学习：
- en: '![](img/Formula_04_001.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![公式4.1](img/Formula_04_001.jpg)'
- en: 'In this equation, ![](img/Formula_04_002.png) represents the output of the
    model, or what we usually call the prediction. The prediction is calculated by
    taking the intercept ![](img/Formula_04_003.png) and the slope ![](img/Formula_04_004.png).
    The slope, which is also referred to as the weight, is applied to all the features
    in the data, which represents ![](img/Formula_04_005.png). When working with the
    data, we usually represent it as a matrix, which makes it easy to understand and
    easy to work with when using Python:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![公式 4.002](img/Formula_04_002.png)代表模型的输出，或者我们通常所说的预测。预测是通过取截距![公式 4.003](img/Formula_04_003.png)和斜率![公式
    4.004](img/Formula_04_004.png)来计算的。斜率，也称为权重，应用于数据中的所有特征，代表![公式 4.005](img/Formula_04_005.png)。在处理数据时，我们通常将其表示为矩阵，这使得它易于理解，并且在使用Python时易于操作：
- en: '![](img/Formula_04_006.jpg)![](img/Formula_04_007.jpg)![](img/Formula_04_008.jpg)![](img/Formula_04_009.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![公式 4.06](img/Formula_04_006.jpg)![公式 4.07](img/Formula_04_007.jpg)![公式 4.08](img/Formula_04_008.jpg)![公式
    4.09](img/Formula_04_009.jpg)'
- en: The number of features describes what type of problem you are solving. If your
    data only has one feature, it is considered a simple linear regression model.
    While it can solve straightforward problems, for more advanced data and problems,
    it can be difficult to map relationships. Therefore, you can create a multiple
    linear regression model by adding more features (![](img/Formula_04_010.png)).
    This allows the model to be more robust and find deeper relationships.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 特征数量描述了你正在解决什么类型的问题。如果你的数据只有一个特征，它被认为是一个简单的线性回归模型。虽然它可以解决直接的问题，但对于更复杂的数据和问题，映射关系可能会很困难。因此，你可以通过添加更多特征来创建一个多元线性回归模型（![公式
    4.10](img/Formula_04_010.png)）。这使得模型更加稳健，并能找到更深层次的关系。
- en: '![Figure 4.9: A simple linear regression model](img/B18934_04_9.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9：一个简单的线性回归模型](img/B18934_04_9.jpg)'
- en: 'Figure 4.9: A simple linear regression model'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：一个简单的线性回归模型
- en: Once we train our model, we need to learn how to evaluate our model and understand
    how it performs against the test data. When it comes to linear regression, the
    two common metrics we use to assess our model are the **Root Mean Square Error**
    (**RMSE**) and the **R**2 metrics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了我们的模型，我们需要学习如何评估我们的模型并理解它在测试数据上的表现。当涉及到线性回归时，我们用来评估模型的两个常用指标是**均方根误差**（**RMSE**）和**R**2指标。
- en: 'The RMSE is the standard deviation of the **residual** errors across the predictions.
    The residual is the measure of the distance from the actual data points to the
    regression line. The further the average distance of all the points is from the
    line, the higher the error is. This indicates a weak model, as it’s unable to
    find the correlation between the data points. This metric can be calculated by
    using this formula where ![](img/Formula_04_011.png) is the actual value, ![](img/Formula_04_012.png)
    is the predicted value, and ![](img/Formula_04_013.png) is the number of data
    points:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE是预测中残差误差的标准差。残差是实际数据点到回归线的距离的度量。所有点的平均距离越远，误差就越高。这表明模型较弱，因为它无法找到数据点之间的相关性。这个指标可以通过以下公式计算，其中![公式
    4.11](img/Formula_04_011.png)是实际值，![公式 4.12](img/Formula_04_012.png)是预测值，![公式
    4.13](img/Formula_04_013.png)是数据点的数量：
- en: '![](img/Formula_04_014.jpg)![Figure 4.10: Calculating the residual of a linear
    regression model](img/B18934_04_10.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![公式 4.14](img/Formula_04_014.jpg)![图 4.10：计算线性回归模型的残差](img/B18934_04_10.jpg)'
- en: 'Figure 4.10: Calculating the residual of a linear regression model'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：计算线性回归模型的残差
- en: 'R2, also known as the coefficient of determination, measures the proportion
    of variance in the dependent variables (Y) that can be explained by the independent
    variables (X). It essentially tells us how well the data fits the model. Unlike
    the RMSE, which can be an arbitrary number, R2 is given as a percentage, which
    can be easier to understand. The higher the percentage, the better the correlation
    of data. Although useful, a higher percentage is not always indicative of a strong
    model. What determines a good R2 value depends on the application and how the
    user understands the data. R2 can be calculated by using this formula:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: R2，也称为确定系数，衡量了因变量（Y）的方差中可以被自变量（X）解释的比例。它本质上告诉我们数据与模型拟合得有多好。与RMSE不同，RMSE可以是一个任意数，R2以百分比的形式给出，这可能更容易理解。百分比越高，数据的关联性越好。尽管很有用，但高百分比并不总是意味着模型强大。决定一个好的R2值取决于应用和用户如何理解数据。R2可以通过以下公式计算：
- en: '![](img/Formula_04_015.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![公式 4.15](img/Formula_04_015.jpg)'
- en: Many more metrics can evaluate the effectiveness of your regression model, but
    these two are more than enough to get an understanding of how your model is performing.
    When building and evaluating your model, it is important to plot and visualize
    your data and model, as this can identify key points. The plots can help you determine
    whether your model is **overfitting** or **underfitting**.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting occurs when your model is too suited to your training data. Your
    RMSE will be really low, and you will have a training accuracy of almost 100%.
    While this seems tempting, it is an indication of a poor model. This can be caused
    by one of two things: not enough data or too many parameters. As a result, when
    you test your model on new data it has not seen before, it will perform very poorly
    due to it not being able to generalize the data.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: An overfitted linear regression model](img/B18934_04_11.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: An overfitted linear regression model'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'To address overfitting, you can try to increase the amount of training data,
    or make the model less complex. It also helps to randomly shuffle your data before
    you split it into the training and testing set. Another important technique is
    called **regularization**. While there are many different regularization techniques
    (L1 or L2 regularization) depending on the model, they all work similarly in that
    they add **bias** or noise into the model to prevent overfitting. In the regression
    equation we previously saw, we can add another term, ![](img/Formula_04_016.png),
    to show that regularization is being applied to our model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_017.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'On the other end, underfitting occurs when your model is unable to find any
    meaningful correlation within the data. This is not as common as overfitting since
    it is easy to find patterns in most data. If this occurs, either your data has
    too much noise and is severely uncorrelated, your model is too simple and doesn’t
    have enough parameters, or the model is not effective for the application at hand.
    It is also useful to debug your code and make sure there are no bugs when it comes
    to preprocessing your data or setting up your model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: An under-fitted linear regression model](img/B18934_04_12.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: An under-fitted linear regression model'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the goal is to find the best-fitting model, between an overfit and
    an underfit. It takes time and experimentation to find a model that works for
    your needs, but using the key indicators and metrics discussed here can help guide
    you in the right direction:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13: A best-fitting linear regression model](img/B18934_04_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: A best-fitting linear regression model'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is a critical part of building a comprehensive model. Understanding
    your data can help determine which features or parameters to include in your model
    so that you can capture the relationship between the independent and dependent
    variables without causing overfitting.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some key notes to keep in mind when collecting and working with the
    data for your model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集和处理模型数据时，有一些关键点需要注意：
- en: '**Normalize your data**: It is possible to have features with very high or
    low numbers, so to prevent them from overwhelming the model and creating biases,
    it is imperative to normalize all your data to make it uniform across the features.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化数据**：可能存在具有非常高的或很低的数值的特征，为了防止它们压倒模型并产生偏差，将所有数据归一化以使它们在特征上保持一致是至关重要的。'
- en: '**Clean your data**: In the real world, the data we collect isn’t always perfect
    and can contain missing or egregious data. It is important to deal with these
    issues because they can cause outliers and impact the model negatively.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理数据**：在现实世界中，我们收集的数据并不总是完美的，可能包含缺失或严重的错误数据。处理这些问题很重要，因为它们可能导致异常值并负面影响模型。'
- en: '**Understand the data**: It is a common practice to perform statistical analysis,
    also known as **Exploratory Data Analysis** (**EDA**), on your data to get a better
    understanding of how the data can impact your model. This can include plotting
    graphs, running statical methods, and even using machine learning techniques to
    reduce the dimensionality of the data, which will be discussed later in the chapter.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解数据**：对数据进行统计分析，也称为**探索性数据分析**（**EDA**），是常见的做法，以更好地了解数据如何影响模型。这可以包括绘制图表、运行统计方法，甚至使用机器学习技术来降低数据的维度，这些将在本章后面讨论。'
- en: In the next section, we will discuss classification models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论分类模型。
- en: Building multiclass classification
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建多类分类
- en: Unlike regression models that produce a continuous output, models are considered
    classification models when they produce a finite output. Some examples include
    email spam detection, image classification, and speech recognition.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与产生连续输出的回归模型不同，当模型产生有限输出时，它们被认为是分类模型。一些例子包括垃圾邮件检测、图像分类和语音识别。
- en: Classification models are considered versatile since they can apply to both
    supervised and unsupervised learning while regression models are mostly used for
    supervised learning. There are some regression models (such as logistic regression
    and support vector machine) that are also considered classification models since
    they use a threshold to split the output of continuous values into different categories.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型被认为是多才多艺的，因为它们可以应用于监督学习和无监督学习，而回归模型主要用于监督学习。有一些回归模型（如逻辑回归和支持向量机）也被认为是分类模型，因为它们使用阈值将连续值的输出分割成不同的类别。
- en: Unsupervised learning is a common application used in today’s market. Although
    supervised learning usually performs better and provides meaningful results since
    we know the expected output, the majority of the data we collect is unlabeled.
    It costs companies time and money for human experts to sift through the data and
    label it. Unsupervised learning helps reduce the cost and time by getting the
    model to try and determine the labels for the data and extract meaningful information.
    They can even perform better than humans sometimes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是当今市场上常用的应用。尽管监督学习通常表现更好，并提供了有意义的成果，因为我们知道预期的输出，但我们收集的大多数数据都是未标记的。对于公司来说，让人类专家筛选数据并进行标记既耗时又费钱。无监督学习通过让模型尝试为数据确定标签并提取有意义的信息，有助于降低成本和时间。有时它们甚至可以比人类表现得更好。
- en: 'The number of categories in the output of a classification model determines
    what type of model it is. For models with only two outputs (i.e., spam and not
    spam), this is called a binary classifier, while models with more than two outputs
    are called multiclass classifiers:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型输出的类别数量决定了它的类型。对于只有两个输出（即垃圾邮件和非垃圾邮件）的模型，这被称为二元分类器，而具有两个以上输出的模型被称为多类分类器：
- en: '![Figure 4.14: Binary and multiclass classifiers](img/B18934_04_14.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图4.14：二元和多类分类器](img/B18934_04_14.jpg)'
- en: 'Figure 4.14: Binary and multiclass classifiers'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14：二元和多类分类器
- en: 'From those classifiers, there are two types of learners: **lazy learners**
    and **eager learners**.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从那些分类器中，有两种类型的学习者：**懒惰学习者**和**急切学习者**。
- en: Lazy learners essentially store the training data and wait until they receive
    new test data. Once they get the test data, the model classifies the new data
    based on the already existing data. These types of learners take less time when
    training since you can continuously add new data without having to retrain the
    entire model, but take longer when performing classification since they have to
    go through all the data points. One common type of lazy learner is the **K-Nearest
    Neighbors** (**KNN**) algorithm.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, eager learners work in the opposite way. Whenever new data
    is added to the model, they have to retrain the model again. Although this takes
    more time compared to lazy learners, querying the model is much faster since they
    don’t have to go through all the data points. Some examples of eager learners
    are decision trees, naïve Bayes, and ANNs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning will generally perform better than unsupervised learning
    since we know what the expected output should be during training, but it is costly
    to have to collect and label the data, so unsupervised learning excels in this
    area of training on unlabeled data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will be looking at a few niche models that can
    be used for unique problems that most basic classification or regression models
    can’t solve.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Text sentiment analysis and topic modeling
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular field in the machine learning field is topic modeling and text analysis.
    With a plethora of text on the internet, being able to understand that data and
    create complex models such as chatbots and translation services has become a hot
    topic. Interacting with human language using software is called **NLP**.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Despite the amount of data we can use to train our models, it is a difficult
    task to create meaningful models. Language itself is complex and contains many
    grammar rules, especially when trying to translate between languages. Certain
    powerful techniques can help us when creating NLP models though.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Before implementing any NLP models, it is imperative to preprocess the data
    in some way. Documents and text tend to contain extraneous data, such as stopping
    words (the/a/and) or random characters, which can affect the model and produce
    flawed results.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The first idea we will discuss is **topic modeling**. This is the process of
    grouping text or words from documents into different topics or fields. This is
    useful when you have a document or text and want to classify and group it into
    a certain genre without having to go through the tedious process of reading documents
    one by one. There are many different models used for topic modeling:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent Semantic** **Analysis** (**LSA**)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probabilistic Latent Semantic** **Analysis** (**PLSA**)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet** **Allocation** (**LDA**)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will focus on LDA. LDA uses statistics to find patterns and repeated occurrences
    of words or phrases and groups them into their topics. It assumes that each document
    contains a mixture of topics and that each topic contains a mixture of words.
    LDA first starts the process of going through the documents and keeping a word
    matrix, where it contains the count of each word in each document:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15: A word matrix](img/B18934_04_15.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: A word matrix'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a word matrix, we determine the number of topics, ![](img/Formula_04_018.png),
    that we want to split up the words into and use statistics to find the probability
    of the words belonging to a certain topic. Using Bayesian statistics, we can then
    calculate the probability and use that to cluster the words into different topics:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16: An LDA model](img/B18934_04_16.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: An LDA model'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Another rising application in NLP is **sentiment analysis**. This involves the
    process of taking words or text and understanding the user’s intent or emotion.
    This is common today when dealing with online reviews or social media posts. It
    determines whether a piece of text contains positive, neutral, or negative emotions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Many different methods and models can solve this problem. The simplest approach
    is through statistics by using Bayes’ theorem. This formula is used for predictive
    analysis, as it uses previous words in a text to update the model. The probability
    can be calculated using this formula:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_019.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Deep learning has become a powerful tool for NLP and can be useful for sentiment
    analysis. CNNs and **Recurrent Neural Networks** (**RNNs**) are two types of deep
    learning models that can drastically improve models for NLP, especially for sentiment
    analysis. We will discuss these neural networks and how they perform more later
    in this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Pattern analysis and forecasting in machine learning
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the uncertainty of time, being able to predict certain trends and patterns
    has become a hot topic in today’s industry. Most regression models, while powerful,
    are not able to make confident time predictions. As a result, some researchers
    have devised models that take time into consideration when making certain predictions,
    such as gas prices, stock market, and sales forecasting. Before we go into the
    different models, we must first understand the different concepts in time-series
    analysis.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step when dealing with time-series problems is familiarizing yourself
    with the data. The data usually contains one of four data components:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '**Trend** – The data follows an increasing or decreasing continuous timeline
    and there are no periodic changes'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seasonality** – The data changes in a set periodic timeline'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cyclical** – The data changes but there is no set periodic timeline'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irregular** – The data changes randomly with no pattern'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.17: Different components of time-series data](img/B18934_04_17.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Different components of time-series data'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'These different trends can be split into two different data types:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '**Stationary** – Certain attributes of the data, such as mean, variance, and
    covariance, do not change over time'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-stationary** – Attributes of the data change over time'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Often, you will work with non-stationary data, and creating machine learning
    models using this type of data will generate unreliable results. To resolve this
    issue, we use certain techniques to change our data into stationary data. They
    include the following methods:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '**Differencing** – A mathematical method used to normalize the mean and remove
    the variance. It can be calculated by using this formula:![](img/Formula_04_020.jpg)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation** – Mathematical methods are used to remove the change in
    variance. Among the transformations, these are three commonly used:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log transform
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Square root
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Power transform
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.18: Differencing non-stationary data](img/B18934_04_18.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Differencing non-stationary data'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Time is already uncertain, and this makes it almost impossible to create a model
    that can confidently predict future trends. The more we can remove uncertainty
    in our data, the better our model can find the relationships in our data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we can transform our data, we can start looking at models to help us with
    forecasting. Of the different models, the most popular model for time-series analysis
    is an **Auto-Regressive Integrated Moving Average** (**ARIMA**) model. This linear
    regression model consists of three subcomponents:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '**Auto-Regression** (**AR**) – A regression model that uses the dependencies
    of the current time and previous time to make predictions'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated** (**I**) – The process of differencing in order to make the data
    stationary'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Moving Average** (**MA**) – Models between the expected data and the residual
    error by calculating the MA of the lagged observed data'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with ARIMA, other machine learning models can be used for time-series
    problems. Another well-known model is the RNN model. This is a type of deep learning
    model used for data that has some sort of sequence. We will be going into more
    detail on how they work in the next section.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing models using deep learning
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the chapter, we briefly discussed deep learning and the advantages
    it brings when enhancing simple machine learning models. In this section, we will
    go into more information on the different deep learning models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Before we can build our model, we will briefly go over the structure of the
    deep learning models. A simple ANN model usually contains about two to three fully
    connected layers and is usually strong enough to model most complex linear functions,
    but as we add more layers, the improvement to the model significantly diminishes,
    and it is unable to perform more complex applications due to overfitting.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning allows us to add multiple hidden layers to our ANN while reducing
    our time to train the model and increasing the performance. We can do this by
    adding one or both types of hidden layers common in deep learning – a CNN or RNN.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN is mostly applied in the image detection and video recognition field
    due to how the neural network is structured. The CNN architecture comprises the
    following key features:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation layers
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The convolutional layer is the core element in the CNN model. Its primary task
    is to convolve or group sections of the data using a **kernel** and produces an
    output called a **feature map**. This map contains all the key features extracted
    from the data, which can then be used for training in the fully connected layer.
    Each element in the feature map indicates a receptive field, which is used to
    denote which part of the input is used to map to the output. As you add more convolutional
    layers, you can extract more features, and this allows your model to adapt to
    more complex models:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19: Convolutional layer output](img/B18934_04_19.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Convolutional layer output'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.19*, we can see how the feature map is created in the convolutional
    layer. The layer slides the kernel across the input data, performs the dot operation,
    and produces an output matrix, which is the result of the convolution function.
    The size of the kernel and the step size can dictate the output size of the feature
    map. In this example, we use a kernel size of 2x2 and a stride or step size of
    2, which gives us a feature map of size 2x2 based on our input size. The output
    of the feature map can then be used for more future convolutional layers depending
    on the requirements of the user.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we use our newly created feature map as an input for another convolutional
    or fully connected layer, we pass the feature map through an activation layer.
    It is important to pass your data through some type of nonlinear function during
    the training process, as this allows your model to map to more complex functions.
    Many different types of activation functions can be used throughout your model
    and have their benefits depending on the type of model you are planning to build.
    Among the many activation functions, these are the most commonly used:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '**Rectified Linear** **Activation** (**ReLU**)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic (sigmoid)**'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Hyperbolic** **Tangent** (**Tanh**)
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ReLU function is the most popular activation function used today. It is
    very simple and helps the model learn and converge more quickly than most other
    activation functions. It is calculated using this function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_021.jpg)![](img/B18934_04_20.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: ReLU function'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic function is another commonly used activation function. This is
    the same function used in logistical regression models. This function helps bound
    the output of the feature map between 0 and 1\. While useful, this function is
    computationally heavy and may slow down the training process. It is calculated
    using this function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_022.jpg)![](img/B18934_04_21.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Sigmoid function'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'The Tanh function is similar to the sigmoid function in that it bounds the
    values from the feature map. Rather than bounding it from 0 to 1, it bounds the
    values from -1 to 1, and it usually performs better than a sigmoid function. The
    function is calculated using this formula:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_023.jpg)![](img/B18934_04_22.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Tanh function'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Each activation has its uses and benefits depending on the task or model at
    hand. The ReLU function is commonly used in CNN models while sigmoid and Tanh
    are mostly found in RNN models, but they can be used interchangeably and bring
    different results.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'After we run our feature map through an activation layer, we come to the final
    piece – the pooling layer. As mentioned before, a key element in deep learning
    is the reduction of parameters. This allows the model to train on fewer parameters
    while still retaining the important features extracted from our convolutional
    layers. The pooling layer is responsible for this step of downsizing our parameter
    size. There are many common pooling functions but the most commonly used is max
    pooling. This is similar to the convolutional layer, where we use a kernel or
    filter to slide through our input data and only take the maximum value from each
    window:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23: Max pooling layer output](img/B18934_04_23.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: Max pooling layer output'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.23*, we are using a kernel size of 2x2 with a stride size of
    2\. Here, we can see our output where only the maximum value from each window
    is selected.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Other layers and functions can be added to the model to help address certain
    issues and applications, such as a batch normalization layer, but with these three
    foundational layers, we can add multiple layers of different sizes and still build
    a powerful model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: In the final layer, we feed our output into the fully connected layer. By that
    time, we are able to extract the important features of the data and still learn
    more complex models with less time to train as compared to a simple ANN model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will go over the RNN architecture. Due to the nature of how the RNN
    model is structured, it is designed for tasks that need to take into consideration
    a set of sequence data, in which the data later in the sequence is dependent on
    earlier data. This model is commonly used for certain fields such as NLP and signal
    processing.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The basics of an RNN are built by having a hidden layer in which the output
    of the layer is fed back into the same hidden layer. This way, the model is able
    to learn based on previous data and adjust the weights accordingly.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24: RNN architecture](img/B18934_04_24.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: RNN architecture'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how the model works, you can envision a single layer for
    each data point. Each layer takes in the data point as an input ![](img/Formula_04_024.png)
    and produces an output ![](img/Formula_04_025.png). We then transfer the weights
    between the layers and then take the total average of all the cost functions from
    all the layers in the model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25: An unraveled RNN](img/B18934_04_25.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.25: An unraveled RNN'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'This model works for most simple problems. As the sequence increases, it encounters
    a few issues:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing gradient**: This occurs during the training process when the gradient
    approaches zero. The weights aren’t updated properly as a result and the model
    performs poorly.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of context**: The model is unidirectional and cannot look further or
    previously into the data. Therefore, the model is only able to predict based on
    data around the current sequence point and is more likely to make a poor prediction
    based on incorrect context.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are different variations of RNNs created to address some of the issues
    mentioned here. Among them, the most common one used today is the **Long Short-Term
    Memory** (**LSTM**) model. The LSTM model comprise three components:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: An input gate
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output gate
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A forget gate
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.26: An LSTM neural network](img/B18934_04_26.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.26: An LSTM neural network'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: These gates work by regulating which data points are needed to contextualize
    the sequence. That way, the model can predict more accurately without being easily
    manipulated.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate is specifically responsible for removing previous data or context
    that is no longer needed. This gate uses the sigmoid function to determine whether
    it uses or “forgets” the data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The input gate is used to determine whether the new data is relevant to the
    current sequence or not. This is so that only important data is being used to
    train the model and not redundant or irrelevant information.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the output gate’s primary function is to filter the current state’s
    information and only send relevant information to the next state. As with the
    other gates, it uses the context from previous states to apply a filter, which
    helps the model properly contextualize the data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: CNN and RNN models are mostly designed for supervised learning problems. When
    it comes to unsupervised learning, different models are needed to solve certain
    problems. Let’s discuss **autoencoders**.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders work by taking the input data, compressing it, and then reconstructing
    it by decompressing it. While straightforward, it can be used for some advanced
    applications, such as generating audio or images, or it can be used as an anomaly
    detector.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'The autoencoder comprises two parts:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: An encoder
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.27: The components of an autoencoder](img/B18934_04_27.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.27: The components of an autoencoder'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: The encoder and decoder are usually built with a one-layer ANN. The encoder
    is responsible for taking the data and compressing or flattening the data. Then,
    the decoder works on taking the flattened data and trying to reconstruct the input
    data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer in the middle of the encode and decoder is usually referred
    to as the bottleneck. The number of nodes in the hidden layer must be less than
    those in the encoder and decoder. This forces the model to try and find the pattern
    or representation in the input data so that it can reconstruct the data with little
    information. Thus, the cost function is there to calculate and minimize the difference
    between the input and output data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: One aspect of autoencoders that is an integral part of deep learning is **dimensionality
    reduction**. This is the process of reducing the number of parameters or features
    used when training your model. As mentioned earlier in this chapter, to build
    a complex model that can build a deeper representation of the data, it is important
    to include more features. However, adding too many features can lead to overfitting,
    so how do we find the best number of features to use in our model?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: There are many models and techniques, such as autoencoders, that can perform
    dimensionality reduction to help us find the best features to use in our model.
    Among the different techniques, **Principle Component Analysis** (**PCA**) is
    the most popular. This technique can take an N-dimensional dataset and reduce
    the number of dimensions in the data using linear algebra. It is a common practice
    to use a dimensionality reduction technique before using your data to train your
    model, as this can help to remove noise in the data and avoid overfitting.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed what is considered artificial intelligence and
    the different sub-fields that it contains.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the different characteristics of regression and classification
    models. From there, we also went over the structure of our data and how the model
    performs when training over our data. We then discussed the different ways of
    analyzing our model’s performance and how to address the different issues that
    we can come across when training our model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: We briefly viewed the different packages and libraries that are used today in
    machine learning models and their different use cases.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: We also analyzed different topics such as topic modeling and time-series analysis
    and what they entail. With that, we were able to look at the different methods
    and techniques used to solve those types of problems.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we went into deep learning and the different ways it improves on machine
    learning. We went over the two different types of neural networks – CNNs and RNNs
    – how they are structured, and their benefits and use cases.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take what we have learned and start looking into
    how we can design and build an end-to-end machine learning system and the different
    components that it contains.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
