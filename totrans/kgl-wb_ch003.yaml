- en: '2 The Makridakis Competitions: M5 on Kaggle for Accuracy and Uncertainty'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 Makridakis竞赛：M5在Kaggle上的准确性和不确定性
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的Discord书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![](img/file1.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file1.png)'
- en: Since 1982, Spyros Makridakis ([https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/](https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/))
    has involved groups of researchers from all over the world in forecasting challenges,
    called M competitions, in order to conduct comparisons of the efficacy of existing
    and new forecasting methods against different forecasting problems. For this reason,
    M competitions have always been completely open to both academics and practitioners.
    The competitions are probably the most cited and referenced event in the forecasting
    community and they have always highlighted the changing state of the art in forecasting
    methods. Each previous M competition has provided both researchers and practitioners
    not only with useful data to train and test their forecasting tools, but also
    with a series of discoveries and approaches that are revolutionizing the way forecasting
    are done.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自1982年以来，Spyros Makridakis ([https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/](https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/))
    一直让来自世界各地的研究团队参与预测挑战，称为M竞赛，以便对现有和新预测方法在不同预测问题上的有效性进行比较。因此，M竞赛始终对学术界和实践者完全开放。这些竞赛可能是预测社区中被引用和参考最多的活动，并且它们始终突出了预测方法领域的最新进展。每个先前的M竞赛都为研究人员和实践者提供了有用的数据来训练和测试他们的预测工具，同时也提供了一系列发现和途径，这些发现和途径正在彻底改变预测的方式。
- en: The recent M5 competition (the M6 is just running as this chapter is being written)
    has been held on Kaggle and it has proved particularly significant in remarking
    the usefulness of gradient boosting methods when trying to solve a host of volume
    forecasts of retail products. In this chapter, focusing on the accuracy track,
    we deal with a time series problem from Kaggle competitions, and by replicating
    one of the top, yet simplest and most clear solutions, we intend to provide our
    readers with code and ideas to successfully handle any future forecasting competition
    that may appear on Kaggle.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最近举行的M5竞赛（M6竞赛正在撰写本章时进行）在Kaggle上举行，它证明了在尝试解决一系列零售产品销量预测问题时，梯度提升方法的有用性尤为重要。在本章中，我们专注于准确性赛道，处理了Kaggle竞赛中的一个时间序列问题，通过复制一个顶级、简单且最清晰的解决方案之一，我们旨在为读者提供代码和思路，以成功应对未来可能出现在Kaggle上的任何预测竞赛。
- en: 'Apart from the competition pages, we found a lot of information regarding the
    competition and its dynamics in the following papers from the International Journal
    of Forecasting:'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了竞赛页面，我们在《国际预测杂志》的以下论文中找到了关于竞赛及其动态的大量信息：
- en: 'Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. *The
    M5 competition: Background, organization, and implementation*. International Journal
    of Forecasting (2021).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makridakis, Spyros, Evangelos Spiliotis, 和 Vassilios Assimakopoulos. *《M5竞赛：背景、组织和实施》*。《国际预测杂志》（2021年）。
- en: 'Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. "M5
    accuracy competition: Results, findings, and conclusions." International Journal
    of Forecasting (2022).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makridakis, Spyros, Evangelos Spiliotis, 和 Vassilios Assimakopoulos。"M5准确性竞赛：结果、发现和结论"《国际预测杂志》（2022年）。
- en: 'Makridakis, Spyros, et al. "The M5 Uncertainty competition: Results, findings
    and conclusions." International Journal of Forecasting (2021).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makridakis, Spyros, 等人。"《M5不确定性竞赛：结果、发现和结论》"《国际预测杂志》（2021年）。
- en: Understanding the competition and the data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解竞争和数据
- en: The competition ran from March to June 2020 and over 7,000 participants took
    part in it on Kaggle. The organizers arranged it into two separated tracks, one
    for point-wise prediction (accuracy track) and another one for estimating reliable
    values at different confidence intervals (uncertainty track)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛从2020年3月持续到6月，超过7,000名参与者参加了在Kaggle上的竞赛。组织者将其分为两个独立的赛道，一个用于点预测（准确性赛道），另一个用于在不同置信区间估计可靠值（不确定性赛道）。
- en: Walmart provided the data. It consisted of 42,840 daily sales time series of
    items hierarchically arranged into departments, categories, and stores spread
    in three U.S. states (the series are somewhat correlated each other). Along with
    the sales, Walmart also provided accompanying information (exogenous variables,
    usually not often provided in forecasting problems) such as the prices of items,
    some calendar information, associated promotions or presence of other events affecting
    the sales.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 沃尔玛提供了数据。它包括42,840个每日销售时间序列，这些序列按部门、类别和店铺分层排列，分布在三个美国州（这些序列彼此之间有一定的相关性）。除了销售数据，沃尔玛还提供了伴随信息（外生变量，通常在预测问题中不常提供），例如商品价格、一些日历信息、相关的促销活动或其他影响销售的事件。
- en: Apart from Kaggle, the data is available, together with the datasets from previous
    M competition, at the address [https://forecasters.org/resources/time-series-data/](https://forecasters.org/resources/time-series-data/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Kaggle，数据以及之前M竞赛的数据集都可以在以下地址找到：[https://forecasters.org/resources/time-series-data/](https://forecasters.org/resources/time-series-data/)。
- en: One interesting aspect of the competitions is that it dealt with consumer goods
    sales both fast moving and slow moving with many examples of the latest presenting
    intermittent sales (sales are often zero but for some rare cases). Intermittent
    series, though common in many industries, are still a challenging case in forecasting
    for many practitioners.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛的一个有趣方面是它处理了快速消费品和慢速消费品销售，有许多例子展示了最新的间歇性销售（销售通常是零，但在一些罕见情况下）。虽然间歇性序列在许多行业中很常见，但对于许多从业者来说，在预测中仍然是一个具有挑战性的案例。
- en: The competition timeline has been arranged in two parts. In the first, from
    the beginning of March 2020 to June 1^(st), competitors could train models on
    the range of days up to day 1,913 and score their submission on the public test
    set (ranging from day 1,914 to 1,941). After that date, until the end of the competition
    on July 1^(st), the public test set was made available as part of the training
    set, allowing participants to tune their models in order to predict from day 1,942
    to 1969 (a time windows of 28 days, i.e. four weeks). In that period, submissions
    were not scored on the leaderboard.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛时间表被安排为两部分。在第一部分，从2020年3月初到6月1日，参赛者可以在1,913天范围内的任何一天训练模型，并在公共测试集（从1,914天到1,941天）上对其提交进行评分。在那之后，直到7月1日的竞赛结束，公共测试集作为训练集的一部分提供，允许参与者调整他们的模型以预测从1,942天到1969天（28天的时间窗口，即四周）。在那个时期，提交在排行榜上没有得分。
- en: The ratio behind such an arrangement of the competition was to allow teams initially
    to test their models on the leaderboard and to have grounds to share their best
    performing methods in notebooks and discussions. After the first phase, the organizers
    wanted to avoid having the leaderboard used for overfitting purposes or hyperparameter
    tuning of the models and they wanted to resemble a forecasting situation, as it
    would happen in the real world. In addition, the requirement to choose only one
    submission as the final one mirrored the same necessity for realism (in the real
    world you cannot use two distinct models predictions and choose the one that suits
    you the best afterwards).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛这种安排背后的比例是为了让团队最初能在排行榜上测试他们的模型，并为他们提供在笔记本和讨论中分享最佳表现方法的基础。在第一阶段之后，组织者希望避免排行榜被用于过度拟合或模型超参数调整的目的，他们希望模拟一种预测情况，就像在现实世界中发生的那样。此外，只选择一个提交作为最终提交的要求，反映了现实世界的相同必要性（在现实世界中，你不能使用两个不同的模型预测，然后在之后选择最适合你的一个）。
- en: 'As for as the data, we mentioned that the data has been provided by Walmart
    and it represents the USA market: it originated from 10 stores in California,
    Wisconsin and Texas. Specifically, the data it is made up by the sales of 3,049
    products, organized into three categories (hobbies, food, and household) that
    can be divided furthermore into 7 departments each. Such hierarchical structure
    is certainly a challenge because you can model sale dynamics at the level of USA
    market, state market, single store, product category, category department and
    finally specific product. All these levels can also combine as different aggregates,
    which are something required to be predicted in the second track, the uncertainty
    track:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据，我们提到数据由沃尔玛提供，并代表美国市场：它源自加利福尼亚州、威斯康星州和德克萨斯州的 10 家商店。具体来说，数据由 3,049 产品的销售组成，分为三个类别（爱好、食品和家庭），每个类别可以进一步分为
    7 个部门。这种层次结构无疑是一个挑战，因为你可以在美国市场、州市场、单个商店、产品类别、类别部门和最终特定产品级别建模销售动态。所有这些级别也可以组合成不同的汇总，这是第二轨道，即不确定性轨道中需要预测的内容：
- en: '| **Level id** | **Level description** | **Aggregation level** | **Number of
    series** |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **级别 ID** | **级别描述** | **汇总级别** | **序列数量** |'
- en: '| 1 | All products, aggregated for all stores and states | Total | 1 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 所有产品，按所有商店和州汇总 | 总计 | 1 |'
- en: '| 2 | All products, aggregated for each state | State | 3 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 所有产品，按每个州汇总 | 州 | 3 |'
- en: '| 3 | All products, aggregated for each store | Store | 10 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 所有产品，按每个商店汇总 | 商店 | 10 |'
- en: '| 4 | All products, aggregated for each category | Category | 3 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 所有产品，按每个类别汇总 | 类别 | 3 |'
- en: '| 5 | All products, aggregated for each department | Department | 7 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 所有产品，按每个部门汇总 | 部门 | 7 |'
- en: '| 6 | All products, aggregated for each state and category | State-Category
    | 9 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 所有产品，按每个州和类别汇总 | 州-类别 | 9 |'
- en: '| 7 | All products, aggregated for each state and department | State-Department
    | 21 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 所有产品，按每个州和部门汇总 | 州-部门 | 21 |'
- en: '| 8 | All products, aggregated for each store and category | Store-Category
    | 30 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 所有产品，按每个商店和类别汇总 | 商店-类别 | 30 |'
- en: '| 9 | All products, aggregated for each store and department | Store-Department
    | 70 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 所有产品，按每个商店和部门汇总 | 商店-部门 | 70 |'
- en: '| 10 | Each product, aggregated for all stores/states | Product | 3,049 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 每个产品，按所有商店/州汇总 | 产品 | 3,049 |'
- en: '| 11 | Each product, aggregated for each state | Product-State | 9,147 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 每个产品，按每个州汇总 | 产品-州 | 9,147 |'
- en: '| 12 | Each product, aggregated for each store | Product-Store | 30,490 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 每个产品，按每个商店汇总 | 产品-商店 | 30,490 |'
- en: '|  |  | Total | 42,840 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 总计 | 42,840 |'
- en: From the point of view of time, the granularity is daily sales record and the
    covered the period spanning from 29 January 2011 to 19 June 2016 which equals
    to 1,969 days in total, 1,913 for training, 28 for validation – public leaderboard
    – 28 for test – private leaderboard. A forecasting horizon of 28 days is actually
    recognized in the retail sector as the proper horizon for handling stocks and
    re-ordering operations for most goods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从时间角度来看，粒度是每日销售记录，覆盖了从 2011 年 1 月 29 日到 2016 年 6 月 19 日的期间，总计 1,969 天，其中 1,913
    天用于训练，28 天用于验证 - 公开排行榜 - 28 天用于测试 - 私人排行榜。实际上，在零售行业中，28 天的预测范围被认为是处理大多数商品的库存和重新订购操作的正确范围。
- en: 'Let’s examine the different data you receive for the competition. You get `sales_train_evaluation.csv`,
    `sell_prices.csv` and `calendar.csv`. The one keeping the time series is `sales_train_evaluation.csv`.
    It is composed of fields that act as identifiers (`item_id`, `dept_id`, `cat_id`,
    `store_id`, and `state_id`) and columns from `d_1` to `d_1941` representing the
    sales of those days:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下比赛中收到的不同数据。你将获得 `sales_train_evaluation.csv`、`sell_prices.csv` 和 `calendar.csv`。其中包含时间序列的是
    `sales_train_evaluation.csv`。它由作为标识符的字段（`item_id`、`dept_id`、`cat_id`、`store_id`
    和 `state_id`）以及从 `d_1` 到 `d_1941` 的列组成，代表那些天的销售情况：
- en: '![Figure 2.1: The sales_train_evaluation.csv data](img/file2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1：sales_train_evaluation.csv 数据](img/file2.png)'
- en: 'Figure 2.1: The sales_train_evaluation.csv data'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：sales_train_evaluation.csv 数据
- en: '`sell_prices.csv` contains instead information about the price of the items.
    The difficulty here is in joining the `wm_yr_wk` (the id of the week) with the
    columns in the training data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`sell_prices.csv` 包含关于商品价格的信息。这里的难点在于将 `wm_yr_wk`（周标识符）与训练数据中的列连接起来：'
- en: '![Figure 2.2: The sell_prices.csv data](img/file3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2：sell_prices.csv 数据](img/file3.png)'
- en: 'Figure 2.2: The sell_prices.csv data'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：sell_prices.csv 数据
- en: 'The last file, `calendar.csv`, contains data relative to events that could
    have affected the sales:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个文件，`calendar.csv`，包含可能影响销售的事件相关数据：
- en: '![Figure 2.3: The calendar.csv data](img/file4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3：calendar.csv数据](img/file4.png)'
- en: 'Figure 2.3: The calendar.csv data'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：calendar.csv数据
- en: Again, the main difficulty seems in joining the data to the columns in the training
    table. Anyway, here you can get an easy key to connect columns (the d field) with
    the `wm_yr_wk`. In addition, in the table we have represented different events
    that may have occurred on particular days as well as SNAP days that are special
    days when the nutrition assistance benefits called the Supplement Nutrition Assistance
    Program (SNAP) can be used.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，主要困难似乎在于将数据与训练表中的列连接起来。无论如何，在这里您可以获得一个简单的键来连接列（d字段）与`wm_yr_wk`。此外，在表中，我们表示了可能发生在特定日期的不同事件，以及SNAP日，这是特别的日子，在这些日子里，可以使用的营养援助福利补充营养援助计划（SNAP）。
- en: Understanding the Evaluation Metric
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解评估指标
- en: 'The accuracy competition introduced a new evaluation metric: Weighted Root
    Mean Squared Scaled Error (WRMSSE). The metric evaluates the deviation of the
    of the point forecasts around the mean of the realized values of the series being
    predicted:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度竞赛引入了一个新的评估指标：加权均方根缩放误差（WRMSSE）。该指标评估点预测围绕预测序列实现值平均值的偏差：
- en: '![](img/file5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/file5.png)'
- en: 'where:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*n* is the length of the training sample'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*n* 是训练样本的长度'
- en: '*h* is the forecasting horizon (in our case it is *h*=28)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*h* 是预测范围（在我们的情况下是*h*=28）'
- en: '*Y*[*t*] is the sales value at time *t*,![](img/file6.png)is the predicted
    value at time *t*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y*[*t*] 是时间 *t* 的销售价值，![](img/file6.png) 是时间 *t* 的预测值'
- en: 'In the competition guidelines ([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/)),
    in regard of WRMSSE, it is remarked that:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛指南([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/))中，关于WRMSSE，指出：
- en: The denominator of RMSSE is computed only for the time-periods for which the
    examined product(s) are actively sold, i.e., the periods following the first non-zero
    demand observed for the series under evaluation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSSE的分母仅针对那些正在积极销售的考察产品的时间段进行计算，即，在评估序列观察到的第一个非零需求之后的时期。
- en: The measure is scale independent, meaning that it can be effectively used to
    compare forecasts across series with different scales.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该度量与规模无关，这意味着它可以有效地用于比较不同规模序列的预测。
- en: In contrast to other measures, it can be safely computed as it does not rely
    on divisions with values that could be equal or close to zero (e.g. as done in
    percentage errors when *Y*[*t*] = 0 or relative errors when the error of the benchmark
    used for scaling is zero).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他度量相比，它可以安全地计算，因为它不依赖于可能等于或接近零的值的除法（例如，在*Y*[*t*] = 0时进行的百分比误差，或者当用于缩放的基准误差为零时进行的相对误差）。
- en: The measure penalizes positive and negative forecast errors, as well as large
    and small forecasts, equally, thus being symmetric.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该度量对正负预测误差、大预测和小预测进行同等惩罚，因此是对称的。
- en: 'A good explanation of the underlying workings of this is provided by this post
    from Alexander Soare ([https://www.kaggle.com/alexandersoare](https://www.kaggle.com/alexandersoare)):
    [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273).
    After having transformed the evaluation metric, Alexandre attributes improving
    performances to improving the ratio between the error in the predictions and the
    day-to-day variation of sales values. If the error is the same as the daily variations
    (ratio=1), it is likely that the model is not much better than a random guess
    based on historical variations. If your error is less than that, it is score in
    a quadratic way (because of the square root) as it approaches zero. Consequently,
    a WRMSSE of 0.5 corresponds to a ratio of 0.7 and a WRMSSE of 0.25 corresponds
    to a ratio of 0.5.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 亚历山大·索阿雷（Alexander Soare）在这篇帖子中提供了对这种工作原理的良好解释([https://www.kaggle.com/alexandersoare](https://www.kaggle.com/alexandersoare))：[https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273)。在转换评估指标后，亚历山大将性能的提高归因于预测误差与日销售价值日变化率之间的比率提高。如果误差与日变化率相同（比率=1），则模型可能并不比基于历史变化的随机猜测好多少。如果您的误差小于这个值，则分数以二次方式（由于平方根）接近零。因此，WRMSSE为0.5对应比率为0.7，WRMSSE为0.25对应比率为0.5。
- en: 'During the competition, many attempts have been made at using the metric not
    only for evaluation besides the leaderboard but also as an objective function.
    First of all, the Tweedie loss (implemented both in XGBoost and LightGBM) worked
    quite well for the problem because it could handle the skewed distributions of
    sales for most products (a lot of them also had intermittent sales and that is
    also handled finely by the Tweedie loss). The Poisson and Gamma distributions
    can be considered extreme cases of the Tweedie distribution: based on the parameter
    power, p, with p=1 you get a Poisson distribution and with p=2 a Gamma one. Such
    power parameter is actually the glue that keeps the mean and the variance of the
    distribution connected by the formula variance = k*mean**p. Using a power value
    between 1 and 2, you actually get a mix of Poisson and Gamma distributions which
    can fit very well the competition problem. Most of the Kagglers involved in the
    competition and using a GBM solution, actually have resorted to Tweedie loss.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛期间，许多尝试不仅将指标用于排行榜的评估，还将其作为目标函数。首先，Tweedie损失（在XGBoost和LightGBM中实现）对于这个问题来说效果相当好，因为它可以处理大多数产品的销售分布的偏斜（其中很多也有间歇性销售，这也被Tweedie损失很好地处理）。泊松和伽马分布可以被认为是Tweedie分布的极端情况：基于参数幂，p，当p=1时得到泊松分布，当p=2时得到伽马分布。这种幂参数实际上是连接分布的均值和方差的粘合剂，通过公式方差
    = k*均值**p。使用介于1和2之间的幂值，实际上可以得到泊松和伽马分布的混合，这可以很好地适应竞赛问题。实际上，大多数参与竞赛并使用GBM解决方案的Kagglers都求助于Tweedie损失。
- en: 'In spite of Tweedie success, some other Kagglers, however, found interesting
    ways to implement an objective loss more similar to WRMSSE for their models:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Tweedie方法取得了成功，然而，一些其他Kagglers却发现了一些有趣的方法来实现一个更接近WRMSSE的目标损失，用于他们的模型：
- en: '* Martin Kovacevic Buvinic with his asymmetric loss: [https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook](https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* Martin Kovacevic Buvinic及其非对称损失：[https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook](https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook)'
- en: '* Timetraveller using PyTorch Autograd to get gradient and hessian for any
    differentiable continuous loss function to be implemented in LighGBM: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '* Timetraveller使用PyTorch Autograd获取梯度和对角线，以实现任何可微分的连续损失函数，并在LighGBM中实现：[https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837)'
- en: Examining the 4th place solution’s ideas from Monsaraida
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检验来自Monsaraida的第四名解决方案的想法
- en: 'There are many solutions available for the competition, to be mostly found
    on the competition Kaggle discussions pages. The top five methods of both challenges
    have also been gathered and published (but one because of proprietary rights)
    by the competition organizers themselves: [https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods)
    (by the way, reproducing the results of the winning submissions was a prerequisite
    for the collection of a competition prize).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次竞赛，有许多解决方案可供选择，主要可以在竞赛Kaggle讨论页面上找到。挑战的前五名方法也已被竞赛组织者自己收集并发布（但有一个因为版权问题）：[https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods)（顺便说一句，重现获奖提交的结果是收集竞赛奖金的前提条件）。
- en: Noticeably, all the Kagglers that have placed in the higher ranks of the competitions
    have used, as their unique model type or in blended/stacked in ensembles, LightGBM
    because of its lesser memory usage and speed of computations that gave it an advantage
    in the competition because of the large amount of times series to process and
    predict. But there are also other reasons for its success. Contrary to classical
    methods based on ARIMA, it doesn’t require relying on the analysis of auto-correlation
    and in specifically figuring out the parameters for each single series in the
    problem. In addition, contrary to methods based on deep learning, it doesn’t require
    looking for improving complicated neural architectures or tuning large number
    of hyperparameters. The strength of the gradient boosting methods in time series
    problems (for extension of every other gradient boosting algorithm, such as for
    instance XGBoost) is to rely on feature engineering, creating the right number
    of features based on time lags, moving averages and averages from groupings of
    the series attributes. Then choosing the right objective function and doing some
    hyper-parameter tuning will suffice to obtain excellent results when the time
    series are enough long (for shorter series, classical statistical methods such
    as ARIMA or exponential smoothing are still the recommended choice).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，所有在比赛中获得较高排名的Kagglers都使用了LightGBM，作为他们独特的模型类型或在集成/堆叠中，因为其较低的内存使用量和计算速度，这使其在处理和预测大量时间序列时在竞争中具有优势。但还有其他原因导致其成功。与基于ARIMA的经典方法相反，它不需要依赖于自相关分析，以及在具体确定问题中每个单个序列的参数。此外，与基于深度学习的方法相反，它不需要寻找改进复杂的神经网络架构或调整大量超参数。梯度提升方法在时间序列问题中的优势（对于扩展其他梯度提升算法，例如XGBoost）是依赖于特征工程，根据时间滞后、移动平均和序列属性分组创建正确的特征数量。然后选择正确的目标函数并进行一些超参数调整，当时间序列足够长时（对于较短的序列，ARIMA或指数平滑等经典统计方法仍然是推荐的选择），就足以获得优秀的结果。
- en: Another advantage of LightGBM and XGBoost against deep learning solutions in
    the competition was the Tweedie loss, not requiring any feature scaling (deep
    learning networks are particularly sensible to the scaling you use) and the speed
    of training that allowed faster iterations while testing feature engineering.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与比赛中深度学习解决方案相比，LightGBM和XGBoost的另一个优势是Tweedie损失，不需要任何特征缩放（深度学习网络对所使用的缩放特别敏感）以及训练速度，这允许在测试特征工程时进行更快的迭代。
- en: 'Among all these available solutions, we found the one proposed by Monsaraida
    (Masanori Miyahara), a Japanese computer scientist, the most interesting one.
    He has proposed a simple and straightforward solution that has ranked four on
    the private leaderboard with a score of 0.53583\. The solution uses just general
    features without prior selection (such as sales statistics, calendar, prices,
    and identifiers). Moreover, it uses a limited number of models of the same kind,
    using LightGBM gradient boosting, without resorting to any kind of blending, recursive
    modelling when predictions feed other hierarchically related predictions or multipliers
    that is choosing constants to fit the test set better. Here is a scheme taken
    from his presentation solution presentation to the M ([https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4](https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4))
    where it can be noted that he treats each of the ten stores by each of the four
    weeks to be looked into the future that in the end corresponds to producing 40
    models:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些可用的解决方案中，我们发现由日本计算机科学家Monsaraida（Masanori Miyahara）提出的方案最为有趣。他提出了一种简单直接的方法，在私人排行榜上排名第4，得分为0.53583。该方案仅使用一般特征，没有进行预先选择（如销售统计、日历、价格和标识符）。此外，它使用有限数量的同类型模型，使用LightGBM梯度提升，没有求助于任何类型的混合、递归建模（当预测反馈给其他层次相关的预测或乘数时，即选择常数以更好地拟合测试集）。以下是他在M（[https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4](https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4)）的演示解决方案中提出的方案，可以注意到他对待每个商店的每个四周，最终对应于产生40个模型：
- en: '![Figure 2.4: explanation by Monsaraida about the structure of his solution](img/file7.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4：Monsaraida关于其解决方案结构的说明](img/file7.png)'
- en: 'Figure 2.4: explanation by Monsaraida about the structure of his solution'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：Monsaraida关于其解决方案结构的说明
- en: Given that Monsaraida has kept his solution simple and practical, like in a
    real-world forecasting project, in this chapter, we will try to replicate his
    example by refactoring his code in order to run in Kaggle notebooks (we will handle
    the memory and the running time limitations by splitting the code into multiple
    notebooks). In this way, we intend to provide the readers with a simple and effective
    way, based on gradient boosting, to approach forecasting problems.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Monsaraida保持了其解决方案的简单和实用，就像在现实世界的预测项目中一样，在这一章中，我们将尝试通过重构他的代码来复制他的示例，以便在Kaggle笔记本中运行（我们将通过将代码拆分成多个笔记本来处理内存和运行时间限制）。这样，我们旨在为读者提供一个简单而有效的方法，基于梯度提升，来处理预测问题。
- en: Computing predictions for specific dates and time horizons
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算特定日期和时间跨度的预测
- en: The plan for replicating Monsaraida’s solution is to create a notebook customizable
    by input parameters in order to produce the necessary processed data for train
    and test and the LightGBM models for predictions. The models, given data in the
    past, will be trained to learn to predict values in a specific number of days
    in the future. The best results can be obtained by having each model to learn
    to predict the values in a specific week range in the future. Since we have to
    predict up to 28 days ahead in the future, we need a model predicting from day
    +1 to day +7 in the future, then another one able to predict from day +8 to day
    +14, another from day +15 to +21 and finally another last one capable of handling
    predictions from day +22 to day +28\. We will need a Kaggle notebook for each
    of these time ranges, thus we need four notebooks. Each of these notebooks will
    be trained to predict that future time span for each of the ten stores part of
    the competitions. In total, each notebook will produce ten models. All together,
    the notebooks will then produce forty models covering all the future range and
    all the stores.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 复制Monsaraida解决方案的计划是创建一个可由输入参数自定义的笔记本，以便生成训练和测试所需的必要处理数据以及用于预测的LightGBM模型。给定过去的数据，这些模型将被训练以学习预测未来特定天数内的值。通过让每个模型学习预测未来特定周范围内的值，可以获得最佳结果。由于我们必须预测未来最多28天，我们需要一个从未来+1天到未来+7天的模型，然后是另一个能够从未来+8天到未来+14天的模型，再是另一个从未来+15天到+21天的模型，最后是一个能够处理从未来+22天到未来+28天的预测的模型。我们需要为每个时间范围创建一个Kaggle笔记本，因此我们需要四个笔记本。每个这些笔记本都将被训练以预测每个参与竞赛的十个店铺的未来时间跨度。总共，每个笔记本将生成十个模型。所有这些笔记本将共同生成四十个模型，覆盖所有未来范围和所有店铺。
- en: Since we need to predict both for the public leaderboard and for the private
    one, it is necessary to repeat this process twice, stopping training at day 1,913
    (predicting days from 1,914 to 1,941) for the public test set submission and at
    day 1,941 (predicting days from 1,942 5o 1,969) for the private one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要为公共排行榜和私有排行榜都进行预测，因此有必要重复这个过程两次，在1,913天（预测1,914天到1,941天的日子）停止训练以提交公共测试集，以及在1,941天（预测1,942天到1,969天的日子）停止训练以提交私有测试集。
- en: 'Given the current limitations for running Kaggle notebooks based on CPU, all
    these eight notebooks can be run in parallel (all the process taking almost 6
    hours and a half). Each notebook can be distinguishable by others by its name,
    containing the parameters’ values relative to the last training day and the look-ahead
    horizon in days. An example of one of these notebooks can be found at: [https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于CPU运行Kaggle笔记本的限制，所有这八个笔记本都可以并行运行（整个过程几乎需要6个半小时）。每个笔记本可以通过其名称与其他笔记本区分开来，包含与最后训练日和前瞻性预测天数相关的参数值。这些笔记本中的一个示例可以在以下链接找到：[https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7)。
- en: Let’s now examine together how the code has been arranged and what we can learn
    from Monsaraida’s solution.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们一起来检查代码是如何安排的，以及我们可以从Monsaraida的解决方案中学到什么。
- en: 'We simply start by importing the necessary packages. You can just notice how,
    apart from NumPy and pandas, the only data science specialized package is LightGBM.
    You may also notice that we are going to use gc (garbage collection): that’s because
    we need to limit the amount of used memory by the script, and we frequently just
    collect and recycle the unused memory. As part of this strategy, we also frequently
    store away on disk models and data structures, instead of keeping them in-memory:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的包。您只需注意到，除了NumPy和pandas之外，唯一的数据科学专用包是LightGBM。您可能还会注意到，我们将使用gc（垃圾回收）：这是因为我们需要限制脚本使用的内存量，并且我们经常只是收集和回收未使用的内存。作为这一策略的一部分，我们也经常将模型和数据结构存储到磁盘上，而不是保留在内存中：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As part of the strategy to limit the memory usage, we resort to the function
    to reduce pandas DataFrame described in the Kaggle book and initially developed
    by Arjan Groen during the Zillow competition (read the discussion [https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844](https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844)):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作为限制内存使用的策略的一部分，我们求助于Kaggle书中描述的用于减少pandas DataFrame的函数，最初由Arjan Groen在Zillow比赛中开发（阅读讨论[https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844](https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844)）：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We keep on defining functions for this solution, because it helps splitting
    the solution into smaller parts and because it is easier to clean up all the used
    variables when you just return from a function (you keep only what you saved to
    disk, and you returned from the function). Our next function helps us to load
    all the data available and compress it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续定义这个解决方案的函数，因为这样有助于将解决方案拆分成更小的部分，并且当您从函数返回时，清理所有使用的变量会更加容易（您只需保留保存到磁盘的内容，并从函数返回）。我们的下一个函数帮助我们加载所有可用的数据并将其压缩：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After preparing the code to retrieve the data relative to prices, volumes, and
    calendar information, we proceed to prepare the first processing function that
    will have the role to create a basic table of information having `item_id`, `dept_id`,
    `cat_id`, `state_id` and `store_id` as row keys, a day column and values column
    containing the volumes. This is achieved starting from rows having all the days’
    data columns by using the pandas command melt ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)).
    The command takes as reference the index of the DataFrame and then picks all the
    remaining features, placing their name on a column and their value on another
    one (`var_name` and `value_name` parameters help you define the name of these
    new columns). In this way, you can unfold a row representing the sales series
    of a certain item in a certain store into multiple rows each one representing
    a single day. The fact that the positional order of the unfolded columns is preserved
    guarantees that now your time series spans on the vertical axis (you can therefore
    apply furthermore transformations on it, such as moving means).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备获取价格、体积和日历信息相关的数据代码之后，我们继续准备第一个处理函数，该函数将具有创建一个基本的信息表的角色，其中`item_id`、`dept_id`、`cat_id`、`state_id`和`store_id`作为行键，一个日期列和一个包含体积的值列。这是通过使用pandas命令melt([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html))从具有所有日期数据列的行开始的。该命令以DataFrame的索引为参考，然后选择所有剩余的特征，将它们的名称放在一个列上，将它们的值放在另一个列上（`var_name`和`value_name`参数帮助您定义这些新列的名称）。这样，您可以将代表某个商店中某个商品的销售额序列的行展开成多个行，每行代表一天。展开列的顺序保持不变，这保证了现在您的时间序列在垂直轴上（因此您可以在其上应用进一步的转换，如移动平均值）。
- en: 'To give you an idea of what is happening, here is the `train_df` before the
    transformation with `pd.melt`. Notice how the volumes of the distinct days are
    column features:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您了解正在发生的情况，这里展示了在`pd.melt`转换之前的`train_df`。注意，不同日期的量被作为列特征：
- en: '![Figure 2.5: The training DataFrame](img/file8.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5：训练数据框](img/file8.png)'
- en: 'Figure 2.5: The training DataFrame'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：训练数据框
- en: 'After the transformation, you obtain a `grid_df` where the days have been distributed
    on separated days:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 变换之后，您将获得一个`grid_df`，其中日期被分配到单独的日期上：
- en: '![Figure 2.5: Applying pd.melt to the training DataFrame](img/file9.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5：将pd.melt应用于训练数据框](img/file9.png)'
- en: 'Figure 2.5: Applying pd.melt to the training DataFrame'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5：将pd.melt应用于训练数据框
- en: The feature d contains the reference to the columns that are not part of the
    index, in essence, all the features from `d_1` to `d_1935`. By simply removing
    the ‘d_’ prefix from its values and converting them to integers, you now have
    a day feature.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 d 包含了对不属于索引的列的引用，本质上，是从 `d_1` 到 `d_1935` 的所有特征。通过简单地从其值中移除‘d_’前缀并将它们转换为整数，你现在就有一个日特征。
- en: Apart from this, the code snippet also separates a holdout of the rows (your
    validation set) based on the time from the training ones. On the training part
    it will also add the rows necessary for your predictions based on the predict
    horizon you provide.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个之外，代码片段还根据时间从训练数据中分离出一部分行（你的验证集）。在训练部分，它还会根据你提供的预测范围添加必要的行来进行预测。
- en: 'Here is the function that creates our basic feature template. As input, it
    takes the `train_df` DataFrame, it expects the day the train ends and the predict
    horizon (the number of days you want to predict in the future):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建我们的基本特征模板的函数。作为输入，它接受 `train_df` DataFrame，它期望训练结束的日期和预测范围（你想要预测的未来天数）：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After handling the function to create the basic feature template, we prepare
    a merge function for pandas DataFrames that will help to save memory space and
    avoid memory errors when handling large sets of data. Given two DataFrame, df1
    and df2 and the set of foreign keys we need them to be merged, the function applies
    a left outer join between df1 and df2 without creating a new merged object but
    simply expanding the existent df1 DataFrame.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完创建基本特征模板的函数之后，我们准备了一个 pandas DataFrame 的合并函数，这有助于在处理大量数据时节省内存空间并避免内存错误。给定两个
    DataFrame，df1 和 df2 以及我们需要它们合并的外键集合，该函数在 df1 和 df2 之间应用左外连接，而不创建新的合并对象，只是简单地扩展现有的
    df1 DataFrame。
- en: The function works first by extracting the foreign keys from df1, then merging
    the extracted keys with df2\. In this way, the function creates a new DataFrame,
    called `merged_gf`, which is ordered as df1\. At this point, we just assign the
    `merged_gf` columsn to df1\. Internally, df1 will pick the reference to the internal
    data structures from `merged_gf`. Such an approach helps minimizing the memory
    usage because only the necessary used data is created at any time (there are no
    duplicates that can fill-up the memory). When the function returns df1, `merged_gf`
    is cancelled but for the data now used by df1.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先从 df1 中提取外键，然后将提取的键与 df2 合并。这样，该函数创建了一个新的 DataFrame，称为 `merged_gf`，其顺序与
    df1 相同。在这个时候，我们只是将 `merged_gf` 列分配给 df1。内部，df1 将从 `merged_gf` 中选择对内部数据结构的引用。这种做法有助于最小化内存使用，因为在任何时刻都只创建了必要使用的数据（没有可以填充内存的重复数据）。当函数返回
    df1 时，`merged_gf` 被取消，但 df1 现在使用的数据。
- en: 'Here is the code for this utility function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个实用函数的代码：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After this necessary step, we proceed to program a new function to process the
    data. This time we handle the prices data, a set of data containing the prices
    of each item by each store for all the weeks. Since it is important to figure
    out if we are talking about a new product appearing in a store or not, the function
    picks the first date of price availability (using the `wm_yr_wk` feature in the
    price table, representing the id of the week) and it copies it to our feature
    template.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这个必要的步骤之后，我们继续编写一个新的数据处理函数。这次我们处理价格数据，这是一组包含每个店铺每个商品所有周的价格的数据集。由于确定我们是否在谈论一个新商品出现在店铺中非常重要，该函数选择了价格可用的第一个日期（使用价格表中的
    `wm_yr_wk` 特征，代表周 id）并将其复制到我们的特征模板中。
- en: 'Here is the code for processing the release dates:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是处理发布日期的代码：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After having handles the day of the product appearance in a store, we definitely
    proceed to deal with the prices. In regard of each item, by each shop, we prepare
    basic price features telling:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完产品在店铺中出现的日期之后，我们肯定要继续处理价格。就每个商品而言，每个店铺，我们准备了一些基本的价格特征，告诉我们：
- en: the actual price (normalized by the maximum)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际价格（按最大值归一化）
- en: the maximum price
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最高价格
- en: the minimum price
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最低价格
- en: the mean price
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均价格
- en: the standard deviation of the price
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格的标准差
- en: the number of different prices the item has taken
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该商品所经历的不同价格数量
- en: the number of items in the store with the same price
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 店铺中具有相同价格的物品数量
- en: 'Besides these basic descriptive statistics of prices, we also add some features
    to describe their dynamics for each item in a store based on different time granularities:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本的价格描述性统计之外，我们还添加了一些特征来描述每个商品在不同时间粒度下的动态变化：
- en: the day momentum, i.e. the ratio before the actual price and its price the previous
    day
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日 momentum，即实际价格与其前一天价格的比例
- en: the month momentum, i.e. the ratio before the actual price and its average price
    the same month
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 月 momentum，即实际价格与其同月平均价格的比例
- en: the year momentum, i.e. the ratio before the actual price and its average price
    the same year
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年 momentum，即实际价格与其同年平均价格的比例
- en: 'Here we use two interesting and essential pandas methods for time series feature
    processing:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两个有趣且必要的pandas方法进行时间序列特征处理：
- en: '* shift : that can move the index forward or backward by n steps ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html))'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '* shift：可以将索引向前或向后移动n步（[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html)）'
- en: '* transform: that applied to a group by, fills a like-index feature with the
    transformed values ([https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html))'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '* transform：应用于group by，用转换后的值填充类似索引的特征（[https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html)）'
- en: 'In addition, the decimal part of the price is processed as a feature, in order
    to reveal a situation when the item is sold at psychological pricing thresholds
    (e.g. $19.99 or £2.98 – see this discussion: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011)).
    The function `math.modf` ([https://docs.python.org/3.8/library/math.html#math.modf](https://docs.python.org/3.8/library/math.html#math.modf))
    helps in doing so because it splits any floating-point number into the fractional
    and integer parts (a two-item tuple).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了揭示商品以心理定价阈值（例如$19.99或£2.98——参见此讨论：[https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011)）出售的情况，处理了价格的小数部分作为特征。`math.modf`函数（[https://docs.python.org/3.8/library/math.html#math.modf](https://docs.python.org/3.8/library/math.html#math.modf)）有助于这样做，因为它将任何浮点数分成小数部分和整数部分（一个两项元组）。
- en: Finally, the resulting table is saved onto disk.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，结果表被保存到磁盘上。
- en: 'Here is the function doing all the feature engineering on prices:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个函数对价格进行所有特征工程的过程：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The next function computes the moon phase, giving back one of its eight phases
    (from new moon to waning crescent). Although moon phases shouldn’t directly influence
    any sales (weather conditions instead do, but we have no weather information in
    the data), they represent a periodic cycle of 29 and a half days which can well
    suit periodic shopping behaviors. There is an interesting discussion, with different
    hypothesis regarding why moon phases may work as a predictor, in this competition
    post: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数计算月相，返回其八个阶段之一（从新月到亏月）。尽管月相不应直接影响任何销售（天气条件反而会，但我们没有数据中的天气信息），但它们代表了一个29天半的周期，这非常适合周期性购物行为。关于为什么月相可能作为预测因素的不同假设，在这次竞赛帖子中有有趣的讨论：[https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776)：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The moon phase function is part of a general function for creating time-based
    features. The function takes the calendar dataset information and places it among
    the features. Such information contains events and their type as well as indication
    of the SNAP periods (a nutrition assistance benefit called the Supplement Nutrition
    Assistance Program – hence SNAP – to help lower-income families) that could drive
    furthermore sales of basic goods. The function also generates numeric features
    such as the day, the month, the year, the day of the week, the week in the month,
    if it is an end of week. Here is the code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 月相函数是创建基于时间特征的通用函数的一部分。该函数接受日历数据集信息并将其放置在特征中。此类信息包含事件及其类型，以及指示SNAP时期（一种名为补充营养援助计划的营养援助福利——简称SNAP——以帮助低收入家庭）的信息，这些信息可能进一步推动基本商品的销售。该函数还生成诸如日期、月份、年份、星期几、月份中的星期以及是否为周末等数值特征。以下是代码：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The following function instead just removes the `wm_yr_wk` feature and transforms
    the d (day) feature into a numeric. It is a necessary step for the following feature
    transformation functions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数只是移除了`wm_yr_wk`特征，并将d（日）特征转换为数值。这是以下特征转换函数的必要步骤。
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our last two feature creation functions will generate more sophisticated feature
    engineering for time series. The first function will produce both lagged sales
    and their moving averages. Fist, using the shift method ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html))
    will generate a range of lagged sales up to 15 days in the past. Then using shift
    in conjunction with rolling ([https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html))
    will create moving means with windows 7, 14, 30, 60 and 180 days.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后两个特征创建函数将为时间序列生成更复杂的特征工程。第一个函数将生成滞后销售和它们的移动平均值。首先，使用移动方法（[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html)）将生成过去15天的滞后销售范围。然后使用移动方法结合滚动（[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html)）将创建7天、14天、30天、60天和180天的移动平均值。
- en: The shift command is necessary because it will allow moving the index so that
    you will always consider available data for your calculations. Hence, if your
    prediction horizon goes up to seven days, the calculations will consider only
    the data available seven days before. Then the rolling command will create a moving
    window observations that can be summarized (in this case by the mean). Having
    a mean over a period (the moving window) and following its evolutions will help
    you to detect better any changes in trends because patterns not repeating across
    the time windows will be leveled off. This a common strategy in time series analysis
    to remove noise and non-interesting patters. For instance, with a rolling mean
    of seven days you will cancel all the daily patterns and just represent what happens
    to your sales on a weekly basis.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 移动命令是必要的，因为它将允许移动索引，这样你就可以始终考虑可用于计算的数据。因此，如果你的预测范围达到七天，计算将只考虑七天前的数据。然后滚动命令将创建一个移动窗口观察值，可以总结（在这种情况下是通过平均值）。在一个时期（移动窗口）上有一个平均值，并跟踪其演变，将帮助你更好地检测趋势中的任何变化，因为不会在时间窗口中重复的图案将被平滑。这是时间序列分析中去除噪声和非有趣模式的一种常见策略。例如，使用七天的滚动平均值，你可以取消所有日间模式，仅表示你的销售在每周发生的情况。
- en: Can you experiment with different moving average windows? Also trying different
    strategies may help. For instance, by exploring the Tabular Playground of January
    2022 ([https://www.kaggle.com/competitions/tabular-playground-series-jan-2022](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022))
    devoted to time series you may find furthermore idea since most solutions are
    built using Gradient Boosting.
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以尝试不同的移动平均值窗口吗？尝试不同的策略可能会有所帮助。例如，通过探索2022年1月的Tabular Playground竞赛（[https://www.kaggle.com/competitions/tabular-playground-series-jan-2022](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022)），该竞赛致力于时间序列，你可能会找到更多的想法，因为大多数解决方案都是使用梯度提升构建的。
- en: 'Here is the code to generate the lag and rolling mean features:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成滞后和滚动平均值特征的代码：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As for as the second advanced feature engineering function, it is an encoding
    function, taking specific groupings of variables among state, store, category,
    department, and sold item and representing their mean and standard deviation.
    Such embeddings are time independent (time is not part of the grouping) and they
    have the role to help the training algorithm to distinguish how items, categories,
    and stores (and their combinations) differentiate among themselves.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 至于第二个高级特征工程函数，它是一个编码函数，它接受变量在州、商店、类别、部门和销售商品中的特定分组，并代表它们的平均值和标准差。这种嵌入是时间无关的（时间不是分组的一部分），并且它们的作用是帮助训练算法区分商品、类别和商店（及其组合）如何相互区分。
- en: The proposed embeddings are quite easy to compute using target encoding, as
    described in *The Kaggle Book* on page 216, can you obtain better results and
    how?
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如同《Kaggle 书》第216页所述，使用目标编码计算所提出的嵌入相当简单，你能获得更好的结果吗？
- en: 'The code works by grouping the features, computing their descriptive statistic
    (the mean or the standard deviation in our case) and then applying the results
    to the dataset using the transform ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html))
    method that we discussed before:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代码通过分组特征，计算它们的描述性统计（在我们的情况下是均值或标准差），然后使用我们之前讨论过的transform([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html))方法将结果应用于数据集：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Having completed the feature engineering part, we now proceed to put together
    all the files we have stored away on disk while generating the features. The following
    function just loads the different datasets of basic features, price features,
    calendar features, lag/rolling and embedded features, and concatenate all together.
    The code then filters only the rows relative to a specific shop to be saved as
    a separated dataset. Such an approach matches the strategy of having a model trained
    on a specific store aimed at predicting for a specific time interval:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了特征工程部分后，我们现在继续将存储在磁盘上的所有文件组合起来，同时生成特征。以下函数仅加载基本特征、价格特征、日历特征、滞后/滚动和嵌入特征的不同数据集，并将它们全部连接起来。然后代码仅过滤与特定商店相关的行，将其保存为单独的数据集。这种做法与针对特定商店训练模型以预测特定时间间隔的策略相匹配：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following function, instead, just further processes the selection from
    the previous one, by removing unused features and reordering the columns and it
    returns the data for a model to be trained on:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数，相反，只是进一步处理前一个选择，通过删除未使用的特征和重新排序列，并返回用于训练模型的数据：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we can now deal with the training phase. The following code snippet
    starts by defining the training parameters as explicated by Monsaraida being the
    most effective on the problem. For training time reasons, we just modified the
    boosting type, choosing using goss instead of gbdt because that can really speed
    up training without much loss in terms of performance. A good speed-up to the
    model is also provided by the subsample parameter and the feature fraction: at
    each learning step of the gradient boosting only half of the examples and half
    of the features will be considered.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们现在可以处理训练阶段了。以下代码片段首先定义了训练参数，正如Monsaraida所解释的，这是最有效的。由于训练时间的原因，我们只修改了提升类型，选择使用goss而不是gbdt，因为这可以在很大程度上加快训练速度，而不会在性能上损失太多。通过subsample参数和特征分数也可以为模型提供良好的加速：在梯度提升的每个学习步骤中，只有一半的示例和一半的特征将被考虑。
- en: 'Also compiling LightGBM on your machine with the right compiling options may
    increase your speed as explained in this interesting competition discussion: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273)'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此外，在您的机器上使用正确的编译选项编译LightGBM也可能提高您的速度，如在这篇有趣的竞赛讨论中所述：[https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273)
- en: The Tweedie loss, with a power value of 1.1 (hence with an underlying distribution
    quite similar to Poisson) seems particularly effective in modeling intermittent
    series (where zero sales prevail). The used metric is just the root mean squared
    error (there is no necessity to use a custom metric for representing the competition
    metric). We also use the `force_row_wise` parameter in order to save memory in
    the Kaggle notebook. All the other parameters are exactly the ones presented by
    Monsaraida in his solution (apart from the subsampling parameter that has been
    disabled because of its incompatibility with the goss boosting type).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Tweedie损失，其幂值为1.1（因此具有与泊松分布相当的基础分布），在建模间歇序列（零销售额占主导地位）时似乎特别有效。所使用的度量标准只是均方根误差（没有必要使用自定义度量标准来表示竞赛度量标准）。我们还使用`force_row_wise`参数在Kaggle笔记本中节省内存。所有其他参数都与Monsaraida在其解决方案中提出的参数完全相同（除了subsampling参数已被禁用，因为它与goss提升类型不兼容）。
- en: In what other Kaggle competition the Tweedie loss has proven useful? Can you
    find useful discussions about this loss and its usage in Meta Kaggle by exploring
    the ForumTopics and ForumMessages tables ([https://www.kaggle.com/datasets/kaggle/meta-kaggle](https://www.kaggle.com/datasets/kaggle/meta-kaggle))?
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Tweedie 损失在哪些 Kaggle 竞赛中已被证明是有用的？你能通过探索 ForumTopics 和 ForumMessages 表来找到关于这种损失及其在
    Meta Kaggle 中使用的有用讨论吗？([https://www.kaggle.com/datasets/kaggle/meta-kaggle](https://www.kaggle.com/datasets/kaggle/meta-kaggle))？
- en: After defining the training parameters, we just iterate over the stores, each
    time uploading the training data of a single store and training the LightGBM model.
    Each model is the pickled. We also extract feature importance from each model
    in order to consolidate it into a file and then aggregate it resulting into having
    for each feature the mean importance across all the stores for that prediction
    horizon.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义训练参数后，我们只需遍历各个商店，每次上传单个商店的训练数据并训练 LightGBM 模型。每个模型都是经过打包的。我们还从每个模型中提取特征重要性，以便将其合并到一个文件中，然后汇总，从而得到每个特征在该预测范围内的所有商店的平均重要性。
- en: 'Here is the complete function for training all the models for a specific prediction
    horizon:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是针对特定预测范围训练所有模型的完整函数：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With the last function prepared, we got all the necessary code up for our pipeline
    to work. For the function wrapping the whole operations together, we need the
    input datasets (the time series dataset, the price dataset, the calendar information)
    together with the last training day (1,913 for predicting on the public leaderboard,
    1,941 for the private one) and the predict horizon (which could be 7, 14, 21 or
    28 days).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好最后一个函数后，我们为我们的管道工作准备好了所有必要的代码。对于封装整个操作的功能，我们需要输入数据集（时间序列数据集、价格数据集、日历信息）以及最后训练日（对于公共排行榜预测为1,913，对于私有排行榜为1,941）和预测范围（可能是7、14、21或28天）。
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Since Kaggle notebook have a limited running time and a limited amount of both
    memory and disk space, our suggested strategy is to replicate four notebooks with
    the code hereby presented and train them with different prediction horizon parameters.
    Using the same name for the notebooks but for a part containing the value of the
    prediction parameter will help gathering and handling the models later as external
    datasets in another notebook.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kaggle 笔记本有有限的运行时间、有限的内存和磁盘空间，我们建议的策略是复制四个包含此处所示代码的笔记本，并使用不同的预测范围参数进行训练。使用相同的笔记本名称，但部分包含预测参数的值，有助于在另一个笔记本中将模型作为外部数据集收集和处理。
- en: 'Here is the first notebook, m5-train-day-1941-horizon-7 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7)):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个笔记本，m5-train-day-1941-horizon-7 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7))：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The second notebook, m5-train-day-1941-horizon-14 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14)):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个笔记本，m5-train-day-1941-horizon-14 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14))：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The third notebook, m5-train-day-1941-horizon-21 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21)):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个笔记本，m5-train-day-1941-horizon-21 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21))：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally the last one, m5-train-day-1941-horizon-28 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28)):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个是 m5-train-day-1941-horizon-28 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28))：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you are working on a local computer with enough disk space and memory resources,
    you can just run all the four prediction horizons together, by using as an input
    the list containing them all: [7, 14, 21, 28]. Now the last step before being
    able to submit our prediction is assembling the predictions.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一台具有足够磁盘空间和内存资源的本地计算机上工作，你可以一次性运行所有四个预测范围，输入包含它们的列表：[7, 14, 21, 28]。现在，在能够提交我们的预测之前，最后一步是组装预测。
- en: Assembling public and private predictions
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组装公共和私有预测
- en: 'You can see an example about how we assembled the predictions for both the
    public and private leaderboards here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到一个关于如何组装公共和私有排行榜预测的示例：
- en: 'Public leaderboard example: [https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard](https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共排行榜示例：[https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard](https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard)
- en: 'Private leaderboard example: [https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard](https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 私人排行榜示例：[https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard](https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard)
- en: 'What changes between the public and private submissions is just the different
    last training day: it determinates what days we are going to predict.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 公共和私人提交之间的变化只是不同的最后训练日：它决定了我们将预测哪些天。
- en: In this conclusive code snippet, after loading the necessary packages, such
    as LightGBM, for every end of training day, and for every prediction horizon,
    we recover the correct notebook with its data. Then, we iterate through all the
    stores and predict the sales for all the items in the time ranging from the previous
    prediction horizon up to the present one. In this way, every model will predict
    on a single week, the one it has been trained on.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个结论性的代码片段中，在加载必要的包，如LightGBM，对于每个训练结束日和每个预测范围，我们恢复正确的笔记本及其数据。然后，我们遍历所有商店，预测所有商品在从上一个预测范围到现在的范围内的销售额。这样，每个模型都将预测它所训练的单周。
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When all the predictions have been gatherd, we merge them using the sample submission
    file as a reference, both for the required rows to be predicted and for the columns
    format (Kaggle expects distinct rows for items in the validation or testing periods
    with daily sales in progressive columns).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有预测都收集完毕后，我们使用样本提交文件作为参考将它们合并，包括需要预测的行和列格式（Kaggle期望在验证或测试期间具有每日销售额的递进列的商品具有不同的行）。
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The solution can reach around 0.54907 in the private leaderboard, resulting
    in 12^(th) position, in the gold medal area. Reverting back to Monsaraida’s LightGBM
    parameters (for instance using gbdt instead of goss for the boosting parameter)
    should result even in higher performances (but you would need to run the code
    in local computer or on the Google Cloud Platform).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案在私人排行榜上可以达到大约0.54907，结果是第12位，位于金牌区域。恢复Monsaraida的LightGBM参数（例如，使用gbdt而不是goss作为提升参数）应该会带来更高的性能（但你需要在本地计算机或Google
    Cloud Platform上运行代码）。
- en: '**Exercise**'
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**练习**'
- en: ''
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an exercise, try comparing a training of LightGBM using the same number of
    iterations with the boosting set to gbdt instead of goss. How much is the difference
    in performance and training time (you may need to use a local machine or cloud
    one because the training may exceed the 12 hours)?
  id: totrans-166
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为练习，尝试比较使用相同迭代次数训练LightGBM，将提升集设置为gbdt而不是goss。性能和训练时间差异有多大（你可能需要使用本地机器或云机器，因为训练可能超过12小时）？
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this second chapter, we took on quite a complex time series competition,
    hence the easiest top solution we tried, it is actually fairly complex, and it
    requires coding quite a lot of processing functions. After you went through the
    chapter, you should have a better idea of how to process time series and have
    them predicted using gradient boosting. Favoring gradient boosting solutions over
    traditional methods, when you have enough data, as with this problem, should help
    you create strong solutions for complex problems with hierarchical correlations,
    intermittent series and availability of covariates such as events or prices or
    market conditions. In the following chapters, you will tackle with even more complex
    Kaggle competitions, dealing with images and texts. You will be amazed at how
    much you can learn by recreating top-scoring solutions and understanding their
    inner workings.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们面对了一场相当复杂的时间序列竞赛，因此我们尝试的最简单的前几名解决方案实际上相当复杂，它需要编写大量的处理函数。在你阅读完本章后，你应该对如何处理时间序列以及如何使用梯度提升进行预测有一个更好的理解。当数据量足够时，例如这个问题，优先考虑梯度提升解决方案而不是传统方法，应该有助于你为具有层次相关性、间歇序列以及事件、价格或市场条件等协变量可用的问题创建强大的解决方案。在接下来的章节中，你将面对更加复杂的数据竞赛，处理图像和文本。你将惊讶于通过重新创建得分最高的解决方案并理解其内部工作原理，你可以学到多少。
