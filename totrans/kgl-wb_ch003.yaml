- en: '2 The Makridakis Competitions: M5 on Kaggle for Accuracy and Uncertainty'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file1.png)'
  prefs: []
  type: TYPE_IMG
- en: Since 1982, Spyros Makridakis ([https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/](https://mofc.unic.ac.cy/dr-spyros-makridakis/https://mofc.unic.ac.cy/dr-spyros-makridakis/))
    has involved groups of researchers from all over the world in forecasting challenges,
    called M competitions, in order to conduct comparisons of the efficacy of existing
    and new forecasting methods against different forecasting problems. For this reason,
    M competitions have always been completely open to both academics and practitioners.
    The competitions are probably the most cited and referenced event in the forecasting
    community and they have always highlighted the changing state of the art in forecasting
    methods. Each previous M competition has provided both researchers and practitioners
    not only with useful data to train and test their forecasting tools, but also
    with a series of discoveries and approaches that are revolutionizing the way forecasting
    are done.
  prefs: []
  type: TYPE_NORMAL
- en: The recent M5 competition (the M6 is just running as this chapter is being written)
    has been held on Kaggle and it has proved particularly significant in remarking
    the usefulness of gradient boosting methods when trying to solve a host of volume
    forecasts of retail products. In this chapter, focusing on the accuracy track,
    we deal with a time series problem from Kaggle competitions, and by replicating
    one of the top, yet simplest and most clear solutions, we intend to provide our
    readers with code and ideas to successfully handle any future forecasting competition
    that may appear on Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the competition pages, we found a lot of information regarding the
    competition and its dynamics in the following papers from the International Journal
    of Forecasting:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. *The
    M5 competition: Background, organization, and implementation*. International Journal
    of Forecasting (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Makridakis, Spyros, Evangelos Spiliotis, and Vassilios Assimakopoulos. "M5
    accuracy competition: Results, findings, and conclusions." International Journal
    of Forecasting (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Makridakis, Spyros, et al. "The M5 Uncertainty competition: Results, findings
    and conclusions." International Journal of Forecasting (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the competition and the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The competition ran from March to June 2020 and over 7,000 participants took
    part in it on Kaggle. The organizers arranged it into two separated tracks, one
    for point-wise prediction (accuracy track) and another one for estimating reliable
    values at different confidence intervals (uncertainty track)
  prefs: []
  type: TYPE_NORMAL
- en: Walmart provided the data. It consisted of 42,840 daily sales time series of
    items hierarchically arranged into departments, categories, and stores spread
    in three U.S. states (the series are somewhat correlated each other). Along with
    the sales, Walmart also provided accompanying information (exogenous variables,
    usually not often provided in forecasting problems) such as the prices of items,
    some calendar information, associated promotions or presence of other events affecting
    the sales.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from Kaggle, the data is available, together with the datasets from previous
    M competition, at the address [https://forecasters.org/resources/time-series-data/](https://forecasters.org/resources/time-series-data/).
  prefs: []
  type: TYPE_NORMAL
- en: One interesting aspect of the competitions is that it dealt with consumer goods
    sales both fast moving and slow moving with many examples of the latest presenting
    intermittent sales (sales are often zero but for some rare cases). Intermittent
    series, though common in many industries, are still a challenging case in forecasting
    for many practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: The competition timeline has been arranged in two parts. In the first, from
    the beginning of March 2020 to June 1^(st), competitors could train models on
    the range of days up to day 1,913 and score their submission on the public test
    set (ranging from day 1,914 to 1,941). After that date, until the end of the competition
    on July 1^(st), the public test set was made available as part of the training
    set, allowing participants to tune their models in order to predict from day 1,942
    to 1969 (a time windows of 28 days, i.e. four weeks). In that period, submissions
    were not scored on the leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: The ratio behind such an arrangement of the competition was to allow teams initially
    to test their models on the leaderboard and to have grounds to share their best
    performing methods in notebooks and discussions. After the first phase, the organizers
    wanted to avoid having the leaderboard used for overfitting purposes or hyperparameter
    tuning of the models and they wanted to resemble a forecasting situation, as it
    would happen in the real world. In addition, the requirement to choose only one
    submission as the final one mirrored the same necessity for realism (in the real
    world you cannot use two distinct models predictions and choose the one that suits
    you the best afterwards).
  prefs: []
  type: TYPE_NORMAL
- en: 'As for as the data, we mentioned that the data has been provided by Walmart
    and it represents the USA market: it originated from 10 stores in California,
    Wisconsin and Texas. Specifically, the data it is made up by the sales of 3,049
    products, organized into three categories (hobbies, food, and household) that
    can be divided furthermore into 7 departments each. Such hierarchical structure
    is certainly a challenge because you can model sale dynamics at the level of USA
    market, state market, single store, product category, category department and
    finally specific product. All these levels can also combine as different aggregates,
    which are something required to be predicted in the second track, the uncertainty
    track:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Level id** | **Level description** | **Aggregation level** | **Number of
    series** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | All products, aggregated for all stores and states | Total | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | All products, aggregated for each state | State | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | All products, aggregated for each store | Store | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | All products, aggregated for each category | Category | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | All products, aggregated for each department | Department | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | All products, aggregated for each state and category | State-Category
    | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | All products, aggregated for each state and department | State-Department
    | 21 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | All products, aggregated for each store and category | Store-Category
    | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | All products, aggregated for each store and department | Store-Department
    | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Each product, aggregated for all stores/states | Product | 3,049 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Each product, aggregated for each state | Product-State | 9,147 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Each product, aggregated for each store | Product-Store | 30,490 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Total | 42,840 |'
  prefs: []
  type: TYPE_TB
- en: From the point of view of time, the granularity is daily sales record and the
    covered the period spanning from 29 January 2011 to 19 June 2016 which equals
    to 1,969 days in total, 1,913 for training, 28 for validation – public leaderboard
    – 28 for test – private leaderboard. A forecasting horizon of 28 days is actually
    recognized in the retail sector as the proper horizon for handling stocks and
    re-ordering operations for most goods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the different data you receive for the competition. You get `sales_train_evaluation.csv`,
    `sell_prices.csv` and `calendar.csv`. The one keeping the time series is `sales_train_evaluation.csv`.
    It is composed of fields that act as identifiers (`item_id`, `dept_id`, `cat_id`,
    `store_id`, and `state_id`) and columns from `d_1` to `d_1941` representing the
    sales of those days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: The sales_train_evaluation.csv data](img/file2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The sales_train_evaluation.csv data'
  prefs: []
  type: TYPE_NORMAL
- en: '`sell_prices.csv` contains instead information about the price of the items.
    The difficulty here is in joining the `wm_yr_wk` (the id of the week) with the
    columns in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: The sell_prices.csv data](img/file3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: The sell_prices.csv data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last file, `calendar.csv`, contains data relative to events that could
    have affected the sales:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: The calendar.csv data](img/file4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The calendar.csv data'
  prefs: []
  type: TYPE_NORMAL
- en: Again, the main difficulty seems in joining the data to the columns in the training
    table. Anyway, here you can get an easy key to connect columns (the d field) with
    the `wm_yr_wk`. In addition, in the table we have represented different events
    that may have occurred on particular days as well as SNAP days that are special
    days when the nutrition assistance benefits called the Supplement Nutrition Assistance
    Program (SNAP) can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Evaluation Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The accuracy competition introduced a new evaluation metric: Weighted Root
    Mean Squared Scaled Error (WRMSSE). The metric evaluates the deviation of the
    of the point forecasts around the mean of the realized values of the series being
    predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n* is the length of the training sample'
  prefs: []
  type: TYPE_NORMAL
- en: '*h* is the forecasting horizon (in our case it is *h*=28)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y*[*t*] is the sales value at time *t*,![](img/file6.png)is the predicted
    value at time *t*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the competition guidelines ([https://mofc.unic.ac.cy/m5-competition/](https://mofc.unic.ac.cy/m5-competition/)),
    in regard of WRMSSE, it is remarked that:'
  prefs: []
  type: TYPE_NORMAL
- en: The denominator of RMSSE is computed only for the time-periods for which the
    examined product(s) are actively sold, i.e., the periods following the first non-zero
    demand observed for the series under evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The measure is scale independent, meaning that it can be effectively used to
    compare forecasts across series with different scales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to other measures, it can be safely computed as it does not rely
    on divisions with values that could be equal or close to zero (e.g. as done in
    percentage errors when *Y*[*t*] = 0 or relative errors when the error of the benchmark
    used for scaling is zero).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The measure penalizes positive and negative forecast errors, as well as large
    and small forecasts, equally, thus being symmetric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good explanation of the underlying workings of this is provided by this post
    from Alexander Soare ([https://www.kaggle.com/alexandersoare](https://www.kaggle.com/alexandersoare)):
    [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273).
    After having transformed the evaluation metric, Alexandre attributes improving
    performances to improving the ratio between the error in the predictions and the
    day-to-day variation of sales values. If the error is the same as the daily variations
    (ratio=1), it is likely that the model is not much better than a random guess
    based on historical variations. If your error is less than that, it is score in
    a quadratic way (because of the square root) as it approaches zero. Consequently,
    a WRMSSE of 0.5 corresponds to a ratio of 0.7 and a WRMSSE of 0.25 corresponds
    to a ratio of 0.5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the competition, many attempts have been made at using the metric not
    only for evaluation besides the leaderboard but also as an objective function.
    First of all, the Tweedie loss (implemented both in XGBoost and LightGBM) worked
    quite well for the problem because it could handle the skewed distributions of
    sales for most products (a lot of them also had intermittent sales and that is
    also handled finely by the Tweedie loss). The Poisson and Gamma distributions
    can be considered extreme cases of the Tweedie distribution: based on the parameter
    power, p, with p=1 you get a Poisson distribution and with p=2 a Gamma one. Such
    power parameter is actually the glue that keeps the mean and the variance of the
    distribution connected by the formula variance = k*mean**p. Using a power value
    between 1 and 2, you actually get a mix of Poisson and Gamma distributions which
    can fit very well the competition problem. Most of the Kagglers involved in the
    competition and using a GBM solution, actually have resorted to Tweedie loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In spite of Tweedie success, some other Kagglers, however, found interesting
    ways to implement an objective loss more similar to WRMSSE for their models:'
  prefs: []
  type: TYPE_NORMAL
- en: '* Martin Kovacevic Buvinic with his asymmetric loss: [https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook](https://www.kaggle.com/code/ragnar123/simple-lgbm-groupkfold-cv/notebook)'
  prefs: []
  type: TYPE_NORMAL
- en: '* Timetraveller using PyTorch Autograd to get gradient and hessian for any
    differentiable continuous loss function to be implemented in LighGBM: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/152837)'
  prefs: []
  type: TYPE_NORMAL
- en: Examining the 4th place solution’s ideas from Monsaraida
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many solutions available for the competition, to be mostly found
    on the competition Kaggle discussions pages. The top five methods of both challenges
    have also been gathered and published (but one because of proprietary rights)
    by the competition organizers themselves: [https://github.com/Mcompetitions/M5-methods](https://github.com/Mcompetitions/M5-methods)
    (by the way, reproducing the results of the winning submissions was a prerequisite
    for the collection of a competition prize).'
  prefs: []
  type: TYPE_NORMAL
- en: Noticeably, all the Kagglers that have placed in the higher ranks of the competitions
    have used, as their unique model type or in blended/stacked in ensembles, LightGBM
    because of its lesser memory usage and speed of computations that gave it an advantage
    in the competition because of the large amount of times series to process and
    predict. But there are also other reasons for its success. Contrary to classical
    methods based on ARIMA, it doesn’t require relying on the analysis of auto-correlation
    and in specifically figuring out the parameters for each single series in the
    problem. In addition, contrary to methods based on deep learning, it doesn’t require
    looking for improving complicated neural architectures or tuning large number
    of hyperparameters. The strength of the gradient boosting methods in time series
    problems (for extension of every other gradient boosting algorithm, such as for
    instance XGBoost) is to rely on feature engineering, creating the right number
    of features based on time lags, moving averages and averages from groupings of
    the series attributes. Then choosing the right objective function and doing some
    hyper-parameter tuning will suffice to obtain excellent results when the time
    series are enough long (for shorter series, classical statistical methods such
    as ARIMA or exponential smoothing are still the recommended choice).
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of LightGBM and XGBoost against deep learning solutions in
    the competition was the Tweedie loss, not requiring any feature scaling (deep
    learning networks are particularly sensible to the scaling you use) and the speed
    of training that allowed faster iterations while testing feature engineering.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Among all these available solutions, we found the one proposed by Monsaraida
    (Masanori Miyahara), a Japanese computer scientist, the most interesting one.
    He has proposed a simple and straightforward solution that has ranked four on
    the private leaderboard with a score of 0.53583\. The solution uses just general
    features without prior selection (such as sales statistics, calendar, prices,
    and identifiers). Moreover, it uses a limited number of models of the same kind,
    using LightGBM gradient boosting, without resorting to any kind of blending, recursive
    modelling when predictions feed other hierarchically related predictions or multipliers
    that is choosing constants to fit the test set better. Here is a scheme taken
    from his presentation solution presentation to the M ([https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4](https://github.com/Mcompetitions/M5-methods/tree/master/Code%20of%20Winning%20Methods/A4))
    where it can be noted that he treats each of the ten stores by each of the four
    weeks to be looked into the future that in the end corresponds to producing 40
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: explanation by Monsaraida about the structure of his solution](img/file7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: explanation by Monsaraida about the structure of his solution'
  prefs: []
  type: TYPE_NORMAL
- en: Given that Monsaraida has kept his solution simple and practical, like in a
    real-world forecasting project, in this chapter, we will try to replicate his
    example by refactoring his code in order to run in Kaggle notebooks (we will handle
    the memory and the running time limitations by splitting the code into multiple
    notebooks). In this way, we intend to provide the readers with a simple and effective
    way, based on gradient boosting, to approach forecasting problems.
  prefs: []
  type: TYPE_NORMAL
- en: Computing predictions for specific dates and time horizons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The plan for replicating Monsaraida’s solution is to create a notebook customizable
    by input parameters in order to produce the necessary processed data for train
    and test and the LightGBM models for predictions. The models, given data in the
    past, will be trained to learn to predict values in a specific number of days
    in the future. The best results can be obtained by having each model to learn
    to predict the values in a specific week range in the future. Since we have to
    predict up to 28 days ahead in the future, we need a model predicting from day
    +1 to day +7 in the future, then another one able to predict from day +8 to day
    +14, another from day +15 to +21 and finally another last one capable of handling
    predictions from day +22 to day +28\. We will need a Kaggle notebook for each
    of these time ranges, thus we need four notebooks. Each of these notebooks will
    be trained to predict that future time span for each of the ten stores part of
    the competitions. In total, each notebook will produce ten models. All together,
    the notebooks will then produce forty models covering all the future range and
    all the stores.
  prefs: []
  type: TYPE_NORMAL
- en: Since we need to predict both for the public leaderboard and for the private
    one, it is necessary to repeat this process twice, stopping training at day 1,913
    (predicting days from 1,914 to 1,941) for the public test set submission and at
    day 1,941 (predicting days from 1,942 5o 1,969) for the private one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the current limitations for running Kaggle notebooks based on CPU, all
    these eight notebooks can be run in parallel (all the process taking almost 6
    hours and a half). Each notebook can be distinguishable by others by its name,
    containing the parameters’ values relative to the last training day and the look-ahead
    horizon in days. An example of one of these notebooks can be found at: [https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now examine together how the code has been arranged and what we can learn
    from Monsaraida’s solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply start by importing the necessary packages. You can just notice how,
    apart from NumPy and pandas, the only data science specialized package is LightGBM.
    You may also notice that we are going to use gc (garbage collection): that’s because
    we need to limit the amount of used memory by the script, and we frequently just
    collect and recycle the unused memory. As part of this strategy, we also frequently
    store away on disk models and data structures, instead of keeping them in-memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As part of the strategy to limit the memory usage, we resort to the function
    to reduce pandas DataFrame described in the Kaggle book and initially developed
    by Arjan Groen during the Zillow competition (read the discussion [https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844](https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/discussion/291844)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We keep on defining functions for this solution, because it helps splitting
    the solution into smaller parts and because it is easier to clean up all the used
    variables when you just return from a function (you keep only what you saved to
    disk, and you returned from the function). Our next function helps us to load
    all the data available and compress it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After preparing the code to retrieve the data relative to prices, volumes, and
    calendar information, we proceed to prepare the first processing function that
    will have the role to create a basic table of information having `item_id`, `dept_id`,
    `cat_id`, `state_id` and `store_id` as row keys, a day column and values column
    containing the volumes. This is achieved starting from rows having all the days’
    data columns by using the pandas command melt ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)).
    The command takes as reference the index of the DataFrame and then picks all the
    remaining features, placing their name on a column and their value on another
    one (`var_name` and `value_name` parameters help you define the name of these
    new columns). In this way, you can unfold a row representing the sales series
    of a certain item in a certain store into multiple rows each one representing
    a single day. The fact that the positional order of the unfolded columns is preserved
    guarantees that now your time series spans on the vertical axis (you can therefore
    apply furthermore transformations on it, such as moving means).
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you an idea of what is happening, here is the `train_df` before the
    transformation with `pd.melt`. Notice how the volumes of the distinct days are
    column features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: The training DataFrame](img/file8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: The training DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the transformation, you obtain a `grid_df` where the days have been distributed
    on separated days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Applying pd.melt to the training DataFrame](img/file9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Applying pd.melt to the training DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: The feature d contains the reference to the columns that are not part of the
    index, in essence, all the features from `d_1` to `d_1935`. By simply removing
    the ‘d_’ prefix from its values and converting them to integers, you now have
    a day feature.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, the code snippet also separates a holdout of the rows (your
    validation set) based on the time from the training ones. On the training part
    it will also add the rows necessary for your predictions based on the predict
    horizon you provide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function that creates our basic feature template. As input, it
    takes the `train_df` DataFrame, it expects the day the train ends and the predict
    horizon (the number of days you want to predict in the future):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After handling the function to create the basic feature template, we prepare
    a merge function for pandas DataFrames that will help to save memory space and
    avoid memory errors when handling large sets of data. Given two DataFrame, df1
    and df2 and the set of foreign keys we need them to be merged, the function applies
    a left outer join between df1 and df2 without creating a new merged object but
    simply expanding the existent df1 DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The function works first by extracting the foreign keys from df1, then merging
    the extracted keys with df2\. In this way, the function creates a new DataFrame,
    called `merged_gf`, which is ordered as df1\. At this point, we just assign the
    `merged_gf` columsn to df1\. Internally, df1 will pick the reference to the internal
    data structures from `merged_gf`. Such an approach helps minimizing the memory
    usage because only the necessary used data is created at any time (there are no
    duplicates that can fill-up the memory). When the function returns df1, `merged_gf`
    is cancelled but for the data now used by df1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for this utility function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After this necessary step, we proceed to program a new function to process the
    data. This time we handle the prices data, a set of data containing the prices
    of each item by each store for all the weeks. Since it is important to figure
    out if we are talking about a new product appearing in a store or not, the function
    picks the first date of price availability (using the `wm_yr_wk` feature in the
    price table, representing the id of the week) and it copies it to our feature
    template.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for processing the release dates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After having handles the day of the product appearance in a store, we definitely
    proceed to deal with the prices. In regard of each item, by each shop, we prepare
    basic price features telling:'
  prefs: []
  type: TYPE_NORMAL
- en: the actual price (normalized by the maximum)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the maximum price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the minimum price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the mean price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the standard deviation of the price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the number of different prices the item has taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the number of items in the store with the same price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides these basic descriptive statistics of prices, we also add some features
    to describe their dynamics for each item in a store based on different time granularities:'
  prefs: []
  type: TYPE_NORMAL
- en: the day momentum, i.e. the ratio before the actual price and its price the previous
    day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the month momentum, i.e. the ratio before the actual price and its average price
    the same month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the year momentum, i.e. the ratio before the actual price and its average price
    the same year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here we use two interesting and essential pandas methods for time series feature
    processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '* shift : that can move the index forward or backward by n steps ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '* transform: that applied to a group by, fills a like-index feature with the
    transformed values ([https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the decimal part of the price is processed as a feature, in order
    to reveal a situation when the item is sold at psychological pricing thresholds
    (e.g. $19.99 or £2.98 – see this discussion: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/145011)).
    The function `math.modf` ([https://docs.python.org/3.8/library/math.html#math.modf](https://docs.python.org/3.8/library/math.html#math.modf))
    helps in doing so because it splits any floating-point number into the fractional
    and integer parts (a two-item tuple).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the resulting table is saved onto disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function doing all the feature engineering on prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function computes the moon phase, giving back one of its eight phases
    (from new moon to waning crescent). Although moon phases shouldn’t directly influence
    any sales (weather conditions instead do, but we have no weather information in
    the data), they represent a periodic cycle of 29 and a half days which can well
    suit periodic shopping behaviors. There is an interesting discussion, with different
    hypothesis regarding why moon phases may work as a predictor, in this competition
    post: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/154776):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The moon phase function is part of a general function for creating time-based
    features. The function takes the calendar dataset information and places it among
    the features. Such information contains events and their type as well as indication
    of the SNAP periods (a nutrition assistance benefit called the Supplement Nutrition
    Assistance Program – hence SNAP – to help lower-income families) that could drive
    furthermore sales of basic goods. The function also generates numeric features
    such as the day, the month, the year, the day of the week, the week in the month,
    if it is an end of week. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The following function instead just removes the `wm_yr_wk` feature and transforms
    the d (day) feature into a numeric. It is a necessary step for the following feature
    transformation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our last two feature creation functions will generate more sophisticated feature
    engineering for time series. The first function will produce both lagged sales
    and their moving averages. Fist, using the shift method ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html))
    will generate a range of lagged sales up to 15 days in the past. Then using shift
    in conjunction with rolling ([https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html))
    will create moving means with windows 7, 14, 30, 60 and 180 days.
  prefs: []
  type: TYPE_NORMAL
- en: The shift command is necessary because it will allow moving the index so that
    you will always consider available data for your calculations. Hence, if your
    prediction horizon goes up to seven days, the calculations will consider only
    the data available seven days before. Then the rolling command will create a moving
    window observations that can be summarized (in this case by the mean). Having
    a mean over a period (the moving window) and following its evolutions will help
    you to detect better any changes in trends because patterns not repeating across
    the time windows will be leveled off. This a common strategy in time series analysis
    to remove noise and non-interesting patters. For instance, with a rolling mean
    of seven days you will cancel all the daily patterns and just represent what happens
    to your sales on a weekly basis.
  prefs: []
  type: TYPE_NORMAL
- en: Can you experiment with different moving average windows? Also trying different
    strategies may help. For instance, by exploring the Tabular Playground of January
    2022 ([https://www.kaggle.com/competitions/tabular-playground-series-jan-2022](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022))
    devoted to time series you may find furthermore idea since most solutions are
    built using Gradient Boosting.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here is the code to generate the lag and rolling mean features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As for as the second advanced feature engineering function, it is an encoding
    function, taking specific groupings of variables among state, store, category,
    department, and sold item and representing their mean and standard deviation.
    Such embeddings are time independent (time is not part of the grouping) and they
    have the role to help the training algorithm to distinguish how items, categories,
    and stores (and their combinations) differentiate among themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The proposed embeddings are quite easy to compute using target encoding, as
    described in *The Kaggle Book* on page 216, can you obtain better results and
    how?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The code works by grouping the features, computing their descriptive statistic
    (the mean or the standard deviation in our case) and then applying the results
    to the dataset using the transform ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transform.html))
    method that we discussed before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Having completed the feature engineering part, we now proceed to put together
    all the files we have stored away on disk while generating the features. The following
    function just loads the different datasets of basic features, price features,
    calendar features, lag/rolling and embedded features, and concatenate all together.
    The code then filters only the rows relative to a specific shop to be saved as
    a separated dataset. Such an approach matches the strategy of having a model trained
    on a specific store aimed at predicting for a specific time interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function, instead, just further processes the selection from
    the previous one, by removing unused features and reordering the columns and it
    returns the data for a model to be trained on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can now deal with the training phase. The following code snippet
    starts by defining the training parameters as explicated by Monsaraida being the
    most effective on the problem. For training time reasons, we just modified the
    boosting type, choosing using goss instead of gbdt because that can really speed
    up training without much loss in terms of performance. A good speed-up to the
    model is also provided by the subsample parameter and the feature fraction: at
    each learning step of the gradient boosting only half of the examples and half
    of the features will be considered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also compiling LightGBM on your machine with the right compiling options may
    increase your speed as explained in this interesting competition discussion: [https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273](https://www.kaggle.com/competitions/m5-forecasting-accuracy/discussion/148273)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Tweedie loss, with a power value of 1.1 (hence with an underlying distribution
    quite similar to Poisson) seems particularly effective in modeling intermittent
    series (where zero sales prevail). The used metric is just the root mean squared
    error (there is no necessity to use a custom metric for representing the competition
    metric). We also use the `force_row_wise` parameter in order to save memory in
    the Kaggle notebook. All the other parameters are exactly the ones presented by
    Monsaraida in his solution (apart from the subsampling parameter that has been
    disabled because of its incompatibility with the goss boosting type).
  prefs: []
  type: TYPE_NORMAL
- en: In what other Kaggle competition the Tweedie loss has proven useful? Can you
    find useful discussions about this loss and its usage in Meta Kaggle by exploring
    the ForumTopics and ForumMessages tables ([https://www.kaggle.com/datasets/kaggle/meta-kaggle](https://www.kaggle.com/datasets/kaggle/meta-kaggle))?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: After defining the training parameters, we just iterate over the stores, each
    time uploading the training data of a single store and training the LightGBM model.
    Each model is the pickled. We also extract feature importance from each model
    in order to consolidate it into a file and then aggregate it resulting into having
    for each feature the mean importance across all the stores for that prediction
    horizon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the complete function for training all the models for a specific prediction
    horizon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With the last function prepared, we got all the necessary code up for our pipeline
    to work. For the function wrapping the whole operations together, we need the
    input datasets (the time series dataset, the price dataset, the calendar information)
    together with the last training day (1,913 for predicting on the public leaderboard,
    1,941 for the private one) and the predict horizon (which could be 7, 14, 21 or
    28 days).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Since Kaggle notebook have a limited running time and a limited amount of both
    memory and disk space, our suggested strategy is to replicate four notebooks with
    the code hereby presented and train them with different prediction horizon parameters.
    Using the same name for the notebooks but for a part containing the value of the
    prediction parameter will help gathering and handling the models later as external
    datasets in another notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the first notebook, m5-train-day-1941-horizon-7 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-7)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The second notebook, m5-train-day-1941-horizon-14 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-14)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The third notebook, m5-train-day-1941-horizon-21 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-21)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally the last one, m5-train-day-1941-horizon-28 ([https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28](https://www.kaggle.com/code/lucamassaron/m5-train-day-1941-horizon-28)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are working on a local computer with enough disk space and memory resources,
    you can just run all the four prediction horizons together, by using as an input
    the list containing them all: [7, 14, 21, 28]. Now the last step before being
    able to submit our prediction is assembling the predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Assembling public and private predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see an example about how we assembled the predictions for both the
    public and private leaderboards here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Public leaderboard example: [https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard](https://www.kaggle.com/lucamassaron/m5-predict-public-leaderboard)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Private leaderboard example: [https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard](https://www.kaggle.com/code/lucamassaron/m5-predict-private-leaderboard)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What changes between the public and private submissions is just the different
    last training day: it determinates what days we are going to predict.'
  prefs: []
  type: TYPE_NORMAL
- en: In this conclusive code snippet, after loading the necessary packages, such
    as LightGBM, for every end of training day, and for every prediction horizon,
    we recover the correct notebook with its data. Then, we iterate through all the
    stores and predict the sales for all the items in the time ranging from the previous
    prediction horizon up to the present one. In this way, every model will predict
    on a single week, the one it has been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When all the predictions have been gatherd, we merge them using the sample submission
    file as a reference, both for the required rows to be predicted and for the columns
    format (Kaggle expects distinct rows for items in the validation or testing periods
    with daily sales in progressive columns).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The solution can reach around 0.54907 in the private leaderboard, resulting
    in 12^(th) position, in the gold medal area. Reverting back to Monsaraida’s LightGBM
    parameters (for instance using gbdt instead of goss for the boosting parameter)
    should result even in higher performances (but you would need to run the code
    in local computer or on the Google Cloud Platform).
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an exercise, try comparing a training of LightGBM using the same number of
    iterations with the boosting set to gbdt instead of goss. How much is the difference
    in performance and training time (you may need to use a local machine or cloud
    one because the training may exceed the 12 hours)?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this second chapter, we took on quite a complex time series competition,
    hence the easiest top solution we tried, it is actually fairly complex, and it
    requires coding quite a lot of processing functions. After you went through the
    chapter, you should have a better idea of how to process time series and have
    them predicted using gradient boosting. Favoring gradient boosting solutions over
    traditional methods, when you have enough data, as with this problem, should help
    you create strong solutions for complex problems with hierarchical correlations,
    intermittent series and availability of covariates such as events or prices or
    market conditions. In the following chapters, you will tackle with even more complex
    Kaggle competitions, dealing with images and texts. You will be amazed at how
    much you can learn by recreating top-scoring solutions and understanding their
    inner workings.
  prefs: []
  type: TYPE_NORMAL
