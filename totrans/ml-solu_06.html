<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 6. Job Recommendation Engine"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Job Recommendation Engine</h1></div></div></div><p>We have already seen how to develop a recommendation system for the e-commerce product in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation Systems  for e-Commerce</em></span>, Now, we will apply the same concepts that you learned in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>,<span class="emphasis"><em> Recommendation Systems  for e-Commerce</em></span> but the type and format of the dataset is different. Basically, we will build a job recommendation engine. For this application, we have taken into account the text dataset. The main concept of building the recommendation engine will not change, but this chapter gives you a fair idea of how to apply the same concepts to different types of datasets.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement</li><li class="listitem" style="list-style-type: disc">Understanding the datasets</li><li class="listitem" style="list-style-type: disc">Building the baseline approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the baseline approach</li><li class="listitem" style="list-style-type: disc">Understanding the testing matrix</li><li class="listitem" style="list-style-type: disc">Problems with the baseline approach</li><li class="listitem" style="list-style-type: disc">Optimizing the baseline approach  </li></ul></div></li><li class="listitem" style="list-style-type: disc">Building the revised approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the revised approach</li><li class="listitem" style="list-style-type: disc">Testing the revised approach</li><li class="listitem" style="list-style-type: disc">Problems with the revised approach</li><li class="listitem" style="list-style-type: disc">Understanding how to improve the revised approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the best approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><p>So, let's discuss the problem statement.</p><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec62"/>Introducing the problem statement</h1></div></div></div><p>In this<a id="id669" class="indexterm"/> chapter, we will build an engine that can recommend jobs to any user. This is the simplest goal we want to achieve. How we are going to build it? In order to answer this question, let me give you an idea about what kind of approaches we will take in order to build a job recommendation system.</p><p>For our baseline approach, we will scrape resumes of dummy users and try to build a job recommendation engine based on the scraped dataset. The reason we are scraping the dataset is that, most of the time, there will not be any dataset available for many data science applications. Suppose you are in a position where you have not found any dataset. What you will do then? I want to provide a solution for these kinds of scenarios. So, you will learn how to scrape the data and build the baseline solution.</p><p>In the revised approach, we will be using a dataset hosted by Kaggle. Using the content-based approach, we will be building a job recommendation engine. For the best approach, we will be using the concept of user-based collaborative filtering for this domain, and build the final job recommendation system.</p><p>Now, let's look into the datasets.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the datasets"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec63"/>Understanding the datasets</h1></div></div></div><p>Here, we are using <a id="id670" class="indexterm"/>two datasets. The two datasets are <a id="id671" class="indexterm"/>as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The scraped dataset</li><li class="listitem" style="list-style-type: disc">The job recommendation challenge dataset </li></ul></div><p>Let's start with the scraped dataset.</p><div class="section" title="Scraped dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec96"/>Scraped dataset</h2></div></div></div><p>For this dataset, we<a id="id672" class="indexterm"/> have scraped the dummy resume from indeed.com (we are using this data just for learning and research purposes). We will download the<a id="id673" class="indexterm"/> resumes of users in PDF format. These will become our dataset. The code for this is given at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py">https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/indeed_scrap.py</a>.</p><p>Take a look at the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_01.jpg" alt="Scraped dataset" width="649" height="596"/><div class="caption"><p>Figure 6.1: Code snippet for scraping the data</p></div></div><p>Using the preceding code, we<a id="id674" class="indexterm"/> can download the resumes. We have used the <code class="literal">requests </code>library and <code class="literal">urllib </code>to scrape the data. All these downloaded resumes are in PDF form, so we need to parse them. To parse the PDF document, we will <a id="id675" class="indexterm"/>use a Python library called <code class="literal">PDFminer</code>. We need to extract the following data attributes from <a id="id676" class="indexterm"/>the PDF documents: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Work experience</li><li class="listitem" style="list-style-type: disc">Education</li><li class="listitem" style="list-style-type: disc">Skills</li><li class="listitem" style="list-style-type: disc">Awards</li><li class="listitem" style="list-style-type: disc">Certifications</li><li class="listitem" style="list-style-type: disc">Additional information</li></ul></div><p>You can take a look at the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_02.jpg" alt="Scraped dataset" width="885" height="437"/><div class="caption"><p>Figure 6.2: Code snippet to parse PDF documents</p></div></div><p>Basically, <code class="literal">PDFminer </code>is converting <a id="id677" class="indexterm"/>the content of PDF into text. Once we have the text data using regular expressions, we can fetch the necessary details. You can see the entire code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py">https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/pdf_parse.py</a>.</p><p>Once we<a id="id678" class="indexterm"/> fetch all<a id="id679" class="indexterm"/> the necessary information, we will save the data in a pickle format. Now, you don't need to scrap the data and fetch all necessary information. I have uploaded the data into a pickle file format at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl">https://github.com/jalajthanaki/Basic_job_recommendation_engine/blob/master/resume_data.pkl</a>
</p><p>We will use the <code class="literal">resume_data.pkl</code> file for our baseline approach.</p></div><div class="section" title="Job recommendation challenge dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec97"/>Job recommendation challenge dataset</h2></div></div></div><p>This <a id="id680" class="indexterm"/>dataset is provided by <a class="ulink" href="http://www.careerbuilder.com">www.careerbuilder.com</a> and is hosted on Kaggle. You can download the dataset using this link: <a class="ulink" href="https://www.kaggle.com/c/job-recommendation/data">https://www.kaggle.com/c/job-recommendation/data</a>. These are the data files that we are <a id="id681" class="indexterm"/>going to use for our revised and best approach. All the values given in these datafiles are tab-separated:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">apps.tsv</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">users.tsv</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">jobs.zip</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">user_history.tsv</code></li></ul></div></div><div class="section" title="apps.tsv"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec98"/>apps.tsv</h2></div></div></div><p>This datafile contains <a id="id682" class="indexterm"/>the records of users' job <a id="id683" class="indexterm"/>applications. It indicates job positions that a particular user has applied for. The job position is described by the JobID column. All the necessary information about this datafile is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_03.jpg" alt="apps.tsv" width="722" height="593"/><div class="caption"><p>Figure 6.3: Data information about apps.tsv</p></div></div><p>There are <a id="id684" class="indexterm"/>five data columns:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">UserId</code>: This indicates<a id="id685" class="indexterm"/> the unique ID for a given user. By using this ID, we can access the user's profile.</li><li class="listitem" style="list-style-type: disc"><code class="literal">WindowsID</code>: This is<a id="id686" class="indexterm"/> the mask data attribute with the constant value of 1. This data attribute is not important for us.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Split</code>: This data attribute<a id="id687" class="indexterm"/> indicates which data records we should consider for training and testing.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Application date</code>: This<a id="id688" class="indexterm"/> is the timestamp at which the user applied for the job.</li><li class="listitem" style="list-style-type: disc"><code class="literal">JobID</code>: This attribute <a id="id689" class="indexterm"/>indicates the <code class="literal">JobIds </code>for which the user nominates themselves. Using this <code class="literal">JobId</code>, we can access other information about a particular job.</li></ul></div></div><div class="section" title="users.tsv"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec99"/>users.tsv</h2></div></div></div><p>This datafile <a id="id690" class="indexterm"/>contains the user profile and all <a id="id691" class="indexterm"/>user-related information. You can find all the necessary information displayed in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_04.jpg" alt="users.tsv" width="856" height="553"/><div class="caption"><p>Figure 6.4 Data information about users.tsv</p></div></div><p>These are the data attributes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">UserID</code>: This <a id="id692" class="indexterm"/>data attribute indicates the user's unique identification number.</li><li class="listitem" style="list-style-type: disc"><code class="literal">WindowID</code>: This is the <a id="id693" class="indexterm"/>mask data attribute with a constant value of 1. This data attribute is not important for us.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Split</code>: This data <a id="id694" class="indexterm"/>attribute indicates which data records we should consider for training and testing.</li><li class="listitem" style="list-style-type: disc"><code class="literal">City</code>: This <a id="id695" class="indexterm"/>attribute indicates the user's current city.</li><li class="listitem" style="list-style-type: disc"><code class="literal">State</code>: This <a id="id696" class="indexterm"/>attribute indicates the user's state.</li><li class="listitem" style="list-style-type: disc">Country: This <a id="id697" class="indexterm"/>attribute indicates the user's country.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ZipCode</code>: This data<a id="id698" class="indexterm"/> attribute indicates the user's ZIP code.</li><li class="listitem" style="list-style-type: disc"><code class="literal">DegreeType</code>: This data <a id="id699" class="indexterm"/>column indicates the user's degree; whether the user is a high school pass-out or has a bachelor's degree.            </li><li class="listitem" style="list-style-type: disc"><code class="literal">Major</code>: This<a id="id700" class="indexterm"/> data attribute indicates the major subject in which the user has a degree.</li><li class="listitem" style="list-style-type: disc"><code class="literal">GraduationDate</code>: This<a id="id701" class="indexterm"/> data attribute indicates the graduation date of the user.      </li><li class="listitem" style="list-style-type: disc"><code class="literal">WorkHistoryCount</code>: This <a id="id702" class="indexterm"/>data attribute indicates the number of companies the user has worked for.        </li><li class="listitem" style="list-style-type: disc"><code class="literal">TotalYearsExperience</code>: This<a id="id703" class="indexterm"/> data column indicates the user's total years of experience.</li><li class="listitem" style="list-style-type: disc"><code class="literal">CurrentlyEmployed</code>: This <a id="id704" class="indexterm"/>data attribute has a binary value. If the user is currently employed, then the value is <span class="emphasis"><em>Yes</em></span>; if not, then the value is <span class="emphasis"><em>No</em></span>.      </li><li class="listitem" style="list-style-type: disc"><code class="literal">ManagedOthers</code>: This <a id="id705" class="indexterm"/>data attribute<a id="id706" class="indexterm"/> has a binary value as well. If the user is managing other people, then the value of this column is <span class="emphasis"><em>Yes</em></span>; if the user is not managing other people, then the value of this column is <span class="emphasis"><em>No</em></span>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ManagedHowMany</code>: This <a id="id707" class="indexterm"/>data attribute has a numerical value. The value of this column indicates the number of people that are managed by the user. If the user is not managing anyone, then the value is 0.</li></ul></div></div><div class="section" title="Jobs.zip"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec100"/>Jobs.zip</h2></div></div></div><p>When you<a id="id708" class="indexterm"/> extract this ZIP file, you can <a id="id709" class="indexterm"/>get the <code class="literal">jobs.tsv </code>file. There is more information available in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_05.jpg" alt="Jobs.zip" width="685" height="543"/><div class="caption"><p>Figure 6.5: Data information about jobs.tsv</p></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">JobID</code>: This is the<a id="id710" class="indexterm"/> unique ID for each job present in the dataset.</li><li class="listitem" style="list-style-type: disc"><code class="literal">WindowID</code>: This is<a id="id711" class="indexterm"/> the mask data attribute that has a constant value of 1. This data attribute is not important for us.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Title</code>: This<a id="id712" class="indexterm"/> data attribute indicates the job title.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Description</code>: This<a id="id713" class="indexterm"/> data attribute indicates the job description.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Requirements</code>: This <a id="id714" class="indexterm"/>data attribute indicates the job requirements.</li><li class="listitem" style="list-style-type: disc"><code class="literal">City</code>: This data<a id="id715" class="indexterm"/> field indicates the job location in terms of the city.</li><li class="listitem" style="list-style-type: disc"><code class="literal">State</code>: This <a id="id716" class="indexterm"/>data field indicates the job location in terms of the state.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Country</code>: This <a id="id717" class="indexterm"/>data field indicates the job location in terms of the country.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Zip5</code>: This<a id="id718" class="indexterm"/> data field indicates the ZIP code of the job location.</li><li class="listitem" style="list-style-type: disc"><code class="literal">StartDate</code>: This<a id="id719" class="indexterm"/> date indicates when the job is posted or is open for applications.</li><li class="listitem" style="list-style-type: disc"><code class="literal">EndDate</code>: This<a id="id720" class="indexterm"/> date is the deadline for the job application.</li></ul></div></div><div class="section" title="user_history.tsv"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec101"/>user_history.tsv</h2></div></div></div><p>The <code class="literal">user_history.tsv</code> file <a id="id721" class="indexterm"/>contains <a id="id722" class="indexterm"/>the user's job history. There is more information available on this in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_06.jpg" alt="user_history.tsv" width="691" height="382"/><div class="caption"><p>Figure 6.6: Data information about user_history.tsv</p></div></div><p>There are only two new columns for this datafile.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Sequence</code>: This<a id="id723" class="indexterm"/> sequence is a numerical field. The number indicates the sequential order of the user's job.  </li><li class="listitem" style="list-style-type: disc"><code class="literal">JobTitle</code>: This <a id="id724" class="indexterm"/>data field indicates the job title of the user.</li></ul></div><p>We have covered all the attributes in our datafiles; now let's start building the baseline approach.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the baseline approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec64"/>Building the baseline approach</h1></div></div></div><p>In this section, we <a id="id725" class="indexterm"/>will be building the baseline approach. We will use the scraped dataset. The main approach we will be <a id="id726" class="indexterm"/>using is TF-IDF (Term-frequency, Inverse Document Frequency) and cosine similarity. Both of these concepts have already been described in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation System for e-commerce.</em></span> The name of the pertinent sections are <span class="emphasis"><em>Generating features using TF-IDF</em></span> and <span class="emphasis"><em>Building the cosine similarity matrix</em></span>.</p><p>As this application has more textual data, we can use TF-IDF, CountVectorizers, cosine similarity, and so on. There are no ratings available for any job. Because of this, we are not using other matrix decomposition methods, such as SVD, or correlation coefficient-based methods, such as Pearsons'R correlation.</p><p>For the baseline approach, we are trying to find out the similarity between the resumes, because that is how we will know how similar the user profiles are. By using this fact, we can recommend jobs to all the users who share a similar kind of professional profile. For the baseline model, our context is to generate the similarity score between the resumes.</p><div class="section" title="Implementing the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec102"/>Implementing the baseline approach</h2></div></div></div><p>In order to develop a<a id="id727" class="indexterm"/> simple job recommendation system, we need to perform the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Defining constants</li><li class="listitem" style="list-style-type: disc">Loading the dataset</li><li class="listitem" style="list-style-type: disc">Defining the helper function</li><li class="listitem" style="list-style-type: disc">Generating <a id="id728" class="indexterm"/>TF-IDF vectors and cosine similarity </li></ul></div><div class="section" title="Defining constants"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec75"/>Defining constants</h3></div></div></div><p>We will define <a id="id729" class="indexterm"/>some constant values. These values are based on the dataset we have scraped. In our dataset, we have scraped the dummy resumes for seven companies, and there are seven data attributes that we have generated by parsing the resumes. We consider 100 resumes as our first training dataset and 50 resumes as our testing dataset. The size of our second training dataset is 50. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_07.jpg" alt="Defining constants" width="839" height="349"/><div class="caption"><p>Figure 6.7: Code snippet for defining constants</p></div></div><p>After this step, we will load the dataset.</p></div><div class="section" title="Loading the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec76"/>Loading the dataset</h3></div></div></div><p>As you know, we <a id="id730" class="indexterm"/>have already parsed the resumes that are in the PDF file format. We store the parsed data into the pickle format, and we need to load that pickle file. We will use the <code class="literal">dill</code> library to load the pickle file. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_08.jpg" alt="Loading the dataset" width="668" height="171"/><div class="caption"><p>Figure 6.8: Code snippet for loading the dataset</p></div></div><p>We have <a id="id731" class="indexterm"/>restored the dataset. As the next step, we need to define the functions so that we can build a basic job recommendation system.</p></div><div class="section" title="Defining the helper function"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec77"/>Defining the helper function</h3></div></div></div><p>There are <a id="id732" class="indexterm"/>various helper functions that will be useful for us. There are a total of three helper functions for this approach: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">my_normalize</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">get_sim_vector</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">get_class</code></li></ul></div><p>The first function is used to normalize the testing score. We will get the testing score in the form of a matrix. You can take a look at the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_09.jpg" alt="Defining the helper function" width="311" height="234"/><div class="caption"><p>Figure 6.9: Code snippet for helper function my_normalize</p></div></div><p>This normalization<a id="id733" class="indexterm"/> is nothing but the weighted average of the testing score matrix. So, it takes the testing score matrix and generates the normalized testing score matrix. Bear with me for a while; we will see what the testing score matrix looks like when we generate the result of this baseline approach.</p><p>The second function basically takes the TF-IDF vector matrix and dataset as an input. As an output, it generates the cosine similarity score. You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_10.jpg" alt="Defining the helper function" width="395" height="100"/><div class="caption"><p>Figure 6.10: Code snippet for helper function get_sim_vector</p></div></div><p>The third function basically takes the cosine similarity array as an input and iterates through it in order to get the maximum cosine value from the cosine similarity array. You can find the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_11.jpg" alt="Defining the helper function" width="257" height="178"/><div class="caption"><p>Figure 6.11: Code snippet for helper function get_class</p></div></div><p>We have understood the input, output, and work of our helper functions. Now, it's time to see their usage <a id="id734" class="indexterm"/>when we generate TF-IDF vectors and the cosine similarity. So, let's move on to the next section.</p></div><div class="section" title="Generating TF-IDF vectors and cosine similarity"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec78"/>Generating TF-IDF vectors and cosine similarity</h3></div></div></div><p>In this section, we<a id="id735" class="indexterm"/> will be developing the core logic of the baseline approach. We will be using a simple TF-IDF concept. In order to build the <a id="id736" class="indexterm"/>job recommendation engine using simple TF-IDF, we need to perform the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Building the training dataset</li><li class="listitem" style="list-style-type: disc">Generating IF-IDF vectors for the training dataset</li><li class="listitem" style="list-style-type: disc">Building the testing dataset</li><li class="listitem" style="list-style-type: disc">Generating the similarity score</li></ul></div><p>Let's build the training dataset.</p><div class="section" title="Building the training dataset"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec45"/>Building the training dataset</h4></div></div></div><p>Basically, we have <a id="id737" class="indexterm"/>not divided our dataset into training and testing. So, for training, we need to generate the training dataset by using the code snippet that is shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_12.jpg" alt="Building the training dataset" width="348" height="159"/><div class="caption"><p>Figure 6.12: Code snippet for generating the training dataset</p></div></div><p>The code is simple to understand. As you can see, we have used the <code class="literal">train1_size</code> constant value, which we have defined earlier, so that we can generate 100 resumes that can be used for training purposes.</p><p>Now, let's move on to the next step.</p></div><div class="section" title="Generating IF-IDF vectors for the training dataset"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec46"/>Generating IF-IDF vectors for the training dataset</h4></div></div></div><p>In order to <a id="id738" class="indexterm"/>generate TF-IDF vectors, we will be using scikit-learn's <code class="literal">TfidfVectorizer </code>API. This basically converts our text data into a numerical format. You can take a look at the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_13.jpg" alt="Generating IF-IDF vectors for the training dataset" width="447" height="58"/><div class="caption"><p>Figure 6.13: Code snippet for generating TF-IDF</p></div></div><p>By using the preceding code, we can convert our textual training dataset into a vectorized format. The matrix of the TF-IDF is used when we generate the predictions for testing the dataset. Now, let's build the testing dataset.</p></div><div class="section" title="Building the testing dataset"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec47"/>Building the testing dataset</h4></div></div></div><p>We have<a id="id739" class="indexterm"/> trained the model. Now, we need to build the test dataset so that we can check how well or how badly our trained model is performing on the test dataset. We have used 100 resumes from our dataset for training purposes, so now, we need to use the resumes that are not the part of the training dataset. In order to generate the testing dataset, we will execute the following code so that we can generate the test dataset. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_14.jpg" alt="Building the testing dataset" width="365" height="196"/><div class="caption"><p>Figure 6.14: Code snippet for generating the testing dataset</p></div></div><p>As you can see, we have generated the test dataset using the index of the resume, and have taken only those documents that are not a part of the training.</p></div><div class="section" title="Generating the similarity score"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec48"/>Generating the similarity score</h4></div></div></div><p>In this<a id="id740" class="indexterm"/> section, first, we will take the test dataset as an input and generate the TF-IDF vectors for them. Once the TF-IDF vector matrix has been generated, we will use the cosine similarity API in order to generate the similarity score. For this API, we will pass the two TF-IDF matrices. One matrix is what we recently generated using the testing dataset, and the second matrix is what we generated using the training dataset. When we pass these two matrices, we will get the cosine similarity array as the output. You can refer to the code snippet given in the following <a id="id741" class="indexterm"/>screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_15.jpg" alt="Generating the similarity score" width="462" height="273"/><div class="caption"><p>Figure 6.15: Code snippet for generating a cosine similarity for the testing dataset</p></div></div><p>As an output, we can generate the cosine similarity array displayed in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_16.jpg" alt="Generating the similarity score" width="694" height="132"/><div class="caption"><p>Figure 6.16: Cosine similarity array</p></div></div><p>The array shown in the preceding screenshot has seven elements. Each element indicates the similarity of that resume for seven companies. So, if the highest cosine value appears in the 0th index, then it means that the given resume is more similar to resumes of other users who are working at Amazon. So, we will recommend a job opening at Amazon<a id="id742" class="indexterm"/> to that particular user, as their resume is more similar to other employees who are working at Amazon.</p><p>Now, let's explore some facts related to the testing matrix.</p></div></div></div><div class="section" title="Understanding the testing matrix"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec103"/>Understanding the testing matrix</h2></div></div></div><p>When we <a id="id743" class="indexterm"/>build a recommendation engine using TF-IDF, count vectorizer, and cosine similarity, we are actually building the content-based recommendation engine. There is no predefined testing matrix available for generating the accuracy score. In this case, either we need to check our recommendations relevance manually, or we can take a heuristic to get the basic intuitive score. In <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation Systems  for E-Commerce</em></span>,  for the baseline approach, we implemented some basic threshold-based heuristics to get a basic idea of how well the recommendation engine was working. I suggest that you refer to the <span class="emphasis"><em>Test the result of baseline approach</em></span> section in <a class="link" href="ch04.xhtml" title="Chapter 4. Recommendation Systems for E-Commerce">Chapter 4</a>, <span class="emphasis"><em>Recommendation Engine for e-commerce</em></span>.</p></div><div class="section" title="Problems with the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec104"/>Problems with the baseline approach</h2></div></div></div><p>There are a number of problems with <a id="id744" class="indexterm"/>the baseline approach. I will list all of them one by one:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are not enough data attributes available in the dataset to build a good job recommendation system.</li><li class="listitem" style="list-style-type: disc">The baseline approach can't really provide accurate job recommendations, because we have the dataset of user resumes only, and based on that, we can just say something like "your resume will look like other employees at Amazon, so please apply for job openings at Amazon". Now, the problem is identifying the kind of jobs we need to recommend to the user: whether we should recommend all job openings at Amazon, or some of them.</li><li class="listitem" style="list-style-type: disc">In my opinion, the baseline solution is not able to provide us the complete picture, because of the quality and quantity of the dataset.</li></ul></div><p>The solution for these problems will be discussed in the next section.</p></div><div class="section" title="Optimizing the baseline approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec105"/>Optimizing the baseline approach</h2></div></div></div><p>In the previous<a id="id745" class="indexterm"/> section, we listed the shortcomings of the baseline approach. In this section, we will look at how we can overcome these shortcomings. We are facing a major problem because we did not use appropriate quality and quantity for the dataset. So, first of all, we need to use the dataset in which we have information about users' profiles as well as information about the job openings. Here, we are not scraping more resumes or posting information about jobs anymore. We are using the dataset released by the career builder. We have already seen basic information about this dataset earlier in this chapter.</p><p>To build the revised approach, we will use this new dataset. Now, let's start building the revised approach.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec65"/>Building the revised approach</h1></div></div></div><p>In this section, we <a id="id746" class="indexterm"/>will be using the readily <a id="id747" class="indexterm"/>available job recommendation challenge dataset. We have already covered the data attributes of this dataset. We will be using a context-based approach to build the recommendation engine. In order to build the revised approach, we need to perform the following steps. The code for the revised approach is given at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb">https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb</a></p><p>Let's implement the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Loading the dataset</li><li class="listitem" style="list-style-type: disc">Splitting the training and testing datasets</li><li class="listitem" style="list-style-type: disc">Exploratory data analysis </li><li class="listitem" style="list-style-type: disc">Building the recommendation engine using the jobs datafile</li></ul></div><div class="section" title="Loading the dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec106"/>Loading the dataset</h2></div></div></div><p>As you know, the <a id="id748" class="indexterm"/>dataset is in various files. We need to load all these files. Remember that all the datafiles are in a <code class="literal">.tsv</code> format, so we need to use the <code class="literal">\t</code> delimiter as a parameter. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_17.jpg" alt="Loading the dataset" width="867" height="600"/><div class="caption"><p>Figure 6.17: Code snippet for loading the dataset</p></div></div><p>As you can see, we <a id="id749" class="indexterm"/>have used the pandas <code class="literal">read_csv</code> method with the delimiter as a parameter, and loaded the dataset in the form of five different dataframes.</p></div><div class="section" title="Splitting the training and testing datasets"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec107"/>Splitting the training and testing datasets</h2></div></div></div><p>There are <a id="id750" class="indexterm"/>three data files in which training and testing both types of data records is present. These dataframes are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">apps</li><li class="listitem" style="list-style-type: disc">user_history</li><li class="listitem" style="list-style-type: disc">users</li></ul></div><p>In the preceding dataframes, some records are tagged as <code class="literal">Train </code>and some records are tagged as <code class="literal">Test</code>. The data attribute <code class="literal">Split </code>indicates which data records are considered a part of the training dataset and which ones are used for testing. So, we need to filter our dataset. You can take a look at the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_18.jpg" alt="Splitting the training and testing datasets" width="650" height="113"/><div class="caption"><p>Figure 6.18: Code snippet for splitting the training and testing datasets</p></div></div><p>We have <a id="id751" class="indexterm"/>applied a simple filter operation for all three dataframes and stored their output in new dataframes.</p><p>Now, let's move on to the <span class="strong"><strong>Exploratory Data Analysis</strong></span> (<span class="strong"><strong>EDA</strong></span>) section.</p></div><div class="section" title="Exploratory Data Analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec108"/>Exploratory Data Analysis</h2></div></div></div><p>In this section, we <a id="id752" class="indexterm"/>will be performing some basic analysis so that we can find out what kind of data is present in our dataset. For the<a id="id753" class="indexterm"/> revised approach, we are building the recommendation system using data attributes given in the jobs dataframe. So, before using it to build the recommendation engine, we will check the quality of the data records. We need to check whether any blank values are present in the dataset. Apart from that, we also need to check the data distribution of this dataframe.</p><p>We will perform EDA specifically on geo-location data attributes. Here, we have performed grouping by operation on three data columns: City, State, and Country. You can take a look at the code snippet given in the following screenshot:</p><p> </p><div class="mediaobject"><img src="Images/B08394_06_19.jpg" alt="Exploratory Data Analysis" width="689" height="392"/><div class="caption"><p>Figure 6.19: Grouping by operation on City, State, and Country</p></div></div><p>
</p><p>As you <a id="id754" class="indexterm"/>can see in the code snippet, there are many records where the state name is not present. We need to take care of them.</p><p>Apart from this, we <a id="id755" class="indexterm"/>also need to count the data records country-wise so that we can find out how many data records are present for each country. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_20.jpg" alt="Exploratory Data Analysis" width="875" height="234"/><div class="caption"><p>Figure 6.20: Code snippet for counting data records country-wise</p></div></div><p>As you can see in the preceding code snippet, there approximately 1 million jobs from the US region. We can say that in our dataset, the country location for most of the jobs is the US. To make <a id="id756" class="indexterm"/>our life easy, we are just considering jobs where the country is the US. You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_21.jpg" alt="Exploratory Data Analysis" width="373" height="546"/><div class="caption"><p>Figure 6.21: Code snippet for all the data records where the country is the US</p></div></div><p>Here, we need to check whether there is an empty data value present for the city or state data columns. After<a id="id757" class="indexterm"/> observing the output of the preceding code, we can see that there are no data records where the city or state name is missing.</p><p>Now, let's look at <a id="id758" class="indexterm"/>the state for which we have maximum job openings. Remember that we have considered only those jobs where the country location is the US. In order to find out the number of jobs state-wise, you can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_22.jpg" alt="Exploratory Data Analysis" width="1000" height="646"/><div class="caption"><p>Figure 6.22: Code snippet for generating state-wise number of jobs</p></div></div><p>You can also refer to the graph shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_23.jpg" alt="Exploratory Data Analysis" width="792" height="775"/><div class="caption"><p>Figure 6.23: Graph for state-wise number of jobs</p></div></div><p>As you can see, maximum<a id="id759" class="indexterm"/> job opportunities <a id="id760" class="indexterm"/>are available in California, Texas, Florida, Illinois, and New York. We have done enough EDA for the revised approach. Now, we are going to start building the recommendation engine.</p></div><div class="section" title="Building the recommendation engine using the jobs datafile"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec109"/>Building the recommendation engine using the jobs datafile</h2></div></div></div><p>In this section, we will explore the code to see how we can build a job recommendation engine. We will use TF-IDF and <a id="id761" class="indexterm"/>cosine similarity concepts in order to build the recommendation engine.</p><p>We have taken the <code class="literal">jobs_US dataframe</code> into account here. This dataframe contains jobs where the country is the US. So, we don't have any junk data records. We will be considering only 10,000 data records for training because training for 1 million data records is time consuming. You can refer to the code shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_24.jpg" alt="Building the recommendation engine using the jobs datafile" width="583" height="228"/><div class="caption"><p>Figure 6.24: Code snippet of the jobs dataset to build the revised approach</p></div></div><p>Here, we will be focusing on the job title and job description in order to build the recommendation engine. As we are using the metadata of jobs, this is the content-based approach. We apply concatenation operation to the job title and job descriptions, as well as replace <code class="literal">nan value</code> with an empty string value. You can refer to the code given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_25.jpg" alt="Building the recommendation engine using the jobs datafile" width="747" height="88"/><div class="caption"><p>Figure 6.25: Code snippet for applying the concatenation operation</p></div></div><p>Now, we will generate the TF-IDF vectors for the concatenated string. We will use the TF-IDF vector matrix in order to generate the cosine similarity score. We will be using the <code class="literal">linear_kernel</code> function from scikit-learn in order to generate the cosine similarity. This function <a id="id762" class="indexterm"/>can generate the cosine similarity in less time compared to the <code class="literal">cosine_similarity</code> function of scikit-learn, which takes longer. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_26.jpg" alt="Building the recommendation engine using the jobs datafile" width="694" height="349"/><div class="caption"><p>Figure 6.26: Code snippet for generating TF-IDF and cosine similarity</p></div></div><p>As you can see, we have generated a high-dimensional TF-IDF matrix here. By using <code class="literal">linear_kernel</code>, we have generated the cosine similarity score as well.</p><p>As we are done with the implementation of the revised approach, we need to test the recommendation now.</p></div><div class="section" title="Testing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec110"/>Testing the revised approach</h2></div></div></div><p>In this section, we <a id="id763" class="indexterm"/>will generate a similar kind of job recommendation based on any given job title. We are passing the job title as the input here, and with the help of the cosine similarity score, we can generate the top 10 similar kinds of jobs that any user can apply for.</p><p>For example, suppose a <a id="id764" class="indexterm"/>person is an SAP business analyst. That person may want to apply to a similar kind of job, so here, our function will take the job title as the input and generate the top 10 similar kinds of jobs for that particular user. The code for generating the top 10 job recommendations is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_27.jpg" alt="Testing the revised approach" width="543" height="564"/><div class="caption"><p>Figure 6.27: Code snippet for generating the top 10 job recommendations</p></div></div><p>When we see the output, the recommendations start making sense. The person who is an SAP business <a id="id765" class="indexterm"/>analyst will get jobs recommendations, such as SAP FI/ Co-business analyst. The result of the revised approach is satisfying for us, and the recommendations seem relevant.</p></div><div class="section" title="Problems with the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec111"/>Problems with the revised approach</h2></div></div></div><p>In this section, we will be discussing the problems with the revised approach. In the best approach, we can resolve <a id="id766" class="indexterm"/>this problem. In the revised approach, we have used only the jobs data attribute. We haven't considered the user's profile or the user's preferences. During the implementation of the best approach, we will also consider the user's profile, and based on the user's profile, we will suggest the jobs to them.</p><p>In the next section, we will take a look at an intuitive idea for how to optimize the revised approach.</p></div><div class="section" title="Understanding how to improve the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec112"/>Understanding how to improve the revised approach</h2></div></div></div><p>Until now, we have <a id="id767" class="indexterm"/>used data attributes given in the jobs datafile, but we haven't used the data attributes from the <code class="literal">users </code>datafile and the <code class="literal">apps </code>datafile. The <code class="literal">users </code>datafile contains the user's profile information, and the <code class="literal">apps </code>datafile contains information about which user applied for which jobs.</p><p>The best approach has three simple steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, with the help of user's profile, we will find and generate the top 10 similar users.</li><li class="listitem">We will try to find out the jobs these 10 people applied for. We can then generate <code class="literal">JobIDs</code>.</li><li class="listitem">Now, we will generate the job title using <code class="literal">JobIDs</code>.</li></ol></div><p>Here, we have taken the user's profile into account, so the recommendations are more specific to the particular user base. Now, let's start implementing it.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec66"/>The best approach</h1></div></div></div><p>We have already <a id="id768" class="indexterm"/>seen the intuitive approach for how we will build the best possible approach. Here, we will use the same techniques <a id="id769" class="indexterm"/>as the ones we used in the revised approach. In this approach, we are adding more data attributes to make the recommendation engine more accurate. You can refer to the code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb">https://github.com/jalajthanaki/Job_recommendation_engine/blob/master/Job_recommendation_engine.ipynb</a>.</p><div class="section" title="Implementing the best approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec113"/>Implementing the best approach</h2></div></div></div><p>These are the <a id="id770" class="indexterm"/>steps we need to take in order to implement the best possible approach: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Filtering the dataset</li><li class="listitem" style="list-style-type: disc">Preparing the training dataset</li><li class="listitem" style="list-style-type: disc">Applying the concatenation operation</li><li class="listitem" style="list-style-type: disc">Generating the TF-IDF and cosine similarity score</li><li class="listitem" style="list-style-type: disc">Generating recommendations</li></ul></div><p>Let's start implementing each of these listed steps.  </p><div class="section" title="Filtering the dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec79"/>Filtering the dataset</h3></div></div></div><p>In this step, we <a id="id771" class="indexterm"/>need to filter the user's dataframe. We are applying the filter on the country data column. We need to consider the US-based users because there are around 300K users based outside of the US, and other users are from elsewhere in the world. The code to filter the user dataframe is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_28.jpg" alt="Filtering the dataset" width="906" height="309"/><div class="caption"><p>Figure 6.28: Code snippet to filter the user's dataframe</p></div></div><p>Now, let's<a id="id772" class="indexterm"/> prepare the training dataset.</p></div><div class="section" title="Preparing the training dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec80"/>Preparing the training dataset</h3></div></div></div><p>There <a id="id773" class="indexterm"/>are 300K users, but we are not considering all of them because of the limited training time and computational power. Here, we are considering only 10,000 users. If you have more computational resources, then you can consider a higher number of users. You can refer to the code snipp.et shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_29.jpg" alt="Preparing the training dataset" width="902" height="272"/><div class="caption"><p>Figure 6.29: Code snippet for selecting data records for training</p></div></div><p>Now, let's move on to the next step.</p></div><div class="section" title="Applying the concatenation operation"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec81"/>Applying the concatenation operation</h3></div></div></div><p>In this step, we <a id="id774" class="indexterm"/>are basically performing the concatenation operation. In order to find a similar user profile, we will concatenate the user's degree type, major, and years of experience. We will generate the TF-IDF and cosine similarity for this concatenated data value. You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_30.jpg" alt="Applying the concatenation operation" width="813" height="111"/><div class="caption"><p>Figure 6.30: Code snippet for applying the concatenation operation</p></div></div><p>Now, we will generate the TF-IDF and cosine similarity score using this concatenated data value.</p></div><div class="section" title="Generating the TF-IDF and cosine similarity score"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec82"/>Generating the TF-IDF and cosine similarity score</h3></div></div></div><p>In this section, we will <a id="id775" class="indexterm"/>generate the TF-IDF and cosine similarity score using the scikit-learn API. We are using the same API that we used in the revised approach. Here, we haven't changed the technique, but we will change the data attributes. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_31.jpg" alt="Generating the TF-IDF and cosine similarity score" width="706" height="332"/><div class="caption"><p>Figure 6.31: Code snippet for generating TF-IDF and cosine similarity</p></div></div><p>As you <a id="id776" class="indexterm"/>can see, we have generated the cosine similarity score, so based on that, we can generate a similar user profile and give them a job recommendation based on their job-application track records.</p></div><div class="section" title="Generating recommendations"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec83"/>Generating recommendations</h3></div></div></div><p>In order to generate the<a id="id777" class="indexterm"/> job recommendation, we need to perform the following steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Step 1</strong></span>: In order to generate the top 10 similar user profiles, we need to pass the UserID, and as an output, we get the 10 UserIDs that are the most similar with respect to the input UserID. You can refer to the following screenshot:<div class="mediaobject"><img src="Images/B08394_06_32.jpg" alt="Generating recommendations" width="608" height="283"/><div class="caption"><p>Figure 6.32: Code snippet for generating the top 10 similar users</p></div></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Step 2</strong></span>: We will take the list of <code class="literal">userIDs </code>that we generated in step 1 and try to find out the same <code class="literal">UserIDs</code> in the apps dataframe. The purpose of this kind of search operation is that we need to know which user applied for which job position. By using the apps data frame, we get <code class="literal">JobIDs</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Step 3</strong></span>: Once we obtain <code class="literal">JobIDs</code>, we will obtain job titles using the jobs dataframe.</li></ul></div><p>The code snippet for step 2 and step 3 is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_06_33.jpg" alt="Generating recommendations" width="754" height="856"/><div class="caption"><p> Figure 6.33: Code snippet to obtain JobIDs and Job title</p></div></div><p>As you can see, we <a id="id778" class="indexterm"/>have obtained similar users for <code class="literal">UserID 47</code>, and as we can see in the job recommendations, we get fairly relevant jobs based on the users' profile and their educational qualification. In the recommendation, we can see medical domain jobs in the Florida location. That is because, in our user base, a majority of the users' profiles are from a medical background. As we have considered both the user profile and job profile, we are able to get the most relevant job recommendations.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec67"/>Summary</h1></div></div></div><p>For this entire chapter, we used a content-based approach in order to develop a job recommendation engine, and you learned how to scrap the dataset and build the baseline job recommendation engine. After that, we explored another dataset. For the revised and best approach, we used the readily available dataset. During the course of the development of the revised approach, we considered the metadata of jobs, and built a recommendation system that works quite well. For the best approach, we tried to find out similar user profiles. Based on the user's profile, we suggested jobs to the group of users.</p><p>In the next chapter, we will be building a summarization application. There, we will take a look at documents for the medical domain and try to summarize them. We will use deep-learning algorithms in order to build an application. So, keep reading!</p></div></div>



  </body></html>