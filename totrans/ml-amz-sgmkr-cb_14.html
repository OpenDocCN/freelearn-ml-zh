<html><head></head><body>
		<div id="_idContainer226">
			<h1 id="_idParaDest-144"><em class="italic"><a id="_idTextAnchor161"/>Chapter 11</em>: Working with Geospatial Data, NLP, and Image Processing </h1>
			<p>In this book thus far, we have focused mainly on numeric and categorical features. This is not always the case in big data, as with big data comes an increasing data variety. Image, text, and geospatial data is becoming increasingly valuable in gaining insight and providing solutions to the most complex problems. Recently, for instance, <strong class="bold">location-based</strong> data has been used to improve the effectiveness of advertising campaigns. For example, different ads can be shown to users according to their location; if they are coffee lovers and close to coffee shops, push notifications could be sent to their mobile devices. In other cases, chatbots, based on advanced text analytics or natural language processing, provide businesses with an efficient and effective avenue to solve customer problems. What is most interesting and an emerging approach to solving commercial problems is the use of <strong class="bold">multimodal</strong> datasets, which combine different variable types in the same project. </p>
			<p>Understandably, the topic of analyzing different variable types is enough to be covered in a book in its own right. Yet providing an overview of the analysis of different variable types is key in grounding the use of DataRobot in building multimodal models that involve text, image, and location data. With that foremost in mind, in this chapter, we will delve into the definitions and approaches to analytics with text, image, and geospatial data. Thereafter, we will use DataRobot to build and make predictions with a model that capitalizes on the uniqueness of a multimodal dataset in predicting house prices. As such, the topics that will be covered are as follows:</p>
			<ul>
				<li>A conceptual introduction to geospatial, text, and image data</li>
				<li>Defining and setting up multimodal data in DataRobot</li>
				<li>Building models using a multimodal dataset in DataRobot</li>
				<li>Making predictions using multimodal datasets in DataRobot</li>
			</ul>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor162"/>Technical requirements</h1>
			<p>Most of the analysis and modeling carried out in this chapter requires access to the DataRobot software. Some manipulations were carried out using other tools, including MS Excel. The dataset utilized in this chapter is the House Dataset.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor163"/><a id="_idTextAnchor164"/>House Dataset</h2>
			<p>The House Dataset can be accessed at Eman Hamed Ahmed's GitHub account (<a href="https://github.com/emanhamed">https://github.com/emanhamed</a>). Each row in this dataset represents a specific house. The initial feature set describes its characteristics, price, zip code, images of the bedroom, bathroom, kitchen, and frontal view. There was no missing data. We went on to develop text descriptions for <a id="_idIndexMarker547"/>each house, based on the number of bedrooms, bathrooms, city, country, state, and actual size of the property. Elsewhere, the ZIP codes were converted into latitude and longitude, which were added to the dataset as columns. More information on the base features is provided at the GitHub link and the data is provided in <strong class="source-inline">.csv</strong> format.</p>
			<p class="callout-heading">Dataset Citation</p>
			<p class="callout"><em class="italic">House Price Estimation from Visual and Textual Features. In Proceedings of the 8th International Joint Conference on Computational Intelligence</em>, <em class="italic">H. Ahmed E. and Moustafa M.</em> <em class="italic">(2016). (IJCCI 2016) ISBN 978-989-758-201-1, pages 62â€“68. DOI: 10.5220/0006040700620068</em></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor165"/>A conceptual introduction to geospatial, text, and image data</h1>
			<p>Just like we use different senses to holistically understand objects around us, a <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model also benefits from data coming from different types of sensors and sources. Having only one type of data (for instance, numeric or categorical) limits the level of <a id="_idIndexMarker548"/>understanding, predictability, and robustness of a model. In this section, we will present a more in-depth discussion of the business importance of different data types in building models, the associated challenges, and the preprocessing steps necessary to mitigate these challenges.<a id="_idTextAnchor166"/><a id="_idTextAnchor167"/> </p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor168"/>Geospatial AI</h2>
			<p>Geospatial understanding has had long-standing implications for decision-making in certain industries, including mineral exploitation, insurance, retail, and real estate. While the commercial importance of data science is well established, location-based AI is just beginning to gain <a id="_idIndexMarker549"/>recognition. The use of ML in improving business performance has brought to the fore the importance of augmenting datasets with location-based information and features in building predictive models. </p>
			<p>Typical ML models built mainly from categorical and numeric data have contributed immensely to realizing business goals, but decisions are governed by more than numeric and categorical information. Indeed, the events take place at certain locations. ML models need location-based information in order for the location context to effectively present commercial insight and predictions. What works in one geography may not work in another.</p>
			<p>The potential commercial impact of using ML and location-based information comes with several challenges:</p>
			<ul>
				<li>A lack of datasets, tools, and people skills. </li>
				<li>Connecting ML <a id="_idIndexMarker550"/>pipelines to native location-based analysis techniques is not straightforward. </li>
				<li>Only a few R and Python packages have geospatial capabilities. </li>
				<li>Understanding these capabilities requires further education and training for analysts. </li>
			</ul>
			<p>DataRobot's location AI capability helps alleviate some of these challenges. The location AI capability complements the existing AutoML experience by adding in a repertoire of geospatial analytic and modeling tools. With DataRobot, location features could be selected from the dataset, but the location AI capability enables the platform to automatically recognize geospatial data and create geospatial features. A variety of geospatial data file formats can be uploaded. These include GeoJSON, Esri shapefiles, and geodatabases, PostGIS tables, as well as traditional latitude and longitude data.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor169"/>Natural language processing</h2>
			<p>As humans, we communicate effectively via a vast range of words with or without limitations on the volume of words to use. More than words, body language, tonality, and words' context are crucial to effective communication. For example, using the same set of words, <em class="italic">the cat is bigger than the dog</em> has a different meaning to <em class="italic">the dog is bigger than the cat</em>. Naturally, humans understand, draw conclusions, and make predictions of the future based on free text. The use of free text comes with valuable information and rich insights can be harvested from it. Yet, since free text fails to follow a consistent structure, they pose challenges to being processed by machines.</p>
			<p>Conversations and other forms of free text are messy and unstructured as they do not fit neatly into traditional tables with rows and columns. <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) sits at the intersection of data science and linguistics and involves the systematic use of <a id="_idIndexMarker551"/>advanced processes for analysis, understanding, and the extraction of data from free text. Through NLP, scientists can leverage free text to generate valuable insight, which is then integrated as features in building better-performing models. Text mining allows the identification of unique words or groups of words that are associated with certain outcomes. For example, in the house price prediction case, the description of the house improves the predictability of the models in estimating the house price. Thinking about it, the description also contributes to an individuals' decision of buying a house. Individuals' propensity to buy houses influences property pricing. NLP algorithms can identify the effect of word sequencing and influence words or phrases, and a word's context within sentences.</p>
			<p>NLP is key to machines being able to extract important information from text. Consequently, NLP allows machines to decide feelings described in free text by giving a number score to a text, indicating its sentiment to a topic or event. Similarly, it aids in the identification of classes that certain words most likely belong to. This capability has given birth to several applications, including text classification, named entity recognition, sentiment analysis, and summarization of text. </p>
			<p>To get free text to provide useful insight or be integrated into models is not an easy task. As earlier alluded to, raw text has no structure, so structure needs to be introduced. Also, numerous words have the same meaning and you could have the same word mean different things in different contexts. In a typical analytics process, there are numerous steps taken to normalize free text. At least four steps are required:</p>
			<ol>
				<li>The first step in most text processing is splitting text corpus into separate words. This step, also called <strong class="bold">tokenization</strong>, enables the identification of keywords and phrases. The separated <a id="_idIndexMarker552"/>words are referred to as tokens. N-grams are the basic units for text analytics.</li>
				<li>Next, there are certain words that contribute little or nothing to the meaning of a text. These are <a id="_idIndexMarker553"/>generally common words; for instance, in the English language, we have words such as <em class="italic">the</em>, <em class="italic">that</em>, <em class="italic">is</em>, and <em class="italic">these</em>. Within the <a id="_idIndexMarker554"/>context of text mining, these words are referred to as noise or are sometimes called stop words. So, this step is called <strong class="bold">noise removal</strong>. </li>
				<li>After that, words are converted to their root meanings. There are a few approaches to this. As an illustration, <strong class="bold">stemming</strong> typically converts to the root word stem by eliminating certain letters. So, words like <em class="italic">happy</em>, <em class="italic">happiness</em>, <em class="italic">happily</em>, and <em class="italic">happiest</em> will all be returned to the root word <em class="italic">happ</em>. Because the same words could have <a id="_idIndexMarker555"/>multiple meanings, disambiguation of words becomes crucial in text processing. Whereas stemming returns words to their roots by cutting off their prefix or suffix, <strong class="bold">lemmatization</strong> examines the context of words to <a id="_idIndexMarker556"/>ensure stemmed words are converted to logical bases called <strong class="bold">lemma</strong>. For example, the word <em class="italic">anticipate</em> when stemmed <a id="_idIndexMarker557"/>might be returned to <em class="italic">ant</em>. Within the context, however, <em class="italic">ant</em> would not make sense; as such, lemming will ensure that the word <em class="italic">anticipate</em> is retained.</li>
				<li>A final, yet important, step is the process of <strong class="bold">featurization</strong> where lemma or root words are converted into features. Again, there are several ways this can be done. The most <a id="_idIndexMarker558"/>straightforward method involves developing features for each unique token and counting the number of that token in each text corpus (<em class="italic">Table 11.1</em> presents a demonstration of this process):<div id="_idContainer211" class="IMG---Figure"><img src="image/B17159_11_01.jpg" alt="Table 11.1 â€“ A demonstration of featurization&#13;&#10;"/></div><p class="figure-caption">Table 11.1 â€“ A demonstration of featurization</p><p>Following featurization, developed text variables are either used as predictors alone or integrated with other variables <a id="_idIndexMarker559"/>in building models. While the importance of analytics with free text within the commercial setting is well established, ancient wisdom suggests that <em class="italic">"a picture is worth a thousand words."</em> This raises the importance of using image analytics in driving business value. As we have established in this section, the core purpose of NLP is the extraction of text features from raw text. Image processing performs a similar function for images, as described in the nex<a id="_idTextAnchor170"/>t section. </p><h2 id="_idParaDest-150"><a id="_idTextAnchor171"/>Image processing</h2><p>Images provide valuable information to customers about products and services. Images are fast becoming crucial to business success as they influence the propensity to buy. As the data landscape <a id="_idIndexMarker560"/>continues to grow, image data is also becoming readily available and important. This offers analysts the opportunity to include image features when creating insights for businesses.</p><p>Image data, like text data, lacks structure. In fact, with image data, there is an uncertainty of features. To bring this to life, let's imagine the case of identifying individuals from their pictures. The image of an individual could be colored or grayscale, the position of an individual's face or body could change, and their background and outfits are unlikely to be the same across images. These and other variations make the data generated from differing images of the same person appear very different. As such, features from an image are unlikely to be consistent with those of another image, despite the individual being the same person; therefore, the image data would have challenges with feature uncertainty. Yet, the human eye can see the individual in those images as the same and easily recognize them.</p><p>The smallest indivisible units within images are known as <strong class="bold">pixels</strong>. For grayscale images, pixels are interpreted <a id="_idIndexMarker561"/>as 2D arrays. Each has a strength represented by a value between <strong class="source-inline">0</strong> and <strong class="source-inline">255</strong>, referred to as <strong class="bold">pixel intensity</strong>. For grayscale images, <strong class="source-inline">0</strong> is shown as completely black, while <a id="_idIndexMarker562"/>completely white gives <strong class="source-inline">255</strong>. On the other hand, color images have 3D arrays with blue, green, and red layers. Like black <a id="_idIndexMarker563"/>on the grayscale images, each of those layers has its own values from <strong class="source-inline">0</strong> to <strong class="source-inline">255</strong>, where the final color is a combination of corresponding values on each of the three layers.</p><p>Image processing typically follows predefined steps in extracting useful and consistent features from images that align with the purpose of extraction. <strong class="bold">Activation maps</strong> are then applied to the extracted images to reduce the computational load required to process the volume of <a id="_idIndexMarker564"/>data from each image. As such, the activation maps essentially reduce the feature space. The reduced feature space is then <strong class="bold">flatted</strong> into a tabular structure, enabling it to be used as variables in the typical ML modeling process. Though this is a simplified illustration of image processing, there are other approaches to it. This ensures we have some understanding of how the challenge of image inconsistency and limited structure can be managed.</p><p>We have so far established the added value of geospatial, text, and image data. We have discussed the challenges in using these data types and also highlighted the key steps in using them <a id="_idIndexMarker565"/>to build models. Sometimes, different data types such as images and text are integrated into the same dataset in training models. This type of dataset is referred to as a <strong class="bold">multimodal dataset</strong>. While a multimodal dataset presents unparalleled opportunities, it comes with huge challenges. The difficulties and steps highlighted for each of the data types are expected to be addressed. DataRobot has capabilities that make use of multimodal data more accessible. The platform enables preprocessing steps and integrates these datasets in making predictions. For the rest of the chapter, we will demonstrate how to use the price listing dataset to train a model and make predictions. </p><h1 id="_idParaDest-151"><a id="_idTextAnchor172"/>Defining and setting up multimodal data in DataRobot</h1><p>DataRobot's location <a id="_idIndexMarker566"/>AI, text mining, and visual <a id="_idIndexMarker567"/>AI automated ML capabilities make working <a id="_idIndexMarker568"/>with text, location, and image data relatively straightforward. With these capabilities, DataRobot can help analysts in building <a id="_idIndexMarker569"/>models and making predictions <a id="_idIndexMarker570"/>against text, image, and location-based <a id="_idIndexMarker571"/>datasets. Data set up for an image model <a id="_idIndexMarker572"/>differs considerably from other types of models. Using our House Dataset, our first task is to set up the data.</p><p>The house dataset has zip codes for the houses. We created and integrated latitude and longitude coordinates from the zip codes as new columns. Other features created from the zip codes not evident in <em class="italic">Table 11.2</em> are the city, county, and state where each of the houses was located. Further text description columns were built from the number of rooms, the size of the house, and its location: </p><p class="figure-caption">  </p><div id="_idContainer212" class="IMG---Figure"><img src="image/B17159_11_02.jpg" alt="Table 11.2 â€“ The developed price list dataset&#13;&#10;"/></div><p class="figure-caption">Table 11.2 â€“ The developed price list dataset</p><p>The original data comes with 2,140 images, each of the 535 houses having a bedroom, bathroom, frontal view, and kitchen. For the image data to be included in the analysis, the data can be set up as a ZIP file with a structured <strong class="source-inline">.csv</strong> file sitting next to the image folder. Four new image columns were created for the bedroom, bathroom, frontal view, and kitchen. As such, each data row on the <strong class="source-inline">.csv</strong> file had paths to their images, as shown in <em class="italic">Table 11.2</em>. The paths point to locations on the corresponding image file in the ZIP file. Within the ZIP file, the <strong class="source-inline">.csv</strong> file with the tabular features sits needs to the folder, containing all the images. Each image has a unique name that is consistent with the image path columns on the <strong class="source-inline">.csv</strong> file. The setup for the ZIP file and <strong class="source-inline">HousePrice</strong> folder is shown in <em class="italic">Figure 11.1</em>. That said, the dataset could still be ingested using the AI Catalog. This also gives DataRobot the ability to connect to other data sources for the images. Furthermore, the Paxata tool can be deployed in the data preprocessing if you have access to that tool:</p><div id="_idContainer213" class="IMG---Figure"><img src="image/B17159_11_03.jpg" alt="Figure 11.3 â€“ Data setup for the ZIP file (left) and image folder (right)&#13;&#10;"/></div><p class="figure-caption">Figure 11.3 â€“ Data setup for the ZIP file (left) and image folder (right)</p><p>The image on the left of <em class="italic">Figure 11.1</em> shows how the ZIP file is set up. The folder containing <a id="_idIndexMarker573"/>the image files, <strong class="source-inline">Houses Dataset</strong>, is <a id="_idIndexMarker574"/>next to the <strong class="source-inline">HousePrice.csv</strong> file. The <a id="_idIndexMarker575"/>image on the right presents <a id="_idIndexMarker576"/>image files within the <strong class="source-inline">Houses Dataset</strong> folder. Here, the images are labeled, with the locations, consistent with cells on data, <strong class="source-inline">HousePrice.csv</strong> (as shown in <em class="italic">Table 11.2</em>). With the data completely set up, the next step is to commence model development.</p><h1 id="_idParaDest-152"><a id="_idTextAnchor173"/>Building models using multimodal datasets in DataRobot</h1><p>Having fully set up our ZIP file with the multimodal dataset, we proceed into initiating the project within DataRobot. The data ingestion using the drag and drop method is like the earlier <a id="_idIndexMarker577"/>project, except in this <a id="_idIndexMarker578"/>case we upload the ZIP file. Following the upload of the ZIP file, the price is selected as the target variable. DataRobot <a id="_idIndexMarker579"/>automatically detects the text, image, and geospatial fields (see <em class="italic">Figure 11.2</em>). The geometry feature is a location-based feature made up of the latitude and longitude variables in <a id="_idIndexMarker580"/>the original dataset. Apart <a id="_idIndexMarker581"/>from latitude and longitude coordinates, location features can be formed from other native geospatial <a id="_idIndexMarker582"/>formats, such as Esri shapefiles, GeoJSON, and PostGIS databases. These can be uploaded using drag and drop, AI Catalog, or URL methods:</p><div id="_idContainer214" class="IMG---Figure"><img src="image/B17159_11_04.jpg" alt="Figure 11.4 â€“ Feature Name list&#13;&#10;"/></div><p class="figure-caption">Figure 11.4 â€“ Feature Name list</p></li>
				<li>The location-based visual representation of the listing price can be viewed by selecting the <strong class="screen-inline">Price</strong> option in the <strong class="screen-inline">Feature Name</strong> list. This <strong class="bold">Exploratory Spatial Data Analysis</strong> (<strong class="bold">ESDA</strong>) is conducted by opening the <strong class="screen-inline">Geospatial Map</strong> tab and clicking on the <strong class="screen-inline">Compute feature over map</strong> button. As seen in <em class="italic">Figure 11.3</em>, the <strong class="screen-inline">Geospatial Maps</strong> window offers a location-based analytics visualization of the dataset. It shows the distribution of properties over space â€“ the number of houses in each area and their average prices: <div id="_idContainer215" class="IMG---Figure"><img src="image/B17159_11_05.jpg" alt="Figure 11.5 â€“ Geospatial Map&#13;&#10;"/></div><p class="figure-caption">Figure 11.5 â€“ Geospatial Map</p><p>The map legend offers vital information about the map. It highlights that the color of the <a id="_idIndexMarker583"/>hexagon shows the average house prices within the location. Elsewhere, it presents <a id="_idIndexMarker584"/>the frequency of cases by the height of the hexagons. This ESDA feature shows not only the <a id="_idIndexMarker585"/>visual distribution of house prices across the map but also an illustration of house counts in differing areas. Similar geospatial analysis can be conducted for other features, such as house area variables and bedrooms. </p><p>This preliminary examination of image features can be conducted by selecting any of the image variables. This shows a sample of images within the <strong class="bold">Feature Name</strong> list. Here, differing image features can be seen and organized by house price ranges. </p></li>
				<li>To further explore the image features at a property level, we click on <strong class="bold">View Raw Data</strong>. This opens the dataset in its final format on DataRobot. Unlike the initial <strong class="source-inline">.csv</strong> file with image paths, the images are integrated into the dataset (see <em class="italic">Figure 11.4</em>). For each of the rows, the images are clearly displayed. A further <a id="_idIndexMarker586"/>scroll <a id="_idIndexMarker587"/>will show the free <a id="_idIndexMarker588"/>text description of the listed properties. This multimodal dataset of text, location, and image features can now be used to build a more robust model and make predictions of house prices:<div id="_idContainer216" class="IMG---Figure"><img src="image/B17159_11_06.jpg" alt="Figure 11.6 â€“ The DataRobot view of the multimodal data&#13;&#10;"/></div><p class="figure-caption">Figure 11.6 â€“ The DataRobot view of the multimodal data</p></li>
				<li>As with earlier projects, we click on <strong class="bold">Start</strong> to commence the model-building process. On completion <a id="_idIndexMarker589"/>of the modeling process, the models <a id="_idIndexMarker590"/>are evaluated <a id="_idIndexMarker591"/>using the <strong class="bold">RMSE metric</strong>. The leaderboard shows DataRobot has built 36 models in total. The <a id="_idIndexMarker592"/>top-performing is the <strong class="bold">Nystroem Kernel SVM Regressor</strong> model. <p>As can be seen in <em class="italic">Figure 11.5</em>, opening the model presents its blueprint, outlining all the steps necessary to make the data ready for this model. Because of <a id="_idIndexMarker593"/>the multimodal nature of the data, the preprocessing steps are quite complex. DataRobot <a id="_idIndexMarker594"/>conducted geospatial processing, which was integrated with some numeric variables <a id="_idIndexMarker595"/>and high-level image and text processing (the latter not visible in <em class="italic">Figure 11.5</em>). For more information on each step, a click on the step box provides some insight on the modeling step and a link to comprehensive documentation on the step: </p><div id="_idContainer217" class="IMG---Figure"><img src="image/B17159_11_07.jpg" alt="Figure 11.7 â€“ The model blueprint for multimodal data modeling&#13;&#10;"/></div><p class="figure-caption">Figure 11.7 â€“ The model blueprint for multimodal data modeling</p></li>
				<li>Within the <strong class="bold">Understand</strong> tab, the <strong class="bold">Feature Impact</strong> view highlights the extent to which features contribute to the overall performance of the model (see <em class="italic">Figure 11.6</em>). The <strong class="bold">Feature Impact</strong> view for this project shows that <strong class="source-inline">Area</strong> is the most impactful feature of the house; next is the <strong class="source-inline">FullDescription</strong> text feature. Thereafter, the <strong class="source-inline">Bedrooms</strong> and <strong class="source-inline">Image_kitchen</strong> features follow suit. What is rather interesting is the fact that <strong class="source-inline">Image_bathroom</strong> seems to have a negative impact on the model accuracy. This suggests that insights from these images lead the model away from actual house prices: </li>
			</ol>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/B17159_11_08.jpg" alt="Figure 11.8 â€“ Feature Impact for multimodal models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 â€“ Feature Impact for multimodal models</p>
			<p>For this reason, leading the <a id="_idIndexMarker596"/>model away from improved performance, we use the <strong class="bold">image embedding</strong> and activation maps to understand how the model <a id="_idIndexMarker597"/>uses <strong class="source-inline">bathroom</strong> images to make predictions. By doing so, we will use the <strong class="source-inline">bathroom</strong> feature to <a id="_idIndexMarker598"/>demonstrate the image feature exploration capabilities available in DataRobot. DataRobot <a id="_idIndexMarker599"/>conducts unsupervised learning to cluster images according to their similarity. Still within the <strong class="bold">Understand</strong> tab, this is presented within the <strong class="bold">Image Embeddings</strong> sub-tab for each image feature. <em class="italic">Figure 11.7</em> presents the image embedding for the <strong class="source-inline">bathroom</strong> views. We can see DataRobot clusters similar images together. It seems that images that are dominantly white goods are presented in the right-hand and upper parts of the visualization. We can filter the visualization in accordance with house prices:</p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/B17159_11_09.jpg" alt="Figure 11.9 â€“ Image embedding&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 â€“ Image embedding</p>
			<p><strong class="bold">Activation Maps</strong> adds to this information by offering insight into what aspect of images the <a id="_idIndexMarker600"/>model is leveraging <a id="_idIndexMarker601"/>in making predictions. This is critical to confirm that the model is using the key aspects of images. <em class="italic">Figure 11.8</em> presents the activation map for the <strong class="source-inline">image_bathroom</strong> variable. It appears <a id="_idIndexMarker602"/>the model makes its predictions mainly from white fixtures in the bathroom. This might offer insight into why <strong class="source-inline">image_bathroom</strong> is seen as having a negative impact on the model performance. It is possible that this extraction from white fixtures misleads the model:</p>
			<div>
				<div id="_idContainer220" class="IMG---Figure">
					<img src="image/B17159_11_10.jpg" alt="Figure 11.10 â€“ Activation map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10 â€“ Activation map</p>
			<p>Location-based information comes with significant information complementing other data types. However, some models struggle in certain geographical areas. Inspecting the performance <a id="_idIndexMarker603"/>of models and <a id="_idIndexMarker604"/>considering locations <a id="_idIndexMarker605"/>empowers the analyst in taking actions on model performance improvement. DataRobot's <strong class="bold">Accuracy Over Space</strong> capability presents a spatial representation of a models' residual at differing locations (see <em class="italic">Figure 11.9</em> for an example). This chart could lead the analyst into considering the rationale behind higher residuals in certain areas: </p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/B17159_11_11.jpg" alt="Figure 11.11 â€“ Accuracy Over Space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11 â€“ Accuracy Over Space</p>
			<p>For instance, as <a id="_idIndexMarker606"/>evident in <em class="italic">Figure 11.9</em>, the Phoenix area has a higher average residual price of over $380k than most places. This area might for instance be considered an area of low income. This visualization <a id="_idIndexMarker607"/>could point the data scientist toward including features around localized economic indices. This might <a id="_idIndexMarker608"/>provide an explanation for the high residual. Including such features could therefore improve the overall performance of the model. The data partition for measuring accuracy could be set by altering between the validation, cross-validation, or holdout partitions. Also, the accuracy metric type and aggregation can be adjusted in accordance with the user's requirements.</p>
			<p>Complementing its location-based features engineering, DataRobot's location-based analytics capabilities <a id="_idIndexMarker609"/>can exploit its location awareness to create <strong class="bold">spatially autocorrelation features</strong>, sometimes known as <strong class="bold">spatially lagged features</strong>, which are extremely insightful. The <strong class="source-inline">eXtreme Gradient Boosted Trees Regressor (Gamma Loss)</strong> model's fourth most important <a id="_idIndexMarker610"/>feature, <strong class="source-inline">GEO_KNL_K10_LAG1_Price</strong>, is one such feature (see <em class="italic">Figure 11.10</em>). This feature describes the spatial dependence structure for price <a id="_idIndexMarker611"/>using a kernel size augmented by distance. The <strong class="bold">k-nearest neighbor</strong> approach can also be deployed:</p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/B17159_11_12.jpg" alt="Figure 11.12 â€“ Spatial lag features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12 â€“ Spatial lag features</p>
			<p>Text analytics information such as the Word Cloud is not available within the <strong class="bold">Understand</strong> tab <a id="_idIndexMarker612"/>for this model. We turn to the <strong class="bold">Insights</strong> view to learn more about the <strong class="source-inline">FullDescription</strong> text feature, which is indeed one of the most impactful features of this <a id="_idIndexMarker613"/>model. Though not visible on the model blueprint (<em class="italic">Figure 11.10</em>), the text variable was scored using <a id="_idIndexMarker614"/>another model, <strong class="source-inline">Auto-Tuned Word N-Gram Text Modeler using token occurrences â€“ FullDescription</strong>, which essentially develops scores using <strong class="bold">N-Gram</strong> and their token occurrence. During this modeled step, the <strong class="source-inline">FullDescription</strong> feature was converted into tokens for a differing number <a id="_idIndexMarker615"/>of words (as <em class="italic">N</em> in <em class="italic">N-grams</em>) and scored. Thereafter, this feature was transformed on the link scale and standardized. For text-related insights, we turn to the <strong class="bold">Insights</strong> view, offering two important text insight capabilities, <strong class="bold">Word Cloud</strong> and <strong class="bold">text mining</strong>.</p>
			<p>The <strong class="bold">Word Cloud</strong> provides a diagrammed representation of the effect of certain words or groups of words (otherwise referred to as tokens) within the <strong class="source-inline">FullDescription</strong> feature in <a id="_idIndexMarker616"/>influencing the house price. The size of the words, as shown in <em class="italic">Figure 11.11</em>, highlights the frequency of the tokens, while the color suggests its effect coefficient. This coefficient <a id="_idIndexMarker617"/>is standardized typically between -1.5 and 1.5. The closer the color of the words is to red, the greater the coefficient and, consequently, the greater the house price is:</p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/B17159_11_13.jpg" alt="Figure 11.13 â€“ Word Cloud for a multimodal dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13 â€“ Word Cloud for a multimodal dataset</p>
			<p>We can <a id="_idIndexMarker618"/>assume that when the <strong class="source-inline">FullDescription</strong> variable contains words in orange, <strong class="bold">alameda county</strong>, and big, the <a id="_idIndexMarker619"/>prices are likely to be high. Similarly, with small-sized words, and <strong class="bold">city riverside</strong>, a lower price is <a id="_idIndexMarker620"/>expected. The text mining capability displays similar information to the Word Cloud using a bar graph. </p>
			<p>Now that we have been able to build models using multimodal datasets, conduct analysis on their features, and evaluate the performance of those models, we will next focus on making predictions with models.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor174"/>Making predictions using a multimodal dataset on DataRobot</h1>
			<p>After building a model, there are many ways to make predictions on a DataRobot. For this use case, we <a id="_idIndexMarker621"/>will illustrate the prediction capability using the <strong class="source-inline">Make Prediction</strong> method, which is available <a id="_idIndexMarker622"/>within the <strong class="bold">Predict</strong> tab. We initially create a prediction ZIP file dataset using the step outline in the <em class="italic">Defining and setting up multimodal data in DataRobot</em> section of this chapter. The developed <a id="_idIndexMarker623"/>prediction dataset is either dragged and dropped into the highlighted area or locally imported. As seen in <em class="italic">Figure 11.12</em>, we select the features we are interested in, including the prediction dataset: </p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/B17159_11_14.jpg" alt="Figure 11.14 â€“ Making a prediction from multimodal datasets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.14 â€“ Making a prediction from multimodal datasets</p>
			<p>In this illustration, we selected <strong class="source-inline">House_id</strong>, <strong class="source-inline">FullDescription</strong>, <strong class="source-inline">Bedrooms</strong>, <strong class="source-inline">City</strong>, and <strong class="source-inline">State</strong>. We can also see that the prediction dataset has 400 houses. Finally, <strong class="bold">Compute predictions</strong> is selected to make predictions. When predictions have been completed, they <a id="_idIndexMarker624"/>are downloaded. This <a id="_idIndexMarker625"/>straightforwardly <a id="_idIndexMarker626"/>creates a downloadable <strong class="source-inline">.csv</strong> file, which has all the requested columns (see <em class="italic">Table 11.3</em>): </p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B17159_11_15.jpg" alt="Table 11.15 â€“ A prediction table from a multimodal dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 11.15 â€“ A prediction table from a multimodal dataset</p>
			<p>The <strong class="bold">Prediction</strong> column presents the predicted price for each row. This finalizes the process of making predictions with multimodal datasets. As expected, after models made from multimodal datasets have been deployed, predictions can be made against them. </p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor175"/>Summary</h1>
			<p>In this chapter, we have explored how insights can be generated from an image, location, and free text. In so doing, we highlighted the benefits that these data types present, as well as the challenges that come with each of them. We also pointed out how these are typically addressed in the mainstream. We proceeded to build models with a multimodal dataset using DataRobot and make predictions from the model. We also looked at a variety of ways to derive insights from the location, free text, and image aspects of the models. By demonstrating the process of model building using a multimodal dataset, we showed how DataRobot simplifies the handling of the challenges different data types pose. </p>
			<p>Having said that, it is important to draw attention to the fact that DataRobot appears to have some limitations in terms of free text processing. Whilst the platform significantly simplifies the process of text processes, at the time of this publication, we are unsure of the extent to which domain-specific stop words can be included in the DataRobot process. It appears generic stop words are dropped, but sometimes there are domain-specific stop words that need to be accounted for. Elsewhere, within the context of multimodal modeling, we are unsure whether the text aspects of models could be tuned to include and alter the methods of stemming and lemming. It is therefore recommended that you perform your own text processing and feature engineering before feeding text into DataRobot to achieve better results. </p>
			<p>In this chapter, as well as the previous ones, we have interfaced with DataRobot using the platform. Although the platform comes with numerous capabilities, these capabilities come with some limitations. These limitations, together with how they can be alleviated using programmatic access to the platform, are extensively covered in <a href="B17159_12_Final_NM_ePub.xhtml#_idTextAnchor176"><em class="italic">Chapter 12</em></a><em class="italic">, DataRobot Python API</em>.</p>
		</div>
	</body></html>