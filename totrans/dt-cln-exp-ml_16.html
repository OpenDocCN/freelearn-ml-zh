<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer212">
<h1 id="_idParaDest-145"><em class="italic"><a id="_idTextAnchor144"/>Chapter 12</em>: K-Nearest Neighbors for Classification</h1>
<p><strong class="bold">K-nearest neighbors</strong> (<strong class="bold">KNN</strong>) is a good choice for a classification model when there are not many observations or features and predicting class membership does not need to be very efficient. It is a lazy learner, so it is quicker to fit than other classification algorithms but considerably slower at classifying new observations. It can also yield less accurate predictions at the extremes, though this can be improved by adjusting <em class="italic">k</em> appropriately. We will consider these choices carefully in the model we will develop in this chapter.</p>
<p>KNN is perhaps the most straightforward non-parametric algorithm we could select, making it a good diagnostic tool. No assumptions need to be made about the distribution of features or the relationship that features have with the target. There are not many hyperparameters to tune, and the two key hyperparameters – nearest neighbors and the distance metric – are quite easy to interpret.</p>
<p>KNN can be used successfully for both binary and multiclass problems without any extensions to the algorithm. </p>
<p>In this chapter, we will cover the following topic:</p>
<ul>
<li>Key concepts of KNN</li>
<li>KNN for binary classification </li>
<li><a id="_idTextAnchor145"/>KNN for multiclass classification</li>
</ul>
<h1 id="_idParaDest-146"><a id="_idTextAnchor146"/>Technical requirements</h1>
<p>In addition to the usual scikit-learn libraries, we will need the <strong class="source-inline">imblearn</strong> (Imbalanced Learn) library to run the code in this chapter. This library helps us handle significant class imbalance. <strong class="source-inline">imblearn</strong> can be installed with <strong class="source-inline">pip install imbalanced-learn</strong>, or <strong class="source-inline">conda install -c conda-forge imbalanced-learn</strong> if you are using Anaconda. All the code has been tested using scikit-learn versions 0.24.2 and 1.0.2.</p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor147"/>Key concepts of KNN</h1>
<p>KNN might be the most intuitive algorithm that we will discuss in this book. The idea is to find <em class="italic">k</em> instances whose attributes are most similar, where that similarity matters for the target. That last <a id="_idIndexMarker945"/>clause is an important, though perhaps obvious, qualification. We care about similarity among those attributes associated with the target’s value.</p>
<p>For each observation where we need to predict the target, KNN finds the <em class="italic">k</em> training observations whose features are most similar to those of that observation. When the target is categorical, KNN selects the most frequent value of the target for the <em class="italic">k</em> training observations. (We often select an odd value for <em class="italic">k</em> for classification problems to avoid ties.)</p>
<p>By <em class="italic">training</em> observations, I mean those observations that have known target values. No real training is done with KNN since it’s a lazy learner. I will discuss that in more detail in this section.</p>
<p>The following diagram illustrates the use of KNN for classification with values of 1 and 3 for <em class="italic">k</em>. When <strong class="bold">k=1</strong>, we would predict that our new observation, <strong class="bold">X</strong>, would be in the circle class. When <strong class="bold">k=3</strong>, it would be assigned to the square class:</p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<img alt="Figure 12.1 – KNN with values of 1 and 3 for k " height="572" src="image/B17978_12_001.jpg" width="846"/>
</div>
</div>
<p class="figure-caption">Figure 12.1 – KNN with values of 1 and 3 for k</p>
<p>But what do we mean by similar, or nearest, instances? There are several ways to measure similarity, but the most common measure is the Euclidean distance. The Euclidean distance <a id="_idIndexMarker946"/>is the sum of the squared difference between two points. This may remind you of the Pythagorean theorem. The Euclidean distance from point <em class="italic">a</em> to point <em class="italic">b</em> is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<img alt="" height="255" src="image/B17978_12_0011.jpg" width="564"/>
</div>
</div>
<p>A reasonable alternative to the Euclidean distance is the Manhattan distance. The Manhattan distance from point <em class="italic">a</em> to point <em class="italic">b</em> is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="" height="213" src="image/B17978_12_002.jpg" width="635"/>
</div>
</div>
<p>The default distance measure in scikit-learn is Minkowski. The Minkowski distance from point <em class="italic">a</em> to point <em class="italic">b</em> is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<img alt="" height="194" src="image/B17978_12_003.jpg" width="598"/>
</div>
</div>
<p>Notice that when <em class="italic">p</em> is 1, it is the same as the Manhattan distance. When <em class="italic">p</em> is 2, it is the same as the Euclidean distance.</p>
<p>The Manhattan <a id="_idIndexMarker947"/>distance is sometimes called the taxicab distance. This is because it reflects the distance between two points along a path on a grid. The following diagram illustrates the Manhattan distance and compares it to the Euclidean distance:</p>
<div>
<div class="IMG---Figure" id="_idContainer208">
<img alt="Figure 12.2 – The Euclidean and Manhattan measures of distance " height="521" src="image/B17978_12_0021.jpg" width="475"/>
</div>
</div>
<p class="figure-caption">Figure 12.2 – The Euclidean and Manhattan measures of distance</p>
<p>Using the Manhattan <a id="_idIndexMarker948"/>distance can yield better results when features are very different in terms of type or scale. However, we can treat the choice of distance measure as an empirical question; that is, we can try both (or other distance measures) and see which gives us the best-performing model. We will demonstrate this with a grid search in the next section.</p>
<p>As you likely suspect, KNN models are sensitive to the choice of <em class="italic">k</em>. Lower values of <em class="italic">k</em> will result in a model that attempts to identify subtle distinctions between observations. There is a substantial risk of overfitting at very low values of <em class="italic">k</em>. But at high values of <em class="italic">k</em>, our model may not be flexible enough. We are once again confronted with the variance-bias trade-off. Lower <em class="italic">k</em> values result in less bias and more variance, while higher values result in the opposite.</p>
<p>There is no definitive answer to the choice of <em class="italic">k</em>. But a good rule of thumb is to use the square root of the number of observations. However, just as we would do for the distance measure, we should test a model’s performance at different values of <em class="italic">k</em>. KNN is a non-parametric algorithm. No assumptions are made about the attributes of the underlying data, such as linearity or normally distributed features. This makes KNN quite flexible. It can be used to model a variety of relationships between features and the target.</p>
<p>As mentioned previously, KNN is a lazy learner algorithm. No calculations are performed at training time. The learning happens only during testing. This has its advantages and disadvantages. It may <a id="_idIndexMarker949"/>not be a good choice when there are many instances or dimensions in the data, and the speed of predictions matters. KNN also tends not to perform well when we have sparse data, such as datasets that contain many 0 values.</p>
<p>We will use KNN in the next section to build a binary classification model, before constructing a couple of multiclass models in the following section.</p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>KNN for binary classification</h1>
<p>The KNN algorithm has some of the same advantages as the decision tree algorithm. No prior <a id="_idIndexMarker950"/>assumptions about the distribution of features or residuals have to be met. It is a suitable <a id="_idIndexMarker951"/>algorithm for the heart disease model we tried to build in the last two chapters. The dataset is not very large (30,000 observations) and does not have too many features.</p>
<p class="callout-heading">Note</p>
<p class="callout">The heart <a id="_idIndexMarker952"/>disease dataset is available for public download at <a href="https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease">https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease</a>. It is derived from the United States Center for Disease Control survey data on more than 400,000 individuals from 2020. I have randomly sampled 30,000 observations from this dataset for the analysis in this section. Data columns include whether respondents ever had heart disease, body mass index, smoking history, heavy alcohol drinking, age, diabetes, and kidney disease.</p>
<p>Let’s get started with our model:</p>
<ol>
<li>First, we must load some of the same libraries we have used over the last couple of chapters. We will also load <strong class="source-inline">KneighborsClassifier</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from imblearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV,\</p><p class="source-code">  RepeatedStratifiedKFold</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from sklearn.feature_selection import SelectKBest, chi2</p><p class="source-code">from scipy.stats import randint</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">from sklearn.model_selection import cross_validate</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">import healthinfo as hi</p></li>
</ol>
<p>The <strong class="source-inline">healthinfo</strong> module <a id="_idIndexMarker953"/>contains all of the code we used in <a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a>, <em class="italic">Logistic Regression</em>, to load the health information data and do the preprocessing. There <a id="_idIndexMarker954"/>is no need to repeat those steps here. If you have not read <a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a>, <em class="italic">Logistic Regression</em>, it might be helpful to at least scan the code in the second section of that chapter. That will give you a better sense of the features.</p>
<ol>
<li value="2">Now, let’s grab the data that’s been processed by the <strong class="source-inline">healthinfo</strong> module and display the feature names:<p class="source-code">X_train = hi.X_train</p><p class="source-code">X_test = hi.X_test</p><p class="source-code">y_train = hi.y_train</p><p class="source-code">y_test = hi.y_test</p><p class="source-code">new_cols = hi.new_cols</p><p class="source-code">new_cols</p><p class="source-code"><strong class="bold">array(['smoking_Yes', 'alcoholdrinkingheavy_Yes',</strong></p><p class="source-code"><strong class="bold">       'stroke_Yes', 'walkingdifficult_Yes',</strong></p><p class="source-code"><strong class="bold">       'physicalactivity_Yes', 'asthma_Yes',</strong></p><p class="source-code"><strong class="bold">       'kidneydisease_Yes', 'skincancer_Yes',</strong></p><p class="source-code"><strong class="bold">       'gender_Male', 'ethnicity_Asian',</strong></p><p class="source-code"><strong class="bold">       'ethnicity_Black', 'ethnicity_Hispanic',</strong></p><p class="source-code"><strong class="bold">       'ethnicity_Other', 'ethnicity_White',</strong></p><p class="source-code"><strong class="bold">       'agecategory', 'genhealth', 'diabetic', 'bmi',</strong></p><p class="source-code"><strong class="bold">       'physicalhealthbaddays', 'mentalhealthbaddays',</strong></p><p class="source-code"><strong class="bold">       'sleeptimenightly'], dtype=object)</strong></p></li>
<li>We <a id="_idIndexMarker955"/>can use K-fold cross-validation to assess this model. We discussed K-fold cross-validation in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>. We will indicate that we want 10 splits <a id="_idIndexMarker956"/>repeated 10 times. The defaults are <strong class="source-inline">5</strong> and <strong class="source-inline">10</strong>, respectively.</li>
</ol>
<p>The precision of our model, the rate at which we are correct when we predict heart disease, is exceptionally poor at <strong class="source-inline">0.17</strong>. Sensitivity, the rate at which we predict heart disease when there is heart disease, is also low at <strong class="source-inline">0.56</strong>:</p>
<p class="source-code">knn_example = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)</p>
<p class="source-code">kf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)</p>
<p class="source-code">pipe0 = make_pipeline(hi.coltrans, hi.smotenc, knn_example)</p>
<p class="source-code">scores = cross_validate(pipe0, X_train,</p>
<p class="source-code">  y_train.values.ravel(), \</p>
<p class="source-code">  scoring=['accuracy','precision','recall','f1'], \</p>
<p class="source-code">  cv=kf, n_jobs=-1)</p>
<p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, precision: %.2f, f1: %.2f"  %</p>
<p class="source-code">  (np.mean(scores['test_accuracy']),\</p>
<p class="source-code">  np.mean(scores['test_recall']),\</p>
<p class="source-code">  np.mean(scores['test_precision']),\</p>
<p class="source-code">  np.mean(scores['test_f1'])))</p>
<p class="source-code"><strong class="bold">accuracy: 0.73, sensitivity: 0.56, precision: 0.17, f1: 0.26</strong></p>
<ol>
<li value="4">We <a id="_idIndexMarker957"/>can improve the performance of our model with some hyperparameter tuning. Let's create <a id="_idIndexMarker958"/>a dictionary for several neighbors and distance metrics. We will also try different values for the number of features selected with our <strong class="source-inline">filter</strong> method:<p class="source-code">knn = KNeighborsClassifier(n_jobs=-1)</p><p class="source-code">pipe1 = make_pipeline(hi.coltrans, hi.smotenc,</p><p class="source-code">   SelectKBest(score_func=chi2), knn)</p><p class="source-code">knn_params = {</p><p class="source-code"> 'selectkbest__k':</p><p class="source-code">    randint(1, len(new_cols)),</p><p class="source-code"> 'kneighborsclassifier__n_neighbors':</p><p class="source-code">    randint(5, 300),</p><p class="source-code"> 'kneighborsclassifier__metric':</p><p class="source-code">    ['euclidean','manhattan','minkowski']</p><p class="source-code">}</p></li>
<li>We <a id="_idIndexMarker959"/>will base <a id="_idIndexMarker960"/>the scoring <a id="_idIndexMarker961"/>in the grid search on the area under the <strong class="bold">receiver operating characteristic curve</strong> (<strong class="bold">ROC curve</strong>). We covered ROC curves in <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for Model Evaluation</em>:<p class="source-code">rs = RandomizedSearchCV(pipe1, knn_params, cv=5, scoring="roc_auc")</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p></li>
<li>We can use the best estimator attribute of the randomized grid search to get the selected features from <strong class="source-inline">selectkbest</strong>:<p class="source-code">selected = rs.best_estimator_['selectkbest'].\</p><p class="source-code">  get_support()</p><p class="source-code">selected.sum()</p><p class="source-code"><strong class="bold">11</strong></p><p class="source-code">new_cols[selected]</p><p class="source-code"><strong class="bold">array(['smoking_Yes', 'alcoholdrinkingheavy_Yes',</strong></p><p class="source-code"><strong class="bold">       'walkingdifficult_Yes', 'ethnicity_Black',</strong></p><p class="source-code"><strong class="bold">       'ethnicity_Hispanic', 'agecategory',</strong></p><p class="source-code"><strong class="bold">       'genhealth', 'diabetic', 'bmi',</strong></p><p class="source-code"><strong class="bold">       'physicalhealthbaddays','mentalhealthbaddays'],</strong></p><p class="source-code"><strong class="bold">      dtype=object)</strong></p></li>
<li>We <a id="_idIndexMarker962"/>can also <a id="_idIndexMarker963"/>take a look at the best parameters and the best score. 11 features (out of 17) were selected, as we saw in the previous step. A <em class="italic">k</em> (<strong class="source-inline">n_neighbors</strong>) of <strong class="source-inline">254</strong> and the Manhattan distance metric were the other hyperparameters of the highest scoring model:<p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'kneighborsclassifier__metric': 'manhattan',</strong></p><p class="source-code"><strong class="bold"> 'kneighborsclassifier__n_neighbors': 251,</strong></p><p class="source-code"><strong class="bold"> 'selectkbest__k': 11}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.8030553205304845</strong></p></li>
<li>Let’s look at some more metrics for this model. We are doing much better with sensitivity but not with any of the other metrics:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred, </p><p class="source-code">    pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.67, sensitivity: 0.82, specificity: 0.66, precision: 0.18</strong></p></li>
<li>We <a id="_idIndexMarker964"/>should also plot the confusion matrix. To do this, we can look at the relatively decent <a id="_idIndexMarker965"/>sensitivity. Here, we correctly identify most of the actual positives as positive. However, this comes at the expense of many false positives. We can see this in the precision score from the previous step. Most of the time we predict positive, we are wrong:<p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(</p><p class="source-code">  confusion_matrix=cm,</p><p class="source-code">  display_labels=['Negative', 'Positive'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Heart Disease Prediction Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer209">
<img alt="Figure 12.3 – Heart disease confusion matrix after hyperparameter tuning " height="442" src="image/B17978_12_0031.jpg" width="566"/>
</div>
</div>
<p class="figure-caption">Figure 12.3 – Heart disease confusion matrix after hyperparameter tuning</p>
<p>In <a id="_idIndexMarker966"/>this section, you learned <a id="_idIndexMarker967"/>how to use KNN with a binary target. We can follow very similar steps to use KNN for a multiclass classification problem.</p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor149"/>KNN for multiclass classification</h1>
<p>Constructing <a id="_idIndexMarker968"/>a KNN <a id="_idIndexMarker969"/>multiclass model is quite straightforward since it involves no special extensions to the algorithm, such as those needed to adapt logistic regression to targets with more than two values. We can see this by working with the same machine failure data that we worked with in the <em class="italic">Multinomial logistic regression</em> section of <a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a>, <em class="italic">Logistic Regression</em>.</p>
<p class="callout-heading">Note</p>
<p class="callout">This <a id="_idIndexMarker970"/>dataset on machine failure is available for public use at <a href="https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification">https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification</a>. There are 10,000 observations, 12 features, and two possible targets. One is binary and specifies whether the machine failed or did not. The other contains types of failure. The instances in this dataset are synthetic, generated by a process designed to mimic machine failure rates and causes.</p>
<p>Let’s <a id="_idIndexMarker971"/>build our machine <a id="_idIndexMarker972"/>failure type model:</p>
<ol>
<li value="1">First, let’s load the now-familiar modules:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder, MinMaxScaler</p><p class="source-code">from imblearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from imblearn.over_sampling import SMOTENC</p><p class="source-code">from sklearn.feature_selection import SelectKBest, chi2</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Let’s <a id="_idIndexMarker973"/>load the machine failure data and take a look at its structure. There are 10,000 observations <a id="_idIndexMarker974"/>and no missing data. There is a combination of categorical and numerical data:<p class="source-code">machinefailuretype = pd.read_csv("data/machinefailuretype.csv")</p><p class="source-code">machinefailuretype.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">RangeIndex: 10000 entries, 0 to 9999</strong></p><p class="source-code"><strong class="bold">Data columns (total 10 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column               Non-Null Count  Dtype  </strong></p><p class="source-code"><strong class="bold">---  ------               --------------  -----  </strong></p><p class="source-code"><strong class="bold"> 0   udi                  10000 non-null  int64  </strong></p><p class="source-code"><strong class="bold"> 1   product              10000 non-null  object </strong></p><p class="source-code"><strong class="bold"> 2   machinetype          10000 non-null  object </strong></p><p class="source-code"><strong class="bold"> 3   airtemp              10000 non-null  float64</strong></p><p class="source-code"><strong class="bold"> 4   processtemperature   10000 non-null  float64</strong></p><p class="source-code"><strong class="bold"> 5   rotationalspeed      10000 non-null  int64  </strong></p><p class="source-code"><strong class="bold"> 6   torque               10000 non-null  float64</strong></p><p class="source-code"><strong class="bold"> 7   toolwear             10000 non-null  int64  </strong></p><p class="source-code"><strong class="bold"> 8   fail                 10000 non-null  int64  </strong></p><p class="source-code"><strong class="bold"> 9   failtype             10000 non-null  object </strong></p><p class="source-code"><strong class="bold">dtypes: float64(3), int64</strong>(4), object(3)</p><p class="source-code">memory usage: 781.4+ KB</p></li>
<li>Let’s also look at a few observations:<p class="source-code">machinefailuretype.head()</p><p class="source-code">   <strong class="bold">udi product machinetype airtemp processtemperature\</strong></p><p class="source-code"><strong class="bold">0   1   M14860      M         298         309 </strong></p><p class="source-code"><strong class="bold">1   2   L47181      L         298         309 </strong></p><p class="source-code"><strong class="bold">2   3   L47182      L         298         308 </strong></p><p class="source-code"><strong class="bold">3   4   L47183      L         298         309 </strong></p><p class="source-code"><strong class="bold">4   5   L47184      L         298         309 </strong></p><p class="source-code"><strong class="bold"> Rotationalspeed  Torque  toolwear  fail   failtype</strong></p><p class="source-code"><strong class="bold">0      1551         43         0      0    No Failure</strong></p><p class="source-code"><strong class="bold">1      1408         46         3      0    No Failure</strong></p><p class="source-code"><strong class="bold">2      1498         49         5      0    No Failure</strong></p><p class="source-code"><strong class="bold">3      1433         40         7      0    No Failure</strong></p><p class="source-code"><strong class="bold">4      1408         40         9      0    No Failure</strong></p></li>
<li>We <a id="_idIndexMarker975"/>should also do some frequencies on the categorical features. The overwhelming <a id="_idIndexMarker976"/>majority of observations, 97%, have no failures. This rather stark class imbalance will likely be difficult to model. There are three machine types – high quality, low quality, and medium quality:<p class="source-code">machinefailuretype.failtype.value_counts(dropna=False).sort_index()</p><p class="source-code"><strong class="bold">Heat Dissipation Failure  112</strong></p><p class="source-code"><strong class="bold">No Failure                9652</strong></p><p class="source-code"><strong class="bold">Overstrain Failure        78</strong></p><p class="source-code"><strong class="bold">Power Failure             95</strong></p><p class="source-code"><strong class="bold">Random Failures           18</strong></p><p class="source-code"><strong class="bold">Tool Wear Failure         45</strong></p><p class="source-code"><strong class="bold">Name: failtype, dtype: int64</strong></p><p class="source-code">machinefailuretype.machinetype.\</p><p class="source-code">  value_counts(dropna=False).sort_index()</p><p class="source-code"><strong class="bold">H      1003</strong></p><p class="source-code"><strong class="bold">L      6000</strong></p><p class="source-code"><strong class="bold">M      2997</strong></p><p class="source-code"><strong class="bold">Name: machinetype, dtype: int64</strong></p></li>
<li>Let’s <a id="_idIndexMarker977"/>collapse some of the <strong class="source-inline">failtype</strong> values and check our work. First, we will define <a id="_idIndexMarker978"/>a function, <strong class="source-inline">setcode</strong>, to map the failure type text to a failure type code. We will assign random failures and tool wear failures to code <strong class="source-inline">5</strong> for other failures:<p class="source-code">def setcode(typetext):</p><p class="source-code">  if (typetext=="No Failure"):</p><p class="source-code">    typecode = 1</p><p class="source-code">  elif (typetext=="Heat Dissipation Failure"):</p><p class="source-code">    typecode = 2</p><p class="source-code">  elif (typetext=="Power Failure"):</p><p class="source-code">    typecode = 3</p><p class="source-code">  elif (typetext=="Overstrain Failure"):</p><p class="source-code">    typecode = 4</p><p class="source-code">  else:</p><p class="source-code">    typecode = 5</p><p class="source-code">  return typecode</p><p class="source-code">machinefailuretype["failtypecode"] = \</p><p class="source-code">  machinefailuretype.apply(lambda x: setcode(x.failtype), axis=1)</p><p class="source-code">machinefailuretype.groupby(['failtypecode','failtype']).size().\</p><p class="source-code">  reset_index()</p><p class="source-code"><strong class="bold">   failtypecode  failtype                   0</strong></p><p class="source-code"><strong class="bold">0   1            No Failure                 9652</strong></p><p class="source-code"><strong class="bold">1   2            Heat Dissipation Failure   112</strong></p><p class="source-code"><strong class="bold">2   3            Power Failure              95</strong></p><p class="source-code"><strong class="bold">3   4            Overstrain Failure         78</strong></p><p class="source-code"><strong class="bold">4   5            Random Failures            18</strong></p><p class="source-code"><strong class="bold">5   5            Tool Wear Failure          45</strong></p></li>
<li>We <a id="_idIndexMarker979"/>should look <a id="_idIndexMarker980"/>at some descriptive statistics for our numeric features:<p class="source-code">num_cols = ['airtemp', 'processtemperature',</p><p class="source-code">  'rotationalspeed', 'torque', 'toolwear']</p><p class="source-code">cat_cols = ['machinetype']</p><p class="source-code">machinefailuretype[num_cols].agg(['min','median','max']).T</p><p class="source-code"><strong class="bold">                       min       median       max</strong></p><p class="source-code"><strong class="bold">airtemp                295       300          304</strong></p><p class="source-code"><strong class="bold">processtemperature     306       310          314</strong></p><p class="source-code"><strong class="bold">rotationalspeed        1,168     1,503        2,886</strong></p><p class="source-code"><strong class="bold">torque                 4         40           77</strong></p><p class="source-code"><strong class="bold">toolwear               0         108          253</strong></p></li>
<li>Now, we <a id="_idIndexMarker981"/>are ready to create training and testing DataFrames. We will use the failure type <a id="_idIndexMarker982"/>code we just created for our target:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(machinefailuretype[num_cols + cat_cols],\</p><p class="source-code">  machinefailuretype[['failtypecode']], test_size=0.2, random_state=0)</p></li>
<li>Now, let’s set up the column transformation. For the numerical features, we will set outliers to the median and then scale the data. We will use min-max scaling, which will return values from 0 to 1 (the default for <strong class="source-inline">MinMaxScaler</strong>). We are using this scaler, rather than the standard scaler, to avoid negative values. The feature selection method we will use later, <strong class="source-inline">selectkbest</strong>, does not work with negative values:<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">standtrans = make_pipeline(</p><p class="source-code">  OutlierTrans(3),SimpleImputer(strategy="median"),</p><p class="source-code">  MinMaxScaler())</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("cat", cattrans, cat_cols),</p><p class="source-code">    ("stand", standtrans, num_cols),</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>Let’s <a id="_idIndexMarker983"/>also take a peek at the columns that we will have after the encoding. We need this <a id="_idIndexMarker984"/>in advance of the oversampling we will do since the <strong class="source-inline">SMOTENC</strong> module needs the column indexes of the categorical features. We are oversampling to handle the significant class imbalance. We discussed this in more detail in <a href="B17978_11_ePub.xhtml#_idTextAnchor135"><em class="italic">Chapter 11</em></a>, <em class="italic">Decision Trees and Random Forest Classification</em>:<p class="source-code">coltrans.fit(X_train.sample(1000))</p><p class="source-code">new_cat_cols = \</p><p class="source-code">  coltrans.\</p><p class="source-code">  named_transformers_['cat'].\</p><p class="source-code">  named_steps['onehotencoder'].\</p><p class="source-code">  get_feature_names(cat_cols)</p><p class="source-code">new_cols = np.concatenate((new_cat_cols, np.array(num_cols)))</p><p class="source-code">print(new_cols)</p><p class="source-code"><strong class="bold">['machinetype_L' 'machinetype_M' 'airtemp' </strong></p><p class="source-code"><strong class="bold">'processtemperature' 'rotationalspeed' 'torque' </strong></p><p class="source-code"><strong class="bold">'toolwear']</strong></p></li>
<li>Next, we <a id="_idIndexMarker985"/>will set up a pipeline for our model. The pipeline will do a column transformation, oversampling using <strong class="source-inline">SMOTENC</strong>, feature selection with <strong class="source-inline">selectkbest</strong>, and then run a KNN model. Remember that we have to pass the <a id="_idIndexMarker986"/>column indexes of the categorical features to <strong class="source-inline">SMOTENC</strong> for it to run correctly:<p class="source-code">catcolscnt = new_cat_cols.shape[0]</p><p class="source-code">smotenc = SMOTENC(categorical_features=np.arange(0,catcolscnt), random_state=0)</p><p class="source-code">knn = KNeighborsClassifier(n_jobs=-1)</p><p class="source-code">pipe1 = make_pipeline(coltrans, smotenc, SelectKBest(score_func=chi2), knn)</p></li>
<li>Now, we are ready to fit our model. We will do a randomized grid search to identify the best value for <em class="italic">k</em> and the distance metric for the KNN. We will also search for the best <em class="italic">k</em> value for the feature selection:<p class="source-code">knn_params = {</p><p class="source-code"> 'selectkbest__k': np.arange(1, len(new_cols)),</p><p class="source-code"> 'kneighborsclassifier__n_neighbors': np.arange(5, 175, 2),</p><p class="source-code"> 'kneighborsclassifier__metric': ['euclidean','manhattan','minkowski']</p><p class="source-code">}</p><p class="source-code">rs = RandomizedSearchCV(pipe1, knn_params, cv=5, scoring="roc_auc_ovr_weighted")</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p></li>
<li>Let’s <a id="_idIndexMarker987"/>take a look at what our grid search found. All the features except for <strong class="source-inline">processtemperature</strong> were worth keeping in the model. The best values for <em class="italic">k</em> and <a id="_idIndexMarker988"/>the distance metric for the KNN were <strong class="source-inline">125</strong> and <strong class="source-inline">minkowski</strong>, respectively.  The best score, based on the area under the ROC curve, was <strong class="source-inline">0.9</strong>:<p class="source-code">selected = rs.best_estimator_['selectkbest'].get_support()</p><p class="source-code">selected.sum()</p><p class="source-code">6</p><p class="source-code">new_cols[selected]</p><p class="source-code">array(['machinetype_L', 'machinetype_M', 'airtemp', </p><p class="source-code">       'rotationalspeed', 'torque', 'toolwear'], </p><p class="source-code">       dtype=object)</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'selectkbest__k': 6,</strong></p><p class="source-code"><strong class="bold"> 'kneighborsclassifier__n_neighbors': 125,</strong></p><p class="source-code"><strong class="bold"> 'kneighborsclassifier__metric': 'minkowski'}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.899426752716227</strong></p></li>
<li>Let’s look at a confusion matrix. Looking at the first row, we can see that a sizeable number of failures were found when no failure happened. However, our model does correctly identify most of the actual heat, power, and overstrain failures. This <a id="_idIndexMarker989"/>may not be a horrible precision and sensitivity trade-off. Depending on the problem, we may accept a large number of false positives to get <a id="_idIndexMarker990"/>an acceptable level of sensitivity in our model:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(confusion_matrix=cm,</p><p class="source-code">   display_labels=['None', 'Heat','Power','Overstrain','Other'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Machine Failure Type Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer210">
<img alt="Figure 12.4 – Confusion matrix for machine failure type after hyperparameter tuning " height="449" src="image/B17978_12_004.jpg" width="575"/>
</div>
</div>
<p class="figure-caption">Figure 12.4 – Confusion matrix for machine failure type after hyperparameter tuning</p>
<ol>
<li value="14">We <a id="_idIndexMarker991"/>should also look at a classification report. You may remember from <a href="B17978_06_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 6</em></a>, <em class="italic">Preparing for</em> <em class="italic">Model Evaluation</em>, that the macro average is a simple average across classes. Here, we <a id="_idIndexMarker992"/>are more interested in the weighted average. The weighted F1-score is not bad at <strong class="source-inline">0.81</strong>. Recall that F1 is the harmonic mean of precision and sensitivity:<p class="source-code">print(skmet.classification_report(y_test, pred,</p><p class="source-code">  target_names=\</p><p class="source-code">  ['None', 'Heat','Power','Overstrain','Other']))</p><p class="source-code">              Precision  recall  f1-score  support</p><p class="source-code">        None  0.99       0.71    0.83      1927</p><p class="source-code">        Heat  0.11       0.90    0.20      21</p><p class="source-code">       Power  0.15       0.61    0.24      18</p><p class="source-code">  Overstrain  0.36       0.76    0.49      21</p><p class="source-code">       Other  0.01       0.31    0.02      13</p><p class="source-code">    accuracy                     0.71      2000</p><p class="source-code">   macro avg  0.33       0.66    0.36      2000</p><p class="source-code">weighted avg  0.96       0.71    0.81      2000</p></li>
</ol>
<p>The class imbalance for the machine failure type makes it particularly difficult to model. Still, our KNN model does relatively well, assuming that the large number of false positives <a id="_idIndexMarker993"/>is not problematic. In this case, a false positive may not be nearly as much of an issue as a false <a id="_idIndexMarker994"/>negative. It may just involve doing more checks on machines that seem to be in danger of failing. If we compare that with being surprised by an actual machine failure, a bias toward sensitivity rather than precision may be appropriate.</p>
<p>Let’s try KNN on another multiclass problem.</p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>KNN for letter recognition</h2>
<p>We can take pretty much the same approach we just took for predicting machine failure with <a id="_idIndexMarker995"/>letter recognition. So long as <a id="_idIndexMarker996"/>we have features that do a good job of discriminating between the letters, KNN is a reasonable choice for that model. We will try this in this section.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will <a id="_idIndexMarker997"/>work with letter recognition data in this section. It is available for public use at <a href="https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition">https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition</a>. There are 26 letters (all capitals) and 20 different fonts. 16 different features capture different attributes of each letter.</p>
<p>Let’s build the model:</p>
<ol>
<li value="1">First, we will load the same libraries we have already been using:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.model_selection import StratifiedKFold, \</p><p class="source-code">  GridSearchCV</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">from scipy.stats import randint</p></li>
<li>Now, we <a id="_idIndexMarker998"/>will load the <a id="_idIndexMarker999"/>data and look at the first few instances. There are 20,000 observations and 17 columns. <strong class="source-inline">letter</strong> is our target:<p class="source-code">letterrecognition = pd.read_csv("data/letterrecognition.csv")</p><p class="source-code">letterrecognition.shape</p><p class="source-code"><strong class="bold">(20000, 17)</strong></p><p class="source-code">letterrecognition.head().T</p><p class="source-code"><strong class="bold">            0     1     2     3     4</strong></p><p class="source-code"><strong class="bold">letter      T     I     D     N     G</strong></p><p class="source-code"><strong class="bold">xbox        2     5     4     7     2</strong></p><p class="source-code"><strong class="bold">ybox        8     12    11    11    1</strong></p><p class="source-code"><strong class="bold">width       3     3     6     6     3</strong></p><p class="source-code"><strong class="bold">height      5     7     8     6     1</strong></p><p class="source-code"><strong class="bold">onpixels    1     2     6     3     1</strong></p><p class="source-code"><strong class="bold">xbar        8     10    10    5     8</strong></p><p class="source-code"><strong class="bold">ybar        13    5     6     9     6</strong></p><p class="source-code"><strong class="bold">x2bar       0     5     2     4     6</strong></p><p class="source-code"><strong class="bold">y2bar       6     4     6     6     6</strong></p><p class="source-code"><strong class="bold">xybar       6     13    10    4     6</strong></p><p class="source-code"><strong class="bold">x2ybar      10     3    3     4     5</strong></p><p class="source-code"><strong class="bold">xy2bar      8     9     7     10    9</strong></p><p class="source-code"><strong class="bold">x-ege       0     2     3     6     1</strong></p><p class="source-code"><strong class="bold">xegvy       8     8     7     10    7</strong></p><p class="source-code"><strong class="bold">y-ege       0     4     3     2     5</strong></p><p class="source-code"><strong class="bold">yegvx       8     10    9     8     10</strong></p></li>
<li>Now, let’s <a id="_idIndexMarker1000"/>create training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(letterrecognition.iloc[:,1:],\</p><p class="source-code">  letterrecognition.iloc[:,0:1], test_size=0.2, </p><p class="source-code">  random_state=0)</p></li>
<li>Next, let’s <a id="_idIndexMarker1001"/>instantiate a KNN instance. We will also set up stratified K-fold cross-validation and a dictionary for the hyperparameters. We will search for the best hyperparameter for <em class="italic">k</em> (<strong class="source-inline">n_neighbors</strong>) and the distance metric:<p class="source-code">knn = KNeighborsClassifier(n_jobs=-1)</p><p class="source-code">kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)</p><p class="source-code">knn_params = {</p><p class="source-code">  'n_neighbors': np.arange(3, 41, 2),</p><p class="source-code">  'metric': ['euclidean','manhattan','minkowski']</p><p class="source-code">}</p></li>
<li>Now, we are ready to do an exhaustive grid search. We are doing an exhaustive search <a id="_idIndexMarker1002"/>here since we do not have a lot of hyperparameters to check. The best-performing distance metric <a id="_idIndexMarker1003"/>is Euclidean. The best value for <em class="italic">k</em> for nearest neighbors is <strong class="source-inline">3</strong>. This model gets us nearly 95% accuracy:<p class="source-code">gs = GridSearchCV(knn, knn_params, cv=kf, scoring='accuracy')</p><p class="source-code">gs.fit(X_train, y_train.values.ravel())</p><p class="source-code">gs.best_params_</p><p class="source-code"><strong class="bold">{'metric': 'euclidean', 'n_neighbors': 3}</strong></p><p class="source-code">gs.best_score_</p><p class="source-code"><strong class="bold">0.9470625</strong></p></li>
<li>Let’s generate predictions and plot a confusion matrix:<p class="source-code">pred = gs.best_estimator_.predict(X_test)</p><p class="source-code">letters = np.sort(letterrecognition.letter.unique())</p><p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = \</p><p class="source-code">  skmet.ConfusionMatrixDisplay(confusion_matrix=cm,</p><p class="source-code">  display_labels=letters)</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Letters', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This <a id="_idIndexMarker1004"/>produces the following <a id="_idIndexMarker1005"/>plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<img alt="Figure 12.5 – Confusion matrix for letter predictions " height="443" src="image/B17978_12_005.jpg" width="505"/>
</div>
</div>
<p class="figure-caption">Figure 12.5 – Confusion matrix for letter predictions</p>
<p>Let’s quickly summarize what we’ve learned in this chapter.</p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor151"/>Summary</h1>
<p>This chapter demonstrated how easy it is to use KNN for binary or multiclass classification. Since KNN does not make assumptions about normality or linearity, it can be used in cases where logistic regression may not yield the best results. This flexibility does bring with it a real risk of overfitting, so care has to be taken with the choice of <em class="italic">k</em>. We also explored how to hyperparameter tune both binary and multiclass models in this chapter. Finally, KNN is not a great option when we care about the speed of our predictions or if we are working with a large dataset. Decision tree or random forest classification, which we explored in the previous chapter, is often a better choice in those cases. </p>
<p>Another really good choice is support vector classification. We will explore support vector classification in the next chapter.</p>
</div>
</div>
</body></html>