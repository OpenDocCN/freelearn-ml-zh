<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Optimizing the Model through Hyperparameter Tuning</h1>
                </header>
            
            <article>
                
<p>Neural networks constitute multiple parameters that can affect the ultimate accuracy in predicting an event or a label. The typical parameters include:</p>
<ul>
<li>Batch size used for training</li>
<li>Number of epochs</li>
<li>Learning rate</li>
<li>Number of hidden layers</li>
<li>Number of hidden units in each hidden layer</li>
<li>The activation function applied in the hidden layer</li>
<li>The optimizer used</li>
</ul>
<p>From the preceding list, we can see that the number of parameters that can be tweaked is very high. This makes finding the optimal combination of hyperparameters a challenge. Hyperparameter tuning as a service provided by Cloud ML Engine comes in handy in such a scenario.</p>
<p>In this chapter, we will go through:</p>
<ul>
<li>Why hyperparameter tuning is required</li>
<li>An overview of how hyperparameter tuning works</li>
<li>Implementing hyperparameter tuning in the cloud</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The intuition of hyperparameter tuning</h1>
                </header>
            
            <article>
                
<p>In order to gain a practical intuition of the need for hyperparameter tuning, let's go through the following scenario in predicting the accuracy of a given neural network architecture on the MNIST dataset:</p>
<ul>
<li><strong>Scenario 1</strong>: High number of epochs and low learning rate</li>
<li><strong>Scenario 2</strong>: Low number of epochs and high learning rate</li>
</ul>
<p>Let us create the train and test datasets in a Google Cloud environment, as follows:</p>
<ol>
<li>Download the dataset:</li>
</ol>
<pre style="padding-left: 60px">mkdir data
curl -O https://s3.amazonaws.com/img-datasets/mnist.pkl.gz
gzip -d mnist.pkl.gz
mv mnist.pkl data/ <strong>  </strong>         </pre>
<p style="padding-left: 60px">The preceding code creates a new folder named <kbd>data</kbd>, downloads the MNIST dataset, and moves it into the <kbd>data</kbd> folder.</p>
<ol start="2">
<li>Open Python in Terminal and import the required packages:</li>
</ol>
<pre style="padding-left: 60px">from __future__ import print_function <br/>import tensorflow as tf<br/>import pickle # for handling the new data source<br/>import numpy as np<br/>from datetime import datetime # for filename conventions<br/>from tensorflow.python.lib.io import file_io # for better file I/O<br/>import sys</pre>
<ol start="3">
<li>Import the MNIST dataset:</li>
</ol>
<pre style="padding-left: 60px">f = file_io.FileIO('data/mnist.pkl', mode='r')<br/>data = pickle.load(f)</pre>
<ol start="4">
<li>Extract the train and test datasets:</li>
</ol>
<pre style="padding-left: 60px">(x_train, y_train), (x_test, y_test) = data<br/># Converting the data from a 28 x 28 shape to 784 columns<br/>x_train = x_train.reshape(60000, 784)<br/>x_train = x_train.astype('float32')<br/># Scaling the train dataset<br/>x_train /= 255<br/># Reshaping the test dataset<br/>x_test = x_test.reshape(10000, 784)<br/>x_test = x_test.astype('float32')<br/># Scaling the test dataset<br/>x_test /= 255<br/># Specifying the type of labels<br/>y_train = y_train.astype(np.int32)<br/>y_test = y_test.astype(np.int32)</pre>
<ol start="5">
<li>Create the estimator functions:</li>
</ol>
<pre style="padding-left: 60px"># Creating the estimator input functions for train and test datasets <br/>train_input_fn = tf.estimator.inputs.numpy_input_fn(<br/> x={"x2": np.array(x_train)},<br/> y=np.array(y_train),<br/> num_epochs=None,<br/> batch_size=1024,<br/> shuffle=True)<br/>test_input_fn = tf.estimator.inputs.numpy_input_fn(<br/> x={"x2": np.array(x_test)},<br/> y=np.array(y_test),<br/> num_epochs=1,<br/> shuffle=False)</pre>
<ol start="6">
<li>Specify the type of column:</li>
</ol>
<pre style="padding-left: 60px">feature_x = tf.feature_column.numeric_column("x2", shape=(784))<br/>feature_columns = [feature_x]</pre>
<ol start="7">
<li>Build a DNN classifier using the parameters in scenario 1; that is, the learning rate is <kbd>0.1</kbd> and the number of steps is <kbd>200</kbd>:</li>
</ol>
<pre style="padding-left: 60px">num_hidden_units = [1000]<br/>lr=0.1<br/>num_steps=200<br/># Building the estimator using DNN classifier<br/># This is where the learning rate hyper parameter is passed<br/>model = tf.estimator.DNNClassifier(feature_columns=feature_columns,<br/>                 hidden_units=num_hidden_units,<br/>                 activation_fn=tf.nn.relu,<br/>                 n_classes=10,<br/>                 optimizer=tf.train.AdagradOptimizer(learning_rate = lr))<br/>model.train(input_fn=train_input_fn, steps=num_steps) <br/># Fetching the model results<br/>result = model.evaluate(input_fn=test_input_fn)<br/>print('Test loss:', result['average_loss'])<br/>print('Test accuracy:', result['accuracy'])</pre>
<p>The test accuracy in such a scenario comes out to be 96.49%.</p>
<p><span>In scenario 2, we will build another DNN classifier using different parameters; now the learning rate is <kbd>0.01</kbd> and the number of steps is <kbd>2000</kbd>:</span></p>
<pre>num_hidden_units = [1000]<br/>lr=0.01<br/>num_steps=2000<br/># Building the estimator using DNN classifier<br/># This is where the learning rate hyper parameter is passed<br/>model = tf.estimator.DNNClassifier(feature_columns=feature_columns,<br/> hidden_units=num_hidden_units,<br/> activation_fn=tf.nn.relu,<br/> n_classes=10,<br/> optimizer=tf.train.AdagradOptimizer(learning_rate = lr))<br/>model.train(input_fn=train_input_fn, steps=num_steps) <br/># Fetching the model results<br/>result = model.evaluate(input_fn=test_input_fn)<br/>print('Test loss:', result['average_loss'])<br/>print('Test accuracy:', result['accuracy']) </pre>
<p>The accuracy on the test dataset in scenario 2 is nearly 98.2%.</p>
<p>The preceding two scenarios show us the importance of how various values of different hyperparameters affect the final result.</p>
<p>Google Cloud ML engine comes in handy in such scenarios, where we can be more intelligent in selecting the more optimal set of hyperparameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of hyperparameter tuning</h1>
                </header>
            
            <article>
                
<p>Hyperparameter tuning works by running multiple trials in a single training job. Each trial is a complete execution of your training application, with values for your chosen hyperparameters set within the limits you specify. The Cloud ML Engine training service keeps track of the results of each trial and makes adjustments for subsequent trials. When the job is finished, you can get a summary of all the trials, along with the most effective configuration of values according to the criteria you specify.</p>
<p>We want to select those hyperparameters that give the best performance. This amounts to an optimization problem, specifically, the problem of optimizing a function <em>f(x) </em>(that is, performance as a function of hyperparameter values) over a compact set <em>A.</em> We can write this mathematically as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1b2c95c0-1900-4be2-942a-1c35f559a5be.png" style="width:62.33em;height:4.00em;"/></div>
<p>Let's take the example of the function <em>(1-x)<sup>ex</sup></em>, which has a maximum value <em>f(x) = 1</em> at <em>x = 0</em>, and so <em>arg max</em> is <em>0</em>.</p>
<p>Many optimization settings, like this one, assume that the objective function <em>f(x)</em> has a known mathematical form, is convex, or is easy to evaluate. But these characteristics do not apply to the problem of finding hyperparameters where the function is unknown and expensive to evaluate. This is where Bayesian optimization comes into play.</p>
<p>In order to implement hyperparameter tuning, Google uses an algorithm called <strong>Gaussian process bandits</strong>, which is a form of Bayesian optimization.</p>
<p>Bayesian optimization is an extremely powerful technique when the mathematical form of the function is unknown or expensive to compute. The main idea behind it is to compute a posterior distribution over the objective function based on the data (using the famous Bayes, theorem), and then select good points to try with respect to this distribution.</p>
<p>To use Bayesian optimization, we need a way to flexibly model distributions over objective functions. This is a bit trickier than modeling a distribution over, say, real numbers, since we'll need one such distribution to represent our beliefs about <em>f(x)</em> for each <em>x</em>. If <em>x</em> contains continuous hyperparameters, there will be infinitely many <em>x</em> for which we must model <em>f(x)</em>, that is, construct a distribution for it. For this problem, Gaussian processes are a particularly elegant technique. In effect, they generalize multidimensional Gaussian distributions, and versions that are flexible enough to model any objective function do <span>exist</span><span>.</span></p>
<p>The preceding process typically works as described in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/920da612-d83c-42ae-9ae5-3f4a61a250e1.png" style=""/></div>
<p>We update the Gaussian model with the results of an iteration, which further helps in identifying the right next sample set of hyperparameters to be tested for the model; the result further improves our Gaussian model to identify the right set of hyperparameters to be picked.</p>
<p>Details of Gaussian distributions are out of the scope of this book, but for this exercise, we will take Google's approach as it is (as a black box) and implement hyperparameter tuning using Google Cloud.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning in Google Cloud</h1>
                </header>
            
            <article>
                
<p>In order for the just-laid-out Gaussian process to run, we have to allow our model builds to be run on Google Cloud so that hyperparameter tuning can be carried out.</p>
<p>In order to run hyperparameter tuning, the following are essential components:</p>
<ul>
<li>Data file and its location</li>
<li>Model file</li>
<li>Hyperparameter configuration file</li>
<li>Setup file</li>
<li><kbd>__init__</kbd> file</li>
</ul>
<p>Given that we are running the model on Google Cloud ML engine, the data should be residing in a Cloud bucket so that it becomes accessible to ML engine.</p>
<p>This can be done by performing the following in the Cloud shell:</p>
<pre><strong>gsutil mb gs://my-mnist-bucket</strong><br/><strong>gsutil cp -r data/mnist.pkl gs://my-mnist-bucket/data/mnist.pkl</strong></pre>
<p>Note that, using the preceding steps, we have created a bucket named <kbd>my-mnist-bucket</kbd> and copied our data to that bucket. The preceding code should result in creating a directory named <kbd>data</kbd> and the <kbd>mnist.pkl</kbd> file in that directory:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/59110e1b-da58-43b9-87c7-27a8b80cb10c.jpg" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The model file</h1>
                </header>
            
            <article>
                
<p><strong> </strong>The model file should be located in a folder that also contains the <kbd>__init__.py</kbd> file.</p>
<p>Let us create a folder named <kbd>trainer</kbd> that contains both the model file and the <kbd>__init__</kbd> file:</p>
<pre>mkdir trainer<br/>cd trainer</pre>
<p>The preceding code creates the <kbd>trainer</kbd> folder and changes directory to the newly-created folder.</p>
<p>Let us go ahead and create the model file as follows:</p>
<pre>vim mnist_mlp_lr_numsteps.py</pre>
<p>Insert the following code into the previously-created file:</p>
<pre>from __future__ import print_function<br/><br/>import argparse<br/>import pickle <br/>from datetime import datetime <br/>import numpy as np<br/>from tensorflow.python.lib.io import file_io # for better file I/O<br/>import sys<br/>import tensorflow as tf<br/><br/>def train_model(train_file='data/mnist.pkl',job_dir='./tmp/mnist_mlp', num_steps = 1, lr=0.1, **args):<br/>  # logs_path gives access to the logs that are generated by the previous epochs of model<br/>  logs_path = job_dir + '/logs/' + str(datetime.now().isoformat())<br/>  print('Using logs_path located at {}'.format(logs_path))<br/>  # by default floats are considered as string<br/>  # Good idea to convert them back into floats<br/>  lr=float(lr)<br/>  num_steps=float(num_steps)<br/>  batch_size = 1024<br/>  num_classes = 10<br/>  # Reading in the pickle file. Pickle works differently with Python 2 vs 3<br/>  # In Python 2 the following code would be:<br/>  # f = file_io.FileIO(train_file, mode='r')<br/>  # data = pickle.load(f)<br/>  f = file_io.FileIO(train_file, mode='rb') <br/>  data = pickle.load(f,encoding='bytes') <br/>  (x_train, y_train), (x_test, y_test) = data<br/>  # Converting the data from a 28X28 shape to 784 columns<br/>  x_train = x_train.reshape(60000, 784)<br/>  x_train = x_train.astype('float32')<br/>  x_test = x_test.reshape(10000, 784)<br/>  x_test = x_test.astype('float32')<br/>  x_train /= 255<br/>  x_test /= 255<br/>  # Specifying the type of following labels<br/>  y_train = y_train.astype(np.int32)<br/>  y_test = y_test.astype(np.int32)<br/>  <br/>  # Creating the estimator following input functions <br/>  train_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>    x={"x2": np.array(x_train)},<br/>    y=np.array(y_train),<br/>    num_epochs=None,<br/>    batch_size=batch_size,<br/>    shuffle=True)<br/>  test_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>    x={"x2": np.array(x_test)},<br/>    y=np.array(y_test),<br/>    num_epochs=1,<br/>    shuffle=False)<br/>  # Specifying the columns as numeric columns<br/>  feature_x = tf.feature_column.numeric_column("x2", shape=(784))<br/>  feature_columns = [feature_x]<br/>  num_hidden_units = [1000]<br/>  # Building the estimator using DNN classifier<br/>  # This is where the learning rate hyper parameter is passed<br/>  model = tf.estimator.DNNClassifier(feature_columns=feature_columns,<br/>                                   hidden_units=num_hidden_units,<br/>                                   activation_fn=tf.nn.relu,<br/>                                   n_classes=num_classes,<br/>                   optimizer=tf.train.AdagradOptimizer(learning_rate = lr))<br/>  # Passing the other parameter: num_steps<br/>  model.train(input_fn=train_input_fn, steps=num_steps) <br/>  # Fetching the model results<br/>  result = model.evaluate(input_fn=test_input_fn)<br/>  print('Test loss:', result['average_loss'])<br/>  print('Test accuracy:', result['accuracy'])<br/>  <br/>if __name__ == '__main__':<br/>  # Parse the input arguments for common Cloud ML Engine options<br/>  # There are 4 arguments that we need to give, as per the preceding model specification <br/>  # training file location, job directory, number of steps and learning rate<br/>  parser = argparse.ArgumentParser()<br/>  parser.add_argument(<br/>    '--train-file',<br/>    help='Cloud Storage bucket or local path to training data')<br/>  parser.add_argument(<br/>    '--job-dir',<br/>    help='Cloud storage bucket to export the model and store temp files')<br/>  parser.add_argument(<br/>    '--num-steps',<br/>    help='number of steps')<br/>  parser.add_argument(<br/>    '--lr',<br/>    help='learning rate') <br/>  <br/>  args = parser.parse_args()<br/>  arguments = args.__dict__<br/>  train_model(**arguments)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration file</h1>
                </header>
            
            <article>
                
<p>Once the model file is set up, we need to provide the configuration file in the same trainer folder so that ML engine knows the parameters that need to be tuned, as well as the typical min and max values of the parameter<span><span>.</span></span></p>
<p>We create the configuration file as follows in the <kbd>trainer</kbd> folder:</p>
<pre>vim hptune.yaml</pre>
<p>The following code is inserted into the preceding file:</p>
<pre>trainingInput:<br/>  pythonVersion: "3.5"<br/>  scaleTier: CUSTOM<br/>  masterType: standard_gpu<br/>  hyperparameters:<br/>    goal: MAXIMIZE<br/>    hyperparameterMetricTag: accuracy<br/>    maxTrials: 10<br/>    maxParallelTrials: 1<br/>    params:<br/>      - parameterName: num-steps<br/>        type: INTEGER<br/>        minValue: 200<br/>        maxValue: 10000<br/>        scaleType: UNIT_LINEAR_SCALE<br/>      - parameterName: lr<br/>        type: DOUBLE<br/>        minValue: 0.001<br/>        maxValue: 0.1<br/>        scaleType: UNIT_LOG_SCALE</pre>
<p>In the preceding block of code, we have specified the Python version to be run on and have also specified whether it is to be run on a CPU or a GPU.</p>
<p>In the <kbd>hyperparameters</kbd> section, we have specified that the metric we need to optimize is accuracy (note that the output of <kbd>model.evaluate</kbd> is <kbd>accuracy</kbd>, <kbd>loss</kbd>, <kbd>average loss</kbd>, and <kbd>global step</kbd>); the goal is to maximize it.</p>
<p>Also, we have specified the maximum number of trials to be run and the maximum number of parallel trials that can be run (changes when the Cloud configuration has multiple cores associated with it).</p>
<p>The <kbd>params</kbd> section contains the parameters that need to be modified, the type of variable it is, and the minimum and maximum values.</p>
<p><kbd>ScaleType</kbd> indicates the type of scaling that would be applied to the parameter:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Value</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>UNIT_LINEAR_SCALE</kbd></p>
</td>
<td>
<p>Scales the feasible space to (0, 1) linearly.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>UNIT_LOG_SCALE</kbd></p>
</td>
<td>
<p>Scales the feasible space logarithmically to (0, 1). The entire feasible space must be strictly positive.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>UNIT_REVERSE_LOG_SCALE</kbd></p>
</td>
<td>
<p>Scales the feasible space reverse logarithmically to (0, 1). The result is that values close to the top of the feasible space are spread out more than points near the bottom. The entire feasible space must be strictly positive.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setup file</h1>
                </header>
            
            <article>
                
<p>In some instances, we might have to install packages that do not come prebuilt. The <kbd>setup.py</kbd> file comes in handy in such scenarios:</p>
<pre>from setuptools import setup, find_packages<br/>setup(name='mnist_mlp_lr_numsteps',<br/>      version='1.0',<br/>      packages=find_packages(),<br/>      include_package_data=True,<br/>      install_requires=[<br/>          'keras',<br/>          'h5py'],<br/>      zip_safe=False)</pre>
<p>In the preceding code, one could include the additional packages that are needed to run the model file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The __init__ file</h1>
                </header>
            
            <article>
                
<p>For Cloud ML engine to create a package for the module we are building, it needs to create a package for the module. For the package to be created, it needs to create the <kbd>__init__.py</kbd> file in the <kbd>trainer</kbd> folder.</p>
<p>For that, we would run the following code:</p>
<pre>touch trainer/__init__.py</pre>
<p>Now that the whole setup is ready, we run the job as follows:</p>
<pre>export BUCKET_NAME=my-mnist-bucket<br/>export JOB_NAME="mnist_mlp_hpt_train_$(date +%Y%m%d_%H%M%S)"<br/>export JOB_DIR=gs://$BUCKET_NAME/$JOB_NAME<br/>export REGION=us-east1<br/>export HPTUNING_CONFIG=hptune.yaml<br/>gcloud ml-engine jobs submit training $JOB_NAME \<br/> --job-dir $JOB_DIR \<br/> --runtime-version 1.6 \<br/> --config $HPTUNING_CONFIG \<br/> --module-name trainer.mnist_mlp_lr_numsteps \<br/> --package-path ./trainer \<br/> --region $REGION \<br/> -- \<br/> --train-file gs://$BUCKET_NAME/data/mnist.pkl \<br/> --num-steps 100 \<br/> --lr 0.01</pre>
<p>Note that we specify the bucket name in which data exists, and the job name and directory in which the logs need to be stored. The region needs to be set and the configuration file is specified.</p>
<p>Also, set the <kbd>--module-name</kbd> argument to the name of your application's main module using your package's namespace dot notation.</p>
<p>Note that, after specifying region, we have a blank, indicating that it is the start of arguments now (they are the training file location, number of steps, and learning rate).</p>
<p>The number of steps and learning rate that we have specified in the preceding code are the default versions, which are changed once passed to the ML engine job.</p>
<p>The output of the code can be visualized in the training output of the job that we ran, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8a1fc4e-70ba-476f-9e52-f1eba7f6b915.png" style=""/></div>
<p>The optimal hyperparameters can then be selected from the preceding output. We can see that a learning rate of <strong>0.0149</strong> and number of steps as <strong>7658</strong> result in a higher test dataset accuracy than the two scenarios that we tested earlier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we understood how different parameter combinations affect the final accuracy measure and how hyperparameter tuning using Cloud ML engine helps in improving the accuracy further.</p>
<p>In the next chapter, we will learn how to identify overfitting and make our models more robust to previously-unseen data by setting the right parameters and defining the proper architectures.</p>


            </article>

            
        </section>
    </body></html>