- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Ensemble Learning – Bagging and Boosting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习 – Bagging 和 Boosting
- en: In the previous chapter, we covered the fundamentals of **machine learning**
    (**ML**), working with data and models, and concepts such as overfitting and **supervised
    learning** (**SL**). We also introduced decision trees and saw how to apply them
    practically in scikit-learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了机器学习（**ML**）的基础知识，包括数据处理和模型，以及过拟合和**监督学习（SL**）等概念。我们还介绍了决策树，并展示了如何在scikit-learn中实际应用它们。
- en: 'In this chapter, we will learn about ensemble learning and the two most significant
    types of ensemble learning: bagging and boosting. We will cover the theory and
    practice of applying ensemble learning to decision trees and conclude the chapter
    by focusing on more advanced boosting methods.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章，我们将学习集成学习以及两种最重要的集成学习类型：Bagging和Boosting。我们将涵盖将集成学习应用于决策树的理论和实践，并通过关注更高级的Boosting方法来结束本章。
- en: By the end of this chapter, you will have a good understanding of ensemble learning
    and how to practically build decision tree ensembles through bagging or boosting.
    We will also be ready to dive deep into LightGBM, including its more advanced
    theoretical aspects.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将很好地理解集成学习以及如何通过Bagging或Boosting实际构建决策树集成。我们还将准备好深入研究LightGBM，包括其更高级的理论方面。
- en: 'The main topics we will cover are set out here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的主要主题如下：
- en: Ensemble learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习
- en: Bagging and random forests
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging 和随机森林
- en: '**Gradient-boosted decision** **trees** (**GBDTs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升决策树（GBDT**）'
- en: Advanced boosting algorithm—**Dropouts meet Multiple Additive Regression** **Trees**
    (**DART**)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级提升算法—**Dropouts meet Multiple Additive Regression Trees（DART**）
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The chapter includes examples of simple ML algorithms and introduces working
    with scikit-learn. You must install a Python environment with scikit-learn, NumPy,
    pandas, and Jupyter. The code for this chapter is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括简单机器学习算法的示例，并介绍了如何使用scikit-learn。你必须安装一个带有scikit-learn、NumPy、pandas和Jupyter的Python环境。本章的代码可在[https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2)找到。
- en: Ensemble learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: '**Ensemble learning** is the practice of combining multiple predictors, or
    models, to create a more robust model. Models can either be of the same type (homogenous
    ensembles) or different types (heterogenous ensembles). Further, ensemble learning
    is not specific to decision trees and can be applied to any ML technique, including
    linear models, **neural networks** (**NNs**), and more.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成学习**是将多个预测器或模型组合起来创建一个更稳健模型的实践。模型可以是同一类型的（同质集成）或不同类型的（异质集成）。此外，集成学习不仅限于决策树，还可以应用于任何机器学习技术，包括线性模型、**神经网络（NNs**）等。'
- en: The central idea behind ensemble learning is that by aggregating the results
    of many models, we compensate for the weaknesses of a single model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习背后的核心思想是通过聚合多个模型的预测结果，来弥补单个模型的弱点。
- en: Of course, training the same models on the same data is not helpful in an ensemble
    (as the models will have similar predictions). Therefore, we aim for **diversity**
    in the models. Diversity refers to the degree to which each model in the ensemble
    differs. A high-diversity ensemble has widely different models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在相同的数据上训练相同的模型在集成中是没有帮助的（因为模型会有相似的预测）。因此，我们追求模型之间的**多样性**。多样性指的是集成中每个模型差异的程度。高多样性集成具有广泛不同的模型。
- en: There are several ways we can ensure diversity in our ensemble. One method is
    to train models on different subsets of the training data. Each model is exposed
    to different patterns and noise in the training data, increasing the diversity
    of the trained models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几种方法可以确保集成中的多样性。一种方法是在训练数据的不同子集上训练模型。每个模型都会接触到训练数据中的不同模式和噪声，从而增加训练模型的多样性。
- en: Similarly, we can train each model on a different subset of features in the
    training data. Some features are more valuable than others, and some might be
    irrelevant, leading to diversity in the model predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以在训练数据的不同特征子集上训练每个模型。一些特征比其他特征更有价值，一些可能是不相关的，导致模型预测的多样性。
- en: We can also train each model with different hyperparameters, leading to different
    models of varying complexity and ability. The impact of hyperparameters is especially
    pronounced in the case of decision trees, where the hyperparameters significantly
    impact the structure of the trees, leading to very different models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以用不同的超参数训练每个模型，导致不同复杂性和能力的模型。超参数的影响在决策树的情况下尤为明显，因为超参数显著影响树的结构，导致非常不同的模型。
- en: Finally, we could diversify the ensemble by using different types of models.
    Each model has unique strengths and weaknesses, leading to diversity in the ensemble.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过使用不同类型的模型来多样化集成。每个模型都有其独特的优势和劣势，从而导致集成中的多样性。
- en: 'The **ensemble learning method** refers to how we introduce diversity in ensemble
    models by specifying how we train the member models and how we combine the results
    of the models. The most common ensemble methods are set out here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成学习方法**指的是我们如何通过指定如何训练成员模型以及如何组合模型结果来引入集成模型中的多样性。最常用的集成方法如下：'
- en: '**Bootstrap aggregation (bagging)**: These are methods where models are trained
    on subsets of the training data (either samples or features), and the predictions
    are aggregated.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自助聚合（Bagging）**：这些方法是在训练数据的子集（样本或特征）上训练模型，并将预测结果进行聚合。'
- en: '**Boosting**: This involves iteratively training models on the errors of previous
    models. The final prediction is made by combining the prediction of all models
    in the chain.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**：这涉及到迭代地在先前模型的错误上训练模型。最终预测是通过结合链中所有模型的预测来完成的。'
- en: '**Stacking**: This involves methods where multiple base models are trained,
    then a higher-order model (known as a meta-model) is trained to learn from the
    base model predictions and make the final prediction.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠**：这涉及到训练多个基模型，然后训练一个更高阶的模型（称为元模型），以从基模型的预测中学习并做出最终预测。'
- en: '**Blending**: This is very similar to stacking. However, the meta-model is
    trained on predictions made by the base models on a *hold-out set* (a part of
    the training data the base learners were not trained on) instead of the whole
    training set.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合**：这与堆叠非常相似。然而，元模型是在基模型对*保留集*（基学习器未训练过的训练数据的一部分）上的预测上训练的，而不是在整个训练集上。'
- en: 'The purpose of ensemble learning methods is to improve our prediction performance,
    and there are several ways ensembles improve the performance over individual models,
    as outlined here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习方法的目的在于提高我们的预测性能，并且有几种方法可以改善集成相对于单个模型的性能，如下所述：
- en: '**Improved accuracy**: By combining predictions, we increase the likelihood
    that the final prediction is accurate as, in aggregate, models make fewer mistakes.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高准确性**：通过结合预测，我们增加了最终预测准确的可能性，因为从总体上看，模型犯的错误更少。'
- en: '**Improved generalization and overfitting**: By aggregating predictions, we
    reduce the variance in the final prediction, improving generalization. Further,
    in some ensemble methods, the models cannot access all data (bagging ensembles),
    reducing noise and overfitting.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高泛化能力和避免过拟合**：通过聚合预测，我们减少了最终预测中的方差，提高了泛化能力。此外，在某些集成方法中，模型无法访问所有数据（Bagging集成），这减少了噪声和过拟合。'
- en: '**Improved prediction stability**: The aggregation of predictions reduces the
    random fluctuations of individual predictions. Ensembles are less sensitive to
    outliers, and outlying predictions of member models have a limited impact on the
    final prediction.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高预测稳定性**：预测的聚合减少了单个预测的随机波动。集成对异常值不太敏感，成员模型的异常预测对最终预测的影响有限。'
- en: Decision trees are well suited for ensemble learning, and decision tree-specific
    algorithms exist for ensemble learning. The following section discusses bagging
    ensembles in decision trees focusing on **random forests**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常适合集成学习，并且存在专门用于集成学习的决策树特定算法。下一节将讨论决策树中的袋装集成，重点关注**随机森林**。
- en: Bagging and random forests
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging和随机森林
- en: '**Bagging** is an ensemble method where multiple models are trained on subsets
    of the training data. The models’ predictions are combined to make a final prediction,
    usually by taking the average for numerical prediction (for regression) or the
    majority vote for a class (for classification). When training each model, we select
    a subset of data from the original training dataset with replacement—that is,
    a specific training pattern can be a member of multiple subsets. Since each model
    is only presented with a sample of the training data, no single model can “memorize”
    the training data, which reduces overfitting. The following diagram illustrates
    the bagging process:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**袋装**是一种集成方法，其中多个模型在训练数据的子集上训练。模型的预测被组合起来做出最终预测，通常是通过取数值预测的平均值（对于回归）或对类别的多数投票（对于分类）。在训练每个模型时，我们从原始训练数据集中选择一个数据子集，并带有替换——也就是说，特定的训练模式可以是多个子集的成员。由于每个模型只接触到训练数据的一个样本，因此没有单个模型可以“记住”训练数据，这减少了过拟合。以下图表说明了袋装过程：'
- en: '![Figure 2.1 – Illustration of the bagging process; each independent classifier
    is trained on a random subsample from the training data and a final prediction
    is made by aggregating the predictions of all classifiers](img/B16690_02_1.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 描述袋装过程的示意图；每个独立的分类器在训练数据的随机子样本上训练，并通过汇总所有分类器的预测来做出最终预测](img/B16690_02_1.jpg)'
- en: Figure 2.1 – Illustration of the bagging process; each independent classifier
    is trained on a random subsample from the training data and a final prediction
    is made by aggregating the predictions of all classifiers
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 描述袋装过程的示意图；每个独立的分类器在训练数据的随机子样本上训练，并通过汇总所有分类器的预测来做出最终预测
- en: Each model in a bagging ensemble is still a complete model, capable of standing
    on its own. As such, bagging works best with strong models—that is, in the case
    of decision trees, deep or wide decision trees.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装集成中的每个模型仍然是一个完整的模型，能够独立存在。因此，袋装与强大的模型结合得最好——也就是说，在决策树的情况下，深或宽的决策树。
- en: While the previous example illustrates sampling patterns from the training set,
    it is also possible to subsample random features from the dataset for each model.
    Selecting features at random when creating a training set is known as a random
    subspace method or feature bagging. Feature bagging prevents occurrences where
    a specific attribute might dominate the prediction or mislead the model and further
    reduces overfitting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前的例子说明了从训练集中抽取样本模式，但也可以为每个模型从数据集中抽取随机特征。在创建训练集时随机选择特征被称为随机子空间方法或特征袋装。特征袋装防止了特定属性可能主导预测或误导模型的情况，并进一步减少了过拟合。
- en: In decision trees, a popular algorithm that applies both sample bagging and
    feature bagging is the random forest. Let’s have a look at this algorithm now.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，一个同时应用样本袋装和特征袋装的流行算法是随机森林。现在让我们来看看这个算法。
- en: Random forest
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random forest** is a decision tree-specific bagging ensemble learning method
    [1]. Instead of building a single decision tree, as the name implies, many decision
    trees are trained using bagging: each tree is trained on either random samples,
    random features from the training data, or both. Random forests support both classification
    and regression.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是一种针对决策树的特定袋装集成学习方法[1]。正如其名称所暗示的，它不是构建单个决策树，而是使用袋装训练多个决策树：每棵树要么在随机样本上训练，要么在训练数据中的随机特征上训练，或者两者都训练。随机森林支持分类和回归。'
- en: The training methodology for individual trees in random forests is the same
    as for a single decision tree, and as explained previously, each tree is a complete
    tree. The final prediction for the forest is made by taking the arithmetic mean
    of all trees in the case of prediction or the majority vote for a class (in the
    case of classification).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林中单个树的训练方法与单个决策树相同，正如之前所解释的，每棵树都是一棵完整的树。对于预测，森林的最终预测是通过取所有树的算术平均值来实现的；对于分类，是通过多数投票来实现的。
- en: Regarding performance, random forest learning produces a more robust model with
    higher accuracy, which tends to avoid overfitting (since no single model can overfit
    on all training data).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关于性能，随机森林学习产生了一个更稳健的模型，具有更高的准确性，并且倾向于避免过拟合（因为单个模型无法对所有训练数据进行过拟合）。
- en: Random forest hyperparameters
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林超参数
- en: 'In scikit-learn, as may be expected, the hyperparameters available for random
    forests are the same as those available to train decision trees. We can specify
    `max_depth`, `min_samples_split`, `max_leaf_nodes`, and so on, which are then
    used to train the individual trees. However, there are three notable additional
    parameters, as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，正如预期的那样，随机森林的可用超参数与训练决策树的可用超参数相同。我们可以指定`max_depth`、`min_samples_split`、`max_leaf_nodes`等，然后用于训练单个树。然而，有三个值得注意的附加参数，如下所示：
- en: '`n_estimators`: Controls the number of trees in the forest. Generally, more
    trees are better. However, a point of diminishing returns is often reached.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：控制森林中的树的数量。通常，树越多越好。然而，通常会达到收益递减的点。'
- en: '`max_features`: Determines the maximum number of features to be used as a subset
    when splitting a node. Setting `max_features=1.0` allows all features to be used
    in the random selection.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：确定在分割节点时用作子集的最大特征数。将`max_features=1.0`设置为允许在随机选择中使用所有特征。'
- en: '`bootstrap` determines whether bagging is used. All trees use the entire training
    set if `bootstrap` is set to `False`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`决定是否使用袋装法。如果`bootstrap`设置为`False`，则所有树将使用整个训练集。'
- en: 'A list of all parameters available in scikit-learn is available here: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中所有可用参数的列表在此处提供：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml)。
- en: ExtraTrees
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExtraTrees
- en: '**Extremely Randomized Trees** (**ExtraTrees**) is a related method for building
    randomized decision trees. With ExtraTrees, when building a decision node, several
    candidate splits are created randomly instead of using the *Gini index* or *information
    gain* metric to calculate the optimal split [2]. The best split from all random
    splits is then chosen for the node. The methodology for ExtraTrees can be applied
    to a single decision tree or used in conjunction with Random Forests. scikit-learn
    implements ExtraTrees as an extension to random forests (https://scikit-learn.org/stable/modules/ensemble.xhtml#extremely-randomized-trees).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**极随机树**（**ExtraTrees**）是构建随机决策树的相关方法。使用ExtraTrees时，在构建决策节点时，会随机创建多个候选分割，而不是使用*Gini指数*或*信息增益*指标来计算最佳分割[2]。然后从所有随机分割中选择最佳分割用于节点。ExtraTrees的方法可以应用于单个决策树，也可以与随机森林结合使用。scikit-learn将ExtraTrees作为随机森林的扩展实现（https://scikit-learn.org/stable/modules/ensemble.xhtml#extremely-randomized-trees）。'
- en: In scikit-learn, the ExtraTrees implementation has the same hyperparameters
    as random forests.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，ExtraTrees的实现与随机森林具有相同的超参数。
- en: Training random forests using scikit-learn
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练随机森林
- en: We’ll now have a look at using random forests with scikit-learn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看如何使用scikit-learn中的随机森林。
- en: In this example, we’ll use the *Forest CoverType* dataset ([https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype)),
    which is available wit[hin scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml#sklearn.data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml%23sklearn.datasets.fetch_covtype)sets.fetch_covtype).
    The dataset is a significant step up from the toy datasets we were using previously.
    The dataset consists of 581,012 samples and has a dimensionality (number of features)
    of 54\. The features describe a 30x30m patch of forest in the US (for example,
    elevation, aspect, slope, and distances to hydrology). We must build a classifier
    to classify each patch into one of seven classes describing the forest cover type.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用*森林覆盖类型*数据集（[https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype)），该数据集在scikit-learn（https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml#sklearn.data）中可用。与之前使用的玩具数据集相比，该数据集是一个显著的提升。数据集包含581,012个样本，具有54个维度（特征数）。特征描述了美国30x30m的森林区域（例如，海拔、坡向、坡度和到水文点的距离）。我们必须构建一个分类器，将每个区域分类为描述森林覆盖类型的七个类别之一。
- en: In addition to training a `RandomForestClassifier`, we’ll train a standalone
    `DecisionTreeClassifier` and an `ExtraTreesClassifier` and compare the performance
    of the algorithms.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练`RandomForestClassifier`之外，我们还将训练一个独立的`DecisionTreeClassifier`和一个`ExtraTreesClassifier`，并比较算法的性能。
- en: '`RandomForestClassifier` and `ExtraTreesClassifier` live in the `sklearn.ensemble`
    package. In addition to our regular imports, we import the classifiers from there,
    like so:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`和`ExtraTreesClassifier`位于`sklearn.ensemble`包中。除了我们的常规导入外，我们还从那里导入分类器，如下所示：'
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The scikit-learn datasets package provides the Forest Cover dataset. We can
    use scikit-learn to fetch the dataset and split it into our training and test
    sets, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn数据集包提供了森林覆盖数据集。我们可以使用scikit-learn获取数据集，并将其拆分为我们的训练集和测试集，如下所示：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we can train our classifiers and evaluate each of them against the
    test set:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以训练我们的分类器，并将它们各自与测试集进行评估：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have set hyperparameters for the models that are appropriate to the problem.
    An additional step would be to optimize the algorithm hyperparameters to discover
    the best parameter values. Parameter optimization is discussed in detail in a
    later chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为模型设置了适合问题的超参数。额外的一步是优化算法超参数，以发现最佳参数值。参数优化将在后面的章节中详细讨论。
- en: 'Running the preceding code, we get the following F1 scores for each of the
    algorithms:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，我们得到以下每个算法的F1分数：
- en: '| **Model** | **F1 score** |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **F1分数** |'
- en: '| Decision Tree | 0.8917 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.8917 |'
- en: '| Random Forest | 0.9209 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.9209 |'
- en: '| ExtraTrees | 0.9231 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| ExtraTrees | 0.9231 |'
- en: Table 2.1 – F1 scores for each of the algorithms on the Forest CoverType dataset
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 – Forest CoverType数据集上每个算法的F1分数
- en: The ExtraTrees model slightly outperforms the random forest model, and both
    perform better than the decision tree classifier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ExtraTrees模型略优于随机森林模型，两者都比决策树分类器表现更好。
- en: 'In this section, we gave an overview of bagging and random forests, a bagging-based
    decision tree ensemble learning method that provides some benefits over standard
    decision trees. The following section examines an alternative ensemble learning
    method: gradient boosting.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了bagging和随机森林，这是一种基于bagging的决策树集成学习方法，它相对于标准决策树提供了一些优势。下一节将探讨另一种集成学习方法：梯度提升。
- en: Gradient-boosted decision trees
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阶梯提升决策树
- en: '**Gradient boosting** is an ensemble learning methodology that combines multiple
    models *sequentially* to produce a more robust ensemble model. Unlike bagging,
    where multiple *strong* models are used (in parallel), with boosting, multiple
    weak learners are trained, each learning from the mistakes of those before it
    to build a more accurate and robust ensemble model. Another distinct difference
    from bagging is that each model uses the entire dataset for training.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升**是一种集成学习方法，它通过**顺序**组合多个模型来产生一个更稳健的集成模型。与bagging不同，在bagging中使用了多个**强大**的模型（并行使用），而在boosting中，训练了多个弱学习器，每个学习器都从前一个学习器的错误中学习，以构建一个更准确和更稳健的集成模型。与bagging的另一个显著区别是，每个模型都使用整个数据集进行训练。'
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As discussed next, gradient boosting always builds a series of regression trees
    to form part of the ensemble, regardless of whether a regression or classification
    problem is solved. Gradient boosting is also called **Multiple Additive Regression**
    **Trees** (**MART**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如下文所述，梯度提升始终构建一系列回归树作为集成的一部分，无论解决的是回归问题还是分类问题。梯度提升也称为**多重加性回归树**（**MART**）。
- en: 'Abstractly, the boosting process starts with a weak base learner. In the case
    of decision trees, the base learner might have only a single split (also known
    as a decision stump). The error residuals (the difference between the predicted
    and actual targets) are then calculated. A new learner is then trained on the
    error residuals of the previous learner, looking to minimize the errors. The final
    prediction is a *summation* of the predictions from all the learners. The following
    diagram illustrates the iterative gradient-boosting process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象地说，boosting过程从弱基学习器开始。在决策树的情况下，基学习器可能只有一个分割（也称为决策树桩）。然后计算误差残差（预测值与实际目标之间的差异）。然后，在先前学习器的误差残差上训练新的学习器，以最小化错误。最终的预测是所有学习器预测的**总和**。以下图示说明了迭代梯度提升过程：
- en: '![Figure 2.2 – Illustration of the gradient-boosting process; in each iteration,
    a new regression tree is added to compensate for the error residuals of the previous
    iteration](img/B16690_02_2.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 阶梯提升过程的示意图；在每次迭代中，都会添加一个新的回归树来补偿前一次迭代的误差残差](img/B16690_02_2.jpg)'
- en: Figure 2.2 – Illustration of the gradient-boosting process; in each iteration,
    a new regression tree is added to compensate for the error residuals of the previous
    iteration
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 梯度提升过程的示意图；在每次迭代中，都会添加一个新的回归树来补偿前一次迭代的误差残差
- en: 'One of the critical questions relates to how we can determine the changes that
    reduce the error residuals. Gradient boosting solves the error minimization problem
    by applying a widely used optimization problem: gradient descent.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键问题之一是，我们如何确定减少误差残差的变化。梯度提升通过应用一个广泛使用的优化问题来解决误差最小化问题：梯度下降。
- en: Gradient descent
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: '**Gradient descent** is an optimization algorithm that attempts to find the
    optimal parameters to minimize a **loss function**. The parameters are updated
    iteratively by taking small steps in the direction of the negative gradient of
    the loss function (thereby decreasing the function value). A loss function is
    similar in concept to an error function but has two important properties, as outlined
    here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**是一种优化算法，试图找到最小化损失函数的最优参数。参数通过在损失函数负梯度的方向上采取小步迭代更新（从而减少函数值）。损失函数在概念上类似于误差函数，但有两个重要的属性，如下所述：'
- en: A loss function produces a numeric value that quantifies the model’s performance
    or precisely how poorly a model is doing. A good loss function produces significantly
    different output for models with different performances. Some error functions
    can also be used as loss functions—for example, **mean squared** **error** (**MSE**).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数产生一个数值，量化了模型的性能或模型表现不佳的精确程度。一个好的损失函数对于不同性能的模型会产生显著不同的输出。一些误差函数也可以用作损失函数——例如，**均方**
    **误差**（**MSE**）。
- en: The second property is that a loss function must be differentiable, specifically
    in the context of gradient descent. An example of an error function that’s not
    differentiable is the F1 score. The F1 score may produce a numeric value of the
    model’s performance, but it is not differentiable and cannot be used as a loss
    function.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个属性是损失函数必须是可微分的，特别是在梯度下降的上下文中。一个不可微分的误差函数是F1分数。F1分数可能产生一个表示模型性能的数值，但它不可微分，不能用作损失函数。
- en: 'The process for gradient descent can, then, be defined as follows. Suppose
    we have a loss function L defined for parameters x. For an initial set of parameters,
    the loss is calculated as L( x 0). Gradient descent proceeds iteratively to minimize
    the loss function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的过程可以定义为以下。假设我们有一个针对参数x定义的损失函数L。对于一组初始参数，损失计算为L(x_0)。梯度下降迭代进行以最小化损失函数：
- en: L(x n+1) < L( x n)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: L(x_{n+1}) < L(x_n)
- en: 'To update the parameters, *we take a step in the direction of the negative
    gradient of* L. We can specify the gradient descent update rule as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新参数，*我们朝着L的负梯度方向迈出一步*。我们可以将梯度下降更新规则指定如下：
- en: x n+1 = x n − γ n ∇ L( x n)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: x_{n+1} = x_n − γ_n ∇L(x_n)
- en: Here, γ n is the **learning rate**, which defines the step size, and ∇ L(x n)
    is the gradient of L at x n.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，γn是**学习率**，它定义了步长，而∇L(xn)是L在xn处的梯度。
- en: 'The graph in *Figure 2**.3* illustrates the gradient descent process:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.3*中的图表说明了梯度下降过程：'
- en: '![Figure 2.3 – Graph showing the gradient descent process to find the minimum
    of a function](img/B16690_02_3.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 显示寻找函数最小值的梯度下降过程的图表](img/B16690_02_3.jpg)'
- en: Figure 2.3 – Graph showing the gradient descent process to find the minimum
    of a function
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 显示寻找函数最小值的梯度下降过程的图表
- en: 'Choosing an appropriate learning rate is critical to gradient descent’s success.
    If the learning rate is too low, the optimization will be very slow, potentially
    not reaching a minimum in the allowed number of iterations. A small learning rate
    could also lead to the process getting stuck in a local minimum: the step size
    being too small to escape. Conversely, suppose the learning rate is too large.
    In that case, we could step over the minimum and miss it entirely or get stuck
    oscillating around a minimum (constantly jumping back and forth but never descending
    to the optimal value).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的学习率对梯度下降的成功至关重要。如果学习率太低，优化过程会非常缓慢，可能无法在允许的迭代次数内达到最小值。学习率太小也可能导致过程陷入局部最小值：步长太小以至于无法逃脱。相反，假设学习率太大。在这种情况下，我们可能会跳过最小值而完全错过它，或者陷入在最小值周围振荡（不断跳来跳去但从未下降到最优值）。
- en: Gradient boosting
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Now that we understand how gradient descent works, we can see how it is applied
    in gradient boosting. We will work through the entire gradient-boosting algorithm
    in detail at the hand of a small example. In our example, we’ll use a regression
    tree, as it is a bit easier to understand than the case for classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了梯度下降的工作原理，我们可以看到它在梯度提升中的应用。我们将通过一个小例子详细地讲解整个梯度提升算法。在我们的例子中，我们将使用回归树，因为它比分类情况更容易理解。
- en: Gradient-boosting algorithm
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升算法
- en: 'The gradient-boosting algorithm is defined as follows, where M is the number
    of boosted trees [3]:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法定义如下，其中 M 是提升树的数量 [3]：
- en: Given training data {(x i, y i)} i=1 n , containing n training samples (defined
    by features x i and target y i) and a differentiable loss function L(y i, F(x)),
    where F(x) are the predictions from model F.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定训练数据 {(x i, y i)} i=1 n ，包含 n 个训练样本（由特征 x i 和目标 y i 定义）和一个可微的损失函数 L(y i, F(x))，其中
    F(x) 是模型 F 的预测。
- en: Initialize the model with a constant prediction value F 0(X) = argmin γ ∑ i=1 n  L(
    y i, γ)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用常数预测值初始化模型 F 0(X) = argmin γ ∑ i=1 n  L( y i, γ)
- en: 'For m = 1 to M:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 m = 1 到 M：
- en: Compute pseudo residuals r im = − [∂ L(y i, F(x i)) _ ∂ F(x i) ] F(X)=F m−1(X)
    for i = 1, … , n
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算伪残差 r im = − [∂ L(y i, F(x i)) _ ∂ F(x i) ] F(X)=F m−1(X) 对于 i = 1, … , n
- en: Fit a regression tree to the r im values and create terminal regions R jm for
    j = 1…J m
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将回归树拟合到 r im 值，并为 j = 1…J m 创建终端区域 R jm
- en: For j = 1…J m compute γ jm = argmin γ ∑ x i∈R ij L( y i, F m−1(x i) + γ)
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 j = 1…J m 计算 γ jm = argmin γ ∑ x i∈R ij L( y i, F m−1(x i) + γ)
- en: Update F m(x) = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm)
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新 F m(x) = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm)
- en: 'Result: F M(x)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果：F M(x)
- en: 'Although the algorithm and especially the mathematics might look intimidating,
    it is practically much more straightforward than it appears. We’ll go through
    the algorithm step by step. Consider the following toy dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然算法和特别是数学可能看起来令人畏惧，但实际上它比看起来要简单得多。我们将一步一步地讲解算法。考虑以下玩具数据集：
- en: '| **Gender** | **Fasting** **Blood Sugar** | **Waist Circumference** | **BMI**
    | **LDL Cholesterol** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **性别** | **空腹** **血糖** | **腰围** | **BMI** | **LDL 胆固醇** |'
- en: '| Male | 105 | 110 | 29.3 | 170 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 105 | 110 | 29.3 | 170 |'
- en: '| Female | 85 | 80 | 21 | 90 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 85 | 80 | 21 | 90 |'
- en: '| Male | 95 | 93 | 26 | 113 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 95 | 93 | 26 | 113 |'
- en: Table 2.2 – Example dataset consisting of a patient’s physical measurements
    and measured low-density lipoprotein (LDL) cholesterol
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – 由患者的物理测量和测量的低密度脂蛋白（LDL）胆固醇组成的示例数据集
- en: Given the physical measurements, we aim to predict a patient’s LDL cholesterol.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 给定物理测量值，我们旨在预测患者的低密度脂蛋白胆固醇。
- en: 'The preceding table defines our training data {(x i, y i)} i=1 n , where x
    is the features (blood sugar, waist circumference, BMI) and y is the target: LDL
    cholesterol.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表格定义了我们的训练数据 {(x i, y i)} i=1 n ，其中 x 是特征（血糖、腰围、BMI）和 y 是目标：低密度脂蛋白胆固醇。
- en: 'We need a differentiable loss function, and to simplify some of the mathematical
    derivations in this example, we choose the following loss function, which is similar
    to the MSE function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个可微的损失函数，为了简化本例中的一些数学推导，我们选择以下损失函数，它与均方误差函数类似：
- en: L =  1 _ 2  ∑ i=0 n (y i − γ i) 2
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: L =  1 _ 2  ∑ i=0 n (y i − γ i) 2
- en: We now work through each algorithm step in detail to see how the gradient-boosted
    tree is produced.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将详细地讲解每个算法步骤，以了解梯度提升树是如何产生的。
- en: The first step is to find F 0(x) = argmin γ ∑ i=1 n  L( y i, γ), where y i is
    our target value and γ is our initial predicted value. *Our initial prediction
    is constant and is simply the average of the target values*. But let’s see why.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是找到 F 0(x) = argmin γ ∑ i=1 n  L( y i, γ)，其中 y i 是我们的目标值，γ 是我们的初始预测值。*我们的初始预测是恒定的，简单来说是目标值的平均值*。但让我们看看为什么。
- en: 'The equation for F 0(x) is stating that we need to find a value for γ that
    minimizes our loss function. To find the minimum, we take the derivative of the
    loss function with respect to gamma:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: F 0(x) 的方程表明我们需要找到一个 γ 的值，以最小化我们的损失函数。为了找到最小值，我们取损失函数关于 γ 的导数：
- en: ∂ L _ ∂ γ  =  2 _ 2 (∑ i=0 n (y i − γ)) × − 1
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ L _ ∂ γ  =  2 _ 2 (∑ i=0 n (y i − γ)) × − 1
- en: 'Then, set it to 0 and solve the following equation:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将其设为 0 并解以下方程：
- en: − (∑ i=0 n (y i − γ)) = 0
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: − (∑ i=0 n (y i − γ)) = 0
- en: −  1 _ n  ∑ i=0 n y i+ γ = 0
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: −  1 _ n  ∑ i=0 n y i+ γ = 0
- en: γ =  1 _ n  ∑ i=0 n y i
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: γ =  1 _ n  ∑ i=0 n y i
- en: The equation simplifies to calculating the average of the target values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程简化为计算目标值的平均值。
- en: 'Updating our table with the predictions, we have the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测更新到表中，我们得到以下内容：
- en: '| **Gender** | **F.** **Blood Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **PredictionF 0(x)** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **性别** | **F.** **血糖** | **W. Circum.** | **BMI** | **LDL 胆固醇** | **预测 F 0(x)**
    |'
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 男 | 105 | 110 | 29.3 | 170 | 125 |'
- en: '| Female | 85 | 80 | 21 | 90 | 125 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 女 | 85 | 80 | 21 | 90 | 125 |'
- en: '| Male | 95 | 93 | 26 | 113 | 125 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 男 | 95 | 93 | 26 | 113 | 125 |'
- en: Table 2.3 – Our initial prediction of LDL cholesterol predictions ( F 0(x))
    for each patient is constant
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.3 – 我们对每个患者的 LDL 胆固醇预测（ F 0(x)）的初始预测是恒定的
- en: We repeat the following M times, where M is the number of trees we choose to
    build.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复以下 M 次，其中 M 是我们选择构建的树的数量。
- en: 'We now need to calculate the pseudo residuals r im = − [∂ L(y i, F(x i)) _ ∂
    F(x i) ] F(X)=F m−1(X). This equation for r im is stating that we use the negative
    of the partial derivative of the loss function, with respect to the predictions,
    to calculate the pseudo residuals. *This portion of the gradient-boosting algorithm
    relates to gradient descent: we are taking the negative of the gradient to minimize
    the residuals*. Fortunately, we have already calculated this derivative:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算伪残差 r im = − [∂ L(y i, F(x i)) _ ∂ F(x i) ] F(X)=F m−1(X)。这个 r im 方程表明我们使用预测的损失函数的负偏导数来计算伪残差。*这部分梯度提升算法与梯度下降相关：我们取负梯度以最小化残差*。幸运的是，我们已经计算了此导数：
- en: − [ ∂ L(y i, F(x i)) _ ∂ F(x i) ] = −  ∂ _ ∂ F(x i) (  1 _ 2  (y i − F(x i)) 2)
    =  2 _ 2 (y i− F(x i)) = ( y i − F(x i))
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: − [ ∂ L(y i, F(x i)) _ ∂ F(x i) ] = −  ∂ _ ∂ F(x i) (  1 _ 2  (y i − F(x i)) 2)
    =  2 _ 2 (y i− F(x i)) = ( y i − F(x i))
- en: 'Here, F(x i) is the predicted value. In other words, *the equation simplifies
    the difference between the target and predicted values*. We can add the residuals
    to the table, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，F(x i)是预测值。换句话说，*该方程简化了目标值和预测值之间的差异*。我们可以将残差添加到表中，如下所示：
- en: '| **Gender** | **F. Blood****Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **Prediction** **F 0(x)** | **Residuals****for** **F 0(x)** |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **性别** | **F. 血糖** | **W. Circum.** | **BMI** | **LDL 胆固醇** | **预测** **F 0(x)**
    | **F 0(x) 的残差** |'
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 | 45 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 男 | 105 | 110 | 29.3 | 170 | 125 | 45 |'
- en: '| Female | 85 | 80 | 21 | 90 | 125 | -35 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 女 | 85 | 80 | 21 | 90 | 125 | -35 |'
- en: '| Male | 95 | 93 | 26 | 113 | 125 | -12 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 男 | 95 | 93 | 26 | 113 | 125 | -12 |'
- en: Table 2.4 – Based on our initial prediction, we can calculate the residuals
    for each patient, as shown in the Residuals for F 0(x) column
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.4 – 根据我们的初始预测，我们可以计算每个患者的残差，如 Residuals for F 0(x) 列所示
- en: 'The next step is straightforward: we build a regression tree to predict the
    residuals. We do not use the predictions of the regression tree directly. Instead,
    we use the *terminal regions* to calculate our updated prediction. The terminal
    regions refer to the tree’s leaf nodes.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步很简单：我们构建一个回归树来预测残差。我们不直接使用回归树的预测。相反，我们使用*终端区域*来计算我们的更新预测。终端区域指的是树的叶节点。
- en: 'For this example, we assume the following simple regression tree is built:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们假设以下简单的回归树已被构建：
- en: '![Figure 2.4 – Regression tree predicting the residuals](img/B16690_02_4.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 预测残差的回归树](img/B16690_02_4.jpg)'
- en: Figure 2.4 – Regression tree predicting the residuals
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 预测残差的回归树
- en: With our regression tree built and our leaf nodes defined, we can proceed with
    the next step. We need to compute γ jm that minimizes our loss function, taking
    into account the previous predictions as per γ jm = argmin γ ∑ x i∈R ij L( y i,
    F m−1(x i) + γ). This is almost precisely what we did in *step 1*, where we showed
    that the equation simplifies taking the average of the predicted values due to
    our choice of the loss function. *Here, that means taking the average of the residuals
    in each leaf node*. Therefore, we have γ 1,1 = − 35 − 12 _ 2  = − 23.5 and γ 2,1
    = 45 _ 1  = 45.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的回归树构建并定义了叶节点后，我们可以进行下一步。我们需要计算 γ jm，它最小化我们的损失函数，并考虑之前的预测，γ jm = argmin γ ∑ x i∈R ij L(
    y i, F m−1(x i) + γ)。这正是我们在*步骤 1*中所做的，我们展示了由于我们选择的损失函数，方程简化为预测值的平均值。*在这里，这意味着取每个叶节点中残差的平均值*。因此，我们有
    γ 1,1 = − 35 − 12 _ 2  = − 23.5 和 γ 2,1 = 45 _ 1  = 45。
- en: 'Finally, we can now calculate our next prediction, F 1(x), defined by: F m(x)
    = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm),     meaning our next prediction consists of the previous prediction plus the γ values
    calculated in *step 2.3*, weighed by a learning rate ν. The summation here means
    that should a sample be part of multiple leaf nodes, we take the sum of the gamma
    values. Let’s calculate F 1(x) for the first sample in our dataset, using a learning
    rate of 0.1\. According to the regression tree from *step 2.2*, our sample (which
    has a BMI > 26) maps to γ 2,1\. Since it’s only mapping to a single leaf, we don’t
    need the summation part of the equation. Therefore, the equation looks like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们现在可以计算我们的下一个预测，F1(x)，其定义为：Fm(x) = Fm-1(x) + ν∑j=1Jmγjm I(x ∈ Rjm)，这意味着我们的下一个预测由先前预测加上在*步骤2.3*中计算的γ值，并乘以学习率ν。这里的求和意味着如果一个样本属于多个叶节点，我们取γ值的总和。让我们使用学习率0.1计算数据集的第一个样本的F1(x)。根据*步骤2.2*中的回归树，我们的样本（BMI
    > 26）映射到γ2,1。因为它只映射到一个叶子节点，所以我们不需要方程中的求和部分。因此，方程看起来是这样的：
- en: F 1(x) = F 0(x) + ν γ 2,1 = 125 + 0.1(45) = 129.5
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: F1(x) = F0(x) + νγ2,1 = 125 + 0.1(45) = 129.5
- en: 'As expected, our prediction has improved in the direction of our target values.
    Doing the same for the other samples, we have the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们的预测在目标值的方向上有所改进。对其他样本做同样的处理，我们得到以下结果：
- en: '| **Gender** | **F. Blood****Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **Prediction** **F 0(x)** | **Residuals****for** **F 0(x)** | **Prediction**
    **F 1(x)** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **性别** | **空腹血糖** | **腰围** | **BMI** | **LDL胆固醇** | **预测F0(x)** | **F0(x)的残差**
    | **预测F1(x)** |'
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 | 45 | 129.5 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 105 | 110 | 29.3 | 170 | 125 | 45 | 129.5 |'
- en: '| Female | 85 | 80 | 21 | 90 | 125 | -35 | 122,65 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 女性 | 85 | 80 | 21 | 90 | 125 | -35 | 122.65 |'
- en: '| Male | 95 | 93 | 26 | 113 | 125 | -12 | 122,65 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 男性 | 95 | 93 | 26 | 113 | 125 | -12 | 122.65 |'
- en: Table 2.5 – Following steps 2.1 to 2.4, we calculate a new prediction, F 1(x),
    based on the initial prediction and the residuals
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.5 – 在遵循步骤2.1到2.4之后，我们根据初始预测和残差计算一个新的预测，F1(x)
- en: 'The purpose of the learning rate is to limit the impact each tree might have
    on the overall prediction: by improving our prediction with small steps, we end
    up with an overall more accurate model.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的目的在于限制每棵树对整体预测的影响：通过小步骤改进我们的预测，我们最终得到一个整体更准确的模型。
- en: '*Step 2* is then repeated until we have a final prediction F M(x).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*然后重复，直到我们得到最终的预测Fm(x)。'
- en: In summary, our gradient-boosting ensemble consists of a weighted (by the learning
    rate) summation of predictions made by a series of regression trees, each predicting
    the pseudo residuals (the error gradient, with respect to previous prediction)
    of previous predictions, thereby minimizing the error of previous predictions
    to produce an accurate final prediction.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的梯度提升集成由一系列回归树的预测加权求和组成（权重由学习率决定），每个回归树预测先前预测的伪残差（相对于先前预测的错误梯度），从而最小化先前预测的错误以产生准确的最终预测。
- en: Gradient boosting for classification
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升用于分类
- en: 'Our explanation of gradient boosting given previously used a regression problem
    as an example. We will not be going through a detailed example for classification
    as the algorithm is the same. However, instead of working with continuous predicted
    values, we use the same techniques as logistic regression [(https://en.wikipedia.org/wiki/Logistic_regressi](https://en.wikipedia.org/wiki/Logistic_regression)on).
    The individual trees, therefore, predict the probabilities of a sample belonging
    to the class. The probabilities are calculated by taking the log odds of a sample
    and converting them to a probability using the logistic function, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前给出的梯度提升解释使用了回归问题作为例子。由于算法相同，我们不会详细说明分类的例子。然而，我们不是处理连续的预测值，而是使用与逻辑回归相同的技巧[(https://en.wikipedia.org/wiki/Logistic_regression)]。因此，单个树预测样本属于类的概率。概率是通过取样本的对数似然并将其转换为概率使用逻辑函数来计算的，如下所示：
- en: p(x) =  1 _ 1 + e −(x−μ)/s
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: p(x) = 1 / (1 + e^(-(x-μ)/s))
- en: 'The pseudo residuals are calculated as the difference between the observed
    value (1 or 0 for the class) and the predicted value (the probability from the
    logistic function). A final difference is the loss function. Instead of a function
    such as MSE, we can use cross-entropy for loss, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 伪残差是观测值（对于类的1或0）与预测值（逻辑函数的概率）之间的差异。最终的差异是损失函数。我们不仅可以使用如MSE之类的函数，还可以使用交叉熵作为损失，如下所示：
- en: H p(q) = −  1 _ N  ∑ i=1 N y ilog(p(y i)) + (1 − y i)log(1 − p(y i))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: H p(q) = −  1 _ N  ∑ i=1 N y ilog(p(y i)) + (1 − y i)log(1 − p(y i))
- en: Gradient-boosted decision tree hyperparameters
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升决策树超参数
- en: 'In addition to the parameters for standard decision tree training, the following
    new hyperparameters are available in scikit-learn specific to gradient-boosted
    trees:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准决策树训练的参数外，scikit-learn还提供了以下新超参数，专门针对梯度提升树：
- en: '`n_estimators`: Controls the number of trees in the ensemble. Generally, more
    trees are better. However, a point of diminishing returns is often reached, and
    overfitting occurs when there are too many trees.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：控制集成中树的数量。一般来说，树越多越好。然而，通常会出现收益递减的点，当树的数量过多时，就会发生过拟合。'
- en: '`learning_rate`: Controls the contribution of each tree to the ensemble. Lower
    learning rates lead to longer training times and may require more trees to be
    built (larger values for `n_estimators`). Setting `learning_rate` to a very large
    value may cause the optimization to miss optimum points and must be combined with
    fewer trees.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：控制每棵树对集成贡献的大小。较低的学习率会导致更长的训练时间，可能需要构建更多的树（`n_estimators`的较大值）。将`learning_rate`设置得非常大可能会导致优化错过最佳点，并且必须与较少的树结合使用。'
- en: A complete list of scikit-learn gradient-boosting hyperparameters can be found
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml)找到scikit-learn梯度提升超参数的完整列表。
- en: Gradient boosting in scikit-learn
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn中的梯度提升
- en: The details of gradient boosting are mathematical and complicated; fortunately,
    the algorithm is as accessible as any other via scikit-learn. The following is
    an example of a `GradientBoostingClassifier` class in scikit-learn, again using
    the *Forest CoverType* dataset we used earlier in the chapter to train a random
    forest classifier.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的细节是数学的且复杂；幸运的是，通过scikit-learn，该算法与其他算法一样易于访问。以下是一个scikit-learn中`GradientBoostingClassifier`类的示例，再次使用我们在本章前面使用的*Forest
    CoverType*数据集来训练随机森林分类器。
- en: 'The classifier is also imported from the `ensemble` package, like so:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器也像这样从`ensemble`包中导入：
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We fetch and split the data as before and then fit the model, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样获取并分割数据，然后拟合模型，如下所示：
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Running the preceding code should produce an F1 score of 0.7119, a score that’s
    significantly worse than even a standard decision tree. We can spend time optimizing
    our hyperparameters to improve performance. However, there is a more significant
    issue. The previous code takes very long to execute—in the order of 45 minutes
    on our hardware—compared to ExtraTrees, which takes approximately 3 minutes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码应该会产生一个F1分数为0.7119，这个分数比标准的决策树还要差得多。我们可以花时间优化超参数以提高性能。然而，有一个更严重的问题。与ExtraTrees相比，之前的代码执行时间非常长——在我们的硬件上大约需要45分钟——而ExtraTrees只需要大约3分钟。
- en: LightGBM addresses both issues we have with the gradient-boosted tree and builds
    a gradient-boosted tree with significantly better performance in a fraction of
    the time.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM解决了我们与梯度提升树相关的问题，并以更短的时间构建了一个性能显著更好的梯度提升树。
- en: 'In the following section, we’ll briefly cover an advanced algorithm related
    to gradient boosting: DART.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要介绍与梯度提升相关的高级算法：DART。
- en: Advanced boosting algorithm – DART
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级提升算法 – DART
- en: '**DART** is an extension of the standard GBDT algorithm discussed in the previous
    section [4]. DART employs **dropouts**, a technique from **deep learning** (**DL**),
    to avoid overfitting by the decision tree ensemble. The extension is straightforward
    and consists of two parts. First, when fitting the next prediction tree, M n+1(x),
    which consists of the scaled sum of all previous trees M n…M 1, a random subset
    of the previous trees is instead used, with other trees dropped from the sum.
    The p drop parameter controls the probability of a previous tree being included.
    The second part of the DART algorithm is to apply additional scaling of the contribution
    of the new tree. Let k be the number of trees dropped when the new tree, M n+1,
    was calculated. Since M n+1 was calculated without the contribution of those k
    trees when updating our prediction, F n+1, which includes all trees, the prediction
    overshoots. Therefore, the new tree is scaled by a factor of 1 _ k  to compensate.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**DART**是前一小节中讨论的标准GBDT算法的扩展[4]。DART采用了**dropout**，这是一种来自**深度学习**（**DL**）的技术，通过决策树集成来避免过拟合。这个扩展很简单，包括两个部分。首先，在拟合下一个预测树M n+1(x)，它由所有先前树的缩放总和M n…M 1组成时，使用先前树的随机子集，而不是从和中删除其他树。p drop参数控制先前树被包含的概率。DART算法的第二部分是对新树的贡献进行额外的缩放。设k为新树M n+1计算时删除的树的数目。由于M n+1是在更新我们的预测F n+1（包括所有树）时没有这些k棵树的贡献下计算的，因此预测会超出。因此，新树通过一个1 _ k 的因子进行缩放以补偿。'
- en: DART has been shown to outperform standard GBDTs while also significantly improving
    overfitting.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: DART已被证明在性能上优于标准的GBDT，同时显著减少了过拟合。
- en: Scikit-learn does not implement DART for GBDTs, but DART is incorporated in
    LightGBM.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn没有为GBDT实现DART，但DART已包含在LightGBM中。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In conclusion, this chapter looked at the two most common methods of ensemble
    learning for decision trees: bagging and boosting. We looked at the Random Forests
    and ExtraTrees algorithms, which build decision tree ensembles using bagging.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章探讨了决策树集成学习的两种最常见方法：bagging和boosting。我们研究了随机森林和ExtraTrees算法，它们使用bagging构建决策树集成。
- en: This chapter also gave a detailed overview of boosting in decision trees by
    going through the GBDT algorithm step by step, illustrating how gradient boosting
    is applied. We covered practical examples of random forests, ExtraTrees, and GBDTs
    for scikit-learn.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还通过逐步介绍GBDT算法，详细概述了决策树中的boosting，说明了梯度提升是如何应用的。我们涵盖了scikit-learn中随机森林、ExtraTrees和GBDT的实用示例。
- en: Finally, we looked at how dropouts can be applied to GBDTs with the DART algorithm.
    We now thoroughly understand decision tree ensemble techniques and are ready to
    dive deep into LightGBM.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了如何使用DART算法将dropout应用于GBDT。我们现在彻底理解了决策树集成技术，并准备好深入研究LightGBM。
- en: The next chapter introduces the LightGBM library in detail, both the theoretical
    advancements made by the library and the practical application thereof. We will
    also look at using LightGBM with Python to solve ML problems.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将详细介绍LightGBM库，包括该库的理论进步及其实际应用。我们还将探讨如何使用Python和LightGBM解决机器学习问题。
- en: References
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| *[**1]* | *L. Breiman, “Random forests,” Machine learning, vol. 45, p.* *5-32,
    2001.* |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| *[**1]* | *L. Breiman, “随机森林，”机器学习，第45卷，第* *5-32页，2001年。* |'
- en: '| *[**2]* | *P. Geurts, D. Ernst and L. Wehenkel, “Extremely randomized trees,”
    Machine learning, vol. 63, p.* *3-42, 2006.* |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| *[**2]* | *P. Geurts, D. Ernst和L. Wehenkel, “Extremely randomized trees，”机器学习，第63卷，第*
    *3-42页，2006年。* |'
- en: '| *[**3]* | *J. H. Friedman, “Greedy function approximation: a gradient boosting
    machine,” The Annals of Statistics, p.* *1189-1232, 2001.* |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| *[**3]* | *J. H. Friedman, “Greedy function approximation: a gradient boosting
    machine，”统计年鉴，第* *1189-1232页，2001年。* |'
- en: '| *[**4]* | *R. K. Vinayak and R. Gilad-Bachrach, “Dart: Dropouts meet multiple
    additive regression trees,” in Artificial Intelligence and* *Statistics, 2015.*
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| *[**4]* | *R. K. Vinayak和R. Gilad-Bachrach, “Dart: Dropouts meet multiple
    additive regression trees，”在人工智能与* *统计学，2015年。* |'
