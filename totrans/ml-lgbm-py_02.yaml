- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble Learning – Bagging and Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the fundamentals of **machine learning**
    (**ML**), working with data and models, and concepts such as overfitting and **supervised
    learning** (**SL**). We also introduced decision trees and saw how to apply them
    practically in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about ensemble learning and the two most significant
    types of ensemble learning: bagging and boosting. We will cover the theory and
    practice of applying ensemble learning to decision trees and conclude the chapter
    by focusing on more advanced boosting methods.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a good understanding of ensemble learning
    and how to practically build decision tree ensembles through bagging or boosting.
    We will also be ready to dive deep into LightGBM, including its more advanced
    theoretical aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics we will cover are set out here:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging and random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient-boosted decision** **trees** (**GBDTs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced boosting algorithm—**Dropouts meet Multiple Additive Regression** **Trees**
    (**DART**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples of simple ML algorithms and introduces working
    with scikit-learn. You must install a Python environment with scikit-learn, NumPy,
    pandas, and Jupyter. The code for this chapter is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2).
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ensemble learning** is the practice of combining multiple predictors, or
    models, to create a more robust model. Models can either be of the same type (homogenous
    ensembles) or different types (heterogenous ensembles). Further, ensemble learning
    is not specific to decision trees and can be applied to any ML technique, including
    linear models, **neural networks** (**NNs**), and more.'
  prefs: []
  type: TYPE_NORMAL
- en: The central idea behind ensemble learning is that by aggregating the results
    of many models, we compensate for the weaknesses of a single model.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, training the same models on the same data is not helpful in an ensemble
    (as the models will have similar predictions). Therefore, we aim for **diversity**
    in the models. Diversity refers to the degree to which each model in the ensemble
    differs. A high-diversity ensemble has widely different models.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways we can ensure diversity in our ensemble. One method is
    to train models on different subsets of the training data. Each model is exposed
    to different patterns and noise in the training data, increasing the diversity
    of the trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can train each model on a different subset of features in the
    training data. Some features are more valuable than others, and some might be
    irrelevant, leading to diversity in the model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We can also train each model with different hyperparameters, leading to different
    models of varying complexity and ability. The impact of hyperparameters is especially
    pronounced in the case of decision trees, where the hyperparameters significantly
    impact the structure of the trees, leading to very different models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we could diversify the ensemble by using different types of models.
    Each model has unique strengths and weaknesses, leading to diversity in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ensemble learning method** refers to how we introduce diversity in ensemble
    models by specifying how we train the member models and how we combine the results
    of the models. The most common ensemble methods are set out here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap aggregation (bagging)**: These are methods where models are trained
    on subsets of the training data (either samples or features), and the predictions
    are aggregated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: This involves iteratively training models on the errors of previous
    models. The final prediction is made by combining the prediction of all models
    in the chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacking**: This involves methods where multiple base models are trained,
    then a higher-order model (known as a meta-model) is trained to learn from the
    base model predictions and make the final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blending**: This is very similar to stacking. However, the meta-model is
    trained on predictions made by the base models on a *hold-out set* (a part of
    the training data the base learners were not trained on) instead of the whole
    training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The purpose of ensemble learning methods is to improve our prediction performance,
    and there are several ways ensembles improve the performance over individual models,
    as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved accuracy**: By combining predictions, we increase the likelihood
    that the final prediction is accurate as, in aggregate, models make fewer mistakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved generalization and overfitting**: By aggregating predictions, we
    reduce the variance in the final prediction, improving generalization. Further,
    in some ensemble methods, the models cannot access all data (bagging ensembles),
    reducing noise and overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved prediction stability**: The aggregation of predictions reduces the
    random fluctuations of individual predictions. Ensembles are less sensitive to
    outliers, and outlying predictions of member models have a limited impact on the
    final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees are well suited for ensemble learning, and decision tree-specific
    algorithms exist for ensemble learning. The following section discusses bagging
    ensembles in decision trees focusing on **random forests**.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bagging** is an ensemble method where multiple models are trained on subsets
    of the training data. The models’ predictions are combined to make a final prediction,
    usually by taking the average for numerical prediction (for regression) or the
    majority vote for a class (for classification). When training each model, we select
    a subset of data from the original training dataset with replacement—that is,
    a specific training pattern can be a member of multiple subsets. Since each model
    is only presented with a sample of the training data, no single model can “memorize”
    the training data, which reduces overfitting. The following diagram illustrates
    the bagging process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Illustration of the bagging process; each independent classifier
    is trained on a random subsample from the training data and a final prediction
    is made by aggregating the predictions of all classifiers](img/B16690_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Illustration of the bagging process; each independent classifier
    is trained on a random subsample from the training data and a final prediction
    is made by aggregating the predictions of all classifiers
  prefs: []
  type: TYPE_NORMAL
- en: Each model in a bagging ensemble is still a complete model, capable of standing
    on its own. As such, bagging works best with strong models—that is, in the case
    of decision trees, deep or wide decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: While the previous example illustrates sampling patterns from the training set,
    it is also possible to subsample random features from the dataset for each model.
    Selecting features at random when creating a training set is known as a random
    subspace method or feature bagging. Feature bagging prevents occurrences where
    a specific attribute might dominate the prediction or mislead the model and further
    reduces overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In decision trees, a popular algorithm that applies both sample bagging and
    feature bagging is the random forest. Let’s have a look at this algorithm now.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Random forest** is a decision tree-specific bagging ensemble learning method
    [1]. Instead of building a single decision tree, as the name implies, many decision
    trees are trained using bagging: each tree is trained on either random samples,
    random features from the training data, or both. Random forests support both classification
    and regression.'
  prefs: []
  type: TYPE_NORMAL
- en: The training methodology for individual trees in random forests is the same
    as for a single decision tree, and as explained previously, each tree is a complete
    tree. The final prediction for the forest is made by taking the arithmetic mean
    of all trees in the case of prediction or the majority vote for a class (in the
    case of classification).
  prefs: []
  type: TYPE_NORMAL
- en: Regarding performance, random forest learning produces a more robust model with
    higher accuracy, which tends to avoid overfitting (since no single model can overfit
    on all training data).
  prefs: []
  type: TYPE_NORMAL
- en: Random forest hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In scikit-learn, as may be expected, the hyperparameters available for random
    forests are the same as those available to train decision trees. We can specify
    `max_depth`, `min_samples_split`, `max_leaf_nodes`, and so on, which are then
    used to train the individual trees. However, there are three notable additional
    parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: Controls the number of trees in the forest. Generally, more
    trees are better. However, a point of diminishing returns is often reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: Determines the maximum number of features to be used as a subset
    when splitting a node. Setting `max_features=1.0` allows all features to be used
    in the random selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bootstrap` determines whether bagging is used. All trees use the entire training
    set if `bootstrap` is set to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A list of all parameters available in scikit-learn is available here: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: ExtraTrees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Extremely Randomized Trees** (**ExtraTrees**) is a related method for building
    randomized decision trees. With ExtraTrees, when building a decision node, several
    candidate splits are created randomly instead of using the *Gini index* or *information
    gain* metric to calculate the optimal split [2]. The best split from all random
    splits is then chosen for the node. The methodology for ExtraTrees can be applied
    to a single decision tree or used in conjunction with Random Forests. scikit-learn
    implements ExtraTrees as an extension to random forests (https://scikit-learn.org/stable/modules/ensemble.xhtml#extremely-randomized-trees).'
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, the ExtraTrees implementation has the same hyperparameters
    as random forests.
  prefs: []
  type: TYPE_NORMAL
- en: Training random forests using scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll now have a look at using random forests with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’ll use the *Forest CoverType* dataset ([https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype)),
    which is available wit[hin scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml#sklearn.data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml%23sklearn.datasets.fetch_covtype)sets.fetch_covtype).
    The dataset is a significant step up from the toy datasets we were using previously.
    The dataset consists of 581,012 samples and has a dimensionality (number of features)
    of 54\. The features describe a 30x30m patch of forest in the US (for example,
    elevation, aspect, slope, and distances to hydrology). We must build a classifier
    to classify each patch into one of seven classes describing the forest cover type.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to training a `RandomForestClassifier`, we’ll train a standalone
    `DecisionTreeClassifier` and an `ExtraTreesClassifier` and compare the performance
    of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomForestClassifier` and `ExtraTreesClassifier` live in the `sklearn.ensemble`
    package. In addition to our regular imports, we import the classifiers from there,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The scikit-learn datasets package provides the Forest Cover dataset. We can
    use scikit-learn to fetch the dataset and split it into our training and test
    sets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train our classifiers and evaluate each of them against the
    test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have set hyperparameters for the models that are appropriate to the problem.
    An additional step would be to optimize the algorithm hyperparameters to discover
    the best parameter values. Parameter optimization is discussed in detail in a
    later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding code, we get the following F1 scores for each of the
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.8917 |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | 0.9209 |'
  prefs: []
  type: TYPE_TB
- en: '| ExtraTrees | 0.9231 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – F1 scores for each of the algorithms on the Forest CoverType dataset
  prefs: []
  type: TYPE_NORMAL
- en: The ExtraTrees model slightly outperforms the random forest model, and both
    perform better than the decision tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we gave an overview of bagging and random forests, a bagging-based
    decision tree ensemble learning method that provides some benefits over standard
    decision trees. The following section examines an alternative ensemble learning
    method: gradient boosting.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-boosted decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gradient boosting** is an ensemble learning methodology that combines multiple
    models *sequentially* to produce a more robust ensemble model. Unlike bagging,
    where multiple *strong* models are used (in parallel), with boosting, multiple
    weak learners are trained, each learning from the mistakes of those before it
    to build a more accurate and robust ensemble model. Another distinct difference
    from bagging is that each model uses the entire dataset for training.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As discussed next, gradient boosting always builds a series of regression trees
    to form part of the ensemble, regardless of whether a regression or classification
    problem is solved. Gradient boosting is also called **Multiple Additive Regression**
    **Trees** (**MART**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Abstractly, the boosting process starts with a weak base learner. In the case
    of decision trees, the base learner might have only a single split (also known
    as a decision stump). The error residuals (the difference between the predicted
    and actual targets) are then calculated. A new learner is then trained on the
    error residuals of the previous learner, looking to minimize the errors. The final
    prediction is a *summation* of the predictions from all the learners. The following
    diagram illustrates the iterative gradient-boosting process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Illustration of the gradient-boosting process; in each iteration,
    a new regression tree is added to compensate for the error residuals of the previous
    iteration](img/B16690_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Illustration of the gradient-boosting process; in each iteration,
    a new regression tree is added to compensate for the error residuals of the previous
    iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the critical questions relates to how we can determine the changes that
    reduce the error residuals. Gradient boosting solves the error minimization problem
    by applying a widely used optimization problem: gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Gradient descent** is an optimization algorithm that attempts to find the
    optimal parameters to minimize a **loss function**. The parameters are updated
    iteratively by taking small steps in the direction of the negative gradient of
    the loss function (thereby decreasing the function value). A loss function is
    similar in concept to an error function but has two important properties, as outlined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: A loss function produces a numeric value that quantifies the model’s performance
    or precisely how poorly a model is doing. A good loss function produces significantly
    different output for models with different performances. Some error functions
    can also be used as loss functions—for example, **mean squared** **error** (**MSE**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second property is that a loss function must be differentiable, specifically
    in the context of gradient descent. An example of an error function that’s not
    differentiable is the F1 score. The F1 score may produce a numeric value of the
    model’s performance, but it is not differentiable and cannot be used as a loss
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process for gradient descent can, then, be defined as follows. Suppose
    we have a loss function L defined for parameters x. For an initial set of parameters,
    the loss is calculated as L( x 0). Gradient descent proceeds iteratively to minimize
    the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: L(x n+1) < L( x n)
  prefs: []
  type: TYPE_NORMAL
- en: 'To update the parameters, *we take a step in the direction of the negative
    gradient of* L. We can specify the gradient descent update rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: x n+1 = x n − γ n ∇ L( x n)
  prefs: []
  type: TYPE_NORMAL
- en: Here, γ n is the **learning rate**, which defines the step size, and ∇ L(x n)
    is the gradient of L at x n.
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph in *Figure 2**.3* illustrates the gradient descent process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Graph showing the gradient descent process to find the minimum
    of a function](img/B16690_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Graph showing the gradient descent process to find the minimum
    of a function
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing an appropriate learning rate is critical to gradient descent’s success.
    If the learning rate is too low, the optimization will be very slow, potentially
    not reaching a minimum in the allowed number of iterations. A small learning rate
    could also lead to the process getting stuck in a local minimum: the step size
    being too small to escape. Conversely, suppose the learning rate is too large.
    In that case, we could step over the minimum and miss it entirely or get stuck
    oscillating around a minimum (constantly jumping back and forth but never descending
    to the optimal value).'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand how gradient descent works, we can see how it is applied
    in gradient boosting. We will work through the entire gradient-boosting algorithm
    in detail at the hand of a small example. In our example, we’ll use a regression
    tree, as it is a bit easier to understand than the case for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-boosting algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The gradient-boosting algorithm is defined as follows, where M is the number
    of boosted trees [3]:'
  prefs: []
  type: TYPE_NORMAL
- en: Given training data {(x i, y i)} i=1 n , containing n training samples (defined
    by features x i and target y i) and a differentiable loss function L(y i, F(x)),
    where F(x) are the predictions from model F.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the model with a constant prediction value F 0(X) = argmin γ ∑ i=1 n  L(
    y i, γ)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For m = 1 to M:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute pseudo residuals r im = − [∂ L(y i, F(x i)) _ ∂ F(x i) ] F(X)=F m−1(X)
    for i = 1, … , n
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a regression tree to the r im values and create terminal regions R jm for
    j = 1…J m
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For j = 1…J m compute γ jm = argmin γ ∑ x i∈R ij L( y i, F m−1(x i) + γ)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Update F m(x) = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Result: F M(x)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although the algorithm and especially the mathematics might look intimidating,
    it is practically much more straightforward than it appears. We’ll go through
    the algorithm step by step. Consider the following toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **Fasting** **Blood Sugar** | **Waist Circumference** | **BMI**
    | **LDL Cholesterol** |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 |'
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Example dataset consisting of a patient’s physical measurements
    and measured low-density lipoprotein (LDL) cholesterol
  prefs: []
  type: TYPE_NORMAL
- en: Given the physical measurements, we aim to predict a patient’s LDL cholesterol.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding table defines our training data {(x i, y i)} i=1 n , where x
    is the features (blood sugar, waist circumference, BMI) and y is the target: LDL
    cholesterol.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a differentiable loss function, and to simplify some of the mathematical
    derivations in this example, we choose the following loss function, which is similar
    to the MSE function:'
  prefs: []
  type: TYPE_NORMAL
- en: L =  1 _ 2  ∑ i=0 n (y i − γ i) 2
  prefs: []
  type: TYPE_NORMAL
- en: We now work through each algorithm step in detail to see how the gradient-boosted
    tree is produced.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to find F 0(x) = argmin γ ∑ i=1 n  L( y i, γ), where y i is
    our target value and γ is our initial predicted value. *Our initial prediction
    is constant and is simply the average of the target values*. But let’s see why.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for F 0(x) is stating that we need to find a value for γ that
    minimizes our loss function. To find the minimum, we take the derivative of the
    loss function with respect to gamma:'
  prefs: []
  type: TYPE_NORMAL
- en: ∂ L _ ∂ γ  =  2 _ 2 (∑ i=0 n (y i − γ)) × − 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, set it to 0 and solve the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: − (∑ i=0 n (y i − γ)) = 0
  prefs: []
  type: TYPE_NORMAL
- en: −  1 _ n  ∑ i=0 n y i+ γ = 0
  prefs: []
  type: TYPE_NORMAL
- en: γ =  1 _ n  ∑ i=0 n y i
  prefs: []
  type: TYPE_NORMAL
- en: The equation simplifies to calculating the average of the target values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating our table with the predictions, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **F.** **Blood Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **PredictionF 0(x)** |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 |'
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 | 125 |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 | 125 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.3 – Our initial prediction of LDL cholesterol predictions ( F 0(x))
    for each patient is constant
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the following M times, where M is the number of trees we choose to
    build.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to calculate the pseudo residuals r im = − [∂ L(y i, F(x i)) _ ∂
    F(x i) ] F(X)=F m−1(X). This equation for r im is stating that we use the negative
    of the partial derivative of the loss function, with respect to the predictions,
    to calculate the pseudo residuals. *This portion of the gradient-boosting algorithm
    relates to gradient descent: we are taking the negative of the gradient to minimize
    the residuals*. Fortunately, we have already calculated this derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: − [ ∂ L(y i, F(x i)) _ ∂ F(x i) ] = −  ∂ _ ∂ F(x i) (  1 _ 2  (y i − F(x i)) 2)
    =  2 _ 2 (y i− F(x i)) = ( y i − F(x i))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, F(x i) is the predicted value. In other words, *the equation simplifies
    the difference between the target and predicted values*. We can add the residuals
    to the table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **F. Blood****Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **Prediction** **F 0(x)** | **Residuals****for** **F 0(x)** |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 | 125 | -35 |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 | 125 | -12 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.4 – Based on our initial prediction, we can calculate the residuals
    for each patient, as shown in the Residuals for F 0(x) column
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is straightforward: we build a regression tree to predict the
    residuals. We do not use the predictions of the regression tree directly. Instead,
    we use the *terminal regions* to calculate our updated prediction. The terminal
    regions refer to the tree’s leaf nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we assume the following simple regression tree is built:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Regression tree predicting the residuals](img/B16690_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Regression tree predicting the residuals
  prefs: []
  type: TYPE_NORMAL
- en: With our regression tree built and our leaf nodes defined, we can proceed with
    the next step. We need to compute γ jm that minimizes our loss function, taking
    into account the previous predictions as per γ jm = argmin γ ∑ x i∈R ij L( y i,
    F m−1(x i) + γ). This is almost precisely what we did in *step 1*, where we showed
    that the equation simplifies taking the average of the predicted values due to
    our choice of the loss function. *Here, that means taking the average of the residuals
    in each leaf node*. Therefore, we have γ 1,1 = − 35 − 12 _ 2  = − 23.5 and γ 2,1
    = 45 _ 1  = 45.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can now calculate our next prediction, F 1(x), defined by: F m(x)
    = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm),     meaning our next prediction consists of the previous prediction plus the γ values
    calculated in *step 2.3*, weighed by a learning rate ν. The summation here means
    that should a sample be part of multiple leaf nodes, we take the sum of the gamma
    values. Let’s calculate F 1(x) for the first sample in our dataset, using a learning
    rate of 0.1\. According to the regression tree from *step 2.2*, our sample (which
    has a BMI > 26) maps to γ 2,1\. Since it’s only mapping to a single leaf, we don’t
    need the summation part of the equation. Therefore, the equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: F 1(x) = F 0(x) + ν γ 2,1 = 125 + 0.1(45) = 129.5
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, our prediction has improved in the direction of our target values.
    Doing the same for the other samples, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **F. Blood****Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **Prediction** **F 0(x)** | **Residuals****for** **F 0(x)** | **Prediction**
    **F 1(x)** |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 | 45 | 129.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 | 125 | -35 | 122,65 |'
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 | 125 | -12 | 122,65 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.5 – Following steps 2.1 to 2.4, we calculate a new prediction, F 1(x),
    based on the initial prediction and the residuals
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of the learning rate is to limit the impact each tree might have
    on the overall prediction: by improving our prediction with small steps, we end
    up with an overall more accurate model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2* is then repeated until we have a final prediction F M(x).'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our gradient-boosting ensemble consists of a weighted (by the learning
    rate) summation of predictions made by a series of regression trees, each predicting
    the pseudo residuals (the error gradient, with respect to previous prediction)
    of previous predictions, thereby minimizing the error of previous predictions
    to produce an accurate final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting for classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our explanation of gradient boosting given previously used a regression problem
    as an example. We will not be going through a detailed example for classification
    as the algorithm is the same. However, instead of working with continuous predicted
    values, we use the same techniques as logistic regression [(https://en.wikipedia.org/wiki/Logistic_regressi](https://en.wikipedia.org/wiki/Logistic_regression)on).
    The individual trees, therefore, predict the probabilities of a sample belonging
    to the class. The probabilities are calculated by taking the log odds of a sample
    and converting them to a probability using the logistic function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p(x) =  1 _ 1 + e −(x−μ)/s
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo residuals are calculated as the difference between the observed
    value (1 or 0 for the class) and the predicted value (the probability from the
    logistic function). A final difference is the loss function. Instead of a function
    such as MSE, we can use cross-entropy for loss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: H p(q) = −  1 _ N  ∑ i=1 N y ilog(p(y i)) + (1 − y i)log(1 − p(y i))
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-boosted decision tree hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the parameters for standard decision tree training, the following
    new hyperparameters are available in scikit-learn specific to gradient-boosted
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: Controls the number of trees in the ensemble. Generally, more
    trees are better. However, a point of diminishing returns is often reached, and
    overfitting occurs when there are too many trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: Controls the contribution of each tree to the ensemble. Lower
    learning rates lead to longer training times and may require more trees to be
    built (larger values for `n_estimators`). Setting `learning_rate` to a very large
    value may cause the optimization to miss optimum points and must be combined with
    fewer trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complete list of scikit-learn gradient-boosting hyperparameters can be found
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting in scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The details of gradient boosting are mathematical and complicated; fortunately,
    the algorithm is as accessible as any other via scikit-learn. The following is
    an example of a `GradientBoostingClassifier` class in scikit-learn, again using
    the *Forest CoverType* dataset we used earlier in the chapter to train a random
    forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classifier is also imported from the `ensemble` package, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We fetch and split the data as before and then fit the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code should produce an F1 score of 0.7119, a score that’s
    significantly worse than even a standard decision tree. We can spend time optimizing
    our hyperparameters to improve performance. However, there is a more significant
    issue. The previous code takes very long to execute—in the order of 45 minutes
    on our hardware—compared to ExtraTrees, which takes approximately 3 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM addresses both issues we have with the gradient-boosted tree and builds
    a gradient-boosted tree with significantly better performance in a fraction of
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we’ll briefly cover an advanced algorithm related
    to gradient boosting: DART.'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced boosting algorithm – DART
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DART** is an extension of the standard GBDT algorithm discussed in the previous
    section [4]. DART employs **dropouts**, a technique from **deep learning** (**DL**),
    to avoid overfitting by the decision tree ensemble. The extension is straightforward
    and consists of two parts. First, when fitting the next prediction tree, M n+1(x),
    which consists of the scaled sum of all previous trees M n…M 1, a random subset
    of the previous trees is instead used, with other trees dropped from the sum.
    The p drop parameter controls the probability of a previous tree being included.
    The second part of the DART algorithm is to apply additional scaling of the contribution
    of the new tree. Let k be the number of trees dropped when the new tree, M n+1,
    was calculated. Since M n+1 was calculated without the contribution of those k
    trees when updating our prediction, F n+1, which includes all trees, the prediction
    overshoots. Therefore, the new tree is scaled by a factor of 1 _ k  to compensate.'
  prefs: []
  type: TYPE_NORMAL
- en: DART has been shown to outperform standard GBDTs while also significantly improving
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn does not implement DART for GBDTs, but DART is incorporated in
    LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In conclusion, this chapter looked at the two most common methods of ensemble
    learning for decision trees: bagging and boosting. We looked at the Random Forests
    and ExtraTrees algorithms, which build decision tree ensembles using bagging.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also gave a detailed overview of boosting in decision trees by
    going through the GBDT algorithm step by step, illustrating how gradient boosting
    is applied. We covered practical examples of random forests, ExtraTrees, and GBDTs
    for scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how dropouts can be applied to GBDTs with the DART algorithm.
    We now thoroughly understand decision tree ensemble techniques and are ready to
    dive deep into LightGBM.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter introduces the LightGBM library in detail, both the theoretical
    advancements made by the library and the practical application thereof. We will
    also look at using LightGBM with Python to solve ML problems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *L. Breiman, “Random forests,” Machine learning, vol. 45, p.* *5-32,
    2001.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *P. Geurts, D. Ernst and L. Wehenkel, “Extremely randomized trees,”
    Machine learning, vol. 63, p.* *3-42, 2006.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**3]* | *J. H. Friedman, “Greedy function approximation: a gradient boosting
    machine,” The Annals of Statistics, p.* *1189-1232, 2001.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**4]* | *R. K. Vinayak and R. Gilad-Bachrach, “Dart: Dropouts meet multiple
    additive regression trees,” in Artificial Intelligence and* *Statistics, 2015.*
    |'
  prefs: []
  type: TYPE_TB
