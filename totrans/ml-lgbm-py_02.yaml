- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble Learning – Bagging and Boosting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the fundamentals of **machine learning**
    (**ML**), working with data and models, and concepts such as overfitting and **supervised
    learning** (**SL**). We also introduced decision trees and saw how to apply them
    practically in scikit-learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about ensemble learning and the two most significant
    types of ensemble learning: bagging and boosting. We will cover the theory and
    practice of applying ensemble learning to decision trees and conclude the chapter
    by focusing on more advanced boosting methods.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a good understanding of ensemble learning
    and how to practically build decision tree ensembles through bagging or boosting.
    We will also be ready to dive deep into LightGBM, including its more advanced
    theoretical aspects.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics we will cover are set out here:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging and random forests
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient-boosted decision** **trees** (**GBDTs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced boosting algorithm—**Dropouts meet Multiple Additive Regression** **Trees**
    (**DART**)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples of simple ML algorithms and introduces working
    with scikit-learn. You must install a Python environment with scikit-learn, NumPy,
    pandas, and Jupyter. The code for this chapter is available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ensemble learning** is the practice of combining multiple predictors, or
    models, to create a more robust model. Models can either be of the same type (homogenous
    ensembles) or different types (heterogenous ensembles). Further, ensemble learning
    is not specific to decision trees and can be applied to any ML technique, including
    linear models, **neural networks** (**NNs**), and more.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The central idea behind ensemble learning is that by aggregating the results
    of many models, we compensate for the weaknesses of a single model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Of course, training the same models on the same data is not helpful in an ensemble
    (as the models will have similar predictions). Therefore, we aim for **diversity**
    in the models. Diversity refers to the degree to which each model in the ensemble
    differs. A high-diversity ensemble has widely different models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways we can ensure diversity in our ensemble. One method is
    to train models on different subsets of the training data. Each model is exposed
    to different patterns and noise in the training data, increasing the diversity
    of the trained models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can train each model on a different subset of features in the
    training data. Some features are more valuable than others, and some might be
    irrelevant, leading to diversity in the model predictions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We can also train each model with different hyperparameters, leading to different
    models of varying complexity and ability. The impact of hyperparameters is especially
    pronounced in the case of decision trees, where the hyperparameters significantly
    impact the structure of the trees, leading to very different models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we could diversify the ensemble by using different types of models.
    Each model has unique strengths and weaknesses, leading to diversity in the ensemble.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ensemble learning method** refers to how we introduce diversity in ensemble
    models by specifying how we train the member models and how we combine the results
    of the models. The most common ensemble methods are set out here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap aggregation (bagging)**: These are methods where models are trained
    on subsets of the training data (either samples or features), and the predictions
    are aggregated.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: This involves iteratively training models on the errors of previous
    models. The final prediction is made by combining the prediction of all models
    in the chain.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacking**: This involves methods where multiple base models are trained,
    then a higher-order model (known as a meta-model) is trained to learn from the
    base model predictions and make the final prediction.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blending**: This is very similar to stacking. However, the meta-model is
    trained on predictions made by the base models on a *hold-out set* (a part of
    the training data the base learners were not trained on) instead of the whole
    training set.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The purpose of ensemble learning methods is to improve our prediction performance,
    and there are several ways ensembles improve the performance over individual models,
    as outlined here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved accuracy**: By combining predictions, we increase the likelihood
    that the final prediction is accurate as, in aggregate, models make fewer mistakes.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved generalization and overfitting**: By aggregating predictions, we
    reduce the variance in the final prediction, improving generalization. Further,
    in some ensemble methods, the models cannot access all data (bagging ensembles),
    reducing noise and overfitting.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved prediction stability**: The aggregation of predictions reduces the
    random fluctuations of individual predictions. Ensembles are less sensitive to
    outliers, and outlying predictions of member models have a limited impact on the
    final prediction.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees are well suited for ensemble learning, and decision tree-specific
    algorithms exist for ensemble learning. The following section discusses bagging
    ensembles in decision trees focusing on **random forests**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and random forests
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bagging** is an ensemble method where multiple models are trained on subsets
    of the training data. The models’ predictions are combined to make a final prediction,
    usually by taking the average for numerical prediction (for regression) or the
    majority vote for a class (for classification). When training each model, we select
    a subset of data from the original training dataset with replacement—that is,
    a specific training pattern can be a member of multiple subsets. Since each model
    is only presented with a sample of the training data, no single model can “memorize”
    the training data, which reduces overfitting. The following diagram illustrates
    the bagging process:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**袋装**是一种集成方法，其中多个模型在训练数据的子集上训练。模型的预测被组合起来做出最终预测，通常是通过取数值预测的平均值（对于回归）或对类别的多数投票（对于分类）。在训练每个模型时，我们从原始训练数据集中选择一个数据子集，并带有替换——也就是说，特定的训练模式可以是多个子集的成员。由于每个模型只接触到训练数据的一个样本，因此没有单个模型可以“记住”训练数据，这减少了过拟合。以下图表说明了袋装过程：'
- en: '![Figure 2.1 – Illustration of the bagging process; each independent classifier
    is trained on a random subsample from the training data and a final prediction
    is made by aggregating the predictions of all classifiers](img/B16690_02_1.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 描述袋装过程的示意图；每个独立的分类器在训练数据的随机子样本上训练，并通过汇总所有分类器的预测来做出最终预测](img/B16690_02_1.jpg)'
- en: Figure 2.1 – Illustration of the bagging process; each independent classifier
    is trained on a random subsample from the training data and a final prediction
    is made by aggregating the predictions of all classifiers
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 描述袋装过程的示意图；每个独立的分类器在训练数据的随机子样本上训练，并通过汇总所有分类器的预测来做出最终预测
- en: Each model in a bagging ensemble is still a complete model, capable of standing
    on its own. As such, bagging works best with strong models—that is, in the case
    of decision trees, deep or wide decision trees.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装集成中的每个模型仍然是一个完整的模型，能够独立存在。因此，袋装与强大的模型结合得最好——也就是说，在决策树的情况下，深或宽的决策树。
- en: While the previous example illustrates sampling patterns from the training set,
    it is also possible to subsample random features from the dataset for each model.
    Selecting features at random when creating a training set is known as a random
    subspace method or feature bagging. Feature bagging prevents occurrences where
    a specific attribute might dominate the prediction or mislead the model and further
    reduces overfitting.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前的例子说明了从训练集中抽取样本模式，但也可以为每个模型从数据集中抽取随机特征。在创建训练集时随机选择特征被称为随机子空间方法或特征袋装。特征袋装防止了特定属性可能主导预测或误导模型的情况，并进一步减少了过拟合。
- en: In decision trees, a popular algorithm that applies both sample bagging and
    feature bagging is the random forest. Let’s have a look at this algorithm now.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，一个同时应用样本袋装和特征袋装的流行算法是随机森林。现在让我们来看看这个算法。
- en: Random forest
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random forest** is a decision tree-specific bagging ensemble learning method
    [1]. Instead of building a single decision tree, as the name implies, many decision
    trees are trained using bagging: each tree is trained on either random samples,
    random features from the training data, or both. Random forests support both classification
    and regression.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是一种针对决策树的特定袋装集成学习方法[1]。正如其名称所暗示的，它不是构建单个决策树，而是使用袋装训练多个决策树：每棵树要么在随机样本上训练，要么在训练数据中的随机特征上训练，或者两者都训练。随机森林支持分类和回归。'
- en: The training methodology for individual trees in random forests is the same
    as for a single decision tree, and as explained previously, each tree is a complete
    tree. The final prediction for the forest is made by taking the arithmetic mean
    of all trees in the case of prediction or the majority vote for a class (in the
    case of classification).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林中单个树的训练方法与单个决策树相同，正如之前所解释的，每棵树都是一棵完整的树。对于预测，森林的最终预测是通过取所有树的算术平均值来实现的；对于分类，是通过多数投票来实现的。
- en: Regarding performance, random forest learning produces a more robust model with
    higher accuracy, which tends to avoid overfitting (since no single model can overfit
    on all training data).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关于性能，随机森林学习产生了一个更稳健的模型，具有更高的准确性，并且倾向于避免过拟合（因为单个模型无法对所有训练数据进行过拟合）。
- en: Random forest hyperparameters
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林超参数
- en: 'In scikit-learn, as may be expected, the hyperparameters available for random
    forests are the same as those available to train decision trees. We can specify
    `max_depth`, `min_samples_split`, `max_leaf_nodes`, and so on, which are then
    used to train the individual trees. However, there are three notable additional
    parameters, as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，正如预期的那样，随机森林的可用超参数与训练决策树的可用超参数相同。我们可以指定`max_depth`、`min_samples_split`、`max_leaf_nodes`等，然后用于训练单个树。然而，有三个值得注意的附加参数，如下所示：
- en: '`n_estimators`: Controls the number of trees in the forest. Generally, more
    trees are better. However, a point of diminishing returns is often reached.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：控制森林中的树的数量。通常，树越多越好。然而，通常会达到收益递减的点。'
- en: '`max_features`: Determines the maximum number of features to be used as a subset
    when splitting a node. Setting `max_features=1.0` allows all features to be used
    in the random selection.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：确定在分割节点时用作子集的最大特征数。将`max_features=1.0`设置为允许在随机选择中使用所有特征。'
- en: '`bootstrap` determines whether bagging is used. All trees use the entire training
    set if `bootstrap` is set to `False`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap`决定是否使用袋装法。如果`bootstrap`设置为`False`，则所有树将使用整个训练集。'
- en: 'A list of all parameters available in scikit-learn is available here: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中所有可用参数的列表在此处提供：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml)。
- en: ExtraTrees
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExtraTrees
- en: '**Extremely Randomized Trees** (**ExtraTrees**) is a related method for building
    randomized decision trees. With ExtraTrees, when building a decision node, several
    candidate splits are created randomly instead of using the *Gini index* or *information
    gain* metric to calculate the optimal split [2]. The best split from all random
    splits is then chosen for the node. The methodology for ExtraTrees can be applied
    to a single decision tree or used in conjunction with Random Forests. scikit-learn
    implements ExtraTrees as an extension to random forests (https://scikit-learn.org/stable/modules/ensemble.xhtml#extremely-randomized-trees).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**极随机树**（**ExtraTrees**）是构建随机决策树的相关方法。使用ExtraTrees时，在构建决策节点时，会随机创建多个候选分割，而不是使用*Gini指数*或*信息增益*指标来计算最佳分割[2]。然后从所有随机分割中选择最佳分割用于节点。ExtraTrees的方法可以应用于单个决策树，也可以与随机森林结合使用。scikit-learn将ExtraTrees作为随机森林的扩展实现（https://scikit-learn.org/stable/modules/ensemble.xhtml#extremely-randomized-trees）。'
- en: In scikit-learn, the ExtraTrees implementation has the same hyperparameters
    as random forests.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，ExtraTrees的实现与随机森林具有相同的超参数。
- en: Training random forests using scikit-learn
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练随机森林
- en: We’ll now have a look at using random forests with scikit-learn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看如何使用scikit-learn中的随机森林。
- en: In this example, we’ll use the *Forest CoverType* dataset ([https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype)),
    which is available wit[hin scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml#sklearn.data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml%23sklearn.datasets.fetch_covtype)sets.fetch_covtype).
    The dataset is a significant step up from the toy datasets we were using previously.
    The dataset consists of 581,012 samples and has a dimensionality (number of features)
    of 54\. The features describe a 30x30m patch of forest in the US (for example,
    elevation, aspect, slope, and distances to hydrology). We must build a classifier
    to classify each patch into one of seven classes describing the forest cover type.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用*森林覆盖类型*数据集（[https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype)），该数据集在scikit-learn（https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.xhtml#sklearn.data）中可用。与之前使用的玩具数据集相比，该数据集是一个显著的提升。数据集包含581,012个样本，具有54个维度（特征数）。特征描述了美国30x30m的森林区域（例如，海拔、坡向、坡度和到水文点的距离）。我们必须构建一个分类器，将每个区域分类为描述森林覆盖类型的七个类别之一。
- en: In addition to training a `RandomForestClassifier`, we’ll train a standalone
    `DecisionTreeClassifier` and an `ExtraTreesClassifier` and compare the performance
    of the algorithms.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练`RandomForestClassifier`之外，我们还将训练一个独立的`DecisionTreeClassifier`和一个`ExtraTreesClassifier`，并比较算法的性能。
- en: '`RandomForestClassifier` and `ExtraTreesClassifier` live in the `sklearn.ensemble`
    package. In addition to our regular imports, we import the classifiers from there,
    like so:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`和`ExtraTreesClassifier`位于`sklearn.ensemble`包中。除了我们的常规导入外，我们还从那里导入分类器，如下所示：'
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The scikit-learn datasets package provides the Forest Cover dataset. We can
    use scikit-learn to fetch the dataset and split it into our training and test
    sets, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn数据集包提供了森林覆盖数据集。我们可以使用scikit-learn获取数据集，并将其拆分为我们的训练集和测试集，如下所示：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we can train our classifiers and evaluate each of them against the
    test set:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以训练我们的分类器，并将它们各自与测试集进行评估：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have set hyperparameters for the models that are appropriate to the problem.
    An additional step would be to optimize the algorithm hyperparameters to discover
    the best parameter values. Parameter optimization is discussed in detail in a
    later chapter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为模型设置了适合问题的超参数。额外的一步是优化算法超参数，以发现最佳参数值。参数优化将在后面的章节中详细讨论。
- en: 'Running the preceding code, we get the following F1 scores for each of the
    algorithms:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，我们得到以下每个算法的F1分数：
- en: '| **Model** | **F1 score** |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **F1分数** |'
- en: '| Decision Tree | 0.8917 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.8917 |'
- en: '| Random Forest | 0.9209 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.9209 |'
- en: '| ExtraTrees | 0.9231 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| ExtraTrees | 0.9231 |'
- en: Table 2.1 – F1 scores for each of the algorithms on the Forest CoverType dataset
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 – Forest CoverType数据集上每个算法的F1分数
- en: The ExtraTrees model slightly outperforms the random forest model, and both
    perform better than the decision tree classifier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ExtraTrees模型略优于随机森林模型，两者都比决策树分类器表现更好。
- en: 'In this section, we gave an overview of bagging and random forests, a bagging-based
    decision tree ensemble learning method that provides some benefits over standard
    decision trees. The following section examines an alternative ensemble learning
    method: gradient boosting.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了bagging和随机森林，这是一种基于bagging的决策树集成学习方法，它相对于标准决策树提供了一些优势。下一节将探讨另一种集成学习方法：梯度提升。
- en: Gradient-boosted decision trees
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阶梯提升决策树
- en: '**Gradient boosting** is an ensemble learning methodology that combines multiple
    models *sequentially* to produce a more robust ensemble model. Unlike bagging,
    where multiple *strong* models are used (in parallel), with boosting, multiple
    weak learners are trained, each learning from the mistakes of those before it
    to build a more accurate and robust ensemble model. Another distinct difference
    from bagging is that each model uses the entire dataset for training.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升**是一种集成学习方法，它通过**顺序**组合多个模型来产生一个更稳健的集成模型。与bagging不同，在bagging中使用了多个**强大**的模型（并行使用），而在boosting中，训练了多个弱学习器，每个学习器都从前一个学习器的错误中学习，以构建一个更准确和更稳健的集成模型。与bagging的另一个显著区别是，每个模型都使用整个数据集进行训练。'
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As discussed next, gradient boosting always builds a series of regression trees
    to form part of the ensemble, regardless of whether a regression or classification
    problem is solved. Gradient boosting is also called **Multiple Additive Regression**
    **Trees** (**MART**).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如下文所述，梯度提升始终构建一系列回归树作为集成的一部分，无论解决的是回归问题还是分类问题。梯度提升也称为**多重加性回归树**（**MART**）。
- en: 'Abstractly, the boosting process starts with a weak base learner. In the case
    of decision trees, the base learner might have only a single split (also known
    as a decision stump). The error residuals (the difference between the predicted
    and actual targets) are then calculated. A new learner is then trained on the
    error residuals of the previous learner, looking to minimize the errors. The final
    prediction is a *summation* of the predictions from all the learners. The following
    diagram illustrates the iterative gradient-boosting process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象地说，boosting过程从弱基学习器开始。在决策树的情况下，基学习器可能只有一个分割（也称为决策树桩）。然后计算误差残差（预测值与实际目标之间的差异）。然后，在先前学习器的误差残差上训练新的学习器，以最小化错误。最终的预测是所有学习器预测的**总和**。以下图示说明了迭代梯度提升过程：
- en: '![Figure 2.2 – Illustration of the gradient-boosting process; in each iteration,
    a new regression tree is added to compensate for the error residuals of the previous
    iteration](img/B16690_02_2.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 阶梯提升过程的示意图；在每次迭代中，都会添加一个新的回归树来补偿前一次迭代的误差残差](img/B16690_02_2.jpg)'
- en: Figure 2.2 – Illustration of the gradient-boosting process; in each iteration,
    a new regression tree is added to compensate for the error residuals of the previous
    iteration
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the critical questions relates to how we can determine the changes that
    reduce the error residuals. Gradient boosting solves the error minimization problem
    by applying a widely used optimization problem: gradient descent.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Gradient descent** is an optimization algorithm that attempts to find the
    optimal parameters to minimize a **loss function**. The parameters are updated
    iteratively by taking small steps in the direction of the negative gradient of
    the loss function (thereby decreasing the function value). A loss function is
    similar in concept to an error function but has two important properties, as outlined
    here:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: A loss function produces a numeric value that quantifies the model’s performance
    or precisely how poorly a model is doing. A good loss function produces significantly
    different output for models with different performances. Some error functions
    can also be used as loss functions—for example, **mean squared** **error** (**MSE**).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second property is that a loss function must be differentiable, specifically
    in the context of gradient descent. An example of an error function that’s not
    differentiable is the F1 score. The F1 score may produce a numeric value of the
    model’s performance, but it is not differentiable and cannot be used as a loss
    function.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process for gradient descent can, then, be defined as follows. Suppose
    we have a loss function L defined for parameters x. For an initial set of parameters,
    the loss is calculated as L( x 0). Gradient descent proceeds iteratively to minimize
    the loss function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: L(x n+1) < L( x n)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'To update the parameters, *we take a step in the direction of the negative
    gradient of* L. We can specify the gradient descent update rule as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: x n+1 = x n − γ n ∇ L( x n)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Here, γ n is the **learning rate**, which defines the step size, and ∇ L(x n)
    is the gradient of L at x n.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph in *Figure 2**.3* illustrates the gradient descent process:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Graph showing the gradient descent process to find the minimum
    of a function](img/B16690_02_3.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Graph showing the gradient descent process to find the minimum
    of a function
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing an appropriate learning rate is critical to gradient descent’s success.
    If the learning rate is too low, the optimization will be very slow, potentially
    not reaching a minimum in the allowed number of iterations. A small learning rate
    could also lead to the process getting stuck in a local minimum: the step size
    being too small to escape. Conversely, suppose the learning rate is too large.
    In that case, we could step over the minimum and miss it entirely or get stuck
    oscillating around a minimum (constantly jumping back and forth but never descending
    to the optimal value).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we understand how gradient descent works, we can see how it is applied
    in gradient boosting. We will work through the entire gradient-boosting algorithm
    in detail at the hand of a small example. In our example, we’ll use a regression
    tree, as it is a bit easier to understand than the case for classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-boosting algorithm
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The gradient-boosting algorithm is defined as follows, where M is the number
    of boosted trees [3]:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Given training data {(x i, y i)} i=1 n , containing n training samples (defined
    by features x i and target y i) and a differentiable loss function L(y i, F(x)),
    where F(x) are the predictions from model F.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the model with a constant prediction value F 0(X) = argmin γ ∑ i=1 n  L(
    y i, γ)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For m = 1 to M:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute pseudo residuals r im = − [∂ L(y i, F(x i)) _ ∂ F(x i) ] F(X)=F m−1(X)
    for i = 1, … , n
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a regression tree to the r im values and create terminal regions R jm for
    j = 1…J m
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For j = 1…J m compute γ jm = argmin γ ∑ x i∈R ij L( y i, F m−1(x i) + γ)
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Update F m(x) = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm)
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Result: F M(x)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although the algorithm and especially the mathematics might look intimidating,
    it is practically much more straightforward than it appears. We’ll go through
    the algorithm step by step. Consider the following toy dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **Fasting** **Blood Sugar** | **Waist Circumference** | **BMI**
    | **LDL Cholesterol** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Example dataset consisting of a patient’s physical measurements
    and measured low-density lipoprotein (LDL) cholesterol
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Given the physical measurements, we aim to predict a patient’s LDL cholesterol.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding table defines our training data {(x i, y i)} i=1 n , where x
    is the features (blood sugar, waist circumference, BMI) and y is the target: LDL
    cholesterol.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a differentiable loss function, and to simplify some of the mathematical
    derivations in this example, we choose the following loss function, which is similar
    to the MSE function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: L =  1 _ 2  ∑ i=0 n (y i − γ i) 2
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We now work through each algorithm step in detail to see how the gradient-boosted
    tree is produced.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to find F 0(x) = argmin γ ∑ i=1 n  L( y i, γ), where y i is
    our target value and γ is our initial predicted value. *Our initial prediction
    is constant and is simply the average of the target values*. But let’s see why.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for F 0(x) is stating that we need to find a value for γ that
    minimizes our loss function. To find the minimum, we take the derivative of the
    loss function with respect to gamma:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: ∂ L _ ∂ γ  =  2 _ 2 (∑ i=0 n (y i − γ)) × − 1
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, set it to 0 and solve the following equation:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: − (∑ i=0 n (y i − γ)) = 0
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: −  1 _ n  ∑ i=0 n y i+ γ = 0
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: γ =  1 _ n  ∑ i=0 n y i
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The equation simplifies to calculating the average of the target values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating our table with the predictions, we have the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **F.** **Blood Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **PredictionF 0(x)** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 | 125 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 | 125 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: Table 2.3 – Our initial prediction of LDL cholesterol predictions ( F 0(x))
    for each patient is constant
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the following M times, where M is the number of trees we choose to
    build.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to calculate the pseudo residuals r im = − [∂ L(y i, F(x i)) _ ∂
    F(x i) ] F(X)=F m−1(X). This equation for r im is stating that we use the negative
    of the partial derivative of the loss function, with respect to the predictions,
    to calculate the pseudo residuals. *This portion of the gradient-boosting algorithm
    relates to gradient descent: we are taking the negative of the gradient to minimize
    the residuals*. Fortunately, we have already calculated this derivative:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: − [ ∂ L(y i, F(x i)) _ ∂ F(x i) ] = −  ∂ _ ∂ F(x i) (  1 _ 2  (y i − F(x i)) 2)
    =  2 _ 2 (y i− F(x i)) = ( y i − F(x i))
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, F(x i) is the predicted value. In other words, *the equation simplifies
    the difference between the target and predicted values*. We can add the residuals
    to the table, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **F. Blood****Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **Prediction** **F 0(x)** | **Residuals****for** **F 0(x)** |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 | 45 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 | 125 | -35 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 | 125 | -12 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: Table 2.4 – Based on our initial prediction, we can calculate the residuals
    for each patient, as shown in the Residuals for F 0(x) column
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is straightforward: we build a regression tree to predict the
    residuals. We do not use the predictions of the regression tree directly. Instead,
    we use the *terminal regions* to calculate our updated prediction. The terminal
    regions refer to the tree’s leaf nodes.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we assume the following simple regression tree is built:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Regression tree predicting the residuals](img/B16690_02_4.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Regression tree predicting the residuals
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: With our regression tree built and our leaf nodes defined, we can proceed with
    the next step. We need to compute γ jm that minimizes our loss function, taking
    into account the previous predictions as per γ jm = argmin γ ∑ x i∈R ij L( y i,
    F m−1(x i) + γ). This is almost precisely what we did in *step 1*, where we showed
    that the equation simplifies taking the average of the predicted values due to
    our choice of the loss function. *Here, that means taking the average of the residuals
    in each leaf node*. Therefore, we have γ 1,1 = − 35 − 12 _ 2  = − 23.5 and γ 2,1
    = 45 _ 1  = 45.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can now calculate our next prediction, F 1(x), defined by: F m(x)
    = F m−1(x) + ν∑ j=1 J m  γ jm I(x ∈ R jm),     meaning our next prediction consists of the previous prediction plus the γ values
    calculated in *step 2.3*, weighed by a learning rate ν. The summation here means
    that should a sample be part of multiple leaf nodes, we take the sum of the gamma
    values. Let’s calculate F 1(x) for the first sample in our dataset, using a learning
    rate of 0.1\. According to the regression tree from *step 2.2*, our sample (which
    has a BMI > 26) maps to γ 2,1\. Since it’s only mapping to a single leaf, we don’t
    need the summation part of the equation. Therefore, the equation looks like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: F 1(x) = F 0(x) + ν γ 2,1 = 125 + 0.1(45) = 129.5
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, our prediction has improved in the direction of our target values.
    Doing the same for the other samples, we have the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '| **Gender** | **F. Blood****Sugar** | **W. Circum.** | **BMI** | **LDL Cholesterol**
    | **Prediction** **F 0(x)** | **Residuals****for** **F 0(x)** | **Prediction**
    **F 1(x)** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| Male | 105 | 110 | 29.3 | 170 | 125 | 45 | 129.5 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| Female | 85 | 80 | 21 | 90 | 125 | -35 | 122,65 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Male | 95 | 93 | 26 | 113 | 125 | -12 | 122,65 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: Table 2.5 – Following steps 2.1 to 2.4, we calculate a new prediction, F 1(x),
    based on the initial prediction and the residuals
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of the learning rate is to limit the impact each tree might have
    on the overall prediction: by improving our prediction with small steps, we end
    up with an overall more accurate model.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2* is then repeated until we have a final prediction F M(x).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our gradient-boosting ensemble consists of a weighted (by the learning
    rate) summation of predictions made by a series of regression trees, each predicting
    the pseudo residuals (the error gradient, with respect to previous prediction)
    of previous predictions, thereby minimizing the error of previous predictions
    to produce an accurate final prediction.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting for classification
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our explanation of gradient boosting given previously used a regression problem
    as an example. We will not be going through a detailed example for classification
    as the algorithm is the same. However, instead of working with continuous predicted
    values, we use the same techniques as logistic regression [(https://en.wikipedia.org/wiki/Logistic_regressi](https://en.wikipedia.org/wiki/Logistic_regression)on).
    The individual trees, therefore, predict the probabilities of a sample belonging
    to the class. The probabilities are calculated by taking the log odds of a sample
    and converting them to a probability using the logistic function, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: p(x) =  1 _ 1 + e −(x−μ)/s
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudo residuals are calculated as the difference between the observed
    value (1 or 0 for the class) and the predicted value (the probability from the
    logistic function). A final difference is the loss function. Instead of a function
    such as MSE, we can use cross-entropy for loss, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: H p(q) = −  1 _ N  ∑ i=1 N y ilog(p(y i)) + (1 − y i)log(1 − p(y i))
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-boosted decision tree hyperparameters
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the parameters for standard decision tree training, the following
    new hyperparameters are available in scikit-learn specific to gradient-boosted
    trees:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: Controls the number of trees in the ensemble. Generally, more
    trees are better. However, a point of diminishing returns is often reached, and
    overfitting occurs when there are too many trees.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: Controls the contribution of each tree to the ensemble. Lower
    learning rates lead to longer training times and may require more trees to be
    built (larger values for `n_estimators`). Setting `learning_rate` to a very large
    value may cause the optimization to miss optimum points and must be combined with
    fewer trees.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complete list of scikit-learn gradient-boosting hyperparameters can be found
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.xhtml).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting in scikit-learn
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The details of gradient boosting are mathematical and complicated; fortunately,
    the algorithm is as accessible as any other via scikit-learn. The following is
    an example of a `GradientBoostingClassifier` class in scikit-learn, again using
    the *Forest CoverType* dataset we used earlier in the chapter to train a random
    forest classifier.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The classifier is also imported from the `ensemble` package, like so:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We fetch and split the data as before and then fit the model, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Running the preceding code should produce an F1 score of 0.7119, a score that’s
    significantly worse than even a standard decision tree. We can spend time optimizing
    our hyperparameters to improve performance. However, there is a more significant
    issue. The previous code takes very long to execute—in the order of 45 minutes
    on our hardware—compared to ExtraTrees, which takes approximately 3 minutes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM addresses both issues we have with the gradient-boosted tree and builds
    a gradient-boosted tree with significantly better performance in a fraction of
    the time.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we’ll briefly cover an advanced algorithm related
    to gradient boosting: DART.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Advanced boosting algorithm – DART
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DART** is an extension of the standard GBDT algorithm discussed in the previous
    section [4]. DART employs **dropouts**, a technique from **deep learning** (**DL**),
    to avoid overfitting by the decision tree ensemble. The extension is straightforward
    and consists of two parts. First, when fitting the next prediction tree, M n+1(x),
    which consists of the scaled sum of all previous trees M n…M 1, a random subset
    of the previous trees is instead used, with other trees dropped from the sum.
    The p drop parameter controls the probability of a previous tree being included.
    The second part of the DART algorithm is to apply additional scaling of the contribution
    of the new tree. Let k be the number of trees dropped when the new tree, M n+1,
    was calculated. Since M n+1 was calculated without the contribution of those k
    trees when updating our prediction, F n+1, which includes all trees, the prediction
    overshoots. Therefore, the new tree is scaled by a factor of 1 _ k  to compensate.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: DART has been shown to outperform standard GBDTs while also significantly improving
    overfitting.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn does not implement DART for GBDTs, but DART is incorporated in
    LightGBM.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In conclusion, this chapter looked at the two most common methods of ensemble
    learning for decision trees: bagging and boosting. We looked at the Random Forests
    and ExtraTrees algorithms, which build decision tree ensembles using bagging.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also gave a detailed overview of boosting in decision trees by
    going through the GBDT algorithm step by step, illustrating how gradient boosting
    is applied. We covered practical examples of random forests, ExtraTrees, and GBDTs
    for scikit-learn.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how dropouts can be applied to GBDTs with the DART algorithm.
    We now thoroughly understand decision tree ensemble techniques and are ready to
    dive deep into LightGBM.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter introduces the LightGBM library in detail, both the theoretical
    advancements made by the library and the practical application thereof. We will
    also look at using LightGBM with Python to solve ML problems.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *L. Breiman, “Random forests,” Machine learning, vol. 45, p.* *5-32,
    2001.* |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *P. Geurts, D. Ernst and L. Wehenkel, “Extremely randomized trees,”
    Machine learning, vol. 63, p.* *3-42, 2006.* |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| *[**3]* | *J. H. Friedman, “Greedy function approximation: a gradient boosting
    machine,” The Annals of Statistics, p.* *1189-1232, 2001.* |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| *[**4]* | *R. K. Vinayak and R. Gilad-Bachrach, “Dart: Dropouts meet multiple
    additive regression trees,” in Artificial Intelligence and* *Statistics, 2015.*
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
