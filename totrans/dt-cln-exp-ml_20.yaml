- en: '*Chapter 15*: Principal Component Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimension reduction is one of the more important concepts/strategies in machine
    learning. It is sometimes equated with feature selection, but that is too narrow
    a view of dimension reduction. Our models often have to deal with an excess of
    features, some of which are capturing the same information. Not addressing the
    issue substantially increases the risk of overfitting or of unstable results.
    But dropping some of our features is not the only tool in our toolbox here. Feature
    extraction strategies, such as **principal component analysis** (**PCA**), can
    often yield good results.
  prefs: []
  type: TYPE_NORMAL
- en: We can use PCA to reduce the dimensions (the number of features) of our dataset
    without losing significant predictive power. The number of principal components
    necessary to capture most of the variance in the data is typically less than the
    number of features, often much less.
  prefs: []
  type: TYPE_NORMAL
- en: These components can be used in our regression or classification models rather
    than the initial features. Not only can this speed up how quickly our model learns,
    but it may decrease the variance of our estimates. The key disadvantage of this
    feature extraction strategy is that the new features will usually be more difficult
    to interpret. PCA is also not a good choice when we have categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: We will develop our understanding of how PCA works by first examining how each
    component is constructed. We will construct a PCA, interpret the results, and
    then use those results in a classification model. Finally, we will use kernels
    to improve PCA when our components might not be linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will explore the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction with PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using kernels with PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this
    chapter. All code was tested with scikit-learn versions 0.24.2 and 1.0.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be downloaded from the GitHub repository: [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA produces multiple linear combinations of features and each linear combination
    is a component. It identifies a component that captures the largest amount of
    variance, and a second component that captures the largest amount of remaining
    variance, and then a third component, and so on until a stopping point we specify
    is reached. The stopping point can be based on the number of components, the percent
    of the variation explained, or domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: One very useful characteristic of principal components is that they are mutually
    orthogonal. This means that they are uncorrelated, which is really good news for
    modeling. *Figure 15.1* shows two components constructed from the features x1
    and x2\. The maximum variance is captured with *PC1*, the maximum remaining variance
    with *PC2*. (The data points in the figure are made up.) Notice that the two vectors
    are orthogonal (perpendicular).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – An illustration of PCA with two features ](img/B17978_15_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – An illustration of PCA with two features
  prefs: []
  type: TYPE_NORMAL
- en: Those of you who have done a lot of factor analysis probably get the general
    idea, even if this is your first time exploring PCA. Principal components are
    not very different from factors, though there are some conceptual and mathematical
    differences. With PCA, all of the variance is analyzed. Only the shared variance
    between variables is analyzed with factor analysis. In factor analysis, the unobserved
    factors are understood as having *caused* the observed variables. No assumptions
    about underlying, unobserved forces need to be made with PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how is this bit of computational magic done? Principal components can be
    calculated by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardize your data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the covariance matrix of your variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvectors and eigenvalues of the covariance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort the eigenvectors by eigenvalues in descending order. The first eigenvector
    is principal component 1, the second is principal component 2, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is not necessary to completely understand these steps to understand the discussion
    in the rest of this chapter. We will get scikit-learn to do this work for us.
    Still, it might improve your intuition if you computed the covariance matrix of
    a very small subset of data (two or three columns and just a few rows) and then
    did the eigendecomposition of that matrix. A somewhat easier way to experiment
    with constructing components, while still being illustrative, is to use the NumPy
    linear algebra functions (`numpy.linalg`). The key point here is how computationally
    straightforward it is to derive the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is used for many machine learning tasks. It can be used to resize images,
    to analyze financial data, or for recommender systems. Essentially, it may be
    a good choice for any application where there are a large number of features and
    many of them are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: Some of you no doubt noticed that I slipped in, without remarking on it, that
    PCA constructs *linear combinations* of features. What do we do when linear separability
    is not feasible, as we encountered with support vector machines? Well, it turns
    out that the kernel trick that we relied on with support vector machines also
    works with PCA. We will explore how to implement kernel PCA in this chapter. However,
    we will start with a relatively straightforward PCA example.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA can be used for dimension reduction in preparation for a model we will run
    subsequently. Although PCA is not, strictly speaking, a feature selection tool,
    we can run it in pretty much the same way we ran the wrapper feature selection
    methods in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection*.
    After some preprocessing (such as handling outliers), we generate the components,
    which we can then use as our new features. Sometimes we do not actually use these
    components in a model. Rather, we generate them mainly to help us visualize our
    data better.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the use of PCA, we will work with data on **National Basketball
    Association** (**NBA**) games. The dataset has statistics from each NBA game from
    the 2017/2018 season through the 2020/2021 season. This includes the home team;
    whether the home team won; the visiting team; shooting percentages for visiting
    and home teams; turnovers, rebounds, and assists by both teams; and a number of
    other measures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: NBA game data is available for download for the public at [https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball).
    This dataset has game data starting with the 1946/1947 NBA season. It uses the
    `nba_api` to pull stats from nba.com. That API is available at [https://github.com/swar/nba_api](https://github.com/swar/nba_api).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use PCA in a model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the required libraries. You have seen all of these in the
    previous chapters except for scikit-learn’s `PCA` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the NBA data and do a little cleaning. A few instances do not
    have values for whether the home team won or lost, `WL_HOME`, so we remove them.
    `WL_HOME` will be our target. We will try to model it later, after we have constructed
    our components. Notice that the home team wins a majority of the time, but the
    class imbalance is not bad:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should look at some descriptive statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output. There are no missing values, but our features
    have very different ranges. We will need to do some scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also examine how our features are correlated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – Heat map of NBA features](img/B17978_15_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – Heat map of NBA features
  prefs: []
  type: TYPE_NORMAL
- en: A number of features are significantly positively or negatively correlated.
    For example, the field goal (shooting) percentage of the home team (`FG_PCT_HOME`)
    and three-point field goal percentage of the home team (`FG3_PCT_HOME`) are positively
    correlated, not surprisingly. Also, rebounds of the home team (`REB_HOME`) and
    defensive rebounds of the home team (`DREB_HOME`) are likely too closely correlated
    for any model to disentangle their impact.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset might be a good candidate for PCA. Although some features are highly
    correlated, we will still lose information by dropping some. PCA at least offers
    the possibility of dealing with the correlation without losing that information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we create training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are ready now to create the components. We, somewhat arbitrarily, indicate
    that we want seven components. (Later, we will use hyperparameter tuning to choose
    the number of components.) We set up our pipeline to do some preprocessing before
    running the PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now use the `components_` attribute of the `pca` object. This returns
    the scores of all 23 features on each of the seven components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – Principal components of NBA features ](img/B17978_15_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – Principal components of NBA features
  prefs: []
  type: TYPE_NORMAL
- en: Each feature accounts for some portion of the variance with each component.
    (If for each component, you square each of the 23 scores and then sum the squares,
    you get a total of 1.) If you want to understand which features really drive a
    component, look for the ones with the largest absolute value. For component 1,
    the field goal percentage of the home team (`FG_PCT_HOME`) is most important,
    followed by the number of rebounds of the away team (`REB_AWAY`).
  prefs: []
  type: TYPE_NORMAL
- en: Recall from our discussion at the beginning of this chapter that each component
    attempts to capture the variance that remains after the previous component or
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s show the five most important features for the first three components.
    The first component seems to be largely about the field goal percentage of the
    home team and the rebounding of each team. The second component does not seem
    very different, but the third one is driven by free throws made and attempted
    (`FTM_HOME` and `FTA_HOME`) and turnovers (`TOV_HOME` and `TOV_AWAY`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use the `explained_variance_ratio_` attribute of the `pca` object to
    examine how much of the variance is captured by each component. The first component
    explains 14.5% of the variance of the features. The second component explains
    another 13.4%. If we use NumPy’s `cumsum` method, we can see that the seven components
    explain about 65% of the variance altogether.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, there is still a fair bit of variance out there. We might want to use more
    components for any model we build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can plot the first two principal components to see how well they can separate
    home team wins and losses. We can use the `transform` method of our pipeline to
    create a DataFrame with the principal components and join that with the DataFrame
    for the target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use the handy `hue` attribute of Seaborn’s `scatterplot` to display wins
    and losses. The first two principal components do an okay job of separating wins
    and losses, despite together only accounting for about 28% of the variance in
    our features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – Scatterplot of wins and losses by first and second principal
    components ](img/B17978_15_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – Scatterplot of wins and losses by first and second principal components
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use principal components to predict whether the home team wins. We just
    add a logistic regression to our pipeline to do that. We also do a grid search
    to find the best hyperparameter values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now look at the best parameters and score. As we suspected from an earlier
    step, the grid search suggests that our logistic regression model does better
    with more components. We get a very high score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We discuss the hyperparameter *C* in detail in [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126)*,
    Logistic Regression*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This section demonstrated how we can generate principal components from our
    dataset and how to interpret those components. We also looked at how to use principal
    components in a model, rather than the initial features. But we assumed that the
    principal components can be well described as linear combinations of features.
    This is often not the case. In the next section, we will use kernel PCA to handle
    nonlinear relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Using kernels with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With some data, it is not possible to construct principal components that are
    linearly separable. This may not actually be easy to visualize in advance of our
    modeling. Fortunately, there are tools we can use to determine the kernel that
    will yield the best results, including a linear kernel. Kernel PCA with a linear
    kernel should perform similarly to standard PCA.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use kernel PCA for feature extraction with data on
    labor force participation rates, educational attainment, teenage birth frequency,
    and participation in politics by gender at the country level.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This dataset on gender-based differences in educational and labor force outcomes
    is made available for public use by the United Nations Development Program at
    [https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development).
    There is one record per country with aggregate employment, income, and education
    data by gender for 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start building the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will import the same libraries we have been using plus scikit-learn’s `KernelPCA`
    module. We will also import the `RandomForestRegressor` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the data on educational and labor force outcomes by gender. We construct
    series for the ratio of female to male incomes, the years of education ratio,
    the labor force participation ratio, and the human development index ratio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at some descriptive statistics. There are a few missing values,
    particularly for `genderinequality` and `humandevratio`. Some features have much
    larger ranges than others:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should also look at some correlations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.5 – Correlation matrix of NBA games data ](img/B17978_15_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.5 – Correlation matrix of NBA games data
  prefs: []
  type: TYPE_NORMAL
- en: '`humandevratio` and `educratio` are highly correlated, as are `genderinequality`
    and `adolescentbirthrate`. We can see that `educratio` and `maternalmortality`
    are highly negatively correlated. It would be hard to build a well-performing
    model with all of these features given their high correlation. However, we might
    be able to reduce dimensions with kernel PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are now ready to instantiate the `KernelPCA` and `RandomForestRegressor`
    objects. We add both to a pipeline. We also create a dictionary with our hyperparameters
    for the kernel PCA and the random forest regressor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dictionary has a range of hyperparameter values for the number of components,
    gamma, and the kernel to use with the kernel PCA. For those kernels that do not
    use gamma, those values are ignored. Notice that one option for kernel is the
    linear kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discuss gamma in more detail in [*Chapter 8*](B17978_08_ePub.xhtml#_idTextAnchor106)*,
    Support Vector Regression*, and [*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152)*,
    Support Vector Machine Classification*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s do a randomized grid search with these hyperparameter values. The
    kernel for the PCA that gives us the best performance with the random forest regressor
    is polynomial. We get a good square for mean squared error, about 10% of the size
    of the mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at other top-performing models. A model with an `rbf` kernel and
    one with a sigmoid kernel do nearly as well. The second and third best-performing
    models have more principal components than the best-performing model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Kernel PCA is a relatively easy-to-implement dimension reduction option. It
    is most useful when we have a number of highly correlated features that might
    not be linearly separable, and interpretation of predictions is not important.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored principal component analysis, including how it works and
    when we might want to use it. We learned how to examine the components created
    from PCA, including how each feature contributes to each component, and how much
    of the variance is explained. We went over how to visualize components and how
    to use components in subsequent analysis. We also examined how to use kernels
    for PCA and when that might give us better results.
  prefs: []
  type: TYPE_NORMAL
- en: We explore another unsupervised learning technique in the next chapter, k-means
    clustering.
  prefs: []
  type: TYPE_NORMAL
