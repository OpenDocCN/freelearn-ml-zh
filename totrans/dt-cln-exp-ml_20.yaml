- en: '*Chapter 15*: Principal Component Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第15章*: 主成分分析'
- en: Dimension reduction is one of the more important concepts/strategies in machine
    learning. It is sometimes equated with feature selection, but that is too narrow
    a view of dimension reduction. Our models often have to deal with an excess of
    features, some of which are capturing the same information. Not addressing the
    issue substantially increases the risk of overfitting or of unstable results.
    But dropping some of our features is not the only tool in our toolbox here. Feature
    extraction strategies, such as **principal component analysis** (**PCA**), can
    often yield good results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低是机器学习中的重要概念/策略之一。有时它与特征选择等同，但这是对维度降低过于狭隘的看法。我们的模型通常必须处理过多的特征，其中一些特征正在捕获相同的信息。不解决这一问题会大大增加过拟合或不稳定结果的风险。但放弃我们的一些特征并不是我们工具箱中唯一的工具。特征提取策略，如**主成分分析**（**PCA**），通常可以产生良好的结果。
- en: We can use PCA to reduce the dimensions (the number of features) of our dataset
    without losing significant predictive power. The number of principal components
    necessary to capture most of the variance in the data is typically less than the
    number of features, often much less.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PCA在不损失显著预测能力的情况下降低数据集的维度（特征数量）。通常，捕捉数据中大部分方差所需的主成分数量小于特征数量，通常要少得多。
- en: These components can be used in our regression or classification models rather
    than the initial features. Not only can this speed up how quickly our model learns,
    but it may decrease the variance of our estimates. The key disadvantage of this
    feature extraction strategy is that the new features will usually be more difficult
    to interpret. PCA is also not a good choice when we have categorical features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成分可以用在我们的回归或分类模型中，而不是初始特征。这不仅能够加快我们的模型学习速度，还可能降低我们估计的方差。这种特征提取策略的关键缺点是，新特征通常更难以解释。当我们有分类特征时，PCA也不是一个好的选择。
- en: We will develop our understanding of how PCA works by first examining how each
    component is constructed. We will construct a PCA, interpret the results, and
    then use those results in a classification model. Finally, we will use kernels
    to improve PCA when our components might not be linearly separable.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过首先检查每个成分是如何构建的来发展我们对PCA工作原理的理解。我们将构建一个PCA，解释结果，然后在一个分类模型中使用这些结果。最后，当我们的成分可能不是线性可分时，我们将使用核来改进PCA。
- en: 'Specifically, we will explore the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中我们将探讨以下主题：
- en: Key concepts of PCA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA的关键概念
- en: Feature extraction with PCA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA进行特征提取
- en: Using kernels with PCA
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA的核
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this
    chapter. All code was tested with scikit-learn versions 0.24.2 and 1.0.2.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要使用pandas、NumPy和scikit-learn库。所有代码都使用scikit-learn版本0.24.2和1.0.2进行了测试。
- en: 'The code for this chapter can be downloaded from the GitHub repository: [https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以从GitHub仓库下载：[https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)。
- en: Key concepts of PCA
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA的关键概念
- en: PCA produces multiple linear combinations of features and each linear combination
    is a component. It identifies a component that captures the largest amount of
    variance, and a second component that captures the largest amount of remaining
    variance, and then a third component, and so on until a stopping point we specify
    is reached. The stopping point can be based on the number of components, the percent
    of the variation explained, or domain knowledge.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PCA产生多个特征组合，每个组合都是一个成分。它识别出一个成分，该成分捕捉到最大的方差量，然后是一个第二个成分，该成分捕捉到剩余的最大方差量，接着是第三个成分，以此类推，直到达到我们指定的停止点。这个停止点可以是基于成分的数量、解释的变异百分比或领域知识。
- en: One very useful characteristic of principal components is that they are mutually
    orthogonal. This means that they are uncorrelated, which is really good news for
    modeling. *Figure 15.1* shows two components constructed from the features x1
    and x2\. The maximum variance is captured with *PC1*, the maximum remaining variance
    with *PC2*. (The data points in the figure are made up.) Notice that the two vectors
    are orthogonal (perpendicular).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分的一个非常有用的特性是它们是相互正交的。这意味着它们是不相关的，这对建模来说是个好消息。*图15.1*展示了由特征x1和x2构建的两个成分。最大方差由*PC1*捕获，剩余的最大方差由*PC2*捕获。（图中的数据点是虚构的。）请注意，这两个向量是正交的（垂直的）。
- en: '![Figure 15.1 – An illustration of PCA with two features ](img/B17978_15_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图15.1 – 具有两个特征的PCA示意图](img/B17978_15_001.jpg)'
- en: Figure 15.1 – An illustration of PCA with two features
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – 具有两个特征的PCA示意图
- en: Those of you who have done a lot of factor analysis probably get the general
    idea, even if this is your first time exploring PCA. Principal components are
    not very different from factors, though there are some conceptual and mathematical
    differences. With PCA, all of the variance is analyzed. Only the shared variance
    between variables is analyzed with factor analysis. In factor analysis, the unobserved
    factors are understood as having *caused* the observed variables. No assumptions
    about underlying, unobserved forces need to be made with PCA.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那些做过大量因子分析的人可能已经得到了一个大致的概念，即使这是你们第一次探索PCA。主成分与因子并没有很大差别，尽管它们在概念和数学上存在一些差异。在PCA中，分析的是所有的方差。而因子分析只分析变量之间的共享方差。在因子分析中，未观察到的因子被认为是**导致**了观察到的变量。在PCA中不需要对潜在的、未观察到的力量做出任何假设。
- en: 'So, how is this bit of computational magic done? Principal components can be
    calculated by following these steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个计算上的魔法是如何实现的呢？主成分可以通过以下步骤计算得出：
- en: Standardize your data.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化你的数据。
- en: Calculate the covariance matrix of your variables.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算你的变量的协方差矩阵。
- en: Calculate the eigenvectors and eigenvalues of the covariance matrix.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征向量和特征值。
- en: Sort the eigenvectors by eigenvalues in descending order. The first eigenvector
    is principal component 1, the second is principal component 2, and so on.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按特征值降序排列特征向量。第一个特征向量是主成分1，第二个是主成分2，依此类推。
- en: It is not necessary to completely understand these steps to understand the discussion
    in the rest of this chapter. We will get scikit-learn to do this work for us.
    Still, it might improve your intuition if you computed the covariance matrix of
    a very small subset of data (two or three columns and just a few rows) and then
    did the eigendecomposition of that matrix. A somewhat easier way to experiment
    with constructing components, while still being illustrative, is to use the NumPy
    linear algebra functions (`numpy.linalg`). The key point here is how computationally
    straightforward it is to derive the principal components.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 完全理解这些步骤并不是理解本章其余部分讨论的必要条件。我们将让scikit-learn为我们完成这项工作。尽管如此，如果你计算了一个非常小的数据子集（两三列和几行）的协方差矩阵，然后计算该矩阵的特征分解，这可能会提高你的直觉。一个相对简单的方法来实验构建成分，同时仍然具有说明性，是使用NumPy线性代数函数（`numpy.linalg`）。这里的关键点是推导主成分的计算有多么简单。
- en: PCA is used for many machine learning tasks. It can be used to resize images,
    to analyze financial data, or for recommender systems. Essentially, it may be
    a good choice for any application where there are a large number of features and
    many of them are correlated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PCA被用于许多机器学习任务。它可以用于调整图像大小，分析金融数据，或用于推荐系统。本质上，它可能是一个在有许多特征且许多特征相关的情况下适用的好选择。
- en: Some of you no doubt noticed that I slipped in, without remarking on it, that
    PCA constructs *linear combinations* of features. What do we do when linear separability
    is not feasible, as we encountered with support vector machines? Well, it turns
    out that the kernel trick that we relied on with support vector machines also
    works with PCA. We will explore how to implement kernel PCA in this chapter. However,
    we will start with a relatively straightforward PCA example.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人无疑已经注意到，我在没有特别指出的情况下，提到了PCA构建**特征线性组合**。当线性可分性不可行时，比如我们在支持向量机中遇到的情况，我们该怎么办？好吧，结果是我们依赖的支持向量机的核技巧也适用于PCA。在本章中，我们将探讨如何实现核PCA。然而，我们将从一个相对简单的PCA示例开始。
- en: Feature extraction with PCA
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA进行特征提取
- en: PCA can be used for dimension reduction in preparation for a model we will run
    subsequently. Although PCA is not, strictly speaking, a feature selection tool,
    we can run it in pretty much the same way we ran the wrapper feature selection
    methods in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection*.
    After some preprocessing (such as handling outliers), we generate the components,
    which we can then use as our new features. Sometimes we do not actually use these
    components in a model. Rather, we generate them mainly to help us visualize our
    data better.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PCA可用于在准备随后运行的模型之前进行降维。尽管PCA严格来说不是一个特征选择工具，但我们可以以与我们在[*第5章*](B17978_05_ePub.xhtml#_idTextAnchor058)中运行的包装特征选择方法相同的方式运行它，即*特征选择*。经过一些预处理（如处理异常值）后，我们生成组件，然后我们可以将它们用作我们的新特征。有时我们实际上并不在模型中使用这些组件。相反，我们主要生成它们来帮助我们更好地可视化数据。
- en: To illustrate the use of PCA, we will work with data on **National Basketball
    Association** (**NBA**) games. The dataset has statistics from each NBA game from
    the 2017/2018 season through the 2020/2021 season. This includes the home team;
    whether the home team won; the visiting team; shooting percentages for visiting
    and home teams; turnovers, rebounds, and assists by both teams; and a number of
    other measures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明PCA的使用，我们将与**国家篮球协会**（**NBA**）比赛的有关数据一起工作。该数据集包含了从2017/2018赛季到2020/2021赛季每场NBA比赛的统计数据。这包括主队；主队是否获胜；客队；客队和主队的投篮命中率；两队的失误、篮板和助攻；以及其他一些指标。
- en: Note
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: NBA game data is available for download for the public at [https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball).
    This dataset has game data starting with the 1946/1947 NBA season. It uses the
    `nba_api` to pull stats from nba.com. That API is available at [https://github.com/swar/nba_api](https://github.com/swar/nba_api).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: NBA比赛数据可在[https://www.kaggle.com/datasets/wyattowalsh/basketball](https://www.kaggle.com/datasets/wyattowalsh/basketball)供公众下载。该数据集从1946/1947赛季的NBA赛季开始。它使用`nba_api`从nba.com获取统计数据。该API可在[https://github.com/swar/nba_api](https://github.com/swar/nba_api)找到。
- en: 'Let’s use PCA in a model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在模型中使用PCA：
- en: 'We start by loading the required libraries. You have seen all of these in the
    previous chapters except for scikit-learn’s `PCA` module:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载所需的库。您在之前的章节中已经看到了所有这些库，除了scikit-learn的`PCA`模块：
- en: '[PRE0]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we load the NBA data and do a little cleaning. A few instances do not
    have values for whether the home team won or lost, `WL_HOME`, so we remove them.
    `WL_HOME` will be our target. We will try to model it later, after we have constructed
    our components. Notice that the home team wins a majority of the time, but the
    class imbalance is not bad:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载NBA数据并进行一些清理。有几个实例没有主队是否获胜的值，`WL_HOME`，所以我们删除了它们。`WL_HOME`将是我们的目标。我们将在构建我们的组件之后尝试对其进行建模。请注意，主队大多数时候都会获胜，但类别不平衡并不严重：
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We should look at some descriptive statistics:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该查看一些描述性统计：
- en: '[PRE2]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following output. There are no missing values, but our features
    have very different ranges. We will need to do some scaling:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出。没有缺失值，但我们的特征范围差异很大。我们需要进行一些缩放：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s also examine how our features are correlated:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也检查我们的特征是如何相关的：
- en: '[PRE4]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces the following plot:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 15.2 – Heat map of NBA features](img/B17978_15_002.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图15.2 – NBA特征的散点图](img/B17978_15_002.jpg)'
- en: Figure 15.2 – Heat map of NBA features
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 – NBA特征的散点图
- en: A number of features are significantly positively or negatively correlated.
    For example, the field goal (shooting) percentage of the home team (`FG_PCT_HOME`)
    and three-point field goal percentage of the home team (`FG3_PCT_HOME`) are positively
    correlated, not surprisingly. Also, rebounds of the home team (`REB_HOME`) and
    defensive rebounds of the home team (`DREB_HOME`) are likely too closely correlated
    for any model to disentangle their impact.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多特征与正或负相关显著。例如，主队的投篮命中率（射门）(`FG_PCT_HOME`)和主队的3分投篮命中率(`FG3_PCT_HOME`)正相关，这并不令人惊讶。此外，主队的篮板(`REB_HOME`)和防守篮板(`DREB_HOME`)可能过于紧密地相关，以至于任何模型都无法分离它们的影响。
- en: This dataset might be a good candidate for PCA. Although some features are highly
    correlated, we will still lose information by dropping some. PCA at least offers
    the possibility of dealing with the correlation without losing that information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集可能是PCA的良好候选者。尽管一些特征高度相关，但我们仍然会通过删除一些特征而丢失信息。PCA至少提供了处理相关性而不丢失这些信息的机会。
- en: 'Now we create training and testing DataFrames:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建训练和测试数据框：
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are ready now to create the components. We, somewhat arbitrarily, indicate
    that we want seven components. (Later, we will use hyperparameter tuning to choose
    the number of components.) We set up our pipeline to do some preprocessing before
    running the PCA:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备创建这些成分。我们有些任意地指出我们想要七个成分。（稍后，我们将使用超参数调整来选择成分的数量。）我们在运行PCA之前设置我们的管道进行一些预处理：
- en: '[PRE6]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now use the `components_` attribute of the `pca` object. This returns
    the scores of all 23 features on each of the seven components:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`pca`对象的`components_`属性。这返回了所有23个特征在每个七个成分上的得分：
- en: '[PRE7]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the following spreadsheet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下电子表格：
- en: '![Figure 15.3 – Principal components of NBA features ](img/B17978_15_003.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图15.3 – NBA特征的成分图](img/B17978_15_003.jpg)'
- en: Figure 15.3 – Principal components of NBA features
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 – NBA特征的成分图
- en: Each feature accounts for some portion of the variance with each component.
    (If for each component, you square each of the 23 scores and then sum the squares,
    you get a total of 1.) If you want to understand which features really drive a
    component, look for the ones with the largest absolute value. For component 1,
    the field goal percentage of the home team (`FG_PCT_HOME`) is most important,
    followed by the number of rebounds of the away team (`REB_AWAY`).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征在每个成分中解释了部分方差。（如果对每个成分，你对23个得分中的每一个进行平方然后求和，你得到总和为1。）如果你想了解哪些特征真正驱动了成分，寻找那些具有最大绝对值的特征。对于成分1，主队的投篮命中率（`FG_PCT_HOME`）是最重要的，其次是客队的篮板球数（`REB_AWAY`）。
- en: Recall from our discussion at the beginning of this chapter that each component
    attempts to capture the variance that remains after the previous component or
    components.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下本章开头我们讨论的内容，每个成分试图捕捉在之前成分或成分之后剩余的方差。
- en: 'Let’s show the five most important features for the first three components.
    The first component seems to be largely about the field goal percentage of the
    home team and the rebounding of each team. The second component does not seem
    very different, but the third one is driven by free throws made and attempted
    (`FTM_HOME` and `FTA_HOME`) and turnovers (`TOV_HOME` and `TOV_AWAY`):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们展示前三个成分最重要的五个特征。第一个成分似乎主要关于主队的投篮命中率以及每个队的篮板球。第二个成分看起来并没有太大区别，但第三个成分是由主队做出的投篮和尝试（`FTM_HOME`和`FTA_HOME`）以及失误（`TOV_HOME`和`TOV_AWAY`）驱动的：
- en: '[PRE8]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can use the `explained_variance_ratio_` attribute of the `pca` object to
    examine how much of the variance is captured by each component. The first component
    explains 14.5% of the variance of the features. The second component explains
    another 13.4%. If we use NumPy’s `cumsum` method, we can see that the seven components
    explain about 65% of the variance altogether.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`pca`对象的`explained_variance_ratio_`属性来检查每个成分捕获了多少方差。第一个成分解释了特征方差的14.5%。第二个成分解释了另一个13.4%。如果我们使用NumPy的`cumsum`方法，我们可以看到七个成分总共解释了约65%的方差。
- en: 'So, there is still a fair bit of variance out there. We might want to use more
    components for any model we build:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仍然存在相当多的方差。我们可能想要在构建任何模型时使用更多的成分：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can plot the first two principal components to see how well they can separate
    home team wins and losses. We can use the `transform` method of our pipeline to
    create a DataFrame with the principal components and join that with the DataFrame
    for the target.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将前两个主成分绘制出来，看看它们能多好地分离主队的胜负。我们可以使用管道的`transform`方法创建一个包含主成分的数据框，并将其与目标数据框连接起来。
- en: 'We use the handy `hue` attribute of Seaborn’s `scatterplot` to display wins
    and losses. The first two principal components do an okay job of separating wins
    and losses, despite together only accounting for about 28% of the variance in
    our features:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Seaborn的`scatterplot`的便捷`hue`属性来显示胜负情况。前两个主成分在仅占特征总方差约28%的情况下，还算不错地分离了胜负：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following plot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 15.4 – Scatterplot of wins and losses by first and second principal
    components ](img/B17978_15_004.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图15.4 – 第一和第二个主成分的胜负散点图](img/B17978_15_004.jpg)'
- en: Figure 15.4 – Scatterplot of wins and losses by first and second principal components
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 – 第一和第二个主成分的胜负散点图
- en: 'Let’s use principal components to predict whether the home team wins. We just
    add a logistic regression to our pipeline to do that. We also do a grid search
    to find the best hyperparameter values:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用主成分来预测主队是否会获胜。我们只需在我们的管道中添加一个逻辑回归即可。我们还进行网格搜索以找到最佳超参数值：
- en: '[PRE11]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can now look at the best parameters and score. As we suspected from an earlier
    step, the grid search suggests that our logistic regression model does better
    with more components. We get a very high score.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以查看最佳参数和得分。正如我们之前一步所怀疑的那样，网格搜索表明我们的逻辑回归模型在更多组件的情况下表现更好。我们得到了一个非常高的分数。
- en: 'We discuss the hyperparameter *C* in detail in [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126)*,
    Logistic Regression*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)*中详细讨论了超参数*C*，即逻辑回归：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This section demonstrated how we can generate principal components from our
    dataset and how to interpret those components. We also looked at how to use principal
    components in a model, rather than the initial features. But we assumed that the
    principal components can be well described as linear combinations of features.
    This is often not the case. In the next section, we will use kernel PCA to handle
    nonlinear relationships.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何从我们的数据集中生成主成分以及如何解释这些成分。我们还探讨了如何在模型中使用主成分而不是初始特征。但我们假设主成分可以很好地描述为特征的线性组合。这通常并不是情况。在下一节中，我们将使用核PCA来处理非线性关系。
- en: Using kernels with PCA
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA核
- en: With some data, it is not possible to construct principal components that are
    linearly separable. This may not actually be easy to visualize in advance of our
    modeling. Fortunately, there are tools we can use to determine the kernel that
    will yield the best results, including a linear kernel. Kernel PCA with a linear
    kernel should perform similarly to standard PCA.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些数据，无法构建出线性可分的主成分。在建模之前，这可能实际上不容易可视化。幸运的是，我们有工具可以使用来确定将产生最佳结果的核，包括线性核。使用线性核的核PCA应该与标准PCA的表现相似。
- en: In this section, we will use kernel PCA for feature extraction with data on
    labor force participation rates, educational attainment, teenage birth frequency,
    and participation in politics by gender at the country level.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用核PCA对劳动力参与率、教育成就、青少年出生频率以及国家层面的性别政治参与数据等特征进行特征提取。
- en: Note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset on gender-based differences in educational and labor force outcomes
    is made available for public use by the United Nations Development Program at
    [https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development).
    There is one record per country with aggregate employment, income, and education
    data by gender for 2015.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该性别在教育成果和劳动力成果方面的差异数据集由联合国开发计划署在[https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development)提供，供公众使用。每个国家有一个记录，包括2015年按性别汇总的就业、收入和教育数据。
- en: 'Let’s start building the model:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建模型：
- en: 'We will import the same libraries we have been using plus scikit-learn’s `KernelPCA`
    module. We will also import the `RandomForestRegressor` module:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将导入我们一直在使用的相同库以及scikit-learn的`KernelPCA`模块。我们还将导入`RandomForestRegressor`模块：
- en: '[PRE13]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We load the data on educational and labor force outcomes by gender. We construct
    series for the ratio of female to male incomes, the years of education ratio,
    the labor force participation ratio, and the human development index ratio:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据性别加载教育和劳动力成果数据。我们构建了女性与男性收入比、教育年限比、劳动力参与比和人类发展指数比的时间序列：
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s look at some descriptive statistics. There are a few missing values,
    particularly for `genderinequality` and `humandevratio`. Some features have much
    larger ranges than others:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看一些描述性统计。有一些缺失值，尤其是对于`genderinequality`和`humandevratio`。一些特征的范围比其他特征大得多：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We should also look at some correlations:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一些相关性：
- en: '[PRE16]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following plot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 15.5 – Correlation matrix of NBA games data ](img/B17978_15_005.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图15.5 – NBA比赛数据的相关矩阵](img/B17978_15_005.jpg)'
- en: Figure 15.5 – Correlation matrix of NBA games data
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 – NBA比赛数据的相关矩阵
- en: '`humandevratio` and `educratio` are highly correlated, as are `genderinequality`
    and `adolescentbirthrate`. We can see that `educratio` and `maternalmortality`
    are highly negatively correlated. It would be hard to build a well-performing
    model with all of these features given their high correlation. However, we might
    be able to reduce dimensions with kernel PCA.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`humandevratio`和`educratio`高度相关，同样`genderinequality`和`adolescentbirthrate`也高度相关。我们可以看到`educratio`和`maternalmortality`高度负相关。考虑到这些特征的高度相关性，构建一个表现良好的模型可能会有困难。然而，我们可能能够通过核PCA来降低维度。'
- en: 'We create training and testing DataFrames:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了训练和测试DataFrame：
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We are now ready to instantiate the `KernelPCA` and `RandomForestRegressor`
    objects. We add both to a pipeline. We also create a dictionary with our hyperparameters
    for the kernel PCA and the random forest regressor.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好实例化`KernelPCA`和`RandomForestRegressor`对象。我们将它们都添加到管道中。我们还创建了一个包含核PCA和随机森林回归器的超参数的字典。
- en: The dictionary has a range of hyperparameter values for the number of components,
    gamma, and the kernel to use with the kernel PCA. For those kernels that do not
    use gamma, those values are ignored. Notice that one option for kernel is the
    linear kernel.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 字典为成分数量、gamma以及与核PCA一起使用的核提供了一系列超参数值。对于不使用gamma的核，这些值被忽略。请注意，核的一个选项是线性核。
- en: 'We discuss gamma in more detail in [*Chapter 8*](B17978_08_ePub.xhtml#_idTextAnchor106)*,
    Support Vector Regression*, and [*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152)*,
    Support Vector Machine Classification*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第8章*](B17978_08_ePub.xhtml#_idTextAnchor106)*，支持向量回归*和[*第13章*](B17978_13_ePub.xhtml#_idTextAnchor152)*，支持向量机分类*中更详细地讨论了gamma：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let’s do a randomized grid search with these hyperparameter values. The
    kernel for the PCA that gives us the best performance with the random forest regressor
    is polynomial. We get a good square for mean squared error, about 10% of the size
    of the mean:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用这些超参数值进行随机网格搜索。对于随机森林回归器，给我们带来最佳性能的PCA核是多项式。我们得到了一个很好的均方误差平方，大约是均方误差的10%大小：
- en: '[PRE19]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s look at other top-performing models. A model with an `rbf` kernel and
    one with a sigmoid kernel do nearly as well. The second and third best-performing
    models have more principal components than the best-performing model:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看其他表现最好的模型。一个具有`rbf`核和一个具有sigmoid核的模型几乎表现相同。表现第二和第三好的模型比表现最好的模型有更多的主成分：
- en: '[PRE20]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Kernel PCA is a relatively easy-to-implement dimension reduction option. It
    is most useful when we have a number of highly correlated features that might
    not be linearly separable, and interpretation of predictions is not important.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 核PCA是一种相对容易实现的降维选项。当我们有许多高度相关的特征，而这些特征可能不是线性可分的时候，它最为有用，而且预测的解释并不重要。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explored principal component analysis, including how it works and
    when we might want to use it. We learned how to examine the components created
    from PCA, including how each feature contributes to each component, and how much
    of the variance is explained. We went over how to visualize components and how
    to use components in subsequent analysis. We also examined how to use kernels
    for PCA and when that might give us better results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了主成分分析，包括其工作原理以及我们可能想要使用它的时机。我们学习了如何检查PCA创建的成分，包括每个特征对每个成分的贡献以及解释了多少方差。我们还讨论了如何可视化成分以及如何在后续分析中使用成分。此外，我们还考察了如何使用核PCA以及何时这可能会给我们带来更好的结果。
- en: We explore another unsupervised learning technique in the next chapter, k-means
    clustering.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一种无监督学习技术，即k-means聚类。
