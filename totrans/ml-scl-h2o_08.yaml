- en: '*Chapter 6*: Advanced Model Building – Part II'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Advanced Model Building – Part I*, we detailed the process for building an enterprise-grade
    **supervised learning** model on the H2O platform. In this chapter, we round out
    our advanced model-building topics by doing the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Demonstrating how to build H2O supervised learning models within an Apache Spark
    pipeline
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing H2O's **unsupervised learning** method
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing best practices for updating H2O models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documenting requirements to ensure H2O model reproducibility
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We begin this chapter by introducing Sparkling Water pipelines, a method for
    embedding H2O models natively within a Spark pipeline. In enterprise settings
    where Spark is heavily utilized, we have found this to be a popular method for
    building and deploying H2O models. We demonstrate by building a Sparkling Water
    pipeline for **sentiment analysis** using data from online reviews of Amazon food
    products.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We then introduce the unsupervised learning methods available in H2O. Using
    credit card transaction data, we build an anomaly detection model using isolation
    forests. In this context, the unsupervised model would be used to flag suspicious
    credit card transactions in a financial fraud-prevention effort.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: We conclude this chapter by addressing issues pertinent to models built in this
    chapter, as well as in [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)*,
    Advanced Model Building – Part I*. These are best practices for updating H2O models
    and ensuring reproducibility of H2O model results.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in Sparkling Water
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UL methods in H2O
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for updating H2O models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring H2O model reproducibility
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code and datasets we introduce in this chapter can be found in the GitHub
    repository at [https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O](https://github.com/PacktPublishing/Machine-Learning-at-Scale-with-H2O).
    If you have not set up your H2O environment at this point, see [*Appendix*](B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268)
    *– Alternative Methods to Launch H2O Clusters for This Book,* to do so.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Modeling in Sparkling Water
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw in [*Chapter 2*](B16721_02_Final_SK_ePub.xhtml#_idTextAnchor024), *Platform
    Components and Key Concepts,* that Sparkling Water is simply H2O-3 in an Apache
    Spark environment. From the Python coder''s point of view, H2O-3 code is virtually
    identical to Sparkling Water code. If the code is the same, why have a separate
    section for modeling in Sparkling Water? There are two important reasons, as outlined
    here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Sparkling Water enables data scientists to leverage Spark's extensive data processing
    capabilities.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparkling Water provides access to production Spark pipelines. We expand upon
    these reasons next.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is rightly known for its data operations that effortlessly scale with
    increasing data volume. Since the presence of Spark in an enterprise setting is
    now almost a given, data scientists should add Spark to their skills toolbelt.
    This is not nearly as hard as it seems, since Spark can be operated from Python
    (using PySpark) with data operations written primarily in Spark SQL. For experienced
    Python and **Structured Query Language** (**SQL**) coders, this is a very easy
    transition indeed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark因其数据操作能够随着数据量的增加而轻松扩展而闻名。由于Spark在企业环境中的存在现在几乎是一个既定的事实，数据科学家应该将Spark添加到他们的技能工具箱中。这并不像看起来那么困难，因为Spark可以通过Python（使用PySpark）操作，数据操作主要使用Spark
    SQL编写。对于经验丰富的Python和**结构化查询语言**（**SQL**）编码者来说，这确实是一个非常容易的过渡。
- en: In the Lending Club example from [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Advanced Model Building – Part I*, data munging and **feature engineering** tasks
    were carried out using native H2O commands on the H2O cluster. These H2O data
    commands work in Sparkling Water as well. However, in an enterprise that has invested
    in a Spark data infrastructure, replacing H2O data commands with their Spark equivalents
    makes a lot of sense. It would then pass the cleaned dataset to H2O to handle
    the subsequent modeling steps. This is our recommended workflow in Sparkling Water.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082)“高级模型构建 – 第一部分”中的Lending
    Club示例中，使用H2O集群上的原生H2O命令执行了数据整理和**特征工程**任务。这些H2O数据命令在Sparkling Water中同样适用。然而，在一个已经投资Spark数据基础设施的企业中，用Spark的等效命令替换H2O数据命令是非常有意义的。然后，将清洗后的数据集传递给H2O以处理后续的建模步骤。这是我们推荐在Sparkling
    Water中的工作流程。
- en: In addition, Spark pipelines are frequently used in enterprise production settings
    for **extract, transform, and load** (**ETL**) and other data processing tasks.
    Sparkling Water's integration of H2O algorithms into Spark pipelines allows for
    seamless training and deployment of H2O models in a Spark environment. In the
    remainder of this section, we show how Spark pipelines can be combined with H2O
    modeling to create a Sparkling Water pipeline. This pipeline is easily promoted
    into production, a topic that we return to in detail in [*Chapter 10*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178),
    *H2O Model Deployment Patterns*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Spark流水线在企业生产环境中经常用于**提取、转换和加载**（**ETL**）以及其他数据处理任务。Sparkling Water将H2O算法集成到Spark流水线中，使得在Spark环境中无缝训练和部署H2O模型成为可能。在本节的剩余部分，我们将展示如何将Spark流水线与H2O建模相结合，创建一个Sparkling
    Water流水线。这个流水线可以轻松地推广到生产环境中，我们将在[*第10章*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178)“H2O模型部署模式”中详细讨论这个话题。
- en: Introducing Sparkling Water pipelines
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍Sparkling Water流水线
- en: '*Figure 6.1* illustrates the Sparkling Water pipeline training and deployment
    process. The pipeline starts with an input data source for model training. Data
    cleaning and feature engineering steps are built sequentially from Spark transformers,
    with the outputs of one transformer becoming the inputs of the subsequent transformer.
    Once the dataset is in a modeling-ready format, H2O takes over to specify and
    build a model. We wrap all the transformer and model steps into a pipeline that
    is trained and then exported for production.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1*展示了Sparkling Water流水线的训练和部署过程。流水线从模型训练的输入数据源开始。数据清洗和特征工程步骤通过Spark转换器依次构建，一个转换器的输出成为后续转换器的输入。一旦数据集以建模准备好的格式存在，H2O将接管以指定和构建模型。我们将所有转换器和模型步骤封装到一个流水线中，该流水线经过训练然后导出用于生产。'
- en: 'In the production environment, we import the pipeline and introduce new data
    to it (in the following diagram, we assume this happens via a data stream, but
    the data could be arriving in batches as well). The pipeline outputs H2O model
    predictions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们导入流水线并向其中引入新的数据（在以下图中，我们假设这是通过数据流完成的，但数据也可以批量到达）。流水线输出H2O模型预测：
- en: '![Figure 6.1 – Sparkling Water pipeline train and deploy illustration'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – Sparkling Water流水线训练和部署示意图'
- en: '](img/B16721_06_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_01.jpg)'
- en: Figure 6.1 – Sparkling Water pipeline train and deploy illustration
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – Sparkling Water流水线训练和部署示意图
- en: Next, let's create a pipeline to implement sentiment analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个流水线来实现情感分析。
- en: Implementing a sentiment analysis pipeline
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现情感分析流水线
- en: We next create a Sparkling Water pipeline for an sentiment analysis classification
    problem. Sentiment analysis is used to model whether a customer has positive or
    negative feelings toward a product or company. It typically requires **natural
    language processing** (**NLP**) to create predictors from text. For our example,
    we use a preprocessed version of the *Amazon Fine Food reviews* dataset from the
    **Stanford Network Analysis Platform** (**SNAP**) repository. (See [https://snap.stanford.edu/data/web-FineFoods.html](https://snap.stanford.edu/data/web-FineFoods.html)
    for the original data.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Let's first verify that Spark is available on our system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Spark startup in Jupyter notebook with PySparkling kernel'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Spark startup in Jupyter notebook with PySparkling kernel
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the Spark output that the **SparkSession** has been started and
    the **SparkContext** initiated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: PySpark and PySparkling
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**PySpark** is Apache''s Python interface for Spark. It provides a shell for
    interactive Spark sessions and access to Spark components such as Spark SQL, DataFrames,
    and Streaming. **PySparkling** is the H2O extension of **PySpark**, enabling H2O
    services to be started on a Spark cluster from Python. Our Jupyter notebook uses
    a PySpark shell.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *internal backend* mode of Sparkling Water, H2O resources piggyback
    on their Spark counterparts, all within the same **Java virtual machine** (**JVM**).
    As illustrated inthe following diagram, an **H2OContext** that sits on top of
    the **SparkContext** is launched and H2O is initialized in each worker node of
    the Spark cluster:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Sparkling Water internal backend mode'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_03.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Sparkling Water internal backend mode
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'PySparkling is used to create an H2OContext and initialize worker nodes, as
    follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Sparkling Water cluster immediately after launch'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Sparkling Water cluster immediately after launch
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: After the H2O server is launched, we interact with it using Python commands.
    We will start by importing the raw data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Importing the raw Amazon data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We import the Amazon training data into a `reviews_spark` Spark DataFrame,
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As an alternative, we could have imported the data using H2O and then converted
    the `reviews_h2o` H2O frame to the `reviews_spark` Spark DataFrame, like so:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This approach has the advantage of allowing us to use H2O Flow for interactive
    data exploration, as demonstrated in [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082),
    *Advanced Model Building – Part I*, before converting to a Spark DataFrame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we print out the data schema to show the input variables and variable
    types. This is done with the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting data schema is shown in the following screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Schema for Amazon Fine Food raw data'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_05.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Schema for Amazon Fine Food raw data
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 亚马逊美食原始数据的模式
- en: For simplicity, we use only the `Time`, `Summary`, and overall `Score` columns
    in this analysis. `Time` is a date-time string, `Score` is an integer value between
    1 and 5, from which sentiment is derived, and `Summary` is a short text summary
    of the product review. Note that the `Text` column contains the actual product
    review. A better model choice would include `Text` in place of—or perhaps in addition
    to—`Summary`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们在这个分析中只使用了`Time`、`Summary`和整体的`Score`列。`Time`是一个日期时间字符串，`Score`是一个介于1到5之间的整数，从它可以导出情感，而`Summary`是产品评论的简短文本摘要。请注意，`Text`列包含实际的产品评论。更好的模型选择将包括`Text`，而不是或可能还包括`Summary`。
- en: 'Save the input data schema into the `schema.json` file using the following
    code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将输入数据模式保存到`schema.json`文件中：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Saving the input data schema will make the deployment of the Sparkling Water
    pipeline a very simple matter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 保存输入数据模式将使Sparkling Water管道的部署变得非常简单。
- en: Input Data and Production Data Structure
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据和生产数据结构
- en: Saving the data schema for deployment presumes that production data will use
    the same schema. As a data scientist building a Sparkling Water pipeline, we strongly
    recommend that your training input data exactly follows the production data schema.
    It is worth the extra effort to track this information down prior to model build
    rather than having to reengineer something at the deployment stage.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 保存数据模式以供部署假定生产数据将使用相同的模式。作为构建Sparkling Water管道的数据科学家，我们强烈建议您的训练输入数据与生产数据模式完全一致。在模型构建之前追踪这些信息是值得的，而不是在部署阶段重新设计某些内容。
- en: Defining Spark pipeline stages
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义Spark管道阶段
- en: Spark pipelines are created by stringing individual data operations or transformers
    together. Each transformer takes as its input the output data from the preceding
    stage, which makes development for a data scientist very simple. A large job can
    be broken down into individual tasks that are daisy-chained together.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Spark管道是通过将单个数据操作或转换器连接在一起来创建的。每个转换器将其输入数据作为前一阶段的输出数据，这使得数据科学家的开发变得非常简单。一个大型作业可以被分解成一系列相互连接的单独任务。
- en: Apache Spark operates through lazy evaluation. This means that computations
    do not execute immediately; rather, operations are cached and execution occurs
    when an action of some sort is triggered. This approach has many advantages, including
    allowing Spark to optimize its compute.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark通过延迟评估来操作。这意味着计算不会立即执行；相反，操作被缓存，当触发某种操作的动作时才会执行。这种方法具有许多优点，包括允许Spark优化其计算。
- en: In our example, all the data cleaning and feature engineering steps will be
    created through Spark transformers. The pipeline is finalized by training an H2O
    XGBoost model. For clarity, we will define a stage number for each transformer
    as we proceed in building the pipeline.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，所有数据清洗和特征工程步骤将通过Spark转换器创建。通过训练一个H2O XGBoost模型来最终确定管道。为了清晰起见，我们在构建管道的过程中将为每个转换器定义一个阶段编号。
- en: Stage 1 – Creating a transformer to select required columns
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1阶段 – 创建一个转换器以选择所需的列
- en: 'The Spark `SQLTransformer` class allows us to use SQL to munge data. The fact
    that most data scientists are already experienced with SQL makes for smooth adoption
    of Spark for data operations. `SQLTransformer` will be widely used in this pipeline.
    Run the following code to import the class:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`SQLTransformer`类允许我们使用SQL来处理数据。由于大多数数据科学家已经熟悉SQL，这使得Spark在数据操作方面的采用变得顺利。`SQLTransformer`将在本管道中得到广泛使用。运行以下代码以导入该类：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define a `colSelect` transformer, like so:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个`colSelect`转换器，如下所示：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we select the `Score`, `Time`, and `Summary` columns,
    converting the timestamp to a readable date-time string. `__THIS__` in the `FROM`
    statement references the output from the previous transformer stage. Since this
    is the first stage, `__THIS__` refers to the input data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们选择了`Score`、`Time`和`Summary`列，将时间戳转换为可读的日期时间字符串。`FROM`语句中的`__THIS__`引用了前一转换器阶段的输出。由于这是第一个阶段，`__THIS__`指的是输入数据。
- en: 'During development, it is helpful to check results at each stage by calling
    the transformer directly. This makes it easy to debug transformer code and understand
    which inputs will be available for the next stage. Calling the transformer will
    cause Spark to execute it along with all unevaluated upstream code. The following
    code snippet illustrates how to call the transformer:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，通过直接调用转换器来检查每个阶段的结果是有帮助的。这使得调试转换器代码和理解下一阶段可用的输入变得容易。调用转换器将导致Spark执行它以及所有未评估的上游代码。以下代码片段说明了如何调用转换器：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The first few rows are shown in the following screenshot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了前几行：
- en: '![Figure 6.6 – Results from the colSelect stage 1 transformer'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – colSelect阶段1转换器的结果'
- en: '](img/B16721_06_06.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_06.jpg)'
- en: Figure 6.6 – Results from the colSelect stage 1 transformer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – colSelect阶段1转换器的结果
- en: This first transformer has taken the original data and boiled it down to three
    columns. We will operate on each column separately to create our modeling-ready
    dataset. Let's begin with the `Time` column.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个转换器已经将原始数据简化为三列。我们将分别对每一列进行操作，以创建我们的建模准备数据集。让我们从`时间`列开始。
- en: Stage 2 – Defining a transformer to create multiple time features
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段2 – 定义一个转换器以创建多个时间特征
- en: 'The goal of this model is to predict sentiment: was the review positive or
    negative? Date and time are factors that could arguably influence sentiment. Perhaps
    people give better reviews on Friday evenings because there is a weekend upcoming.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的目标是预测情感：评论是正面还是负面？日期和时间可能是影响情感的因素。也许人们在周五晚上给出更好的评论，因为周末即将到来。
- en: 'The `Time` column is stored internally as a timestamp. To be useful in modeling,
    we need to extract the date-and-time information in a format that is understandable
    by the predictive algorithms we employ. We define an `expandTime` transformer
    using SparkSQL data methods (such as `hour`, `month`, and `year`) to engineer
    multiple new features from the raw timestamp information, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`时间`列以时间戳的形式内部存储。为了在建模中变得有用，我们需要以预测算法可以理解的形式提取日期和时间信息。我们定义了一个`expandTime`转换器，使用SparkSQL数据方法（如`hour`、`month`和`year`）从原始时间戳信息中构建多个新特征，如下所示：'
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Note that `Score` and `Summary` are selected in the `expandTime` code, but
    we do not operate on them. This simply passes those columns along to subsequent
    transformers. We engineer several features from the `Time` column: `Day`, `Month`,
    `Year`, `WeekNum`, `Weekday`, `HourOfDay`, `Weekend`, and `Season`. And once more,
    `__THIS__` refers to the output from the `colSelect` stage 1 transformer.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`expandTime`代码中选择了`评分`和`摘要`，但我们没有对它们进行操作。这仅仅是将这些列传递给后续的转换器。我们从`时间`列中构建了几个特征：`日`、`月`、`年`、`周数`、`星期几`、`小时`、`周末`和`季节`。而且，`__THIS__`再次指的是`colSelect`阶段1转换器的输出。
- en: 'To check our progress in development and perhaps debug our code, we inspect
    the output of the second stage, which uses as its input the first-stage results
    stored in `selected`, as illustrated in the following code snippet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的开发进度并可能调试我们的代码，我们检查第二阶段的输出，该阶段使用存储在`selected`中的第一阶段结果作为输入，如下面的代码片段所示：
- en: '[PRE41]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了输出。
- en: '![Figure 6.7 – Results from the expandTime stage 2 transformer'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – expandTime阶段2转换器的结果'
- en: '](img/B16721_06_07.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16721_06_07.jpg)'
- en: Figure 6.7 – Results from the expandTime stage 2 transformer
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – expandTime阶段2转换器的结果
- en: The output confirms that we have successfully replaced the `Time` column with
    a collection of newly created features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确认我们已经成功将`时间`列替换为一系列新创建的特征。
- en: Stage 3 – Creating a response from Score while removing neutral reviews
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶段3 – 从评分创建响应，同时移除中性评论
- en: In this stage, we create our `Sentiment` response variable using values from
    the `Score` column. We could model *positive* versus *not positive* as the response,
    but we choose to remove neutral reviews (`Score=3`) and compare `Positive` with
    `Negative`. This is a standard approach in **net promoter score** (**NPS**) analyses
    and is common in sentiment analysis. It makes sense because we assume that records
    with a neutral response contain little information the model could learn from.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们使用`评分`列的值创建我们的`情感`响应变量。我们可以将`正面`与`非正面`建模为响应，但选择移除中性评论（`评分=3`），并将`正面`与`负面`进行比较。这是**净推荐者得分**（**NPS**）分析中的标准方法，并且在情感分析中也很常见。这样做是有道理的，因为我们假设具有中性响应的记录包含的信息很少，模型难以从中学习。
- en: 'We create our `createResponse` transformer like so:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建`createResponse`转换器的方式如下：
- en: '[PRE43]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The `IF` statement assigns scores of 1 or 2 to `Negative` sentiment and all
    others to `Positive`, filtering out neutral reviews with the `WHERE` clause. Now,
    inspect the results of this intermediate step by running the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`IF`语句将1或2分给`Negative`情感，并将所有其他情感归为`Positive`，通过`WHERE`子句过滤掉中性的评论。现在，通过运行以下代码来检查这个中间步骤的结果：'
- en: '[PRE49]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This results in the following output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 6.8 – Results from the createResponse stage 3 transformer'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 – createResponse阶段3的输出结果](img/B16721_06_08.jpg)'
- en: '](img/B16721_06_08.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_08.jpg](img/B16721_06_08.jpg)'
- en: Figure 6.8 – Results from the createResponse stage 3 transformer
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – createResponse阶段3的输出结果
- en: The only remaining feature engineering steps are those replacing the text in
    the `Summary` column with appropriately representative numerical values. Stages
    4 through 8 will leverage Spark's built-in NLP data transformation capabilities
    to create features based on the text in `Summary`. While this is not a formal
    deep dive into NLP, we will describe each transformation step in enough detail
    to make our model understandable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一特征工程步骤是将`Summary`列中的文本替换为适当的代表性数值。第4到第8阶段将利用Spark内置的NLP数据转换功能，根据`Summary`中的文本创建特征。虽然这不是一个正式的深度NLP研究，但我们将详细描述每个转换步骤，以便使我们的模型易于理解。
- en: Stage 4 – Tokenizing the summary
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4阶段 – 分词摘要
- en: 'Tokenization breaks a text sequence into individual terms. Spark provides a
    simple `Tokenizer` class and a more flexible `RegexTokenizer` class, which we
    use here. The `pattern` parameter specifies a `"[!,\" ]"` `\"` escaped quote in
    the regex), and we specify `This` and `this` will be considered identical terms
    upon later processing. The code is illustrated in the following snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分词将文本序列分解成单个术语。Spark提供了一个简单的`Tokenizer`类和一个更灵活的`RegexTokenizer`类，我们在这里使用后者。`pattern`参数指定了一个正则表达式中的转义引号`"[!,\"
    ]"`，并且我们指定`This`和`this`在后续处理中将被视为相同的术语。代码在下面的代码片段中展示：
- en: '[PRE51]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Inspect the tokenized values from `Summary`, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`Summary`中的分词值，如下所示：
- en: '[PRE56]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出在下面的屏幕截图中显示：
- en: '![Figure 6.9 – Results from the regexTokenizer stage 4 transformer'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9 – regexTokenizer阶段4的输出结果](img/B16721_06_09.jpg)'
- en: '](img/B16721_06_09.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_09.jpg](img/B16721_06_09.jpg)'
- en: Figure 6.9 – Results from the regexTokenizer stage 4 transformer
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – regexTokenizer阶段4的输出结果
- en: Phrases have now been broken up into lists of individual terms or tokens. Since
    our goal is to extract information from these tokens, we next filter out words
    that carry little information.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在短语已经被分解成单个术语或标记的列表。由于我们的目标是提取这些标记中的信息，我们接下来过滤掉那些信息量较小的单词。
- en: Stage 5 – Removing stop words
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5阶段 – 去除停用词
- en: 'Some words occur so frequently in language that they have very little predictive
    value. These are termed *stop words* and we use Spark''s `StopWordsRemover` transformer
    to delete them, as illustrated in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有些单词在语言中出现的频率很高，它们几乎没有预测价值。这些被称为*停用词*，我们使用Spark的`StopWordsRemover`转换器来删除它们，如下面的截图所示：
- en: '[PRE59]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s compare the tokenized results before and after removing stop words,
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较去除停用词前后的分词结果，如下所示：
- en: '[PRE63]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The results are displayed in the following screenshot:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在下面的屏幕截图中显示：
- en: '![Figure 6.10 – Results from the removeStopWords stage 5 transformer'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10 – removeStopWords阶段5的输出结果](img/B16721_06_10.jpg)'
- en: '](img/B16721_06_10.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16721_06_10.jpg](img/B16721_06_10.jpg)'
- en: Figure 6.10 – Results from the removeStopWords stage 5 transformer
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – removeStopWords阶段5的输出结果
- en: 'Inspecting the results in *Figure 6.10* is illustrative. For the most part,
    removing stop words such as `as`, `it`, or `the` has little effect on meaning:
    the *Great! Just as good as the expensive brands!* statement being reduced to
    tokens `[great, good, expensive, brands]` seems reasonable. But what about *Not
    as advertised!* being reduced to `[advertised]`? The `not` in the statement would
    seem to carry important information that is lost by its removal. This is a valid
    concern that could be addressed by NLP concepts such as n-grams (bigrams, trigrams,
    and so on). For an example demonstrating Sparkling Water pipelines, we will acknowledge
    this as a potential issue but move on for simplicity.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 检查*图6.10*的结果是很有说明性的。就大部分而言，移除诸如`as`、`it`或`the`之类的停用词对意义的影响很小：将*Great! Just as
    good as the expensive brands!*的陈述缩减为标记`[great, good, expensive, brands]`似乎是合理的。但*Not
    as advertised!*缩减为`[advertised]`呢？陈述中的`not`似乎承载着重要信息，而移除它会导致信息丢失。这是一个有效的问题，可以通过NLP概念如n-gram（双词组、三词组等）来解决。为了演示Sparkling
    Water管道的例子，我们将承认这是一个潜在问题，但为了简单起见，我们将继续前进。
- en: NLP in predictive modeling represents information in text as numbers. A popular
    approach is **term frequency-inverse document frequency** (**TF-IDF**). TF is
    simply the number of times a term appears in a document divided by the number
    of words in a document. In a corpus (collection of documents), IDF measures how
    rare a term is across its constituent documents. A term such as *linear* may have
    high frequency, but its information value decreases as the number of documents
    it appears in increases. On the other hand, a word such as *motorcycle* may have
    lower frequency but also be found in fewer documents in a corpus, making its information
    content higher. Multiplying TF by IDF gives a rescaled TF value that has proven
    quite useful. TF-IDF values are maximized when a term is found frequently but
    only in one document (*Which article reviewed motorcycles?*).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测建模中，自然语言处理（NLP）将文本信息表示为数字。一种流行的方法是**词频-逆文档频率**（**TF-IDF**）。TF简单地是指一个词在文档中出现的次数除以文档中的单词数。在一个语料库（文档集合）中，IDF衡量一个词在其构成文档中的稀有程度。例如，*线性*这样的词可能频率很高，但随着它在文档中出现的次数增加，其信息价值会降低。另一方面，像*摩托车*这样的词可能频率较低，但在语料库中出现的文档较少，使其信息含量更高。将TF乘以IDF得到一个经过缩放的TF值，这已被证明非常有用。当一个词频繁出现在一个文档中但只在一个文档中出现时，TF-IDF值达到最大（*哪篇文章回顾了摩托车？*）。
- en: TF-IDF is widely used in information retrieval, text mining, recommender systems,
    and search engines, as well as in predictive modeling. The next two pipeline stages
    will compute the TF and IDF values, respectively.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF在信息检索、文本挖掘、推荐系统和搜索引擎以及预测建模中得到广泛应用。接下来的两个管道阶段将分别计算TF和IDF值。
- en: Stage 6 – Hashing words for TF
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6阶段 – 为TF哈希单词
- en: Our preferred way to compute TF in Spark is `CountVectorizer`, which preserves
    the mapping from the index back to the word using an internal vocabulary. That
    is, `countVectorizerModel.vocabulary[5]` looks up the word stored in index 5.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Spark中计算TF的首选方法是`CountVectorizer`，它使用内部词汇表将索引映射回单词。也就是说，`countVectorizerModel.vocabulary[5]`查找存储在索引5中的单词。
- en: 'A trick for building better TF-IDF models is to remove terms that are too infrequent
    by setting the `minDF` parameter as an integer or proportion, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 构建更好的TF-IDF模型的一个技巧是通过将`minDF`参数设置为整数或比例来移除过于罕见的词，如下所示：
- en: '`minDF = 100`: Omit terms that appear in fewer than 100 documents'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minDF = 100`：省略在不到100个文档中出现的词'
- en: '`minDF = 0.05`: Omit terms that appear in fewer than 5% of documents'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minDF = 0.05`：省略在不到5%的文档中出现的词'
- en: A `maxDF` parameter is also available for removing terms that occur too frequently
    across a corpus. For instance, setting `maxDF = 0.95` in NLP for document retrieval
    might improve model performance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个`maxDF`参数可以用来移除在语料库中过于频繁出现的词。例如，在NLP文档检索中设置`maxDF = 0.95`可能会提高模型性能。
- en: 'We create a `countVectorizer` transformer, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`countVectorizer`转换器，如下所示：
- en: '[PRE67]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Note that our corpus is the output column of the `removeStopWords` transformer,
    with each row as a document. We output the frequencies and set `minDF` to 100\.
    Because `countVectorizer` is a model, it is a good idea to manually train it before
    executing it in the pipeline. This is a good practice for any model that is a
    pipeline component as it allows us to determine its behavior and perhaps fine-tune
    it before pipeline execution commences. The code is illustrated in the following
    snippet:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We can explore this model by inspecting its vocabulary size and individual
    terms, as well as any other appropriate due diligence. Here''s the code we''d
    need to accomplish this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The vocabulary results are shown here:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Vocabulary size and vocabulary of the countVecModel transformer'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_11.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – Vocabulary size and vocabulary of the countVecModel transformer
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Our total vocabulary size shown in *Figure 6.11* is 1,431 terms. Inspect the
    data with the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The vectorized result is shown in the following screenshot:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Intermediate results from the countVecModel transformer'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_12.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – Intermediate results from the countVecModel transformer
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.12* shows the cleaned summary tokens with TF vectors side by side
    for each row. To describe the output for the first row, the 1431 value is the
    vocabulary size. The next sequence of values—`[1,10,11,38]`—refers to the indices
    of the `[good, quality, dog, food]` terms in the vocabulary vector. The last series
    of values— `[1.0,1.0,1.0,1.0]`—are the TFs for their respective terms. Thus, `dog`
    is referenced by index `11` and occurs once in the `CleanedSummary` column.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Stage 7 – Creating an IDF model
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use Spark''s `IDF` estimator to scale frequencies from `countVectorizer`,
    yielding TF-IDF values. We execute the following code to accomplish this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Manually train the IDF model to see the results before we execute the pipeline,
    like so:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Inspect the data again, noting especially the scaled TF-IDF frequencies, as
    follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The first five rows of the resulting TF-IDF model are shown in the following
    screenshot:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – TF-IDF frequencies from the Spark transformer'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_13.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 – TF-IDF frequencies from the Spark transformer
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Stage 8 – Selecting modeling dataset columns
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the `Sentiment` response variable and all the features engineered
    from the `Time` variable, the output of `idf` includes the original `Summary`
    column as well as `Tokenized`, `CleanedSummary`, `frequencies`, and `TFIDF`. Of
    these, we wish to keep only `TFIDF`. The following code selects the desired columns:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Now that we are finished building our model-ready data, the next step is to
    build a predictive model using one of H2O's supervised learning algorithms.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Stage 9 – Creating an XGBoost model using H2O
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Up to this point, all our data wrangling and feature engineering efforts have
    used Spark methods exclusively. Now, we turn to H2O to train an XGBoost model
    on the `Sentiment` column. For simplicity, we train using default settings. The
    code is illustrated in the following snippet:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Note – Training Models in Sparkling Water
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082), *Advanced
    Model Building – Part I*, we demonstrated in detail a process for building and
    tuning a high-quality XGBoost model. We stop at a simple baseline model here to
    emphasize the utility of the overall pipeline. In a real application, much more
    effort should be spent on the modeling component of this pipeline.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Sparkling Water pipeline
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have all the transformers defined, we are ready to create a pipeline.
    Doing so is simple—we just name each transformer in order in the `stages` list
    parameter of `Pipeline`, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Training the pipeline model is made simple by using the `fit` method. We pass
    as a parameter the Spark DataFrame containing the raw data, as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: During the `pipeline.fit` process, the data munging and feature engineering
    stages are all applied to the raw data in the order defined before the XGBoost
    model is fit. These pipeline stages operate identically after deployment in production
    with the XGBoost stage, producing predictions.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead – a production preview
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Putting the Sparkling Water pipeline into production is simply a matter of
    *saving* the `pipeline` model, *loading* it onto the production system, then calling
    the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: In [*Chapter 10*](B16721_10_Final_SK_ePub.xhtml#_idTextAnchor178), *H2O Model
    Deployment Patterns*, we show how to deploy this pipeline as a Spark streaming
    application, with the pipeline receiving raw streaming data and outputting predictions
    in real time.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: UL methods in H2O
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O includes several unsupervised learning algorithms including **Generalized
    Low Rank Models** (**GLRM**), **Principal Component Analysis** (**PCA**), and
    an aggregator for dimensionality reduction. Clustering use cases can utilize k-means
    clustering, H2O aggregator, GLRM, or PCA. Unsupervised learning also underlies
    a set of useful feature transformers used in predictive modeling applications—for
    example, the distance of an observation to a specific data cluster identified
    by an unsupervised method. In addition, H2O provides an isolation forest algorithm
    for anomaly detection.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: What is anomaly detection?
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most **machine learning** (**ML**) algorithms attempt, in some manner, to find
    patterns in data. These patterns are leveraged to make predictions in supervised
    learning models. Many unsupervised learning algorithms try to uncover patterns
    through clustering similar data or estimating boundaries between data segments.
    Unsupervised anomaly detection algorithms take the opposite approach: data points
    that do not follow known patterns are what we want to discover.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, the term *anomaly* is value-free. It may refer to an unusual
    observation because it is the first of its kind; more data could yield additional
    similar observations. Anomalies might be indicative of unexpected events and serve
    as a diagnostic. For instance, a failed sensor in a manufacturing data-collection
    application could yield atypical measurements. Anomalies may also indicate malicious
    actors or actions: security breaches and fraud are two classic examples resulting
    in anomalous data points.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection approaches may include supervised, semi-supervised, or unsupervised
    methods. Supervised models are the gold standard in fraud detection. However,
    obtaining labels for each observation can be costly and is often not feasible.
    An unsupervised approach is required when labels are absent. Semi-supervised approaches
    refer to situations where only some of the data records are labeled, usually a
    small minority of records.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Isolation forest is a unsupervised learning algorithm for anomaly detection—we'll
    introduce this next.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Isolation forests in H2O
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The isolation forest algorithm is based on decision trees and a clever observation:
    outliers tend to be split out very early in the building of a decision tree. But
    decision trees are a supervised method, so how is this unsupervised? The trick
    is to create a target column of random values and train a decision tree on it.
    We repeat this many times and record the average depth at which observations are
    split into their own leaf. The earlier an observation is isolated, the more likely
    it is to be anomalous. Depending on the use case, these anomalous points may be
    filtered out or escalated for further investigation.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see a representation of an isolation forest in the following screenshot:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – An isolation forest'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_14.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – An isolation forest
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: We show how to build an isolation forest in H2O using the Kaggle credit card
    transaction data ([https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)).
    There are 492 fraudulent and 284,807 non-fraudulent transactions in this dataset,
    which makes the target class highly imbalanced. Because we are demonstrating an
    unsupervised anomaly detection approach, we will drop the labeled target during
    the model build.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'The H2O code for loading the data is shown here:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We fit our isolation forest using the `H2OIsolationForestEstimator` method.
    We set the number of trees to `100` and omit the last column, which contains the
    target class label, as shown in the following code snippet:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Once the model is trained, prediction is straightforward, as we can see here:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Isolation forest predictions'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_15.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – Isolation forest predictions
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'The predictions in *Figure 6.15* consist of two columns: the normalized anomaly
    score and the average number of splits across all trees to isolate the observation.
    Note that the anomaly score is perfectly correlated with mean length, increasing
    as mean length decreases.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we go from either an anomaly score or mean length to an actual prediction?
    One of the best ways is through a quantile-based threshold. If we have an idea
    about the prevalence of fraud, we can find the corresponding quantile value of
    the score and use it as a threshold for our predictions. Suppose we know that
    5% of our transactions are fraudulent. Then, we estimate the correct quantile
    using the following H2O code:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'The resulting quantile output is shown in the following screenshot:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Choosing a quantile-based threshold'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_16.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – Choosing a quantile-based threshold
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'We now can use the threshold to predict the anomalous class using the following
    code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The first 10 observations of the `predictions` frame are shown in the following
    screenshot:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Identifying anomalous values'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16721_06_17.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – Identifying anomalous values
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The `predict` column in *Figure 6.17* has only one observation greater than
    0.198324, the threshold for the 95th percentile shown in *Figure 6.16*. The `predicted_class`
    column indicates this with a value of `1`. Also, note that the `mean_length` value
    for this observation of `6.14` is less than the mean length values for the other
    nine observations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The `class` column contains the transaction fraud indicator that we omitted
    in building the unsupervised isolation forest model. For the anomalous observation,
    a class value of `0` indicates that the transaction was not fraudulent. When we
    have access to an actual target value as in this example, we could use the `predicted_class`
    and `class` columns to study the effectiveness of the anomaly detection algorithm
    in detecting fraud. We should note that the definition of fraud and anomalous
    in this context are not equivalent. In other words, not all frauds are anomalous
    and not all anomalies will indicate fraud. These two models have separate, albeit
    complementary, purposes.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: We now turn our attention to updating models.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for updating H2O models
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the famous British statistician George Box stated, *All models are wrong,
    but some are useful*. Good modelers are aware of the purpose as well as the limitations
    of their models. This should be especially true for those who build enterprise
    models that go into production.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: One such limitation is that predictive models as a rule degrade over time. This
    is largely because, in the real world, things change. Perhaps what we are modeling—customer
    behavior, for example—itself changes, and the data we collect reflects that change.
    Even if customer behavior is static but our mix of business changes (think more
    teenagers and fewer retirees), our model's predictions will likely degrade. In
    both cases but for different reasons, the population that was sampled to create
    our predictive model is not the same now as it was before.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Detecting model degradation and searching for its root cause is the subject
    of diagnostics and model monitoring, which we do not address here. Rather, once
    a model is no longer satisfactory, what should a data scientist do? We address
    retraining and checkpointing models in the following sections.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Retraining models
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Developing a parametric model entails:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Finding the correct structural form for the process being modeled and then
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using data to estimate the parameters of that structure. Over time, if the structure
    of the model remains the same but the data changes, then we could *refit* (or
    *retrain* or *re-estimate* or *update*) the model's parameter estimates. This
    is a simple and relatively straightforward procedure.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, if the underlying process changes in a way that means the structural
    form of the model is no longer valid, then modeling consists of both discovering
    the correct form of the model and estimating parameters. This is almost, but not
    quite, the same thing as starting from scratch. *Rebuilding* or *updating the
    model* (as opposed to *updating the parameter estimates*) are better terms for
    this larger activity.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of ML or other nonparametric models, the structural form of the
    model is determined by the data along with any parameter estimates. This is one
    of the selling points of nonparametric models: they are incredibly data-driven
    and nearly assumption-free. The differences between refitting or retraining and
    rebuilding have little meaning in this context; these terms become, in effect,
    synonyms.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing models
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Checkpoint** option in H2O lets you save the state of a model build, allowing
    a new model to be built as a *continuation* of a previously generated model rather
    than building one from scratch. This can be used to update a model in production
    with additional, more current data.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The checkpoint option is available for **Distributed Random Forest** (**DRF**),
    **Gradient Boosting Machine** (**GBM**), XGBoost, and **deep learning** (**DL**)
    algorithms. For tree-based algorithms, the number of trees specified must be greater
    than the number of trees in the referenced model. That is, if the original model
    included 20 trees and you specify 30 trees, then 10 new trees will be built. The
    same concept is true for DL using epochs rather than trees.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'Checkpointing is feasible for these algorithms *only* when the following are
    the same as the checkpointed model:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: The training data model type, response type, columns, categorical factor levels,
    and the total number of predictors
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The identical validation dataset if one was used in the checkpointed model (cross-validation
    is not currently supported for checkpointing)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional parameters that you can specify with checkpointing vary based on
    the algorithm that was used for model training.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing Caveats
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Although it is technically feasible, we do not recommend checkpointing on new
    data for GBM or XGBoost algorithms. Recall that boosting works by fitting sequential
    models to the residuals of previous models. The early splits are thus the most
    important. By the time new data has been introduced, the structure of the model
    has been largely determined in its absence.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing **random forest** models does not suffer from these concerns due
    to differences between boosting and bagging.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring H2O model reproducibility
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a laboratory or experimental setting, repeating a process under the same
    protocols and conditions should lead to similar results. Natural variability may
    of course occur, but this can be measured and attributed to appropriate factors.
    This is termed *repeatability*. The enterprise data scientist should ensure that
    their model builds are well coded and sufficiently documented to make the process
    repeatable.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '**Reproducibility** in the context of model building is a much stronger condition:
    the results when a process is repeated must be identical. From a regulatory or
    compliance perspective, reproducibility may be required.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, reproducibility requires the same hardware, software, data,
    and settings. Let's review this specifically for H2O setups. We begin with two
    cases depending on the H2O cluster type.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Case 1 – Reproducibility in single-node clusters
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A single-node cluster is the simplest H2O hardware configuration. Reproducibility
    can be attained if the following conditions are met:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '**Software requirements**: The same version of H2O-3 or Sparkling Water is
    used.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data requirements**: The same training data is used (note that H2O requires
    files to be imported individually rather than as an entire directory to guarantee
    reproducibility).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_rate`, `sample_rate_per_class`, `col_sample_rate`, `col_sample_rate_per_level`,
    `col_sample_rate_per_tree`.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If early stopping is enabled, reproducibility is only guaranteed when the `score_tree_interval` parameter
    is explicitly set and the same validation dataset is used.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case 2 – Reproducibility in multi-node clusters
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adding nodes to a cluster creates additional hardware conditions that must
    be met in order to achieve reproducibility. The software, data, and settings requirements
    are the same as in single-node clusters detailed previously in *Case 1*. These
    requirements are outlined here:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: The hardware cluster must be configured identically. Specifically, clusters
    must have the same number of nodes with the same number of CPU cores per node
    or—alternatively—the same restriction on the number of threads.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster's leader node must initiate model training. In Hadoop, the leader
    node is automatically returned to the user. In standalone deployments, the leader
    node must be manually identified. See the H2O documentation for more details.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For reproducibility, you must ensure that the cluster configuration is identical.
    The parallelization level (number of nodes and CPU cores/threads) controls how
    a dataset is partitioned in memory. H2O runs its tasks in a predictable order
    on these partitions. If the number of partitions is different, the results will
    not be reproducible.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the cluster configuration is not identical, it may be possible
    to constrain the resources of computations being reproduced. This process involves
    replicating data partitions in the original environment. We refer you to the H2O
    documentation for more information.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility for specific algorithms
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complexity of DL, GBM, and **automated ML** (**AutoML**) algorithms introduces
    additional constraints that must be met in order to ensure reproducibility. We
    will review these requirements in this section.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: DL
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: H2O DL models are not reproducible by default for performance reasons. There
    is a `reproducible` option that can be enabled, but we recommend doing this only
    for small data. The model takes significantly more time to generate because only
    one thread is used for computation.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: GBM
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GBM is deterministic up to floating-point rounding errors when reproducibility
    criteria are met for single- or multi-node clusters.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: AutoML
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To ensure reproducibility in AutoML, the following criteria must be met:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: DL must be excluded.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `max_models` constraint rather than `max_runtime_secs` must be used.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a rule, time-based constraints are resource-limited. This means that AutoML
    may be able to train more models on one run than another if available compute
    resources differ between runs. Specifying the number of models to build will ensure
    reproducibility.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for reproducibility
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ensure reproducibility, think of the four requirement categories emphasized
    earlier: hardware, software, data, and settings. These categories are explained
    in more detail here:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware**: You should always document the hardware resources the H2O cluster
    is running on—this includes the number of nodes, CPU cores, and threads. (This
    information can be found in the log files.)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software**: You should document the version of H2O-3 or Sparkling Water used.
    (This information can be found in the log files.)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data**: Obviously, you must use the same input data. You should save all
    scripts that were used to process the data prior to model training. All data column
    modifications should be documented (for example, if you converted a numeric column
    to a categorical one).'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Settings**: Save the H2O logs and the H2O binary model. The logs contain
    a wealth of information. More importantly, the binary model contains the H2O version
    (software) and the parameters used to train the model (settings).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have rounded out our advanced modeling topics by showing
    how to build H2O models in Spark pipelines with a hands-on sentiment analysis
    modeling example. We summarized the unsupervised learning methods available in
    H2O and showed how to build an anomaly detection model using the isolation forest
    algorithm for a credit card fraud transaction use case. We also reviewed how to
    update models, including refitting versus checkpointing, and showed requirements
    to ensure model reproducibility.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127), *Understanding
    ML Models,* we discuss approaches for understanding and reviewing our ML models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
