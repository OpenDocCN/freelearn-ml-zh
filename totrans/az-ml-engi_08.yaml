- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the recent research areas to emerge in **artificial intelligence** (**AI**)
    is making models responsible and accountable, thus producing accurate results,
    as opposed to biased or incomplete results. This is a new area of computer science,
    but it is also something many in the data science field are looking into. Microsoft
    is concentrating its efforts on a number of areas, including **Fairness**, **Reliability
    and Safety**, **Privacy** **and** **Security**, **Inclusiveness**, **Transparency**,
    and **Accountability**. Microsoft has provided a toolbox that can be used and
    applied to datasets and models to address these topics. In this chapter, we will
    be exploring what these terms mean and how Microsoft’s Responsible AI Toolbox
    can be leveraged to address these concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response AI Toolbox overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responsible AI dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretability dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responsible AI principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, there are six core principles – Fairness, Reliability and Safety,
    Privacy and Security, Inclusiveness, Transparency, and Accountability – that Microsoft
    has incorporated into their Responsible AI Toolbox. We will briefly explain these
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fairness** in the context of AI systems is a sociotechnical challenge that
    scientists and developers need to address to ensure that people are treated equally
    and to reduce unfairness relating to the specific use case we are building the
    model for. In a variety of domains and use cases, AI systems can be used to provide
    resources and opportunities, and through checking fairness we can ensure that
    we are not reinforcing existing stereotypes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if we are predicting what ethnic groups there are in a certain
    segment of the population and what their food preferences are, not all customer
    segments of cities or states have the same ethnic diversity. So, when we design
    the model, we have to take into consideration all ethnic groups, giving each equal
    distribution within the data, building fairness into the model. Otherwise, the
    model might be skewed by the ethnic group with the highest population available
    in the test set. This is also hard because all cities and states have unequal
    distributions of ethnic groups.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reliability and Safety**: One important thing to keep in mind is to ensure
    models are safe for others to consume and that the output creates no harm. When
    a model makes predictions, make sure those predictions lead to safe and reliable
    decisions for all consumers. Understand that a wrong prediction can have an adverse
    negative effect when applying the model’s outcome. Spend time making the model’s
    outcome safe and reliable for the use case that it has been designed and developed
    for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy and Security**: Use cases that depend on human-related data, such
    as predicting patient diagnosis or attrition analysis for companies, use a lot
    of personal data. Proper precautions should be in place to protect **personally
    identifiable information** (**PII**) and privacy. Depending on where the model
    is run, try to run the model closer to where data is stored, and do not move it
    or copy it to ensure privacy. Also, make sure permission is granted individually
    to each person who needs access to the data and not everyone. In some cases, we
    can either encrypt the data or convert PII into something unidentifiable before
    building the model, if possible. There is also talk about synthetic data creation
    for modeling. Depending on the use case, the business problem we are attempting
    to solve must drive the decision on how to protect privacy and apply security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inclusiveness** is a core value stating that systems should empower all people,
    users and customers alike. The ability of the AI system to work with every human
    being on the planet regardless of who they are is always the goal. Inclusiveness
    is very similar to fairness. The question to ask is how we can include all of
    the various groups or include all ethnic minority communities in the model, ensuring
    no one is left out. Our dataset should represent all communities so that everyone
    gets proper representation when building our model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency** in a system is key to ensuring a system can be trusted. The
    system should be transparent, allowing users to know what is going on at any given
    time. This improves trust in the system. We can talk about two different categories
    of transparency:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency on how AI systems are built**: We need to be transparent on
    how the AI systems are built and provide proper documentation on the process and
    purpose of the model we’ve built. Anyone using the model should feel comfortable
    and able to trust the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency on pitfalls of the models**: Using interpretability to explain
    how the decision was made and what features were used is also an important way
    to gain trust in the model to be consumed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability**: Given that model building is somewhat unpredictable, we
    must be accountable for what we are building. Proper processes must be in place
    in order for us to accept accountability for the model we build. We should make
    sure that what we build takes privacy into account and that we accept responsibility
    for the trustworthiness of the model. For example, a data scientist who is building
    a model is responsible for building the model by following all of these principles
    and will be held accountable for that model. First and foremost, every organization
    has to have the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A responsible AI strategy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A methodology to develop AI/**machine learning** (**ML**) responsibly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to have a responsible AI strategy, you need to develop business guidelines
    relating to how AI/ML can be used to drive business outcomes, establish how teams
    can apply responsible AI and build better model outcomes, and decide what tools
    and techniques need to be in place to ensure responsible AI. This strategy should
    be the foundation of all AI/ML model development and should be used in any other
    systems. You should also create a framework around which to report and monitor
    risks or leaked AI/ML behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section of this chapter, we will discuss how to leverage **raiwidgets**
    (or the **Responsible AI Toolkit SDK**) and how we can analyze the six core principles.
    Not all of these principles are covered by the SDK. For example, while Fairness
    and Transparency are covered by the SDK, Privacy and Security, Reliability, and
    Accountability will be covered by other programming mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow this URL for more details and up-to-date information: [https://www.microsoft.com/en-us/ai/responsible-ai](https://www.microsoft.com/en-us/ai/responsible-ai).'
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI Toolbox overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest challenges we face in data science is understanding what
    the model does. For example, if the algorithms we use are all black boxes, it’s
    not that easy to know how the decisions are made. To discern how our algorithms
    make decisions, we can make use of responsible AI. This will give us the opportunity
    to explain the model’s decisions, find the features that contribute to the prediction,
    do error analysis on the dataset, and also ensure fairness in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft recently developed a Responsible AI Toolbox that encompasses interpretability,
    fairness, counterfactual analysis, and causal decision-making through three dashboards:
    a **fairness dashboard**, an **error analysis dashboard**, and an **interpretability
    dashboard**.'
  prefs: []
  type: TYPE_NORMAL
- en: Dashboards simplify the **user interface** (**UI**) by bringing all the toolkit
    output into one UI. Before the toolbox, it was hard because we needed to download
    a separate library and build code for each of the dashboards. Now, it’s very easy,
    and I will show some code to achieve this with the Responsible AI Toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The toolbox is in public preview, and there are new developments on the way.
    Make sure your libraries are updated often to check which new features are available
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Toolbox is available here: [https://github.com/microsoft/responsible-ai-toolbox](https://github.com/microsoft/responsible-ai-toolbox).'
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To leverage the Toolbox, we will create a model to apply the Toolbox to. Let’s
    look at the process of creating and analyzing responsible AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the steps to create a model in the Azure ML service:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure ML Studio UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the compute instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Notebook** in the **Author** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new notebook with Python 3.8 with Azure ML as the kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the notebook `RAIDashboard.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to install or upgrade libraries. Only install libraries if they
    aren’t already on your system. At the time of writing, Python 3.8 with the Azure
    ML kernel was used, and it already has `raiwidgets` installed. If there is an
    older version, please use the following upgrade command to upgrade to the latest
    version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Install or upgrade the Responsible AI Toolbox](img/B18003_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Install or upgrade the Responsible AI Toolbox
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s upgrade the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Upgrade the pandas libraries to the newest version](img/B18003_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Upgrade the pandas libraries to the newest version
  prefs: []
  type: TYPE_NORMAL
- en: Restart the kernel and clear all the outputs. Once the libraries are installed,
    it’s always a good practice to restart the kernel to reflect the installed libraries.
    Some packages force us to restart the kernel to be effective.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are now going to load the libraries for us to consume in the notebook. The
    Responsible AI dashboard is used to generate a nice UI to interact with and do
    **what-if analysis**, as well as other types of analysis, including **error analysis**,
    **feature permutation**, and **counterfactual analysis**, on the model created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For `RAIInsights`, this is where we calculate all the calculations to generate
    the dashboard. It usually takes time to execute and analyze the model and get
    insights into what the model is doing. Whatever is displayed on the Responsible
    AI dashboard will be calculated by the `RAIInsights` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Importing the Responsible AI libraries](img/B18003_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Importing the Responsible AI libraries
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a sample regression model to which to apply Responsible AI.
    Let’s import the `shap` and `sklearn` libraries for our model. Here, we are going
    to load the ML libraries and also the library required to split the data for training
    and testing. Later, we will split the dataset into two parts: one for training
    and one for testing. Usually, 80% is used for training and 20% is used for testing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Importing the model’s libraries](img/B18003_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Importing the model’s libraries
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s load the sample datasets and split the data for training and testing.
    Also, let’s assign the column that is used for the target feature, or label the
    column to be predicted. Here, we are loading a sample dataset for diabetes. Then,
    we are going to assign the column to predict, which is `'y'`. Sometimes, it’s
    called `target_feature`. Then, we use the remaining columns as features to train
    the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Loading the sample dataset](img/B18003_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Loading the sample dataset
  prefs: []
  type: TYPE_NORMAL
- en: Split the data and assign it to different dataframes for modeling. Usually,
    when modeling, the training data is split into two datasets, one for training
    and one for testing. Next, the predicting column, which is called the label or
    target, is also split into training and testing. The following screenshot shows
    the features used for modeling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Splitting data for training and testing](img/B18003_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Splitting data for training and testing
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have made our model, we need to run it. This is necessary before
    we analyze the model using the Responsible AI Toolbox for fairness, privacy, reliability,
    and bias. Bias means we want to have equal records for the categories we use.
    For example, if we use a dataset that contains ethnicity or sex features, we have
    to make sure we have an equal amount of data for each represented group to avoid
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now create the responsible AI code by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to train the model before we pass it to `RAIInsights` to calculate
    the fairness, bias, and other error analyses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Model configuration and training](img/B18003_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Model configuration and training
  prefs: []
  type: TYPE_NORMAL
- en: The model is ready. The next step is to apply responsible AI. Let’s configure
    the `RAIInsights` details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to pass the following parameters: `model`, `train_data`, `test_data`,
    `target_feature`, the model type (for example, `regression` or `classification`),
    and `categorical_features`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, `RAIInsights` is going to produce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An error analysis dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An interpretability dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fairness dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Responsible AI dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Invoking RAIInsights](img/B18003_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Invoking RAIInsights
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to invoke interpretability, error analysis, and fairness and
    build the dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IntepretML` is used to understand how the columns affect the prediction made
    by the model. `IntepretML` is an open source package. Here, we combine all the
    open source package output, making it seamless, and show it in `ResponsibleAIDashboard`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Error Analysis` package for this.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DiCE` is the open source package that we use to do this analysis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EconML` open source package. This package allows us to change conditions and
    see the impact. For example, what would the impact be if we introduced a new product
    to a customer segment or if we introduced new strategies into the company?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Adding explainer, error analysis, and counterfactual packages](img/B18003_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Adding explainer, error analysis, and counterfactual packages
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s carry out interpretability, error, and counterfactual analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, it will take some time to do all the calculations. Depending on the
    model type and the size of the dataset, be prepared to spend a little while here.
    Depending on the ML type, causal analysis using `EconML` will also be computed
    for what-if analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Compute responsible AI calculation insights](img/B18003_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Compute responsible AI calculation insights
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to build the Responsible AI dashboard. The Toolbox consolidates
    all the output in a dashboard and provides one place to conduct analysis. Please
    remember that not all the principles have been implemented yet, and research and
    development is still an ongoing process with the SDK. *Figure 8**.11* shows the
    creation of the Responsible AI dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Responsible AI dashboard](img/B18003_08_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Responsible AI dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Once the dashboard is built, click on the hyperlink to open it. For each type
    of analysis, please see the heading in the dashboard link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The link is available here: `responsible-ai-toolbox/tour.ipynb at main ·` `microsoft/responsible-ai-toolbox`
    ([https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/tour.ipynb](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/tour.ipynb)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dashboard created from the preceding code will have a few options to customize
    and work with cohorts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dashboard navigation**: Allows us to filter any analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cohort settings**: Allows us to work with cohorts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Switch global cohort**: Allows us to switch between cohorts and also display
    statistics in popups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create new cohort**: Allows us to create new cohorts as needed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we are going to deep dive into the error analysis dashboard to understand
    feature errors.
  prefs: []
  type: TYPE_NORMAL
- en: Error analysis dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Error analysis allows us to analyze where the model is underperforming and also
    find errors in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Error analysis can be done as either a tree map or a heat map. Based on the
    analysis, red represents an error. If you click one of the insight bubbles, you’ll
    see the path the model traversed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dashboard analysis will be based on the type of ML modeling we choose. The
    two types are classification and regression. In our example, we chose regression.
    The following are two of the accuracy metrics used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean** **squared error**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean** **absolute error**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following screenshot shows the error analysis dashboard UI created by the
    Responsible AI SDK.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Error analysis dashboard](img/B18003_08_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Error analysis dashboard
  prefs: []
  type: TYPE_NORMAL
- en: This dashboard provides options to save the error analysis for further analysis
    or share it with others. This feature is very helpful if we need to share information
    with other data scientists or subject matter experts to understand a model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the error analysis screen with **Heat** **map**
    selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Heat map error analysis](img/B18003_08_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Heat map error analysis
  prefs: []
  type: TYPE_NORMAL
- en: In this heat map, we need to select the features for which we want to do error
    analysis. Once we select the features, the system will analyze the error percentage.
  prefs: []
  type: TYPE_NORMAL
- en: Options are available for **Quantile binning** and **Binning threshold**.
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows us **Feature List**. This section shows which
    features have been selected and how important each one is to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Feature List](img/B18003_08_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Feature List
  prefs: []
  type: TYPE_NORMAL
- en: This feature illustrates the importance of the columns or features to the predictions.
    There are options to change the maximum depth and leaves of the decision tree
    created by the responsible AI dashboard, as well as how the minimum number of
    samples in one leaf is analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s look at the interpretability section of the dashboard. This is where
    each feature is analyzed and the importance and impact of the features are shown.
    The top features are either aggregated or available individually for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Aggregated feature importance](img/B18003_08_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – Aggregated feature importance
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see the aggregated feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select **Individual feature importance** to analyze the data row by row. Then,
    select the rows to analyze, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Selecting data points for feature importance](img/B18003_08_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Selecting data points for feature importance
  prefs: []
  type: TYPE_NORMAL
- en: Once the feature data points are selected, scroll down to see the charts. The
    following chart shows the feature importance and its impact on the row that is
    selected. For every row in the dataset, you can analyze the dataset by selecting
    a row.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Individual feature importance plot](img/B18003_08_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Individual feature importance plot
  prefs: []
  type: TYPE_NORMAL
- en: Now click **Individual conditional expectations (ICE) plot** to change the plot.
    The ICE plot will show all the rows with respect to the feature selected and show
    how much impact each row had on the model.
  prefs: []
  type: TYPE_NORMAL
- en: On selecting **Individual conditional expectation (ICE) plot**, you should see
    what is shown in *Figure 8**.17*. The *x* axis should be the feature selected
    and the *y* axis the prediction. Click the **Feature** dropdown and select the
    feature to see the impact. Also, options to set minimum and maximum steps are
    available. To get more information, click **How to read** **this chart**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Individual conditional expectation (ICE) plot](img/B18003_08_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Individual conditional expectation (ICE) plot
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at model statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Model statistics allow us to analyze distribution across prediction values,
    the model’s performance, and model metrics. Options to switch between a cohort
    and a dataset are also available. Select the *y* value to see the error in the
    dataset, the predicted Y value (the label for the column that the model predicted),
    and what the true Y value (label for the column provided in the training dataset)
    was. These options allow us to understand the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: You can also swap the axes to show errors on the x axis and predictions on the
    Y axis. Also, if you click **Cohort**, there is the option to select a dataset
    and the features to plot and see how the model performed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Model statistics](img/B18003_08_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Model statistics
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to look into the **Data explorer** feature on the Responsible
    AI dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Data explorer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data explorer** is another visual tool for analyzing the predicted value,
    error, and features. You can use either aggregated or individual features. It
    also allows you to select predicted features and training features for what-if
    analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.20* shows a box plot of how age is grouped or binned into chunks.
    The *x* axis is the index and the *y* axis is the age groups, which can be age
    data points or bins of ages. You have the choice to select either **Aggregate
    plots** or **Individual datapoints** to visualize the difference between aggregated
    and individual plots. Box plots provide a clear visualization of where the outliers
    are and show the range of plotted points.'
  prefs: []
  type: TYPE_NORMAL
- en: We can change the *x* axis from the index to the predicted *y* or true *y* value
    to see the patterns. There is also the option to change the number of bins. In
    the following example, we chose five bins. Also, if you click **age**, there is
    the option to change the feature as well to see how the prediction value changes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Aggregated plots](img/B18003_08_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Aggregated plots
  prefs: []
  type: TYPE_NORMAL
- en: '**Data explorer** allows us to select the predicted and original values and
    plot either aggregated or individual scatter plots to see how the values align
    or intersect. In the following chart, the *y* axis is the age and the *x* axis
    is the index, and the predicted outcome is a scatter plot. You can change the
    cohort to see various values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Scatter plot for individual values](img/B18003_08_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – Scatter plot for individual values
  prefs: []
  type: TYPE_NORMAL
- en: The preceding charts provide a way to visualize the predicted and error values
    of our model. This enables us to build a better model with more accuracy that
    is more realistic.
  prefs: []
  type: TYPE_NORMAL
- en: What-if counterfactuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we can change the datapoints and click **Create what-if counterfactuals**.
    What-if analysis allows us to select the predictor and compare with features to
    see the impact on the outcome. We can also change the index and age and switch
    to see predicted *y* or true *y* values and the features to see the counterfactuals.
    Then, we can select the index value and select one to see how the charts change.
  prefs: []
  type: TYPE_NORMAL
- en: We can see a counterfactual chart in the following screenshot and analyze the
    chart by changing features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Counterfactuals](img/B18003_08_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – Counterfactuals
  prefs: []
  type: TYPE_NORMAL
- en: Select the index and then click **Create what-if counterfactual** to see the
    features percentage in the distribution chart. You can also save the chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.23 – What-if counterfactuals](img/B18003_08_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 – What-if counterfactuals
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to see how fairness can be analyzed using
    the SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fairness is a topic that we need to investigate in use cases where people are
    involved. Proper precautions should be taken to figure out whether the dataset
    is fair or not. In Azure ML, with the Responsible AI Toolbox, we can create a
    fairness dashboard. To do that, first, we need to know which features need to
    be fair, such as sex and race. Once we know that, we can create a dashboard, as
    shown in *Figure 8**.24*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to create a fairness dashboard with a sample
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure ML Studio UI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the compute instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Notebook** in the **Author** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new notebook with Python 3.8 with Azure ML as the kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new notebook called `FairnessDashboard`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import all the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Fairness imports](img/B18003_08_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 – Fairness imports
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to load the sample dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Getting the sample data](img/B18003_08_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – Getting the sample data
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we are going to specify the sensitive features and then use categorical
    columns and label encoder to convert strings to numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.26 – Feature engineering](img/B18003_08_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – Feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have feature engineering taken care of, the next step is to split
    the dataset for training and testing. We will split the dataset into 80% for training
    and 20% for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.27 – Splitting data for training and testing](img/B18003_08_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.27 – Splitting data for training and testing
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, configure the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.28 – Training](img/B18003_08_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.28 – Training
  prefs: []
  type: TYPE_NORMAL
- en: Train the model with the dataset. Here, we are using logistic regression for
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s create a fairness dashboard. A fairness dashboard needs these three
    parameters at a minimum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitive features** – columns that have sensitive data, such as sex or race'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Original** **label value**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted value** **to analyze**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Click the URL created by the responsible AI SDK. A new web page will open. Follow
    the navigation in the dashboard to select the sensitive column and see how the
    data is distributed across those features. The main aim here is to provide insights
    into how the features are balanced with sensitive information. For example, if
    we have data with male and female datapoints, we want to make sure that men and
    women have the same amount of representation in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.29 – Creating the fairness dashboard](img/B18003_08_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.29 – Creating the fairness dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the main page of the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.30 – Fairness dashboard – intro page](img/B18003_08_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.30 – Fairness dashboard – intro page
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Get started** to view the options to analyze fairness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step of the process is to select the sensitive features. In the following
    screenshot, we have two columns, **sex** and **race**. In the **sex** column,
    we have two categories, **male** and **female**. For **race**, we have **white**,
    **black**, **Asian-Pac-Islander**, **Other**, and **Amer-Indian-Eskimo**. Let’s
    first select **sex** to examine the sensitive features, and then click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.31 – Fairness dashboard – 01 Sensitive features](img/B18003_08_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.31 – Fairness dashboard – 01 Sensitive features
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s choose which performance metric to use for our analysis. Options
    for performance metrics are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accuracy**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Balanced accuracy**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-score**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The definitions of these metrics are provided in *Figure 8**.32*. For our purposes,
    we are going to choose **Accuracy** as the metric, with **Sex** as the sensitive
    column for us to analyze. Click **Next** to go to **Fairness metrics**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.32 – Fairness dashboard – 02 Performance metrics](img/B18003_08_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.32 – Fairness dashboard – 02 Performance metrics
  prefs: []
  type: TYPE_NORMAL
- en: Next is one of the most important screens. Select the appropriate fairness metric
    for our analysis. In the following screenshot, you can see the choice. For our
    example, we are choosing **Demographic parity difference**. Then click **Next**
    to see the charts to analyze.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.33 – Fairness dashboard – 03 Fairness metrics](img/B18003_08_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.33 – Fairness dashboard – 03 Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen, we can change these selections, see how the fairness metrics
    are calculated, and examine which sensitive features are selected and how fair
    they are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.34 – Fairness results](img/B18003_08_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.34 – Fairness results
  prefs: []
  type: TYPE_NORMAL
- en: The preceding chart shows that women are underrepresented in the model, which
    also suggests the model might be biased toward men. So, now we can go back and
    see whether we can get a more balanced dataset to make our model fairer. Try to
    switch the charts to false positive and false negative rates to drill down and
    see the rates of false positives and negatives. You can also change **Performance
    metric**, **Sensitive feature**, and **Fairness metric** and analyze the fairness.
    As these options change, the charts will reflect the changes. This tool provides
    a single place to analyze data for bias and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.35 – Fairness results – False positive and false negative rates](img/B18003_08_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.35 – Fairness results – False positive and false negative rates
  prefs: []
  type: TYPE_NORMAL
- en: We can now drill deeper into black-box models and see how the model made decisions,
    and also analyze the dataset for any bias or errors and ensure fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using the Responsible AI Toolbox SDK, we can analyze data for fairness and
    errors and look deep into decision trees to understand how the model makes decisions.
    Note that there is work to be done in this field. The SDK is still going through
    development, and features are being added, so please remember that the functionality
    will change and new features will be added. At the time of writing, we tested
    with the LightGBM, XGBoost, and PyTorch algorithms for fairness. The Toolbox allows
    us to open black-box models and see how decisions are made, and also produce output
    that is fair and unbiased.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to productionalize ML models.
  prefs: []
  type: TYPE_NORMAL
