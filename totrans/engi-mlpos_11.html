<html><head></head><body>
		<div id="_idContainer122">
			<h1 id="_idParaDest-152"><a id="_idTextAnchor176"/>Chapter 9: Testing and Securing Your ML Solution</h1>
			<p>In this chapter, we will delve into <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) solution testing and security aspects. You can expect to get a primer on various types of tests to test the robustness and scalability of your ML solution, as well as the knowledge required to secure your ML solution. We will look into multiple attacks on ML solutions and ways to defend your ML solution.</p>
			<p>In this chapter, we will be learning with examples as we perform load testing and security testing for the business use case of weather prediction we have been previously working on. We will start by reflecting on the need for testing and securing your ML solution and go on to explore the other following topics in the chapter:</p>
			<ul>
				<li><a id="_idTextAnchor177"/>Understanding the need for testing and securing your ML application</li>
				<li>Testing your ML solution by design</li>
				<li>Securing your ML solution by design</li>
			</ul>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor178"/>Understanding the need for testing and securing your ML application</h1>
			<p>The growing <a id="_idIndexMarker646"/>adoption of data-driven and ML-based solutions is <a id="_idIndexMarker647"/>causing businesses to have to handle growing workloads, exposing them to extra levels of complexities and vulnerabilities.</p>
			<p>Cybersecurity is the most alarming risk for AI developers and adopters. According to a survey released by Deloitte (<a href="https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html">https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html</a>), in July 2020, 62% of adopters saw cybersecurity risks as a significant or extreme threat, but only 39% said they felt prepared to address those risks.</p>
			<p>In this section, we will look into the need for securing ML-based systems and solutions. We will reflect on some of the broader challenges of ML systems such as bias, ethics, and explainability. We will also <a id="_idIndexMarker648"/>study some of the challenges present at each stage of the ML life cycle relating <a id="_idIndexMarker649"/>to confidentiality, integrity, and availability using the guidelines for ML testing and security by design.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor179"/>Testing your ML solution by design</h1>
			<p>On top of performing regular software development tests, such as unit tests, integration tests, system testing, and <a id="_idIndexMarker650"/>acceptance testing, ML solutions need additional tests because data and ML models are involved. Both the data and models change dynamically over time. Here are some concepts for testing by design; applying them to your use cases can ensure robust ML solutions are produced as a result.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor180"/>Data testing</h2>
			<p>The goal of testing <a id="_idIndexMarker651"/>data is to ensure that the data is of a high enough quality for ML model training. The better the quality of the data, the better the models trained for the given tasks. So how do we assess the quality of data? It can be done by inspecting the following five factors of the data:</p>
			<ul>
				<li>Accuracy</li>
				<li>Completeness (no missing values)</li>
				<li>Consistency (in terms of expected data format and volume)</li>
				<li>Relevance (data should meet the intended need and requirements)</li>
				<li>Timeliness (the latest or up-to-date data)</li>
			</ul>
			<p>Based on these factors, if a company can manage each dataset's data quality when received or created, the data quality is guaranteed. Here are some steps that your team or company can use as quality assurance measures for your data:</p>
			<ol>
				<li value="1"><strong class="bold">Meticulous data cataloging and control of incoming data</strong>: A combination of data cataloging (to document and store data in the required format or pattern) and control functions can ensure a high quality of incoming data. Data cataloging and control can be done by monitoring data factors such as data formats and patterns, value distributions and anomalies, completeness, and consistency can help to provide good incoming data quality.</li>
				<li><strong class="bold">Curating data pipelines carefully to avoid duplicate data</strong>: When duplicated data is manufactured from the same data source and using the same logic by different people, it can get complicated to manage lineages, authenticity, and data integrity. This can produce cascading effects throughout multiple systems or databases. It is better to avoid duplicating data as much as possible.</li>
				<li><strong class="bold">Data governance with enforced integrity</strong>: In today's world, maintaining data integrity has become crucial. Not having the mindset of enforcing data integrity can be costly for an <a id="_idIndexMarker652"/>organization. The data could eventually become incomplete, delayed, or out of date, leading to serious data quality issues.</li>
				<li><strong class="bold">Maintaining end-to-end traceability and lineage</strong>: Data lineage and traceability can be achieved by the smart use of metadata and the data itself. Using both, we can document critical information such as unique keys for each dataset, adding a timestamp to each record, and logging data changes. Making sure data lineage and end-to-end traceability is enabled can give us the possibility to reproduce models and debug errors and pipelines.</li>
			</ol>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor181"/>Model testing</h2>
			<p> Model <a id="_idIndexMarker653"/>tests need to cover server issues such as the following:</p>
			<ul>
				<li>Evaluating the accuracy or key metric of the ML model</li>
				<li>Testing on random data points</li>
				<li>Testing for acceptable loss or performance on your task</li>
				<li>Unit tests for model robustness using real data</li>
			</ul>
			<p>These tests can be orchestrated in two phases: pre-training and post-training. Having these tests facilitated <a id="_idIndexMarker654"/>in the workflow can produce robust models for production. Let's look at what pre-train and post-train tests can be done by design.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor182"/>Pre-training tests</h2>
			<p>Tests can be performed to catch flaws <a id="_idIndexMarker655"/>before we proceed to the training stage. These flaws could be in the data, pipelines, or parameters. <em class="italic">Figure 9.1</em> suggests running pre-training and post-training tests as part of a proposed workflow for developing high-quality models:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B16572_09_01.jpg" alt="Figure 9.1 – Proposed workflow for developing high-quality models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Proposed workflow for developing high-quality models</p>
			<p>Here are some ways to detect and avoid pre-training flaws using pre-training tests:</p>
			<ul>
				<li>Eliminating data pipeline debt by handling any data leakage, edge cases, and optimizing to make the pipeline time- and resource-efficient</li>
				<li>Making sure the shape of your model output matches the labels in your dataset</li>
				<li>Examining the output ranges to make sure they match our expectations (such as checking that the output of a classification model is a distribution with class probabilities that sum to 1)</li>
				<li>Examining your training and validation datasets for label leakage</li>
				<li>Making sure the ETL pipeline outputs or fetches data in the required format</li>
			</ul>
			<p>Pre-training tests do not need <a id="_idIndexMarker656"/>parameters to run, but they can be quite useful in catching bugs before running the model training.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor183"/>Post-training tests</h2>
			<p>Post-training tests enable us to investigate model performance and the logic behind model <a id="_idIndexMarker657"/>predictions and foresee any possible flaws in the model before deploying the model to production. Post-training tests enable us to detect flaws in model performance and functionality. Post-training tests involve a model performance evaluation test, invariance test, and minimum functionality test. Here is a recommended read for more insights on post-training tests: <em class="italic">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</em> (<a href="https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf">https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf</a>)</p>
			<p>Deployment and inference testing</p>
			<p>Deployment <a id="_idIndexMarker658"/>testing involves testing <strong class="bold">Continuous Integration/Continuous Delivery</strong> (<strong class="bold">CI/CD</strong>) pipeline delivery, integration <a id="_idIndexMarker659"/>tests, and testing that <a id="_idIndexMarker660"/>deployment is successful. It is critical to test the deployed model and that is where inference testing comes in to stress- or load-test the deployed model and test its performance on real-time data.</p>
			<p>In the next section, we will load test a previously deployed model (for a use case).</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor184"/>Hands-on deployment and inference testing (a business use case)</h1>
			<p>When you have your <a id="_idIndexMarker661"/>service (either API or ML) ready and you are about to serve it to the users but you don't have any clue about how many users it can actually handle and how it will react when many users access it simultaneously, that's where load testing is useful to benchmark how many users your service can serve and to validate whether the service can cater to the business requirements.</p>
			<p>We will perform load testing for the service we deployed previously (in <a href="B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Building Robust CI and CD Pipelines</em>). <strong class="source-inline">Locust.io</strong> will be used for load testing. <strong class="source-inline">locust.io</strong> is an open source load-testing tool. For this, we will install <strong class="source-inline">locust</strong> (using <strong class="source-inline">pip</strong>) and curate a Python script using the locust.io SDK to test an endpoint. Let's get started by installing <strong class="source-inline">locust</strong>:</p>
			<ol>
				<li value="1">Install <strong class="source-inline">locust</strong>: Go to your terminal and execute the following command:<p class="source-code"><strong class="bold">pip install locust</strong></p><p>Using <strong class="source-inline">pip</strong>, <strong class="source-inline">locust</strong> will be installed – it takes around a minute to install. After installation is successful it's time to curate the Python script using the <strong class="source-inline">locust.io</strong> SDK to test an endpoint.</p></li>
				<li>Curate the <strong class="source-inline">load_test.py</strong> script: Go to your favorite IDE and start curating the script or follow the steps in the premade script. To access the premade script go to the <em class="italic">Engineering MLOps</em> repository cloned previously, access the <strong class="source-inline">09_Testing_Security</strong> folder, and <a id="_idIndexMarker662"/>go to the <strong class="source-inline">load_test.py</strong> file. Let's demystify the code in <strong class="source-inline">load_test.py</strong> – firstly, the needed libraries are imported as follows:<p class="source-code">import time</p><p class="source-code">import json</p><p class="source-code">from locust import HttpUser, task, between</p><p>We imported the <strong class="source-inline">time</strong>, <strong class="source-inline">json</strong>, and <strong class="source-inline">locust</strong> libraries, and then from <strong class="source-inline">locust</strong> we import the following required functions: <strong class="source-inline">HttpUser</strong> (a user agent that can visit different endpoints), <strong class="source-inline">task</strong>, and <strong class="source-inline">between</strong>. </p></li>
				<li>Create a <strong class="source-inline">test_data</strong> variable with sample test data to infer the ML model during the load test. Define <strong class="source-inline">headers</strong> we will use for the API calls in our load test:<p class="source-code">test_data = json.dumps({"data": [[8.75, 0.83, 70, 259, 15.82, 1016.51, 1.0]]})</p><p class="source-code">headers = {'Content-Type': 'application/json'}</p></li>
				<li>Next, we will implement the core functionality of the load test as part of the <strong class="source-inline">MLServiceUser</strong> class (you can name it whatever you want) by extending <strong class="source-inline">HttpUser</strong>. <strong class="source-inline">HttpUser</strong> is the user agent that can visit different endpoints:<p class="source-code">class MLServiceUser(HttpUser):</p><p class="source-code">    wait_time = between(1, 5)</p><p class="source-code">    @task</p><p class="source-code">    def test_weather_predictions(self):</p><p class="source-code">        self.client.post("", data=test_data, headers=headers)</p><p>We <a id="_idIndexMarker663"/>created a <strong class="source-inline">wait_time</strong> variable using the <strong class="source-inline">between()</strong> function, which specifies the time it takes between finishing testing one endpoint and switching to test the next endpoint. So, we specify <strong class="source-inline">wait_time</strong> as <strong class="source-inline">1</strong> to <strong class="source-inline">5</strong> seconds in the <strong class="source-inline">between(1,5)</strong> function. The next part is the crux of defining a task that tests an endpoint.</p><p>For this, we use a <strong class="source-inline">@task</strong> wrapper or decorator to start defining our task to test an endpoint of our choice using a custom function. Define a custom function, <strong class="source-inline">def</strong> <strong class="source-inline">test_weather_predictions()</strong>, and make a <strong class="source-inline">post</strong> request to the endpoint using <strong class="source-inline">test_data</strong> and the headers defined previously. Now, we are set to run the load testing!</p></li>
				<li>Run the <strong class="source-inline">locust.io</strong> server: Go to your terminal and change to the location where you have the <strong class="source-inline">load_test.py</strong> file (such as in the <strong class="source-inline">09_Testing_Security</strong> folder of the cloned repository used in this book), then run the following command to spin up a <strong class="source-inline">locust.io</strong> server:<p class="source-code">Locust -f load_test-py</p><p>The execution of the previous command will spin up the <strong class="source-inline">locust</strong> server at port <strong class="source-inline">8089</strong>. We can perform load tests on the web interface rendered by <strong class="source-inline">locust.io</strong>. To access the web service, open a browser of your choice and go to the following web address: <strong class="source-inline">http://0.0.0.0:8089/</strong>, as shown in <em class="italic">Figure 9.2</em>:</p><div id="_idContainer115" class="IMG---Figure"><img src="image/B16572_09_02.jpg" alt="Figure 9.2 – Access the Locust.io web service&#13;&#10;"/></div><p class="figure-caption">Figure 9.2 – Access the Locust.io web service</p></li>
				<li>Run the <a id="_idIndexMarker664"/>load test: Opening the web service will prompt you to specify options such as the number of users, spawn rate, and host (the endpoint to test). Specify the number of users to simulate, and the spawn rate (how many users will be spawned per second) as per your requirements, to validate whether your endpoint is capable of serving your business/user needs, for example, 50 users and a spawn rate of 50.</li>
				<li>Lastly, enter the endpoint or host you would like to load test and hit <strong class="bold">Start swarming</strong> to start performing the load test. In <a href="B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Building Robust CI and CD Pipelines</em>, we deployed an endpoint. It is recommended to test the deployed endpoint. </li>
				<li>Go to your Azure ML workspace, access the <strong class="bold">Endpoints</strong> section, access the deployed endpoint named <strong class="bold">dev-webservice</strong>, and copy and paste the endpoint web address into the host textbox. </li>
				<li>Next, click <strong class="bold">Start</strong> <strong class="bold">swarming</strong> to start load testing the endpoint. This will start the load test and open a new page where you can monitor your load tests in real time as shown in <em class="italic">Figure 9.3</em>:<div id="_idContainer116" class="IMG---Figure"><img src="image/B16572_09_03.jpg" alt="Figure 9.3 – Monitor the load test in real time&#13;&#10;"/></div><p class="figure-caption">Figure 9.3 – Monitor the load test in real time</p></li>
				<li>Analyzing <a id="_idIndexMarker665"/>load testing results: You can monitor statistics, charts, failures, and exceptions in real time. For instance, in <em class="italic">Figure 9.3</em> we are monitoring the load test for the <strong class="bold">/score</strong> endpoint, which is inferred using a <strong class="source-inline">POST</strong> request with test data. The number of requests made (<strong class="bold">77459</strong>), the number of fails (<strong class="bold">0</strong>), the average response time (<strong class="bold">75ms</strong>), and other information can be monitored. It is important to check that there are no failures and that the average response time is within the range to serve your business/user needs with efficiency or no major speed breaker.<p>If you have no failed requests and the average response time is within the range required, then your endpoint has passed the load test and is ready to be served to users. After or during the load testing, you can view charts of the load-testing performance with critical information such as total requests per second, response times, and the number of users with the progression of time. We can view this information in real time as shown in <em class="italic">Figure 9.4</em> and <em class="italic">Figure 9.5</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B16572_09_04.jpg" alt="Figure 9.4 – Charts showing the total requests per second and response times&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – Charts showing the total requests per second and response times</p>
			<p>In <em class="italic">Figure 9.4</em> we can <a id="_idIndexMarker666"/>notice that the number of requests per second is in the range of 18-22 as the simulated users of <strong class="source-inline">locust.io</strong> make requests, and the response time in milliseconds varies from 70 to 500 in some cases, with a 430ms variance between the minimum and maximum. The average request time is 75ms (as seen in <em class="italic">Figure 9.3</em>).</p>
			<p>Please note that this kind of performance may or may not be ideal for a given use case, depending on your business or user needs. A more stable response time is desirable; for instance, a response time variance of no more than 50ms between the minimum and maximum response times may be preferable for a stable performance. To achieve such performance it is recommended to deploy your models on higher-end infrastructure as appropriate, for example, a GPU or a high-end CPU, unlike the deployment on a CPU in an Azure container instance. Similarly, in <em class="italic">Figure 9.5</em> we can see the response times versus the number of users:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B16572_09_05.jpg" alt="Figure 9.5 – Charts showing the total requests per second and the number of users&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Charts showing the total requests per second and the number of users</p>
			<p>We can see that the <a id="_idIndexMarker667"/>number of users spawned per second is 50, as mentioned (in <em class="italic">Figure 9.2</em>). As time progresses, the spawn rate is constant and response times vary between 70-500ms, with 75ms as the average response time.</p>
			<ol>
				<li value="1">Document or download results: After the load test has been executed successfully you can document or present the results of the load test to the relevant stakeholders (QA/product manager) using a test report. To download or access the test report, go to the <strong class="bold">Download Data</strong> section and download the required information, such as request statistics, failures, or exceptions, in the form of <strong class="source-inline">.csv</strong> files, as shown in <em class="italic">Figure 9.6</em>:</li>
			</ol>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B16572_09_06.jpg" alt="Figure 9.6 – Download the test results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Download the test results</p>
			<p>Download the <a id="_idIndexMarker668"/>statistics or failure reports required for further inspection as per your needs, and note that you can access the full test report by clicking <strong class="bold">Download the Report</strong>, as shown in <em class="italic">Figure 9.7</em>:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B16572_09_07.jpg" alt="Figure 9.7 – Download the test results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Download the test results</p>
			<p>A comprehensive test report is presented with critical information, such as the endpoint inferred average request time and the minimum and maximum request times, and this information is also presented in the form of visualized charts as seen in <em class="italic">Figure 9.7</em>. You can also download this full report to present to your respective stakeholders.</p>
			<p>Congratulations, you <a id="_idIndexMarker669"/>have performed a hands-on load test to validate your endpoint and check whether your ML service is able to serve your business or user needs with efficiency.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor185"/>Securing your ML solution by design</h1>
			<p>Securing your ML <a id="_idIndexMarker670"/>applications is more important than ever due to the growing adoption of AI to provide smart applications. Designing and developing ML systems without keeping security in mind can be costly in terms of exposing the system to hackers, leading to manipulation, data breaches, and non-compliance. Robustness and security play an important role in ensuring an AI system is trustworthy. To build trustworthy ML applications, keeping security in mind is vital to not leave any stones unturned.</p>
			<p><em class="italic">Figure 9.8</em> shows a framework for creating secure ML applications by design. The framework addresses key areas in the ML life cycle, ensuring confidentiality, integrity, and availability within those specific stages. Let's reflect upon each area of the ML life cycle and address the issues of confidentiality, integrity, and availability in each area:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B16572_09_08.jpg" alt="Figure 9.8 – Framework for securing the ML life cycle by design&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Framework for securing the ML life cycle by design</p>
			<p>Let's reflect upon each <a id="_idIndexMarker671"/>area of the ML life cycle and address confidentiality, integrity, and availability in each area while looking at the different types of attacks.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor186"/>Types of attacks</h2>
			<p>We will explore some of the most <a id="_idIndexMarker672"/>common attacks on ML systems. At a high level, attacks by hackers can be broken down into four categories: poisoning, input attacks and evasion, reverse engineering, and backdoor attacks. Let's see how attackers manage to infiltrate ML systems via these attacks.</p>
			<h3>Poisoning</h3>
			<p>A hacker or attacker seeks to compromise an AI model in a poisoning attack. Poisoning attacks can happen <a id="_idIndexMarker673"/>at any stage (training, deployment, or real-time inference). They occur typically in training and inference. Let's see how poisoning attacks are implemented in three typical ways:</p>
			<ul>
				<li><strong class="bold">Dataset poisoning</strong>: Training datasets contain the knowledge on which the model is trained. An <a id="_idIndexMarker674"/>attacker can manipulate this knowledge by infiltrating the training dataset. Here, the attacker introduces wrongly labeled or incorrect data into the training dataset, and with this the entire learning process is distorted. This is a direct way to poison a model. Training datasets can be poisoned during the data collection and curation phases, and it can be hard to notice or detect it as the training datasets can come from multiple sources, can be large, and also as the attacker can infiltrate within data distributions.</li>
				<li><strong class="bold">Algorithm poisoning</strong> happens when an attacker meddles with the algorithm used to train the <a id="_idIndexMarker675"/>model. It can be as simple as infiltrating hyperparameters or fiddling with the architecture of the algorithm. For example, let's take federated learning (which aims to preserve the privacy of individuals' data) where model training is done on multiple subsets of private data (such as healthcare data from multiple hospitals while preserving patients' confidential information). Multiple models are derived from each subset and then combined to form a final model. During this, an attacker can manipulate any subset of the data and influence the final resulting model. The attacker can also create a fake model from fake data and concatenate it with models produced from training on multiple subsets of private data to produce a final model that deviates from performing the task efficiently, or serves the attacker's motives.</li>
				<li><strong class="bold">Model poisoning</strong> occurs when an attacker replaces a deployed model with an alternative <a id="_idIndexMarker676"/>model. This kind of attack is identical to a typical cyber-attack where the electronic files containing the model could be modified or replaced.</li>
			</ul>
			<h3>Input attack and evasion</h3>
			<p>An input or <a id="_idIndexMarker677"/>evasion attack happens when the attacker modifies input <a id="_idIndexMarker678"/>to the ML system in <a id="_idIndexMarker679"/>such a manner that it causes the system to malfunction (or give a wrong prediction). These <a id="_idIndexMarker680"/>perturbations or changes can be hard to detect as these changes are very subtle or small.</p>
			<p>For example, input attacks are popular for computer vision algorithms. This can be done by just changing a few pixels in the input image. As a result, the system might identify an image in a way that it is not supposed to or make a wrong prediction. Such small changes can effectively manipulate the prediction resulting in wrong actions being taken by the system. As a result, the ML system behaves as it should, while the output is manipulated.</p>
			<p>ML systems are highly prone to input attacks. Hence, having an anomaly detector to monitor incoming data can be quite handy to avoid such perturbations in the incoming data. Irrespective of input data, the majority of classification models choose a valid class from their training. Another way of protecting the ML system is by preprocessing the inputs with a proxy binary model that tells you, for example, whether an input image is of a person or an animal before sending this image to the final image classifier.</p>
			<h3>Reverse engineering</h3>
			<p>For a user of <a id="_idIndexMarker681"/>an AI system, it can be a black box or <a id="_idIndexMarker682"/>opaque. It is common in AI systems to accept inputs to generate outputs without revealing what is going on inside (in terms of both the logic and algorithm). Training datasets, which effectively contain all the trained system's knowledge, are also usually kept confidential. This, in theory, makes it impossible for an outsider to predict why particular outputs are produced or what is going on inside the AI system in terms of the algorithm, training data, or logic. However, in some cases, these systems can be prone to reverse engineering. The attacker or hackers' goal in reverse engineering attacks is to replicate the original model deployed as a service and use it to their advantage.</p>
			<p>In a paper titled <em class="italic">Model Extraction Attacks against Recurrent Neural Networks</em> (<a href="https://arxiv.org/pdf/2002.00123.pdf">https://arxiv.org/pdf/2002.00123.pdf</a>), published in February 2020, researchers conducted experiments on model extraction attacks against an RNN and an LSTM trained with publicly available academic datasets. The researchers effectively reproduce the functionality of an ML system via a model extraction attack. They demonstrate that a model <a id="_idIndexMarker683"/>extraction attack with high accuracy can be extracted efficiently, primarily by replicating or <a id="_idIndexMarker684"/>configuring a loss function or architecture from the target model.</p>
			<p>In another instance, researchers from the Max Planck Institute for Informatics showed in 2018 how they were able to infer information from opaque models by using a sequence of input-output queries.</p>
			<h3>Backdoor attacks</h3>
			<p>In backdoor attacks, the <a id="_idIndexMarker685"/>attacks can embed patterns <a id="_idIndexMarker686"/>of their choice in the model in the tra<a id="_idTextAnchor187"/>ining or inference stages and infer the deployed model using pre-curated inputs to produce unexpected outputs or triggers to the ML system. Therefore, backdoor attacks can happen both in the training and inference phases, whereas evasion and poisoning attacks can occur in a single phase during training or inference.</p>
			<p>Poison attacks can be used as part of the attack in backdoor attacks, and in some instances, the student model can learn to hack some backdoors from the teacher model using transfer learning.</p>
			<p>Backdoor attacks can cause integrity challenges, especially in the training stage, if the attacker manages to use a poison attack to infiltrate training data and trigger an update to the model or system. Also, backdoor attacks can be aimed to degrade performance, exhaust or redirect resources that can lead to the system's failure, or attempt to introduce peculiar behavior and outputs from the AI system.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor188"/>Summary</h1>
			<p>In this chapter, we have learned the key principles of testing and security by design. We explored the various methods to test ML solutions in order to secure them. For a comprehensive understanding and hands-on experience, implementation was done to load test our previously deployed ML model (from <a href="B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Building Robust CI and CD Pipelines</em>) to predict the weather. With this, you are ready to handle the diverse testing and security scenarios that will be channeled your way.</p>
			<p>In the next chapter, we will delve into the secrets of deploying and maintaining robust ML services in production. This will enable you to deploy robust ML solutions in production. Let's delve into it.</p>
		</div>
	</body></html>