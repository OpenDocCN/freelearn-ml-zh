<html><head></head><body>
		<div id="_idContainer259">
			<h1 id="_idParaDest-175"><em class="italic"><a id="_idTextAnchor177"/>Chapter 10</em>: Outlier Detection </h1>
			<p>In the first section of this book, we discussed anomaly detection in depth, a feature that allows us to detect unusual behavior in time series data in an unsupervised fashion. This works well when we want to detect whether one of our applications is experiencing unusual latency at a particular time or whether a host on our corporate network is transmitting an unusual number of bytes. </p>
			<p>In this chapter, we will learn about the second unsupervised learning feature in the Elastic Stack: outlier detection, which allows us to detect unusual entities in non-time series-based indices. Some interesting applications of outlier detection could involve, for example, detecting unusual cells in a tissue sample, investigating unusual houses, or areas in a local real estate market and catching unusual binaries installed on your computer. </p>
			<p>The outlier detection functionality in the Elastic Stack is based on an ensemble or a grouping of four different outlier detection techniques. Two of these techniques are density-based – that is, they try to determine which data points in your index are far away from the bulk of the data – and two are distance-based – that is, they try to determine which points are far away from all other points. While, individually, each of the four algorithms has its strengths and weaknesses, taken together as an ensemble (or a grouping), they perform robust outlier detection. We discuss what each of these algorithms does on a conceptual level later in the chapter.</p>
			<p>In addition to exploring the technology that powers outlier detection, we will take a look at how outlier detection differs from anomaly detection, how to configure an outlier detection job in Elasticsearch, how to interpret the results of outlier detection, as well as how to understand which features were responsible for a given point being declared an outlier. We will explore the following topics in this chapter:</p>
			<ul>
				<li>Discovering how outlier detection works under the hood</li>
				<li>Applying outlier detection in practice</li>
				<li>Evaluating outlier detection with the Evaluate API</li>
				<li>Hyperparameter tuning for outlier detection</li>
			</ul>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor178"/>Technical requirements</h1>
			<p>The material in this chapter relies on using Elasticsearch version 7.9 or above. The figures in this chapter have been generated using Elasticsearch 7.10. Code snippets and code examples used in this chapter are under the <strong class="source-inline">chapter10</strong> folder in the book's GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition</a>.</p>
			<p><a id="_idTextAnchor179"/>Discovering how outlier detection works</p>
			<p><strong class="bold">Outlier detection</strong> can offer <a id="_idIndexMarker711"/>insights into datasets by discovering which points are different or unusual, but how does outlier detection in the Elastic Stack work? To understand how outlier detection functionality can be constructed, let's start by thinking conceptually <a id="_idIndexMarker712"/>about how you would design the algorithm, and then see how our conceptual ideas can be formalized into the four separate algorithms that make up the outlier detection ensemble in Elasticsearch. </p>
			<p>Suppose for a second that we have a two-dimensional set of weight and circumference measurements of pumpkins and we wish to discover which pumpkins are outliers in this population (perhaps we want to use that information to find out <em class="italic">why</em> they are outliers). A good first step would be to plot the data to see whether there are any obvious data points that appear to be far from others:</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/B17040_10_001.jpg" alt="Figure 10.1 – Points A and B appear to be outliers in this dataset because they are located far from the general mass of data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Points A and B appear to be outliers in this dataset because they are located far from the general mass of data</p>
			<p>Human eyes are exceptionally good at picking up patterns and a quick glance of the plot in <em class="italic">Figure 10.1</em> will tell you that points A and B appear to be outliers. What is the underlying intuitive <a id="_idIndexMarker713"/>reasoning that led us to this conclusion? Our visual system tells us that both points A and B appear to be, in some sense, far away from the two other distinct groups of points in the two-dimensional space. This observation and its formalization is the crux on which the outlier detection techniques used in the Elastic Stack are based.<a id="_idTextAnchor180"/> </p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor181"/>Discovering the four techniques used for outlier detection</h2>
			<p>As mentioned <a id="_idIndexMarker714"/>in the previous section, the outlier detection algorithm in the Elastic Stack is an ensemble, or grouping, of four distinct outlier detection techniques. These techniques can be further subdivided into two categories: <strong class="bold">distance-based techniques</strong> and <strong class="bold">density-based techniques</strong>. We will examine each in turn in the following sections<a id="_idTextAnchor182"/>. </p>
			<h3>Distance-based techniques</h3>
			<p>As we saw <a id="_idIndexMarker715"/>in the previous <a id="_idIndexMarker716"/>section, the human visual system is incredibly adept at picking up outliers in two-dimensional images and the gist of how we are able to do this is by picking up the points that seem far away from the general mass of data. This observation is what the two distance-based techniques, <strong class="bold">distance to kth-nearest neighbors</strong> and <strong class="bold">average distance to kth-nearest neighbors</strong>, aim to capture. </p>
			<p>Suppose we <a id="_idIndexMarker717"/>have a two-dimensional dataset that is <a id="_idIndexMarker718"/>spatially distributed, as in <em class="italic">Figure 10.2</em>, and suppose we pick the value of <em class="italic">k</em> to be 3. At this point, we are just picking an arbitrarily low value of <em class="italic">k</em> for illustration purposes. This would mean that for point A in <em class="italic">Figure 10.2</em>, we find the third closest point and compute the distance of point A to it (marked with a thicker arrow in <em class="italic">Figure 10.2</em>). This approach is great in its simplicity but is also prone to noise. To make this a bit more robust, we can also compute the <strong class="bold">average distance to the kth-nearest neighbors</strong> (illustrated in <em class="italic">Figure 10.2</em>): </p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B17040_10_002.jpg" alt="Figure 10.2 – Distance to kth-nearest neighbor from point A and average distance to kth-nearest neighbor for point A when k=3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Distance to kth-nearest neighbor from point A and average distance to kth-nearest neighbor for point A when k=3</p>
			<p>While <a id="_idIndexMarker719"/>distance-based methods are great in their simplicity <a id="_idIndexMarker720"/>and interpretability, they are unable to capture certain subtleties in the spatial distribution of the data, in particular, how sparse or dense the neighborhood of each data point is. To capture these properties, we have to look at density-based techn<a id="_idTextAnchor183"/>iques. </p>
			<h3>Density-based techniques</h3>
			<p>One factor that the distance-based methods fail to capture in full is the difference between the density of points in the neighborhood of our point of interest and the density of points around its neighbors. Computing the local outlier factor of a point captures exactly this: how different a given point's neighborhood is from other points in the neighborhood. </p>
			<p><em class="italic">Figure 10.3</em> illustrates the basic idea of this technique for k=3. In this case, we compare the neighborhood of point A to the neighborhoods of its three nearest neighbors (illustrated by the <a id="_idIndexMarker721"/>dotted circles in the diagram in <em class="italic">Figure 10.3</em>). A value of 1 for the local outlier factor measure means that the <a id="_idIndexMarker722"/>neighborhood of point A is comparable to that of its neighbors – it is neither more sparse nor more dense. A value greater than 1 means that the neighborhood of A is sparser than that of its neighbors and it could potentially be an outlier. Conversely, a value less than 1 means that the point is densely surrounded by its neighbors and thus unlikely to be an outlier: </p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/B17040_10_003.jpg" alt="Figure 10.3 – Local outlier factor compares the neighborhood of point A to the neighborhoods of its kth-nearest neighbors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Local outlier factor compares the neighborhood of point A to the neighborhoods of its kth-nearest neighbors</p>
			<p>The final <a id="_idIndexMarker723"/>method in our ensemble of four methods is the <strong class="bold">local distance-based outlier factor</strong> (<strong class="bold">LDOF</strong>). Similarly to the <strong class="bold">local outlier factor</strong> (<strong class="bold">LOF</strong>), the goal of <strong class="bold">LDOF</strong> is to compare the neighborhood of a given point, A, to the <a id="_idIndexMarker724"/>neighborhoods of its neighbors. In this case, we compute the average distance to the kth-nearest neighbors of A for some fixed value of <em class="italic">k</em>, <em class="italic">avg(A)</em>. Then, for each of the kth-nearest neighbors of A, we compute the pairwise distances and take the average value of them, <em class="italic">avgkk(A)</em>. </p>
			<p>Finally, we inspect the ratio <em class="italic">avg(A)/avgkk(A)</em> to see how close it is to the value 1. If the ratio approaches 1, it means that point A is surrounded by a local density of other points and is thus unlikely to be an outlier. </p>
			<p>The final overall outlier score that is assigned to each data point is a combination of the values derived <a id="_idIndexMarker725"/>from the four methods above. The closer the value is to 1, the more likely the point is to be an outlier. </p>
			<p>While <a id="_idIndexMarker726"/>sometimes we simply want to figure out which of the points in our datasets are outliers, on other occasions, we also want to see why the outlier detection algorithm is suggesting that a particular point is an outlier. Is there a particular feature or field value or perhaps a group of values that is making the point unusual? This is the topic we will address in the ne<a id="_idTextAnchor184"/>xt section. </p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor185"/>Understanding feature influence</h2>
			<p>Let's return for a moment to our fictional pumpkin dataset from the beginning of this chapter. Suppose we are analyzing this dataset using outlier detection. After the analysis is complete, we have, for each pumpkin, a score from 0 to 1 that measures the unusualness of <a id="_idIndexMarker727"/>the pumpkin. In addition to knowing the score, we might also be interested in understanding which feature – the weight of the pumpkin or the circumference of the pumpkin – contributed to its unusualness.</p>
			<p>This is exactly the problem that <strong class="bold">feature influence</strong> aims to solve. In a nutshell, feature influence assigns to each feature (or <strong class="bold">field</strong> if we are thinking in terms of the vocabulary we usually use to describe Elasticsearch documents) a score from 0 to 1 that describes how significant the feature was in determining that the data point was an outlier. The total sum of feature influence scores across all features adds up to 1. </p>
			<p>Let's take a closer look at feature influence with the help of the fictional pumpkin dataset in <em class="italic">Figure 10.4</em>. Suppose that our outlier detection algorithm has identified points A and B as the outliers in this dataset. Now, let's consider how the feature influence values of pumpkin weight and pumpkin circumference would be relative to one another in points A and B: </p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/B17040_10_004.jpg" alt="Figure 10.4 – Feature influence scores measure how influential a given feature is in determining the unusualness of a data point &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Feature influence scores measure how influential a given feature is in determining the unusualness of a data point </p>
			<p>Pumpkin A has a weight that is far outside of the normal weight range of the pumpkins, but its circumference falls somewhere in the middle of the circumference values in the pumpkins of <a id="_idIndexMarker728"/>the left-hand cluster. Thus, we would expect the feature influence value for pumpkin weight to be high for point A, but the feature influence value for circumference to be low.</p>
			<p>Now let's take a look at outlier pumpkin B, where the situation is reversed. While the weight of pumpkin B falls into the mid-range of the dataset, the circumference of pumpkin B is much higher than almost any other data point. Thus, for pumpkin B, the feature influence value of circumference would be higher than the valu<a id="_idTextAnchor186"/>e of weight. </p>
			<h3>How is feature influence for each point calculated? </h3>
			<p>In the interpretation of these feature influence scores, it is often helpful to know exactly what goes <a id="_idIndexMarker729"/>into the calculation. Let's return for a moment to our two-dimensional pumpkin dataset to illustrate the steps that go into the calculation of feature influence. What we want to establish is how much of an effect a particular feature, <em class="italic">X</em>, say the weight of the pumpkin, has on the final outlier score. A natural way to try and quantify this effect is to imagine that we don't include this feature at all in our outlier calculation. <em class="italic">Figure 10.5</em> shows how this would look in practice for our pumpkin example. For each pumpkin data point, we project the value of the <strong class="bold">Weight</strong> feature to a fixed value 0 and see by how much each data point's unusualness has changed.</p>
			<p>We can see from <em class="italic">Figure 10.5</em> that for point A, removing or projecting the value of the weight to 0 will ultimately result in point A becoming an inlier. Hence, we can conclude that the <strong class="source-inline">Weight</strong> feature has a large influence on point A's unusualness. On the other hand, if we look at point B on the right-hand side diagram in <em class="italic">Figure 10.5</em>, we can see that as a result of projecting its <strong class="source-inline">Weight</strong> feature to 0, the unusualness has not changed. Hence, we can <a id="_idIndexMarker730"/>conclude that the value of the <strong class="source-inline">Weight</strong> feature does not have much bearing on the unusualness of point B and thus its feature influence value will be low: </p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/B17040_10_005.jpg" alt="Figure 10.5 – Feature influence is calculated by asking how much a given data point's unusualness will change if a given feature is projected to a fixed value or removed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Feature influence is calculated by asking how much a given data point's unusualness will change if a given feature is projected to a fixed value or removed</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor187"/>How does outlier detection differ from anomaly detection? </h2>
			<p>While reading <a id="_idIndexMarker731"/>this chapter, you may have noticed <a id="_idIndexMarker732"/>that both outlier detection and anomaly detection are unsupervised learning methods that try to achieve a similar goal: to find unusual or outlying data points. A natural question would then be to ask, <em class="italic">how does anomaly detection differ from outlier detection?</em> In this section, we are going to outline and explain the main differences between the two. A summary of the main points is given in <em class="italic">Figure 10.6</em>, which we will<a id="_idTextAnchor188"/> see very soon. </p>
			<h3>Probability model-based versus instance-based</h3>
			<p>To make the distinction between anomaly detection and outlier detection clearer in our minds, let's <a id="_idIndexMarker733"/>first take a brief look at the available anomaly detection methods. The anomaly detection functionality allows us to detect unusual features in time series-based data. It does this by chunking <a id="_idIndexMarker734"/>up the time series into discrete time units called <strong class="bold">buckets</strong>, then applying a detector function such as mean or sum to the individual values in the bucket. Each bucket value is then used as an individual data point in a probability distribution that is continuously updated as the anomaly detector sees more and more data. The buckets that have a low probability of occurring under the probability distribution are flagged up as anomalies. </p>
			<p>Instead of building a probabilistic model that tracks the evolution of our data across time, outlier detection uses an ensemble (or a group) of four techniques – two distance-based techniques and two density-based techniques, which were covered earlier in the chapter. The further away a data point is from the general mass of data in the dataset, the more likely it is to be an outlier. No probability model is constructed<a id="_idTextAnchor189"/> for the dataset. </p>
			<h3>Scoring </h3>
			<p>This major difference in the two techniques leads us, in turn, to a difference in scoring. In anomaly <a id="_idIndexMarker735"/>detection, the anomalousness of a bucket is determined by how unlikely it is to occur under the model that the anomaly detector has learned from the data. The lower the probability, the more anomalous the bucket is. </p>
			<p>In contrast, in outlier detection, we compute an outlier score, instead of a probability. The outlier score is a continuous measure ranging from 0 to 1 that captures a summary measure of how far the given data point is from the general mass of data in the whole dataset. As we saw earlier in the chapter, this measure is computed using four different techniques. The higher the outlier score, the more anomalous or unusual the data point <a id="_idTextAnchor190"/>is in the dataset. </p>
			<h3>Data characteristics</h3>
			<p>In addition to scoring, another major difference between the two techniques is the type of data for <a id="_idIndexMarker736"/>which they are intended. Anomaly detection is suitable only for time series data, while outlier detection can be used on single or multidimensional datasets that may or may not contain a ti<a id="_idTextAnchor191"/>me-based component. </p>
			<h3>Online versus batch </h3>
			<p>Finally, the last major difference between the two unsupervised learning techniques is how amenable <a id="_idIndexMarker737"/>they are for updates if new data is ingested into the index. Users familiar with anomaly detection will know that this technique is great for streaming data. As soon as a new bucket of data arrives in the cluster and is processed, the probability model can be updated to reflect the new data. </p>
			<p>Outlier detection, on the other hand, is not amenable to online updates in the same fashion as anomaly detection. If a group of new data points is ingested into the source index, we have to rerun the outlier detection job on the source index once again. The reason for this is that outlier detection is an instance-based method that uses the spatial and density distribution of data points to determine which are normal and which are outliers. Any new points ingested into the source index could alter the spatial distribution of the data to such an extent that points previously classified as outliers would no longer be outliers and hence, a re-evaluation of new data requires the outlier score to be recomputed for the whole dataset in a batch add a lead-in sentence before the following table. </p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/B17040_10_006.jpg" alt="Figure 10.6 – A summary of the main differences between anomaly detection and outlier detection "/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – A summary of the main differences between anomaly detection an<a id="_idTextAnchor192"/>d outlier detection </p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor193"/>Applying outlier detection in practice</h1>
			<p>In this section, we will take a look at a practical example of outlier detection using a public dataset <a id="_idIndexMarker738"/>describing the physicochemical properties of wine. This dataset is available for download from the <strong class="bold">University of California Irvine (UCI)</strong> repository (<a href="https://archive.ics.uci.edu/ml/datasets/wine+quality">https://archive.ics.uci.edu/ml/datasets/wine+quality</a>). </p>
			<p>The wine dataset is composed of two CSV files: one describing the physicochemical properties of white wine, the other those of red wine. In this walk-through, we will be focusing on the white wine dataset, but you are welcome to use the data for red wine as well since most of the steps described in this chapter should be applicable to both.</p>
			<p>First let's import the dataset into our Elasticsearch cluster using the <strong class="bold">Data Visualizer</strong> tool, which you can find under the <strong class="bold">Machine Learning</strong> app in Kibana.  We will make an index for the white wine dataset and call it <strong class="source-inline">winequality-white</strong>: </p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B17040_10_007.jpg" alt="Figure 10.7 – The Data Visualizer tool can be found in the Machine Learning app in Kibana and is handy for importing small data files that can be used for experimentation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – The Data Visualizer tool can be found in the Machine Learning app in Kibana and is handy for importing small data files that can be used for experimentation</p>
			<p>Taking a brief look at the data in the <strong class="bold">Discover</strong> tab will show us that each document represents a <a id="_idIndexMarker739"/>single wine and contains information about the alcohol content, the acidity, the pH, and the sugar content, among other chemical measurements, as well as a qualitative score for quality, which has been assigned by human tasters. The goal of our investigation will be to use outlier detection to detect which wines are outliers in terms of their chemical makeup and then see whether this correlates with the quality score they have received from human tasters. Our hypothesis is that wines that are unusual in terms of chemical makeup will also be outliers in terms of quality. Follow the steps outlined here to explore this hypothesis:</p>
			<ol>
				<li>Let's begin by creating an outlier detection job using the <strong class="bold">Data Frame Analytics</strong> wizard as seen in <em class="italic">Figure 10.8</em>: <div id="_idContainer247" class="IMG---Figure"><img src="image/B17040_10_008.jpg" alt="Figure 10.8 – Use the Data Frame Analytics wizard to create an outlier detection job.&#13;&#10;"/></div><p class="figure-caption">Figure 10.8 – Use the Data Frame Analytics wizard to create an outlier detection job.</p></li>
				<li>Because we <a id="_idIndexMarker740"/>are interested in comparing wines that are outliers in chemical composition to the wines that are outliers in quality score, we will exclude the quality score from the outlier detection job as shown in <em class="italic">Figure 10.9</em>: <div id="_idContainer248" class="IMG---Figure"><img src="image/B17040_10_009.jpg" alt="Figure 10.9 – Exclude the quality score from the outlier detection job by unticking the box next to the field name &#13;&#10;"/></div><p class="figure-caption">Figure 10.9 – Exclude the quality score from the outlier detection job by unticking the box next to the field name </p></li>
				<li>We will use the default settings for the rest of the configuration options. Once the job has been completed, we can examine the results using the <strong class="bold">Data Frame Analytics</strong> results viewer: <div id="_idContainer249" class="IMG---Figure"><img src="image/B17040_10_010.jpg" alt="Figure 10.10 – Use the Data frame analytics jobs management UI to see when the outlier detection job has been completed&#13;&#10;"/></div><p class="figure-caption">Figure 10.10 – Use the Data frame analytics jobs management UI to see when the outlier detection job has been completed</p></li>
				<li>A view of the <strong class="bold">Data Frame Analytics</strong> results UI is displayed in <em class="italic">Figure 10.11</em>. There are several key elements in this UI that will help us discover which data points (which of our white wine samples) were outliers and what were the key features <a id="_idIndexMarker741"/>that determined their unusualness. Let's take a look at each in turn. <p>On the left in <em class="italic">Figure 10.11</em>, the UI displays the quantity <strong class="source-inline">ml.outlier_score</strong>. The outlier score is a floating value between 0 and 1, which captures how outlying a given data point is with respect to the dataset. The closer a given point scores to 1, the more outlying it is and vice versa. </p><p>The rest of the columns in the table show us the values of a selection of other fields from the dataset. Each cell is shaded according to a gradient value from 0 to 1, which captures the feature influence, in other words, how important the feature was in determining the unusualness of the data point. The darker shade of blue a given cell is, the more important that feature was for the unusualness of the point. </p><p>For example, by looking at the values in the <strong class="source-inline">ml.outlier_score</strong> column in <em class="italic">Figure 10.11</em>, we can <a id="_idIndexMarker742"/>see that the four most unusual wines in the dataset each scored an outlier score of 0.998: </p><div id="_idContainer250" class="IMG---Figure"><img src="image/B17040_10_0011.jpg" alt="Figure 10.11 – The results UI for outlier detection displays a summary table that captures the outlier score of each data point as well as a selection of fields (in alphabetical order) color-coded with the feature influence score&#13;&#10;"/></div><p class="figure-caption">Figure 10.11 – The results UI for outlier detection displays a summary table that captures the outlier score of each data point as well as a selection of fields (in alphabetical order) color-coded with the feature influence score</p><p class="callout-heading">Important note</p><p class="callout">An interesting question to ask at this point is <em class="italic">what is the threshold at which we declare a point as an outlier?</em> Do we say that all points that score above 0.5 are outliers? Or do we set a more conservative threshold and say that only points above 0.9 are outliers? The process of setting a threshold for a continuous score to bin each data point as either normal or an outlier is known as binarization and is often determined by combining domain knowledge and goals that the user has. However, in the presence of a labeled dataset (for example, a dataset where each data point has already been labeled with the ground truth values of normal/outlier), it is possible to conduct a slightly more systematic process for choosing the threshold. We will return to this topic in the next section when we take a look at the Evaluate API.</p></li>
				<li>Next, let's return to the results UI and take a look at the shading of the cells to see whether we can glean some interesting information about which factors make a certain wine unusual. We can toggle the hidden columns switch in the UI and add all of the remaining features so that we see a full picture of feature influence for the topmost outlying data points, as shown in <em class="italic">Figure 10.12</em>:<div id="_idContainer251" class="IMG---Figure"><img src="image/B17040_10_012.jpg" alt="Figure 10.12 – Feature influence displayed for all fields in the wine quality dataset&#13;&#10;"/></div><p class="figure-caption">Figure 10.12 – Feature influence displayed for all fields in the wine quality dataset</p><p>As we can <a id="_idIndexMarker743"/>see from the annotated areas, outlying wines are unusual for different reasons. For the first data point, the fields with the highest feature influence scores are chloride and citric acid content, while for the following three the most important features in determining unusualness seem to be density and the pH of the wine. </p></li>
				<li>Finally, we can return to the question we posed at the beginning of this section. <em class="italic">Does the unusualness of the wine correlate with the quantitative quality score assigned to it by human tasters?</em> To see whether we can glean any correlation with a quick glance, let's add the quality score to the dataset alongside the outlier score (<em class="italic">Figure 10.13</em>): </li>
			</ol>
			<div>
				<div id="_idContainer252" class="IMG---Figure">
					<img src="image/B17040_10_013.jpg" alt="Figure 10.13 – The most unusual white wines sorted by descending outlier score along with the qualitative quality score assigned by human tasters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – The most unusual white wines sorted by descending outlier score along with the qualitative quality score assigned by human tasters</p>
			<p>As we can see, none of <a id="_idIndexMarker744"/>the top 10 most outlying white wines score in the best category (a quality score of 9). Instead, most of them score in the lower range of 3-6. While this is not conclusive evidence, we have a hint that the most chemically unusual wines are usually not the best tasting!</p>
			<h1 id="_idParaDest-181">E<a id="_idTextAnchor194"/><a id="_idTextAnchor195"/>valuating outlier detection with the Evaluate API </h1>
			<p>In the <a id="_idIndexMarker745"/>previous section, we touched <a id="_idIndexMarker746"/>on the fact it can be hard for a user to know how to set the threshold for outlier scores in order to group the data points in the dataset into normal and outlier categories. In this section, we will show how to approach this issue if you have a labeled dataset that contains, for each point, the ground truth values that record whether the point is an outlier. Before we dive into the practical demonstration, let's take a moment to understand some key performance metrics that are used in evaluating the performance of the outlier detection algorithm.</p>
			<p>One of <a id="_idIndexMarker747"/>the simplest ways we can measure the performance of the algorithm is to compute the number of data points that it <a id="_idIndexMarker748"/>correctly predicted as outliers; in other words, the number of <strong class="bold">true positives </strong>(<strong class="bold">TPs</strong>). In addition, we also want to know the number of <strong class="bold">true negatives </strong>(<strong class="bold">TNs</strong>): how many normal data points were correctly predicted as normal. By extension, we also want to record the number of times the outlier detection algorithm made one of two possible mistakes: either normal points were mislabeled as outliers (<strong class="bold">false positives </strong>(<strong class="bold">FPs</strong>)) or vice versa (<strong class="bold">false negatives </strong>(<strong class="bold">FNs</strong>)). </p>
			<p>These four <a id="_idIndexMarker749"/>measures can be conveniently summarized in a table known as a <strong class="bold">confusion matrix</strong>. An example confusion matrix is displayed in <em class="italic">Figure 10.14</em>:</p>
			<div>
				<div id="_idContainer253" class="IMG---Figure">
					<img src="image/B17040_10_014.jpg" alt="Figure 10.14 – A confusion matrix displaying the true positive, true negative, false positive, and false negative rates &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – A confusion matrix displaying the true positive, true negative, false positive, and false negative rates </p>
			<p>The following <a id="_idIndexMarker750"/>two measures, the <strong class="bold">precision</strong> and <strong class="bold">recall</strong>, can be built on top of the four metrics we just described. </p>
			<p>Precision is <a id="_idIndexMarker751"/>the proportion of true positives among all of the points that were predicted positive or outlying. Recall, on the other hand, is the proportion of true positives among all of the points that were actually positive. These quantities can be summarized as equations as follows: </p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="image/Formula_10_001.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="image/Formula_10_002.jpg" alt=""/>
				</div>
			</div>
			<p>Based on the definitions given in the preceding paragraphs, it seems that in order to compute the number of true positives, true negatives, and so forth, we require each of the points in our destination index to be assigned a class label. </p>
			<p>However, the result of the outlier detection job, as we described a few sections ago, is not a binary <a id="_idIndexMarker752"/>class label that assigns each point as an outlier or normal, but instead a numeric outlier score that ranges from 0 to 1.</p>
			<p>This presents <a id="_idIndexMarker753"/>a problem for us when it comes to computing our desired metrics because we have to make a decision and specify a cut-off point. Everything that scores higher than the cut-off point is assigned to the <a id="_idIndexMarker754"/>outlying class and everything that scores below the cut-off point is assigned to the normal class. We will call this the <strong class="bold">binarization threshold</strong>. </p>
			<p>It can be challenging to know exactly what value to set this threshold to, which brings us back to our original goal in this chapter – using the Evaluate API to understand the different performance metrics above at different thresholds so that we can make an informed choice. </p>
			<p>Let's now take a practical walk-through and see how we can apply this knowledge in practice: </p>
			<ol>
				<li value="1">Let's examine the public dataset that we are going to use for this section. The source of the original dataset is the UCI repository here : <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29</a>. To better fit the purpose of this exercise, we have modified the dataset slightly by creating an extra field called Outlier which records whether or not the given data point is an outlier or not. The modified dataset is in a file called <strong class="source-inline">breast-cancer-wisconsin-outlier.csv</strong> and is available for download in the book's GitHub repository here <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2010%20-%20Outlier%20Detection%20Analysis">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2010%20-%20Outlier%20Detection%20Analysis</a>. <p>Once you have downloaded this dataset, you can use the data import functionality in the Data Visualizer to import the dataset. For a refresher on the Data Visualizer and how to import data, please see the section Applying Outlier Detection in Practice .</p><p>The dataset <a id="_idIndexMarker755"/>describes features <a id="_idIndexMarker756"/>measured from malignant and benign breast cancer tissue and includes a <strong class="bold">Class</strong> field, which can take either the value 2 (benign) or 4 (malignant). For the purpose of this section, we will treat the data points labeled as malignant as outlie<a id="_idTextAnchor196"/><a id="_idTextAnchor197"/><a id="_idTextAnchor198"/><a id="_idTextAnchor199"/><a id="_idTextAnchor200"/><a id="_idTextAnchor201"/><a id="_idTextAnchor202"/>rs: </p><div id="_idContainer256" class="IMG---Figure"><img src="image/B17040_10_015.jpg" alt="Figure 10.15 – Each data point in the dataset is labeled with a Class label. We have converted the Class label into a Boolean label and stored it in a new field called Outlier &#13;&#10;"/></div><p class="figure-caption">Figure 10.15 – Each data point in the dataset is labeled with a Class label. We have converted the Class label into a Boolean label and stored it in a new field called Outlier </p><p>It is worth mentioning at this point that the Evaluate API (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/evaluate-dfanalytics.html</a>), which we will be using to understand how well the outlier detection algorithm performed against the ground truth labels, requires the ground <a id="_idIndexMarker757"/>truth label to be a Boolean 0 (for a normal data point) and 1 for an outlier data point. Therefore, we have <a id="_idIndexMarker758"/>slightly adjusted the original dataset by adding an extra field called <strong class="bold">Outlier</strong>, which converts the <strong class="bold">Class</strong> field into a suitable format for consumption by the Evaluate API. A sample document from the dataset is displayed in <em class="italic">Figure 10.15</em>. </p></li>
				<li>Let's use the <strong class="bold">Data Frame Analytics</strong> wizard to create an outlier detection job with this dataset. We will exclude the field that contains the <strong class="bold">Class</strong> label, the field that contains the ground truth label, as well as the sample code number, as shown in <em class="italic">Figure 10.16</em>: <div id="_idContainer257" class="IMG---Figure"><img src="image/B17040_10_016.jpg" alt="Figure 10.16 – Excluding the Class, Outlier, and Sample_code_number fields from the outlier detection job&#13;&#10;"/></div><p class="figure-caption">Figure 10.16 – Excluding the Class, Outlier, and Sample_code_number fields from the outlier detection job</p></li>
				<li>Once the job has been completed, we can use the destination index that contains <a id="_idIndexMarker759"/>the results of the outlier <a id="_idIndexMarker760"/>detection job together with the Evaluate API to compute how well our outlier detection algorithm worked when compared to the ground truth labels. </li>
				<li>We will interact with the Evaluate API through the <strong class="bold">Dev Tools console</strong> in Kibana. Let's take a look at a sample Evaluate API call to understand the require<a id="_idTextAnchor203"/>d parts: <p class="source-code">POST _ml/data_frame/_evaluate</p><p class="source-code">{</p><p class="source-code">  "index": "breast-cancer-wisconsin-outlier", </p><p class="source-code">  "evaluation": {</p><p class="source-code">    "outlier_detection": {</p><p class="source-code">      "actual_field": "Outlier", </p><p class="source-code">      "predicted_probability_field": "ml.outlier_score",</p><p class="source-code">      "metrics" : {</p><p class="source-code">        "confusion_matrix" : {"at": [0.25, 0.5, 0.75]}, </p><p class="source-code">        "precision" : {"at": [0.25, 0.5, 0.75]}, </p><p class="source-code">        "recall" : {"at": [0.25, 0.5, 0.75]} </p><p class="source-code">      }</p><p class="source-code">    }</p><p class="source-code">  }</p><p class="source-code">}</p><p>The first important piece of information is the destination index of the outlier detection job. This index contains the predictions of our algorithm and is thus required. The second piece of configuration we require is <strong class="source-inline">actual_field</strong>. This is the <a id="_idIndexMarker761"/>field that contains <a id="_idIndexMarker762"/>the ground truth label for our data. In our case, this is the field called <strong class="bold">Outlier</strong>. Finally, we proceed to define the metrics that we would like the API to return for us as well as the thresholds at which these should be calculated. </p><p>The parameters in the REST API call allow us to specify a wide variety of possible thresholds or cut-off points for which to compute the performance metrics. In the preceding example, we asked the Evaluate API to return the values of the performance metrics at three different binarization thresholds: 0.25, 0.5, and 0.75, but equally we could have picked another set of values.</p></li>
				<li>Next, we <a id="_idIndexMarker763"/>will examine the <a id="_idIndexMarker764"/>results returned by the Evaluate API. The response is shown here:<p class="source-code">{</p><p class="source-code">  "outlier_detection" : {</p><p class="source-code">    "confusion_matrix" : {</p><p class="source-code">      "0.25" : {</p><p class="source-code">        "tp" : 0,</p><p class="source-code">        "fp" : 15,</p><p class="source-code">        "tn" : 429,</p><p class="source-code">        "fn" : 239</p><p class="source-code">      },</p><p class="source-code">      "0.5" : {</p><p class="source-code">        "tp" : 0,</p><p class="source-code">        "fp" : 5,</p><p class="source-code">        "tn" : 439,</p><p class="source-code">        "fn" : 239</p><p class="source-code">      },</p><p class="source-code">      "0.75" : {</p><p class="source-code">        "tp" : 0,</p><p class="source-code">        "fp" : 1,</p><p class="source-code">        "tn" : 443,</p><p class="source-code">        "fn" : 239</p><p class="source-code">      }</p><p class="source-code">    },</p><p class="source-code">    "precision" : {</p><p class="source-code">      "0.25" : 0.0,</p><p class="source-code">      "0.5" : 0.0,</p><p class="source-code">      "0.75" : 0.0</p><p class="source-code">    },</p><p class="source-code">    "recall" : {</p><p class="source-code">      "0.25" : 0.0,</p><p class="source-code">      "0.5" : 0.0,</p><p class="source-code">      "0.75" : 0.0</p><p class="source-code">    }</p><p class="source-code">  }</p><p class="source-code">}</p><p>As we can see, the Evaluate API has returned a response where each of the metrics is computed three different times – once for each of the thresholds that were specified in the REST API call. </p></li>
			</ol>
			<p>The different <a id="_idIndexMarker765"/>values of the confusion <a id="_idIndexMarker766"/>matrix indicate that the outlier detection algorithm has done quite poorly when it comes to this particular dataset. Not a single one of the thresholds yields any true positives, which means that we were not able to detect any outliers with the default settings. In the next section, we are going to see how <strong class="bold">hyperparameter tuning</strong> can help us achieve better results with outli<a id="_idTextAnchor204"/>er detection. </p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor205"/>Hyperparameter tuning for outlier detection </h1>
			<p>For the <a id="_idIndexMarker767"/>more advanced user, the <strong class="bold">Data Frame Analytics</strong> wizard <a id="_idIndexMarker768"/>offers an <a id="_idIndexMarker769"/>opportunity to configure and tune <strong class="bold">hyperparameters</strong> – various knobs and dials that fine-tune how the outlier detection algorithm works. The available hyperparameters are displayed in <em class="italic">Figure 10.17</em>. For example, we can direct the outlier detection job to use only a certain type of outlier detection method instead of the ensemble, to use a certain value for the number of nearest neighbors that are used in the computation in the ensemble, and to assume that a certain portion of the data is outlying. </p>
			<p>Please note that while it is good to play around with these settings to experiment and get a feel for how they affect the final results, if you want to customize any of these for a production usecase, you should carefully study the characteristics of your data and have an awareness of how these characteristics will interact with your chosen hyperparameter settings. More information on each of these hyperparameters is available in the documentation here <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/put-dfanalytics.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/put-dfanalytics.html</a>.</p>
			<p>In our case, we know that the dataset contains about 30% malicious samples. Therefore, the number <a id="_idIndexMarker770"/>of outliers we expect is close to <a id="_idIndexMarker771"/>that as well. We can configure this as the value for <strong class="bold">Outlier fraction</strong> and rerun our job. This is shown in <em class="italic">Figure 10.17</em>: </p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="image/B17040_10_017.jpg" alt="Figure 10.17 – It is possible to fine-tune the behavior of the outlier detection job by tuning hyperparameters through the Data Frame Analytics wizard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – It is possible to fine-tune the behavior of the outlier detection job by tuning hyperparameters through the Data Frame Analytics wizard</p>
			<p>Let's recreate our outlier detection job with this new hyperparameter and compare the result to the one in <em class="italic">Figure 10.17</em>:</p>
			<ol>
				<li value="1">Follow the steps outlined during the creation of our first outlier detection job in the <em class="italic">Evaluating outlier detection with the Evaluate API section</em>, but adjust the <strong class="bold">Outlier fraction</strong> setting under the <strong class="bold">Hyperparameters</strong> dialog as shown in <em class="italic">Figure 10.17</em>. Create and run the outlier detection job. </li>
				<li>After the job has finished running, we can rerun the Evaluate API commands for this new results index. We have used the name <strong class="source-inline">breast-cancer-wisconsin-outlier-fraction</strong> as the name of the destination index that contains <a id="_idIndexMarker772"/>the results of the job with <a id="_idIndexMarker773"/>tuned hyperparameters. Hence our new Evaluate API call is as follows: <p class="source-code">POST _ml/data_frame/_evaluate</p><p class="source-code">{</p><p class="source-code">  "index": "breast-cancer-wisconsin-outlier-fraction", </p><p class="source-code">  "evaluation": {</p><p class="source-code">    "outlier_detection": {</p><p class="source-code">      "actual_field": "Outlier", </p><p class="source-code">      "predicted_probability_field": "ml.outlier_score",</p><p class="source-code">      "metrics" : {</p><p class="source-code">        "confusion_matrix" : {"at": [0.25, 0.5, 0.75]}, </p><p class="source-code">        "precision" : {"at": [0.25, 0.5, 0.75]}, </p><p class="source-code">        "recall" : {"at": [0.25, 0.5, 0.75]} </p><p class="source-code">      }</p><p class="source-code">    }</p><p class="source-code">  }</p><p class="source-code">}</p></li>
				<li>Let's see how much our confusion matrix has changed for the three different thresholds. The response we receive from the Evaluate API is displayed here: <p class="source-code">{</p><p class="source-code">  "outlier_detection" : {</p><p class="source-code">    "confusion_matrix" : {</p><p class="source-code">      "0.25" : {</p><p class="source-code">        "tp" : 239,</p><p class="source-code">        "fp" : 210,</p><p class="source-code">        "tn" : 234,</p><p class="source-code">        "fn" : 0</p><p class="source-code">      },</p><p class="source-code">      "0.5" : {</p><p class="source-code">        "tp" : 86,</p><p class="source-code">        "fp" : 48,</p><p class="source-code">        "tn" : 396,</p><p class="source-code">        "fn" : 153</p><p class="source-code">      },</p><p class="source-code">      "0.75" : {</p><p class="source-code">        "tp" : 0,</p><p class="source-code">        "fp" : 13,</p><p class="source-code">        "tn" : 431,</p><p class="source-code">        "fn" : 239</p><p class="source-code">      }</p><p class="source-code">    },</p><p class="source-code">    "precision" : {</p><p class="source-code">      "0.25" : 0.532293986636971,</p><p class="source-code">      "0.5" : 0.6417910447761194,</p><p class="source-code">      "0.75" : 0.0</p><p class="source-code">    },</p><p class="source-code">    "recall" : {</p><p class="source-code">      "0.25" : 1.0,</p><p class="source-code">      "0.5" : 0.3598326359832636,</p><p class="source-code">      "0.75" : 0.0</p><p class="source-code">    }</p><p class="source-code">  }</p><p class="source-code">}</p><p>As we can see from the values of the confusion matrix, we are doing slightly better in terms of the detection of the true positives, the true outliers, but slightly worse in terms of detect<a id="_idTextAnchor206"/>ing false negatives. </p></li>
			</ol>
			<p>Comparing the evaluation metrics from the preceding outlier detection job and the outlier detection job we created in the <em class="italic">Evaluating outlier detection with the Evaluate API</em> section illustrates that hyperparameter choice can have a significant bearing on the result of the outlier detection job. How should you, then, proceed in choosing sensible hyperparameters? </p>
			<p>There are <a id="_idIndexMarker774"/>many subtleties and advanced topics that you can dive into when choosing hyperparameters, but a good guideline is <a id="_idIndexMarker775"/>to remember the iterative nature of the process. It is good to start with a labeled dataset and try the quality of results using the default settings (in other words, without adjusting anything under the <strong class="bold">Hyperparameters</strong> dialog in the Data Frame Analytics wizards). </p>
			<p>The defaults have usually been sensibly chosen and tested on a variety of datasets. If the quality of results is not satisfactory, you can start coming up with a plan to adjust and fine-tune the various hyperparameter settings. A good first step is to fix known issues and examine how this affects the quality of the results. For example, we knew from prior experience that the breast cancer dataset contains outliers around 30% or 0.3 of the data, which allowed us to adjust this setting and achieve a slightly bett<a id="_idTextAnchor207"/>er true positive rate. </p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor208"/>Summary </h1>
			<p>To conclude the chapter, let's remind ourselves of the main features of the second unsupervised learning feature in the Elastic Stack: outlier detection. Outlier detection can be used to detect unusual data points in single or multidimensional datasets. </p>
			<p>The algorithm is based on an ensemble of four separate measures: two distance-based measures based on kth-nearest neighbors and two density-based measures. The combination of these measures captures how far a given data point is from its neighbors and from the general mass of data in the dataset. This unusualness is captured in a numerical outlier score that ranges from 0 to 1. The closer a given data point scores to 1, the more unusual it is in the dataset. </p>
			<p>In addition to the outlier score, for each feature or field of a point, we compute a quantity known as the feature influence. The higher the feature influence for a given field, the more that field is responsible for a given point being unusual. These feature influence scores can be used to understand why a particular point received a particular outlier score. </p>
			<p>In contrast with the other unsupervised learning functionality in the Elastic Stack, anomaly detection, outlier detection does not require the data to have a time component or be a time series of any kind. Moreover, outlier detection, in contrast with anomaly detection, does not learn a probabilistic model to understand which data points have a low probability of occurring. Instead, it uses distance and density-based measures to compute unusualness. Because of this difference in methodologies, it is not possible for outlier detection to function in an online manner, updating its computations of outlier-ness on the fly as new data is added to the source index. Instead, if we wish to predict the unusualness of new points as they are added to the index, we have to rerun the outlier detection calculation for the whole source index in batch mode. </p>
			<p>In the next chapter, we will leave unsupervised learning methods behind us and dive into the exciting world of supervised learning, starting with classification. </p>
		</div>
	</body></html>