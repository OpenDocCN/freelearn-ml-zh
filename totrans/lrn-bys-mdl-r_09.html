<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Bayesian Modeling at Big Data Scale" id="aid-20R681"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Bayesian Modeling at Big Data Scale</h1></div></div></div><p>When we learned the principles of Bayesian inference in <a class="link" title="Chapter 3. Introducing Bayesian Inference" href="part0030.xhtml#aid-SJGS2">Chapter 3</a>, <span class="emphasis"><em>Introducing Bayesian Inference</em></span>, we saw that as the amount of training data increases, contribution to the parameter estimation from data overweighs that from the prior distribution. Also, the uncertainty in parameter estimation decreases. Therefore, you may wonder why one needs Bayesian modeling in large-scale data analysis. To answer this question, let us look at one such problem, which is building recommendation systems for e-commerce products.</p><p>In a typical e-commerce store, there will be millions of users and tens of thousands of products. However, each user would have purchased only a small fraction (less than 10%) of all the products found in the store in their lifetime. Let us say the e-commerce store is collecting users' feedback for each product sold as a rating on a scale of 1 to 5. Then, the store can create a user-product rating matrix to capture the ratings of all users. In this matrix, rows would correspond to users and columns would correspond to products. The entry of each cell would be the rating given by the user (corresponding to the row) to the product (corresponding to the column). Now, it is easy to see that although the overall size of this matrix is huge, only less than 10% entries would have values since every user would have bought only less than 10% products from the store. So, this is a highly sparse dataset. Whenever there is a machine learning task where, even though the overall data size is huge, the data is highly sparse, overfitting can happen and one should rely on Bayesian methods (reference 1 in the <span class="emphasis"><em>References</em></span> section of this chapter). Also, many models such as Bayesian networks, Latent Dirichlet allocation, and deep belief networks are built on the Bayesian inference paradigm. </p><p>When these models are trained on a large dataset, such as text corpora from Reuters, then the underlying problem is large-scale Bayesian modeling. As it is, Bayesian modeling is computationally intensive since we have to estimate the whole posterior distribution of parameters and also do model averaging of the predictions. The presence of large datasets will make the situation even worse. So what are the computing frameworks that we can use to do Bayesian learning at a large scale using R? In the next two sections, we will discuss some of the latest developments in this area.</p><div class="section" title="Distributed computing using Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec62"/>Distributed computing using Hadoop</h1></div></div></div><p>In the last decade, tremendous progress was made in distributed computing when two research engineers from Google developed a computing paradigm called the <a id="id411" class="indexterm"/>
<span class="strong"><strong>MapReduce</strong></span> framework and an associated distributed filesystem called Google File System (reference 2 in the <span class="emphasis"><em>References</em></span> section of this chapter). Later on, Yahoo developed an open source version of this distributed filesystem named <span class="strong"><strong>Hadoop</strong></span> that<a id="id412" class="indexterm"/> became the hallmark of Big Data computing. Hadoop is ideal for processing large amounts of data, which cannot fit into the memory of a single large computer, by distributing the data into multiple computers and doing the computation on each node locally from the disk. An example would be extracting relevant information from log files, where typically the size of data for a month would be in the order of terabytes.</p><p>To use<a id="id413" class="indexterm"/> Hadoop, one has to write programs using MapReduce framework to parallelize the computing. A Map operation splits the data into multiple key-value pairs and sends it to different nodes. At each of those nodes, a computation is done on each of the key-value pairs. Then, there is a shuffling operation where all the pairs with the same value of key are brought together. After this, a Reduce operation sums up all the results corresponding to the same key from the previous computation step. Typically, these MapReduce operations can be written using a high-level language called <a id="id414" class="indexterm"/>
<span class="strong"><strong>Pig</strong></span>. One can also write MapReduce programs in R using the <span class="strong"><strong>RHadoop</strong></span> package, which we will describe in the next section.</p></div></div>
<div class="section" title="RHadoop for using Hadoop from R" id="aid-21PMQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec63"/>RHadoop for using Hadoop from R</h1></div></div></div><p>RHadoop is a<a id="id415" class="indexterm"/> collection <a id="id416" class="indexterm"/>of open source packages using which an R user can manage and analyze data stored in the <a id="id417" class="indexterm"/>
<span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>). In the background, RHadoop will translate these as MapReduce operations in Java and run them on HDFS. </p><p>The various packages in RHadoop and their uses are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>rhdfs</strong></span>: Using<a id="id418" class="indexterm"/> this package, a user can connect to an HDFS from R and perform basic actions such as read, write, and modify files.</li><li class="listitem"><span class="strong"><strong>rhbase</strong></span>: This is <a id="id419" class="indexterm"/>the package to connect to a HBASE database from R and to read, write, and modify tables.</li><li class="listitem"><span class="strong"><strong>plyrmr</strong></span>: Using this<a id="id420" class="indexterm"/> package, an R user can do the common data manipulation tasks such as the slicing and dicing of datasets. This is similar to the function of packages such as <span class="strong"><strong>plyr</strong></span> or <span class="strong"><strong>reshape2</strong></span>.</li><li class="listitem"><span class="strong"><strong>rmr2</strong></span>: Using this package, a <a id="id421" class="indexterm"/>user can write MapReduce functions in R and execute them in an HDFS.</li></ul></div><p>Unlike the <a id="id422" class="indexterm"/>other packages discussed in this book, the packages associated with RHadoop are not available from CRAN. They can be downloaded from the GitHub repository at <a class="ulink" href="https://github.com/RevolutionAnalytics">https://github.com/RevolutionAnalytics</a> and are installed from the local drive.</p><p>Here is a sample MapReduce code written using the rmr2 package to count the number of words in a corpus (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter):</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step involves loading the <code class="literal">rmr2</code> library:<div class="informalexample"><pre class="programlisting">&gt;library(rmr2)
&gt;LOCAL &lt;- T #to execute rmr2 locally</pre></div></li><li class="listitem">The second step involves writing the Map function. This function takes each line in the text document and splits it into words. Each word is taken as a token. The function emits key-value pairs where each distinct word is a <span class="emphasis"><em>key</em></span> and <span class="emphasis"><em>value = 1</em></span>:<div class="informalexample"><pre class="programlisting">&gt;#map function
&gt;map.wc &lt;- function(k,lines){ 
       words.list &lt;- strsplit(lines,'\\s+^' )
        words &lt;- unlist(words.list)         
        return(keyval(words,1))
    }</pre></div></li><li class="listitem">The third step involves writing a reduce function. This function groups all the same <span class="emphasis"><em>key</em></span> from different mappers and sums their <span class="emphasis"><em>value</em></span>. Since, in this case, each word is a <span class="emphasis"><em>key</em></span> and the <span class="emphasis"><em>value = 1</em></span>, the output of the reduce will be the count of the words:<div class="informalexample"><pre class="programlisting">&gt;#reduce function
&gt;reduce.wc&lt;-function(word,counts){
          return(keyval(word,sum(counts) ))
}</pre></div></li><li class="listitem">The fourth step involves writing a word count function combining the map and reduce functions and executing this function on a file named <code class="literal">hdfs.data</code> stored in the HDFS containing the input text:<div class="informalexample"><pre class="programlisting">&gt;#word count function
&gt;wordcount&lt;-function(input,output=NULL){
            mapreduce(input = input,output = output,input.format = "text",map = map.wc,reduce = reduce.wc,combine = T)
}
&gt;out&lt;-wordcount(hdfs.data,hdfs.out)</pre></div></li><li class="listitem">The fifth <a id="id423" class="indexterm"/>step involves getting the output file from HDFS and printing the top five lines:<div class="informalexample"><pre class="programlisting">&gt;results&lt;-from.dfs(out)
&gt;results.df&lt;-as.data.frame(results,stringAsFactors=F)
&gt;colnames(results.df)&lt;-c('word^' ,^' count^')
&gt;head(results.df)</pre></div></li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Spark &#x2013; in-memory distributed computing" id="aid-22O7C1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Spark – in-memory distributed computing</h1></div></div></div><p>One of the issues <a id="id424" class="indexterm"/>with Hadoop is that after a MapReduce operation, the resulting files are written to the hard disk. Therefore, when there is a large data processing operation, there would be many read and write operations on the hard disk, which makes processing in Hadoop very slow. Moreover, the network latency, which is the time required to shuffle data between different nodes, also contributes to this problem. Another disadvantage is that one cannot make real-time queries from the files stored in HDFS. For machine learning problems, during training phase, the MapReduce will not persist over iterations. All this makes Hadoop not an ideal platform for machine learning.</p><p>A solution to this problem was invented at Berkeley University's AMP Lab in 2009. This came out of the PhD work of Matei Zaharia, a Romanian born computer scientist. His paper <span class="emphasis"><em>Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</em></span> (reference 4 in the <span class="emphasis"><em>References</em></span> section of this chapter) gave rise to the Spark project that eventually became a fully open source project under Apache. Spark is an in-memory distributed computing framework that solves many of the problems of Hadoop mentioned earlier. Moreover, it supports more type of operations that just MapReduce. Spark can be used for processing iterative algorithms, interactive data mining, and streaming applications. It is based on an abstraction called <a id="id425" class="indexterm"/>
<span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDD</strong></span>). Similar to HDFS, it is also fault-tolerant.</p><p>Spark is written in a language called Scala. It has interfaces to use from Java and Python and from the recent version 1.4.0; it also supports R. This is called SparkR, which we will describe in the next section. The four classes of libraries available in Spark are SQL and DataFrames, Spark Streaming, MLib (machine learning), and GraphX (graph algorithms). Currently, SparkR supports only SQL and DataFrames; others are definitely in the roadmap. Spark <a id="id426" class="indexterm"/>can be downloaded from the Apache project <a id="id427" class="indexterm"/>page at <a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>. Starting from 1.4.0 version, SparkR is included in Spark and no separate download is required.</p></div>
<div class="section" title="SparkR" id="aid-23MNU1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec65"/>SparkR</h1></div></div></div><p>Similar<a id="id428" class="indexterm"/> to RHadoop, SparkR is an R package that allows R users to use Spark APIs through the <code class="literal">RDD</code> class. For example, using SparkR, users can run jobs on Spark from RStudio. SparkR can be evoked from RStudio. To enable this, include the following lines in your <code class="literal">.Rprofile</code> file that R uses at startup to initialize the environments:</p><div class="informalexample"><pre class="programlisting">Sys.setenv(SPARK_HOME/.../spark-1.5.0-bin-hadoop2.6")
#provide the correct path where spark downloaded folder is kept for SPARK_HOME 
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"),""R",""lib"),".libPaths()))</pre></div><p>Once this is done, start RStudio and enter the following commands to start using SparkR:</p><div class="informalexample"><pre class="programlisting">&gt;library(SparkR)
&gt;sc &lt;- sparkR.init(master="local")</pre></div><p>As mentioned, as of the latest version 1.5 when this chapter is in writing, SparkR supports limited functionalities of R. This mainly includes data slicing and dicing and summary stat functions. The current version does not support the use of contributed R packages; however, it is planned for a future release. On machine learning, currently SparkR supports the <code class="literal">glm( )</code> function. We will do an example in the next section.</p></div>
<div class="section" title="Linear regression using SparkR" id="aid-24L8G1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Linear regression using SparkR</h1></div></div></div><p>In the<a id="id429" class="indexterm"/> following example, we will illustrate how to use SparkR for machine learning. For this, we will use the same dataset of energy efficiency measurements that we used for linear regression in <a class="link" title="Chapter 5. Bayesian Regression Models" href="part0041.xhtml#aid-173721">Chapter 5</a>, <span class="emphasis"><em>Bayesian Regression Models</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt;library(SparkR)
&gt;sc &lt;- sparkR.init(master="local")
&gt;sqlContext &lt;- sparkRSQL.init(sc)

#Importing data
&gt;df &lt;- read.csv("/Users/harikoduvely/Projects/Book/Data/ENB2012_data.csv",header = T)
&gt;#Excluding variable Y2,X6,X8 and removing records from 768 containing mainly null values
&gt;df &lt;- df[1:768,c(1,2,3,4,5,7,9)]
&gt;#Converting to a Spark R Dataframe
&gt;dfsr &lt;- createDataFrame(sqlContext,df) 
&gt;model &lt;- glm(Y1 ~ X1 + X2 + X3 + X4 + X5 + X7,data = dfsr,family = "gaussian")
 &gt; summary(model)</pre></div></div>
<div class="section" title="Computing clusters on the cloud"><div class="titlepage" id="aid-25JP22"><div><div><h1 class="title"><a id="ch09lvl1sec67"/>Computing clusters on the cloud</h1></div></div></div><p>In order to <a id="id430" class="indexterm"/>process large datasets using Hadoop and associated R packages, one needs a cluster of computers. In today's world, it is easy to get using cloud computing services provided by Amazon, Microsoft, and others. One needs to pay only for the amount of CPU and storage used. No need for upfront investments on infrastructure. The top four cloud computing services are AWS by Amazon, Azure by Microsoft, Compute Cloud by Google, and Bluemix by IBM. In this section, we will discuss running R programs on AWS. In particular, you will learn how to create an AWS instance; install R, RStudio, and other packages in that instance; develop and run machine learning models.</p><div class="section" title="Amazon Web Services"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec49"/>Amazon Web Services</h2></div></div></div><p>Popularly<a id="id431" class="indexterm"/> known as AWS, Amazon Web Services started as an internal project in Amazon in 2002 to meet the dynamic computing requirements to support their e-commerce business. This grew as an <span class="strong"><strong>infrastructure as a service</strong></span> and<a id="id432" class="indexterm"/> in 2006 Amazon launched two services to the world, <span class="strong"><strong>Simple Storage Service</strong></span> (<span class="strong"><strong>S3</strong></span>) and <a id="id433" class="indexterm"/>
<span class="strong"><strong>Elastic Computing Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>). From there, AWS grew at incredible pace. Today, they have more than 40 different types of services using millions of servers.</p></div><div class="section" title="Creating and running computing instances on AWS"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec50"/>Creating and running computing instances on AWS</h2></div></div></div><p>The<a id="id434" class="indexterm"/> best place to <a id="id435" class="indexterm"/>learn how to set up an AWS account and start using EC2 is the freely available e-book from Amazon Kindle store named <span class="emphasis"><em>Amazon Elastic Compute Cloud (EC2) User Guide</em></span> (reference 6 in the <span class="emphasis"><em>References</em></span> section of this chapter). </p><p>Here, we only summarize the essential steps involved in the process:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create an AWS account.</li><li class="listitem">Sign in to the AWS management console (<a class="ulink" href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a>).</li><li class="listitem">Click on the EC2 service.</li><li class="listitem">Choose <span class="strong"><strong>Amazon Machine Instance (AMI)</strong></span>.</li><li class="listitem">Choose an instance type.</li><li class="listitem">Create a public-private key-pair.</li><li class="listitem">Configure instance.</li><li class="listitem">Add storage.</li><li class="listitem">Tag instance.</li><li class="listitem">Configure a security group (policy specifying who can access the instance).</li><li class="listitem">Review and launch the instance.</li></ol><div style="height:10px; width: 1px"/></div><p>Log in to <a id="id436" class="indexterm"/>your instance using SSH (from Linux/Ubuntu), Putty (from Windows), or a browser using the private key provided at the time of configuring security and the IP address given at the time of launching. Here, we are assuming that the instance you have launched is a Linux instance.</p></div><div class="section" title="Installing R and RStudio"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec51"/>Installing R and RStudio</h2></div></div></div><p>To<a id="id437" class="indexterm"/> install R and RStudio, you need to be an authenticated <a id="id438" class="indexterm"/>user. So, create a new user <a id="id439" class="indexterm"/>and give the user administrative privilege (sudo). After that, execute the following steps from the Ubuntu shell:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Edit the <code class="literal">/etc/apt/sources.list</code> file.</li><li class="listitem">Add the following line at the end:<div class="informalexample"><pre class="programlisting">deb http://cran.rstudio.com/bin/linux/ubuntu trusty .</pre></div></li><li class="listitem">Get the keys for the repository to run:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-key adv  --keyserver keyserver.ubuntu.com –recv-keys 51716619E084DAB9</strong></span>
</pre></div></li><li class="listitem">Update the package list:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get update</strong></span>
</pre></div></li><li class="listitem">Install the latest version of R:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install r-base-core</strong></span>
</pre></div></li><li class="listitem">Install <a id="id440" class="indexterm"/>gdebi to install Debian packages from the local disk:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install gdebi-core</strong></span>
</pre></div></li><li class="listitem">Download the RStudio package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://download2.rstudio.org/r-studio-server-0.99.446-amd64.deb</strong></span>
</pre></div></li><li class="listitem">Install RStudio:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo gdebi r-studio-server-0.99.446-amd64.deb</strong></span>
</pre></div></li></ol><div style="height:10px; width: 1px"/></div><p>Once the<a id="id441" class="indexterm"/> installation is completed successfully, RStudio running on your AWS instance can be accessed from a browser. For this, open a browser and enter the URL <code class="literal">&lt;your.aws.ip.no&gt;:8787</code>.</p><p>If you are<a id="id442" class="indexterm"/> able to use your RStudio running on the AWS instance, you can then install other packages such as rhdfs, rmr2, and more from RStudio, build any machine learning models in R, and run them on the AWS cloud.</p><p>Apart from R and RStudio, AWS also supports Spark (and hence SparkR). In the following section, you will learn how to run Spark on an EC2 cluster.</p></div><div class="section" title="Running Spark on EC2"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec52"/>Running Spark on EC2</h2></div></div></div><p>You can<a id="id443" class="indexterm"/><a id="id444" class="indexterm"/> launch and manage Spark clusters on Amazon EC2 using the <code class="literal">spark-ec2</code> script located in the <code class="literal">ec2</code> directory of Spark in your local machine. To launch a Spark cluster on EC2, use the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the <code class="literal">ec2</code> directory in the Spark folder in your local machine.</li><li class="listitem">Run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;</strong></span>
</pre></div><p>Here, <code class="literal">&lt;keypair&gt;</code> is the name of the keypair you used for launching the EC2 service mentioned in the <span class="emphasis"><em>Creating and running computing instances on AWS</em></span> section of this chapter. The <code class="literal">&lt;key-file&gt;</code> is the path in your local machine where the private key has been downloaded and kept. The number of worker nodes is specified by <code class="literal">&lt;num-slaves&gt;</code>.</p></li><li class="listitem">To run your programs in the cluster, first SSH into the cluster using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; login &lt;cluster-name&gt;</strong></span>
</pre></div><p>After logging into the cluster, you can use Spark as you use on the local machine.</p></li></ol><div style="height:10px; width: 1px"/></div><p>More<a id="id445" class="indexterm"/> details <a id="id446" class="indexterm"/>on how to use Spark on EC2 can be found in the Spark documentation and AWS documentation (references 5, 6, and 7 in the <span class="emphasis"><em>References</em></span> section of the chapter).</p></div><div class="section" title="Microsoft Azure"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec53"/>Microsoft Azure</h2></div></div></div><p>Microsoft Azure<a id="id447" class="indexterm"/> has full support for R and Spark. Microsoft <a id="id448" class="indexterm"/>had bought Revolution Analytics, a company that started building and supporting an enterprise version of R. Apart from this, Azure has a machine learning service where there are APIs for some Bayesian machine learning models as well. A nice video tutorial of how to launch instances on Azure and how to use their machine learning as a service can be found at the Microsoft Virtual Academy website (reference 8 in the <span class="emphasis"><em>References</em></span> section of the chapter).</p></div><div class="section" title="IBM Bluemix"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec54"/>IBM Bluemix</h2></div></div></div><p>Bluemix <a id="id449" class="indexterm"/>has full <a id="id450" class="indexterm"/>support for R through the full set of R libraries available on their instances. IBM also has integration of Spark into their cloud services in their roadmap plans. More details can be found at their documentation page (reference 9 in the <span class="emphasis"><em>References</em></span> section of the chapter).</p></div></div>
<div class="section" title="Other R packages for large scale machine learning" id="aid-26I9K1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec68"/>Other R packages for large scale machine learning</h1></div></div></div><p>Apart from<a id="id451" class="indexterm"/> RHadoop and SparkR, there are several other native R packages specifically built for large-scale machine learning. Here, we give a brief overview of them. Interested readers should refer to <span class="emphasis"><em>CRAN Task View: High-Performance and Parallel Computing with R</em></span> (reference 10 in the <span class="emphasis"><em>References</em></span> section of the chapter).</p><p>Though R is <a id="id452" class="indexterm"/>single-threaded, there exists several packages for parallel computation in R. Some of the well-known packages are <a id="id453" class="indexterm"/>
<span class="strong"><strong>Rmpi</strong></span> (R version of the popular message passing interface), <a id="id454" class="indexterm"/>
<span class="strong"><strong>multicore</strong></span>, <span class="strong"><strong>snow</strong></span> (for building R clusters), and <a id="id455" class="indexterm"/>
<span class="strong"><strong>foreach</strong></span>. From R 2.14.0, a <a id="id456" class="indexterm"/>new package called <a id="id457" class="indexterm"/>
<span class="strong"><strong>parallel</strong></span> started shipping with the base R. We will discuss some of its features here.</p><div class="section" title="The parallel R package"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec55"/>The parallel R package</h2></div></div></div><p>The <a id="id458" class="indexterm"/>
<span class="strong"><strong>parallel</strong></span> package is <a id="id459" class="indexterm"/>built on top of the multicore and snow packages. It is useful for running a single program on multiple datasets such as K-fold cross validation. It can be used for parallelizing in a single machine over multiple CPUs/cores or across several machines. For parallelizing across a cluster of machines, it evokes MPI (message passing interface) using the Rmpi package.</p><p>We will illustrate the use of parallel package with the simple example of computing a square of numbers in the list 1:100000. This example will not work in Windows since the corresponding R does not support the multicore package. It can be tested on any Linux or OS X platform.</p><p>The sequential way of performing this operation is to use the <code class="literal">lapply</code> function as follows:</p><div class="informalexample"><pre class="programlisting">&gt;nsquare &lt;- function(n){return(n*n)}
&gt;range &lt;- c(1:100000)
&gt;system.time(lapply(range,nsquare))</pre></div><p>Using the <code class="literal">mclapply</code> function of the parallel package, this computation can be achieved in much less time:</p><div class="informalexample"><pre class="programlisting">&gt;library(parallel) #included in R core packages, no separate installation required
&gt;numCores&lt;-detectCores( )  #to find the number of cores in the machine
&gt;system.time(mclapply(range,nsquare,mc.cores=numCores))</pre></div><p>If the dataset is so large that it needs a cluster of computers, we can use the <code class="literal">parLapply</code> function to run the program over a cluster. This needs the Rmpi package:</p><div class="informalexample"><pre class="programlisting">&gt;install.packages(Rmpi)#one time
&gt;library(Rmpi)
&gt;numNodes&lt;-4 #number of workers nodes
&gt;cl&lt;-makeCluster(numNodes,type="MPI")
&gt;system.time(parLapply(cl,range,nsquare))
&gt;stopCluster(cl)
&gt;mpi.exit( )</pre></div></div><div class="section" title="The foreach R package"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec56"/>The foreach R package</h2></div></div></div><p>This<a id="id460" class="indexterm"/> is a new looping <a id="id461" class="indexterm"/>construct in R that can be executed in parallel across multicores or clusters. It has two important operators: <code class="literal">%do%</code> for repeatedly doing a task and <code class="literal">%dopar%</code> for executing tasks in parallel. </p><p>For example, the squaring function we discussed in the previous section can be implemented using a single line command using the foreach package:</p><div class="informalexample"><pre class="programlisting">&gt;install.packages(foreach)#one time
&gt;install.packages(doParallel)#one time
&gt;library(foreach)
&gt;library(doParallel)
&gt;system.time(foreach(i=1:100000)   %do%  i^2) #for executing sequentially
&gt;system.time(foreach(i=1:100000)   %dopar%  i^2) #for executing in parallel</pre></div><p>We will also do an example of quick sort using the <code class="literal">foreach</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;qsort&lt;- function(x) {
  n &lt;- length(x)
  if (n == 0) {
    x
  } else {
    p &lt;- sample(n,1)
    smaller &lt;- foreach(y=x[-p],.combine=c) %:% when(y &lt;= x[p]) %do% y
    larger  &lt;- foreach(y=x[-p],.combine=c) %:% when(y &gt;  x[p]) %do% y
    c(qsort(smaller),x[p],qsort(larger))
  }
}
qsort(runif(12))</pre></div><p>These packages are still undergoing a lot of development. They have not yet been used in a large way for Bayesian modeling. It is easy to use them for Bayesian inference applications such as Monte Carlo simulations.</p></div></div>
<div class="section" title="Exercises" id="aid-27GQ61"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec69"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Revisit the<a id="id462" class="indexterm"/> classification problem in <a class="link" title="Chapter 6. Bayesian Classification Models" href="part0049.xhtml#aid-1ENBI1">Chapter 6</a>, <span class="emphasis"><em>Bayesian Classification Models</em></span>. Repeat the same problem using the <code class="literal">glm()</code> function of SparkR.</li><li class="listitem">Revisit the linear regression problem, we did in this chapter, using SparkR. After creating the AWS instance, repeat this problem using RStudio server on AWS.</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-28FAO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec70"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">"MapReduce Implementation of Variational Bayesian Probabilistic Matrix Factorization Algorithm". In: IEEE Conference on Big Data. pp 145-152. 2013</li><li class="listitem">Dean J. and Ghemawat S. "MapReduce: Simplified Data Processing on Large Clusters". Communications of the ACM 51 (1). 107-113</li><li class="listitem"><a class="ulink" href="https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R">https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R</a></li><li class="listitem">Chowdhury M., Das T., Dave A., Franklin M.J., Ma J., McCauley M., Shenker S., Stoica I., and Zaharia M. "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing". NSDI 2012. 2012</li><li class="listitem"><span class="emphasis"><em>Amazon Elastic Compute Cloud (EC2) User Guide</em></span>, Kindle e-book by Amazon Web Services, updated April 9, 2014</li><li class="listitem">Spark documentation for AWS at <a class="ulink" href="http://spark.apache.org/docs/latest/ec2-scripts.html">http://spark.apache.org/docs/latest/ec2-scripts.html</a></li><li class="listitem">AWS documentation for Spark at <a class="ulink" href="http://aws.amazon.com/elasticmapreduce/details/spark/">http://aws.amazon.com/elasticmapreduce/details/spark/</a></li><li class="listitem">Microsoft Virtual Academy website at <a class="ulink" href="http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning">http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning</a></li><li class="listitem">IBM Bluemix Tutorial at <a class="ulink" href="http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html">http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html</a></li><li class="listitem">CRAN Task View for contributed packages in R at <a class="ulink" href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">https://cran.r-project.org/web/views/HighPerformanceComputing.html</a></li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-29DRA1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec71"/>Summary</h1></div></div></div><p>In this last chapter of the book, we covered various frameworks to implement large-scale machine learning. These are very useful for Bayesian learning too. For example, to simulate from a posterior distribution, one could run a Gibbs sampling over a cluster of machines. We learned how to connect to Hadoop from R using the RHadoop package and how to use R with Spark using SparkR. We also discussed how to set up clusters in cloud services such as AWS and how to run Spark on them. Some of the native parallelization frameworks such as parallel and foreach functions were also covered.</p><p>The overall aim of this book was to introduce readers to the area of Bayesian modeling using R. Readers should have gained a good grasp of theory and concepts behind Bayesian machine learning models. Since the examples were mainly given for the purposes of illustration, I urge readers to apply these techniques to real-world problems to appreciate the subject of Bayesian inference more deeply.</p></div></body></html>