- en: What's Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The projects covered in this book can be considered bite-sized projects. They
    can be completed within a day or two. A real project will often take months. They
    require a combination of machine learning expertise, engineering expertise, and
    DevOps expertise. It would not quite be feasible to write about such projects
    without spanning multiple chapters while keeping the same level of detail. In
    fact, as can be witnessed by the progression of this book, as projects get more
    complex, the level of detail drops. In fact, the last two chapters are pretty
    thin.
  prefs: []
  type: TYPE_NORMAL
- en: 'All said and done, we''ve achieved quite a bit in this book. However, there
    is quite a bit we have not covered. This is owing to my own personal lack of expertise
    in some other fields in machine learning. In the introductory chapter, I noted
    that there are multiple classification schemes for machine learning systems and
    that we''d be choosing the common view that there are only unsupervised and supervised
    types of learning. Clearly, there are other classification schemes. Allow me to
    share another, one that has five classifications of machine learning systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Connectionist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analogizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symbolist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, I use the term machine learning. Others may use the term artificial intelligence
    to classify these systems. The difference is subtle. These five classes are technically
    schools of thought within artificial intelligence. And this sets a much larger
    stage for the topics at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Except for two, we have, in this book, explored the different schools of thought
    in artificial intelligence. In the Connectionist school, we started with linear
    regression in [Chapter 2](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml), *Linear
    Regression – House Price Prediction*, and the various neural networks from [Chapters
    8](26529196-995f-4689-91d7-0039b62337e9.xhtml), *Basic Facial Detection*, and
    [Chapter 10](3d4d68ef-7bd8-4e5d-9c3a-2d6a05d17842.xhtml), *What's Next?*. In the
    Bayesian school, we have Naive Bayes from [Chapter 3](d0447032-8c26-4198-a13c-b41ec20c02e8.xhtml),
    *Classification – Spam Email Detection*, as well as the DMMClust algorithm in
    [Chapter 6](d54b14d1-3403-4f2a-a98e-dd12adfe585a.xhtml), *Neural Networks – MNIST
    Handwriting Recognition*; we also have the various distance and clustering algorithms,
    which somewhat fall into the analogizer school of thought.
  prefs: []
  type: TYPE_NORMAL
- en: The two schools of thought on artificial intelligence that are not covered are
    the Evolutionary school and the Symbolist school. The former I only have theoretical
    experiences of. My understanding of the Evolutionary school of artificial intelligence
    is not great. I have much to learn from the likes of Martin Nowak. The latter,
    I am familiar with—I have been told that my introduction to Go betrays a lot of
    my experience with the Symbolist school of thought.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason why I didn't write anything about the Symbolist school of thought
    is that as a subject matter it is too dense, and I am not a good enough writer
    to actually tackle the subject. It opens up hairy philosophical implications more
    immediately than the Connectionist school does. These implications are something
    I am not yet ready to deal with, though the reader might be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having said that, one of the most exhilarating times in my life was building
    DeepMind''s AlphaGo algorithm in Go. You can find the code here: [https://github.com/gorgonia/agogo](https://github.com/gorgonia/agogo).
    It''s a behemoth of a project, and successfully pulled off by a small team of
    four. It was an immensely rewarding experience. The AlphaGo algorithm merges Connectionist
    deep neural networks with Symbolist tree search. Despite pulling off such a feat,
    I still do not think I am ready to write about the symbolic approach to artificial
    intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this brings up the question: what''s next?'
  prefs: []
  type: TYPE_NORMAL
- en: What should the reader focus on?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This question has been asked of me every time I give a class on machine learning
    and artificial intelligence. I mentioned in the introductory chapter that one
    may want to be a machine learning practitioner or a machine learning researcher.
    My professional role straddles both. This allows me some experience to provide
    a bit of advice for readers interested in either field.
  prefs: []
  type: TYPE_NORMAL
- en: The practitioner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To the practitioner, the most important skill is not in machine learning. The
    most important skill is in understanding the problem. Implicit in this statement
    is that the practitioner should also at least understand which machine learning
    algorithms would be suitable for the problem at hand. Obviously this entails understanding
    how the machine learning algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'New people in the field often ask me whether deep learning will solve all their
    problems. The answer is emphatically no. The solution must be tailored to the
    problem. Indeed, often, non-deep-learning solutions outperform deep learning solutions
    in terms of speed and accuracy. These are typically simple problems, so that''s
    a good rule of thumb there: if the problem is non-compositional, you most likely
    do not need to use deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: What do I mean by non-compositional? Recall from [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml), *How
    to Solve All Machine Learning Problems*, when I introduced the types of problems,
    and how problems may be broken down into subproblems. If the subproblems are themselves
    composed of further subproblems, well, that means the problem is *composed* of
    subproblems. Problems that aren't compositional do not need deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Granted, this is a very gross overview of the issue. A finer understanding of
    the problem is always required.
  prefs: []
  type: TYPE_NORMAL
- en: The researcher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To the researcher, the most important skill is understanding how a machine learning
    algorithm works at a high level. Following this, understanding data structures
    is the most important. From there, an actual algorithm may be written.
  prefs: []
  type: TYPE_NORMAL
- en: Of note would be the difference between data representation and data structure.
    Perhaps some day in the future—hopefully not too far from now—we will have programming
    languages where data representation does not matter. But now, data representation
    still matters. A good representation will yield an efficient algorithm. A poor
    representation yields poor algorithm performance.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, my advice is to start simple, by making things as understandable
    as possible as first. Then start subtracting the parts that are not necessary.
    A good example is shown in [Chapter 3](d0447032-8c26-4198-a13c-b41ec20c02e8.xhtml), *Classification
    – Spam Email Detection*, in Naive Bayes. A direct representation of the Bayesian
    function would be quite clunky. But in understanding the moving parts of the algorithm,
    we are able to make it efficient and small.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, some complexity is unavoidable. Some complexities are unavoidable
    because the algorithm is fundamentally complex. Some complexities are tradeoffs
    that are required. An example of this is the use of Gorgonia. Deep learning is
    at its heart, just writing a long mathematical expression. To update the weights,
    backpropagation is used. Backpropagation is simply differentiation. But nobody
    wants to manually calculate the differentiation! We want to mechanically evaluate
    our calculus! Therefore some complexity is unavoidable.
  prefs: []
  type: TYPE_NORMAL
- en: Wisdom lies in knowing when these complexities are unavoidable. Wisdom comes
    from experience, so to the researcher, my advice is to do as much as possible.
    Doing things at different scales also brings out different experiences. For example,
    performing K-means at scale across multiple machines is a very different code
    from the one presented in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The researcher, the practitioner, and their stakeholder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A word on scale—there is a tendency to reach out to packages or external programs,
    such as Spark, to solve the problem. Often they do solve the problem. But it's
    been my experience that ultimately, when doing things at scale, there is no one-size-fits-all
    solution. Therefore, it's good to learn the basics, so that when necessary, you
    may refer to the basics and extrapolate them to your situation.
  prefs: []
  type: TYPE_NORMAL
- en: Again on the topic of scale—both researchers and practitioners would do well
    to learn to plan projects. This is one thing that I am exceedingly bad at. Even
    with the help of multiple project managers, machine learning projects have a tendency
    to spiral out of control. It does take quite a bit of discipline to manage these.
    This is both on the implementor's part and on the stakeholder's part.
  prefs: []
  type: TYPE_NORMAL
- en: Last, learn to manage the expectations of stakeholders. Many of my projects
    fail. That I can say the projects fail is itself a qualifying statement. For most
    projects I enter into, I have defined success and failure criteria. If it's a
    more traditional statistics-based project, then these are your simple null hypotheses.
    Failing to reject the null hypothesis would then be a failure. Likewise, more
    complicated projects would have multiple hypotheses—these come in form of F-scores
    and the like. Learn these tools well, and communicate them to your stakeholders.
    You must be aware that a large majority of machine learning projects fail on their
    first few attempts.
  prefs: []
  type: TYPE_NORMAL
- en: Where can I learn more?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I strongly believe machine learning methods should not be tied to programming
    languages. If tomorrow a new language comes out that offers better performance
    than Go, while keeping the developer friendliness of Go, I'd move to that language
    in a heartbeat. I wouldn't have to be worried about having to relearn new machine
    learning methods. I already know them. I can simply rewrite them in that new language.
    As such, my recommendations would be language-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about machine learning algorithms, I recommend Christopher
    Bishop's, *Pattern Recognition and Machine Learning*. It's a slightly older book,
    but you'll be surprised at how many new developments in machine learning have
    their roots in that tome.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about deep learning, I recommend Ian Goodfellow and
    Yoshua Bengio's, *Deep Learning*. It's a new book—it's extremely theoretical,
    with no code, but the insights gained will be priceless.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about deep learning using Go and Gorgonia, there is
    an upcoming book by Darrell Chua and Gareth Seneque, published by Packt. It covers
    a wide range of deep-learning-related topics.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about data science and machine learning in Go, I also
    recommend Daniel Whitenack's, *Machine Learning with Go*. It's one of the first
    books on machine learning in Go, and to this day, it still stands as an excellent
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about Go, I highly recommend *The Go Programming Language*,
    by Alan Donovan and Brian Kernighan. **Kernighan** is the **K** in the famous
    **K&amp;R** book on C. Here, he performs a similar feat.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for reading this book; I hope it has been useful to you.
  prefs: []
  type: TYPE_NORMAL
