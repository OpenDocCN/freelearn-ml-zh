- en: What's Next?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: The projects covered in this book can be considered bite-sized projects. They
    can be completed within a day or two. A real project will often take months. They
    require a combination of machine learning expertise, engineering expertise, and
    DevOps expertise. It would not quite be feasible to write about such projects
    without spanning multiple chapters while keeping the same level of detail. In
    fact, as can be witnessed by the progression of this book, as projects get more
    complex, the level of detail drops. In fact, the last two chapters are pretty
    thin.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书涵盖的项目可以被认为是小项目。它们可以在一天或两天内完成。一个真实的项目通常需要几个月。它们需要机器学习专业知识、工程专业知识以及DevOps专业知识。在没有跨越多个章节的同时保持相同详细程度的情况下，很难详细地描述这样的项目。事实上，正如这本书的进展所见证的，随着项目的复杂度增加，详细程度会降低。事实上，最后两章相当简略。
- en: 'All said and done, we''ve achieved quite a bit in this book. However, there
    is quite a bit we have not covered. This is owing to my own personal lack of expertise
    in some other fields in machine learning. In the introductory chapter, I noted
    that there are multiple classification schemes for machine learning systems and
    that we''d be choosing the common view that there are only unsupervised and supervised
    types of learning. Clearly, there are other classification schemes. Allow me to
    share another, one that has five classifications of machine learning systems:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们在这本书中已经取得了相当大的成就。然而，我们还有很多没有涉及的内容。这归因于我在机器学习的一些其他领域缺乏个人专业知识。在介绍章节中，我提到机器学习系统有多种分类方案，并且我们将选择常见的观点，即只有无监督学习和监督学习两种类型。显然，还有其他的分类方案。让我分享另一个，它将机器学习系统分为五个类别：
- en: Connectionist
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接主义
- en: Evolutionary
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化
- en: Bayesian
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯
- en: Analogizer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类比者
- en: Symbolist
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号
- en: Here, I use the term machine learning. Others may use the term artificial intelligence
    to classify these systems. The difference is subtle. These five classes are technically
    schools of thought within artificial intelligence. And this sets a much larger
    stage for the topics at hand.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用术语机器学习。其他人可能使用人工智能来对这些系统进行分类。这种差异是微妙的。这五个类别在技术上属于人工智能的思想流派。这为手头的话题设定了一个更大的舞台。
- en: Except for two, we have, in this book, explored the different schools of thought
    in artificial intelligence. In the Connectionist school, we started with linear
    regression in [Chapter 2](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml), *Linear
    Regression – House Price Prediction*, and the various neural networks from [Chapters
    8](26529196-995f-4689-91d7-0039b62337e9.xhtml), *Basic Facial Detection*, and
    [Chapter 10](3d4d68ef-7bd8-4e5d-9c3a-2d6a05d17842.xhtml), *What's Next?*. In the
    Bayesian school, we have Naive Bayes from [Chapter 3](d0447032-8c26-4198-a13c-b41ec20c02e8.xhtml),
    *Classification – Spam Email Detection*, as well as the DMMClust algorithm in
    [Chapter 6](d54b14d1-3403-4f2a-a98e-dd12adfe585a.xhtml), *Neural Networks – MNIST
    Handwriting Recognition*; we also have the various distance and clustering algorithms,
    which somewhat fall into the analogizer school of thought.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了两个之外，在这本书中，我们已经探讨了人工智能的不同思想流派。在连接主义学派中，我们从第二章的线性回归开始，即*线性回归 – 房价预测*，以及第八章的*基本面部检测*和第十章的*接下来是什么*中提到的各种神经网络。在贝叶斯学派中，我们有第三章的朴素贝叶斯，即*分类
    – 垃圾邮件检测*，以及第六章的DMMClust算法，即*神经网络 – MNIST手写识别*；我们还有各种距离和聚类算法，这些多少有些属于类比学派。
- en: The two schools of thought on artificial intelligence that are not covered are
    the Evolutionary school and the Symbolist school. The former I only have theoretical
    experiences of. My understanding of the Evolutionary school of artificial intelligence
    is not great. I have much to learn from the likes of Martin Nowak. The latter,
    I am familiar with—I have been told that my introduction to Go betrays a lot of
    my experience with the Symbolist school of thought.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 没有涵盖的人工智能两种思想流派是进化学派和符号学派。前者我只有一些理论经验。我对人工智能进化学派的理解并不深刻。我从马丁·诺瓦克这样的人那里有很多要学习的。后者，我比较熟悉——有人告诉我，我对围棋的介绍暴露了我对符号学派思想的大量经验。
- en: The main reason why I didn't write anything about the Symbolist school of thought
    is that as a subject matter it is too dense, and I am not a good enough writer
    to actually tackle the subject. It opens up hairy philosophical implications more
    immediately than the Connectionist school does. These implications are something
    I am not yet ready to deal with, though the reader might be.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有写关于符号主义学派的文章的主要原因是因为作为一个主题，它太密集了，而且我并不是一个足够好的作家来真正处理这个主题。它比连接主义学派更直接地开启了棘手的哲学含义。这些含义是我目前还没有准备好处理的，尽管读者可能准备好了。
- en: 'Having said that, one of the most exhilarating times in my life was building
    DeepMind''s AlphaGo algorithm in Go. You can find the code here: [https://github.com/gorgonia/agogo](https://github.com/gorgonia/agogo).
    It''s a behemoth of a project, and successfully pulled off by a small team of
    four. It was an immensely rewarding experience. The AlphaGo algorithm merges Connectionist
    deep neural networks with Symbolist tree search. Despite pulling off such a feat,
    I still do not think I am ready to write about the symbolic approach to artificial
    intelligence.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，我一生中最令人兴奋的时刻之一就是构建DeepMind的AlphaGo围棋算法。你可以在这里找到代码：[https://github.com/gorgonia/agogo](https://github.com/gorgonia/agogo)。这是一个庞大的项目，由一个由四个人组成的小团队成功完成。这是一次极具回报的经历。AlphaGo算法将连接主义深度神经网络与符号主义树搜索相结合。尽管取得了这样的成就，我仍然认为自己还没有准备好撰写关于符号人工智能方法的文章。
- en: 'All of this brings up the question: what''s next?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都引发了一个问题：接下来是什么？
- en: What should the reader focus on?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者应该关注什么？
- en: This question has been asked of me every time I give a class on machine learning
    and artificial intelligence. I mentioned in the introductory chapter that one
    may want to be a machine learning practitioner or a machine learning researcher.
    My professional role straddles both. This allows me some experience to provide
    a bit of advice for readers interested in either field.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我上机器学习和人工智能的课程时，都会有人问我这个问题。我在引言章节中提到，一个人可能想成为一名机器学习从业者或研究者。我的专业角色横跨这两个领域。这使得我有了一些经验，可以为对这两个领域感兴趣的读者提供一些建议。
- en: The practitioner
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践者
- en: To the practitioner, the most important skill is not in machine learning. The
    most important skill is in understanding the problem. Implicit in this statement
    is that the practitioner should also at least understand which machine learning
    algorithms would be suitable for the problem at hand. Obviously this entails understanding
    how the machine learning algorithm works.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从业者来说，最重要的技能不在于机器学习。最重要的技能在于理解问题。这句话隐含的意思是，从业者至少应该了解哪些机器学习算法适合当前的问题。显然，这需要理解机器学习算法是如何工作的。
- en: 'New people in the field often ask me whether deep learning will solve all their
    problems. The answer is emphatically no. The solution must be tailored to the
    problem. Indeed, often, non-deep-learning solutions outperform deep learning solutions
    in terms of speed and accuracy. These are typically simple problems, so that''s
    a good rule of thumb there: if the problem is non-compositional, you most likely
    do not need to use deep learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 新进入这个领域的人经常问我，深度学习是否能够解决他们所有的问题。答案是明确地不。解决方案必须针对问题量身定制。实际上，在速度和准确性方面，非深度学习解决方案往往优于深度学习解决方案。这些通常是简单的问题，所以这里有一个好的经验法则：如果问题是不可组合的，你很可能不需要使用深度学习。
- en: What do I mean by non-compositional? Recall from [Chapter 1](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml), *How
    to Solve All Machine Learning Problems*, when I introduced the types of problems,
    and how problems may be broken down into subproblems. If the subproblems are themselves
    composed of further subproblems, well, that means the problem is *composed* of
    subproblems. Problems that aren't compositional do not need deep learning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我所说的不可组合是指什么？回想一下[第1章](3d68e167-a44d-4195-a270-f8180ff8f85f.xhtml)，“如何解决所有机器学习问题”，当我介绍问题类型以及问题如何分解为子问题时。如果子问题本身又由更小的子问题组成，那么，这意味着问题是由子问题组成的。不可组合的问题不需要深度学习。
- en: Granted, this is a very gross overview of the issue. A finer understanding of
    the problem is always required.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个非常粗略的问题概述。对问题的更深入理解总是必要的。
- en: The researcher
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究者
- en: To the researcher, the most important skill is understanding how a machine learning
    algorithm works at a high level. Following this, understanding data structures
    is the most important. From there, an actual algorithm may be written.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对研究者来说，最重要的技能是理解机器学习算法在高级别是如何工作的。在此基础上，理解数据结构是最重要的。然后，才能编写实际的算法。
- en: Of note would be the difference between data representation and data structure.
    Perhaps some day in the future—hopefully not too far from now—we will have programming
    languages where data representation does not matter. But now, data representation
    still matters. A good representation will yield an efficient algorithm. A poor
    representation yields poor algorithm performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是数据表示和数据结构之间的区别。也许在未来的某一天——希望不会太远——我们将拥有编程语言，其中数据表示不再重要。但现在，数据表示仍然很重要。良好的表示会产生高效的算法。差的表示会导致算法性能不佳。
- en: For the most part, my advice is to start simple, by making things as understandable
    as possible as first. Then start subtracting the parts that are not necessary.
    A good example is shown in [Chapter 3](d0447032-8c26-4198-a13c-b41ec20c02e8.xhtml), *Classification
    – Spam Email Detection*, in Naive Bayes. A direct representation of the Bayesian
    function would be quite clunky. But in understanding the moving parts of the algorithm,
    we are able to make it efficient and small.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我的建议是先从简单开始，尽可能使事物易于理解。然后开始去除不必要的部分。一个很好的例子可以在[第3章](d0447032-8c26-4198-a13c-b41ec20c02e8.xhtml)中找到，*分类
    – 垃圾邮件检测*，在朴素贝叶斯中。贝叶斯函数的直接表示会相当笨拙。但在理解算法的动态部分时，我们能够使其高效且小巧。
- en: Sometimes, some complexity is unavoidable. Some complexities are unavoidable
    because the algorithm is fundamentally complex. Some complexities are tradeoffs
    that are required. An example of this is the use of Gorgonia. Deep learning is
    at its heart, just writing a long mathematical expression. To update the weights,
    backpropagation is used. Backpropagation is simply differentiation. But nobody
    wants to manually calculate the differentiation! We want to mechanically evaluate
    our calculus! Therefore some complexity is unavoidable.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，一些复杂性不可避免。有些复杂性不可避免，因为算法本质上很复杂。有些复杂性是必须做出的权衡。一个例子是使用Gorgonia。深度学习在本质上只是写一个长的数学表达式。为了更新权重，使用反向传播。反向传播仅仅是微分。但没有人愿意手动计算微分！我们希望机械地评估我们的微积分！因此，一些复杂性不可避免。
- en: Wisdom lies in knowing when these complexities are unavoidable. Wisdom comes
    from experience, so to the researcher, my advice is to do as much as possible.
    Doing things at different scales also brings out different experiences. For example,
    performing K-means at scale across multiple machines is a very different code
    from the one presented in the previous chapters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 智慧在于知道何时这些复杂性不可避免。智慧来自经验，因此对研究者来说，我的建议是尽可能多做。在不同的规模上做事也会带来不同的经验。例如，在多台机器上执行K-means算法与前面章节中展示的代码非常不同。
- en: The researcher, the practitioner, and their stakeholder
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究者、实践者和他们的利益相关者
- en: A word on scale—there is a tendency to reach out to packages or external programs,
    such as Spark, to solve the problem. Often they do solve the problem. But it's
    been my experience that ultimately, when doing things at scale, there is no one-size-fits-all
    solution. Therefore, it's good to learn the basics, so that when necessary, you
    may refer to the basics and extrapolate them to your situation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于规模的问题——有一种趋势是求助于包或外部程序，例如Spark，来解决问题。通常它们确实解决了问题。但根据我的经验，最终，在规模化的工作中，没有一种适合所有情况的解决方案。因此，学习基础知识是很好的，这样在必要时，你可以参考基础知识并将它们应用到你的情况中。
- en: Again on the topic of scale—both researchers and practitioners would do well
    to learn to plan projects. This is one thing that I am exceedingly bad at. Even
    with the help of multiple project managers, machine learning projects have a tendency
    to spiral out of control. It does take quite a bit of discipline to manage these.
    This is both on the implementor's part and on the stakeholder's part.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 再次谈到规模的问题——研究人员和从业者都应该学会规划项目。这是我最不擅长的事情之一。即使有多个项目经理的帮助，机器学习项目也往往容易失控。管理这些项目确实需要相当多的自律。这既涉及实施者的部分，也涉及利益相关者的部分。
- en: Last, learn to manage the expectations of stakeholders. Many of my projects
    fail. That I can say the projects fail is itself a qualifying statement. For most
    projects I enter into, I have defined success and failure criteria. If it's a
    more traditional statistics-based project, then these are your simple null hypotheses.
    Failing to reject the null hypothesis would then be a failure. Likewise, more
    complicated projects would have multiple hypotheses—these come in form of F-scores
    and the like. Learn these tools well, and communicate them to your stakeholders.
    You must be aware that a large majority of machine learning projects fail on their
    first few attempts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，学会管理利益相关者的期望。我的许多项目都失败了。我能说项目失败了本身就是一种资格证明。对于我参与的多数项目，我都已经定义了成功和失败的标准。如果是一个更传统的基于统计学的项目，那么这些就是你的简单零假设。未能拒绝零假设则被视为失败。同样，更复杂的项目会有多个假设——这些通常以F分数等形式出现。熟练掌握这些工具，并将它们传达给你的利益相关者。你必须意识到，绝大多数机器学习项目在最初的几次尝试中都会失败。
- en: Where can I learn more?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我在哪里可以学到更多？
- en: I strongly believe machine learning methods should not be tied to programming
    languages. If tomorrow a new language comes out that offers better performance
    than Go, while keeping the developer friendliness of Go, I'd move to that language
    in a heartbeat. I wouldn't have to be worried about having to relearn new machine
    learning methods. I already know them. I can simply rewrite them in that new language.
    As such, my recommendations would be language-agnostic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信机器学习方法不应该与编程语言绑定。如果明天出现了一种新的语言，它提供了比Go更好的性能，同时保持了Go的开发友好性，我会毫不犹豫地转向这种语言。我不必担心不得不重新学习新的机器学习方法。我已经知道了它们。我只需简单地在新语言中重写它们。因此，我的建议将是语言无关的。
- en: If you want to learn more about machine learning algorithms, I recommend Christopher
    Bishop's, *Pattern Recognition and Machine Learning*. It's a slightly older book,
    but you'll be surprised at how many new developments in machine learning have
    their roots in that tome.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于机器学习算法的知识，我推荐克里斯托弗·贝斯特的《*模式识别与机器学习*》。这是一本稍微有些年代的书，但你可能会惊讶地发现，许多机器学习的新发展都源于这本书。
- en: If you want to learn more about deep learning, I recommend Ian Goodfellow and
    Yoshua Bengio's, *Deep Learning*. It's a new book—it's extremely theoretical,
    with no code, but the insights gained will be priceless.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于深度学习的知识，我推荐伊恩·古德费洛和约书亚·本吉奥的《*深度学习*》。这是一本新书——它非常理论化，没有代码，但获得的见解将是无价的。
- en: If you want to learn more about deep learning using Go and Gorgonia, there is
    an upcoming book by Darrell Chua and Gareth Seneque, published by Packt. It covers
    a wide range of deep-learning-related topics.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于使用Go和Gorgonia进行深度学习的知识，有一本即将出版的书由达雷尔·丘亚和加雷思·塞内克撰写，由Packt出版。它涵盖了广泛的深度学习相关主题。
- en: If you want to learn more about data science and machine learning in Go, I also
    recommend Daniel Whitenack's, *Machine Learning with Go*. It's one of the first
    books on machine learning in Go, and to this day, it still stands as an excellent
    resource.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于Go语言中的数据科学和机器学习，我也推荐丹尼尔·惠特纳克的《*用Go进行机器学习*》。这是关于Go语言中机器学习的第一本书之一，时至今日，它仍然是一个出色的资源。
- en: If you want to learn more about Go, I highly recommend *The Go Programming Language*,
    by Alan Donovan and Brian Kernighan. **Kernighan** is the **K** in the famous
    **K&amp;R** book on C. Here, he performs a similar feat.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于Go语言的知识，我强烈推荐艾伦·多诺万和布莱恩·克尼汉的《*Go编程语言*》。**克尼汉**是著名C语言书籍**K&R**中的**K**。在这里，他完成了类似的壮举。
- en: Thank you
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谢谢
- en: Thank you for reading this book; I hope it has been useful to you.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您阅读这本书；我希望它对您有所帮助。
