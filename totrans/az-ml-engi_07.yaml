- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying ML Models for Batch Scoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deploying ML models for batch scoring** supports making predictions using
    a large volume of data. This solution supports use cases when you don’t need your
    model predictions immediately, but rather minutes or hours later. If you need
    to provide inferencing once a day, week, or month, using a large dataset, batch
    inferencing is ideal.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch inferencing allows data scientists and ML professionals to leverage cloud
    compute when needed, rather than paying for compute resources to be available
    for real-time responses. This means that compute resources can be spun up to support
    batch inferencing and spun down after the results have been provided to the business
    users. We are going to show you how to leverage the Azure Machine Learning service
    to deploy trained models to managed endpoints, which are HTTPS REST APIs that
    clients can invoke to get the score results of a trained model for batch inferencing
    using the studio and the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model for batch inferencing using the Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a model for batch inferencing through the Python SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to access your workspace, recall the steps from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the left side of the workspace’s UI, on click **Compute**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the compute screen, select your compute instance and select **Start**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Start compute](img/B18003_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Start compute
  prefs: []
  type: TYPE_NORMAL
- en: Your compute instance will change from the **Stopped** to the **Starting** status.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous chapter, we cloned the Git repository. If you have not done
    this, continue with this step. If you have already cloned the repository, skip
    to *step 7*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal on your compute instance. Note that the path will include
    your user in the directory. Type the following into the terminal to clone the
    sample notebooks into your working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Clicking on the refresh icon shown in*Figure 7**.2* will update and refresh
    the notebooks displayed on your screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Refresh Icon](img/B18003_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Refresh Icon
  prefs: []
  type: TYPE_NORMAL
- en: 'Review the notebooks in your `Azure-Machine-Learning-Engineering` directory.
    This will display the files cloned into your working directory, as shown in *Figure
    7**.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Azure-Machine-Learning-Engineering](img/B18003_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Azure-Machine-Learning-Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with deploying a model for batch inferencing using the studio next.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model for batch inferencing using the Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 3*](B18003_03.xhtml#_idTextAnchor053), *Training Machine Learning
    Models in AMLS*, we trained a model and registered it in an Azure Machine Learning
    workspace. We are going to deploy that model to a managed batch endpoint for batch
    scoring:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to your Azure Machine Learning workspace, select **Models** from the
    left menu bar to see the models registered in your workspace, and select **titanic_servival_model_**,
    as shown in *Figure 7**.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.4 – List of models registered in the workspace](img/B18003_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – List of models registered in the workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Deploy** and select **Deploy to batch endpoint**, as shown in *Figure
    7**.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Deploy the selected model to a batch endpoint](img/B18003_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Deploy the selected model to a batch endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'This opens the deployment wizard. Use the following values for the required
    fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`titanic-survival-batch-endpoint`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: Retain the default of **titanic_survival_model_**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`titanic-deployment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment**: For the selected model, the scoring script and environment
    are auto-generated for you'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cluster cpu-cluster`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Review your batch deployment specs and click **Create deployment**, as shown
    in *Figure 7**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Create batch deployment](img/B18003_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Create batch deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'After a minute or so, you should see a page that shows that your batch endpoint
    has been deployed successfully. It also has some information about your batch
    endpoint, as shown in *Figure 7**.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Successful batch deployment](img/B18003_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Successful batch deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have a batch endpoint up and running, let’s create a scoring/inferencing
    job that invokes your endpoint. To do so, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Endpoints** in the left menu bar and click on your recently created
    batch endpoint, as shown in *Figure 7**.8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – List of deployed batch endpoints](img/B18003_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – List of deployed batch endpoints
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **titanic-survival-batch-endpoint** page, select the **Jobs** tab, as
    shown in *Figure 7**.9*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.9 – titanic-survival-batch-endpoint Jobs tab](img/B18003_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – titanic-survival-batch-endpoint Jobs tab
  prefs: []
  type: TYPE_NORMAL
- en: Click **Create job** and pick the corresponding values for the fields in the
    wizard as shown in *Figure 7**.10*. Click **Create** once you’re done. Please
    note that the deployment is pre-selected for you, but make sure the correct deployment
    is pre-selected. You should also have a test dataset called **titanic-test-data**,
    which was created in [*Chapter 3*](B18003_03.xhtml#_idTextAnchor053)*, Training
    Machine Learning Models* *in AMLS*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Create a batch scoring job](img/B18003_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Create a batch scoring job
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the size of the test dataset that you selected in the last step,
    it will take some time for the job to complete. Once it is complete, you will
    see a pipeline representing the batch scoring job for **titanic-survival-batch-endpoint**,
    as shown in *Figure 7**.11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Batch scoring job for titanic-survival-batch-endpoint](img/B18003_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Batch scoring job for titanic-survival-batch-endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in order to view the scoring results, click on the **batchscoring** step
    of the pipeline, click on **Job overview** on the right, select the **Outputs
    + logs** tab, and finally, click on **Show data outputs**, as shown in *Figure
    7**.12*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Batch scoring results](img/B18003_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – Batch scoring results
  prefs: []
  type: TYPE_NORMAL
- en: 'After you click on `predictions.csv` file has been saved. Click on this file
    and select the **Edit** tab in order to view the scoring results, as shown in
    *Figure 7**.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Viewing batch scoring results](img/B18003_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Viewing batch scoring results
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to deploy an existing model to a managed endpoint
    for batch inferencing using the studio. In the next section, we will show you
    how to deploy a model to a managed endpoint for batch scoring using the Python
    SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model for batch inferencing through the Python SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to deploy an existing model to a managed endpoint
    for batch inferencing using the Python SDK by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the workspace user interface on the left side, click **Compute**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.14 –  Compute instance icon](img/B18003_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Compute instance icon
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Compute** screen, select your compute instance and select **Start**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Start compute](img/B18003_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Start compute
  prefs: []
  type: TYPE_NORMAL
- en: Your compute instance will change from **Stopped** to **Starting**. Once the
    compute instance moves from **Starting** to **Running**, it is ready for use,
    so go ahead and clone our repository, which contains some sample notebooks to
    walk through.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Terminal** hyperlink under applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will open the terminal on your compute instance. Note that the path will
    include your user in the directory path. Type the following into the terminal
    to clone the sample notebooks into your working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Click on the Jupyter link under applications, and this will display the folders
    that were just cloned. Navigate to `chapter 7` and click on `Deploy_Model_for_Batch_Scoring.ipynb`
    to bring up the notebook for this section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 7**.16* shows the libraries that need to
    be imported and how to connect to the Azure ML workspace in your compute instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Import required libraries and connecting to the Azure Machine
    Learning workspace](img/B18003_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – Import required libraries and connecting to the Azure Machine
    Learning workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 7**.17* shows how to create a batch endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Creating a batch endpoint for inferencing](img/B18003_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Creating a batch endpoint for inferencing
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 7**.18* shows how to retrieve an existing
    model from the workspace and how to create a batch deployment. The batch deployment
    must specify the batch endpoint, the trained model, and the compute cluster that
    is used for scoring, along with other deployment parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Creating a batch deployment](img/B18003_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – Creating a batch deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, that your batch endpoint is ready to be invoked, you are going to pass
    some test data to the endpoint, as shown in *Figure 7**.19*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Creating a batch deployment](img/B18003_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Creating a batch deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 7**.20* shows the Python code required to
    monitor the batch scoring job that was submitted in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Creating a batch deployment](img/B18003_07_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – Creating a batch deployment
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on the output link from the previous step will open the workspace displaying
    the batch scoring job, as shown in *Figure 7**.21*. You can follow *steps 9* and
    *10* from the last section to see the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Batch scoring results](img/B18003_07_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Batch scoring results
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize the chapter next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot of topics in this chapter. We have shown you how to deploy
    a model for batch scoring using the studio and through the Python SDK. We also
    showed you how to pass some test data to be scored by the deployed model by invoking
    the batch endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about responsible AI and the capabilities
    within Azure Machine Learning that allow you to develop, assess, and deploy models
    more responsibly to minimize unwanted bias in your AI systems.
  prefs: []
  type: TYPE_NORMAL
