- en: Chapter 9. Describing and Matching Interest Points
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章 描述和匹配兴趣点
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Matching local templates
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配局部模板
- en: Describing and matching local intensity patterns
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述和匹配局部强度模式
- en: Matching keypoints with binary descriptors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用二进制描述符匹配关键点
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we learned how to detect special points in an image
    with the objective of subsequently performing local image analysis. These keypoints
    are chosen to be distinctive enough so that if a keypoint is detected on the image
    of an object, then the same point is expected to be detected in other images depicting
    the same object. We also described some more sophisticated interest point detectors
    that can assign a representative scale factor and/or an orientation to a keypoint.
    As we will see in this chapter, this additional information can be useful to normalize
    scene representations with respect to viewpoint variations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何通过检测图像中的特殊点来进行局部图像分析。这些关键点是选择得足够独特，以至于如果一个关键点在某个对象的图像上被检测到，那么在描述同一对象的其它图像中，我们期望同样点也会被检测到。我们还描述了一些更复杂的兴趣点检测器，这些检测器可以为关键点分配一个代表性的尺度因子和/或一个方向。正如我们将在本章中看到的，这些附加信息可以用来根据视点变化对场景表示进行归一化。
- en: In order to perform image analysis based on interest points, we now need to
    build rich representations that uniquely describe each of these keypoints. This
    chapter looks at different approaches that have been proposed to extract descriptors
    from interest points. These descriptors are generally 1D or 2D vectors of binary,
    integer, or floating-point numbers that describe a keypoint and its neighborhood.
    A good descriptor should be distinctive enough to uniquely represent each keypoint
    of an image; it should be robust enough to have the same points represented similarly
    in spite of possible illumination changes or viewpoint variations. Ideally, it
    should also be compact to reduce memory load and improve computational efficiency.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基于兴趣点进行图像分析，我们现在需要构建丰富的表示，以独特地描述每个关键点。本章将探讨已经提出的从兴趣点中提取描述符的不同方法。这些描述符通常是描述关键点和其邻域的1D或2D的二进制、整数或浮点数向量。一个好的描述符应该足够独特，能够唯一地表示图像中的每个关键点；它应该足够鲁棒，即使在可能的光照变化或视点变化的情况下，也能以相似的方式表示相同的点。理想情况下，它还应该紧凑，以减少内存负载并提高计算效率。
- en: One of the most common operations accomplished with keypoints is image matching.
    This task could be performed, for example, to relate two images of the same scene
    or to detect the occurrence of a target object in an image. Here, we will study
    some basic matching strategies, a subject that will be further discussed in the
    next chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用关键点完成的最常见操作之一是图像匹配。这项任务可以用来关联同一场景的两个图像，或者检测图像中目标对象的出现。在这里，我们将研究一些基本的匹配策略，这些策略将在下一章中进一步讨论。
- en: Matching local templates
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配局部模板
- en: Feature point matching is the operation by which one can put in correspondence
    points from one image to points from another image (or points from an image set).
    Image points should match when they correspond to the image of the same scene
    element in the real world.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征点匹配是通过将一个图像中的点与另一个图像（或图像集中的点）进行对应的过程。当图像点对应于现实世界中相同场景元素时，图像点应该匹配。
- en: A single pixel is certainly not sufficient to make a decision on the similarity
    of two keypoints. This is why an image patch around each keypoint must be considered
    during the matching process. If two patches correspond to the same scene element,
    then one might expect their pixels to exhibit similar values. A direct pixel-by-pixel
    comparison of pixel patches is the solution presented in this recipe. This is
    probably the simplest approach to feature point matching, but as we will see,
    it is not the most reliable one. Nevertheless, in several situations, it can give
    good results.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 单个像素当然不足以对两个关键点的相似性做出判断。这就是为什么在匹配过程中必须考虑每个关键点周围的图像块。如果两个块对应于同一场景元素，那么可以预期它们的像素将具有相似值。本食谱中提出的解决方案是直接逐像素比较像素块。这可能是特征点匹配中最简单的方法，但正如我们将看到的，它并不是最可靠的。尽管如此，在几种情况下，它仍然可以给出良好的结果。
- en: How to do it...
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Most often, patches are defined as squares of odd sizes centered at the keypoint
    position. The similarity between two square patches can then be measured by comparing
    the corresponding pixel intensity values inside the patches. A simple **Sum of
    Squared Differences** (**SSD**) is a popular solution. The feature matching strategy
    then works as follows. First, the keypoints are detected in each image. Here,
    we use the FAST detector:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，补丁被定义为以关键点位置为中心的奇数大小的正方形。然后可以通过比较补丁内相应的像素强度值来测量两个正方形补丁之间的相似性。简单的**平方差之和**（**SSD**）是一个流行的解决方案。特征匹配策略的工作方式如下。首先，在每个图像中检测关键点。在这里，我们使用
    FAST 检测器：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note how we used the generic `cv::Ptr<cv::FeatureDetector>` pointer type, which
    can refer to any feature detector. One can then test this code on different interest
    point detectors just by changing the detector to be used when calling the `detect`
    function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们使用了通用的 `cv::Ptr<cv::FeatureDetector>` 指针类型，它可以指向任何特征检测器。然后，只需通过更改调用 `detect`
    函数时使用的检测器，就可以在不同的兴趣点检测器上测试此代码。
- en: 'The second step is to define a rectangle of, for example, size `11x11` that
    will be used to define patches around each keypoint:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是定义一个矩形，例如大小为 `11x11` 的矩形，该矩形将用于定义每个关键点周围的区域：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The keypoints in one image are compared with all the keypoints in the other
    image. For each keypoint of the first image, the most similar patch in the second
    image is identified. This process is implemented using two nested loops, as shown
    in the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图像中的关键点与另一个图像中的所有关键点进行比较。对于第一图像中的每个关键点，确定第二图像中最相似的补丁。这个过程通过使用两个嵌套循环来实现，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note the use of the `cv::matchTemplate` function, which we will describe in
    the next section and that computes the patch similarity score. When a potential
    match is identified, this match is represented through the use of a `cv::DMatch`
    object. This utility class stores the index of the two matching `keypoints` as
    well as their similarity score.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `cv::matchTemplate` 函数的使用，我们将在下一节中描述它，该函数计算补丁相似度得分。当识别出潜在的匹配时，此匹配通过使用 `cv::DMatch`
    对象来表示。这个实用类存储了两个匹配 `关键点` 的索引以及它们的相似度得分。
- en: 'The more similar the two image patches are, the higher the probability that
    these patches correspond to the same scene point. This is why it is a good idea
    to sort the resulting match points by their similarity scores:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 两个图像补丁越相似，这些补丁对应于同一场景点的概率就越高。这就是为什么按相似度得分对结果匹配点进行排序是一个好主意：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can then simply retain the matches that pass a given similarity threshold.
    Here, we chose to keep only the `N` best matching points (we use `N=25` to facilitate
    the visualization of the matching results).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以简单地保留通过给定相似度阈值的匹配。在这里，我们选择只保留 `N` 个最佳匹配点（我们使用 `N=25` 以便于可视化匹配结果）。
- en: 'Interestingly, there is an OpenCV function that can display the matching results
    by concatenating the two images and joining each corresponding point by a line.
    The function is used as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，OpenCV 中有一个函数可以通过连接两个图像并使用线条连接每个对应点来显示匹配结果。该函数的使用方法如下：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the match results:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是匹配结果：
- en: '![How to do it...](img/image_09_001-1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/image_09_001-1.jpg)'
- en: How it works...
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The results obtained are certainly not perfect, but a visual inspection of the
    matched image points shows a number of successful matches. It can also be observed
    that the symmetry of the two towers of the church causes some confusion. Also,
    since we tried to match all the points in the left image with the ones in the
    right image, we obtained cases where a point in the right image was matched with
    multiple points in the left image. This is an asymmetrical matching situation
    that can be corrected by, for example, keeping only the match with the best score
    for each point in the right image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的结果当然不是完美的，但通过视觉检查匹配的图像点显示有许多成功的匹配。还可以观察到教堂两座塔的对称性造成了一些混淆。此外，由于我们试图将左图像中的所有点与右图像中的点匹配，我们得到了右图像中的一个点与左图像中的多个点匹配的情况。这是一种不对称的匹配情况，可以通过例如，只保留右图像中每个点的最佳得分匹配来纠正。
- en: 'To compare the image patches from each image, here we used a simple criterion,
    that is, a pixel-per-pixel sum of the squared difference specified using the `cv::TM_SQDIFF`
    flag. If we compare the point `(x,y)` of image `I1` with a putative match at `(x'',y'')`
    in image `I2`, then the similarity measure is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较每张图像的图像块，我们在这里使用了一个简单的标准，即使用 `cv::TM_SQDIFF` 标志指定的像素级平方差之和。如果我们比较图像 `I1`
    中的点 `(x,y)` 与图像 `I2` 中的潜在匹配点 `(x',y')`，那么相似度度量如下：
- en: '![How it works...](img/image_09_002-1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_09_002-1.jpg)'
- en: Here, the sum of the `(i,j)` point provides the offset to cover the square template
    centered at each point. Since the difference between adjacent pixels in similar
    patches should be small, the best-matching patches should be the ones with the
    smallest sum. This is what is done in the main loop of the matching function;
    that is, for each keypoint in one image, we identify the keypoint in the other
    image that gives the lowest sum of the squared difference. We can also reject
    matches for which this sum is over a certain threshold value. In our case, we
    simply sort them from the most similar to the least similar ones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`(i,j)` 点的和提供了覆盖以每个点为中心的正方形模板的偏移量。由于相似块中相邻像素之间的差异应该很小，因此最佳匹配的块应该是具有最小和的块。这正是匹配函数主循环中所做的；也就是说，对于一张图像中的每个关键点，我们识别另一张图像中给出平方差异和最低的关键点。我们还可以拒绝那些和超过某个阈值值的匹配。在我们的情况下，我们只是按相似度从高到低排序它们。
- en: In our example, the matching was done with square patches of size `11x11`. A
    larger neighborhood creates more distinctive patches, but it also makes them more
    sensitive to local scene variations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，匹配是通过大小为 `11x11` 的正方形块完成的。更大的邻域会产生更独特的块，但它也使它们对局部场景变化更敏感。
- en: 'Comparing two image windows from a simple sum of square differences will work
    relatively well as long as the two images show the scene from similar points of
    views and similar illumination. Indeed, a simple lighting change will increase
    or decrease all the pixel intensities of a patch, resulting in a large square
    difference. To make matching more invariant to lighting changes, other formulae
    could be used to measure the similarity between two image windows. OpenCV offers
    a number of these. A very useful formula is the normalized sum of square differences
    (the `cv::TM_SQDIFF_NORMED` flag):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 只要两张图像从相似的角度和相似的光照显示场景，比较两个图像窗口的简单平方差之和就会相对有效。确实，简单的光照变化会增加或减少一个块的所有像素强度，从而导致大的平方差。为了使匹配对光照变化更不变，可以使用其他公式来测量两个图像窗口之间的相似度。OpenCV
    提供了这些公式中的许多。一个非常有用的公式是归一化平方差之和（`cv::TM_SQDIFF_NORMED` 标志）：
- en: '![How it works...](img/image_09_003-1.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_09_003-1.jpg)'
- en: 'Other similarity measures are based on the concept of correlation, defined
    in the signal processing theory, as follows (with the `cv::TM_CCORR` flag):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其他相似度度量基于相关性的概念，这在信号处理理论中定义为以下（使用 `cv::TM_CCORR` 标志）：
- en: '![How it works...](img/image_09_004-1.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_09_004-1.jpg)'
- en: This value will be maximal when two patches are similar.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个块相似时，此值将达到最大。
- en: The identified matches are stored in a vector of `cv::DMatch` instances. Essentially,
    the `cv::DMatch` data structure contains a first index that refers to an element
    in the first vector of keypoints and a second index that refers to the matching
    feature in the second vector of keypoints. It also contains a real value that
    represents the distance between the two matched descriptors. This distance value
    is used in the definition of `operator<` when comparing two `cv::DMatch` instances.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 识别的匹配存储在一个 `cv::DMatch` 实例的向量中。本质上，`cv::DMatch` 数据结构包含一个指向第一个关键点向量中元素的第一个索引，以及一个指向第二个关键点向量中匹配特征的第二个索引。它还包含一个代表两个匹配描述符之间距离的实数值。这个距离值用于比较两个
    `cv::DMatch` 实例时 `operator<` 的定义。
- en: When we drew the matches in the previous section, we wanted to limit the number
    of lines to make the results more readable. Therefore, we only displayed the `25`
    matches that had the lowest distance. To do this, we used the `std::nth_element`
    function, which positions the Nth element at the Nth position, with all smaller
    elements placed before this element. Once this is done, the vector is simply purged
    of its remaining elements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中绘制匹配时，我们希望限制线条数量以使结果更易于阅读。因此，我们只显示了具有最低距离的 `25` 个匹配。为此，我们使用了 `std::nth_element`
    函数，该函数将第 N 个元素放置在第 N 个位置，所有较小的元素都放置在此元素之前。一旦完成，向量就简单地清除了其剩余的元素。
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The `cv::matchTemplate` function is at the heart of our feature matching method.
    We used it here in a very specific way, which is to compare two image patches.
    However, this function has been designed to be used in a more generic way.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv::matchTemplate`函数是我们特征匹配方法的核心。我们在这里以非常具体的方式使用了它，即比较两个图像块。然而，这个函数被设计成可以以更通用的方式使用。'
- en: Template matching
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模板匹配
- en: A common task in image analysis is to detect the occurrence of a specific pattern
    or object in an image. This can be done by defining a small image of the object,
    a template, and searching for a similar occurrence in a given image. In general,
    the search is limited to a region of interest inside which we think the object
    can be found. The template is then slid over this region, and a similarity measure
    is computed at each pixel location. This is the operation performed by the `cv::matchTemplate`
    function. The input is a template image of a small size and an image over which
    the search is performed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分析中的一个常见任务是检测图像中特定模式或对象的发生。这可以通过定义一个对象的小图像、模板，并在给定的图像中搜索相似的发生来实现。通常，搜索被限制在感兴趣的区域，我们认为对象可能位于该区域。然后，模板在这个区域上滑动，并在每个像素位置计算相似度度量。这是`cv::matchTemplate`函数执行的操作。输入是一个小尺寸的模板图像和搜索图像。
- en: 'The result is a `cv::Mat` function of floating-point values that correspond
    to the similarity score at each pixel location. If the template is of size `MxN`
    and the image is of size `WxH`, then the resulting matrix will have a size of
    `(W-M+1)x(H-N+1)`. In general, you will be interested in the location of the highest
    similarity; so, the typical template-matching code will look as follows (assuming
    that the target variable is our template):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个对应于每个像素位置相似度得分的浮点值`cv::Mat`函数。如果模板的大小为`MxN`，图像的大小为`WxH`，则结果矩阵的大小将为`(W-M+1)x(H-N+1)`。通常，你将关注最高相似度的位置；因此，典型的模板匹配代码将如下所示（假设目标变量是我们的模板）：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Remember that this is a costly operation, so you should limit the search area
    and use a template having a size of only a few pixels.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这是一个代价高昂的操作，所以你应该限制搜索区域，并使用只有几个像素大小的模板。
- en: See also
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The next recipe, *Describing and matching local intensity patterns*, describes
    the `cv::BFMatcher` class, which implements the matching strategy that was used
    in this recipe
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个方法，*描述和匹配局部强度模式*，描述了`cv::BFMatcher`类，该类实现了在本方法中使用的匹配策略。
- en: Describing and matching local intensity patterns
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 描述和匹配局部强度模式
- en: The SURF and SIFT keypoint detection algorithms, discussed in [Chapter 8](ch08.html
    "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*, define
    a location, an orientation, and a scale for each of the detected features. The
    scale factor information is useful for defining the size of a window of analysis
    around each feature point. Thus, the defined neighborhood would include the same
    visual information no matter at what scale of the object to which the feature
    belongs has been pictured. This recipe will show you how to describe an interest
    point's neighborhood using feature descriptors. In image analysis, the visual
    information included in this neighborhood can be used to characterize each feature
    point in order to make each point distinguishable from the others. Feature descriptors
    are usually N-dimensional vectors that describe a feature point in a way that
    is invariant to change in lighting and to small perspective deformations. Generally,
    descriptors can be compared using simple distance metrics, for example, the Euclidean
    distance. Therefore, they constitute a powerful tool that can be used in object
    matching applications.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html "第8章. 检测兴趣点") "检测兴趣点"中讨论的SURF和SIFT关键点检测算法，为每个检测到的特征定义了一个位置、一个方向和一个尺度。尺度因子信息对于定义每个特征点周围分析窗口的大小是有用的。因此，定义的邻域将包含无论对象所属特征的缩放级别如何的相同视觉信息。这个方法将向你展示如何使用特征描述符来描述兴趣点的邻域。在图像分析中，这个邻域包含的视觉信息可以用来表征每个特征点，以便使每个点与其他点区分开来。特征描述符通常是描述特征点的N维向量，以对光照变化和小型透视变形不变的方式描述。通常，描述符可以使用简单的距离度量进行比较，例如欧几里得距离。因此，它们构成了一个强大的工具，可以用于对象匹配应用。
- en: How to do it...
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The `cv::Feature2D` abstract class defines a number of member functions that
    are used to compute the descriptors of a list of keypoints. As most feature-based
    methods include both a detector and a descriptor component, the associated classes
    include both a `detect` function (to detect the interest points) and a `compute`
    function (to compute their descriptors). This is the case of the `cv::SURF` and
    `cv::SIFT` classes. Here is, for example, how you can detect and describe feature
    points in two images using one instance of `cv::SURF`:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv::Feature2D`抽象类定义了用于计算一系列关键点描述符的成员函数。由于大多数基于特征的方法都包括检测器和描述符组件，相关的类包括一个`detect`函数（用于检测兴趣点）和一个`compute`函数（用于计算它们的描述符）。这是`cv::SURF`和`cv::SIFT`类的情况。例如，以下是如何使用一个`cv::SURF`实例在两张图像中检测和描述特征点的方法：'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For SIFT, you simply call the `cv::SIFT::create` function. The result of the
    computation of the interest point descriptors is a matrix (that is, a `cv::Mat`
    instance) that will contain as many rows as the number of elements in the keypoint
    vector. Each of these rows is an N-dimensional descriptor vector. In the case
    of the SURF descriptor, it has a default size of `64`, and for SIFT, the default
    dimension is `128`. This vector characterizes the intensity pattern surrounding
    a feature point. The more similar the two feature points, the closer their descriptor
    vectors should be. Note that you do not have to necessarily use the SURF (SIFT)
    descriptor with SURF (SIFT) points; detectors and descriptors can be used in any
    combination.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SIFT，你只需调用`cv::SIFT::create`函数。计算兴趣点描述符的结果是一个矩阵（即一个`cv::Mat`实例），它将包含与关键点向量中元素数量一样多的行。这些行中的每一行都是一个N维描述符向量。在SURF描述符的情况下，它有一个默认大小为`64`，而对于SIFT，默认维度是`128`。这个向量表征了特征点周围的强度模式。两个特征点越相似，它们的描述符向量应该越接近。请注意，你不必一定使用SURF（SIFT）描述符与SURF（SIFT）点一起使用；检测器和描述符可以以任何组合使用。
- en: 'These descriptors will now be used to match our keypoints. Exactly as we did
    in the previous recipe, each feature descriptor vector in the first image is compared
    to all the feature descriptors in the second image. The pair that obtains the
    best score (that is, the pair with the lowest distance between the two descriptor
    vectors) is then kept as the best match for that feature. This process is repeated
    for all the features in the first image. Very conveniently, this process is implemented
    in OpenCV in the `cv::BFMatcher` class, so we do not need to re-implement the
    double loops that we previously built. This class is used as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些描述符现在将被用来匹配我们的关键点。正如我们在之前的食谱中所做的那样，第一张图像中的每个特征描述符向量都会与第二张图像中的所有特征描述符进行比较。获得最佳分数的配对（即两个描述符向量之间距离最低的配对）将被保留为该特征的最佳匹配。这个过程会重复应用于第一张图像中的所有特征。非常方便的是，这个过程在OpenCV的`cv::BFMatcher`类中得到了实现，因此我们不需要重新实现之前构建的双循环。这个类可以这样使用：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This class is a subclass of the `cv::DescriptorMatcher` class that defines the
    common interface for different matching strategies. The result is a vector of
    the `cv::DMatch` instances.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类是`cv::DescriptorMatcher`类的子类，它定义了不同匹配策略的通用接口。结果是`cv::DMatch`实例的向量。
- en: 'With the current Hessian threshold for SURF, we obtained `74` keypoints for
    the first image and `71` for the second. The brute-force approach will then produce
    `74` matches. Using the `cv::drawMatches` class as in the previous recipe produces
    the following image:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用当前SURF的Hessian阈值，我们为第一张图像获得了`74`个关键点，为第二张图像获得了`71`个。 brute-force方法将产生`74`个匹配。使用与之前食谱中的`cv::drawMatches`类产生以下图像：
- en: '![How to do it...](img/B05388_09_02.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/B05388_09_02.jpg)'
- en: 'As it can be seen, several of these matches correctly link a point on the left-hand
    side with its corresponding point on the right-hand side. You might notice some
    errors; some of these are due to the fact that the observed building has a symmetrical
    facade, which makes some of the local matches ambiguous. For SIFT, with the same
    number of keypoints, we obtained the following match result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，这些匹配中的几个正确地将左侧的一个点与其右侧的对应点联系起来。你可能会注意到一些错误；其中一些是由于观察到的建筑具有对称的立面，这使得一些局部匹配变得模糊。对于SIFT，在相同数量的关键点下，我们得到了以下匹配结果：
- en: '![How to do it...](img/B05388_09_03.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/B05388_09_03.jpg)'
- en: How it works...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Good feature descriptors must be invariant to small changes in illumination
    and viewpoint and to the presence of image noise. Therefore, they are often based
    on local intensity differences. This is the case for the SURF descriptors, which
    locally apply the following simple kernels around a keypoint:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的特征描述符必须对光照和视点的微小变化以及图像噪声的存在保持不变性。因此，它们通常基于局部强度差异。这是SURF描述符的情况，它们在关键点周围局部应用以下简单的核：
- en: '![How it works...](img/image_09_007.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_09_007.jpg)'
- en: 'The first kernel simply measures the local intensity difference in the horizontal
    direction (designated as `dx`), and the second measures this difference in the
    vertical direction (designated as `dy`). The size of the neighborhood used to
    extract the descriptor vector is generally defined as `20` times the scale factor
    of the feature (that is, `20σ`). This square region is then split into `4x4` smaller
    square subregions. For each subregion, the kernel responses (`dx` and `dy`) are
    computed at `5x5` regularly-spaced locations (with the kernel size being `2σ`).
    All of these responses are summed up as follows in order to extract four descriptor
    values for each subregion:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个核简单地测量水平方向上的局部强度差异（指定为`dx`），第二个核测量垂直方向上的这种差异（指定为`dy`）。用于提取描述符向量的邻域大小通常定义为特征缩放因子的`20`倍（即`20σ`）。然后，这个正方形区域被分割成`4x4`个更小的正方形子区域。对于每个子区域，在`5x5`均匀分布的位置（核大小为`2σ`）计算核响应（`dx`和`dy`）。所有这些响应如下求和，以从每个子区域提取四个描述符值：
- en: '![How it works...](img/image_09_008-1.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_09_008-1.jpg)'
- en: Since there are `4x4=16` subregions, we have a total of `64` descriptor values.
    Note that in order to give more importance to the neighboring pixels, that is,
    values closer to the keypoint, the kernel responses are weighted by a Gaussian
    centered at the keypoint location (with `σ=3.3`).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有`4x4=16`个子区域，我们总共有`64`个描述符值。注意，为了给邻近像素更多的权重，即更接近关键点的值，核响应通过一个以关键点为中心的高斯加权（`σ=3.3`）。
- en: The `dx` and `dy` responses are also used to estimate the orientation of the
    feature. These values are computed (with a kernel size of `4σ`) within a circular
    neighborhood of radius `6σ` at locations regularly spaced by intervals of `σ`.
    For a given orientation, the responses inside a certain angular interval (`π/3`)
    are summed, and the orientation giving the longest vector is defined as the dominant
    orientation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`dx`和`dy`响应也用于估计特征的方向。这些值在以`σ`为间隔均匀分布的半径为`6σ`的圆形邻域内（以`4σ`的核大小）计算。对于给定的方向，在一定的角度间隔（`π/3`）内的响应被求和，给出最长向量的方向被定义为主导方向。'
- en: SIFT is a richer descriptor that uses an image gradient instead of simple intensity
    differences. It also splits the square neighborhood around each keypoint into
    `4x4` subregions (it is also possible to use `8x8` or `2x2` subregions). Inside
    each of these regions, a histogram of gradient orientations is built. The orientations
    are discretized into `8` bins, and each gradient orientation entry is incremented
    by a value proportional to the gradient magnitude.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT是一种更丰富的描述符，它使用图像梯度而不是简单的强度差异。它还将每个关键点周围的正方形邻域分割成`4x4`子区域（也可以使用`8x8`或`2x2`子区域）。在这些区域内部，构建梯度方向的直方图。方向被离散化为`8`个桶，每个梯度方向条目通过一个与梯度幅度成比例的值增加。
- en: 'This is illustrated by the following figure, inside which each star-shaped
    arrow set represents a local histogram of gradient orientations:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这由以下图示说明，其中每个星形箭头集代表梯度方向的局部直方图：
- en: '![How it works...](img/B05388_09_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/B05388_09_05.jpg)'
- en: These `16` histograms of `8` bins each concatenated together then produce a
    descriptor of `128` dimensions. Note that as for SURF, the gradient values are
    weighted by a Gaussian filter centered at the keypoint location in order to make
    the descriptor less sensitive to sudden changes in gradient orientations at the
    perimeter of the defined neighborhood. The final descriptor is then normalized
    to make the distance measurement more consistent.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个子区域的`16`个`8`桶直方图连接起来，然后产生一个`128`维度的描述符。注意，对于SURF来说，梯度值通过一个以关键点为中心的高斯滤波器加权，以使描述符对定义的邻域边缘梯度方向突然变化的敏感性降低。然后，将最终的描述符归一化，以使距离测量更加一致。
- en: 'With SURF and SIFT features and descriptors, scale-invariant matching can be
    achieved. Here is an example that shows the SURF match result for two images at
    different scales (here, the `50` best matches have been displayed):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SURF和SIFT特征和描述符，可以实现尺度不变匹配。以下是一个示例，展示了两个不同尺度图像的SURF匹配结果（这里，显示了`50`个最佳匹配）：
- en: '![How it works...](img/B05388_09_06.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/B05388_09_06.jpg)'
- en: 'Note that the `cv::Feature2D` class includes a convenient member function that
    detects the interest points and compute their descriptors at the same time, for
    example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到`cv::Feature2D`类包括一个方便的成员函数，它可以同时检测兴趣点并计算它们的描述符，例如：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There's more...
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The match result produced by any matching algorithm always contains a significant
    number of incorrect matches. In order to improve the quality of the match set,
    there exist a number of strategies. Three of them are discussed here.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 任何匹配算法产生的匹配结果总是包含大量不正确的匹配。为了提高匹配集的质量，存在许多策略。这里讨论了其中三种。
- en: Cross-checking matches
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉检查匹配
- en: 'A simple approach to validating the matches obtained is to repeat the same
    procedure a second time, but this time, each keypoint of the second image is compared
    with all the keypoints of the first image. A match is considered valid only if
    we obtain the same pair of keypoints in both directions (that is, each keypoint
    is the best match of the other). The `cv::BFMatcher` function gives the option
    to use this strategy. It is indeed included as a flag; when set to true, it forces
    the function to perform the reciprocal match cross-check:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 验证获得的匹配的一个简单方法是在第二次重复相同的程序，但这次，第二幅图像的每个关键点都与第一幅图像的所有关键点进行比较。只有当我们从两个方向获得相同的配对关键点时，才认为匹配是有效的（也就是说，每个关键点是另一个的最佳匹配）。`cv::BFMatcher`函数提供了使用此策略的选项。这确实是一个标志；当设置为true时，它强制函数执行互匹配交叉检查：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The improved match results are as shown in the following image (in the case
    of SURF):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的匹配结果如下所示（以SURF为例）：
- en: '![Cross-checking matches](img/B05388_09_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![交叉检查匹配](img/B05388_09_07.jpg)'
- en: The ratio test
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比率测试
- en: We have already noted that repetitive elements in scene objects create unreliable
    results because of the ambiguity in matching visually similar structures. What
    happens in such cases is that a keypoint will match well with more than one other
    keypoint. Since the probability of selecting the wrong correspondence is high,
    it might be preferable to reject a match in these ambiguous cases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经指出，场景对象中的重复元素由于匹配视觉上相似结构的歧义性，会导致不可靠的结果。在这种情况下发生的情况是，一个关键点会与多个其他关键点很好地匹配。由于选择错误对应关系的概率很高，在这些模糊的情况下拒绝匹配可能更可取。
- en: 'To use this strategy, we then need to find the best two matching points of
    each keypoint. This can be done by using the `knnMatch` method of the `cv::DescriptorMatcher`
    class. Since we want only two best matches, we specify `k=2`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此策略，我们接下来需要找到每个关键点的最佳两个匹配点。这可以通过使用`cv::DescriptorMatcher`类的`knnMatch`方法来完成。由于我们只想找到两个最佳匹配，我们指定`k=2`：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to reject all the best matches with a matching distance similar
    to that of their second best match. Since `knnMatch` produces a `std::vector`
    class of `std::vector` (this second vector is of size `k`), we do this by looping
    over each keypoint match and perform a ratio test, that is, computing the ratio
    of the second best distance over the best distance (this ratio will be one if
    the two best distances are equal). All matches that have a high ratio are judged
    ambiguous and are therefore rejected. Here is how we can do it:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是拒绝所有与第二最佳匹配距离相似的最好匹配。由于`knnMatch`产生一个`std::vector`类的`std::vector`（这个第二个向量的大小为`k`），我们通过遍历每个关键点匹配并执行比率测试来完成此操作，即计算第二最佳距离与最佳距离的比率（如果两个最佳距离相等，则此比率将为1）。所有具有高比率的匹配都被判断为模糊，因此被拒绝。以下是我们可以这样做的方法：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The initial match set made up of `74` pairs is now reduced to `23` pairs:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 初始由`74`对组成的匹配集现在已减少到`23`对：
- en: '![The ratio test](img/B05388_09_08.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![比率测试](img/B05388_09_08.jpg)'
- en: Distance thresholding
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 距离阈值法
- en: 'An even simpler strategy consists of rejecting matches for which the distance
    between their descriptors is too high. This is done using the `radiusMatch` method
    of the `cv::DescriptorMatcher` class:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更简单的策略是拒绝那些其描述符之间的距离过高的匹配。这是通过使用`cv::DescriptorMatcher`类的`radiusMatch`方法来完成的：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result is again a `std::vector` instance of `std::vector` because the method
    will retain all the matches with a distance smaller than the specified threshold.
    This means that a given keypoint might have more than one matching point in the
    other image. Conversely, other keypoints will not have any matches associated
    with them (the corresponding inner `std::vector` class will then have a size of
    `0`). For our example, the result is a match set of `50` pairs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果再次是一个`std::vector`实例的`std::vector`，因为该方法将保留所有距离小于指定阈值的匹配。这意味着一个给定的关键点可能在另一张图像中有多于一个的匹配点。相反，其他关键点将没有任何与之关联的匹配（相应的内部`std::vector`类的大小将为`0`）。对于我们的示例，结果是`50`对匹配集：
- en: '![Distance thresholding](img/B05388_09_09.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![距离阈值](img/B05388_09_09.jpg)'
- en: Obviously, you can combine all these strategies in order to improve your matching
    results.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你可以将这些策略结合起来以提高你的匹配结果。
- en: See also
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Detecting scale-invariant features* recipe in [Chapter 8](ch08.html "Chapter 8. Detecting
    Interest Points") , *Detecting Interest Points*, presents the associated SURF
    and SIFT feature detectors and provides more references on the subject
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第8章[检测兴趣点](ch08.html "第8章. 检测兴趣点")的*检测尺度不变特征*配方中，介绍了相关的SURF和SIFT特征检测器，并提供了更多关于该主题的参考资料
- en: The *Matching images using random sample consensus* recipe in [Chapter 10](ch10.html
    "Chapter 10. Estimating Projective Relations in Images") , *Estimating Projective
    Relations in Images*, explains how to use the image and the scene geometry in
    order to obtain a match set of even better quality
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第10章[估计图像中的投影关系](ch10.html "第10章. 估计图像中的投影关系")的*使用随机样本一致性匹配图像*配方中，解释了如何使用图像和场景几何来获得质量更好的匹配集
- en: The *Detecting objects and peoples with Support Vector Machines and histograms
    of oriented gradients* recipe in [Chapter 14](ch14.html "Chapter 14. Learning
    from Examples") , *Learning from Examples*, describes the HOG, another descriptor
    similar to SIFT
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第14章[从示例中学习](ch14.html "第14章. 从示例中学习")的*使用支持向量机和方向梯度直方图检测对象和人物*配方中，描述了HOG，这是与SIFT类似的另一个描述符
- en: 'The article *Matching feature points in stereo pairs: A comparative study of
    some matching strategies* by *E. Vincent* and *R. Laganière* in *Machine, Graphics
    and Vision*, pp. 237-260, 2001, describes other simple matching strategies that
    could be used to improve the quality of the match set'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由E. Vincent和R. Laganière在2001年发表的《Machine, Graphics and Vision》杂志上的文章*立体对中特征点的匹配：一些匹配策略的比较研究*，描述了其他可以用来提高匹配集质量的简单匹配策略
- en: Matching keypoints with binary descriptors
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用二进制描述符匹配关键点
- en: In the previous recipe, we learned how to describe a keypoint using rich descriptors
    extracted from the image intensity gradient. These descriptors are floating-point
    vectors that have a dimension of `64`, `128`, or sometimes even longer. This makes
    them costly to manipulate. In order to reduce the memory and computational load
    associated with these descriptors, the idea of using descriptors composed of a
    simple sequence of bits (0s and 1s) has been introduced. The challenge here is
    to make them easy to compute and yet keep them robust to scene and viewpoint changes.
    This recipe describes some of these binary descriptors. In particular, we will
    look at the ORB and BRISK descriptors for which we presented their associated
    feature point detectors in [Chapter 8](ch08.html "Chapter 8. Detecting Interest
    Points") , *Detecting Interest Points*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们学习了如何使用从图像强度梯度中提取的丰富描述符来描述关键点。这些描述符是具有`64`、`128`或有时甚至更长的维度的浮点向量，这使得它们在操作上成本较高。为了减少与这些描述符相关的内存和计算负载，引入了使用由简单的位序列（0和1）组成的描述符的想法。这里的挑战是使它们易于计算，同时保持它们对场景和视点的鲁棒性。本配方描述了一些这些二进制描述符。特别是，我们将查看ORB和BRISK描述符，我们在第8章[检测兴趣点](ch08.html
    "第8章. 检测兴趣点")中介绍了它们相关的特征点检测器。
- en: How to do it...
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Thanks to the common interface of the OpenCV detectors and descriptors, using
    a binary descriptor such as ORB is no different from using descriptors such as
    SURF and SIFT. The complete feature-based image matching sequence is then as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenCV检测器和描述符具有通用接口，使用二进制描述符如ORB与使用SURF和SIFT等描述符并无区别。完整的基于特征的图像匹配序列如下：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The only difference resides in the use of the Hamming norm (the `cv::NORM_HAMMING`
    flag), which measures the distance between two binary descriptors by counting
    the number of bits that are dissimilar. On many processors, this operation is
    efficiently implemented by using an exclusive OR operation, followed by a simple
    bit count. The matching results are the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于使用了汉明范数（`cv::NORM_HAMMING` 标志），它通过计算两个二进制描述符中不同位的数量来衡量它们之间的距离。在许多处理器上，此操作通过使用异或操作后跟简单的位计数来高效实现。匹配结果如下：
- en: '![How to do it...](img/B05388_09_10.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/B05388_09_10.jpg)'
- en: 'Similar results will be obtained with another popular binary feature detector/descriptor:
    BRISK. In this case, the `cv::Feature2D` instance is created by the `cv::BRISK::create`
    call. As we learned in the previous chapter, its first parameter is a threshold
    that controls the number of detected points.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用另一个流行的二进制特征检测器/描述符：BRISK，也会得到相似的结果。在这种情况下，`cv::Feature2D` 实例是通过 `cv::BRISK::create`
    调用创建的。正如我们在上一章所学，它的第一个参数是一个阈值，用于控制检测到的点的数量。
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The ORB algorithm detects oriented feature points at multiple scales. Based
    on this result, the ORB descriptor extracts a representation of each keypoint
    by using simple intensity comparisons. In fact, ORB builds on a previously proposed
    descriptor called BRIEF. This later creates a binary descriptor by simply selecting
    a random pair of points inside a defined neighborhood around the keypoint. The
    intensity values of the two pixel points are then compared, and if the first point
    has a higher intensity, then the value `1` is assigned to the corresponding descriptor
    bit value. Otherwise, the value `0` is assigned. Repeating this test on a number
    of random pairs generates a descriptor that is made up of several bits; typically,
    `128` to `512` bits (pairwise tests) are used.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ORB 算法在多个尺度上检测方向特征点。基于此结果，ORB 描述符通过使用简单的强度比较来提取每个关键点的表示。实际上，ORB 是基于之前提出的描述符
    BRIEF 构建的。随后，它通过简单地选择关键点周围定义的邻域内的随机点对来创建二进制描述符。然后比较两个像素点的强度值，如果第一个点具有更高的强度，则将值
    `1` 分配给相应的描述符位值。否则，分配值 `0`。在多个随机点对上重复此测试会生成一个由多个位组成的描述符；通常，使用 `128` 到 `512` 位（成对测试）。
- en: This is the scheme used by ORB. Then, the decision to be made is which set of
    point pairs should be used to build the descriptor. Indeed, even if the point
    pairs are randomly chosen, once they have been selected, the same set of binary
    tests must be performed to build the descriptor of all the keypoints in order
    to ensure consistency of the results. To make the descriptor more distinctive,
    intuition tells us that some choices must be better than others. Also, the fact
    that the orientation of each keypoint is known introduces some bias in the intensity
    pattern distribution when this one is normalized with respect to this orientation
    (that is, when the point coordinates are given relative to this keypoint orientation).
    From these considerations and the experimental validation, ORB has identified
    a set of `256` point pairs with high variance and minimal pairwise correlation.
    In other words, the selected binary tests are the ones that have an equal chance
    of being `0` or `1` over a variety of keypoints and also those that are as independent
    from each other as possible.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 ORB 所使用的方案。然后，需要做出的决定是使用哪组点对来构建描述符。实际上，即使点对是随机选择的，一旦它们被选中，就必须执行相同的二进制测试集来构建所有关键点的描述符，以确保结果的一致性。为了使描述符更具独特性，直觉告诉我们，某些选择可能比其他选择更好。此外，由于每个关键点的方向是已知的，当将强度模式分布与此方向归一化时（即，当点坐标相对于此关键点方向给出时），这引入了一些偏差。从这些考虑和实验验证中，ORB
    已经确定了一组具有高方差和最小成对相关性的 `256` 个点对。换句话说，所选的二进制测试是那些在各种关键点上具有相等机会为 `0` 或 `1` 的测试，并且尽可能相互独立。
- en: The descriptor of BRISK is very similar. It is also based on pairwise intensity
    comparisons with two differences. First, instead of randomly selecting the points
    from the `31x31` points of the neighborhood, the chosen points are selected from
    a sampling pattern of a set of concentric circles (made up of `60` points) with
    locations that are equally spaced. Second, the intensity at each of these sample
    points is a Gaussian-smoothed value with a `σ` value proportional to the distance
    from the central keypoint. From these points, BRISK selects `512` point pairs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK的描述符非常相似。它也是基于成对强度比较，但有两大不同之处。首先，不是从邻域的`31x31`点中随机选择点，而是从由同心圆（由`60`个点组成）的采样模式中选择点，这些同心圆的点是等间距的。其次，这些样本点的强度是一个高斯平滑值，其`σ`值与中心关键点的距离成比例。从这些点中，BRISK选择`512`个点对。
- en: There's more...
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Several other binary descriptors exist, and interested readers should take a
    look at the scientific literature to learn more on this subject. Since it is also
    available in the OpenCV contrib module, we will describe one additional descriptor
    here.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着几个其他的二进制描述符，感兴趣的读者应该查阅科学文献以了解更多关于这个主题的信息。由于它也包含在OpenCV contrib模块中，我们在这里将描述一个额外的描述符。
- en: FREAK
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FREAK
- en: '**FREAK** stands for **Fast Retina Keypoint**. This is also a binary descriptor,
    but it does not have an associated detector. It can be applied on any set of keypoints
    detected, for example, SIFT, SURF, or ORB.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**FREAK**代表**快速视网膜关键点**。这同样是一种二进制描述符，但它没有关联的检测器。它可以应用于任何一组检测到的关键点，例如SIFT、SURF或ORB。'
- en: Like BRISK, the FREAK descriptor is also based on a sampling pattern defined
    on concentric circles. However, to design their descriptor, the authors used an
    analogy of the human eye. They observed that on the retina, the density of the
    ganglion cells decreases as the distance to the fovea increase. Consequently,
    they built a sampling pattern made of `43` points in which the density of a point
    is much greater near the central point. To obtain its intensity, each point is
    filtered with a Gaussian kernel that has a size that also increases with the distance
    to the center.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与BRISK一样，FREAK描述符也是基于同心圆上的采样模式。然而，为了设计他们的描述符，作者使用了人类眼睛的类比。他们观察到在视网膜上，随着距离黄斑的距离增加，神经节细胞的密度会降低。因此，他们构建了一个由`43`个点组成的采样模式，其中点的密度在中心点附近要大得多。为了获得其强度，每个点都通过一个大小也随中心距离增加的高斯核进行过滤。
- en: In order to identify the pairwise comparisons that should be performed, an experimental
    validation has been performed by following a strategy similar to the one used
    for ORB. By analyzing several thousands of keypoints, the binary tests with the
    highest variance and lowest correlation are retained, resulting in `512` pairs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定应该执行成对比较，已经通过遵循与ORB使用的类似策略进行了实验验证。通过分析数千个关键点，保留了具有最高方差和最低相关性的二进制测试，从而得到`512`对。
- en: FREAK also introduced the idea of performing the descriptor comparisons in cascade.
    That is, the first `128` bits representing coarser information (corresponding
    to the tests performed at the periphery on larger Gaussian kernels) are performed
    first. Only if the compared descriptors pass this initial step will the remaining
    tests be performed.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK还引入了在级联中进行描述符比较的想法。也就是说，首先执行代表较粗信息（对应于在较大高斯核外围进行的测试）的前`128`位。只有当比较的描述符通过这个初始步骤后，才会执行剩余的测试。
- en: 'Using the keypoints detected with ORB, we extract the FREAK descriptors by
    simply creating the `cv::DescriptorExtractor` instance, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ORB检测到的关键点，我们通过简单地创建`cv::DescriptorExtractor`实例来提取FREAK描述符，如下所示：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The match result is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配结果如下：
- en: '![FREAK](img/B05388_09_11.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![FREAK](img/B05388_09_11.jpg)'
- en: 'The following figure illustrates the sampling pattern used for the three descriptors
    presented in this recipe:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了本配方中展示的三个描述符所使用的采样模式：
- en: '![FREAK](img/B05388_09_12.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![FREAK](img/B05388_09_12.jpg)'
- en: The first square is the ORB/BRIEF descriptor in which point pairs are randomly
    selected on a square grid. Each pair of points linked by a line represents a possible
    test to compare the two pixel intensities. Here, we show only eight such pairs;
    the default ORB uses `256` pairs. The middle square corresponds to the BRISK sampling
    pattern. Points are uniformly sampled on the circles shown (for clarity, we only
    identify the points on the first circle here). Finally, the third square shows
    the log-polar sampling grid of FREAK. While BRISK has a uniform distribution of
    points, FREAK has a higher density of points closer to the center. For example,
    in BRISK, you find `20` points on the outer circle, while in the case of FREAK,
    its outer circle includes only six points.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个正方形是 ORB/BRIEF 描述符，其中在正方形网格上随机选择点对。通过线条连接的点对代表比较两个像素强度的可能测试。在这里，我们只展示了八个这样的点对；默认的
    ORB 使用 `256` 对。中间的正方形对应于 BRISK 采样模式。点在显示的圆上均匀采样（为了清晰起见，我们在这里只标识第一个圆上的点）。最后，第三个正方形显示了
    FREAK 的对数极坐标采样网格。虽然 BRISK 具有均匀的点分布，但 FREAK 在中心附近的点密度更高。例如，在 BRISK 中，你会在外圆上找到 `20`
    个点，而在 FREAK 的情况下，其外圆只包括六个点。
- en: See also
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Detecting FAST features at multiple scales* recipe in [Chapter 8](ch08.html
    "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*, presents
    the associated BRISK and ORB feature detectors and provides more references on
    the subject
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [第8章](ch08.html "第8章. 检测兴趣点") 的 *Detecting FAST features at multiple scales*
    菜谱中，介绍了相关的 BRISK 和 ORB 特征检测器，并提供了更多关于该主题的参考资料
- en: 'The *BRIEF: Computing a Local Binary Descriptor Very Fast* article by *E. M.
    Calonder*, *V. Lepetit*, *M. Ozuysal*, *T. Trzcinski*, *C. Strecha*, and *P. Fua*
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2012, describes
    the BRIEF feature descriptor that inspires the presented binary descriptors'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '由 *E. M. Calonder*、*V. Lepetit*、*M. Ozuysal*、*T. Trzcinski*、*C. Strecha* 和
    *P. Fua* 撰写的 *BRIEF: Computing a Local Binary Descriptor Very Fast* 文章，发表在 *IEEE
    Transactions on Pattern Analysis and Machine Intelligence*，2012年，描述了启发所提出二值描述符的
    BRIEF 特征描述符'
- en: 'The *FREAK: Fast Retina Keypoint* article by *A. Alahi*, *R. Ortiz*, and *P.
    Vandergheynst* in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2012, describes the FREAK feature descriptor'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '由 *A. Alahi*、*R. Ortiz* 和 *P. Vandergheynst* 撰写的 *FREAK: Fast Retina Keypoint*
    文章，发表在 *IEEE Conference on Computer Vision and Pattern Recognition*，2012年，描述了
    FREAK 特征描述符'
