- en: Chapter 9. Describing and Matching Interest Points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Matching local templates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing and matching local intensity patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching keypoints with binary descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to detect special points in an image
    with the objective of subsequently performing local image analysis. These keypoints
    are chosen to be distinctive enough so that if a keypoint is detected on the image
    of an object, then the same point is expected to be detected in other images depicting
    the same object. We also described some more sophisticated interest point detectors
    that can assign a representative scale factor and/or an orientation to a keypoint.
    As we will see in this chapter, this additional information can be useful to normalize
    scene representations with respect to viewpoint variations.
  prefs: []
  type: TYPE_NORMAL
- en: In order to perform image analysis based on interest points, we now need to
    build rich representations that uniquely describe each of these keypoints. This
    chapter looks at different approaches that have been proposed to extract descriptors
    from interest points. These descriptors are generally 1D or 2D vectors of binary,
    integer, or floating-point numbers that describe a keypoint and its neighborhood.
    A good descriptor should be distinctive enough to uniquely represent each keypoint
    of an image; it should be robust enough to have the same points represented similarly
    in spite of possible illumination changes or viewpoint variations. Ideally, it
    should also be compact to reduce memory load and improve computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common operations accomplished with keypoints is image matching.
    This task could be performed, for example, to relate two images of the same scene
    or to detect the occurrence of a target object in an image. Here, we will study
    some basic matching strategies, a subject that will be further discussed in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Matching local templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature point matching is the operation by which one can put in correspondence
    points from one image to points from another image (or points from an image set).
    Image points should match when they correspond to the image of the same scene
    element in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: A single pixel is certainly not sufficient to make a decision on the similarity
    of two keypoints. This is why an image patch around each keypoint must be considered
    during the matching process. If two patches correspond to the same scene element,
    then one might expect their pixels to exhibit similar values. A direct pixel-by-pixel
    comparison of pixel patches is the solution presented in this recipe. This is
    probably the simplest approach to feature point matching, but as we will see,
    it is not the most reliable one. Nevertheless, in several situations, it can give
    good results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most often, patches are defined as squares of odd sizes centered at the keypoint
    position. The similarity between two square patches can then be measured by comparing
    the corresponding pixel intensity values inside the patches. A simple **Sum of
    Squared Differences** (**SSD**) is a popular solution. The feature matching strategy
    then works as follows. First, the keypoints are detected in each image. Here,
    we use the FAST detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note how we used the generic `cv::Ptr<cv::FeatureDetector>` pointer type, which
    can refer to any feature detector. One can then test this code on different interest
    point detectors just by changing the detector to be used when calling the `detect`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to define a rectangle of, for example, size `11x11` that
    will be used to define patches around each keypoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The keypoints in one image are compared with all the keypoints in the other
    image. For each keypoint of the first image, the most similar patch in the second
    image is identified. This process is implemented using two nested loops, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note the use of the `cv::matchTemplate` function, which we will describe in
    the next section and that computes the patch similarity score. When a potential
    match is identified, this match is represented through the use of a `cv::DMatch`
    object. This utility class stores the index of the two matching `keypoints` as
    well as their similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The more similar the two image patches are, the higher the probability that
    these patches correspond to the same scene point. This is why it is a good idea
    to sort the resulting match points by their similarity scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can then simply retain the matches that pass a given similarity threshold.
    Here, we chose to keep only the `N` best matching points (we use `N=25` to facilitate
    the visualization of the matching results).
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, there is an OpenCV function that can display the matching results
    by concatenating the two images and joining each corresponding point by a line.
    The function is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the match results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_09_001-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The results obtained are certainly not perfect, but a visual inspection of the
    matched image points shows a number of successful matches. It can also be observed
    that the symmetry of the two towers of the church causes some confusion. Also,
    since we tried to match all the points in the left image with the ones in the
    right image, we obtained cases where a point in the right image was matched with
    multiple points in the left image. This is an asymmetrical matching situation
    that can be corrected by, for example, keeping only the match with the best score
    for each point in the right image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the image patches from each image, here we used a simple criterion,
    that is, a pixel-per-pixel sum of the squared difference specified using the `cv::TM_SQDIFF`
    flag. If we compare the point `(x,y)` of image `I1` with a putative match at `(x'',y'')`
    in image `I2`, then the similarity measure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_09_002-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the sum of the `(i,j)` point provides the offset to cover the square template
    centered at each point. Since the difference between adjacent pixels in similar
    patches should be small, the best-matching patches should be the ones with the
    smallest sum. This is what is done in the main loop of the matching function;
    that is, for each keypoint in one image, we identify the keypoint in the other
    image that gives the lowest sum of the squared difference. We can also reject
    matches for which this sum is over a certain threshold value. In our case, we
    simply sort them from the most similar to the least similar ones.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the matching was done with square patches of size `11x11`. A
    larger neighborhood creates more distinctive patches, but it also makes them more
    sensitive to local scene variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing two image windows from a simple sum of square differences will work
    relatively well as long as the two images show the scene from similar points of
    views and similar illumination. Indeed, a simple lighting change will increase
    or decrease all the pixel intensities of a patch, resulting in a large square
    difference. To make matching more invariant to lighting changes, other formulae
    could be used to measure the similarity between two image windows. OpenCV offers
    a number of these. A very useful formula is the normalized sum of square differences
    (the `cv::TM_SQDIFF_NORMED` flag):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_09_003-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Other similarity measures are based on the concept of correlation, defined
    in the signal processing theory, as follows (with the `cv::TM_CCORR` flag):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_09_004-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This value will be maximal when two patches are similar.
  prefs: []
  type: TYPE_NORMAL
- en: The identified matches are stored in a vector of `cv::DMatch` instances. Essentially,
    the `cv::DMatch` data structure contains a first index that refers to an element
    in the first vector of keypoints and a second index that refers to the matching
    feature in the second vector of keypoints. It also contains a real value that
    represents the distance between the two matched descriptors. This distance value
    is used in the definition of `operator<` when comparing two `cv::DMatch` instances.
  prefs: []
  type: TYPE_NORMAL
- en: When we drew the matches in the previous section, we wanted to limit the number
    of lines to make the results more readable. Therefore, we only displayed the `25`
    matches that had the lowest distance. To do this, we used the `std::nth_element`
    function, which positions the Nth element at the Nth position, with all smaller
    elements placed before this element. Once this is done, the vector is simply purged
    of its remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `cv::matchTemplate` function is at the heart of our feature matching method.
    We used it here in a very specific way, which is to compare two image patches.
    However, this function has been designed to be used in a more generic way.
  prefs: []
  type: TYPE_NORMAL
- en: Template matching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common task in image analysis is to detect the occurrence of a specific pattern
    or object in an image. This can be done by defining a small image of the object,
    a template, and searching for a similar occurrence in a given image. In general,
    the search is limited to a region of interest inside which we think the object
    can be found. The template is then slid over this region, and a similarity measure
    is computed at each pixel location. This is the operation performed by the `cv::matchTemplate`
    function. The input is a template image of a small size and an image over which
    the search is performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is a `cv::Mat` function of floating-point values that correspond
    to the similarity score at each pixel location. If the template is of size `MxN`
    and the image is of size `WxH`, then the resulting matrix will have a size of
    `(W-M+1)x(H-N+1)`. In general, you will be interested in the location of the highest
    similarity; so, the typical template-matching code will look as follows (assuming
    that the target variable is our template):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Remember that this is a costly operation, so you should limit the search area
    and use a template having a size of only a few pixels.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next recipe, *Describing and matching local intensity patterns*, describes
    the `cv::BFMatcher` class, which implements the matching strategy that was used
    in this recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing and matching local intensity patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SURF and SIFT keypoint detection algorithms, discussed in [Chapter 8](ch08.html
    "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*, define
    a location, an orientation, and a scale for each of the detected features. The
    scale factor information is useful for defining the size of a window of analysis
    around each feature point. Thus, the defined neighborhood would include the same
    visual information no matter at what scale of the object to which the feature
    belongs has been pictured. This recipe will show you how to describe an interest
    point's neighborhood using feature descriptors. In image analysis, the visual
    information included in this neighborhood can be used to characterize each feature
    point in order to make each point distinguishable from the others. Feature descriptors
    are usually N-dimensional vectors that describe a feature point in a way that
    is invariant to change in lighting and to small perspective deformations. Generally,
    descriptors can be compared using simple distance metrics, for example, the Euclidean
    distance. Therefore, they constitute a powerful tool that can be used in object
    matching applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `cv::Feature2D` abstract class defines a number of member functions that
    are used to compute the descriptors of a list of keypoints. As most feature-based
    methods include both a detector and a descriptor component, the associated classes
    include both a `detect` function (to detect the interest points) and a `compute`
    function (to compute their descriptors). This is the case of the `cv::SURF` and
    `cv::SIFT` classes. Here is, for example, how you can detect and describe feature
    points in two images using one instance of `cv::SURF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For SIFT, you simply call the `cv::SIFT::create` function. The result of the
    computation of the interest point descriptors is a matrix (that is, a `cv::Mat`
    instance) that will contain as many rows as the number of elements in the keypoint
    vector. Each of these rows is an N-dimensional descriptor vector. In the case
    of the SURF descriptor, it has a default size of `64`, and for SIFT, the default
    dimension is `128`. This vector characterizes the intensity pattern surrounding
    a feature point. The more similar the two feature points, the closer their descriptor
    vectors should be. Note that you do not have to necessarily use the SURF (SIFT)
    descriptor with SURF (SIFT) points; detectors and descriptors can be used in any
    combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'These descriptors will now be used to match our keypoints. Exactly as we did
    in the previous recipe, each feature descriptor vector in the first image is compared
    to all the feature descriptors in the second image. The pair that obtains the
    best score (that is, the pair with the lowest distance between the two descriptor
    vectors) is then kept as the best match for that feature. This process is repeated
    for all the features in the first image. Very conveniently, this process is implemented
    in OpenCV in the `cv::BFMatcher` class, so we do not need to re-implement the
    double loops that we previously built. This class is used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This class is a subclass of the `cv::DescriptorMatcher` class that defines the
    common interface for different matching strategies. The result is a vector of
    the `cv::DMatch` instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the current Hessian threshold for SURF, we obtained `74` keypoints for
    the first image and `71` for the second. The brute-force approach will then produce
    `74` matches. Using the `cv::drawMatches` class as in the previous recipe produces
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As it can be seen, several of these matches correctly link a point on the left-hand
    side with its corresponding point on the right-hand side. You might notice some
    errors; some of these are due to the fact that the observed building has a symmetrical
    facade, which makes some of the local matches ambiguous. For SIFT, with the same
    number of keypoints, we obtained the following match result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Good feature descriptors must be invariant to small changes in illumination
    and viewpoint and to the presence of image noise. Therefore, they are often based
    on local intensity differences. This is the case for the SURF descriptors, which
    locally apply the following simple kernels around a keypoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first kernel simply measures the local intensity difference in the horizontal
    direction (designated as `dx`), and the second measures this difference in the
    vertical direction (designated as `dy`). The size of the neighborhood used to
    extract the descriptor vector is generally defined as `20` times the scale factor
    of the feature (that is, `20σ`). This square region is then split into `4x4` smaller
    square subregions. For each subregion, the kernel responses (`dx` and `dy`) are
    computed at `5x5` regularly-spaced locations (with the kernel size being `2σ`).
    All of these responses are summed up as follows in order to extract four descriptor
    values for each subregion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_09_008-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since there are `4x4=16` subregions, we have a total of `64` descriptor values.
    Note that in order to give more importance to the neighboring pixels, that is,
    values closer to the keypoint, the kernel responses are weighted by a Gaussian
    centered at the keypoint location (with `σ=3.3`).
  prefs: []
  type: TYPE_NORMAL
- en: The `dx` and `dy` responses are also used to estimate the orientation of the
    feature. These values are computed (with a kernel size of `4σ`) within a circular
    neighborhood of radius `6σ` at locations regularly spaced by intervals of `σ`.
    For a given orientation, the responses inside a certain angular interval (`π/3`)
    are summed, and the orientation giving the longest vector is defined as the dominant
    orientation.
  prefs: []
  type: TYPE_NORMAL
- en: SIFT is a richer descriptor that uses an image gradient instead of simple intensity
    differences. It also splits the square neighborhood around each keypoint into
    `4x4` subregions (it is also possible to use `8x8` or `2x2` subregions). Inside
    each of these regions, a histogram of gradient orientations is built. The orientations
    are discretized into `8` bins, and each gradient orientation entry is incremented
    by a value proportional to the gradient magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated by the following figure, inside which each star-shaped
    arrow set represents a local histogram of gradient orientations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These `16` histograms of `8` bins each concatenated together then produce a
    descriptor of `128` dimensions. Note that as for SURF, the gradient values are
    weighted by a Gaussian filter centered at the keypoint location in order to make
    the descriptor less sensitive to sudden changes in gradient orientations at the
    perimeter of the defined neighborhood. The final descriptor is then normalized
    to make the distance measurement more consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'With SURF and SIFT features and descriptors, scale-invariant matching can be
    achieved. Here is an example that shows the SURF match result for two images at
    different scales (here, the `50` best matches have been displayed):'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the `cv::Feature2D` class includes a convenient member function that
    detects the interest points and compute their descriptors at the same time, for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The match result produced by any matching algorithm always contains a significant
    number of incorrect matches. In order to improve the quality of the match set,
    there exist a number of strategies. Three of them are discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-checking matches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A simple approach to validating the matches obtained is to repeat the same
    procedure a second time, but this time, each keypoint of the second image is compared
    with all the keypoints of the first image. A match is considered valid only if
    we obtain the same pair of keypoints in both directions (that is, each keypoint
    is the best match of the other). The `cv::BFMatcher` function gives the option
    to use this strategy. It is indeed included as a flag; when set to true, it forces
    the function to perform the reciprocal match cross-check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The improved match results are as shown in the following image (in the case
    of SURF):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross-checking matches](img/B05388_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ratio test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already noted that repetitive elements in scene objects create unreliable
    results because of the ambiguity in matching visually similar structures. What
    happens in such cases is that a keypoint will match well with more than one other
    keypoint. Since the probability of selecting the wrong correspondence is high,
    it might be preferable to reject a match in these ambiguous cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this strategy, we then need to find the best two matching points of
    each keypoint. This can be done by using the `knnMatch` method of the `cv::DescriptorMatcher`
    class. Since we want only two best matches, we specify `k=2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to reject all the best matches with a matching distance similar
    to that of their second best match. Since `knnMatch` produces a `std::vector`
    class of `std::vector` (this second vector is of size `k`), we do this by looping
    over each keypoint match and perform a ratio test, that is, computing the ratio
    of the second best distance over the best distance (this ratio will be one if
    the two best distances are equal). All matches that have a high ratio are judged
    ambiguous and are therefore rejected. Here is how we can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial match set made up of `74` pairs is now reduced to `23` pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ratio test](img/B05388_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Distance thresholding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An even simpler strategy consists of rejecting matches for which the distance
    between their descriptors is too high. This is done using the `radiusMatch` method
    of the `cv::DescriptorMatcher` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is again a `std::vector` instance of `std::vector` because the method
    will retain all the matches with a distance smaller than the specified threshold.
    This means that a given keypoint might have more than one matching point in the
    other image. Conversely, other keypoints will not have any matches associated
    with them (the corresponding inner `std::vector` class will then have a size of
    `0`). For our example, the result is a match set of `50` pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance thresholding](img/B05388_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Obviously, you can combine all these strategies in order to improve your matching
    results.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Detecting scale-invariant features* recipe in [Chapter 8](ch08.html "Chapter 8. Detecting
    Interest Points") , *Detecting Interest Points*, presents the associated SURF
    and SIFT feature detectors and provides more references on the subject
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Matching images using random sample consensus* recipe in [Chapter 10](ch10.html
    "Chapter 10. Estimating Projective Relations in Images") , *Estimating Projective
    Relations in Images*, explains how to use the image and the scene geometry in
    order to obtain a match set of even better quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Detecting objects and peoples with Support Vector Machines and histograms
    of oriented gradients* recipe in [Chapter 14](ch14.html "Chapter 14. Learning
    from Examples") , *Learning from Examples*, describes the HOG, another descriptor
    similar to SIFT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The article *Matching feature points in stereo pairs: A comparative study of
    some matching strategies* by *E. Vincent* and *R. Laganière* in *Machine, Graphics
    and Vision*, pp. 237-260, 2001, describes other simple matching strategies that
    could be used to improve the quality of the match set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching keypoints with binary descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we learned how to describe a keypoint using rich descriptors
    extracted from the image intensity gradient. These descriptors are floating-point
    vectors that have a dimension of `64`, `128`, or sometimes even longer. This makes
    them costly to manipulate. In order to reduce the memory and computational load
    associated with these descriptors, the idea of using descriptors composed of a
    simple sequence of bits (0s and 1s) has been introduced. The challenge here is
    to make them easy to compute and yet keep them robust to scene and viewpoint changes.
    This recipe describes some of these binary descriptors. In particular, we will
    look at the ORB and BRISK descriptors for which we presented their associated
    feature point detectors in [Chapter 8](ch08.html "Chapter 8. Detecting Interest
    Points") , *Detecting Interest Points*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Thanks to the common interface of the OpenCV detectors and descriptors, using
    a binary descriptor such as ORB is no different from using descriptors such as
    SURF and SIFT. The complete feature-based image matching sequence is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference resides in the use of the Hamming norm (the `cv::NORM_HAMMING`
    flag), which measures the distance between two binary descriptors by counting
    the number of bits that are dissimilar. On many processors, this operation is
    efficiently implemented by using an exclusive OR operation, followed by a simple
    bit count. The matching results are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/B05388_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar results will be obtained with another popular binary feature detector/descriptor:
    BRISK. In this case, the `cv::Feature2D` instance is created by the `cv::BRISK::create`
    call. As we learned in the previous chapter, its first parameter is a threshold
    that controls the number of detected points.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ORB algorithm detects oriented feature points at multiple scales. Based
    on this result, the ORB descriptor extracts a representation of each keypoint
    by using simple intensity comparisons. In fact, ORB builds on a previously proposed
    descriptor called BRIEF. This later creates a binary descriptor by simply selecting
    a random pair of points inside a defined neighborhood around the keypoint. The
    intensity values of the two pixel points are then compared, and if the first point
    has a higher intensity, then the value `1` is assigned to the corresponding descriptor
    bit value. Otherwise, the value `0` is assigned. Repeating this test on a number
    of random pairs generates a descriptor that is made up of several bits; typically,
    `128` to `512` bits (pairwise tests) are used.
  prefs: []
  type: TYPE_NORMAL
- en: This is the scheme used by ORB. Then, the decision to be made is which set of
    point pairs should be used to build the descriptor. Indeed, even if the point
    pairs are randomly chosen, once they have been selected, the same set of binary
    tests must be performed to build the descriptor of all the keypoints in order
    to ensure consistency of the results. To make the descriptor more distinctive,
    intuition tells us that some choices must be better than others. Also, the fact
    that the orientation of each keypoint is known introduces some bias in the intensity
    pattern distribution when this one is normalized with respect to this orientation
    (that is, when the point coordinates are given relative to this keypoint orientation).
    From these considerations and the experimental validation, ORB has identified
    a set of `256` point pairs with high variance and minimal pairwise correlation.
    In other words, the selected binary tests are the ones that have an equal chance
    of being `0` or `1` over a variety of keypoints and also those that are as independent
    from each other as possible.
  prefs: []
  type: TYPE_NORMAL
- en: The descriptor of BRISK is very similar. It is also based on pairwise intensity
    comparisons with two differences. First, instead of randomly selecting the points
    from the `31x31` points of the neighborhood, the chosen points are selected from
    a sampling pattern of a set of concentric circles (made up of `60` points) with
    locations that are equally spaced. Second, the intensity at each of these sample
    points is a Gaussian-smoothed value with a `σ` value proportional to the distance
    from the central keypoint. From these points, BRISK selects `512` point pairs.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several other binary descriptors exist, and interested readers should take a
    look at the scientific literature to learn more on this subject. Since it is also
    available in the OpenCV contrib module, we will describe one additional descriptor
    here.
  prefs: []
  type: TYPE_NORMAL
- en: FREAK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**FREAK** stands for **Fast Retina Keypoint**. This is also a binary descriptor,
    but it does not have an associated detector. It can be applied on any set of keypoints
    detected, for example, SIFT, SURF, or ORB.'
  prefs: []
  type: TYPE_NORMAL
- en: Like BRISK, the FREAK descriptor is also based on a sampling pattern defined
    on concentric circles. However, to design their descriptor, the authors used an
    analogy of the human eye. They observed that on the retina, the density of the
    ganglion cells decreases as the distance to the fovea increase. Consequently,
    they built a sampling pattern made of `43` points in which the density of a point
    is much greater near the central point. To obtain its intensity, each point is
    filtered with a Gaussian kernel that has a size that also increases with the distance
    to the center.
  prefs: []
  type: TYPE_NORMAL
- en: In order to identify the pairwise comparisons that should be performed, an experimental
    validation has been performed by following a strategy similar to the one used
    for ORB. By analyzing several thousands of keypoints, the binary tests with the
    highest variance and lowest correlation are retained, resulting in `512` pairs.
  prefs: []
  type: TYPE_NORMAL
- en: FREAK also introduced the idea of performing the descriptor comparisons in cascade.
    That is, the first `128` bits representing coarser information (corresponding
    to the tests performed at the periphery on larger Gaussian kernels) are performed
    first. Only if the compared descriptors pass this initial step will the remaining
    tests be performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the keypoints detected with ORB, we extract the FREAK descriptors by
    simply creating the `cv::DescriptorExtractor` instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The match result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FREAK](img/B05388_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure illustrates the sampling pattern used for the three descriptors
    presented in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FREAK](img/B05388_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first square is the ORB/BRIEF descriptor in which point pairs are randomly
    selected on a square grid. Each pair of points linked by a line represents a possible
    test to compare the two pixel intensities. Here, we show only eight such pairs;
    the default ORB uses `256` pairs. The middle square corresponds to the BRISK sampling
    pattern. Points are uniformly sampled on the circles shown (for clarity, we only
    identify the points on the first circle here). Finally, the third square shows
    the log-polar sampling grid of FREAK. While BRISK has a uniform distribution of
    points, FREAK has a higher density of points closer to the center. For example,
    in BRISK, you find `20` points on the outer circle, while in the case of FREAK,
    its outer circle includes only six points.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Detecting FAST features at multiple scales* recipe in [Chapter 8](ch08.html
    "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*, presents
    the associated BRISK and ORB feature detectors and provides more references on
    the subject
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *BRIEF: Computing a Local Binary Descriptor Very Fast* article by *E. M.
    Calonder*, *V. Lepetit*, *M. Ozuysal*, *T. Trzcinski*, *C. Strecha*, and *P. Fua*
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2012, describes
    the BRIEF feature descriptor that inspires the presented binary descriptors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *FREAK: Fast Retina Keypoint* article by *A. Alahi*, *R. Ortiz*, and *P.
    Vandergheynst* in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2012, describes the FREAK feature descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
