- en: '*Chapter 8:*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy an ML model as an API using the R plumber package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop serverless APIs using AWS SageMaker, AWS Lambda, and AWS API Gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create infrastructure from scratch using AWS CloudFormation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy an ML model as an API using Docker containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to host, deploy, and manage models on AWS
    and Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we studied model improvements and explored the various
    techniques within hyperparameter tuning to improve model performance and develop
    the best model for a given use case. The next step is to deploy the machine learning
    model into production so that it can be easily consumed by or integrated into
    a large software product.
  prefs: []
  type: TYPE_NORMAL
- en: Most data science professionals assume that the process of developing machine
    learning models ends with hyperparameter tuning when we have the best model in
    place. In reality, the value and impact delivered by a machine learning model
    is limited (mostly futile) if it isn't deployed and (or) integrated with other
    software services/products into a large tech ecosystem. Machine learning and software
    engineering are definitely two separate disciplines. Most data scientists have
    limited proficiency in understanding the software engineering ecosystem and, similarly,
    software engineers have a limited understanding of the machine learning field.
    Thus, in large enterprises where they build a product where a machine learning
    use case evolves into a major feature for a software product, there is a need
    for data scientists and software engineers to collaborate. However, collaboration
    between software engineers and data scientists in most cases is extremely challenging,
    as both find each other's fields highly overwhelming to comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, there has been a lot of effort invested in developing tools
    and resources by large corporations to aid data scientists to easily embrace a
    few software engineering components and vice versa. These tools have enabled easier
    collaboration between the two disciplines, accelerating the process of developing
    large-scale, enterprise-grade machine learning products.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about a few approaches to deploying machine learning
    models as web services that can easily integrate with other services in a large
    software ecosystem. We will also discuss the pros and cons of the different approaches
    and best practices for model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: What is an API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before delving into the specifics of model deployment, we need to study an important
    software engineering topic that simplifies the entire process of model deployment,
    that is, an **Application Program Interface**, commonly referred to as an **API**.
    An API is a set of clearly defined methods for communication between various software
    components. Software development has been made significantly easier with the advent
    of APIs. If a developer, say, wanted to develop an iPhone app that would add some
    filters to an image, they need not write the entire code to capture the image
    from the phone's camera, save it to the library, and then apply their app-specific
    filters to it. Instead, they can use the phone camera API, which provides an easy
    way to communicate with the camera and only focus on writing code that would add
    filters to an image. In a nutshell, an API is the means for heterogeneous software
    components to communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: In a large software product, there would be several components that are responsible
    for a specific task. These components interact with each other through a defined
    language that ensures the smooth communication of data, events, and alerts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some salient features of APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: APIs help in **modularizing** software applications and enable the building
    of better products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APIs are commonly known by software engineers and are **language agnostic**.
    Thus, heterogenous applications developed in a completely different language or
    system can also effectively communicate with each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication between services is also enabled using a common language, that
    is, **JavaScript Object Notation** (short for, **JSON**). However, there are other
    popular languages too, for example, XML.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They also support HTTP, which means APIs are accessible through web browsers
    (such as Google Chrome or Mozilla Firefox) or a tool such as *Postman*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Postman is a free tool that offers easy-to-use services in the entire life cycle
    of an API, such as designing, debugging, testing, documenting, monitoring, and
    publishing. It is available for download on the Windows, Linux, and macOS platforms.
    You can learn more about Postman at https://www.getpostman.com/.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We are particularly interested in the development of RESTful APIs, that is,
    APIs that communicate over HTTP. RESTful APIs are also called **REST APIs** and
    have two main types of methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GET method**: This is used when we want to read data from a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**POST method**: This is used when we want to send data to a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few other methods are **head**, **put**, and **delete**.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying machine learning models as (web service) REST APIs eases the process
    of integrating a service with other services. Software engineers are fond of using
    REST APIs and since the service is language-agnostic, we have a tremendous advantage
    in developing the model in the language of our choice. The software engineer could
    use Python, Java, Ruby, or one of many other languages for the development of
    other services, whereas we can develop the model in R, Python, Julia, and so on,
    and yet effectively and effortlessly integrate services into a large software
    product.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a fair understanding of APIs, let's understand how we can deploy
    an ML model in R as an API.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to plumber
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Plumber** is an R package that helps in translating R functions into an HTTP
    API that can be invoked from other machines within a network, enabling communication
    between systems. By using R plumber, we will be able to achieve the advantages
    discussed, such as developing modularized, language agnostic, common communication
    language (JSON) based HTTP rest APIs that provide a defined path of communication
    between systems. Using plumber is extremely straightforward. With a few lines
    of code, we can convert our existing R functions into a web service that can be
    served as an endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will extend the same model and use case we built in *Chapter
    7*, *Model Improvements*, to classify whether a patient is diabetic using the
    `PimaIndiasDiabetes` dataset in the `mlbench` library. Later, we will extend the
    same use case to deploy the model as a web service using a Docker container and
    serverless applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 98: Developing an ML Model and Deploying It as a Web Service Using
    Plumber'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will develop a logistic regression model using three independent
    variables and deploy it as a REST API using Plumber. We will create a simple binary
    classification model and use the `plumber` package's services to wrap the model
    as an API by defining the HTTP get and post methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an R script named `model.R` using RStudio or Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the required libraries and build a logistic regression model. Now, define
    the `get` methods that accept the input parameters and return the prediction as
    an outcome:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This data has been taken from the UCI Repository Of Machine Learning Databases
    from the following URLs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ftp://ftp.ics.uci.edu/pub/machine-learning-databases
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: http://www.ics.uci.edu/~mlearn/MLRepository.html
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It was converted to the R format by Friedrich Leisch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train a logistic regression model with the `df` DataFrame object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the API endpoint as a function with the additional `#'' @get /` construct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within the function, convert the parameters into `numeric` values using the
    `as.numeric` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create a DataFrame with the same column names as we did in the previous
    step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the newly created `sample` DataFrame to make predictions on the trained
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Package the result as a `list` for the function''s return call and complete/close
    the function definition with `}` parentheses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous exercise demonstrates regular R model code with one additional
    construct. The model we developed is rather a simple one with only three independent
    variables and one dependent variable (unlike the eight independent variables we
    saw in *Chapter 7*, *Model Improvements)*. We also created a function that will
    accept three input parameters, each representing one independent variable for
    the model. This function will be used as the endpoint when we deploy the model
    as a REST API. We added one additional construct just before the function (refer
    to the fourth step of the previous exercise):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This construct defines that the `predict_data` endpoint will be serving the
    GET requests. The function we defined for this endpoint accepts three parameters
    with no default values. Let's now install the `plumber` package and make a call
    using the web server.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final complete `model.R` file should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can also refer to the GitHub URL: https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson08/Docker/model.R.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create another R script called `main.R`. Now, install the `plumber` package
    and load it into memory, and then deploy the R function using the `plumb` function,
    as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to pass the R file to the `plumb` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After executing the previous code, the plumber library creates a web server
    within your `localhost` and responds to the requests. To test whether the endpoint
    functions we wrote are functioning in the expected way, we will invoke the endpoint
    from the browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The parameters are passed to the endpoint after the `?` symbol, and multiple
    parameters are separated by the `&` symbol.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since we deployed the endpoint to localhost, that is, `127.0.0.1` on the `8080`
    port, we will have the following API definition for the endpoint. Invoke the API
    using the browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the previous API definition, which will return the following prediction
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To test the APIs, we can use a better tool, such as Postman, instead of the
    browser. Postman (https://www.getpostman.com/) is currently one of the most popular
    tools used in API testing. It is available for free on the Windows, Mac, and Linux
    platforms. Using Postman is relatively simple and doesn't include any new learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: In case you would like to explore additional details about Postman, you can
    explore the learning resources provided at https://learning.getpostman.com/docs/postman/launching_postman/installation_and_updates/.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After you download and install Postman for your system, you can test the API
    endpoint by pasting it in the input window, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1: Plumber UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_08_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.1: Plumber UI'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can execute the API by clicking on the **Send** button, and the results are
    displayed in the highlighted area of the previous screenshot. Observing the output
    from Postman, we can see that our machine learning model has been deployed successfully
    as an API endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in Deploying Models with plumber
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can see that deploying the model as an API using plumber is simple and can
    be done easily with a few additional lines of code. The `plumber` package provides
    additional features that we have not explored in this exercise. Some important
    topics that might be interesting to explore are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filters**: Filters can be used to define a *pipeline* with a flow of incoming
    requests. This functionality helps to further modularize the deployment logic
    and workflow. You can read more at https://www.rplumber.io/docs/routing-and-input.html#filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling**: With larger applications, the code base and the complexity
    of applications increase exponentially. It becomes increasingly important to add
    exception handlers and ease the process of debugging applications. You can read
    more about it at https://www.rplumber.io/docs/rendering-and-output.html#error-handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods have a major drawback. While it is relatively easy to set up an
    endpoint in a single host, there might be issues faced when deploying the same
    solution on a different system. These issues might arise due to the difference
    in the system architecture, software version, operating system, and so on. To
    mitigate the conflicts that you may face when deploying the endpoint, one technique
    is to make the process environment-agnostic, that is, a solution developed in
    one host system can be deployed without any issues in any other host with a different
    architecture, platform, or operating system. This can be achieved using **Docker
    containers** along with plumber for deployment instead of directly using plumber.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief History of the Pre-Docker Era
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving deep into the Docker tool, let's understand some background and
    history.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge of deploying an application in an environment-agnostic framework
    was achieved earlier using virtualization, that is, the entire application, dependencies,
    libraries, necessary frameworks, and the operating system itself was virtualized
    and packaged as a solution that could be deployed on a host. Multiple virtual
    environments could run on an infrastructure (called a **hypervisor**), and applications
    became environment-agnostic. However, this approach has a major trade-off. Packaging
    the entire operating system into the **virtual machine** (**VM**) of an application
    made the package heavy and often resulted in wasting memory and computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more intuitive approach to this problem was to exclude the operating system
    from the package and only include the application-related libraries and dependencies.
    Additionally, enable a mechanism such that the package becomes infrastructure-agnostic,
    keeping the app lightweight. This is when Docker was introduced. The following
    visual sheds light on the high-level view of how Docker was improvised to solve
    problems previously solved by virtual machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: The architectural difference between a virtual machine and a
    Docker container'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_08_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.2: The architectural difference between a virtual machine and a Docker
    container'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's now understand Docker containers in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker is a simple tool that eases the process of developing, deploying, and
    executing software applications using containers. A **container** is analogous
    to a shipping industry container, and allows a developer to package an entire
    application with its dependencies, and ship it all out as one package. Once built
    on a system, the package will work on any other system as well, regardless of
    the differences in the infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With Docker, we can create a single document (called a **Dockerfile**) that
    defines a simplified step for setting up the required environment for the application.
    The Dockerfile is then used to build a **Docker image**. A container is an instance
    of a Docker image. For the same application, we might sometimes have multiple
    containers that will help in load balancing for high-traffic applications.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the ML Model Using Docker and plumber
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will leverage the same plumber application with Docker for environment- and
    infrastructure-agnostic deployment. First, we will need to download and install
    Docker on our system. It's free and easy. Create an account and download Docker
    for your system from https://www.docker.com/get-started. Once installed, you can
    verify whether Docker is running by executing the `docker` command in the terminal
    or Windows PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 99: Create a Docker Container for the R plumber Application'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will extend the previously created plumber application
    as a Docker container. In order to develop environment-agnostic models that can
    be deployed to any production system without any issues, we can deploy a plumber
    app using Docker containers. These containers can then be deployed on any other
    machine that supports Docker Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a Dockerfile, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, install the libraries for the `plumber` package using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, install the `plumber` package, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the following command to copy all files from the current directory
    into the current folder for the container. This will copy our `model.R` and `plumber.R`
    file into a container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, define a port to be exposed where the container will be deployed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the first script to run when the container starts after the build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have three files in our project folder, as shown in the following diagram.
    Note that the Dockerfile is a simple text file with no extensions. On running
    a `build` command from the terminal within this folder, **Docker Engine** searches
    for the Dockerfile and prepares the environment based on the instructions provided
    within the document:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.3: Project files'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_08_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.3: Project files'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The final Dockerfile with all the commands, as defined in the previous steps,
    will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now build the Docker image from the Dockerfile using the `docker build`
    command, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `.` after the `r_ml_demo` indicates that the Dockerfile is present in the
    current folder. The build process takes a while as it creates the container image
    with all the necessary dependencies. Once the image is built, we can run the Docker
    image using the following command by mapping the machine's `8080` port to the
    container's published `8080` port.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the Docker image using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can refer to the complete code from GitHub at https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/tree/master/Lesson08/Docker.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The container can be tested again in the same way we tested our plumber application
    using Postman, and we will get exactly the same result. We can, therefore, deploy
    an R application using plumber and Docker on any other system, regardless of the
    operating system and missing libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Docker cannot be installed on Windows Home edition. Only Windows Pro editions
    support Hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages of Using plumber to Deploy R Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While this process comes with a few advantages of easy and fast implementation,
    it also comes with some disadvantages. The major disadvantage of plumber is scaling
    the application endpoint for large complex use cases. The scale here refers to
    the number of times the endpoint is invoked as well as the amount of data that
    can be invoked through the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major drawbacks of using plumber is that it doesn't directly support
    passing JSON objects or arrays or lists to the endpoint. This becomes a bottleneck
    when we are dealing with bigger models with more than 20 independent variables.
    The previous use case, in *Exercise 2*, *Create a Docker Container for the R Plumber
    Application*, was a fairly small and lightweight model with three independent
    variables. Therefore, the API definition was short and sweet. However, as the
    number of parameters increases (which is definitely bound to happen for real production
    models), plumber model endpoint definitions will not be the best ones to use.
    Also, the plumber framework is not ideal for large complex software use cases.
    The small community around the framework, lack of proper documentation, and limited
    support makes it a risky choice for deploying a model into a large scale machine
    learning product or service.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at leveraging cloud services for deploying R ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Web Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Web Services** (**AWS**) is the leading provider of cloud services.
    With the advent of the cloud, the tech industry has seen a dramatic shift in the
    process of building large-scale enterprise applications leveraging cloud services
    rather than self-hosted services. Other prominent players in the cloud services
    market are Microsoft, Google, and IBM. While all leading cloud providers have
    an exhaustive suite of services to build all kinds of software applications, we
    will focus only on AWS for the scope of this chapter. You are highly encouraged
    to explore alternative services for similar use cases from other cloud providers
    (not restricted to Google or Microsoft).'
  prefs: []
  type: TYPE_NORMAL
- en: AWS has a ton of services readily available that can be used to make large,
    complex enterprise applications of any scale with no upfront commitments. You
    pay as you go, and there are also a large number of services that you can explore
    and test for free for one year (with certain limits). For the scope of the set
    of experiments we will perform in the upcoming exercises, the free tier should
    suffice. In case you do not already have an AWS account, create one at https://aws.amazon.com/free/activate-your-free-tier-account/.
  prefs: []
  type: TYPE_NORMAL
- en: You will need a valid credit/debit card for the signup process. We will only
    leverage the free tier service from AWS for the exercises.
  prefs: []
  type: TYPE_NORMAL
- en: There are several approaches that we could take to deploy a machine learning
    model using cloud services. Some may be well-suited for small applications, some
    for medium-sized and moderately complex applications, and others for large and
    very complex applications. We will explore the approach that has the least amount
    of software engineering yet provides effective flexibility and can easily scale
    into large-scale applications while easily integrating into complex applications.
  prefs: []
  type: TYPE_NORMAL
- en: The use of an API and delivering the machine learning model as an API makes
    the entire process of integrating the service into other applications fairly straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing AWS SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon SageMaker** is a cloud service that provides developers and data scientists
    with a platform to build, train, and deploy machine learning models quickly. It
    is an extremely effective service in aiding data scientists with limited development
    knowledge to deploy highly scalable ML models while abstracting the entire complexities
    of the infrastructure and underlying services.'
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker automates the entire process of deploying a model as an API with the
    defined resources and creates an *endpoint* that can be used for inferencing within
    the other AWS services. To enable the endpoint to be inferenced by other external
    applications, we would need to orchestrate the flow of requests using two other
    AWS services, called **AWS API Gateway** and **AWS Lambda**. We will explore these
    new services later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's begin deploying our model using AWS SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an ML Model Endpoint Using SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker, by default, doesn't provide a direct way to create R models, but
    there is an easy alternative provided by Amazon. AWS provides the functionality
    of **Infrastructure as Code** with **AWS CloudFormation**, that is, a service
    where we can codify the entire flow of provisioning and the setup of infrastructure
    resources for a project. With CloudFormation templates, we can automate the process
    of provisioning a tech stack as per our needs and reuse it any number of times.
    Amazon has provided a lucid and elaborate guide to get started with R notebooks
    on SageMaker using the CloudFormation template.
  prefs: []
  type: TYPE_NORMAL
- en: To find out more, you can refer to the guide at https://aws.amazon.com/blogs/machine-learning/using-r-with-amazon-sagemaker/
    for a detailed understanding of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 100: Deploy the ML Model as a SageMaker Endpoint'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will deploy an ML model as a SageMaker endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your AWS account and launch the CloudFormation script to create an
    R notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, access the CloudFormation template from https://amzn.to/2ZzUM28 to create
    the R Notebook on SageMaker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will create and launch the stack in AWS using the previous CloudFormation
    template.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on the template; it directly navigates you to the CloudFormation service,
    as shown in the following screenshot. The cloud formation template (which is a
    YAML file) is hosted in a public S3 bucket and has already been added to the input
    box under **Specify an Amazon S3 template URL**. Click on the **Next** button
    and navigate to the **Details** page:![Figure 8.4: CloudFormation—Create Stack
    page'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.4: CloudFormation—Create Stack page'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the next page, specify the SSH key pair that you will use to log in to the
    EC2 instance. This is a secure way to access the cloud instance or virtual machine
    that we provision in the cloud. If you do not have a key already created for your
    account, you can create one using the steps provided on the Amazon website: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the key pair is created or if you already have a pair, it will appear
    in the dropdown in the highlighted box, as shown in the following screenshot.
    Select your key pair and click on the **Next** button:![Figure 8.5: Creating a
    key pair'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.5: Creating a key pair'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: On the next **Option** page, we can directly click on the **Next** button and
    navigate to the next page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, on the review page, select the **I acknowledge that AWS CloudFormation
    might create IAM resources with custom names** checkbox and click on the **Next**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The process will create the stack (it might take a while to show on the screen—refresh
    the screen after 1-2 minutes). Once created, you will see the stack ready under
    CloudFormation, as shown in the following screenshot. The output tabs will have
    the SSH command to be used to log in; copy the value in the highlighted section
    and run the command in a terminal or Command Prompt:![Figure 8.6: Stacks—SSH key'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.6: Stacks—SSH key'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On running the SSH command from the terminal, it forwards port `8787` to your
    computer while connecting to the new instance. Once connected, open a browser
    window and type `https://127.0.0.1:8787` in the address bar to open the RStudio
    login page. The default username and password is set to `rstudio`. Enter the username
    and password and click on the **Sign In** button:![Figure 8.7: RStudio—Sign In
    page'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.7: RStudio—Sign In page'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Log in to RStudio and create a new R script with any name, say `Sagemaker.R`,
    load the necessary libraries, and get the SageMaker session ready:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the SageMaker session and define the default bucket as well as the role
    to be used for the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the `mlbench` package and load the data for our use case. In the following
    command, we''ll first set the `seed` for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To explore SageMaker''s automated hyperparameter tuning, we will be developing
    an XGBoost model instead of a logistic regression model on the same use case as
    the previous one. Therefore, we need the target variable and all the independent
    variables in a numeric type. Also, SageMaker expects the data in a form where
    the first column is the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Place the target variable as the first column in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create 70% train and 30% test datasets using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the train and test data created from the `df` DataFrame into memory and
    upload the CSV files into an S3 bucket on AWS. The session has been defined with
    a default bucket, therefore, we can directly use the `upload_data` command with
    the path and the required constructs to upload the dataset on our default S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the train and test dataset (validation data) for the SageMaker session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: SageMaker provides AWS optimized pre-configured containers that can be leveraged
    directly for model training. We would need to choose a container base from the
    same region that our resources are hosted in. In this case, it is **us-east-1**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the container for the estimator and the output folder in S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the container for the estimator using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the output folder as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the SageMaker estimator, the job, and the input data. Here, we would
    need to provide the type of instance that we would like to use for the model training
    process, and we will choose `ml.m5.large`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the hyperparameters of interest for the model and define the training and
    validation datasets for the model as a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can read more about the different types of instances that can be used for
    the purpose of model training at https://aws.amazon.com/sagemaker/pricing/instance-types/.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train/fit the model we defined. The model training process will take a while
    (~10-12 minutes). In the background, SageMaker will provision an instance that
    was defined by us in the model definition, trigger the necessary background operations
    to orchestrate the training process, and finally, train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Deploy the train model as an endpoint. We will have to, again, provide the
    type of instance we would want SageMaker to provision for the model inference.
    Since this is just a sample model, we can choose the instance with the lowest
    configuration. This process will also take some time, as SageMaker will orchestrate
    a series of services in the background to deploy the model as an endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the model endpoint is created, we can test it by invoking it with the
    right form of test data. Since we are saving the test data as CSV files, we will
    pass comma-separated text to be serialized into JSON format. Therefore, we specify
    `text/csv` and `csv_serializer` for the endpoint. Let's prepare a sample test
    data feed for a quick test.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, make a copy of the test dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, delete the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a single test sample using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, delete the column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a prediction using the model endpoint on the sample data that we created
    in the previous step. Invoke the SageMaker endpoint and pass the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print the result using the `print` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This output helps us to understand that the model has been correctly deployed
    and is functioning as expected. We can check whether the endpoint is created by
    navigating to the SageMaker service in the AWS account and opening the *Endpoint*
    section from the right-hand-side sidebar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: Endpoint page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_08_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.8: Endpoint page'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This endpoint can be used directly by other services within AWS to invoke and
    get predictions. The only requirement is that the input data should be provided
    as expected. To enable our model endpoint to be accessible by other services outside
    AWS, we would need to orchestrate the API request using AWS API Gateway and AWS
    Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can access the complete code file on GitHub at https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson08/RStudio_SageMaker.R.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's study these services a bit before delving into the solution.
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon Lambda?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**AWS Lambda** is an event-driven, serverless computing platform provided by
    Amazon as a part of Amazon Web Services. It is a computing service that runs code
    in response to events and automatically manages the computing resources required
    by that code. The service enables us to develop serverless applications. The term
    serverless indicates that we do not actually need to manage and provision the
    infrastructure resources; instead, they are managed by the cloud service provider,
    and we only pay for what we use, say, pay per event or execution. AWS Lambda can
    be configured to execute a defined function in response to a specific event, say,
    when someone uploads a new file to a defined S3 bucket or invokes another function
    or another service in AWS.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon API Gateway?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon API Gateway** is a fully managed service that makes it easy for developers
    to create, publish, maintain, monitor, and secure APIs at any scale. With this
    service, we can develop REST as well as WebSocket APIs that act as a *front door*
    for applications to access data, business logic, or functionality from other backend
    services, while protecting the backend services within the private network. These
    backend services could be any applications that are running on AWS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall flow of our service can be represented as in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9: Workflow of API Gateway, AWS Lambda, and SageMaker'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_08_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.9: Workflow of API Gateway, AWS Lambda, and SageMaker'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The client (say, a web browser), calls an Amazon API Gateway's defined action
    and passes the appropriate parameter values. API Gateway passes the request to
    AWS Lambda, while it also seals the backend so that AWS Lambda stays and executes
    in a protected private network.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we will use Lambda to help us tailor the data received from API
    Gateway into an appropriate form that can be consumed by the SageMaker endpoint.
    This is necessary because there is a difference between the structure of data
    passed through a REST API and that of what SageMaker expects. The SageMaker model
    performs the prediction and returns the predicted value to AWS Lambda. The Lambda
    function parses the returned value and sends it back to API Gateway, after which
    API Gateway responds to the client with the result. This entire flow is orchestrated
    without us actually provisioning any infrastructure; it is entirely managed by
    AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This workflow is explained in further detail in a blog post by Amazon. You can
    read more at https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/.
  prefs: []
  type: TYPE_NORMAL
- en: There is one additional challenge that we will need to tackle. As of today,
    AWS Lambda doesn't support R for defining functions. It supports Python, Java,
    Go, and a few others, but R is not on the list as of now. The lambda function
    will be in charge of transforming the data passed by the API into the required
    form. We will leverage Python scripts for this task. In the future, we can expect
    R to be supported by AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the required context about the necessary services, let's deploy
    our model on a serverless application.
  prefs: []
  type: TYPE_NORMAL
- en: Building Serverless ML Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Serverless computing is the new paradigm within cloud computing. It allows us
    to build and run applications and services without thinking about servers. In
    reality, the application we build still runs on a cloud server, but the entire
    process for server management is done by the cloud service provider, such as AWS.
    By leveraging the serverless platform, we can build and deploy robust, large-scale,
    complex applications by only focusing on our application code instead of worrying
    about provisioning, configuring, and managing servers.
  prefs: []
  type: TYPE_NORMAL
- en: We have explored some important components of the AWS serverless platform such
    as AWS Lambda in this chapter, and we can now leverage these solutions to build
    a machine learning application where we can only focus on the core ML code and
    forget about provisioning infrastructure and scaling applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 101: Building a Serverless Application Using API Gateway, AWS Lambda,
    and SageMaker'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will build a machine learning model using AWS SageMaker
    and deploy it as an endpoint (using automated SageMaker functions). To enable
    the model endpoint to be invoked by any service (within or outside AWS) we define
    an AWS Lambda function and expose the endpoint to public networks through API
    Gateway.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this exercise is to create a serverless application that will use
    the SageMaker model endpoint we created in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an IAM role in AWS that will allow Lambda to execute endpoints from
    the SageMaker service. From the AWS dashboard, search for `IAM`, and click on
    the **Roles** option on the IAM page:![Figure 8.10: Creating IAM roles'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.10: Creating IAM roles'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the **Roles** page loads, click on the **Create role** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Create role** page, select **AWS Service** as the type of trusted
    entity and **Lambda** as the service that will use this role. Click on the **Next:
    Permissions** button to proceed:![Figure 8.11: Selecting the AWS service option'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.11: Selecting the AWS service option'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the `sagemaker` keyword, select the `AmazonSageMakerFullAccess` policy,
    and click on the **Next: Tags** option:![Figure 8.12: The Create role screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.12: The Create role screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: On the **Tags** page, you can directly click on **Next** and proceed to the
    final page to name the **Role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, on the final page, add a suitable name (say, `lambda_sagemaker_execution`)
    and click on **Create role**. The role will be created for us to use:![Figure
    8.13: Review page—creating role'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.13: Review page—creating role'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the AWS console, search for AWS Lambda and click on the **Create function**
    button. The create function page will have some inputs that need to be defined.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `lambda_sagemaker_connection`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, select `lambda_sagemaker_execution`. Click on the **Create function**
    button:![Figure 8.14: Creating a function form'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.14: Creating a function form'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Define a Python function that will accept the input request from the API, parse
    the payload, and invoke the SageMaker endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The endpoint name will be available in the SageMaker page under the endpoint
    section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will additionally define the environment variable for the function that
    will store the SageMaker endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can refer to the complete code on GitHub at https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson08/Amazon_Lambda_Function.py.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on `API Gateway` in the AWS console, and create a new API function by
    selecting the following highlighted options in the screenshot. Give the API a
    suitable name, say, `api_lambda_connect`:![Figure 8.15: Amazon API Gateway'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.15: Amazon API Gateway'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the **Actions** dropdown, select **Create Resource**, add a suitable resource
    name, and then click on the **Create Resource** button:![Figure 8.16: Creating
    a new child resource'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.16: Creating a new child resource'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, from the **Actions** dropdown, select **Create Method** and select the
    method type as **POST**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the **Integration Type** as **Lambda Function** and mention the **Lambda
    Function** name in the input label, as shown in the following screenshot. Next,
    click on the **Save** button:![Figure 8.17: Creating a Lambda function'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.17: Creating a Lambda function'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, select `test`) and click on the **Deploy** option. The API will now be
    deployed and will be ready to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the API is deployed, we can find the URL of the API to be invoked by navigating
    to the `https://****amazonaws.com/[deployment-stage]/[resource-name]/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Call the API from Postman. Open Postman, select a **POST** call, and paste
    the URL we copied from the API Gateway stage. Then, click on **Body** and add
    raw data, that is, the JSON formatted test data, in the body, as shown in the
    following screenshot:![Figure 8.19: Calling the API via Postman'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12624_08_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.19: Calling the API via Postman'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Click on the **Send** button to invoke the API with the provided raw data. The
    result is showcased in the lower window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We received a prediction of "no," which indicates that the model has been successfully
    deployed as a serverless application. We can now invoke the API from anywhere
    in the world in a browser or Postman and get predictions for the model. This API
    call be integrated with other services in a larger product and can be scaled as
    and when there is more demand.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting All Cloud Resources to Stop Billing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the resources we have provisioned will need to be deleted/terminated to
    ensure that they are no longer billed. The following steps will need to be performed
    to ensure that all resources created in the book of the exercise are deleted:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to CloudFormation and click on **Delete** stack (the one we provisioned
    for RStudio).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to SageMaker, open Endpoints from the right-hand-side sidebar, check
    the endpoint we created for the exercise, and delete it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to AWS Lambda and delete the Lambda function we created for the exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to AWS API Gateway and delete the API we created for the exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Further notes on AWS SageMaker**'
  prefs: []
  type: TYPE_NORMAL
- en: We leveraged the existing containers of the algorithm provided by Amazon to
    train the model. This step was followed to keep things simple. We can bring our
    own custom trained algorithms to SageMaker and leverage the platform to deploy
    the model as a service. SageMaker takes care of the entire process of orchestrating
    the background resources to provision instances, configure model artefacts, and
    build the endpoint. We would, however, need to provide the data and model artifacts
    in a specific format for SageMaker to deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Additional details on the process of building custom models can be found at
    https://docs.aws.amazon.com/sagemaker/latest/dg/build-model.html.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: Deploy an R Model Using plumber'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will develop a regression model in R and deploy it as an
    API endpoint using plumber. We will be using another use case for supervised learning
    in R, and we will build a regression model using a different dataset, that is,
    **Boston Housing**. The dataset is available within R's Mlbench library, which
    we already installed and provides information on the median price of a house within
    Boston when given a number of house attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will write two R scripts: `model.r` to house the regression model as well
    as the prediction function and `plumber.R` to house the necessary functions to
    deploy the model as an API endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Additional details about the dataset can be explored at https://www.rdocumentation.org/packages/mlbench/versions/2.1-1/topics/BostonHousing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `model.r` script, which will load the required libraries, data, and
    fit a regression model and the necessary functions to make predictions on unseen
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `mlbench` library, which has the data for this activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `BostonHousing` data into a DataFrame, `df`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a train dataset using the first `400` rows of `df` and test with the
    remaining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a logistic regression model using the `lm` function with the dependent variable
    as `medv` (median value) and `10` independent variables, such as, `crim`, `zn`,
    `indus`, `chas`, `nox`, `rm`, `age`, `dis`, `rad`, and `tax`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a model endpoint as `predict_data`; this will be used as the API endpoint
    for plumber.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the function, convert the parameters to `numeric` and **factor** (since
    the API call will pass them as a string only).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap the 10 independent features for the model as a DataFrame named `sample`,
    with the same name for the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the `sample` DataFrame to the predict function with the model (created
    in step 4), and return the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `plumber` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a plumber object using the `plumb` function and pass the `model.r` file
    (created in part 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the plumber object by passing the hostname as `localhost` or `127.0.0.1`
    and a port, say `8080`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the deployed model using the browser or Postman and invoke the API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'API invocation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: http://127.0.0.1:8080/predict_data?crim=0.01&zn=18&indus=2.3&chas=0&nox=0.5&rm=6&age=65&dis=4&rad=1&tax=242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 463.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we studied how to deploy our machine learning models with traditional
    server-based deployment strategies using R's plumber, and enhanced approaches
    using plumber for R with Docker containers. We then studied how serverless applications
    can be built using cloud services and how we can easily scale applications as
    needed with minimal code.
  prefs: []
  type: TYPE_NORMAL
- en: We explored various web services, such as Amazon Lambda, Amazon SageMaker, and
    Amazon API Gateway, and studied how services can be orchestrated to deploy our
    machine learning model as a serverless application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will work on a capstone project by taking up one of
    the latest research papers based on a real-world problem and reproducing the result.
  prefs: []
  type: TYPE_NORMAL
