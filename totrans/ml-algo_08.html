<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Decision Trees and Ensemble Learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to discuss binary decision trees and ensemble methods. Even if they're probably not the most common methods for classification, they offer a good level of simplicity and can be adopted in many tasks that don't require a high level of complexity. They're also quite useful when it's necessary to show how a decision process works because they are based on a structure that can be shown <span>easily</span> <span>i</span><span>n presentations and described step by step.</span></p>
<p>Ensemble methods are a powerful alternative to complex algorithms because they try to exploit the statistical concept of majority vote. Many weak learners can be trained to capture different elements and make their own predictions, which are not globally optimal, but using a sufficient number of elements, it's statistically probable that a majority will evaluate correctly. In particular, we're going to discuss random forests of decision trees and some boosting methods that are slightly different algorithms that can optimize the learning process by focusing on misclassified samples or by continuously minimizing a target loss function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Binary decision trees</h1>
                </header>
            
            <article>
                
<p>A binary decision tree is a structure based on a sequential decision process. Starting from the root, a feature is evaluated and one of the two branches is selected. This procedure is repeated until a final leaf is reached, which normally represents the classification target we’re looking for. Considering other algorithms, decision trees seem to be simpler in their dynamics; however, if the dataset is splittable while keeping an internal balance, the overall process is intuitive and rather fast in its predictions. Moreover, decision trees can <span>work</span><span> </span><span>efficiently with unnormalized datasets because their internal structure is not influenced by the values assumed by each feature. In the following figure, there are plots of an unnormalized bidimensional dataset and the cross-validation scores obtained using a logistic regression and a decision tree:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/2a649bb6-16ae-498f-8793-ec09a532f5a7.png"/></div>
<p>The decision tree <span>always</span><span> </span><span>achieves a score close to 1.0, while the logistic regression has an average slightly greater than 0.6. However, without proper limitations, a decision tree could potentially grow until a single sample (or a very low number) is present in every node. This situation drives to overfit the model, and the tree becomes unable to generalize correctly. Using a consistent test set or cross-validation can help in avoiding this problem; however, in the section dedicated to scikit-learn implementation, we're going to discuss how to limit the growth of the tree. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Binary decisions</h1>
                </header>
            
            <article>
                
<p><br/>
Let's consider an input dataset <em>X</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="44" width="247" src="assets/b6bcf22c-1fc0-43e8-9754-45fff47b2740.png"/></div>
<p>Every vector is made up of <em>m</em> features, so each of them can be a good candidate to create a node based on the (feature, threshold)<span> </span><span>tuple</span><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="195" width="163" class="image-border" src="assets/c271f7b4-3755-4aa5-895b-99fe6e8e5125.png"/></div>
<p>According to the feature and the threshold, the structure of the tree will change. Intuitively, we should pick the feature that best separates our data in other words, a perfect separating feature will be present only in a node and the two subsequent branches won't be based on it anymore. In real problems, this is often impossible, so it's necessary to find the feature that minimizes the number of following decision steps.</p>
<p>For example, let's consider a class of students where all males have dark hair and all females have blonde hair, while both subsets have samples of different sizes. If our task is to determine the composition of the class, we can start with the following subdivision:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="238" width="190" class="image-border" src="assets/394a80d4-1e15-4d40-adeb-fd7118942aec.png"/></p>
<p>However, the block <strong>Dark color?</strong> will contain both males and females (which are the targets we want to classify). This concept is expressed using the term <strong>purity</strong> (or, more often, its opposite concept, <strong>impurity</strong>). An ideal scenario is based on nodes where the impurity is null so that all subsequent decisions will be taken only on the remaining features. In our example, we can simply start from the color block:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="167" width="181" class="image-border" src="assets/c956d527-24cf-45c5-8370-675d3938117d.png"/></div>
<p>The two resulting sets are now pure according to the color feature, and this can be enough for our task. If we need further details, such as hair length, other nodes must be added; their impurity won't be null because we know that there are, for example, both male and female students with long hair.</p>
<p>More formally, suppose we define the selection tuple as:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="33" width="81" src="assets/cf6063f4-1921-4999-9c64-1e323aa8332e.png"/></p>
<p>Here, the first element is the index of the feature we want to use to split our dataset at a certain node (it will be the entire dataset only at the beginning; after each step, the number of samples decreases), while the second is the threshold that determines left and right branches. The choice of the best threshold is a fundamental element because it determines the structure of the tree and, therefore, its performance. The goal is to reduce the residual impurity in the least number of splits so as to have a very short decision path between the sample data and the classification result.<br/>
We can also define a total impurity measure by considering the two branches:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="55" width="297" src="assets/c8f37002-3e38-4d4e-b11e-177273d11e6c.png"/></p>
<p>Here, <em>D</em> is the whole dataset at the selected node, <em>D<sub>left</sub></em> and <em>D<sub>right</sub></em> are the resulting subsets (by applying the selection tuple), and the <em>I</em> are impurity measures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Impurity measures</h1>
                </header>
            
            <article>
                
<p>To define the most used impurity measures, we need to consider the total number of target classes:</p>
<p class="CDPAlignCenter CDPAlign"><img height="36" width="303" src="assets/0ddfc3e6-2e30-4dd5-8552-7417a5a52162.png"/></p>
<p>In a certain node <em>j</em>, we can define the probability <em>p(i|j)<strong> </strong></em>where <em>i</em> is an index [1, <em>n</em>] associated with each class. In other words, according to a frequentist approach, this value is the ratio between the number of samples belonging to class <em>i</em> and the total number of samples belonging to the selected node. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gini impurity index</h1>
                </header>
            
            <article>
                
<p>The Gini impurity index is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="52" width="225" src="assets/756bb2ca-86ba-4bb4-a005-ad55c466998b.png"/></div>
<p>Here, the sum is always extended to all classes. This is a very common measure and it's used as a default value by scikit-learn. Given a sample, the Gini impurity measures the probability of a misclassification if a label is randomly chosen using the probability distribution of the branch. The index reaches its minimum (0.0) when all the samples of a node are classified into a single category.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cross-entropy impurity index</h1>
                </header>
            
            <article>
                
<p>The cross-entropy measure is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="52" width="265" src="assets/8e62e0f8-d0d9-408c-98e6-af159ab08fae.png"/></div>
<p>This measure is based on information theory, and assumes null values only when samples belonging to a single class are present in a split, while it is maximum when there's a uniform distribution among classes (which is one of the worst cases in decision trees because it means that there are still many decision steps until the final classification). This index is very similar to the Gini impurity, even though, more formally, the cross-entropy allows you to select the split that minimizes the uncertainty about the classification, while the Gini impurity minimizes the probability of misclassification.</p>
<p>In <a href="c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml" target="_blank">Chapter 2</a>, <em>Important Elements in Machine Learning</em>, we defined the concept of mutual information <em>I(X; Y) = H(X) - H(X|Y)</em> as the amount of information shared by both variables, thereby reducing the uncertainty about <em>X</em> provided by the knowledge of <em>Y</em>. We can use this to define the information gain provided by a split:</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="256" src="assets/7cf49f95-bbd3-48d1-95ff-58eb3b5069b6.png"/></div>
<p>When growing a tree, we start by selecting the split that provides the highest information gain and proceed until one of the following conditions is verified:</p>
<ul>
<li>All nodes are pure</li>
<li>The information gain is null</li>
<li>The maximum depth has been reached</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Misclassification impurity index</h1>
                </header>
            
            <article>
                
<p>The misclassification impurity is the simplest index, defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="40" width="245" src="assets/656ff3b2-b969-48be-a787-c2f969543a8c.png"/></div>
<p>In terms of quality performance, this index is not the best choice because it's not particularly sensitive to different probability distributions (which can easily drive the selection to a subdivision using Gini or cross-entropy indexes).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature importance</h1>
                </header>
            
            <article>
                
<p>When growing a decision tree with a multidimensional dataset, it can be useful to evaluate the importance of each feature in predicting the output values. In <a href="7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml" target="_blank">Chapter 3</a>, <em>Feature Selection and Feature Engineering</em>, we discussed some methods to reduce the dimensionality of a dataset by selecting only the most significant features. Decision trees offer a different approach based on the impurity reduction determined by every single feature. In particular, considering a feature <em>x<sub>i</sub></em>, its importance can be determined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="59" width="214" src="assets/e20b55ba-7249-45f6-abce-66899ac81638.png"/> </div>
<p>The sum is extended to all nodes where <em>x<sub>i</sub></em> is used, and <em>N<sub>k</sub></em> is the number of samples reaching the node <em>k</em>. Therefore, the importance is a weighted sum of all impurity reductions computed considering only the nodes where the feature is used to split them. If the Gini impurity index is adopted, this measure is also called <strong>Gini importance</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree classification with scikit-learn</h1>
                </header>
            
            <article>
                
<p>scikit-learn contains the <kbd>DecisionTreeClassifier</kbd> <span>class,</span><span> </span><span>which can train a binary decision tree with Gini and cross-entropy impurity measures. In our example, let's consider a dataset with three features and three classes:</span></p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=3, n_informative=3, n_redundant=0, n_classes=3, n_clusters_per_class=1)</strong></pre>
<p>Let's <span>first</span><span> </span><span>consider a classification with default Gini impurity:</span></p>
<pre><strong>from sklearn.tree import DecisionTreeClassifier</strong><br/><strong>from sklearn.model_selection import cross_val_score</strong><br/><br/><strong>&gt;&gt;&gt; dt = DecisionTreeClassifier()</strong><br/><strong>&gt;&gt;&gt; print(cross_val_score(dt, X, Y, scoring='accuracy', cv=10).mean())</strong><br/><strong>0.970</strong></pre>
<p>A very interesting feature is given by the possibility of exporting the tree in <kbd>Graphviz</kbd> format and converting it into a PDF.</p>
<div class="packt_infobox">Graphviz is a free tool that can be downloaded from <a href="http://www.graphviz.org">http://www.graphviz.org</a>.</div>
<p>To export a trained tree, it is necessary to use the built-in function <kbd>export_graphviz()</kbd>:</p>
<pre><strong>from sklearn.tree import export_graphviz</strong><br/><br/><strong>&gt;&gt;&gt; dt.fit(X, Y)</strong><br/><strong>&gt;&gt;&gt; with open('dt.dot', 'w') as df:</strong><br/><strong>      df = export_graphviz(dt, out_file=df, </strong><br/><strong>                           feature_names=['A','B','C'], </strong><br/><strong>                           class_names=['C1', 'C2', 'C3'])</strong></pre>
<p>In this case, we have used <kbd>A</kbd>, <kbd>B</kbd>, and <kbd>C</kbd> as feature names and <kbd>C1</kbd>, <kbd>C2</kbd>, and <kbd>C3</kbd> as class names. Once the file has been created, it's possible converting to PDF using the command-line tool:</p>
<pre><strong>&gt;&gt;&gt; &lt;Graphviz Home&gt;bindot -Tpdf dt.dot -o dt.pdf</strong></pre>
<p>The graph for our example is rather large, so in the following feature you can see only a part of a branch:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a660ff28-558c-4cb6-99bf-fe44060afc50.png"/></div>
<p>As you can see, there are two kinds of nodes:</p>
<ul>
<li>Nonterminal, which contains the splitting tuple (as feature &lt;= threshold) and a positive impurity measure</li>
<li>Terminal, where the impurity measure is null and a final target class is present</li>
</ul>
<p>In both cases, you can always check the number of samples. This kind of graph is very useful in understanding how many decision steps are needed. Unfortunately, even if the process is quite simple, the dataset structure can lead to very complex trees, while other methods can immediately find out the most appropriate class. Of course, not all features have the same importance. If we consider the root of the tree and the first nodes, we find features that separate a lot of samples; therefore, their importance must be higher than that of all terminal nodes, where the residual number of samples is minimum. In scikit-learn, it's possible to assess the Gini importance of each feature after training a model:</p>
<pre><strong>&gt;&gt;&gt; dt.feature_importances_</strong><br/><strong>array([ 0.12066952,  0.12532507,  0.0577379 ,  0.14402762,  0.14382398,</strong><br/><strong>        0.12418921,  0.14638565,  0.13784106])</strong><br/><br/><strong>&gt;&gt;&gt; np.argsort(dt.feature_importances_)</strong><br/><strong>array([2, 0, 5, 1, 7, 4, 3, 6], dtype=int64)</strong></pre>
<p>The following figure shows a plot of the importances:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/d4af9918-ddb0-45ff-86df-e7713741a7fe.png"/></div>
<p>The most important features are 6, 3, 4, and 7, while feature 2, for example, separates a very small number of samples, and can be considered noninformative for the classification task.</p>
<p>In terms of efficiency, a tree can also be pruned using the <kbd>max_depth</kbd> parameter; however, it's not always so simple to understand which value is the best (grid search can help in this task). On the other hand, it's easier to decide what the maximum number of features to consider at each split should be. The parameter <kbd>max_features</kbd> can be used for this purpose:</p>
<ul>
<li>If it's a number, the value is directly taken into account at each split</li>
<li>If it's <kbd>'auto'</kbd> or <kbd>'sqrt'</kbd>, the square root of the number of features will be adopted</li>
<li>If it's <kbd>'log2'</kbd>, the logarithm (base 2) will be used</li>
<li>If it's <kbd>'None'</kbd>, all the features will be used (this is the default value)</li>
</ul>
<p>In general, when the number of total features is not too high, the default value is the best choice, although it's useful to introduce a small compression (via <kbd>sqrt</kbd> or <kbd>log2</kbd>) when too many features can interfere among themselves, reducing the efficiency. Another parameter useful for controlling both performance and efficiency is <kbd>min_samples_split</kbd>, which specifies the minimum number of samples to consider for a split. Some examples are shown in the following snippet:</p>
<pre><strong>&gt;&gt;&gt; cross_val_score(DecisionTreeClassifier(), X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.77308070807080698</strong><br/><br/><strong>&gt;&gt;&gt; cross_val_score(DecisionTreeClassifier(max_features='auto'), X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.76410071007100711</strong><br/><br/><strong>&gt;&gt;&gt; cross_val_score(DecisionTreeClassifier(min_samples_split=100), X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.72999969996999692</strong></pre>
<p>As already explained, finding the best parameters is generally a difficult task, and the best way to carry it out is to perform a grid search while including all the values that could affect the accuracy.</p>
<p>Using logistic regression on the previous set (only for comparison), we get:</p>
<pre><strong>from sklearn.linear_model import LogisticRegression</strong><br/><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><strong>&gt;&gt;&gt; cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.9053368347338937</strong></pre>
<p>So the score is higher, as expected. However, the original dataset was quite simple, and based on the concept of having a single cluster per class. This allows a simpler and more precise linear separation. If we consider a slightly different scenario with more variables and a more complex structure (which is hard to capture by a linear classifier), we can compare an ROC curve for both linear regression and decision trees:</p>
<pre><strong>&gt;&gt;&gt; nb_samples = 1000</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=8, n_informative=6, n_redundant=2,     n_classes=2, n_clusters_per_class=4)</strong></pre>
<p>The resulting ROC curve is shown in the following figure:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="511" width="500" class="image-border" src="assets/bf28527f-cb09-4f43-9f80-f3d3cec8c2e9.png"/></div>
<p>Using a grid search with the most common parameters on the MNIST digits dataset, we can get:</p>
<pre><strong>from sklearn.model_selection import GridSearchCV</strong><br/><br/><strong>param_grid = [</strong><br/><strong> { </strong><br/><strong>   'criterion': ['gini', 'entropy'],</strong><br/><strong>   'max_features': ['auto', 'log2', None],</strong><br/><strong>   'min_samples_split': [ 2, 10, 25, 100, 200 ],</strong><br/><strong>   'max_depth': [5, 10, 15, None]</strong><br/><strong> }</strong><br/><strong>]</strong><br/><br/><strong>&gt;&gt;&gt; gs = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid,</strong><br/><strong> scoring='accuracy', cv=10, n_jobs=multiprocessing.cpu_count())</strong><br/><br/><strong>&gt;&gt;&gt; gs.fit(digits.data, digits.target)</strong><br/><strong>GridSearchCV(cv=10, error_score='raise',</strong><br/><strong>       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,</strong><br/><strong>            max_features=None, max_leaf_nodes=None,</strong><br/><strong>            min_impurity_split=1e-07, min_samples_leaf=1,</strong><br/><strong>            min_samples_split=2, min_weight_fraction_leaf=0.0,</strong><br/><strong>            presort=False, random_state=None, splitter='best'),</strong><br/><strong>       fit_params={}, iid=True, n_jobs=8,</strong><br/><strong>       param_grid=[{'max_features': ['auto', 'log2', None], 'min_samples_split': [2, 10, 25, 100, 200], 'criterion': ['gini', 'entropy'], 'max_depth': [5, 10, 15, None]}],</strong><br/><strong>       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,</strong><br/><strong>       scoring='accuracy', verbose=0)</strong><br/><br/><strong>&gt;&gt;&gt; gs.best_estimator_</strong><br/><strong>DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,</strong><br/><strong>            max_features=None, max_leaf_nodes=None,</strong><br/><strong>            min_impurity_split=1e-07, min_samples_leaf=1,</strong><br/><strong>            min_samples_split=2, min_weight_fraction_leaf=0.0,</strong><br/><strong>            presort=False, random_state=None, splitter='best')</strong><br/><br/><strong>&gt;&gt;&gt; gs.best_score_</strong><br/><strong>0.8380634390651085</strong></pre>
<p>In this case, the element that impacted accuracy the <span>most</span><span> is the minimum number of samples to consider for a split. This is reasonable, considering the structure of this dataset and the need to have many branches to capture even small changes.  </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble learning</h1>
                </header>
            
            <article>
                
<p>Until now, we have trained models on single instances, iterating an algorithm in order to minimize a target loss function. This approach is based on so-called strong learners, or methods that are optimized to solve a specific problem by looking for the best possible solution. Another approach is based on a set of weak learners that can be trained in parallel or sequentially (with slight modifications on the parameters) and used as an ensemble based on a majority vote or the averaging of results. These methods can be classified into two main categories:</p>
<ul>
<li><strong>Bagged (or Bootstrap) trees</strong>: In this case, the ensemble is built completely. The training process is based on a random selection of the splits and the predictions are based on a majority vote. Random forests are an example of bagged tree ensembles.</li>
<li><strong>Boosted trees</strong>: The ensemble is built sequentially, focusing on the samples that have been previously misclassified. Examples of boosted trees are AdaBoost and gradient tree boosting.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p>A random forest is a set of decision trees built on random samples with a different policy for splitting a node: Instead of looking for the best choice, in such a model, a random subset of features (for each tree) is used, trying to find the threshold that best separates the data. As a result, there will be many trees trained in a weaker way and each of them will produce a different prediction.</p>
<p>There are two ways to interpret these results; the more common approach is based on a majority vote (the most voted class will be considered correct). However, scikit-learn implements an algorithm based on averaging the results, which yields very accurate predictions. Even if they are theoretically different, the probabilistic average of a trained random forest cannot be very different from the majority of predictions (otherwise, there should be different stable points); therefore the two methods often drive to comparable results.</p>
<p>As an example, let's consider the MNIST dataset with random forests made of a different number of trees:</p>
<pre><strong><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier<br/>&gt;&gt;&gt; nb_classifications = 100<br/></span>&gt;&gt;&gt; accuracy = []</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(1, <span class="n">nb_classifications</span>):</strong><br/><strong>      a = cross_val_score(RandomForestClassifier(n_estimators=i), digits.data, digits.target,  scoring='accuracy', cv=10).mean()</strong><br/><strong>      rf_accuracy.append(a)</strong></pre>
<p>The resulting plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="372" width="539" class="image-border" src="assets/f68b4c9a-a179-49c2-a94d-4eac0c13aee3.png"/></div>
<p>As expected, the accuracy is low when the number of trees is under a minimum threshold; however, it starts increasing rapidly with fewer than 10 trees. A value between 20 and 30 trees yields the optimal result (95%), which is higher than for a single decision tree. When the number of trees is low, the variance of the model is very high and the averaging process produces many incorrect results; however, increasing the number of trees reduces the variance and allows the model to converge to a very stable solution. scikit-learn <span>also</span><span> </span><span>offers a variance that enhances the randomness in selecting the best threshold. Using the</span> <kbd>ExtraTreesClassifier</kbd> <span>class, it's possible to implement a model that randomly computes thresholds and picks the best one. As discussed in the official documentation, this allows us to further reduce the variance:</span></p>
<pre><strong>from sklearn.ensemble import ExtraTreesClassifier</strong><br/><strong>&gt;&gt;&gt; nb_classifications = 100<br/></strong><br/><strong>&gt;&gt;&gt; for i in range(1, nb_classifications):</strong><br/><strong>      a = cross_val_score(ExtraTreesClassifier(n_estimators=i), digits.data, digits.target,  scoring='accuracy', cv=10).mean()</strong><br/><strong>      et_accuracy.append(a)</strong></pre>
<p>The results (with the same number of trees) in terms of accuracy are slightly better, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="325" width="466" class="image-border" src="assets/11da180a-5d20-4db6-be64-ecf3af8a4271.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature importance in random forests</h1>
                </header>
            
            <article>
                
<p>The concept of feature importance that we previously introduced can also be applied to random forests, computing the average over all trees in the forest:</p>
<div class="CDPAlignCenter CDPAlign"><img height="60" width="274" src="assets/1018e9d6-c672-4a27-9924-dd0ba2ba4108.png"/></div>
<p>We can easily test the importance evaluation with a dummy dataset containing 50 features with 20 noninformative elements:</p>
<pre><strong>&gt;&gt;&gt; nb_samples = 1000</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=50, n_informative=30, n_redundant=20, n_classes=2, n_clusters_per_class=5)</strong></pre>
<p>The importance of the first 50 features according to a random forest with 20 trees is plotted in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/29c72b5b-4378-47c0-862c-29260e8218a4.png"/></div>
<p>As expected, there are a few <em>very</em> important features, a block of features with a medium importance, and a tail containing features that have quite a low influence on the predictions. This type of plot is also useful during the analysis stage to better understand how the decision process is structured. With multidimensional datasets, it's rather difficult to understand the influence of every factor, and sometimes many important business decisions are made without a complete awareness of their potential impact. Using decision trees or random forests, it's possible to assess the "real" importance of all features and exclude all the elements under a fixed threshold. In this way, a complex decision process can be simplified and, at the same time, be partially denoised. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaBoost</h1>
                </header>
            
            <article>
                
<p>Another technique is called <strong>AdaBoost</strong> (short for <strong>Adaptive Boosting</strong>) and works in a slightly different way than many other classifiers. The basic structure behind this can be a decision tree, but the dataset used for training is continuously adapted to force the model to focus on those samples that are misclassified. Moreover, the classifiers are added sequentially, so a new one boosts the previous one by improving the performance in those areas where it was not as accurate as expected.</p>
<p>At each iteration, a weight factor is applied to each sample so as to increase the importance of the samples that are wrongly predicted and decrease the importance of others. In other words, the model is repeatedly boosted, starting as a very weak learner until the maximum <kbd>n_estimators</kbd> number is reached. The predictions, in this case, are always obtained by majority vote.</p>
<p>In the scikit-learn implementation, there's also a parameter called <kbd>learning_rate</kbd> that weighs the effect of each classifier. The default value is 1.0, so all estimators are considered to have the same importance. However, as we can see with the MNIST dataset, it's useful to decrease this value so that each contribution is weakened:</p>
<pre><strong>from sklearn.ensemble import AdaBoostClassifier</strong><br/><br/><strong>&gt;&gt;&gt; accuracy = []<br/><br/>&gt;&gt;&gt; nb_classifications = 100</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(1, nb_classifications):</strong><br/><strong>       a = cross_val_score(AdaBoostClassifier(n_estimators=i, learning_rate=0.1), digits.data, digits.target, scoring='accuracy', cv=10).mean()</strong><br/><strong>&gt;&gt;&gt; ab_accuracy.append(a)</strong></pre>
<p>The result is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="358" width="512" class="image-border" src="assets/f8b2c4ac-f53a-47a2-a06d-734e1138572f.png"/></div>
<p>The accuracy is not so high as in the previous examples; however, it's possible to see that when the boosting adds about 20-30 trees, it reaches a stable value. A grid search on <kbd>learning_rate</kbd> could allow you to find the optimal value; however, the sequential approach in this case is not preferable. A classic random forest, which works with a fixed number of trees since the first iteration, performs better. This may well be due to the strategy adopted by AdaBoost; in this set, increasing the weight of the correctly classified samples and decreasing the strength of misclassifications can produce an oscillation in the loss function, with a final result that is not the optimal minimum point. Repeating the experiment with the Iris dataset (which is structurally much simpler) yields better results:</p>
<pre><strong>from sklearn.datasets import load_iris</strong><br/><br/><strong>&gt;&gt;&gt; iris = load_iris()</strong><br/><br/><strong>&gt;&gt;&gt; ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)</strong><br/><strong>&gt;&gt;&gt; cross_val_score(ada, iris.data, iris.target, scoring='accuracy', cv=10).mean()</strong><br/><strong>0.94666666666666666</strong></pre>
<p>In this case, a learning rate of 1.0 is the best choice, and it's easy to understand that the boosting process can be stopped after a few iterations. In the following figure, you can see a plot showing the accuracy for this dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/d10abc62-9d30-48a7-b8bd-4f0ebe340f9b.png"/></div>
<p>After about 10 iterations, the accuracy becomes stable (the residual oscillation can be discarded), reaching a value that is compatible with this dataset. The advantage of using AdaBoost can be appreciated in terms of resources; it doesn't work with a fully configured set of classifiers and the whole set of samples. Therefore, it can help save time when training on large datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient tree boosting</h1>
                </header>
            
            <article>
                
<p>Gradient tree boosting is a technique that allows you to build a tree ensemble step by step with the goal of minimizing a target loss function. The generic output of the ensemble can be represented as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="119" src="assets/a294a9fd-b0b4-43c7-a5e5-3fa85a71f887.png"/> </div>
<p class="CDPAlignLeft CDPAlign">Here, <em>f<sub>i</sub>(x)</em> is a <span>function representing a</span> weak learner. The algorithm is based on the concept of adding a new decision tree at each step so as to minimize the global loss function using the steepest gradient descent method (see <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">https://en.wikipedia.org/wiki/Method_of_steepest_descent</a>, for further information):</p>
<div class="CDPAlignCenter CDPAlign"><img height="36" width="192" src="assets/7b2d39b4-17ea-47a0-ab11-0799233a1087.png"/></div>
<p class="CDPAlignLeft CDPAlign">After introducing the gradient, the previous expression becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="55" width="421" src="assets/4c2818e8-e0f9-4770-805e-2a12349731ac.png"/></div>
<p class="CDPAlignLeft CDPAlign">scikit-learn implements the <kbd>GradientBoostingClassifier</kbd> <span>class,</span><span> </span><span>supporting two classification loss functions:</span></p>
<ul>
<li>Binomial/multinomial negative log-likelihood (which is the default choice)</li>
<li>Exponential (such as AdaBoost)</li>
</ul>
<p>Let's evaluate the accuracy of this method using a more complex dummy dataset made up of 500 samples with four features (three informative and one redundant) and three classes:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=4, n_informative=3, n_redundant=1, n_classes=3)</strong></pre>
<p>Now we can collect the cross-validation average accuracy for a number of estimators in the range (1, 50). The loss function is the default one (multinomial negative log-likelihood):</p>
<pre><strong>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.model_selection import cross_val_score<br/><br/>&gt;&gt;&gt; a = []</strong><br/><strong>&gt;&gt;&gt; max_estimators = 50</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(1, max_estimators):</strong><br/><strong>&gt;&gt;&gt; score = cross_val_score(GradientBoostingClassifier(n_estimators=i, learning_rate=10.0/float(i)), X, Y, cv=10, scoring='accuracy').mean()</strong><br/><strong>&gt;&gt;&gt; a.append(score)</strong></pre>
<p>While increasing the number of estimators (parameter <kbd>n_estimators</kbd>), it's important to decrease the learning rate (parameter <kbd>learning_rate</kbd>). The optimal value cannot be easily predicted; therefore, it's often useful to perform a grid search. In our example, I've set a very high learning rate at the beginning (5.0), which converges to 0.05 when the number of estimators is equal to 100. This is not a perfect choice (unacceptable in most real cases!), and it has been made only to show the different accuracy performances. The results are shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/fb8c7747-9a50-4406-9e86-646860d593f4.png"/></div>
<p class="CDPAlignLeft CDPAlign">As it's possible to see, the optimal number of estimators is about 50, with a learning rate of 0.1. The reader can try different combinations and compare the performances of this algorithm with the other ensemble methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Voting classifier</h1>
                </header>
            
            <article>
                
<p>A very interesting ensemble solution is offered by the class <kbd>VotingClassifier</kbd>, which isn't an actual classifier but a wrapper for a set of different ones that are trained and evaluated in parallel. The final decision for a prediction is taken by majority vote according to two different strategies:</p>
<ul>
<li><strong>Hard voting</strong>: In this case, the class that received the major number of votes, <em>N<sub>c</sub>(y<sub>t</sub>)</em>, will be chosen:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="28" width="244" src="assets/50ee8383-9a24-45df-bcf6-452d5f607f28.png"/></div>
<ul>
<li><strong>Soft voting</strong>: In this case, the probability vectors for each predicted class (<span>for</span> <span>all</span><span> classifiers</span>) are summed up and averaged. The winning class is the one corresponding to the highest value:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="52" width="312" src="assets/5a9b01d2-6cf5-4792-97a0-8ca60ad399c2.png"/></div>
<p>Let's consider a dummy dataset and compute the accuracy with a hard voting strategy:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, n_classes=2)</strong></pre>
<p>For our examples, we are going to consider three classifiers: logistic regression, decision tree (with default Gini impurity), and an SVM (with a polynomial kernel and <kbd>probability=True</kbd> in order to generate the probability vectors). This choice has been made only for didactic purposes and may not be the best one. When creating an ensemble, it's useful to consider the different features of each involved classifier and avoid "duplicate" algorithms (for example, a logistic regression and a linear SVM or a perceptron are likely to yield very similar performances). In many cases, it can be useful to mix nonlinear classifiers with random forests or AdaBoost classifiers. The reader can repeat this experiment with other combinations, comparing the performance of each single estimator and the accuracy of the voting classifier:</p>
<pre><strong>from sklearn.linear_model import LogisticRegression</strong><br/><strong>from sklearn.svm import SVC</strong><br/><strong>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import VotingClassifier</strong><br/><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><strong>&gt;&gt;&gt; svc = SVC(kernel='poly', probability=True)</strong><br/><strong>&gt;&gt;&gt; dt = DecisionTreeClassifier()</strong><br/><br/><strong>&gt;&gt;&gt; classifiers = [('lr', lr),</strong><br/><strong>                   ('dt', dt),</strong><br/><strong>                   ('svc', svc)]<br/><br/>&gt;&gt;&gt; vc = VotingClassifier(estimators=classifiers, voting='hard')</strong></pre>
<p>Computing the cross-validation accuracies, we get:</p>
<pre><strong>from sklearn.model_selection import cross_val_score</strong><br/><br/><strong>&gt;&gt;&gt; a = []</strong><br/><br/><strong>&gt;&gt;&gt; a.append(cross_val_score(lr, X, Y, scoring='accuracy', cv=10).mean())</strong><br/><strong>&gt;&gt;&gt; a.append(cross_val_score(dt, X, Y, scoring='accuracy', cv=10).mean())</strong><br/><strong>&gt;&gt;&gt; a.append(cross_val_score(svc, X, Y, scoring='accuracy', cv=10).mean())</strong><br/><strong>&gt;&gt;&gt; a.append(cross_val_score(vc, X, Y, scoring='accuracy', cv=10).mean())<br/><br/>&gt;&gt;&gt; print(np.array(a))<br/>[ 0.90182873  0.84990876  0.87386955  0.89982873]<br/></strong></pre>
<p>The accuracies of each single classifier and of the ensemble are plotted in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/9cd9bfad-be11-4f1c-b9c0-64bb0a5ec04a.png"/></div>
<p class="CDPAlignLeft CDPAlign">As expected, the ensemble takes advantage of the different algorithms and yields better performance than any single one. We can now repeat the experiment with soft voting, considering that it's also possible to introduce a weight vector (through the parameter <kbd>weights</kbd>) to give more or less importance to each classifier:</p>
<div class="CDPAlignCenter CDPAlign"><img height="58" width="319" src="assets/bef91f33-3792-4e56-bfbc-974b1f8ef89a.png"/></div>
<p class="CDPAlignLeft CDPAlign">For example, considering the previous figure, we can decide to give more importance to the logistic regression and less to the decision tree and SVM:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; weights = [1.5, 0.5, 0.75]</strong><br/><br/><strong>&gt;&gt;&gt; vc = VotingClassifier(estimators=classifiers, weights=weights, voting='soft')</strong></pre>
<p>Repeating the same calculations for the cross-validation accuracies, we get:</p>
<pre>&gt;&gt;&gt; <strong>print(np.array(a))<br/>[ 0.90182873  0.85386795  0.87386955  0.89578952]</strong></pre>
<p>The resulting plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="338" width="501" class="image-border" src="assets/e9d252d4-2b89-4a6d-b0de-599c98eaadfa.png"/></div>
<p class="CDPAlignLeft CDPAlign">Weighting is not limited to the soft strategy. It can also be applied to hard voting, but in that case, it will be used to filter (reduce or increase) the number of actual occurrences.</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="304" src="assets/412a2a2a-34f5-43ed-8605-f6537b94992d.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, <em>N<sub>c</sub>(y<sub>t</sub>,w)</em> is the number of votes for each target class, each of them multiplied by the corresponding classifier weighting factor.</p>
<p class="CDPAlignLeft CDPAlign">A voting classifier can be a good choice whenever a single strategy is not able to reach the desired accuracy threshold; while exploiting the different approaches, it's possible to capture many microtrends using only a small set of strong (but sometimes limited) learners.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Louppe G., Wehenkel L., Sutera A., and Geurts P., <em>Understanding variable importances in forests of randomized trees</em>, NIPS Proceedings 2013.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced decision trees as a particular kind of classifier. The basic idea behind their concept is that a decision process can become sequential by using splitting nodes, where, according to the sample, a branch is chosen until we reach a final leaf. In order to build such a tree, the concept of impurity was introduced; starting from a complete dataset, our goal is to find a split point that creates two distinct sets that should share the minimum number of features and, at the end of the process, should be associated with a single target class. The complexity of a tree depends on the intrinsic purity—in other words, when it's always easy to determine a feature that best separates a set, the depth will be lower. However, in many cases, this is almost impossible, so the resulting tree needs many intermediate nodes to reduce the impurity until it reaches the final leaves.</p>
<p>We also discussed some ensemble learning approaches: random forests, AdaBoost, gradient tree boosting and voting classifiers. They are all based on the idea of training several weak learners and evaluating their predictions using a majority vote or an average. However, while a random forest creates a set of decision trees that are partially randomly trained, AdaBoost and gradient boost trees adopt the technique of boosting a model by adding a new one, step after step, and focusing only on those samples that have been previously misclassified or by focusing on the minimization of a specific loss function. A voting classifier, instead, allows the mixing of different classifiers, adopting a majority vote to decide which class must be considered as the winning one during a prediction.<br/>
In the next chapter, we're going to introduce the first unsupervised learning approach, k-means, which is one of most diffused clustering algorithms. We will concentrate on its strengths and weaknesses, and explore some alternatives offered by scikit-learn.</p>


            </article>

            
        </section>
    </body></html>