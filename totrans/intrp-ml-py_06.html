<html><head></head><body>
  <div id="_idContainer170" class="Basic-Text-Frame">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-157" class="chapterTitle">Anchors and Counterfactual Explanations</h1>
    <p class="normal">In previous chapters, we learned how to attribute model decisions to features and their interactions with state-of-the-art global and local model interpretation methods. However, the decision boundaries are not always easy to define or interpret with these methods. Wouldn’t it be nice to be able to derive human-interpretable rules from model interpretation methods? In this chapter, we will cover a few human-interpretable, local, classification-only model interpretation methods. We will first learn how to use scoped rules called <strong class="keyWord">anchors</strong> to explain complex models with statements such as <em class="italic">if X conditions are met, then Y is the outcome</em>. Then, we will explore <strong class="keyWord">counterfactual</strong> explanations that follow the form <em class="italic">if Z conditions aren’t met, then Y is not the outcome</em>.</p>
    <p class="normal">These are the main topics we are going to cover in this chapter:</p>
    <ul>
      <li class="bulletList">Understanding anchor explanations</li>
      <li class="bulletList">Exploring counterfactual explanations</li>
    </ul>
    <p class="normal">Let’s begin!</p>
    <h1 id="_idParaDest-158" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">catboost</code>, <code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">alibi</code>, <code class="inlineCode">tensorflow</code>, <code class="inlineCode">shap</code>, and <code class="inlineCode">witwidget</code> libraries. Instructions on how to install all of these libraries are in the <em class="italic">Preface</em>. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/tH0y7"><span class="url">https://packt.link/tH0y7</span></a>.</p>
    </div>
    <h1 id="_idParaDest-159" class="heading-1">The mission</h1>
    <p class="normal">In the<a id="_idIndexMarker608"/> United States, for the last two decades, private companies and nonprofits have developed <a id="_idIndexMarker609"/>criminal <strong class="keyWord">Risk Assessment Instruments/Tools</strong> (<strong class="keyWord">RAIs</strong>), most of which employ statistical models. As many states can no longer afford their large prison populations, these methods have increased in popularity, <em class="italic">guiding</em> judges and parole boards through every step of the prison system. </p>
    <p class="normal">These are high-impact decisions that can determine if a person is released from prison. Can we afford for these decisions to be wrong? Can we accept the recommendations from these systems without understanding why they were made? Worst of all, we don’t exactly know how an assessment was made. The risk is usually calculated with a white-box model, but, in practice, a black-box model is used because it is proprietary. Predictive performance is also relatively low, with median AUC scores for a sample of nine tools ranging between 0.57 and 0.74 according to the paper <em class="italic">Performance of Recidivism Risk Assessment Instruments in U.S. Correctional Settings</em>.</p>
    <p class="normal">Even though traditional statistical methods are still the norm for criminal justice models, to improve performance, some researchers have proposed leveraging more complex models, such as Random Forest with larger datasets. Far from being science fiction drawn from <em class="italic">Minority Report</em> or <em class="italic">Black Mirror</em>, in some countries, scoring people based on their likelihood of engaging in antisocial, or even antipatriotic, behavior with big data and machine learning is already a reality.</p>
    <p class="normal">As more and more AI solutions attempt to make life-changing predictions about us with our data, fairness must be properly assessed, and all its ethical and practical implications must be adequately discussed. <em class="chapterRef">Chapter 1</em>, <em class="italic">Interpretation, Interpretability, and Explainability; and Why Does It All Matter?</em>, covered how fairness is an integral concept for machine learning interpretation. You can evaluate fairness in any model, but fairness is especially difficult when it involves human behavior. The dynamics between human psychological, neurological, and sociological factors are extremely complicated. In the context of predicting criminal behavior, it boils down to what factors are potentially to blame for a crime, as it wouldn’t be fair to include anything else in a model, and how these factors interact.</p>
    <p class="normal">Quantitative criminologists are still debating the best predictors of criminality and their root causes. They’re also debating whether it is ethical to <em class="italic">blame</em> a criminal for these factors to begin with. Thankfully, demographic traits such as race, gender, and nationality are no longer used in criminal risk assessments. But this doesn’t mean that these methods are no longer biased. Scholars recognize the problem and are proposing solutions.</p>
    <p class="normal">This<a id="_idIndexMarker610"/> chapter will examine racial bias in one of the most widely used risk assessment tools. Given this topic’s sensitive and relevant nature, it was essential to provide a modicum of context about criminal risk assessment tools and how machine learning and fairness connect with all of them. We won’t go into much more detail, but understanding the context is important to appreciate how machine learning could perpetuate structural inequality and unfair biases.</p>
    <p class="normal">Now, let’s introduce you to your mission for this chapter!</p>
    <h2 id="_idParaDest-160" class="heading-2">Unfair bias in recidivism risk assessments</h2>
    <p class="normal">Imagine a scenario of an investigative journalist writing an article on an African American defendant detained while awaiting trial. A<a id="_idIndexMarker611"/> tool called <strong class="keyWord">Correctional Offender Management Profiling for Alternative Sanction</strong> (<strong class="keyWord">COMPAS</strong>) deemed him as being at risk of recidivism. <strong class="keyWord">Recidivism</strong> is <a id="_idIndexMarker612"/>when someone relapses into criminal behavior. And the score convinced the judge that the defendant had to be detained without <a id="_idIndexMarker613"/>considering any other arguments or testimonies. He was locked up for many months, and in the trial, was found not guilty. Over five years have passed since the trial, and he hasn’t been accused of any crime. You could say the prediction for recidivism was a false positive.</p>
    <p class="normal">The journalist has reached out to you because she would like to ascertain with data science whether there was unfair bias in this case. The COMPAS risk assessment is computed using 137 questions (<a href="https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html"><span class="url">https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html</span></a>). It includes questions such as the following:</p>
    <ul>
      <li class="bulletList">“Based on the screener’s observations, is this person a suspected or admitted gang member?”</li>
      <li class="bulletList">“How often have you moved in the last 12 months?”</li>
      <li class="bulletList">“How often do you have barely enough money to get by?”</li>
      <li class="bulletList">Psychometric LIKERT scale questions such as “I have never felt sad about things in my life.”</li>
    </ul>
    <p class="normal">Even though race is not one of the questions, many of these questions may correlate with race. Not to mention, in some cases, they can be more a question of subjective opinion than fact, and thus be prone to bias.</p>
    <p class="normal">The<a id="_idIndexMarker614"/> journalist cannot provide you with the 137 answered questions or the COMPAS model because this data is not publicly available. However, thankfully, all defendants’ demographic and recidivism data for the same county in Florida is available.</p>
    <h1 id="_idParaDest-161" class="heading-1">The approach</h1>
    <p class="normal">You have decided <a id="_idIndexMarker615"/>to do the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Train a proxy model</strong>: You <a id="_idIndexMarker616"/>don’t have the original features or model, but all is not lost because you have the COMPAS scores—the labels. And we also have features relevant to the problem we can connect to these labels with models. By approximating the COMPAS model via the proxies, you can assess the fairness of the COMPAS decisions. In this chapter, we will train a CatBoost model.</li>
      <li class="bulletList"><strong class="keyWord">Anchor explanations</strong>: Using <a id="_idIndexMarker617"/>this method will unearth insights into why the proxy model makes specific predictions using a series of rules called anchors, which tell you where the decision boundaries lie. The boundaries are relevant for our mission because we want to know why the defendant has been wrongfully predicted to recidivate. It’s an approximate boundary to the original model, but there’s still some truth to it.</li>
      <li class="bulletList"><strong class="keyWord">Counterfactual explanations</strong>: While Anchor explains why a decision was made, counterfactuals <a id="_idIndexMarker618"/>can be useful to examine why a decision was not made. This is particularly useful in inspecting the fairness of decisions. We will use an unbiased method to find counterfactuals and then use the <strong class="keyWord">What-If Tool</strong> (<strong class="keyWord">WIT</strong>) to explore counterfactuals and fairness further.</li>
    </ul>
    <h1 id="_idParaDest-162" class="heading-1">The preparations</h1>
    <p class="normal">You will <a id="_idIndexMarker619"/>find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/06/Recidivism.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/06/Recidivism.ipynb</span></a>.</p>
    <h2 id="_idParaDest-163" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run this<a id="_idIndexMarker620"/> example, you need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code> and <code class="inlineCode">numpy</code> to manipulate the dataset</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn), and <code class="inlineCode">catboost</code> to split the data and fit the models</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">alibi</code>, <code class="inlineCode">tensorflow</code>, <code class="inlineCode">shap</code>, and <code class="inlineCode">witwidget</code> to visualize the interpretations</li>
    </ul>
    <p class="normal">You should load all of them first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">from</span> catboost <span class="hljs-keyword">import</span> CatBoostClassifier
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> alibi.utils.mapping <span class="hljs-keyword">import</span> ohe_to_ord, ord_to_ohe
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> alibi.explainers <span class="hljs-keyword">import</span> AnchorTabular, CounterFactualProto
<span class="hljs-keyword">import</span> shap
<span class="hljs-keyword">import</span> witwidget
<span class="hljs-keyword">from</span> witwidget.notebook.visualization <span class="hljs-keyword">import</span> WitWidget, \ WitConfigBuilder
</code></pre>
    <p class="normal">Let’s check that TensorFlow has loaded the right version with <code class="inlineCode">print(tf.__version__)</code>. It should be 2.0 or above. We should also disable eager execution and verify that it worked with this command. The output should say that it’s <code class="inlineCode">False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.compat.v1.disable_eager_execution()
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Eager execution enabled:'</span>, tf.executing_eagerly())
</code></pre>
    <h2 id="_idParaDest-164" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">We load the <a id="_idIndexMarker621"/>data like this into a DataFrame called <code class="inlineCode">recidivism_df</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">recidivism_df = mldatasets.load(<span class="hljs-string">"recidivism-risk"</span>, prepare=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">There should be almost 15,000 records and 23 columns. We can verify this was the case with <code class="inlineCode">info()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">recidivism_df.info()
</code></pre>
    <p class="normal">The following output checks out. All features are numeric with no missing values, and categorical features have already been one-hot-encoded for us:</p>
    <pre class="programlisting con"><code class="hljs-con">Int64Index: 14788 entries, 0 to 18315
Data columns (total 23 columns):
#   Column                 Non-Null Count  Dtype
--  ------                 --------------  -----
0   age                    14788 non-null  int8
1   juv_fel_count          14788 non-null  int8
2   juv_misd_count         14788 non-null  int8
3   juv_other_count        14788 non-null  int64
4   priors_count           14788 non-null  int8
5   is_recid               14788 non-null  int8
6   sex_Female             14788 non-null  uint8
7   sex_Male               14788 non-null  uint8
8   race_African-American  14788 non-null  uint8
9   race_Asian             14788 non-null  uint8
13  race_Other             14788 non-null  uint8
14  c_charge_degree_(F1)   14788 non-null  uint8
15  c_charge_degree_(F2)   14788 non-null  uint8
21  c_charge_degree_Other  14788 non-null  uint8
22  compas_score           14788 non-null  int64
</code></pre>
    <h3 id="_idParaDest-165" class="heading-3">The data dictionary</h3>
    <p class="normal">There are only <a id="_idIndexMarker622"/>nine features, but they become 22 columns because of the categorical encoding:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">age</code>: Continuous; the age of the defendant (between 18 and 96).</li>
      <li class="bulletList"><code class="inlineCode">juv_fel_count</code>: Ordinal; the number of juvenile felonies (between 0 and 2).</li>
      <li class="bulletList"><code class="inlineCode">juv_misd_count</code>: Ordinal; the number of juvenile misdemeanors (between 0 and 1).</li>
      <li class="bulletList"><code class="inlineCode">juv_other_count</code>: Ordinal; the number of juvenile convictions that are neither felonies nor misdemeanors (between 0 and 1).</li>
      <li class="bulletList"><code class="inlineCode">priors_count</code>: Ordinal; the number of prior crimes committed (between 0 and 13).</li>
      <li class="bulletList"><code class="inlineCode">is_recid</code>: Binary; did the defendant recidivate within 2 years (1 for yes, 0 for no)?</li>
      <li class="bulletList"><code class="inlineCode">sex</code>: Categorical; the gender of the defendant.</li>
      <li class="bulletList"><code class="inlineCode">race</code>: Categorical; the race of the defendant.</li>
      <li class="bulletList"><code class="inlineCode">c_charge_degree</code>: Categorical; the degree of what the defendant is currently being charged with. The United States classifies criminal offenses as felonies, misdemeanors, and infractions, ordered from most serious to least. These are subclassified in the form of degrees, which go from 1<sup class="superscript">st</sup> (most serious offenses) to 3<sup class="superscript">rd</sup> or 5<sup class="superscript">th</sup> (least severe). However, even though this is standard for federal offenses, it is tailored to state law. For felonies, Florida (<a href="http://www.dc.state.fl.us/pub/scoresheet/cpc_manual.pdf"><span class="url">http://www.dc.state.fl.us/pub/scoresheet/cpc_manual.pdf</span></a>) has a level system<a id="_idIndexMarker623"/> that determines the severity of a crime regardless of the degree, and this goes from 10 (most severe) to 1 (least). 
    <p class="normal">The categories of this feature are prefixed with <em class="italic">F</em> for felonies and <em class="italic">M</em> for misdemeanors. They are followed by a number, which is a level for felonies and a degree for misdemeanors.</p></li>
    </ul>
    <ul>
      <li class="bulletList"><code class="inlineCode">compas_score</code>: Binary; COMPAS scores defendants as “low,” “medium,” or “high” risk. In practice, “medium” is often treated as “high” by decision-makers, so this feature has been converted to binary to reflect this behavior: 1: high/medium risk and 0: low risk.</li>
    </ul>
    <h3 id="_idParaDest-166" class="heading-3">Examining predictive bias with confusion matrices</h3>
    <p class="normal">There<a id="_idIndexMarker624"/> are two binary features in the dataset. The first one is the recidivism risk prediction made by COMPAS (<code class="inlineCode">compas_score</code>). The second one (<code class="inlineCode">is_recid</code>) is the <em class="italic">ground truth</em> because it’s what happened within 2 years of the defendant’s arrest. Just as you would with the prediction of any model against its training labels, you can build confusion matrices with these two features. scikit-learn can produce one with the <code class="inlineCode">confusion_matrix</code> function (<code class="inlineCode">cf_matrix</code>), and we can then create a Seaborn <code class="inlineCode">heatmap</code> with it. Instead of plotting the number of <strong class="keyWord">True Negatives</strong> (<strong class="keyWord">TNs</strong>), <strong class="keyWord">False Positives</strong> (<strong class="keyWord">FPs</strong>), <strong class="keyWord">False Negatives</strong> (<strong class="keyWord">FNs</strong>), and <strong class="keyWord">True Positives</strong> (<strong class="keyWord">TPs</strong>), we can plot percentages with a simple division (<code class="inlineCode">cf_matrix/np.sum(cf_matrix)</code>). The other parameters of <code class="inlineCode">heatmap</code> only assist with formatting:</p>
    <pre class="programlisting code"><code class="hljs-code">cf_matrix = metrics.confusion_matrix(
    recidivism_df.is_recid,
    recidivism_df.compas_score
)
sns.heatmap(
    cf_matrix/np.<span class="hljs-built_in">sum</span>(cf_matrix),
    annot=<span class="hljs-literal">True</span>,
    fmt=<span class="hljs-string">'.2%'</span>,
    cmap=<span class="hljs-string">'Blues'</span>,
    annot_kws={<span class="hljs-string">'size'</span>:<span class="hljs-number">16</span>}
)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker625"/> preceding code outputs <em class="italic">Figure 6.1</em>. The top-right corner is FPs, which are nearly one-fifth of all predictions, and together with the FNs in the bottom-left corner, they make up over two-thirds:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_01.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 6.1: Confusion matrix between the predicted risk of recidivism (compas_score) and the ground truth (is_recid)</p>
    <p class="normal"><em class="italic">Figure 6.1</em> shows that the COMPAS model’s predictive performance is not very good, especially if <a id="_idIndexMarker626"/>we assume that criminal justice decision-makers are taking medium or high risk assessments at face value. It also tells us that FP and FNs occur at a similar rate. Nevertheless, simple visualizations such as the confusion matrix obscure predictive disparities between subgroups of a population. We can quickly compare disparities between two subgroups that historically have been treated differently by the United States criminal justice system. To this end, we first subdivide our DataFrame into two DataFrames: one for Caucasians (<code class="inlineCode">recidivism_c_df</code>) and another for African Americans (<code class="inlineCode">recidivism_aa_df</code>). Then we can generate confusion matrices for each DataFrame and plot them side by side with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">recidivism_c_df =\
recidivism_df[recidivism_df[<span class="hljs-string">'race_Caucasian'</span>] == <span class="hljs-number">1</span>]
recidivism_aa_df =\
recidivism_df[recidivism_df[<span class="hljs-string">'race_African-American'</span>] == <span class="hljs-number">1</span>]
_ = mldatasets.compare_confusion_matrices(
    recidivism_c_df.is_recid,
    recidivism_c_df.compas_score,
    recidivism_aa_df.is_recid,
    recidivism_aa_df.compas_score,
    <span class="hljs-string">'Caucasian'</span>,
    <span class="hljs-string">'African-American'</span>,
    compare_fpr=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker627"/>preceding snippet generated <em class="italic">Figure 6.2</em>. At a glance, you can tell that it’s like the confusion matrix for Caucasians has been flipped 90 degrees to form the African American confusion matrix, and even then, it is still less unfair. Pay close attention to the difference between FPs and TNs. As a Caucasian defendant, a result is more than half as likely to be an FP than a TN, but as an African American, it is a few percentage points more likely. In other words, an African American defendant who doesn’t re-offend is predicted to be at risk of recidivating more than half of the time:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_02.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 6.2: Comparison of the confusion matrices for the predicted risk of recidivism (compas_score) and the ground truth (is_recid) between African Americans and Caucasians in the dataset</p>
    <p class="normal">Instead of <a id="_idIndexMarker628"/>eyeballing it by looking at the plots, we can measure the <strong class="keyWord">False Positive Rate</strong> (<strong class="keyWord">FPR</strong>), which<a id="_idIndexMarker629"/> is the ratio between these two measures (FP / (FP + TN)) at risk of recidivism more often.</p>
    <h2 id="_idParaDest-167" class="heading-2">Data preparation</h2>
    <p class="normal">Before we <a id="_idIndexMarker630"/>move on to the modeling and interpretation, we have one last step.</p>
    <p class="normal">Since <code class="inlineCode">prepare=True</code> for the data loading, all we do now is split the data into a training and test dataset. As usual, it is critical to set your random states so that all your findings are reproducible. We will then set <code class="inlineCode">y</code> to be our target variable (<code class="inlineCode">compas_score</code>) and set <code class="inlineCode">X</code> as every other feature except for <code class="inlineCode">is_recid</code>, because this is the ground truth. Lastly, we split <code class="inlineCode">y</code> and <code class="inlineCode">X</code> into train and test datasets as we have before:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span>
np.random.seed(rand)
y = recidivism_df[<span class="hljs-string">'compas_score'</span>]
X = recidivism_df.drop(
    [<span class="hljs-string">'compas_score'</span>, <span class="hljs-string">'is_recid'</span>],
    axis=<span class="hljs-number">1</span>).copy()
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span class="hljs-number">0.2</span>, random_state=rand
)
</code></pre>
    <p class="normal">Now, let’s get started!</p>
    <h3 id="_idParaDest-168" class="heading-3">Modeling</h3>
    <p class="normal">First, let’s quickly<a id="_idIndexMarker631"/> train the model we will use throughout this chapter.</p>
    <p class="normal"><strong class="keyWord">Proxy models</strong> are<a id="_idIndexMarker632"/> a means to emulate output from a black-box model just like <strong class="keyWord">global surrogate models</strong>, which<a id="_idIndexMarker633"/> we covered in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>. So, are they the same thing? In machine learning, surrogate and proxy are terms that are often used interchangeably. However, semantically, surrogacy relates to substitution and proxy relates more to a representation. So, we call these proxy models to distinguish that we don’t have the exact training data. Therefore, you only represent the original model because you cannot substitute it. For the same reason, unlike interpretation with surrogates, which is best served by simpler models, a proxy is best suited to complex models that can make up for the difference in training data with complexity.</p>
    <p class="normal">We will<a id="_idIndexMarker634"/> train a <strong class="keyWord">CatBoost</strong> classifier. For<a id="_idIndexMarker635"/> those of you who aren’t familiar with CatBoost, it’s an efficient boosted ensemble tree method. It’s similar<a id="_idIndexMarker636"/> to <strong class="keyWord">LightGBM</strong>, except it <a id="_idIndexMarker637"/>uses a new technique called <strong class="keyWord">Minimal Variance Sampling</strong> (<strong class="keyWord">MVS</strong>) instead of <strong class="keyWord">Gradient-Based One-Side Sampling</strong> (<strong class="keyWord">GOSS</strong>). Unlike LightGBM, it <a id="_idIndexMarker638"/>grows trees in a balanced fashion. It’s called CatBoost because it can automatically encode categorical features, and it’s particularly good at tackling overfitting, with unbiased treatment of categorical features and class imbalances. We won’t go into a whole lot of detail, but it was chosen for this exercise for those reasons.</p>
    <p class="normal">As a tree-based model class, you can specify a maximum <code class="inlineCode">depth</code> value for <code class="inlineCode">CatBoostClassifier</code>. We are setting a relatively high <code class="inlineCode">learning_rate</code> value and a lower <code class="inlineCode">iterations</code> value (the default is 1,000). Once we have used <code class="inlineCode">fit</code> on the model, we can evaluate the results with <code class="inlineCode">evaluate_class_mdl</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">cb_mdl = CatBoostClassifier(
    iterations=<span class="hljs-number">500</span>,
    learning_rate=<span class="hljs-number">0.5</span>,
    depth=<span class="hljs-number">8</span>
)
fitted_cb_mdl = cb_mdl.fit(
    X_train,
    y_train,
    verbose=<span class="hljs-literal">False</span>
)
y_train_cb_pred, y_test_cb_prob, y_test_cb_pred = \
mldatasets.evaluate_class_mdl(
    fitted_cb_mdl, X_train, X_test, y_train, y_test
)
</code></pre>
    <p class="normal">You can appreciate the output of <code class="inlineCode">evaluate_class_mdl</code> for our CatBoost model in <em class="italic">Figure 6.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_03.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.3: Predictive performance of our CatBoost model</p>
    <p class="normal">From the<a id="_idIndexMarker639"/> optics of fairness, we care more about FPs than FNs because it’s more unfair to put an <em class="italic">innocent</em> person in prison than it is to leave a <em class="italic">guilty</em> person on the streets. Therefore, we should aspire to have higher <em class="italic">precision</em> than <em class="italic">recall</em>. <em class="italic">Figure 6.3</em> confirms this, as well as a healthy ROC curve, ROC-AUC, and MCC.</p>
    <p class="normal">The predictive performance for the model is reasonably accurate considering it’s a <em class="italic">proxy model</em> meant to only approximate the real thing with different, yet related, data.</p>
    <h3 id="_idParaDest-169" class="heading-3">Getting acquainted with our “instance of interest”</h3>
    <p class="normal">The <a id="_idIndexMarker640"/>journalist reached out to you with a case in mind: the African American defendant who was falsely predicted to have a high risk of recidivism. This case is #<code class="inlineCode">5231</code> and is your main <em class="italic">instance of interest</em>. Since our focus is racial bias, we’d like to compare it with similar instances but of different races. To that end, we found case #<code class="inlineCode">10127</code> (Caucasian) and #<code class="inlineCode">2726</code> (Hispanic).</p>
    <p class="normal">We can look at the data for all three. Since we will keep referring to these instances throughout this chapter, let’s first save the indexes of the African American (<code class="inlineCode">idx_aa</code>), Hispanic (<code class="inlineCode">idx_h</code>), and Caucasian (<code class="inlineCode">idx_c</code>) cases. Then, we can subset the test dataset by these indexes. Since we have to make sure that our predictions match, we will concatenate this subsetted test dataset to the true labels (<code class="inlineCode">y_test</code>) and the CatBoost predictions (<code class="inlineCode">y_test_cb_pred</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">idx_aa = <span class="hljs-number">5231</span>
idx_h = <span class="hljs-number">2726</span>
idx_c = <span class="hljs-number">10127</span>
eval_idxs = X_test.index.isin([idx_aa, idx_h, idx_c])
X_test_evals = X_test[eval_idxs]
eval_compare_df = pd.concat(
    [
        pd.DataFrame(
            {<span class="hljs-string">'y'</span>:y_test[eval_idxs]},
            index=[idx_c, idx_h, idx_aa]
        ),
        pd.DataFrame(
            {<span class="hljs-string">'y_pred'</span>:y_test_cb_pred[eval_idxs]},
            index=[idx_c, idx_h, idx_aa]
        ),
        X_test_evals
    ],
    axis=<span class="hljs-number">1</span>
).transpose()
eval_compare_df
</code></pre>
    <p class="normal">The preceding code produces the DataFrame in <em class="italic">Figure 6.4</em>. You can tell that the predictions match the true labels, and our main <em class="italic">instance of interest</em> was the only one predicted <a id="_idIndexMarker641"/>as a medium or high risk of recidivism. Besides race, the only other differences are with <code class="inlineCode">c_charge_degree</code> and one minor age difference:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_04.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.4: Observations #5231, #10127, and #2726 side by side with feature differences highlighted</p>
    <p class="normal">Throughout this chapter, we will pay close attention to these differences to see whether they played a large role in producing the prediction difference. All the methods we will cover will complete the picture of what can determine or change the proxy model’s decision, and, potentially, the COMPAS model by extension. Now that we have completed the setup, we will move forward with employing the interpretation methods.</p>
    <h1 id="_idParaDest-170" class="heading-1">Understanding anchor explanations</h1>
    <p class="normal">In <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>, we learned that <strong class="keyWord">LIME</strong> trains a local surrogate <a id="_idIndexMarker642"/>model (specifically a <strong class="keyWord">weighted sparse linear model</strong>) on a <strong class="keyWord">perturbed</strong> version of your dataset in the <strong class="keyWord">neighborhood</strong> of your <em class="italic">instance of interest</em>. The result is that you <a id="_idIndexMarker643"/>approximate a <strong class="keyWord">local decision boundary</strong> that can help you interpret the model’s prediction for it.</p>
    <p class="normal">Like LIME, <strong class="keyWord">anchors</strong> are<a id="_idIndexMarker644"/> also derived from a model-agnostic perturbation-based strategy. However, they are not about <a id="_idIndexMarker645"/>the <em class="italic">decision boundary</em> but the <strong class="keyWord">decision region</strong>. Anchors<a id="_idIndexMarker646"/> are also known <a id="_idIndexMarker647"/>as <strong class="keyWord">scoped rules</strong> because they list <a id="_idIndexMarker648"/>some <strong class="keyWord">decision rules</strong> that apply to your instance and its <em class="italic">perturbed</em> neighborhood. This neighborhood is also known as<a id="_idIndexMarker649"/> the <strong class="keyWord">perturbation space</strong>. An important detail is to what extent the rules apply<a id="_idIndexMarker650"/> to it, known as <strong class="keyWord">precision</strong>.</p>
    <p class="normal">Imagine the neighborhood around your instance. You would expect the points to have more similar predictions the closer you got to your instance, right? So, if you had decision rules that defined these predictions, the smaller the area surrounding your instance, the more precise your rules. This concept<a id="_idIndexMarker651"/> is called <strong class="keyWord">coverage</strong>, which is the percentage of your <em class="italic">perturbation space</em> that yields a specific <em class="italic">precision</em>.</p>
    <p class="normal">Unlike LIME, anchors don’t fit a local surrogate model to explain your chosen instance’s prediction. Instead, they <a id="_idIndexMarker652"/>explore possible candidate decision rules using an algorithm called <strong class="keyWord">Kullback-Leibler divergence Lower and Upper Confidence Bounds</strong> (<strong class="keyWord">KL-LUCB</strong>), which is derived from a <strong class="keyWord">Multi-Armed Bandit</strong> (<strong class="keyWord">MAB</strong>) algorithm.</p>
    <p class="normal">MABs are<a id="_idIndexMarker653"/> a family of <em class="italic">reinforcement learning algorithms</em> about maximizing the payoff when you have limited resources to explore all unknown possibilities. The algorithm originated from understanding how casino slot machine players could maximize their payoff by playing multiple machines. It’s called multi-armed bandit because slot machine players are known as one-armed bandits. Yet players don’t know which machine will yield the highest payoff, can’t try all of them at once, and have finite funds. The trick is to learn how to balance exploration (trying unknown slot machines) with exploitation (using those you already have reasons to prefer).</p>
    <p class="normal">In the anchors’ case, each slot machine is a potential decision rule, and the payoff is how much precision it yields. The KL-LUCB algorithm uses confidence regions based on the <strong class="keyWord">Kullback-Leibler divergence</strong> between <a id="_idIndexMarker654"/>the distributions to find the decision rule with the highest precision sequentially, yet efficiently.</p>
    <h2 id="_idParaDest-171" class="heading-2">Preparations for anchor and counterfactual explanations with alibi</h2>
    <p class="normal">Several <a id="_idIndexMarker655"/>small steps need to be performed to <a id="_idIndexMarker656"/>help the <code class="inlineCode">alibi</code> library produce human-friendly explanations. The first one pertains to the prediction since the model may output a 1 or 0, but it’s easier to understand a prediction by its name. To help us with this, we need a list with the class names where the 0 position matches our negative class name and 1 matches the positive one:</p>
    <pre class="programlisting code"><code class="hljs-code">class_names = [<span class="hljs-string">'Low Risk'</span>, <span class="hljs-string">'Medium/High Risk'</span>]
</code></pre>
    <p class="normal">Next, let’s create a <code class="inlineCode">numpy</code> array with our main <em class="italic">instance of interest</em> and print it out. Please note that the single-dimension array needs to be expanded (<code class="inlineCode">np.expand_dims</code>) so that it’s understood by <code class="inlineCode">alibi</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">X_test_eval = np.expand_dims(
    X_test.values[
        X_test.index.get_loc(idx_aa)
    ],
     axis=<span class="hljs-number">0</span>
)
<span class="hljs-built_in">print</span>(X_test_eval)
</code></pre>
    <p class="normal">The preceding code <a id="_idIndexMarker657"/>outputs an array with the 21 features, of which 12 were the result of <strong class="keyWord">One-Hot Encoding</strong> (<strong class="keyWord">OHE</strong>):</p>
    <pre class="programlisting con"><code class="hljs-con">[[23  0  0  0  2  0  1  1  0  ... 0  1  0  0  0  0]]
</code></pre>
    <p class="normal">A problem with making human-friendly explanations arises when you have OHE categories. To both the machine learning model and the explainer, each OHE feature is separate from the others. Still, to the human interpreting the outcomes, they cluster together as categories of their original features.</p>
    <p class="normal">The <code class="inlineCode">alibi</code> library has several utility functions to deal with this problem, such as <code class="inlineCode">ohe_to_ord</code>, which takes a one-hot-encoded instance and puts it in an ordinal format. To use this function, we first define a dictionary <code class="inlineCode">(cat_vars_ohe</code>) that tells <code class="inlineCode">alibi</code> where the categorical <a id="_idIndexMarker658"/>variables are in our features and how <a id="_idIndexMarker659"/>many categories each one has. For instance, in our data, gender starts at the 5<sup class="superscript">th</sup> index and has two categories, which is why our <code class="inlineCode">cat_vars_ohe</code> dictionary begins with <code class="inlineCode">5: 2</code>. Once you have this dictionary, <code class="inlineCode">ohe_to_ord</code> can take your instance (<code class="inlineCode">X_test_eval</code>) and output it in ordinal format, where each categorical variable takes up a single feature. This utility function will prove useful for Alibi’s counterfactual explanations, where the explainer will need this dictionary to map categorical features together:</p>
    <pre class="programlisting code"><code class="hljs-code">cat_vars_ohe = {<span class="hljs-number">5</span>: <span class="hljs-number">2</span>, <span class="hljs-number">7</span>: <span class="hljs-number">6</span>, <span class="hljs-number">13</span>: <span class="hljs-number">8</span>}
<span class="hljs-built_in">print</span>(ohe_to_ord(X_test_eval, cat_vars_ohe)[<span class="hljs-number">0</span>])
</code></pre>
    <p class="normal">The preceding code outputs the following array:</p>
    <pre class="programlisting con"><code class="hljs-con">[[23  0  0  0  2  1  0  3]]
</code></pre>
    <p class="normal">For when it’s in ordinal format, Alibi will need a dictionary that provides names for each category and a list of feature names:</p>
    <pre class="programlisting code"><code class="hljs-code">category_map = {
    <span class="hljs-number">5</span>: [<span class="hljs-string">'Female'</span>, <span class="hljs-string">'Male'</span>],
    <span class="hljs-number">6</span>: [
        <span class="hljs-string">'African-American'</span>,
        <span class="hljs-string">'Asian'</span>,
        <span class="hljs-string">'</span><span class="hljs-string">Caucasian'</span>,
        <span class="hljs-string">'Hispanic'</span>,
        <span class="hljs-string">'Native American'</span>,
        <span class="hljs-string">'Other'</span>
    ],
    <span class="hljs-number">7</span>: [
        <span class="hljs-string">'Felony 1st Degree'</span>,
        <span class="hljs-string">'Felony 2nd Degree'</span>,
        <span class="hljs-string">'Felony 3rd Degree'</span>,
        <span class="hljs-string">'Felony 7th Degree'</span>,
        <span class="hljs-string">'Misdemeanor 1st Degree'</span>,
        <span class="hljs-string">'</span><span class="hljs-string">Misdemeanor 2nd Degree'</span>,
        <span class="hljs-string">'Misdemeanor 3rd Degree'</span>,
        <span class="hljs-string">'Other Charge Degree'</span>
    ]
}
feature_names = [
    <span class="hljs-string">'age'</span>,
    <span class="hljs-string">'juv_fel_count'</span>,
    <span class="hljs-string">'juv_misd_count'</span>,
    <span class="hljs-string">'juv_other_count'</span>,
    <span class="hljs-string">'priors_count'</span>,
    <span class="hljs-string">'sex'</span>,
    <span class="hljs-string">'race'</span>,
    <span class="hljs-string">'</span><span class="hljs-string">c_charge_degree'</span>
]
</code></pre>
    <p class="normal">However, Alibi’s <a id="_idIndexMarker660"/>anchor explanations use the data as it is provided to our models. We are using OHE data, so we need a category map for that format. Of course, the OHE features are<a id="_idIndexMarker661"/> all binary, so they only have two “categories” each:</p>
    <pre class="programlisting code"><code class="hljs-code">category_map_ohe = {
    <span class="hljs-number">5</span>: [<span class="hljs-string">'Not Female'</span>, <span class="hljs-string">'Female'</span>],
    <span class="hljs-number">6</span>: [<span class="hljs-string">'Not Male'</span>, <span class="hljs-string">'Male'</span>],
    <span class="hljs-number">7</span>:[<span class="hljs-string">'Not African American'</span>, <span class="hljs-string">'</span><span class="hljs-string">African American'</span>],
    <span class="hljs-number">8</span>:[<span class="hljs-string">'Not Asian'</span>, <span class="hljs-string">'Asian'</span>], <span class="hljs-number">9</span>:[<span class="hljs-string">'Not Caucasian'</span>, <span class="hljs-string">'Caucasian'</span>],
    <span class="hljs-number">10</span>:[<span class="hljs-string">'Not Hispanic'</span>, <span class="hljs-string">'Hispanic'</span>],
    <span class="hljs-number">11</span>:[<span class="hljs-string">'Not Native American'</span>, <span class="hljs-string">'Native American'</span>],
    <span class="hljs-number">12</span>:[<span class="hljs-string">'Not Other Race'</span>, <span class="hljs-string">'Other Race'</span>],
    <span class="hljs-number">19</span>:[<span class="hljs-string">'Not Misdemeanor 3rd Deg'</span>, <span class="hljs-string">'Misdemeanor 3rd Deg'</span>],
    <span class="hljs-number">20</span>:[<span class="hljs-string">'</span><span class="hljs-string">Not Other Charge Degree'</span>, <span class="hljs-string">'Other Charge Degree'</span>]
}
</code></pre>
    <h2 id="_idParaDest-172" class="heading-2">Local interpretations for anchor explanations</h2>
    <p class="normal">All Alibi explainers <a id="_idIndexMarker662"/>require a <code class="inlineCode">predict</code> function, so we create a <code class="inlineCode">lambda</code> function called <code class="inlineCode">predict_cb_fn</code> for our CatBoost model. Please note that we are using <code class="inlineCode">predict_proba</code> for the classifier’s probabilities. Then, to initialize <code class="inlineCode">AnchorTabular</code>, we also provide it with our features’ names as they are in our OHE dataset and the category map (<code class="inlineCode">category_map_ohe</code>). Once it has initialized, we fit it with our training data:</p>
    <pre class="programlisting code"><code class="hljs-code">predict_cb_fn = <span class="hljs-keyword">lambda</span> x: fitted_cb_mdl.predict_proba(x)
anchor_cb_explainer = AnchorTabular(
    predict_cb_fn,
    X_train.columns,
    categorical_names=category_map_ohe
)
anchor_cb_explainer.fit(X_train.values)
</code></pre>
    <p class="normal">Before we leverage the explainer, it’s good practice to check that the anchor “holds.” In other words, we should check that the MAB algorithm found decision rules that help explain the prediction. To verify this, you use the <code class="inlineCode">predictor</code> function to check that the prediction is the same as the one you expect for this instance. Right now, we are using <code class="inlineCode">idx_aa</code>, which is the case of the African American defendant:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(
    <span class="hljs-string">'Prediction: %s'</span> %  class_names[anchor_cb_explainer.
    predictor(X_test.loc[idx_aa].values)[<span class="hljs-number">0</span>]]
)
</code></pre>
    <p class="normal">The preceding code outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Prediction: Medium/High Risk
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker663"/>proceed to use the <code class="inlineCode">explain</code> function to generate an explanation for our instance. We can set our precision threshold to <code class="inlineCode">0.85</code>, which means we expect the predictions on anchored observations to be the same as our instance at least 85% of the time. Once we have an explanation, we can print the anchors as well as their precision and coverage:</p>
    <pre class="programlisting code"><code class="hljs-code">anchor_cb_explanation = anchor_cb_explainer.explain(
    X_test.loc[idx_aa].values,threshold=<span class="hljs-number">0.85</span>, seed=rand
)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Anchor: %s'</span> % (<span class="hljs-string">' AND'</span>.join(anchor_cb_explanation.anchor)))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Precision: %.3f'</span> % anchor_cb_explanation.precision)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Coverage: %.3f'</span> % anchor_cb_explanation.coverage)
</code></pre>
    <p class="normal">The following output was generated by the preceding code. You can tell that <code class="inlineCode">age</code>, <code class="inlineCode">priors_count</code>, and <code class="inlineCode">race_African-American</code> are factors at 86% precision. Impressively, this rule applies to almost a third of all the perturbation space’s instances with a coverage of 0.29:</p>
    <pre class="programlisting con"><code class="hljs-con">Anchor: age &lt;= 25.00 AND
    priors_count &gt; 0.00 AND
    race_African-American = African American
Precision: 0.863
Coverage: 0.290
</code></pre>
    <p class="normal">We can try the same code but with a 5% bump in the precision threshold set to 0.9. We observe the same three anchors that were generated from the previous example with three additional anchors:</p>
    <pre class="programlisting con"><code class="hljs-con">Anchor: age &lt;= 25.00 AND
    priors_count &gt; 0.00 AND
    race_African-American = African American AND
    c_charge_degree_(M1) = Not Misdemeanor 1st Deg AND
    c_charge_degree_(F3) = Not Felony 3rd Level AND
    race_Caucasian = Not Caucasian
Precision: 0.903
Coverage: 0.290
</code></pre>
    <p class="normal">Interestingly enough, although precision did increase by a few percentage points, coverage stayed the same. At this level of precision, we may confirm that race is a significant <a id="_idIndexMarker664"/>factor because being African American is an anchor, but so is not being Caucasian. Another factor was <code class="inlineCode">c_charge_degree</code>. The explanation reveals that being accused of a first-degree misdemeanor or third-level felony would have been better. Understandably, a seventh-level felony is a more serious charge than these two.</p>
    <p class="normal">Another way of understanding why a model made a specific prediction is by looking for a similar datapoint that had the opposite prediction and examining why the alternative decision was made. The decision boundary crosses between both points, so it’s helpful to contrast decision explanations from both sides of the boundary. This time, we will use <code class="inlineCode">idx_c</code>, which is the case for the Caucasian defendant with a threshold of 85% and which outputs the anchors as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Anchor: priors_count &lt;= 2.00 AND
    race_African-American = Not African American AND
    c_charge_degree_(M1) = Misdemeanor 1st Deg
Precision: 0.891 
Coverage: 0.578
</code></pre>
    <p class="normal">The first anchor is <code class="inlineCode">priors_count &lt;= 2.00</code>, but on the other side of the boundary, the first two anchors were <code class="inlineCode">age &lt;= 25.00</code> and <code class="inlineCode">priors_count &gt; 0.00</code>. In other words, for an African American under or equal to the age of 25, any number of priors is enough to categorize them as having a medium/high risk of recidivism (86% of the time). On the other hand, a Caucasian person will be predicted as low risk if the priors don’t exceed two and they were not accused of a first-degree misdemeanor (89% of the time and with 58% coverage). These decision rules suggest a biased decision based on race with different standards applied to different racial groups. A double standard is when different rules are applied when, in principle, the situation is the same. In this case, the different rules for <code class="inlineCode">priors_count</code> and the absence of <code class="inlineCode">age</code> as a factor for Caucasians constitute double standards.</p>
    <p class="normal">We can now try a Hispanic defendant (<code class="inlineCode">idx_h</code>) to observe whether double standards are also found in this instance. We just run the same code as before but replace <code class="inlineCode">idx_c</code> with <code class="inlineCode">idx_h</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">Anchor: priors_count &lt;= 2.00 AND 
    race_African-American = Not African American AND
    juv_fel_count &lt;= 0.00 AND 
    sex_Male = Male
Precision: 0.851
Coverage: 0.578
</code></pre>
    <p class="normal">The<a id="_idIndexMarker665"/> explanations for the Hispanic defendant confirm the different standard with <code class="inlineCode">priors_count</code> and that <code class="inlineCode">race</code> continues to be a strong factor since there’s one anchor for not being African American and another one for being Hispanic.</p>
    <p class="normal">For specific model decisions, anchor explanations answer the question <em class="italic">why?</em> However, by comparing similar instances that are only slightly different but have different predictions, we have explored the question <em class="italic">what if?</em> In the next section, we will expand on this question further.</p>
    <h1 id="_idParaDest-173" class="heading-1">Exploring counterfactual explanations</h1>
    <p class="normal"><strong class="keyWord">Counterfactuals</strong> are<a id="_idIndexMarker666"/> an integral part of human reasoning. How many of us have muttered the words “If I had done <em class="italic">X</em> instead, my outcome <em class="italic">y</em> would have been different”? There’s always one or two things that, if done differently, could lead to the outcomes we prefer!</p>
    <p class="normal">In <a id="_idIndexMarker667"/>machine learning outcomes, you can leverage this way of reasoning to make for extremely human-friendly explanations where we can explain decisions in terms of what would need to change to get the opposite outcome (the <strong class="keyWord">counterfactual class</strong>). After all, we are often interested in knowing how to make a negative outcome better. For instance, how do you get your denied loan application approved or decrease your risk of cardiovascular disease from high to low? However, hopefully, answers to those questions aren’t a huge list of changes. You prefer the smallest number of changes required to change your outcome.</p>
    <p class="normal">Regarding fairness, counterfactuals are an important interpretation method, in particular when there are elements involved that <em class="italic">we can’t change</em> or shouldn’t have to change. For instance, if you perform exactly the same job and have the same level of experience as your co-worker, you expect to have the same salary, right? If you and your spouse share the same assets and credit history but have different credit scores, you have to wonder why. Does it have to do with gender, race, age, or even political affiliations? Whether it’s a compensation, credit rating, or recidivism risk model, you’d hope that similar people were treated similarly.</p>
    <p class="normal">Finding <a id="_idIndexMarker668"/>counterfactuals is not particularly hard. All we have to do is change our <em class="italic">instance of interest</em> slightly until it changes the outcome. And maybe there’s an instance already in the dataset just like that!</p>
    <p class="normal">In fact, you could say that the three instances we examined with anchors in the previous section are close enough to be counterfactuals of each other, except for the Caucasian and Hispanic cases, which have the same outcome. But the Caucasian and Hispanic instances were “<em class="italic">cherry-picked</em>” by looking for datapoints with the same criminal history but different races to the <em class="italic">instance of interest</em>. Perhaps by comparing similar points, mostly except for race, we limited the scope in such a way that we confirmed what we hoped to confirm, which is that race matters to the model’s decision-making.</p>
    <p class="normal">This is an example <a id="_idIndexMarker669"/>of <em class="italic">selection bias</em>. After all, counterfactuals are inherently selective because they focus on a few feature changes. And even with a few features, there are so many possible permutations that change the outcome, which means that a single point could have hundreds of counterfactuals. And not all of these will tell a consistent story. This phenomenon is called<a id="_idIndexMarker670"/> the <strong class="keyWord">Rashomon effect</strong>. It is named after a famous Japanese movie about a murder mystery. And as we have come to expect from murder mysteries, witnesses have different recollections of what happened. But in the same way that it’s difficult to rely on a single witness, you cannot rely on a single counterfactual. Also, in the same way that great detectives are trained to look for clues everywhere in connection to the scene of a crime (even if it contradicts their instincts), counterfactuals shouldn’t be “cherry-picked” because they conveniently tell the story we want them to tell.</p>
    <p class="normal">Fortunately, there are algorithmic ways of looking for counterfactual instances in an unbiased manner. Typically, these involve finding the closest points with different outcomes, but there are different ways of measuring the distance between points. For starters, there’s the <strong class="keyWord">L1</strong> distance (also known as the <strong class="keyWord">Manhattan distance</strong>) and <strong class="keyWord">L2</strong> distance (also known as the <strong class="keyWord">Euclidean distance</strong>), among many others. But there’s also the question of normalizing the distances because not all features have the same scale. Otherwise, they would be biased against features with smaller scales, such as one-hot-encoded features. There are many normalization schemes to choose from too. You could use <strong class="keyWord">standard deviation</strong>, <strong class="keyWord">min-max scaling</strong>, or even <strong class="keyWord">median absolute deviation</strong> [9].</p>
    <p class="normal">In this section, we will explain and use one advanced counterfactual finding method. Then, we will explore Google’s <strong class="keyWord">What If Tool</strong> (<strong class="keyWord">WIT</strong>). It has a simple L1- and L2-based counterfactual finder, which is limited to the dataset but makes up for it with other useful interpretation features.</p>
    <h2 id="_idParaDest-174" class="heading-2">Counterfactual explanations guided by prototypes</h2>
    <p class="normal">The most sophisticated counterfactual finding algorithms do the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Loss</strong>: These leverage a <em class="italic">loss function</em> that helps us optimize finding the counterfactuals closest to our <em class="italic">instance of interest</em>.</li>
      <li class="bulletList"><strong class="keyWord">Perturbation</strong>: These tend to operate with a <em class="italic">perturbation space</em> much like anchors do, changing as few features as possible. Please note that counterfactuals don’t have to be real points in your dataset. That would be far too limiting. Counterfactuals exist in the realm of the possible, not of the necessarily known.</li>
      <li class="bulletList"><strong class="keyWord">Distribution</strong>: However, counterfactuals have to be realistic, and therefore, interpretable. For example, a loss function could help determine that <code class="inlineCode">age &lt; 0</code> alone is enough to make any medium-/high-risk instance low-risk. This is why counterfactuals should lie close to the statistical distributions of your data, especially <em class="italic">class-specific distributions</em>. They also should not be biased against smaller-scale features, namely categorical variables.</li>
      <li class="bulletList"><strong class="keyWord">Speed</strong>: These run fast enough to be useful in real-world scenarios.</li>
    </ul>
    <p class="normal">Alibi’s <strong class="keyWord">Counterfactuals Guided by Prototypes</strong> (<code class="inlineCode">CounterFactualProto</code>) has all these properties. It has a loss function that includes both L1 (<em class="italic">Lasso</em>) and L2 (<em class="italic">Ridge</em>) regularization as a linear combination, just like <strong class="keyWord">Naïve Elastic Net</strong> does <img src="../Images/B18406_06_001.png" alt="" role="presentation"/> but with a weight (<img src="../Images/B18406_03_014.png" alt="" role="presentation"/>) only on the L1 term. The clever part of this algorithm is that it can (optionally) use an <em class="italic">autoencoder</em> to understand the distributions. We will leverage one in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>. However, what’s important to note here is that autoencoders, in general, are neural networks that learn a compressed representation of your training data. This method incorporates loss terms from the autoencoder, such as one for the nearest prototype. A prototype is the dimensionality-reduced representation of the counterfactual class.</p>
    <p class="normal">If an autoencoder is not available, the algorithm uses a tree often used for multidimensional search (<em class="italic">k-d trees</em>) instead. With this tree, the algorithm can efficiently capture the class distributions and choose the nearest prototype. Once it has the prototype, the perturbations are guided by it. Incorporating a prototype loss term in the loss function ensures that the resulting perturbations will be close enough to the prototype that is in distribution for the counterfactual class. Many modeling classes and interpretation methods overlook the importance of treating continuous and categorical features differently. </p>
    <p class="normal"><code class="inlineCode">CounterFactualProto</code> can use two different distance metrics to compute the pairwise distances between categories of a categorical variable: <strong class="keyWord">Modified Value Difference Metric</strong> (<strong class="keyWord">MVDM</strong>) and <strong class="keyWord">Association-Based Distance Metric</strong> (<strong class="keyWord">ABDM</strong>) and can even combine both. Another way in which <code class="inlineCode">CounterFactualProto</code> ensures meaningful counterfactuals is by limiting permutated features to predefined ranges. We can use the minimum and maximum values of features to generate a tuple of arrays (<code class="inlineCode">feature_range</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">feature_range = (
    X_train.values.<span class="hljs-built_in">min</span>(axis=<span class="hljs-number">0</span>).reshape(<span class="hljs-number">1</span>,<span class="hljs-number">21</span>).astype(np.float32),\
    X_train.values.<span class="hljs-built_in">max</span>(axis=<span class="hljs-number">0</span>).reshape(<span class="hljs-number">1</span>,<span class="hljs-number">21</span>).astype(np.float32)
)
<span class="hljs-built_in">print</span>(feature_range)
</code></pre>
    <p class="normal">The preceding code outputs two arrays – the first one with the minimum and the second with the maximum of all features:</p>
    <pre class="programlisting con"><code class="hljs-con">(array([[18.,  0.,  ... , 0.,  0.,  0.]], dtype=float32), array([[96., 20., ... ,  1.,  1.,  1.]], dtype=float32))
</code></pre>
    <p class="normal">We can now instantiate an explainer with <code class="inlineCode">CounterFactualProto</code>. As arguments, it requires the model’s <code class="inlineCode">predict</code> function (<code class="inlineCode">predict_cb_fn</code>), the shape of the instance you want to explain (<code class="inlineCode">X_test_eval.shape</code>), the maximum amount of optimization iterations to perform (<code class="inlineCode">max_iterations</code>), and the feature range for perturbed instances (<code class="inlineCode">feature_range</code>). Many hyperparameters can be chosen, including the <img src="../Images/B18406_03_014.png" alt="" role="presentation"/> weight to apply to the L1 loss (<code class="inlineCode">beta</code>) and the θ weight to apply to the prototype loss (<code class="inlineCode">theta</code>). Also, you must specify whether to use the <em class="italic">k-d tree</em> or not (<code class="inlineCode">use_kdtree</code>) when the autoencoder model isn’t provided. Once the explainer is instantiated, you fit it to the test dataset. We are specifying the distance metric for categorical features (<code class="inlineCode">d_type</code>) as the combination of ABDM and MVDM:</p>
    <pre class="programlisting code"><code class="hljs-code">cf_cb_explainer = CounterFactualProto(
    predict_cb_fn,
    c_init=<span class="hljs-number">1</span>,
    X_test_eval.shape,
    max_iterations=<span class="hljs-number">500</span>,
    feature_range=feature_range,
    beta=<span class="hljs-number">.01</span>,
    theta=<span class="hljs-number">5</span>,
    use_kdtree=<span class="hljs-literal">True</span>
)
cf_cb_explainer.fit(X_test.values, d_type=<span class="hljs-string">'abdm-mvdm'</span>)
</code></pre>
    <p class="normal">Creating an explanation with an explainer is similar to how it was with anchors. Just pass the instance (<code class="inlineCode">X_test_eval</code>) to the <code class="inlineCode">explain</code> function. However, outputting the results is not as straightforward, mainly because of the features converting between one-hot-encoded and ordinal, and interactions among the features. The documentation for Alibi (<a href="https://docs.seldon.io/projects/alibi/"><span class="url">https://docs.seldon.io/projects/alibi/</span></a>) has a detailed example of how this is done. </p>
    <p class="normal">We will instead use a utility function called <code class="inlineCode">describe_cf_instance</code> that does this for us using the <em class="italic">instance of interest</em> (<code class="inlineCode">X_test_eval</code>), explanation (<code class="inlineCode">cf_cb_explanation</code>), class names (<code class="inlineCode">class_names</code>), one-hot-encoded category locations (<code class="inlineCode">cat_vars_ohe</code>), category map (<code class="inlineCode">category_map</code>), and feature names (<code class="inlineCode">feature_names</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">cf_cb_explanation = cf_cb_explainer.explain(X_test_eval) mldatasets.describe_cf_instance(
    X_test_eval,
    cf_cb_explanation,
    class_names,
    cat_vars_ohe,
    category_map,
    feature_names
)
</code></pre>
    <p class="normal">The following output was produced by the preceding code:</p>
    <pre class="programlisting con"><code class="hljs-con">Instance Outcomes and Probabilities
-----------------------------------------------
       original:  Medium/High Risk
                  [0.46732193 0.53267807]
counterfactual:  Low Risk
                  [0.50025815 0.49974185]
Categorical Feature Counterfactual Perturbations
------------------------------------------------
                sex:  Male  --&gt;  Female
               race:  African-American  --&gt;  Asian
    c_charge_degree:  Felony 7th Degree  --&gt;  Felony 1st Degree
Numerical Feature Counterfactual Perturbations
------------------------------------------------
       priors_count:  2.00  --&gt;  1.90
</code></pre>
    <p class="normal">You can appreciate from the output that the <em class="italic">instance of interest</em> (“original”) has a 53.26% probability of being <em class="italic">Medium/High Risk</em>, but the counterfactual is barely on the <em class="italic">Low Risk</em> side with 50.03%! A counterfactual that is slightly on the other side is what we would like to see because that likely means that it is as close as possible to our <em class="italic">instance of interest</em>. There are four feature differences between them, three of which are categorical (<code class="inlineCode">sex</code>, <code class="inlineCode">race</code>, and <code class="inlineCode">c_charge_degree</code>). The fourth difference is the <code class="inlineCode">priors_count</code> numerical feature, which is treated as continuous since the explainer doesn’t know it’s discrete. In any case, the relationship should be <em class="italic">monotonic</em>, meaning an increase in one variable is consistent with a decrease or increase in the other. In this case, fewer priors should always mean lower risk, which means we can interpret the 1.90 as a 1 because if 0.1 fewer priors helped reduce the risk, a whole prior should also do so.</p>
    <p class="normal">A more powerful insight derived from <code class="inlineCode">CounterFactualProto</code>'s output is that two demographic features were present in the closest counterfactual to this feature. One was found with a method that is designed to follow our classes’ statistical distributions and isn’t biased against or in favor of specific types of features. And even though it is surprising to see Asian females in our counterfactual because it doesn’t fit the narrative that Caucasian males are getting preferential treatment, it is concerning to realize that <code class="inlineCode">race</code> appears in the counterfactual at all.</p>
    <p class="normal">The Alibi library has several counterfactual finding methods, including one that leverages reinforcement learning. Alibi also uses <em class="italic">k-d trees</em> for its trust score, which I highly recommend as well! The trust score measures the agreement between any classifier and a modified nearest neighbors classifier. The reasoning behind this is that a model’s predictions should be consistent on a local level to be trustworthy. In other words, if you and your neighbor are almost the same in every way, why would you be treated differently?</p>
    <h2 id="_idParaDest-175" class="heading-2">Counterfactual instances and much more with WIT</h2>
    <p class="normal">Google’s WIT is a very versatile tool. It requires very little input or preparation and opens up in your Jupyter or Colab notebook as an interactive dashboard with three tabs:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Datapoint editor</strong>: To visualize your datapoints, edit them, and explain their predictions.</li>
      <li class="bulletList"><strong class="keyWord">Performance</strong>: To see high-level model performance metrics (for all regression and classification models). For binary classification, this tab is called <strong class="keyWord">Performance and Fairness</strong> because, in addition to high-level metrics, predictive fairness can be compared between your dataset’s feature-based slices.</li>
      <li class="bulletList"><strong class="keyWord">Features</strong>: To view general feature statistics.</li>
    </ul>
    <p class="normal">Given that the <strong class="keyWord">Features</strong> tab doesn’t relate to model interpretations, we will explore only the first two in this section.</p>
    <h3 id="_idParaDest-176" class="heading-3">Configuring WIT</h3>
    <p class="normal">Optionally, we can enrich our interpretations in WIT by creating attributions, which are values that explain how much each feature contributes to each prediction. You could use any method to generate attributions, but we will use SHAP. We covered SHAP first in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>. Since we will interpret our CatBoost model in the WIT dashboard, the SHAP explainer that is most suitable is <code class="inlineCode">TreeExplainer</code>, but <code class="inlineCode">DeepExplainer</code> would work for the neural network (and <code class="inlineCode">KernelExplainer</code> for both). To initialize <code class="inlineCode">TreeExplainer</code>, we need to pass the fitted model (<code class="inlineCode">fitted_cb_mdl</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">shap_cb_explainer = shap.TreeExplainer(fitted_cb_mdl)
</code></pre>
    <p class="normal">WIT requires all the features in the dataset (including the labels). We will use the test dataset, so you could concatenate <code class="inlineCode">X_test</code> and <code class="inlineCode">y_test</code>, but even those two exclude the ground truth feature (<code class="inlineCode">is_recid</code>). One way of getting all of them is to subset <code class="inlineCode">recidivism_df</code> with the test dataset indexes (<code class="inlineCode">y_test.index</code>). WIT also needs our data and columns in list format so we can save them as variables for later use (<code class="inlineCode">test_np</code> and <code class="inlineCode">cols_l</code>). </p>
    <p class="normal">Lastly, for predictions and attributions, we will need to remove our ground truth (<code class="inlineCode">is_recid</code>) and classification label (<code class="inlineCode">compas_score</code>), so let’s save the index of these columns (<code class="inlineCode">delcol_idx</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">test_df = recidivism_df.loc[y_test.index]
test_np = test_df.values
cols_l = test_df.columns
delcol_idx = [
    cols_l.get_loc(<span class="hljs-string">"is_recid"</span>),
    cols_l.get_loc(<span class="hljs-string">"compas_score"</span>)
]
</code></pre>
    <p class="normal">WIT has several useful functions for customizing the dashboard, such as setting a custom distance metric (<code class="inlineCode">set_custom_distance_fn</code>), displaying class names instead of numbers (<code class="inlineCode">set_label_vocab</code>), setting a custom <code class="inlineCode">predict</code> function (<code class="inlineCode">set_custom_predict_fn</code>), and a second <code class="inlineCode">predict</code> function to compare two models (<code class="inlineCode">compare_custom_predict_fn</code>).</p>
    <p class="normal">In addition to <code class="inlineCode">set_label_vocab</code>, we are going to only use a custom <code class="inlineCode">predict</code> function (<code class="inlineCode">custom_predict_with_shap</code>). All it needs to function is to take an array with your <code class="inlineCode">examples_np</code> dataset and produce some predictions (<code class="inlineCode">preds</code>). However, we first must remove features that we want in the dashboard but weren’t used for the training (<code class="inlineCode">delcol_idx</code>). This function’s output is a dictionary with the predictions stored in a <code class="inlineCode">predictions</code> key. But we’d also like some attributions, which is why we need an <code class="inlineCode">attributions</code> key in that dictionary. Therefore, we take our SHAP explainer and generate <code class="inlineCode">shap_values</code>, which is a <code class="inlineCode">numpy</code> array. However, attributions need to be a list of dictionaries to be understood by the WIT dashboard. To this end, we iterate <code class="inlineCode">shap_output</code> and convert each observation’s SHAP values array into a dictionary (<code class="inlineCode">attrs</code>) and then append this to a list (<code class="inlineCode">attributions</code>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">custom_predict_with_shap</span>(<span class="hljs-params">examples_np</span>):
    <span class="hljs-comment">#For shap values, we only need the same features</span>
    <span class="hljs-comment">#that were used for training</span>
    inputs_np = np.delete(np.array(examples_np),delcol_idx,axis=<span class="hljs-number">1</span>)
    <span class="hljs-comment">#Get the model's class predictions</span>
    preds = predict_cb_fn(inputs_np)
    <span class="hljs-comment">#With test data, generate SHAP values which converted</span>
    <span class="hljs-comment">#to a list of dictionaries format</span>
    keepcols_l = [c <span class="hljs-keyword">for</span> i, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(cols_l)\
                  <span class="hljs-keyword">if</span> i <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> delcol_idx]
    shap_output = shap_cb_explainer.shap_values(inputs_np)
    attributions = []
    <span class="hljs-keyword">for</span> shap <span class="hljs-keyword">in</span> shap_output:
        attrs = {}
        <span class="hljs-keyword">for</span> i, col <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(keepcols_l):
            attrs[col] = shap[i]
        attributions.append(attrs)  
    <span class="hljs-comment">#Prediction function must output predictions/attributions</span>
    <span class="hljs-comment">#in dictionary</span>
    output = {<span class="hljs-string">'predictions'</span>: preds, <span class="hljs-string">'attributions'</span>: attributions} 
    <span class="hljs-keyword">return</span> output
</code></pre>
    <p class="normal">Before we build the WIT dashboard, it’s important to note that to find our <em class="italic">instance of interest</em> in the dashboard, we need to know its position within the <code class="inlineCode">numpy</code> array provided to WIT because these don’t have indexes as <code class="inlineCode">pandas</code> DataFrames do. To find the position, all we need to do is provide the <code class="inlineCode">get_loc</code> function with the index:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(y_test.index.get_loc(<span class="hljs-number">5231</span>))
</code></pre>
    <p class="normal">The preceding code outputs as <code class="inlineCode">2910</code>, so we can take note of this number. Building the WIT dashboard is fairly straightforward now. We first initialize a config (<code class="inlineCode">WitConfigBuilder</code>) with our test dataset in <code class="inlineCode">numpy</code> format (<code class="inlineCode">test_np</code>) and our list of features (<code class="inlineCode">cols_l</code>). Both are converted to lists with <code class="inlineCode">tolist()</code>. Then, we set our custom <code class="inlineCode">predict</code> function with <code class="inlineCode">set_custom_predict_fn</code> and our target feature (<code class="inlineCode">is_recid</code>) and provide our class names. We will use the ground truth this time to evaluate fairness from the perspective of what really happened. Once the config is initialized, the widget (<code class="inlineCode">WitWidget</code>) builds the dashboard with it. You can optionally provide a height (the default is 1,000 pixels):</p>
    <pre class="programlisting code"><code class="hljs-code">wit_config_builder = WitConfigBuilder(
    test_np.tolist(),
    feature_names=cols_l.tolist()
).set_custom_predict_fn(custom_predict_with_shap
).set_target_feature(<span class="hljs-string">"is_recid"</span>).set_label_vocab(class_names)
WitWidget(wit_config_builder, height=<span class="hljs-number">800</span>)
</code></pre>
    <h3 id="_idParaDest-177" class="heading-3">Datapoint editor</h3>
    <p class="normal">In <em class="italic">Figure 6.5</em>, you can see the WIT dashboard with its three tabs. We will first explore the first tab (<strong class="screenText">Datapoint editor</strong>). It has <strong class="screenText">Visualize</strong> and <strong class="screenText">Edit</strong> panes on the left, and on the right, it can show you either <strong class="screenText">Datapoints</strong> or <strong class="screenText">Partial dependence plots</strong>. When you have <strong class="screenText">Datapoints</strong> selected, you can visualize the datapoints in many ways using the controls in the upper right (the highlighted area <em class="italic">A</em>). What we have done in <em class="italic">Figure 6.5</em> is set the following:</p>
    <ul>
      <li class="bulletList"><strong class="screenText">Binning</strong> | <strong class="screenText">X-axis</strong>: <code class="inlineCode">c_charge_degree_(F7)</code></li>
      <li class="bulletList"><strong class="screenText">Binning</strong> | <strong class="screenText">Y-axis</strong>: <code class="inlineCode">compas_score</code></li>
      <li class="bulletList"><strong class="screenText">Color By</strong>: <code class="inlineCode">race_African-American</code></li>
    </ul>
    <p class="normal">Everything else stays the same.</p>
    <p class="normal">These settings resulted in all our datapoints being neatly organized into two rows and two columns and color-coded by African American or not. The right column is for those with a level 7 charge degree, and the upper row is for those with a <em class="italic">Medium/High Risk</em> COMPAS score. We can look for datapoint <code class="inlineCode">2910</code> in this subgroup (<em class="italic">B</em>) by clicking on the top-rightmost item. It should appear in the <strong class="screenText">Edit</strong> pane (<em class="italic">C</em>). Interestingly enough, the SHAP attributions for this datapoint are three times higher for <code class="inlineCode">age</code> than they are for <code class="inlineCode">race_African-American</code>. But still, race altogether is second to age in importance. Also, notice that in the <strong class="screenText">Infer</strong> pane, you see the predicted probability for <em class="italic">Medium/High Risk</em> is approximately 83%:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_05.png" alt="Graphical user interface, treemap chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.5: WIT dashboard with our instance of interest</p>
    <p class="normal">WIT can find the nearest counterfactual using L1 or L2 distances. And it can use either feature values or attributions to calculate the distances. As mentioned earlier, WIT can also include a custom distance-finding function if you add it to the configuration. For now, we will select <strong class="screenText">L2</strong> with the <strong class="screenText">Feature value</strong>. In <em class="italic">Figure 6.6</em>, these options appear in the highlighted <em class="italic">A</em> area. Once you choose a distance metric and enable <strong class="screenText">Nearest counterfactual</strong>, it appears side by side with our <em class="italic">instance of interest</em> (area <em class="italic">B</em>), and it compares their predictions as shown in <em class="italic">Figure 6.6</em> (area <em class="italic">C</em>). You can sort the features by <strong class="screenText">Absolute attribution</strong> for a clearer understanding of feature importance on a local level. The counterfactual is only 3 years older but has zero priors instead of two, yet that was enough to reduce the <strong class="screenText">Medium/High Risk</strong> to nearly 5%:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_06.png" alt="Graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.6: How to find the nearest counterfactual in WIT</p>
    <p class="normal">While <a id="_idIndexMarker671"/>both our <em class="italic">instance of interest</em> and counterfactual remain selected, we can visualize them along with all other points. By doing this, you take insights from local interpretations and can create enough context for global understandings. For instance, let’s change our visualization settings to the following:</p>
    <ul>
      <li class="bulletList"><strong class="screenText">Binning</strong> | <strong class="screenText">X-axis</strong>: <code class="inlineCode">Inference label</code></li>
      <li class="bulletList"><strong class="screenText">Binning</strong> | <strong class="screenText">Y-axis</strong>: <code class="inlineCode">(none)</code></li>
      <li class="bulletList"><strong class="screenText">Scatter</strong> | <strong class="screenText">X-axis</strong>: <code class="inlineCode">age</code></li>
      <li class="bulletList"><strong class="screenText">Scatter</strong> | <strong class="screenText">Y-axis</strong>: <code class="inlineCode">priors_count</code></li>
    </ul>
    <p class="normal">Everything else stays the same.</p>
    <p class="normal">The <a id="_idIndexMarker672"/>result of this visualization is depicted in <em class="italic">Figure 6.7</em>. You can tell that the <strong class="screenText">Low Risk</strong> bins’ points tend to hover in the lower end of <code class="inlineCode">priors_count</code>. Both bins show that <code class="inlineCode">prior_count</code> and <code class="inlineCode">age</code> have a slight correlation, although this is substantially more pronounced in the <strong class="screenText">Medium/High Risk</strong> bin. However, what is most interesting is the sheer density of African American datapoints deemed <strong class="screenText">Medium/High Risk</strong> in <code class="inlineCode">age</code> ranging 18–25 and with <code class="inlineCode">prior_count</code> below three, compared to those in the <strong class="screenText">Low Risk</strong> bin. It suggests that both lower <code class="inlineCode">age</code> and higher <code class="inlineCode">priors_count</code> increases risk more for African Americans than others:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_07.png" alt="Graphical user interface, chart, scatter chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.7: Visualizing age versus priors_count in WIT</p>
    <p class="normal">We can<a id="_idIndexMarker673"/> try creating our own counterfactuals by editing the datapoint. What happens when we reduce <code class="inlineCode">priors_count</code> to one? The answer to this question is depicted in <em class="italic">Figure 6.8</em>. Once you make the change and click on the <strong class="screenText">Predict</strong> button in the <strong class="screenText">Infer</strong> pane, it adds an entry to the last prediction history in the <strong class="screenText">Infer</strong> pane. You can tell in <strong class="keyWord">Run #2</strong> that the risk reduces nearly to 33.5%, down nearly 50%!</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_08.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.8: Editing the datapoint to decrease priors_count in WIT</p>
    <p class="normal">Now, what happens if <code class="inlineCode">age</code> is only 2 years older but there are two priors? In <em class="italic">Figure 6.9</em>, <strong class="keyWord">Run #3</strong> tells you that it barely made it inside the <strong class="screenText">Low Risk</strong> score:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_09.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.9: Editing the datapoint to increase the age in WIT</p>
    <p class="normal">Another feature that the <strong class="screenText">Datapoint editor</strong> tab has is <strong class="screenText">Partial dependence plots</strong>, which we covered in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>. If you click on this radio button, it will modify the right pane to look like <em class="italic">Figure 6.10</em>. By default, if a datapoint is selected, the PDPs are local, meaning they pertain to the chosen datapoint. But you can switch to global. In any case, it’s best to sort plots by variation as done in <em class="italic">Figure 6.10</em>, where <code class="inlineCode">age</code> and <code class="inlineCode">priors_count</code> have the highest variation. Interestingly, neither of them is monotonic, which doesn’t make sense. The model should learn that an increase in <code class="inlineCode">priors_count</code> should consistently increase risk. It should be the same with a decrease in <code class="inlineCode">age</code>. After all, academic research shows that crime tends to peak in the mid-20s and that higher priors increase the likelihood of recidivism. The relationship between these two variables is also well understood, so perhaps some data engineering and monotonic constraints could make sure a model was consistent with known phenomena rather than learning the inconsistencies in the data that lead to unfairness. We will cover this in <em class="chapterRef">Chapter 12</em>, <em class="italic">Monotonic Constraints and Model Tuning for Interpretability</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_10.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.10: Local partial dependence plot for age and priors_count</p>
    <p class="normal">Is there something that can be done to improve fairness in a model that has already been trained? Indeed, there is. The <strong class="screenText">Performance &amp; Fairness</strong> tab can help with that.</p>
    <h3 id="_idParaDest-178" class="heading-3">Performance &amp; Fairness</h3>
    <p class="normal">When you click on the <strong class="screenText">Performance &amp; Fairness</strong> tab, you will see that it has <strong class="screenText">Configure</strong> and <strong class="screenText">Fairness</strong> panes on the left. And on the right, you can explore the overall performance of the model (see <em class="italic">Figure 6.11</em>). In the upper part of this pane, it has <strong class="screenText">False Positives (%)</strong>, <strong class="screenText">False Negatives (%)</strong>, <strong class="screenText">Accuracy (%)</strong>, and <strong class="screenText">F1</strong> fields. If you expand the pane, it shows the ROC curve, PR curve, confusion matrix, and mean attributions – the average Shapley values. We covered these terms in the previous chapters of this book either directly or indirectly, except for the PR curve. The <strong class="keyWord">Precision-Recall</strong> (<strong class="keyWord">PR</strong>) is very much like the ROC curve, except it plots precision against recall instead of TPR versus FPR. In this plot, precision is expected to decrease as recall decreases. Unlike ROC, it’s considered worse than a coin toss when the line is close to the <em class="italic">x</em>-axis, and it’s best suited to imbalanced classification problems:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_11.png" alt="Graphical user interface, chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.11: Performance &amp; Fairness tab initial view</p>
    <p class="normal">A classification model will output probabilities that an observation belongs to a class label. We usually take every observation above or equal to 0.5 to belong to the positive class. Otherwise, we predict it to belong to the negative class. This threshold is called the <strong class="keyWord">classification threshold</strong>, and <a id="_idIndexMarker674"/>you don’t always have to use the standard 0.5.</p>
    <p class="normal">There are many cases in which it is appropriate to <a id="_idIndexMarker675"/>perform <strong class="keyWord">threshold tuning</strong>. One of the most compelling reasons is imbalanced classification problems because often models optimize performance on accuracy alone but end up with bad recall or precision. Adjusting the threshold will improve the metric you care most about:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_12.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.12: Slicing performance metrics by race_African-American</p>
    <p class="normal">Another <a id="_idIndexMarker676"/>primary reason to adjust thresholds is for fairness. To this end, you need to examine the metric you most care about across different slices of your data. In our case, <strong class="screenText">False Positives (%)</strong> is where we can appreciate unfairness the most. For instance, take a look at <em class="italic">Figure 6.12</em>. In the <strong class="screenText">Configure</strong> pane, we can slice the data by <code class="inlineCode">race_African-American</code>, and to the right of it, we can see what we observed at the beginning of this chapter, which is that FPs for African Americans are substantially higher than for other segments. One way to fix this is through an automatic optimization method <a id="_idIndexMarker677"/>such as <strong class="keyWord">Demographic parity</strong> or <strong class="keyWord">Equal opportunity</strong>. If <a id="_idIndexMarker678"/>you are to use one of these, it’s best to adjust <strong class="screenText">Cost Ratio (FP/FN)</strong> to tell the optimizer that FPs are worth more than FNs:</p>
    <figure class="mediaobject"><img src="../Images/B18406_06_13.png" alt="Chart  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 6.13: Adjusting the classification threshold for the dataset sliced by race_African-American</p>
    <p class="normal">We can also adjust thresholds manually using the default <strong class="screenText">Custom thresholds</strong> setting (see <em class="italic">Figure 6.13</em>). For these slices, if we want approximate parity with our FPs, we should use 0.78 as our threshold for when <code class="inlineCode">race_African-American=1</code>. The drawback is that FNs will increase for this group, not achieving parity on that end. A cost ratio would help determine<a id="_idIndexMarker679"/> whether 14.7% in FPs justifies 24.4% in FNs, but to do this, we would have to understand the average costs involved. We will examine odds calibration methods further in <em class="chapterRef">Chapter 11</em>, <em class="italic">Bias Mitigation and Causal Inference Methods</em>.</p>
    <h1 id="_idParaDest-179" class="heading-1">Mission accomplished</h1>
    <p class="normal">This chapter’s mission was to see whether there was unfair bias in predicting whether a particular defendant would recidivate. We demonstrated that the FPR for African American defendants is 1.87 times higher than for Caucasian defendants. This disparity was confirmed with WIT, indicating that the model in question is much more likely to misclassify the positive class based on race. However, this is a global interpretation method, so it doesn’t answer our question regarding a specific defendant. Incidentally, in <em class="chapterRef">Chapter 11</em>, <em class="italic">Bias Mitigation and Causal Inference Methods</em>, we will cover other global interpretation methods for unfairness.</p>
    <p class="normal">To ascertain whether the model was racially biased against the defendant in question, we leveraged anchor and counterfactual explanations – they both output race as a primary feature in their explanations. The anchor did it with relatively high precision and coverage, and <em class="italic">Counterfactuals Guided by Prototypes</em> found that the closest decision has a different race. That being said, in both cases, race wasn’t the only feature in the explanations. The features usually included any or all of the following: <code class="inlineCode">priors_count</code>, <code class="inlineCode">age</code>, <code class="inlineCode">charge_degree</code>, and <code class="inlineCode">sex</code>. The inconsistent rules involving the first three regarding <code class="inlineCode">race</code> suggests double standards and the involvement of <code class="inlineCode">sex</code> suggests intersectionality. <strong class="keyWord">Double standards</strong> are<a id="_idIndexMarker680"/> when rules are applied unfairly to different groups. <strong class="keyWord">Intersectionality</strong> is <a id="_idIndexMarker681"/>how overlapping identities create different systems of interconnected modes of discrimination. However, we know that females of all races are less likely to recidivate according to academic research. Still, we have to ask ourselves whether females have a structural advantage that makes them privileged in this context. A healthy dose of skepticism can help, since when it comes to bias, there’s usually a more elaborate dynamic going on than meets the eye. The bottom line is that despite all the other factors that interplay with race, and provided that there’s no relevant criminological information that we are missing, yes—there’s racial bias involved in this particular prediction.</p>
    <h1 id="_idParaDest-180" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, you should know how to leverage anchors, to understand the decision rules that impact a classification, and counterfactuals, to grasp what needs to change for the predicted class to change. You also learned how to assess fairness using confusion matrices and Google’s WIT. In the next chapter, we will study interpretation methods for <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>).</p>
    <h1 id="_idParaDest-181" class="heading-1">Dataset sources</h1>
    <ul>
      <li class="bulletList">ProPublica Data Store, 2019, COMPAS Recidivism Risk Score Data and Analysis. Originally retrieved from <a href="https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis"><span class="url">https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis</span></a> </li>
    </ul>
    <h1 id="_idParaDest-182" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Desmarais, S.L., Johnson, K.L., and Singh, J.P., 2016, <em class="italic">Performance of recidivism risk assessment instruments in U.S. correctional settings</em>. Psychol Serv;13(3):206-222: <a href="https://doi.org/10.1037/ser0000075"><span class="url">https://doi.org/10.1037/ser0000075</span></a></li>
      <li class="bulletList">Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A., 2017, <em class="italic">Fairness in Criminal Justice Risk Assessments: The State of the Art</em>. Sociological Methods &amp; Research.</li>
      <li class="bulletList">Angwin, J., Larson, J., Mattu, S., and Kirchner, L., 2016, <em class="italic">Machine Bias. There’s software used across the county to predict future criminals</em>: <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"><span class="url">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</span></a></li>
      <li class="bulletList">Ribeiro, M.T., Singh, S., and Guestrin, C., 2018, <em class="italic">Anchors: High-Precision Model-Agnostic Explanations</em>. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society: <a href="https://doi.org/10.1609/aaai.v32i1.11491"><span class="url">https://doi.org/10.1609/aaai.v32i1.11491</span></a></li>
      <li class="bulletList">Rocque, M., Posick, C., &amp; Hoyle, J., 2015, <em class="italic">Age and Crime</em>. The encyclopedia of crime and punishment, 1–8: <a href="https://doi.org/10.1002/9781118519639.wbecpx275"><span class="url">https://doi.org/10.1002/9781118519639.wbecpx275</span></a></li>
      <li class="bulletList">Dhurandhar, A., Chen, P., Luss, R., Tu, C., Ting, P., Shanmugam, K., and Das, P., 2018, <em class="italic">Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives</em>. NeurIPS: <a href="https://arxiv.org/abs/1802.07623"><span class="url">https://arxiv.org/abs/1802.07623</span></a></li>
      <li class="bulletList">Jiang, H., Kim, B., and Gupta, M.R., 2018, <em class="italic">To Trust Or Not To Trust A Classifier</em>. NeurIPS: <a href="https://arxiv.org/pdf/1805.11783.pdf"><span class="url">https://arxiv.org/pdf/1805.11783.pdf</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_6.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>