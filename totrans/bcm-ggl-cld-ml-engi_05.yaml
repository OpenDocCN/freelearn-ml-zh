- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Neural Networks and Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since its debut in 2012, **Deep Learning** (**DL**) has made a huge breakthrough
    and has been applied in many research and industrial areas including computer
    vision, **Natural Language Processing** (**NLP**), and so on. In this chapter,
    we will introduce basic concepts, including the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks and DL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost function
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The optimizer algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation functions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After we master the concepts, we will discuss several neural network models
    and their business use cases, including the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L****ong Short-Term Memory** (**LSTM**) networks'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding neural networks and DL concepts, common models, and business use
    cases is extremely important in our cloud ML journey. Let’s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks and DL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the history of us human beings, there are many interesting milestones, from
    vision development and language development to making and using tools. How did
    humans evolve and how can we train a computer to *see*, *speak*, and *use* tools?
    Looking for answers to these questions has led us to the modern AI arena.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'How do our brains work? Modern science reveals that in the brain, there is
    a layered neural network consisting of a set of neurons. A typical neuron collects
    electrical signals from others through a fine structure called **dendrites** and
    sends out spikes of signals through a conducting structure called an **axon**,
    which splits into many branches. At the end of each branch, a synapse converts
    the signals from the axon into electrical effects to excite activity on the target
    neuron. *Figure 5.1* shows the working mechanism of a biological neuron:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – How a biological neuron works ](img/Figure_5.1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – How a biological neuron works
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by the biological neural network model, an **Artificial Neural Network**
    (**ANN**) model consists of artificial neurons called **perceptrons**. A perceptron
    receives weighted inputs from the other perceptrons, applies the transfer function,
    which is the sum of the weighted inputs, and the activation function, which adds
    nonlinear activation to the sum, and outputs to excite the next perceptron. *Figure
    5.2* shows the working mechanism of an artificial neuron (perceptron):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – How an artificial neuron (perceptron) works ](img/Figure_5.2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – How an artificial neuron (perceptron) works
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'ANNs consist of perceptrons working together via layers. *Figure 5.3* shows
    the structure of a multilayer ANN where each circular node represents a perceptron,
    and a line represents the connection from the output of one perceptron to the
    input of another. There are three types of layers in a neural network: an input
    layer, one or more hidden layers, and an output layer. The neural network in *Figure
    5.3* has one input layer, two hidden layers, and an output layer:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – A multilayer ANN ](img/Figure_5.3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 多层人工神经网络](img/Figure_5.3.jpg)'
- en: Figure 5.3 – A multilayer ANN
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 多层人工神经网络
- en: 'Using neural networks to perform ML model training, the data flows in the network
    as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络进行机器学习模型训练时，数据在网络中的流动如下：
- en: A dataset (*x*1*, x*2*, x*3*, ..., x*n) is prepared and sent to the input layer,
    which has the same amount of perceptrons as the number of features of the dataset.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个数据集 (*x*1*, x*2*, x*3*, ..., x*n*) 并将其发送到输入层，该层的感知器数量与数据集的特征数量相同。
- en: The data then moves through to the hidden layers. At each hidden layer, the
    perceptron processes the weighted inputs (sum and activate, as described earlier),
    and sends the output to the neurons at the next hidden layer.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，数据移动到隐藏层。在每个隐藏层中，感知器处理加权输入（求和并激活，如前所述），并将输出发送到下一隐藏层的神经元。
- en: After the hidden layers, the data finally moves to the output layer, which provides
    the outputs.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在隐藏层之后，数据最终移动到输出层，该层提供输出。
- en: 'The objective of the neural network is to determine the weights that minimize
    the cost function (average prediction error for the dataset). Similar to the regression
    model training process we discussed in the previous chapters, DL model training
    is implemented by iterations of a two-part process, forward propagation and backpropagation,
    as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的目标是确定最小化成本函数（数据集的平均预测误差）的权重。类似于我们在前几章中讨论的回归模型训练过程，深度学习模型训练通过迭代两个部分的过程实现，即正向传播和反向传播，如下所示：
- en: '**Forward propagation** is the path that information flows from the input layer
    to the output layer, through the hidden layers. At the beginning of the training
    process, data arrives at the input layer where they are multiplied with the weights
    randomly initialized, then passed to the first hidden layer. Since the input layer
    has multiple nodes, each one is connected to each node in the first hidden layer;
    a node in the hidden layer sums up the weighted values to it and applies an activation
    function (adds nonlinearity). It then sends the output to the nodes of the next
    layer, where the nodes do the same, till the output of the last hidden layer is
    multiplied by the weights and becomes the input to the final output layer, where
    further functions are applied to generate the output.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向传播**是信息从输入层流向输出层，通过隐藏层的路径。在训练过程的开始，数据到达输入层，在那里它们与随机初始化的权重相乘，然后传递到第一隐藏层。由于输入层有多个节点，每个节点都与第一隐藏层中的每个节点相连；隐藏层中的节点将加权值求和并应用激活函数（添加非线性）。然后它将输出发送到下一层的节点，那里的节点执行相同的操作，直到最后一个隐藏层的输出乘以权重并成为最终输出层的输入，在该层进一步应用函数以生成输出。'
- en: '**Backpropagation** is the path information flows from the output layer all
    the way back to the input layer. During this process, the neural network compares
    the predicted output to the actual output as the first step of backpropagation
    and calculates the cost function or prediction error. If the cost function is
    not good enough, it moves back to adjust the weights based on algorithms such
    as **Gradient Descent** (**GD**) and then starts the forward propagation again
    with the new weights.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**是信息从输出层流回输入层的路径。在这个过程中，神经网络将预测输出与实际输出进行比较，作为反向传播的第一步，并计算成本函数或预测误差。如果成本函数不够好，它就会根据如**梯度下降**（**GD**）等算法回退以调整权重，然后使用新的权重再次开始正向传播。'
- en: Forward propagation and backpropagation are repeated multiple times—each time
    the network adjusts the weights, trying to get a better cost function value—until
    the network gets a good cost function (an acceptable accuracy) at the output layer.
    At this time, the model training is completed and we have got the optimized weights,
    which are the results of the training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播和反向传播会重复多次——每次网络调整权重，试图获得更好的成本函数值——直到网络在输出层获得良好的成本函数（可接受的准确度）。此时，模型训练完成，我们得到了优化的权重，这是训练的结果。
- en: DL is training ML models with neural networks. If you compare the preceding
    DL model training process using neural networks with the ML model training process
    we discussed in the *Training the model* section in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094),
    *Developing and Deploying ML Models*, you will find that the concepts of ML and
    DL are very similar. Via iterative forward propagation and backward propagation,
    both are trying to minimize the cost function of the model—ML is more about computers
    learning from data with traditional algorithms, while DL is more about computers
    learning from data mimicking the human brain and neural networks. Relatively speaking,
    ML requires less computing power and DL needs less human intervention. In the
    following sections, we will take a close look at the cost function, the optimizer
    algorithm, and the activation function for DL with neural networks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The cost function
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the concept of the cost function in the *Linear regression* section
    in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094). The cost function gives us
    a mathematical way of determining how much error the current model has—it assigns
    a cost for making an incorrect prediction and provides a way to measure the model
    performance. The cost function is a key metric in ML model training—choosing the
    right cost function can improve model performance dramatically.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The common cost functions for regression models are MAE and MSE. As we have
    discussed in previous chapters, MAE defines a summation of the absolute differences
    between the prediction values and the label values. MSE defines the summation
    of squares of the differences between the prediction values and the label values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost functions for classification models are quite different. Conceptually,
    the cost function for a classification model is the difference between the probability
    distributions for different classes. For binary classification models where the
    model outputs are binary, 1 for yes or 0 for no, we use **binary cross entropy**.
    For multi-class classification models, depending on the dataset labels, we use
    **categorical cross entropy** and **sparse categorical cross entropy** as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: If the labels are integers, for example, to classify an image of a dog, a cat,
    or a cow, then we use sparse categorical cross entropy since the output is one
    exclusive class.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, if the labels are encoded as a series of zeros and ones for each
    class (same for the one-hot-encoding format that we have discussed in the previous
    chapters), we’ll use categorical cross entropy. For example, given an image, you
    need to detect whether there exists a driver’s license, a passport, or a social
    security card, we will use categorical cross entropy as cost functions since the
    output has a combination of classes.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost function is a way of measuring our models so we can adjust the model
    parameters to minimize them—the model optimization process. In the following section,
    we’ll talk about the optimizer algorithms that minimize the cost function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer algorithm
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Linear regression* section in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094),
    we discussed the **GD** algorithm, which optimizes the linear regression cost
    function. In neural networks, the optimizer is an algorithm used to minimize the
    cost function in model training. The commonly used optimizers are **Stochastic
    Gradient Descent** (**SGD**), **RMSprop**, and **Adam** as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**SGD** is useful for very large datasets. Instead of GD, which runs through
    all of the samples in your training dataset to update parameters, SGD uses one
    or a subset of training samples.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSprop** improves SGD by introducing variable learning rates. The learning
    rate, as we discussed in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), impacts
    model performances—larger learning rates can reduce training time but may lead
    to model oscillation and may miss the optimal model parameter values. Lower learning
    rates can make the training process longer. In SGD, the learning rate is fixed.
    RMSprop adapts the learning rate as training progresses, and thus it allows you
    to start with big learning rates when the model has a high cost function, but
    it gradually reduces the learning rate when the cost function decreases.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam** stands for **Adaptive Moment Estimation** and is one of the most widely
    used optimizers. Adam adds momentum to the adaptive learning rate from RMSprop,
    and thus it allows changes to the model to accelerate while moving in the same
    direction during training, making the model training process quicker and better.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right cost function and optimizer algorithms is very important
    for model performance and training speed. Google’s TensorFlow framework provides
    many optimizer algorithms. For further details, please refer to [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Other important features for neural networks are non-linearity and output normalization,
    which are provided by the activation functions. We will examine them in the following
    section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The activation functions
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you can see from the preceding section, the activation function is part
    of the training process. The purpose of the activation function is to transform
    the weighted-sum input to the nodes: non-linearize and change the output range.
    There are many activation functions in neural networks. We will discuss some of
    the most used ones: the sigmoid function, the tanh activation function, the ReLu
    function, and the LeakyReLU function. *Figure 5.4* shows the curves of these functions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Activation functions ](img/Figure_5.4.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Activation functions
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect each of the preceding activation functions as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下方式检查前面的每个激活函数：
- en: The sigmoid activation function was discussed earlier ithe T*he cost function*
    section. We use the sigmoid function to change continuous values to a range between
    0 and 1, which fits the models to predict the probability as an output.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sigmoid 激活函数在之前的“成本函数”部分已经讨论过。我们使用 sigmoid 函数将连续值转换为 0 到 1 之间的范围，这样模型就可以预测概率作为输出。
- en: The tanh activation function is very similar to sigmoid, but the output is from
    -1 to +1 and thus it is preferred to sigmoid due to the output being zero-centered.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tanh 激活函数与 sigmoid 非常相似，但输出范围是从 -1 到 +1，因此由于输出是零中心化的，它比 sigmoid 更受欢迎。
- en: The ReLU activation function stands for Rectified Linear Unit. It is widely
    used since it converts the negative values to 0 and keeps the positive values
    as such. Its range is between 0 and infinity. Because the gradient value is 0
    in the negative area, the weights and biases for some neurons may not be updated
    during the training process, causing dead neurons that never get activated.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU 激活函数代表线性整流单元。它被广泛使用，因为它将负值转换为 0 并保持正值不变。其范围在 0 到无穷大之间。由于负区域的梯度值为 0，因此在训练过程中，某些神经元的权重和偏差可能不会更新，导致这些神经元永远不会被激活。
- en: The LeakyReLU is an improved version of the ReLU function to solve the dying
    ReLU problem as it has a small positive slope in the negative area. The advantages
    of LeakyReLU are the same as that of the ReLU, in addition to the fact that it
    enables training even for negative input values.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeakyReLU 是 ReLU 函数的改进版本，它通过在负区域具有一个小正斜率来解决“死亡 ReLU”问题。LeakyReLU 的优点与 ReLU 相同，除此之外，它还允许对负输入值进行训练。
- en: Another activation function is the *softmax* function, which is often used in
    the output layer for multi-class classifications. The softmax activation function
    converts the output layer values into probabilities summing up to 1 and thus outputs
    probabilities for each class in multi-class classification problems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个激活函数是 *softmax* 函数，它通常用于多类分类的输出层。softmax 激活函数将输出层值转换为概率之和为 1 的概率，从而在多类分类问题中为每个类别输出概率。
- en: Among all of these activation functions, which shall we choose? The answer depends
    on factors such as the type of predictions, the architecture of the network, the
    number of layers, the current layer in the network, and so on. For example, sigmoid
    is more used for binary classification use cases, whereas softmax is often applied
    for multi-classifications, and regression problems may or may not use activation
    functions. While there will be trial and error involved at the beginning, experience
    will build up good practices.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些激活函数中，我们应该选择哪一个？答案取决于诸如预测类型、网络架构、层数、网络中的当前层等因素。例如，sigmoid 函数更常用于二分类用例，而
    softmax 函数通常用于多分类，回归问题可能或可能不使用激活函数。虽然一开始可能会有试错的过程，但经验会积累出良好的实践。
- en: Now that we have introduced the concepts of neural networks and activation functions,
    let’s examine some neural networks that are commonly used in computer vision,
    **Natural Language Processing** (**NLP**), and other areas.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了神经网络和激活函数的概念，让我们来看看在计算机视觉、**自然语言处理**（NLP）和其他领域常用的神经网络。
- en: Convolutional Neural Networks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Now that we have learned about neural networks and DL, let’s look at some business
    use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了神经网络和深度学习，让我们看看一些商业用例。
- en: The first case is image recognition. How can we teach a computer to recognize
    an image? It is an easy task for a human being but a very difficult one for a
    computer. The first thing we need to do, since computers are only good at working
    with 1s and 0s, is to transform the image into a numerical matrix using pixels.
    As an example, *Figure 5.5* shows a black and white image for a single digit number,
    *8*, represented by a 28x28 pixel matrix. While human beings can easily recognize
    the image as a number *8* by some *magic sensors* in our eyes, a computer needs
    to input all of the 28x28=784 pixels, each having a **pixel value—a** single number
    representing the brightness of the pixel. The pixel value has possible values
    from 0 to 255, with 0 as black and 255 as white. Values in between make up the
    different shades of gray. If we have a color image, the pixel will have three
    numerical RGB values (red, green, and blue) to represent its color instead of
    one black value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Representing the number 8 with pixel values ](img/Figure_5.5.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Representing the number 8 with pixel values
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: After we have a pixel matrix representation of the image, we can start developing
    a **Multi-Layer Perceptron** (**MLP**) network for training. We will construct
    the input layer with 784 nodes and input 784 pixel values, one for each. Each
    node from the input layer will then output to each node in the next layer (a hidden
    layer), and so on. When the number of layers increases, the total number of calculations
    will be huge for the entire network. To decrease the total calculations, the idea
    of feature filtering comes into play and leads to the concept of a **CNN**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs are widely used in computer vision, especially in image recognition and
    processing. A CNN consists of three layers: the convolutional layer, the pooling
    layer, and the fully connected layer. The convolutional layer convolutes the inputs
    and filters the image features, the pooling layer compresses the filtered features,
    and the fully connected layer, which is basically an MLP, does the model training.
    Let’s examine each of these layers in detail.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **convolutional layer** performs convolution, which is applied to the input
    data to filter the information and produce a feature map. The filter is used as
    a sliding window to scan the entire image and autonomously recognize features
    in the images. As shown in *Figure 5.6*, a 3x3 filter, which is also called the
    **Kernel** (**K**), scans the whole **Image** (**I**) and generates a feature
    map, denoted as *I*K* since its element comes from the product of *I* and *K*
    (in the example of *Figure 5.6*: *1x1+0x0+1x0+0x1+1x1+0x0+1x1+0x1+1x1=4*).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 5.6 – The convolution operation ](img/Figure_5.6.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The convolution operation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Going through the convolution process extracts the image features and generates
    a feature map that still has a large amount of data and makes it hard to train
    the neural network. To compress the data, we go through the pooling layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The pooling layer
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **pooling layer** receives the results from a convolutional layer, the feature
    map, and compresses it using a filter. Depending on the function used for calculation,
    it can either be maximum pooling or average pooling. As shown in *Figure 5.7*,
    a 2x2 filter patch scans the feature map and compresses it. With max pooling,
    it takes the maximum value from the scanning windows, *max(15,8,20,9) = 20*, and
    so on. With average pooling, it takes the average value, *average(15,8,20,9) =
    13*. As you can see, the filter of a pooling layer is always smaller than a feature
    map.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The pooling layer ](img/Figure_5.7.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The pooling layer
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: From the input image, the process of convolution and pooling iterates, and the
    final result is input to a fully connected layer (MLP) to process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: The fully connected layer
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the convolution and pooling layers, we need to flatten the result and
    pass it to an MLP, a fully connected neural network, for classification. The final
    result will then be activated with the softmax activation function to yield the
    final output – an understanding of the image.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second type of neural network is an RNN. RNNs are widely used in time series
    analysis, such as NLP. The concept of an RNN came about in the 1980s, but it’s
    not until recently that it gained its momentum in DL.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, in traditional feedforward neural networks such as CNNs, a node
    in the neural network only counts the current input and does not memorize the
    precious inputs. Therefore, it cannot handle time series data, which needs the
    previous inputs. For example, to predict the next word of a sentence, the previous
    words will be required to do the inference. By introducing a hidden state, which
    remembers some information about the sequence, RNNs solved this issue.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Different from feedforward networks, RNNs are a type of neural network where
    the output from the previous step is fed as the input to the current step; using
    a loop structure to keep the information allows the neural network to take the
    sequence of input. As shown in *Figure 5.8*, a loop for node *A* is unfolded to
    explain its process; first, node *A* takes *X*0 from the sequence of input, and
    then it outputs *h*0, which, together with *X*1, is the input for the next step.
    Similarly, *h*1 and *X*2 are inputs for the next step, and so on and so forth.
    Using the loop, the network keeps remembering the context while training:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – The RNN unrolled loop (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    ](img/Figure_5.8.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8 – The RNN unrolled loop (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The drawback for a simple RNN model is the vanishing gradient problem, which
    is caused by the fact that the same weights are used to calculate a node’s output
    at each time step during training and also done during backpropagation. When we
    move backward further, the error signal becomes bigger or smaller, thus causing
    difficulty in memorizing the contexts that are further away in the sequence. To
    overcome this drawback, the **LSTM** neural network was developed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory Networks
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An LSTM network was designed to overcome the vanishing gradient problem. LSTMs
    have feedback connections, and the key to LSTMs is the cell state—a horizontal
    line running through the entire chain with only minor linear interactions, which
    persists the context information. LSTM adds or removes information to the cell
    state by gates, which are composed of activation functions, such as sigmoid or
    tanh, and a pointwise multiplication operation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – An LSTM model (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    ](img/Figure_5.9.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9 – An LSTM model (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.9* shows an LSTM that has the gates to protect and control the cell
    state. Using the cell state, LSTM solves the issue of vanishing gradients and
    thus is particularly good at processing time series sequences of data, such as
    text and speech inference.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial networks
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GANs** are algorithmic architectures that are used to generate new synthetic
    instances of data that can pass for real data. As shown in *Figure 5.10*, GAN
    is a generative model that trains the following two models simultaneously:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: A **Generative** (**G**) model that captures the data distribution to generate
    plausible data. The latent space input and random noise can be sampled and fed
    into the generator network to generate samples that become the negative training
    examples for the discriminator.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Discriminative** (**D**) model that compares the generated image with a
    real image and tries to identify whether the given image is fake or real. It estimates
    the probability that a sample came from the training data rather than the real
    data to distinguish the generator’s fake data from real data. The discriminator
    penalizes the generator for producing implausible results.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – The GAN (source: https://developers.google.com/machine-learning/recommendation)
    ](img/Figure_5.10.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10 – The GAN (source: https://developers.google.com/machine-learning/recommendation)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The model training starts with the generator generating fake data and the discriminator
    learns to tell that it’s false by comparing it with real samples. The GAN then
    sends the results to the generator and the discriminator to update the model.
    This fine tuning training process iterates and finally produces some extremely
    real-looking data. GANs can be used to generate text, images, and video, and color
    or denoise images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks and DL have added the modern color to the traditional ML spectrum.
    In this chapter, we started by learning the concepts of neural networks and DL
    by examining the cost functions, optimizer algorithms, and activation functions.
    Then, we introduced advanced neural networks, including CNN, RNN, LSTM, and GAN.
    As we can see, by introducing neural networks, DL extended ML concepts and made
    a breakthrough in many applications such as computer vision, NLP, and others.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter concludes part two of the book: *Machine Learning and Deep Learning*.
    In part three, we will focus on *Machine Learning the Google Way*, where we will
    talk about how Google does ML and DL in Google Cloud. We will start part three
    with learning about BQML, Google TensorFlow, and Keras in the following chapter.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further insights on the topics learned in this chapter, you can refer to
    the following links:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/what-is/neural-network/](https://aws.amazon.com/what-is/neural-network/)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developers.google.com/machine-learning/gan](https://developers.google.com/machine-learning/gan)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: Mastering ML in GCP'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part, we learn how Google does ML in the Google Cloud Platform. First,
    we discover Google’s BigQuery ML for structured data, and then we look at Google’s
    ML frameworks, TensorFlow and Keras. We examine Google’s end-to-end ML suite,
    Vertex AI, and the ML services it provides. We then look at the Google pre-trained
    model APIs for ML development: GCP ML APIs. We end this part with a summary of
    the ML implementation best practices in Google Cloud.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18333_06.xhtml#_idTextAnchor133), Learning BQML, TensorFlow,
    and Keras'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18333_07.xhtml#_idTextAnchor143), Exploring Google Cloud Vertex
    AI'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18333_08.xhtml#_idTextAnchor159), Discovering Google Cloud ML
    API'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18333_09.xhtml#_idTextAnchor168), Using Google Cloud ML Best
    Practices'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
