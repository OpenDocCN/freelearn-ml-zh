<html><head></head><body>
		<div id="_idContainer047">
			<h1 id="_idParaDest-49"><a id="_idTextAnchor053"/>Chapter 3: Code Meets Data</h1>
			<p>In this chapter, we'll get started with hands-on <strong class="bold">MLOps</strong> implementation as we learn by solving a business problem using the MLOps workflow discussed in the previous chapter. We'll also discuss effective methods of source code management for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), explore data quality characteristics, and analyze and shape data for an ML solution.</p>
			<p>We begin this chapter by categorizing the business problem to curate a best-fit MLOps solution for it. Following this, we'll set up the required resources and tools to implement the solution. 10 guiding principles for source code management for ML are discussed to apply clean code practices. We will discuss what constitutes good-quality data for ML and much more, followed by processing a dataset related to the business problem and ingesting and versioning it to the ML workspace. Most of the chapter is hands-on and designed to equip you with a good understanding of and experience with MLOps. For this, we're going to cover the following main topics in this chapter:</p>
			<ul>
				<li>Business problem analysis and categorizing the problem </li>
				<li>Setting up resources and tools</li>
				<li>10 principles of source code management for machine learning </li>
				<li>Good data for machine learning</li>
				<li>Data preprocessing</li>
				<li>Data registration and versioning</li>
				<li>Toward an ML pipeline</li>
			</ul>
			<p>Without further ado, let's jump into demystifying the business problem and implementing the solution using an MLOps approach.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor054"/>Business problem analysis and categorizing the problem</h1>
			<p>In the previous chapter, we looked into the following business problem statement. In this section, we will <a id="_idIndexMarker154"/>demystify the problem statement by categorizing it <a id="_idIndexMarker155"/>using the principles to curate an implementation roadmap. We will glance at the dataset given to us to address the business problem and decide what type of ML model will address the business problem efficiently. Lastly, we'll categorize the MLOps approach for implementing robust and scalable ML operations and decide on tools for implementation. </p>
			<p class="callout-heading">Here is the problem statement:</p>
			<p class="callout">You work as a data scientist with a small team of data scientists for a cargo shipping company based in Finland. 90% of goods are imported into Finland via cargo shipping. You are tasked with saving 20% of the costs for cargo operations at the port of Turku, Finland. This can be achieved by developing an ML solution that predicts weather conditions at the port 4 hours in advance. You need to monitor for possible rainy conditions, which can distort operations at the port with human resources and transportation, which in turn affects supply chain operations at the port. Your ML solution will help port authorities to predict possible rain 4 hours in advance; this will save 20% of costs and enable smooth supply chain operations at the port.</p>
			<p>The first step in solving a problem is to simplify and categorize it using an appropriate approach. In the previous chapter, we discussed how to categorize a business problem to solve it using ML. Let's apply those principles to chart a clear roadmap to implementing it.</p>
			<p>First, we'll see what type of model we will train to yield the maximum business value. Secondly, we will identify the right approach for our MLOps implementation.</p>
			<p>In order to decide on the type of model to train, we can start by having a glance at the dataset available on GitHub: <a href="https://github.com/PacktPublishing/EngineeringMLOps">https://github.com/PacktPublishing/EngineeringMLOps</a>.</p>
			<p>Here is a snapshot of <strong class="source-inline">weather_dataset_raw.csv</strong>, in <em class="italic">Figure 3.1</em>. The file size is 10.7 MB, the number of rows is 96,453, and the file is in CSV format:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B16572_03_01.jpg" alt="Figure 3.1 – Dataset snapshot&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Dataset snapshot</p>
			<p>By assessing the data, we can categorize the business problem as follows:</p>
			<ul>
				<li><strong class="bold">Model type</strong>: In order to <a id="_idIndexMarker156"/>save 20% of the operational costs at the port of Turku, a supervised learning model is required to predict by classifying whether it will rain or not rain. Data is labeled, and the <strong class="source-inline">Weather condition</strong> column depicts whether an event has recorded rain, snow, or clear conditions. This can be framed or relabeled as <strong class="source-inline">rain</strong> or <strong class="source-inline">no rain</strong> and used to perform binary classification. Hence, it is straightforward to solve the business problem with a supervised learning approach.</li>
				<li><strong class="bold">MLOps approach</strong>: By observing the <a id="_idIndexMarker157"/>problem statement and data, here are the facts:<p>(a) Data: The training data is 10.7 MB. The data size is reasonably small (it cannot be considered big data).</p><p>(b) Operations: We need to train, test, deploy, and monitor an ML model to forecast the weather at the port of Turku every hour (4 hours in advance) when new data is recorded.</p><p>(c) Team size: A small/medium team of data scientists, no DevOps engineers.</p></li>
			</ul>
			<p>Based on the preceding facts, we can categorize the operations into <strong class="bold">small team ops</strong>; there is <a id="_idIndexMarker158"/>no need for big data processing and the team is small and agile. Now we will look at some suitable tools to implement the operations needed to solve the business problem at hand.</p>
			<p>For us to get a holistic understanding of MLOps implementation, we will <a id="_idIndexMarker159"/>implement the business <a id="_idIndexMarker160"/>problems using two different tools simultane<a id="_idTextAnchor055"/>ously:</p>
			<ul>
				<li><strong class="bold">Azure Machine Learning </strong>(Microsoft Azure)</li>
				<li><strong class="bold">MLflow (</strong>an open source cloud and platform-agnostic tool<strong class="bold">)</strong></li>
			</ul>
			<p>We use <a id="_idIndexMarker161"/>these two tools to <a id="_idIndexMarker162"/>see how things work from a pure cloud-based approach and from an open source / cloud-agnostic approach. All the code and CI/CD operations will be managed and orchestrated using Azure DevOps, as shown in <em class="italic">Figure 3.2</em>:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B16572_03_02.jpg" alt="Figure 3.2 – MLOps tools for the solution &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – MLOps tools for the solution </p>
			<p>Now, we <a id="_idIndexMarker163"/>will set up the tools and resources needed to implement the solution <a id="_idIndexMarker164"/>for the business problem. As we will use Python as the primary programming language, it is a pre-requisite to have <strong class="bold">Python 3</strong> installed within your Mac, Linux, or Windows OS.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor056"/>Setting up the resources and tools</h1>
			<p>If you have <a id="_idIndexMarker165"/>these tools already installed and set up on your PC, feel free to <a id="_idIndexMarker166"/>skip this section; otherwise, follow the detailed instructions to get them up and running. </p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor057"/>Installing MLflow</h2>
			<p>We get started by <a id="_idIndexMarker167"/>installing MLflow, which is an open source platform for managing the ML life cycle, including experimentation, reproducibility, deployment, and a central model registry.</p>
			<p>To install MLflow, go to your terminal and execute the following command:</p>
			<p class="source-code">pip3 install mlflow</p>
			<p>After successful installation, test the installation by executing the following command to start the <strong class="source-inline">mlflow</strong> tracking UI:</p>
			<p class="source-code">mlflow ui</p>
			<p>Upon running the <strong class="source-inline">mlflow</strong> tracking UI, you will be running a server listening at port <strong class="source-inline">5000</strong> on your machine, and it outputs a message like the following:</p>
			<p class="source-code">[2021-03-11 14:34:23 +0200] [43819] [INFO] Starting gunicorn 20.0.4</p>
			<p class="source-code">[2021-03-11 14:34:23 +0200] [43819] [INFO] Listening at: http://127.0.0.1:5000 (43819)</p>
			<p class="source-code">[2021-03-11 14:34:23 +0200] [43819] [INFO] Using worker: sync</p>
			<p class="source-code">[2021-03-11 14:34:23 +0200] [43821] [INFO] Booting worker with pid: 43821</p>
			<p>You can access and view the <strong class="source-inline">mlflow</strong> UI at <strong class="source-inline">http://localhost:5000</strong>. When you have successfully installed <strong class="source-inline">mlflow</strong> and run the tracking UI, you are ready to install the next tool.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor058"/>Azure Machine Learning</h2>
			<p>Azure Machine Learning provides a cloud-based ML platform for training, deploying, and managing ML <a id="_idIndexMarker168"/>models. This service is available on Microsoft Azure, so the <a id="_idIndexMarker169"/>pre-requisite is to have a free subscription to Microsoft Azure. Please create a free account with around $170 of credit, which is sufficient to implement the solution, here: <a href="https://azure.microsoft.com/">https://azure.microsoft.com/</a>.</p>
			<p>When you have access/a subscription to Azure, move on to the next section to get Azure Machine Learning up and running.</p>
			<h3>Creating a resource group</h3>
			<p>A <strong class="bold">resource group</strong> is a collection of related <a id="_idIndexMarker170"/>resources for an Azure <a id="_idIndexMarker171"/>solution. It is a container that ties up all the resources related to a service or solution. Creating a resource group enables easy access and management of a solution. Let's get started by creating your own resource group:</p>
			<ol>
				<li>Open the Azure portal.</li>
				<li>Access the portal menu (go to the portal's home page if you are not there by default) and hover over the resource group icon in the navigation section. A <strong class="bold">Create</strong> button will appear; click on it to create a new resource group:<div id="_idContainer036" class="IMG---Figure"><img src="image/B16572_03_03.jpg" alt="Figure 3.3 – Creating a resource group&#13;&#10;"/></div><p class="figure-caption">Figure 3.3 – Creating a resource group</p></li>
				<li>Create a resource group with the name of your choice (<strong class="source-inline">Learn_MLOps</strong> is recommended), as shown in <em class="italic">Figure 3.3</em>.</li>
				<li>Select a region <a id="_idIndexMarker172"/>close to you to get the optimal <a id="_idIndexMarker173"/>performance and pricing. For example, in <em class="italic">Figure 3.3</em> a resource group with the name <strong class="source-inline">Learn MLOps</strong> and region <strong class="bold">(Europe) North Europe</strong> is ready to be created. After you click the <strong class="bold">Review + Create</strong> button and Azure validates the request, the final <strong class="bold">Create</strong> button will appear. The final <strong class="bold">Create</strong> button should be pressed to create the new resource group. </li>
			</ol>
			<p>When the resource group is reviewed and created, you can set up and manage all the services related to the ML solution in this resource group. The newly created resource group will be listed in the resource group list.</p>
			<h3>Creating an Azure Machine Learning workspace</h3>
			<p>An ML workspace is a <a id="_idIndexMarker174"/>central hub for tracking and managing your ML training, deploying, and monitoring experiments. To create an Azure Machine Learning workspace, go to the Azure portal menu, click on <strong class="bold">Create a resource</strong>, then search for <strong class="source-inline">Machine Learning</strong> and select it. You will see the following screen:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B16572_03_04.jpg" alt="Figure 3.4 – Creating an Azure Machine Learning workspace&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Creating an Azure Machine Learning workspace</p>
			<p>Name the workspace with the <a id="_idIndexMarker175"/>name of your choice (for example, we've named it <strong class="bold">MLOps_WS</strong> in <em class="italic">Figure 3.4</em>). Select the resource group you created earlier to tie this ML service to it (<strong class="bold">Learn_MLOps</strong> is selected in <em class="italic">Figure 3.4</em>). Finally, hit the <strong class="bold">Review + create</strong> button and you will be taken to a new screen with the final <strong class="bold">Create</strong> button. Press the final <strong class="bold">Create</strong> button to create your Azure Machine Learning workspace.</p>
			<p>After creating the Azure Machine Learning workspace (<strong class="source-inline">Learn_MLOps</strong>), the Azure platform will deploy all the resources this service needs. The resources deployed with the Azure Machine Learning instance (<strong class="source-inline">Learn_MLOps</strong>), such as Blob Storage, Key Vault, and Application Insights, are provisioned and tied to the workspace. These resources will be consumed or used via the workspace and the SDK.</p>
			<p>You can find detailed instructions on creating an Azure Machine Learning instance here: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace</a>.</p>
			<h3>Installing Azure Machine Learning SDK</h3>
			<p>Go to the terminal or <a id="_idIndexMarker176"/>command line in your PC and install the Azure Machine Learning SDK, which will be extensively used in the code to orchestrate the experiment. To install it, run the following command:</p>
			<p class="source-code">pip3 install --upgrade azureml-sdk</p>
			<p>You can find detailed instructions here: <a href="https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py">https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py</a>.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor059"/>Azure DevOps</h2>
			<p>All the <a id="_idIndexMarker177"/>source code and CI/CD-related operations will be managed and orchestrated using Azure DevOps. The <a id="_idIndexMarker178"/>code we manage in the repository in Azure DevOps will be used to train, deploy, and monitor ML models enabled by CI/CD pipelines. Let's start by creating an Azure DevOps subscription:</p>
			<ol>
				<li value="1">Create a free account at <a href="http://dev.azure.com">dev.azure.com</a>. A free account can be created using a pre-existing Microsoft or GitHub account.</li>
				<li>Create a project named <strong class="source-inline">Learn_MLOps</strong> (make it public or private depending on your preference).</li>
				<li>Go to the <strong class="bold">repos</strong> section. In the <strong class="bold">Import a repository</strong> section, press the <strong class="bold">Import</strong> button.</li>
				<li>Import a repository from a public GitHub project from this repository: <a href="https://github.com/PacktPublishing/EngineeringMLOps ">https://github.com/PacktPublishing/EngineeringMLOps</a> (as shown in <em class="italic">Figure 3.5</em>):</li>
			</ol>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B16572_03_05.jpg" alt="Figure 3.5 – Import the GitHub repository into the Azure DevOps project&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Import the GitHub repository into the Azure DevOps project</p>
			<p>After importing the GitHub repository, files from the imported repository will be displayed.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor060"/>JupyterHub</h2>
			<p>Lastly, we'll <a id="_idIndexMarker179"/>need an interactive data analysis and visualization tool to process <a id="_idIndexMarker180"/>data using our code. For this, we use <strong class="bold">JupyterHub</strong>. This is a common data science tool used widely by data scientists to process data, visualize data, and train ML models. To install it, follow two simple steps:</p>
			<ol>
				<li value="1">Install JupyterHub via the command line on your PC:<p class="source-code">python3 -m pip install jupyterhub</p><p>You may find detailed instructions here: <a href="https://jupyterhub.readthedocs.io/en/stable/quickstart.html">https://jupyterhub.readthedocs.io/en/stable/quickstart.html</a>.</p></li>
				<li>Install Anaconda.<p>Anaconda is needed as it installs dependencies, setup environments, and services to support the JupyterHub. Download Anaconda and install it as per the detailed instructions here: <a href="https://docs.anaconda.com/anaconda/install/">https://docs.anaconda.com/anaconda/install/</a>.</p><p>Now that we are set up for the hands-on implementation, let's look at what it takes to manage good code and data.</p></li>
			</ol>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor061"/>10 principles of source code management for ML</h1>
			<p>Here are 10 <a id="_idIndexMarker181"/>principles that can be applied to your code to ensure the quality, robustness, and scalability of your code:</p>
			<ul>
				<li><strong class="bold">Modularity:</strong> It is better to have modular code than to have one big chunk. Modularity encourages reusability <a id="_idIndexMarker182"/>and facilitates upgrading by replacing the required components. To avoid needless complexity and repetition, follow this golden rule:<p class="author-quote">Two or more ML components should be paired only when one of them uses the other. If none of them uses each other, then pairing should be avoided.</p><p>An ML component that is not tightly paired with its environment can be more easily modified or replaced than a tightly paired component.</p></li>
				<li><strong class="bold">Single task dedicated functions:</strong> Functions are important building blocks of pipelines and the system, and <a id="_idIndexMarker183"/>they are small sections of code that are used to perform particular tasks. The purpose of functions is to avoid repetition of commands and enable reusable code. They can easily become a complex set of commands to facilitate tasks. For readable and reusable code, it is more efficient to have a single function dedicated to a single task instead of multiple tasks. It is better to have multiple functions than one long and complex function.</li>
				<li><strong class="bold">Structuring:</strong> Functions, classes, and <a id="_idIndexMarker184"/>statements should be structured in a readable, modular, and concise form. Nobody wants to see an error like <strong class="source-inline">Error 300</strong>. Structuring blocks of code and trying to limit the maximum levels of indentation for functions and classes can enhance the readability of the code.</li>
				<li><strong class="bold">Clean code:</strong> If you have to explain the code, it's not that good. Clean code is self-explanatory. It <a id="_idIndexMarker185"/>focuses on high readability, optimal modularity, reusability, non-repeatability, and optimal performance. Clean code reduces the cost of maintaining and upgrading your ML pipelines. It enables a team to perform efficiently and can be extended to other developers.<p>To understand this in depth, read <em class="italic">Clean Code: A Handbook of Agile Software Craftsmanship</em> by <strong class="bold">Robert C Martin</strong>.</p></li>
				<li><strong class="bold">Testing:</strong> It is vital to ensure the robustness of a system, and testing plays an important role in this. In general, testing <a id="_idIndexMarker186"/>extends to unit testing and acceptance testing. Unit testing is a method by which components of source code are tested for robustness with coerced data and usage methods to determine whether the component is fit for the production system. Acceptance tests are done to test the overall system to ensure the system realizes user requirements; end-to-end business flows are verified in real-time scenarios. Testing is vital to ensure the efficient working of code: "if it isn't tested, it is broken."</li>
			</ul>
			<p>To learn more about the implementation of unit testing, read this documentation: <a href="https://docs.python.org/3/library/unittest.html">https://docs.python.org/3/library/unittest.html</a>.</p>
			<ul>
				<li><strong class="bold">Version control (code, data and models):</strong> Git is used for version control of code in ML systems. The <a id="_idIndexMarker187"/>purpose of version control is to ensure that all the team members working on the system have access to up-to-date code and that code is not lost when there is a hardware failure. One rule of working with Git should be to not break the master (branch). This means when you have working code in the repository and you add new features or make improvements, you do this in a feature branch, which is merged to the master branch when the code is working and reviewed. Branches should be given a short descriptive name, such as feature/label-encoder. Branch naming and approval guidelines should be properly communicated and agreed upon with the team to avoid any complexity and unnecessary conflicts. Code review is done with pull requests to the repository of the code. Usually, it is best to review code in small sets, less than 400 lines. In practice, it often means one module or a submodule at a time.<p>Versioning of data is essential for ML systems as it helps us to keep track of which data was used for a particular version of code to generate a model. Versioning data can enable reproducing models and compliance with business needs and law. We can always backtrack <a id="_idIndexMarker188"/>and see the reason for certain actions taken by the ML system. Similarly, versioning of models (artifacts) is important for tracking which version of a model has generated certain results or actions for the ML system. We can also track or log parameters used for training a certain version of the model. This way, we can enable end-to-end traceability for model artifacts, data, and code. Version control for code, data, and models can enhance an ML system with great transparency and efficiency for the people developing and maintaining it.</p></li>
				<li><strong class="bold">Logging:</strong> In production, a logger is useful as it is possible to monitor and identify <a id="_idIndexMarker189"/>important information. The <strong class="source-inline">print</strong> statements are good for testing and debugging but not ideal for production. The logger contains information, especially system information, warnings, and errors, that are quite useful in the monitoring of production systems.</li>
				<li><strong class="bold">Error handling:</strong> Error handling is vital for handling edge cases, especially ones that are hard to anticipate. It is <a id="_idIndexMarker190"/>recommended to catch and handle exceptions even if you think you don't need to, as prevention is better than cure. Logging combined with exception handling can be an effective way of dealing with edge cases.</li>
				<li><strong class="bold">Readability:</strong> Code readability <a id="_idIndexMarker191"/>enables information transfer, code efficiency, and code maintainability. It can be achieved by following principles such as following industry-standard coding practices such as PEP-8 (<a href="https://www.python.org/dev/peps/pep-0008/">https://www.python.org/dev/peps/pep-0008/</a>) or the JavaScript standard style (depending on the language you are using). Readability is also increased by using docstrings. A docstring is a text that is written at the beginning of, for example, a function, describing what it does and possibly what it takes as input. In some cases, it is enough to have a one-liner explanation, such as this:<p class="source-code">def swap(a,b):</p><p class="source-code">"""Swaps the variables a and b. Returns the swapped variables""" </p><p class="source-code">return b, a</p><p>A longer docstring is <a id="_idIndexMarker192"/>needed for a more complex function. Explaining the arguments and returns is a good idea:</p><p class="source-code">def function_with_types_in_docstring(param1, param2): """Example function with types documented in the docstring.</p><p class="source-code">`PEP 484`_ type annotations are supported. If attribute, parameter, and</p><p class="source-code">return types are annotated according to `PEP 484`_, they do not need to be</p><p class="source-code">included in the docstring:</p><p class="source-code">Args:</p><p class="source-code">    param1 (int): The first parameter. </p><p class="source-code">    param2 (str): The second parameter.</p><p class="source-code">Returns:</p><p class="source-code">     bool: The return value. True for success, False otherwise.</p><p class="source-code">                              """</p></li>
			</ul>
			<p><strong class="bold">Commenting and documenting:</strong> Commenting and documentation are vital for maintaining sustainable code. It is not <a id="_idIndexMarker193"/>always possible to explain the code clearly. Comments can be useful in such cases to prevent confusion and explain the code. Comments can convey information such as copyright info, intent, clarification of code, possible warnings, and elaboration of code. Elaborate documentation of the system and modules can enable a team to perform efficiently, and the code and assets can be extended to other developers. For documentation, open source tools are available for documenting APIs such <a id="_idIndexMarker194"/>as Swagger (<a href="https://swagger.io">https://swagger.io</a>) and Read the <a id="_idIndexMarker195"/>Docs (<a href="https://readthedocs.org">https://readthedocs.org</a>). Using the right tools for documentation can enable efficiency and standardize knowledge for developers. </p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor062"/>What is good data for ML?</h1>
			<p>Good ML models are a result of training on good-quality data. Before proceeding to ML training, a <a id="_idIndexMarker196"/>pre-requisite is to have good-quality data. Therefore, we need to process the data to increase its quality. So, determining the quality of data is essential. Five characteristics will enable us to discern the quality of data, as follows:</p>
			<ul>
				<li><strong class="bold">Accuracy</strong>: Accuracy is a crucial <a id="_idIndexMarker197"/>characteristic of data quality, as having inaccurate data can lead to poor ML model performance and consequences in real life. To check the accuracy of the data, confirm whether the information represents a real-life situation or not. </li>
				<li><strong class="bold">Completeness</strong>: In most cases, incomplete <a id="_idIndexMarker198"/>information is unusable and can lead to incorrect outcomes if an ML model is trained on it. It is vital to check the comprehensiveness of the data.</li>
				<li><strong class="bold">Reliability</strong>: Contradictions or duplications in data can lead to the unreliability of the data. Reliability is a vital <a id="_idIndexMarker199"/>characteristic; trusting the data is essential, primarily when it is used to make real-life decisions using ML. To some degree, we can assess the reliability of data by examining bias and distribution. In case of any extremities, the data might not be reliable for ML training or might carry bias.  </li>
				<li><strong class="bold">Relevance</strong>: The relevance of data plays an essential role in contextualizing and determining <a id="_idIndexMarker200"/>if irrelevant information is being gathered. Having relevant data can enable appropriate decisions in real-life contexts using ML. </li>
				<li><strong class="bold">Timeliness</strong>: Obsolete or out-of-date information costs businesses time and money; having up-to-date <a id="_idIndexMarker201"/>information is vital in some cases and can improve the quality of data. Decisions enabled by ML using untimely data can be costly and can lead to wrong decisions.</li>
			</ul>
			<p>When these five characteristics are maximized, it ensures the highest data quality. With these principles in mind, let's delve into the implementation, where code meets data. </p>
			<p>Firstly, let's assess the data and process it to get it ready for ML training. To get started, clone the repository you <a id="_idIndexMarker202"/>imported to your Azure DevOps project (from GitHub):</p>
			<p class="source-code">git clone <a href="mailto:https://xxxxxxxxx@dev.azure.com">https://xxxxxxxxx@dev.azure.com</a>/xxxxx/Learn_MLOps/_git/Learn_MLOps</p>
			<p>Next, open your terminal and access the folder of the cloned repository and spin up the JupyterLab server for data processing. To do so, type the following command in the terminal:</p>
			<p class="source-code">jupyter lab</p>
			<p>This will automatically open a window in your browser at <strong class="source-inline">http://localhost:8888</strong> where you can code and execute the code on the JupyterLab interface. In the <strong class="source-inline">Code_meets_data_c3</strong> folder, there is a Python script (<strong class="source-inline">dataprocessing.py</strong>) and a <strong class="source-inline">.ipynb</strong> notebook (<strong class="source-inline">dataprocessing.ipynb</strong>); feel free to run any of these files or create a new notebook and follow the upcoming steps.</p>
			<p>We will perform computing for tasks as described in <em class="italic">Figure 3.6</em>. Data processing will be done locally on your PC, followed by ML training, deploying, and monitoring on compute targets in the cloud. This is to acquire experience of implementing models in various setups. In the rest of this chapter, we will do data processing (locally) to get the data to the best quality in order to do ML training (in the cloud, which is described in the next chapter).</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B16572_03_06.jpg" alt="Figure 3.6 – Computation locations for data and ML tasks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Computation locations for data and ML tasks</p>
			<p>To process raw data and <a id="_idIndexMarker203"/>get it ready for ML, you will do the compute and data processing on your local PC. We start by installing and importing the required packages and importing the raw dataset (as shown in the <strong class="source-inline">dataprocessing.ipynb</strong> and <strong class="source-inline">.py</strong> scripts). Python instructions in the notebooks must be executed in the existing notebook:</p>
			<p class="source-code">%matplotlib inline</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from matplotlib import pyplot as plt</p>
			<p class="source-code">from matplotlib.pyplot import figure</p>
			<p class="source-code">import seaborn as sns</p>
			<p class="source-code">from azureml.core import Workspace, Dataset</p>
			<p class="source-code">#import dataset</p>
			<p class="source-code">df = pd.read_csv('Dataset/weather_dataset_raw.csv')</p>
			<p>With this, you have imported the dataset into a pandas DataFrame, <strong class="source-inline">df</strong>, for further processing. </p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor063"/>Data preprocessing</h1>
			<p>Raw data cannot be directly passed to the ML model for training purposes. We have to refine or preprocess the <a id="_idIndexMarker204"/>data before training the ML model. To further analyze the imported data, we will perform a series of steps to preprocess the data into a suitable shape for the ML training. We start by assessing the quality of the data to check for accuracy, completeness, reliability, relevance, and timeliness. After this, we calibrate the required data and encode text into numerical data, which is ideal for ML training. Lastly, we will analyze the correlations and time series, and filter out irrelevant data for training ML models.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor064"/>Data quality assessment</h2>
			<p>To assess the quality <a id="_idIndexMarker205"/>of the data, we look for accuracy, completeness, reliability, relevance, and timeliness. Firstly, let's check if the data is complete and reliable by assessing the formats, cumulative statistics, and anomalies such as missing data. We use pandas functions as follows:</p>
			<p class="source-code">df.describe()</p>
			<p>By using the <strong class="source-inline">describe</strong> function, we can observe descriptive statistics in the output as follows:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B16572_03_07.jpg" alt="Figure 3.7 – Descriptive statistics of the DataFrame&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Descriptive statistics of the DataFrame</p>
			<p>Some observations can be made to conclude the data is coherent, and relevant as it depicts real-life statistics such as a mean temperature of ~11 C and a wind speed of ~10 kmph. Minimum temperatures in Finland tend to reach around ~-21 C, and there is an average visibility of 10 km. Facts like these depict the relevance and data origin conditions. Now, let's <a id="_idIndexMarker206"/>observe the column formats:</p>
			<p class="source-code">df.dtypes</p>
			<p>Here are the formats of each column:</p>
			<ul>
				<li><strong class="source-inline">S_No</strong>                                           <strong class="source-inline">int64</strong></li>
				<li><strong class="source-inline">Timestamp</strong>                                <strong class="source-inline">object</strong></li>
				<li><strong class="source-inline">Location</strong>                                     <strong class="source-inline">object</strong></li>
				<li><strong class="source-inline">Temperature_C</strong>                         <strong class="source-inline">float64</strong></li>
				<li><strong class="source-inline">Apparent_Temperature_C</strong>      <strong class="source-inline">float64</strong></li>
				<li><strong class="source-inline">Humidity</strong>                                    <strong class="source-inline">float64</strong></li>
				<li><strong class="source-inline">Wind_speed_kmph</strong>                  <strong class="source-inline">float64</strong></li>
				<li><strong class="source-inline">Wind_bearing_degrees</strong>           <strong class="source-inline">int64</strong></li>
				<li><strong class="source-inline">Visibility_km</strong>                              <strong class="source-inline">float64</strong></li>
				<li><strong class="source-inline">Pressure_millibars</strong>                    <strong class="source-inline">float64</strong></li>
				<li><strong class="source-inline">Weather_conditions</strong>                <strong class="source-inline">object</strong></li>
				<li><strong class="source-inline">dtype:</strong>                                         <strong class="source-inline">object</strong></li>
			</ul>
			<p>Most of the columns are numerical (<strong class="source-inline">float</strong> and <strong class="source-inline">int</strong>), as expected. The <strong class="source-inline">Timestamp</strong> column is in <strong class="source-inline">object</strong> format, which needs to be changed to <strong class="source-inline">DateTime</strong> format:</p>
			<p class="source-code">df['Timestamp'] = pd.to_datetime(df['Timestamp'])</p>
			<p>Using pandas' <strong class="source-inline">to_datetime</strong> function, we convert <strong class="source-inline">Timestamp</strong> to <strong class="source-inline">DateTime</strong> format. Next, let's see if there are any null values. We use pandas' <strong class="source-inline">isnull</strong> function to check this: </p>
			<p class="source-code">df.isnull().values.any()</p>
			<p>Upon checking for any null values, if null values are discovered, as a next step the calibration of missing data is essential.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor065"/>Calibrating missing data</h2>
			<p>It is not ideal to have <a id="_idIndexMarker207"/>missing values in the data as it is a <a id="_idIndexMarker208"/>sign of poor data quality. Missing data or values can be replaced using various techniques without compromising the correctness and reliability of data. After inspecting the data we have been working on, some missing values are observed. We use the <strong class="source-inline">Forward fill</strong> method to handle missing data:</p>
			<p class="source-code">df['Weather_conditions'].fillna(method='ffill', inplace=True, axis=0)</p>
			<p><strong class="source-inline">NaN</strong> or null values have only been observed in the <strong class="source-inline">Weather_conditions</strong> column. We replace the <strong class="source-inline">NaN</strong> values by using the <strong class="source-inline">fillna()</strong> method from pandas and the forward fill (<strong class="source-inline">ffill</strong>) method. As weather is progressive, it is likely to replicate the previous event in the data. Hence, we use the forward fill method, which replicates the last observed non-null value until another non-null value is encountered.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor066"/>Label encoding </h2>
			<p>As the machines do not <a id="_idIndexMarker209"/>understand human language or text, all <a id="_idIndexMarker210"/>the text has to be converted into numbers. Before that, let's process the text. We have a <strong class="source-inline">Weather_conditons</strong> column in text with values or labels such as <strong class="source-inline">rain</strong>, <strong class="source-inline">snow</strong>, and <strong class="source-inline">clear</strong>. These values are found using pandas' <strong class="source-inline">value_counts()</strong> function, as follows: </p>
			<p class="source-code">df['Weather_conditions'].value_counts()</p>
			<p><strong class="source-inline">Weather_conditions</strong> can be simplified by categorizing the column label into two labels, <strong class="source-inline">rain</strong> or <strong class="source-inline">no_rain</strong>. Forecasting in these two categories will enable us to solve the business problem for the cargo company:</p>
			<p class="source-code">df['Weather_conditions'].replace({"snow": "no_rain",  "clear": "no_rain"}, inplace=True)</p>
			<p>This will replace both <strong class="source-inline">snow</strong> and <strong class="source-inline">clear</strong> values with <strong class="source-inline">no_rain</strong> as both conditions imply no rain conditions at the port. Now that labels are processed, we can convert the <strong class="source-inline">Weather_conditions</strong> column into a machine-readable form or numbers using <strong class="bold">label encoding</strong>. Label encoding is a <a id="_idIndexMarker211"/>method of converting categorical values into a machine-readable form or numbers by assigning each category a unique value. As we have only two categories, <strong class="source-inline">rain</strong> and <strong class="source-inline">no_rain</strong>, label encoding can be efficient as it converts these values to 0 and 1. If there are more than two values, <strong class="bold">one-hot encoding</strong> is a good <a id="_idIndexMarker212"/>choice because assigning incremental numbers to categorical variables can give the variables higher priority or numerical bias during training. One-hot encoding prevents bias or higher preference for any variable, ensuring neutral privileges to each value of categorical variables. In our case, as we have only two categorical variables, we perform label encoding using scikit-learn as follows:</p>
			<p class="source-code">from sklearn.preprocessing import LabelEncoder</p>
			<p class="source-code">le = LabelEncoder()</p>
			<p class="source-code">y = df['Weather_conditions']</p>
			<p class="source-code">y = le.fit_transform(y)</p>
			<p>Here, we import the <strong class="source-inline">LabelEncoder()</strong> function, which will encode the <strong class="source-inline">Weather_conditions</strong> column into 0s and 1s using the <strong class="source-inline">fit_transform()</strong> method. We can do this by replacing the <a id="_idIndexMarker213"/>previous textual column with a label encoded or machine-readable form to column <strong class="source-inline">Weather_condition</strong> as follows:</p>
			<p class="source-code">y = pd.DataFrame(data=y, columns=["Weather_condition"])</p>
			<p class="source-code">df = pd.concat([df,y], axis=1)</p>
			<p class="source-code">df.drop(['Weather_conditions'], axis=1, inplace=True)</p>
			<p>Here, we concatenate our new label-encoded or machine-readable <strong class="source-inline">Weather_condition</strong> column to the DataFrame and drop the previous non-machine readable or textual <strong class="source-inline">Weather_conditions</strong> column. Data is now in machine-readable form and ready for further processing. You can check the transformed data by executing <strong class="source-inline">df.head()</strong> in the notebook (optional).</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor067"/>New feature – Future_weather_condition</h2>
			<p>As we are tasked with <a id="_idIndexMarker214"/>forecasting weather conditions 4 hours in the future, we create a new feature named <strong class="source-inline">Future_weather_condition</strong> by shifting <strong class="source-inline">Current_weather_condition</strong> by four rows, as each row is recorded with a time gap of an hour. <strong class="source-inline">Future_weather_condition</strong> is the label of future<a id="_idIndexMarker215"/> weather conditions 4 hours ahead. We will use this new feature as a dependent variable to forecast using ML:</p>
			<p class="source-code">df['Future_weather_condition'] = df.Current_weather_condition.shift(4, axis = 0) </p>
			<p class="source-code">df.dropna(inplace=True)</p>
			<p>We will use pandas' <strong class="source-inline">dropna()</strong> function on the DataFrame to discard or drop null values, because some rows will have null values due to shifting to a new column.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor068"/>Data correlations and filtering</h2>
			<p>Now that the <a id="_idIndexMarker216"/>data is fully machine readable, we <a id="_idIndexMarker217"/>can observe the correlations using the <strong class="bold">Pearson correlation coefficient</strong> to observe <a id="_idIndexMarker218"/>how every single column is related to the other columns. Data and feature correlation is a vital step before feature selection for ML model training, especially when the features are continuous, like in our case. The Pearson correlation coefficient is a statistical linear correlation between each variable (<em class="italic">X</em> and <em class="italic">y</em>) that produces a value between <em class="italic">+1</em> and <em class="italic">-1</em>. A value of <em class="italic">+1</em> is a positive linear correlation, <em class="italic">-1</em> is a negative linear correlation, and <em class="italic">0</em> is no linear correlation. It can be used to understand the relationship between continuous variables, though it is worth noting that Pearson correlation does not mean causation. We can observe Pearson correlation coefficients for our data using pandas as follows:</p>
			<p class="source-code">df.corr(method="pearson")</p>
			<p class="source-code"># Visualizing using heatmap</p>
			<p class="source-code">corrMatrix = df.corr()</p>
			<p class="source-code">sn.heatmap(corrMatrix, annot=True)</p>
			<p class="source-code">plt.show()</p>
			<p>Here is the heatmap of the <strong class="source-inline">Pearson</strong> correlation results:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B16572_03_08.jpg" alt="Figure 3.8 – Heatmap of correlation scores&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – Heatmap of correlation scores</p>
			<p>From the <a id="_idIndexMarker219"/>heatmap in <em class="italic">Figure 3.8</em>, we can see <a id="_idIndexMarker220"/>that the <strong class="source-inline">Temperature</strong> and <strong class="source-inline">Apparent_Temperature_C</strong> coefficient is <strong class="source-inline">0.99</strong>. <strong class="source-inline">S_No</strong> (Serial number) is a continuous value, which is more or less like an incremental index for a DataFrame and can be discarded or filtered out as it does not provide great value. Hence both <strong class="source-inline">Apparent_Temperature</strong> and <strong class="source-inline">S_No</strong> are dropped or filtered. Now let's observe our dependent variable, <strong class="source-inline">Future_weather_condition</strong>, and its correlation with other independent variables:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B16572_03_09.jpg" alt="Figure 3.9 – Pearson correlation for Future_weather_condition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Pearson correlation for Future_weather_condition</p>
			<p>Anything <a id="_idIndexMarker221"/>between 0.5 and 1.0 has a positive correlation and anything between -0.5 and -1.0 has a negative <a id="_idIndexMarker222"/>correlation. Judging from the graph, there is a positive correlation with <strong class="source-inline">Current_weather_condition</strong>, and <strong class="source-inline">Temperature_C</strong> is also positively correlated with <strong class="source-inline">Future_weather_c</strong>.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor069"/>Time series analysis</h2>
			<p>As the <a id="_idIndexMarker223"/>temperature is a continuous variable, it is worth <a id="_idIndexMarker224"/>observing its progression over time. We can visualize a time series plot using matplotlib as follows:</p>
			<p class="source-code">time = df['Timestamp]</p>
			<p class="source-code">temp = df['Temperature_C']</p>
			<p class="source-code"># plot graph</p>
			<p class="source-code">plt.plot(time, temp)</p>
			<p class="source-code">plt.show()</p>
			<p>Here's the resulting plot:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B16572_03_10.jpg" alt=" Figure 3.10 – Time series progression of Temperature in C&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 3.10 – Time series progression of Temperature in C</p>
			<p>After assessing the time series progression of temperature in <em class="italic">Figure 3.10</em>, we can see that it depicts a <a id="_idIndexMarker225"/>stationary pattern since the mean, variance, and <a id="_idIndexMarker226"/>covariance are observed to be stationary over time. Stationary behaviors can be trends, cycles, random walks, or a combination of the three. It makes sense, as temperature changes over seasons and follows seasonal patterns. This brings us to the end of data analysis and processing; we are now ready to register the processed data in the workspace before proceeding to train the ML model. </p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor070"/>Data registration and versioning</h1>
			<p>It is vital to register <a id="_idIndexMarker227"/>and version the data in the workspace before <a id="_idIndexMarker228"/>starting ML training as it enables us to backtrack our experiments or ML models to the source of data used for training the models. The purpose of versioning the data is to backtrack at any point, to replicate a model's training, or to explain the workings of the model as per the inference or testing data for explaining the ML model. For these reasons, we will register the processed data and version it to use it for our ML pipeline. We will register and version the processed data to the Azure Machine Learning workspace using the Azure Machine Learning SDK as follows:</p>
			<p class="source-code">subscription_id = '---insert your subscription ID here----'</p>
			<p class="source-code">resource_group = 'Learn_MLOps'</p>
			<p class="source-code">workspace_name = 'MLOps_WS' </p>
			<p class="source-code">workspace = Workspace(subscription_id, resource_group, workspace_name)</p>
			<p>Fetch <a id="_idIndexMarker229"/>your <strong class="source-inline">subscription ID</strong>, <strong class="source-inline">resource_group</strong> and <strong class="source-inline">workspace_name</strong> from the Azure Machine Learning portal, as <a id="_idIndexMarker230"/>shown in <em class="italic">Figure 3.11</em>:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B16572_03_11.jpg" alt="Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and Workspace name)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and Workspace name)</p>
			<p>By requesting the workspace credentials, a workspace object is obtained. When running the <strong class="source-inline">Workspace()</strong> function, your notebook will be connected to the Azure platform. You will be prompted to click on an authentication link and provide a random code and the Azure account details. After that, the script will confirm the authentication. Using the workspace object, we access the default data store and upload the required data files to the data store on Azure Blob Storage connected to the workspace:</p>
			<p class="source-code"># get the default datastore linked to upload prepared data</p>
			<p class="source-code">datastore = workspace.get_default_datastore()</p>
			<p class="source-code">#upload the local file from src_dir to target_path in datastore</p>
			<p class="source-code">datastore.upload(src_dir='Dataset', target_path='data')</p>
			<p class="source-code">dataset =  /</p>
			<p class="source-code">Dataset.Tablular.from_delimited_files(datastore.path('data/weather_dataset_processed.csv'))</p>
			<p><strong class="source-inline">Tabular.from_delimited_files()</strong> may cause a failure in Linux or MacOS machines that do not have .NET Core 2.1 installed. For correct installation of this dependency, follow <a id="_idIndexMarker231"/>these instructions: <a href="https://docs.microsoft.com/en-us/dotnet/core/install/linux">https://docs.microsoft.com/en-us/dotnet/core/install/linux</a>. After successfully executing the <a id="_idIndexMarker232"/>preceding commands, you will upload the data file to the data store and see the result shown in <em class="italic">Figure 3.12</em>. You can preview the dataset from the datastore as follows:</p>
			<p class="source-code"># preview the first 3 rows of the dataset from the datastore</p>
			<p class="source-code">dataset.take(3).to_pandas_dataframe()</p>
			<p>When the data is uploaded to the data store, then we will register the dataset to the workspace and version it as follows:</p>
			<p class="source-code">weather_ds = dataset.register(workspace=workspace, name=weather_ds_portofTurku, description='processed weather data')</p>
			<p>The <strong class="source-inline">register(...)</strong> function registers the dataset to the workspace, as shown in <em class="italic">Figure 3.12</em>. For detailed documentation, visit https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#register-datasets:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B16572_03_12.jpg" alt="Figure 3.12 – Processed dataset registered in the ML workspace&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Processed dataset registered in the ML workspace</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor071"/>Toward the ML Pipeline</h1>
			<p>So far, we have processed the data by working on irregularities such as missing data, selected features by observing correlations, created new features, and finally ingested and versioned the processed data to the Machine learning workspace. There are two ways to fuel the data ingestion for ML model training in the ML pipeline. One way is from the central storage (where all your raw data is stored) and the second way is using a feature store. As knowledge is power, Let's get to know the use of the feature store before we move to the ML pipeline.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor072"/>Feature Store </h2>
			<p>A feature store compliments the central storage by storing important features and make them available for training or inference. A feature store is a store where you transform raw data into useful features that ML models can use directly to train and infer to make predictions. Raw Data typically comes from various data sources, which are structured, unstructured, streaming, batch, and real-time. It all needs to get pulled, transformed (using a feature pipeline), and stored somewhere, and that somewhere can be the feature store. The feature store then takes the data and makes it available for consumption. Data scientists tend to duplicate work (especially data processing). It can be avoided if we have a centralized feature store. Feature store allows data scientists to efficiently share and reuse features with other teams and thereby increase their productivity as they don't have to pre-process features from scratch. </p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B16572_03_13.jpg" alt="Figure 3.13: Feature store workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: Feature store workflow</p>
			<p>As we can see in <em class="italic">Figure 3.13</em>, a <strong class="bold">Feature Store</strong> is using a <strong class="bold">Feature Pipeline</strong> connected to a <strong class="bold">Central Storage</strong> (which stores data from multiple sources) to transform and store raw data into useful features for ML training. The features stored in the feature store can be retrieved for training, serving, or discovering insights or trends. Here are some benefits of using a feature Store:</p>
			<ul>
				<li>Efficient <strong class="bold">Feature Engineering</strong> for <strong class="bold">Training Data</strong> </li>
				<li>Avoid unnecessary data pre-processing before training </li>
				<li>Avoid repetitive feature engineering </li>
				<li>Features available for quick inferencing (testing) </li>
				<li>System support for serving of features </li>
				<li>Exploratory Data Analysis by feature Store </li>
				<li>Opportunity to reuse models features </li>
				<li>Quick Queries on features </li>
				<li>Reproducibility for training data sets </li>
				<li>Monitoring feature drift in production (we will learn about feature drift in <a href="B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222"><em class="italic">Chapter 12</em></a>, <em class="italic">Model Serving and Monitoring</em>) </li>
				<li>Features available for data drift monitoring </li>
			</ul>
			<p>It is good to know the advantages of a feature store as it can be useful to fuel the ML pipeline (especially the data ingestion step), however not suitable for all cases. It depends on your use case. For our use case implementation, we will not use feature store but proceed to liaisoning data directly from central storage where we have preprocessed and registered the datasets we need for training and testing. With ingested and versioned data, you are set to proceed towards building your ML Pipeline. The ML pipeline will enable further feature engineering, feature scaling, curating training, and testing datasets that will be used to train ML models and tune hyperparameters for machine learning training. The ML pipeline and functionalities will be performed over cloud computing resources, unlike locally on your computer as we did in this chapter. It will be purely cloud-based.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor073"/>Summary</h1>
			<p>In this chapter, we have learned how to identify a suitable ML solution to a business problem and categorize operations to implement suitable MLOps. We set up our tools, resources, and development environment. 10 principles of source code management were discussed, followed by data quality characteristics. Congrats! So far, you have implemented a critical building block of the MLOps workflow – data processing and registering processed data to the workspace. Lastly, we had a glimpse into the essentials of the ML pipeline.</p>
			<p>In the next chapter, you will do the most exciting part of MLOps: building the ML pipeline. Let's press on!</p>
		</div>
	</body></html>