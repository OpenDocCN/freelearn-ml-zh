- en: Chapter 10. Estimating Projective Relations in Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章.在图像中估计投影关系
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Calibrating a camera
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 校准相机
- en: Computing the fundamental matrix of an image pair
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图像对的基本矩阵
- en: Matching images using a random sample consensus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机样本一致性匹配图像
- en: Computing a homography between two images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算两个图像之间的单应性
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Images are generally produced using a digital camera, which captures a scene
    by projecting light going through its lens onto an image sensor. The fact that
    an image is formed by the projection of a 3D scene onto a 2D plane implies the
    existence of important relationships between a scene and its image and between
    different images of the same scene. Projective geometry is the tool that is used
    to describe and characterize, in mathematical terms, the process of image formation.
    In this chapter, we will introduce you to some of the fundamental projective relations
    that exist in multiview imagery and explain how these can be used in computer
    vision programming. You will learn how matching can be made more accurate through
    the use of projective constraints and how a mosaic from multiple images can be
    composited using two-view relations. Before we start the recipes, let's explore
    the basic concepts related to scene projection and image formation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常使用数字相机产生，该相机通过将穿过其镜头的光线投射到图像传感器上来捕获场景。一个图像是通过将3D场景投影到2D平面上形成的，这暗示了场景与其图像以及同一场景的不同图像之间存在重要的关系。投影几何是用于用数学术语描述和表征图像形成过程的工具。在本章中，我们将向您介绍多视图图像中存在的一些基本投影关系，并解释这些关系如何在计算机视觉编程中使用。您将学习如何通过使用投影约束来提高匹配的准确性，以及如何使用双视图关系将多图像拼贴组合在一起。在我们开始食谱之前，让我们探索与场景投影和图像形成相关的基本概念。
- en: Image formation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像形成
- en: 'Fundamentally, the process used to produce images has not changed since the
    beginning of photography. The light coming from an observed scene is captured
    by a camera through a frontal **aperture**; the captured light rays hit an **image
    plane** (or an **image sensor**) located at the back of the camera. Additionally,
    a lens is used to concentrate the rays coming from the different scene elements.
    This process is illustrated by the following figure:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，自摄影术开始以来，用于产生图像的过程没有改变。来自观察场景的光线通过一个正面的**孔径**被相机捕获；捕获的光线击中位于相机后部的**图像平面**（或**图像传感器**）。此外，使用镜头来集中来自不同场景元素的光线。这个过程如下图所示：
- en: '![Image formation](img/00161.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图像形成](img/00161.jpeg)'
- en: 'Here, **do** is the distance from the lens to the observed object, **di** is
    the distance from the lens to the image plane, and **f** is the focal length of
    the lens. These quantities are related by the so-called thin lens equation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**do** 是从镜头到观察对象的距离，**di** 是从镜头到图像平面的距离，**f** 是镜头的焦距。这些量通过所谓的薄透镜方程相关联：
- en: '![Image formation](img/00162.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图像形成](img/00162.jpeg)'
- en: 'In computer vision, this camera model can be simplified in a number of ways.
    First, we can neglect the effect of the lens by considering that we have a camera
    with an infinitesimal aperture since, in theory, this does not change the image
    appearance. (However, by doing so, we ignore the focusing effect by creating an
    image with an infinite **depth of field**.) In this case, therefore, only the
    central ray is considered. Second, since most of the time we have `do>>di`, we
    can assume that the image plane is located at the focal distance. Finally, we
    can note from the geometry of the system that the image on the plane is inverted.
    We can obtain an identical but upright image by simply positioning the image plane
    in front of the lens. Obviously, this is not physically feasible, but from a mathematical
    point of view, this is completely equivalent. This simplified model is often referred
    to as the **pin-hole camera** model, and it is represented as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，这个相机模型可以通过多种方式简化。首先，我们可以通过考虑我们有一个具有无穷小孔径的相机来忽略镜头的影响，因为从理论上讲，这不会改变图像的外观。（然而，这样做会通过创建具有无限**景深**的图像而忽略聚焦效果。）在这种情况下，因此，只考虑中心光线。其次，由于大多数时候我们有
    `do>>di`，我们可以假设图像平面位于焦距处。最后，我们可以从系统的几何学中注意到，平面上的图像是倒置的。我们可以通过简单地将图像平面放置在镜头前方来获得一个相同但正立的图像。显然，这在物理上是不切实际的，但从数学角度来看，这是完全等价的。这个简化的模型通常被称为**针孔相机**模型，其表示如下：
- en: '![Image formation](img/00163.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图像形成](img/00163.jpeg)'
- en: 'From this model, and using the law of similar triangles, we can easily derive
    the basic projective equation that relates a pictured object with its image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个模型出发，并使用相似三角形的定律，我们可以轻松推导出将图像中的物体与其图像相关联的基本投影方程：
- en: '![Image formation](img/00164.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图像形成](img/00164.jpeg)'
- en: The size (**hi**) of the image of an object (of height **ho**) is therefore
    inversely proportional to its distance (**do**) from the camera, which is naturally
    true. In general, this relation describes where a 3D scene point will be projected
    on the image plane given the geometry of the camera.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个物体（高度为**ho**）的图像大小（**hi**）与其与摄像机的距离（**do**）成反比，这是自然而然的事情。一般来说，这种关系描述了在给定摄像机几何形状的情况下，三维场景点将在图像平面上投影的位置。
- en: Calibrating a camera
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标定摄像机
- en: From the introduction of this chapter, we learned that the essential parameters
    of a camera under the pin-hole model are its focal length and the size of the
    image plane (which defines the **field of view** of the camera). Also, since we
    are dealing with digital images, the number of pixels on the image plane (its
    **resolution**) is another important characteristic of a camera. Finally, in order
    to be able to compute the position of an image's scene point in pixel coordinates,
    we need one additional piece of information. Considering the line coming from
    the focal point that is orthogonal to the image plane, we need to know at which
    pixel position this line pierces the image plane. This point is called the **principal
    point**. It might be logical to assume that this principal point is at the center
    of the image plane, but in practice, this point might be off by a few pixels depending
    on the precision at which the camera has been manufactured.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章的介绍中，我们了解到在针孔模型下，摄像机的关键参数是其焦距和图像平面的尺寸（这定义了摄像机的**视场**）。此外，由于我们处理的是数字图像，图像平面上像素的数量（其**分辨率**）也是摄像机的一个重要特征。最后，为了能够计算图像场景点的像素坐标，我们需要额外的一块信息。考虑到来自焦点的垂直于图像平面的直线，我们需要知道这条直线在哪个像素位置穿透图像平面。这个点被称为**主点**。从逻辑上讲，这个主点可能位于图像平面的中心，但在实际操作中，这个点可能因为摄像机制造的精度问题而偏离几个像素。
- en: Camera calibration is the process by which the different camera parameters are
    obtained. One can obviously use the specifications provided by the camera manufacturer,
    but for some tasks, such as 3D reconstruction, these specifications are not accurate
    enough. Camera calibration will proceed by showing known patterns to the camera
    and analyzing the obtained images. An optimization process will then determine
    the optimal parameter values that explain the observations. This is a complex
    process that has been made easy by the availability of OpenCV calibration functions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像机标定是一个获取不同摄像机参数的过程。显然，可以使用摄像机制造商提供的规格，但对于某些任务，例如三维重建，这些规格可能不够精确。摄像机标定将通过向摄像机展示已知图案并分析获得的图像来进行。然后，一个优化过程将确定解释观察结果的参数值。这是一个复杂的过程，但由于OpenCV标定函数的可用性，这个过程变得简单易行。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: To calibrate a camera, the idea is to show it a set of scene points for which
    their 3D positions are known. Then, you need to observe where these points project
    on the image. With the knowledge of a sufficient number of 3D points and associated
    2D image points, the exact camera parameters can be inferred from the projective
    equation. Obviously, for accurate results, we need to observe as many points as
    possible. One way to achieve this would be to take one picture of a scene with
    many known 3D points, but in practice, this is rarely feasible. A more convenient
    way is to take several images of a set of some 3D points from different viewpoints.
    This approach is simpler but requires you to compute the position of each camera
    view in addition to the computation of the internal camera parameters, which fortunately
    is feasible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要标定一个摄像机，想法是向它展示一组已知三维位置的场景点。然后，你需要观察这些点在图像上的投影位置。有了足够数量的三维点和相关的二维图像点的知识，可以从投影方程中推断出精确的摄像机参数。显然，为了获得准确的结果，我们需要观察尽可能多的点。实现这一目标的一种方法是对一个包含许多已知三维点的场景拍摄一张照片，但在实际操作中，这很少可行。更方便的方法是从不同的视角拍摄一组三维点的多张照片。这种方法更简单，但除了计算内部摄像机参数外，还需要计算每个摄像机视图的位置，幸运的是这是可行的。
- en: 'OpenCV proposes that you use a chessboard pattern to generate the set of 3D
    scene points required for calibration. This pattern creates points at the corners
    of each square, and since this pattern is flat, we can freely assume that the
    board is located at `Z=0`, with the *X* and *Y* axes well-aligned with the grid.
    In this case, the calibration process simply consists of showing the chessboard
    pattern to the camera from different viewpoints. Here is one example of a `6x4`
    calibration pattern image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00165.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'The good thing is that OpenCV has a function that automatically detects the
    corners of this chessboard pattern. You simply provide an image and the size of
    the chessboard used (the number of horizontal and vertical inner corner points).
    The function will return the position of these chessboard corners on the image.
    If the function fails to find the pattern, then it simply returns `false`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output parameter, `imageCorners`, will simply contain the pixel coordinates
    of the detected inner corners of the shown pattern. Note that this function accepts
    additional parameters if you need to tune the algorithm, which are not discussed
    here. There is also a special function that draws the detected corners on the
    chessboard image, with lines connecting them in a sequence:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following image is obtained:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00166.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: The lines that connect the points show the order in which the points are listed
    in the vector of detected image points. To perform a calibration, we now need
    to specify the corresponding 3D points. You can specify these points in the units
    of your choice (for example, in centimeters or in inches); however, the simplest
    is to assume that each square represents one unit. In that case, the coordinates
    of the first point would be `(0,0,0)` (assuming that the board is located at a
    depth of `Z=0`), the coordinates of the second point would be `(1,0,0)`, and so
    on, the last point being located at `(5,3,0)`. There are a total of `24` points
    in this pattern, which is too small to obtain an accurate calibration. To get
    more points, you need to show more images of the same calibration pattern from
    various points of view. To do so, you can either move the pattern in front of
    the camera or move the camera around the board; from a mathematical point of view,
    this is completely equivalent. The OpenCV calibration function assumes that the
    reference frame is fixed on the calibration pattern and will calculate the rotation
    and translation of the camera with respect to the reference frame.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now encapsulate the calibration process in a `CameraCalibrator` class.
    The attributes of this class are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that the input vectors of the scene and image points are in fact made
    of `std::vector` of point instances; each vector element is a vector of the points
    from one view. Here, we decided to add the calibration points by specifying a
    vector of the chessboard image filename as input:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first loop inputs the 3D coordinates of the chessboard, and the corresponding
    image points are the ones provided by the `cv::findChessboardCorners` function.
    This is done for all the available viewpoints. Moreover, in order to obtain a
    more accurate image point location, the `cv::cornerSubPix` function can be used,
    and as the name suggests, the image points will then be localized at a subpixel
    accuracy. The termination criterion that is specified by the `cv::TermCriteria`
    object defines the maximum number of iterations and the minimum accuracy in subpixel
    coordinates. The first of these two conditions that is reached will stop the corner
    refinement process.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个循环输入棋盘的3D坐标，相应的图像点是`cv::findChessboardCorners`函数提供的点。这适用于所有可用的视角。此外，为了获得更精确的图像点位置，可以使用`cv::cornerSubPix`函数，正如其名称所暗示的，图像点将被定位在亚像素精度。由`cv::TermCriteria`对象指定的终止准则定义了最大迭代次数和亚像素坐标中的最小精度。这两个条件中先达到的一个将停止角细化过程。
- en: 'When a set of chessboard corners have been successfully detected, these points
    are added to our vectors of the image and scene points using our `addPoints` method.
    Once a sufficient number of chessboard images have been processed (and consequently,
    a large number of 3D scene point / 2D image point correspondences are available),
    we can initiate the computation of the calibration parameters as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当一组棋盘角成功检测到后，这些点将使用我们的`addPoints`方法添加到图像点和场景点的向量中。一旦处理了足够数量的棋盘图像（因此，有大量的3D场景点/2D图像点对应关系可用），我们就可以开始计算校准参数，如下所示：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In practice, 10 to 20 chessboard images are sufficient, but these must be taken
    from different viewpoints at different depths. The two important outputs of this
    function are the camera matrix and the distortion parameters. These will be described
    in the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，10到20张棋盘图像就足够了，但这些图像必须从不同视角和不同深度拍摄。这个函数的两个重要输出是相机矩阵和畸变参数。这些将在下一节中描述。
- en: How it works...
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In order to explain the result of the calibration, we need to go back to the
    figure in the introduction, which describes the pin-hole camera model. More specifically,
    we want to demonstrate the relationship between a point in 3D at the position
    (X,Y,Z) and its image (x,y) on a camera specified in pixel coordinates. Let''s
    redraw this figure by adding a reference frame that we position at the center
    of the projection as seen here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释校准结果，我们需要回到引言中的图，该图描述了针孔相机模型。更具体地说，我们想展示3D空间中位置为(X,Y,Z)的点与其在指定像素坐标的相机上的图像(x,y)之间的关系。让我们通过添加一个参考框架来重新绘制这个图，我们将这个框架放置在投影中心的此处：
- en: '![How it works...](img/00167.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00167.jpeg)'
- en: 'Note that the *y* axis is pointing downward to get a coordinate system compatible
    with the usual convention that places the image origin at the upper-left corner.
    We learned previously that the point **(X,Y,Z)** will be projected onto the image
    plane at `(fX/Z,fY/Z)`. Now, if we want to translate this coordinate into pixels,
    we need to divide the 2D image position by the pixel''s width (`px`) and height
    (`py`), respectively. Note that by dividing the focal length given in world units
    (generally given in millimeters) by `px`, we obtain the focal length expressed
    in (horizontal) pixels. Let''s then define this term as `fx`. Similarly, `fy =f/py`
    is defined as the focal length expressed in vertical pixel units. Therefore, the
    complete projective equation is as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*y*轴向下指向，以获得与通常将图像原点放置在左上角的惯例兼容的坐标系。我们之前了解到，点**(X,Y,Z)**将被投影到图像平面上，位置为`(fX/Z,fY/Z)`。现在，如果我们想将这个坐标转换为像素，我们需要分别除以像素的宽度(`px`)和高度(`py`)。注意，通过将以世界单位（通常以毫米为单位）给出的焦距除以`px`，我们得到以（水平）像素表示的焦距。然后，我们定义这个术语为`fx`。同样，`fy
    =f/py`被定义为以垂直像素单位表示的焦距。因此，完整的投影方程如下：
- en: '![How it works...](img/00168.jpeg)![How it works...](img/00169.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00168.jpeg)![它是如何工作的...](img/00169.jpeg)'
- en: Recall that (u[0],v[0]) is the principal point that is added to the result in
    order to move the origin to the upper-left corner of the image. These equations
    can be rewritten in the matrix form through the introduction of **homogeneous
    coordinates**, in which 2D points are represented by 3-vectors and 3D points are
    represented by 4-vectors (the extra coordinate is simply an arbitrary scale factor,
    `S`, that needs to be removed when a 2D coordinate needs to be extracted from
    a homogeneous 3-vector).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，(u[0],v[0])是添加到结果中以将原点移动到图像右上角的主点。这些方程可以通过引入**齐次坐标**来重写为矩阵形式，其中二维点由3-向量表示，三维点由4-向量表示（额外的坐标只是一个任意比例因子`S`，当需要从齐次3-向量中提取二维坐标时需要移除）。
- en: 'Here is the rewritten projective equation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是重写的投影方程：
- en: '![How it works...](img/00170.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/00170.jpeg)'
- en: The second matrix is a simple projection matrix. The first matrix includes all
    of the camera parameters, which are called the intrinsic parameters of the camera.
    This `3x3` matrix is one of the output matrices returned by the `cv::calibrateCamera`
    function. There is also a function called `cv::calibrationMatrixValues` that returns
    the value of the intrinsic parameters given by a calibration matrix.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个矩阵是一个简单的投影矩阵。第一个矩阵包括所有摄像机参数，这些参数被称为摄像机的内在参数。这个`3x3`矩阵是`cv::calibrateCamera`函数返回的输出矩阵之一。还有一个名为`cv::calibrationMatrixValues`的函数，它返回由校准矩阵给出的内在参数值。
- en: 'More generally, when the reference frame is not at the projection center of
    the camera, we will need to add a rotation vector (a `3x3` matrix) and a translation
    vector (a `3x1` matrix). These two matrices describe the rigid transformation
    that must be applied to the 3D points in order to bring them back to the camera
    reference frame. Therefore, we can rewrite the projection equation in its most
    general form:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，当参考系不在摄像机的投影中心时，我们需要添加一个旋转向量（一个`3x3`矩阵）和一个平移向量（一个`3x1`矩阵）。这两个矩阵描述了必须应用于3D点以将它们带回到摄像机参考系的刚性变换。因此，我们可以将投影方程重写为其最一般的形式：
- en: '![How it works...](img/00171.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/00171.jpeg)'
- en: Remember that in our calibration example, the reference frame was placed on
    the chessboard. Therefore, there is a rigid transformation (made of a rotation
    component represented by the matrix entries `r1` to `r9` and a translation represented
    by `t1`, `t2`, and `t3`) that must be computed for each view. These are in the
    output parameter list of the `cv::calibrateCamera` function. The rotation and
    translation components are often called the **extrinsic parameters** of the calibration,
    and they are different for each view. The intrinsic parameters remain constant
    for a given camera/lens system. The intrinsic parameters of our test camera obtained
    from a calibration based on 20 chessboard images are `fx=167`, `fy=178`, `u0=156`,
    and `v0=119`. These results are obtained by `cv::calibrateCamera` through an optimization
    process aimed at finding the intrinsic and extrinsic parameters that will minimize
    the difference between the predicted image point position, as computed from the
    projection of the 3D scene points, and the actual image point position, as observed
    on the image. The sum of this difference for all the points specified during the
    calibration is called the **re-projection error**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在我们的校准示例中，参考系被放置在棋盘上。因此，对于每个视图，必须计算一个刚性变换（由矩阵项`r1`到`r9`表示的旋转部分和由`t1`、`t2`和`t3`表示的平移）。这些在`cv::calibrateCamera`函数的输出参数列表中。旋转和平移部分通常被称为校准的**外参数**，它们对于每个视图是不同的。对于给定的摄像机/镜头系统，内在参数保持不变。我们从基于20张棋盘图像的校准中获得的测试摄像机的内在参数是`fx=167`、`fy=178`、`u0=156`和`v0=119`。这些结果是通过`cv::calibrateCamera`通过一个优化过程获得的，该过程旨在找到最小化从3D场景点的投影计算出的预测图像点位置与在图像上观察到的实际图像点位置之间的差异的内在和外参数。校准期间指定的所有点的这个差异之和被称为**重投影误差**。
- en: Let's now turn our attention to the distortion parameters. So far, we have mentioned
    that under the pin-hole camera model, we can neglect the effect of the lens. However,
    this is only possible if the lens that is used to capture an image does not introduce
    important optical distortions. Unfortunately, this is not the case with lower
    quality lenses or with lenses that have a very short focal length. You may have
    already noted that the chessboard pattern shown in the image that we used for
    our example is clearly distorted—the edges of the rectangular board are curved
    in the image. Also, note that this distortion becomes more important as we move
    away from the center of the image. This is a typical distortion observed with
    a fish-eye lens, and it is called **radial distortion**. The lenses used in common
    digital cameras usually do not exhibit such a high degree of distortion, but in
    the case of the lens used here, these distortions certainly cannot be ignored.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向畸变参数。到目前为止，我们提到，在针孔相机模型下，我们可以忽略镜头的影响。然而，这只在用于捕获图像的镜头不引入重要的光学畸变时才可能。不幸的是，对于低质量镜头或焦距非常短的镜头来说，情况并非如此。您可能已经注意到，我们在示例中使用的图像中显示的棋盘图案明显畸变——矩形板的边缘在图像中是弯曲的。此外，请注意，这种畸变随着我们远离图像中心而变得更加重要。这是鱼眼镜头观察到的典型畸变，被称为**径向畸变**。常用数字相机中使用的镜头通常不会表现出如此高的畸变程度，但在这里使用的镜头，这些畸变肯定不能被忽略。
- en: 'It is possible to compensate for these deformations by introducing an appropriate
    distortion model. The idea is to represent the distortions induced by a lens by
    a set of mathematical equations. Once established, these equations can then be
    reverted in order to undo the distortions visible on the image. Fortunately, the
    exact parameters of the transformation that will correct the distortions can be
    obtained together with the other camera parameters during the calibration phase.
    Once this is done, any image from the newly calibrated camera will be undistorted.
    Therefore, we have added an additional method to our calibration class:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入适当的畸变模型，可以补偿这些变形。想法是通过一组数学方程来表示由镜头引起的畸变。一旦建立，这些方程就可以被逆转，以消除图像上可见的畸变。幸运的是，在标定阶段，可以同时获得将纠正畸变的变换的确切参数以及其他相机参数。一旦这样做，任何来自新标定相机的图像都将被去畸变。因此，我们在我们的标定类中添加了一个额外的方法：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Running this code results in the following image:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将产生以下图像：
- en: '![How it works...](img/00172.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00172.jpeg)'
- en: As you can see, once the image is undistorted, we obtain a regular perspective
    image.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一旦图像被去畸变，我们获得了一个正常的透视图像。
- en: To correct the distortion, OpenCV uses a polynomial function that is applied
    to the image points in order to move them at their undistorted position. By default,
    five coefficients are used; a model made of eight coefficients is also available.
    Once these coefficients are obtained, it is possible to compute two `cv::Mat`
    mapping functions (one for the `x` coordinate and one for the `y` coordinate)
    that will give the new undistorted position of an image point on a distorted image.
    This is computed by the `cv::initUndistortRectifyMap` function, and the `cv::remap`
    function remaps all the points of an input image to a new image. Note that because
    of the nonlinear transformation, some pixels of the input image now fall outside
    the boundary of the output image. You can expand the size of the output image
    to compensate for this loss of pixels, but you will now obtain output pixels that
    have no values in the input image (they will then be displayed as black pixels).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正畸变，OpenCV使用一个多项式函数应用于图像点，以便将它们移动到未畸变的位置。默认情况下，使用五个系数；也有由八个系数组成的模型可用。一旦获得这些系数，就可以计算两个`cv::Mat`映射函数（一个用于`x`坐标，一个用于`y`坐标），这将给出在畸变图像上图像点的新未畸变位置。这是通过`cv::initUndistortRectifyMap`函数计算的，`cv::remap`函数将输入图像的所有点重新映射到新图像。请注意，由于非线性变换，输入图像的一些像素现在超出了输出图像的边界。您可以通过扩展输出图像的大小来补偿这种像素损失，但您现在将获得在输入图像中没有值的输出像素（它们将显示为黑色像素）。
- en: There's more...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: More options are available when it comes to camera calibration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在相机标定方面，还有更多选项可用。
- en: Calibration with known intrinsic parameters
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用已知内在参数进行标定
- en: When a good estimate of the camera's intrinsic parameters is known, it could
    be advantageous to input them in the `cv::calibrateCamera` function. They will
    then be used as initial values in the optimization process. To do so, you just
    need to add the `CV_CALIB_USE_INTRINSIC_GUESS` flag and input these values in
    the calibration matrix parameter. It is also possible to impose a fixed value
    for the principal point (`CV_CALIB_FIX_PRINCIPAL_POINT`), which can often be assumed
    to be the central pixel. You can also impose a fixed ratio for the focal lengths
    `fx` and `fy` (`CV_CALIB_FIX_RATIO`); in which case, you assume the pixels of
    the square shape.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当已知相机内参的良好估计时，可以在`cv::calibrateCamera`函数中输入它们。然后它们将在优化过程中用作初始值。为此，您只需添加`CV_CALIB_USE_INTRINSIC_GUESS`标志并在校准矩阵参数中输入这些值。也可以为主点（`CV_CALIB_FIX_PRINCIPAL_POINT`）指定一个固定值，这通常可以假设是中心像素。您还可以为焦距`fx`和`fy`指定一个固定比率（`CV_CALIB_FIX_RATIO`）；在这种情况下，您假设像素是正方形形状的。
- en: Using a grid of circles for calibration
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用圆形网格进行校准
- en: 'Instead of the usual chessboard pattern, OpenCV also offers the possibility
    to calibrate a camera by using a grid of circles. In this case, the centers of
    the circles are used as calibration points. The corresponding function is very
    similar to the function we used to locate the chessboard corners:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与常用的棋盘格图案不同，OpenCV还提供了使用圆形网格校准相机的可能性。在这种情况下，圆的中心被用作校准点。相应的函数与我们用来定位棋盘格角落的函数非常相似：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: See also
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Computing a homography between two images* recipe in this chapter will
    examine the projective equation in special situations
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章中的*计算两张图像之间的单应性*配方将检查特殊情况下投影方程
- en: The *A flexible new technique for camera calibration* article by Z. Zhang in
    *IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no 11,
    2000*, is a classic paper on the problem of camera calibration
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Z. Zhang在*IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.
    22, no 11, 2000*上发表的*一种灵活的相机校准新方法*文章是关于相机校准问题的一篇经典论文
- en: Computing the fundamental matrix of an image pair
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图像对的基本矩阵
- en: The previous recipe showed you how to recover the projective equation of a single
    camera. In this recipe, we will explore the projective relationship that exists
    between two images that display the same scene. These two images could have been
    obtained by moving a camera at two different locations to take pictures from two
    viewpoints or by using two cameras, each of them taking a different picture of
    the scene. When these two cameras are separated by a rigid baseline, we use the
    term **stereovision**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方向您展示了如何恢复单相机的投影方程。在本配方中，我们将探讨显示相同场景的两个图像之间存在的投影关系。这两张图像可能是通过在两个不同位置移动相机从两个视点拍照或使用两个相机，每个相机拍摄场景的不同图片而获得的。当这两个相机通过一个刚体基线分离时，我们使用术语**立体视觉**。
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let''s now consider two cameras observing a given scene point, as shown in
    the following figure:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑两个相机观察一个给定的场景点，如图所示：
- en: '![Getting ready](img/00173.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/00173.jpeg)'
- en: We learned that we can find the image **x** of a 3D point **X** by tracing a
    line joining this 3D point with the camera's center. Conversely, the scene point
    that has its image at the position **x** on the image plane can be located anywhere
    on this line in the 3D space. This implies that if we want to find the corresponding
    point of a given image point in another image, we need to search along the projection
    of this line onto the second image plane. This imaginary line is called the **epipolar
    line** of point **x**. It defines a fundamental constraint that must satisfy two
    corresponding points; that is, the match of a given point must lie on the epipolar
    line of this point in the other view, and the exact orientation of this epipolar
    line depends on the respective position of the two cameras. In fact, the configuration
    of the epipolar line characterizes the geometry of a two-view system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们可以通过追踪连接该三维点与相机中心的线来找到三维点**X**的图像**x**。相反，场景点在图像平面上其图像位于**x**位置，可以在三维空间中的这条线上任何位置。这意味着如果我们想在另一张图像中找到给定图像点的对应点，我们需要沿着这条线在第二张图像平面上的投影进行搜索。这条想象中的线被称为点**x**的**极线**。它定义了一个必须满足两个对应点的基本约束；也就是说，给定点的匹配必须位于另一个视图中该点的极线上，而这个极线的确切方向取决于两个相机的相对位置。实际上，极线的配置表征了双视系统的几何形状。
- en: Another observation that can be made from the geometry of this two-view system
    is that all the epipolar lines pass through the same point. This point corresponds
    to the projection of one camera's center onto the other camera. This special point
    is called an **epipole**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个双视图系统的几何学中可以得出的另一个观察结果是，所有极线都通过同一点。这个点对应于一个相机的中心在另一个相机上的投影。这个特殊点被称为**极点**。
- en: 'Mathematically, the relationship between an image point and its corresponding
    epipolar line can be expressed using a `3x3` matrix as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，图像点与其对应的极线之间的关系可以用一个 `3x3` 矩阵表示如下：
- en: '![Getting ready](img/00174.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/00174.jpeg)'
- en: In projective geometry, a 2D line is also represented by a 3-vector. It corresponds
    to the set of 2D points, `(x',y')`, that satisfy the equation *l* *[1]* *'x'+
    l* *[2]* *'y'+ l* *[3]* *'=0* (the prime superscript denotes that this line belongs
    to the second image). Consequently, the matrix **F**, called the fundamental matrix,
    maps a 2D image point in one view to an epipolar line in the other view.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在射影几何中，二维直线也由一个三维向量表示。它对应于满足方程 *l* *[1]* *'x'+ l* *[2]* *'y'+ l* *[3]* *'=0*
    的二维点集，`(x',y')`（上标撇表示这条线属于第二图像）。因此，矩阵 **F**，称为基本矩阵，将一个视图中的二维图像点映射到另一个视图中的极线。
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The fundamental matrix of an image pair can be estimated by solving a set of
    equations that involve a certain number of known matched points between the two
    images. The minimum number of such matches is seven. In order to illustrate the
    fundamental matrix estimation process and using the image pair from the previous
    chapter, we can manually select seven good matches. These will be used to compute
    the fundamental matrix using the `cv::findFundamentalMat` OpenCV function, as
    shown in the following screenshot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决涉及两个图像之间一定数量的已知匹配点的方程组，可以估计图像对的基本矩阵。这种匹配的最小数量是七个。为了说明基本矩阵估计过程并使用前一章中的图像对，我们可以手动选择七个良好的匹配点。这些点将用于使用
    `cv::findFundamentalMat` OpenCV 函数计算基本矩阵，如下面的截图所示：
- en: '![How to do it...](img/00175.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![如何做到这一点...](img/00175.jpeg)'
- en: 'If we have the image points in each image as the `cv::keypoint` instances (for
    example, if they were detected using a keypoint detector as in [Chapter 8](part0058_split_000.html#page
    "Chapter 8. Detecting Interest Points"), *Detecting Interest Points*), they first
    need to be converted into `cv::Point2f` in order to be used with `cv::findFundamentalMat`.
    An OpenCV function can be used to this end:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个图像中的图像点作为 `cv::keypoint` 实例（例如，如果它们像在[第8章](part0058_split_000.html#page
    "第8章. 检测兴趣点"), *检测兴趣点*）中使用关键点检测器检测到的那样），它们首先需要转换为 `cv::Point2f`，以便与 `cv::findFundamentalMat`
    一起使用。可以为此使用 OpenCV 函数：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The two vectors `selPoints1` and `selPoints2` contain the corresponding points
    in the two images. The keypoint instances are `keypoints1` and `keypoints2`. The
    `pointIndexes1` and `pointIndexes2` vectors contain the indexes of the keypoints
    to be converted. The call to the `cv::findFundamentalMat` function is then as
    follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量 `selPoints1` 和 `selPoints2` 包含两个图像中相应的点。关键点实例是 `keypoints1` 和 `keypoints2`。`pointIndexes1`
    和 `pointIndexes2` 向量包含要转换的关键点的索引。调用 `cv::findFundamentalMat` 函数的方式如下：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'One way to visually verify the validity of the fundamental matrix is to draw
    the epipolar lines of some selected points. Another OpenCV function allows the
    epipolar lines of a given set of points to be computed. Once these are computed,
    they can be drawn using the `cv::line` function. The following lines of code accomplish
    these two steps (that is, computing and drawing epipolar lines in the image on
    the right from the points in the image on the left):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 验证基本矩阵有效性的一个方法是绘制一些选定点的极线。另一个 OpenCV 函数允许计算给定点的极线。一旦计算出来，就可以使用 `cv::line` 函数绘制。以下代码行完成了这两个步骤（即从左图像中的点计算并绘制右图像上的极线）：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result can be seen in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在下面的截图中看到：
- en: '![How to do it...](img/00176.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![如何做到这一点...](img/00176.jpeg)'
- en: Remember that the epipole is at the intersection of all the epipolar lines,
    and it is the projection of the other camera's center. This epipole is visible
    in the preceding image. Often, the epipolar lines intersect outside the image
    boundaries. In the case of our example, it is at the location where the first
    camera would be visible if the two images were taken at the same instant. Note
    that the results can be quite instable when the fundamental matrix is computed
    from seven matches. Indeed, substituting one match for another could lead to a
    significantly different set of epipolar lines.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，极点位于所有极线的交点处，它是另一台相机的中心的投影。这个极点在前面的图像中是可见的。通常，极线会在图像边界之外相交。在我们的例子中，如果两个图像在同一瞬间拍摄，它将位于第一个相机可见的位置。请注意，当从七个匹配点计算基本矩阵时，结果可能会非常不稳定。确实，用一个匹配点替换另一个匹配点可能会导致极线集合发生显著变化。
- en: How it works...
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We previously explained that for a point in one image, the fundamental matrix
    gives the equation of the line on which its corresponding point in the other view
    should be found. If the corresponding point of a point `p` (expressed in homogenous
    coordinates) is `p''` and if `F` is the fundamental matrix between the two views,
    then since `p''` lies on the epipolar line `Fp`, we have the following equation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前解释过，对于一张图像中的一个点，基本矩阵给出了另一个视图中其对应点所在直线的方程。如果点`p`（以齐次坐标表示）的对应点是`p'`，并且如果`F`是两个视图之间的基本矩阵，那么由于`p'`位于极线`Fp`上，我们就有以下方程：
- en: '![How it works...](img/00177.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00177.jpeg)'
- en: This equation expresses the relationship between two corresponding points and
    is known as the **epipolar constraint**. Using this equation, it becomes possible
    to estimate the entries of the matrix using known matches. Since the entries of
    the `F` matrix are given up to a scale factor, there are only eight entries to
    be estimated (the ninth can be arbitrarily set to `1`). Each match contributes
    to one equation. Therefore, with eight known matches, the matrix can be fully
    estimated by solving the resulting set of linear equations. This is what is done
    when you use the `CV_FM_8POINT` flag with the `cv::findFundamentalMat` function.
    Note that in this case, it is possible (and preferable) to input more than eight
    matches. The obtained over-determined system of linear equations can then be solved
    in a mean-square sense.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程表达了两个对应点之间的关系，被称为**极线约束**。使用这个方程，就可以通过已知的匹配点来估计矩阵的元素。由于`F`矩阵的元素只给出一个比例因子，因此只有八个元素需要估计（第九个可以任意设置为`1`）。每个匹配点对应一个方程。因此，如果有八个已知的匹配点，就可以通过解这组线性方程来完全估计矩阵。这就是当你使用`cv::findFundamentalMat`函数的`CV_FM_8POINT`标志时所执行的操作。请注意，在这种情况下，输入超过八个匹配点是可能的（并且是首选的）。然后可以在均方意义上求解得到的超定线性方程组。
- en: To estimate the fundamental matrix, an additional constraint can also be exploited.
    Mathematically, the F matrix maps a 2D point to a 1D pencil of lines (that is,
    lines that intersect at a common point). The fact that all these epipolar lines
    pass through this unique point (that is, the epipole) imposes a constraint on
    the matrix. This constraint reduces the number of matches required to estimate
    the fundamental matrix to seven. Unfortunately, in this case, the set of equations
    become nonlinear with up to three possible solutions (in this case, `cv::findFundamentalMat`
    will return a fundamental matrix of the size `9x3`, that is, three `3x3` matrices
    stacked up). The seven-match solution of the `F` matrix estimation can be invoked
    in OpenCV by using the `CV_FM_7POINT` flag. This is what we did in the example
    of the preceding section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计基本矩阵，还可以利用一个额外的约束。从数学上讲，`F`矩阵将一个二维点映射到一个一维的直线束（即相交于一个共同点的直线）。所有这些极线都通过这个独特的点（即极点）这一事实对矩阵施加了一个约束。这个约束将估计基本矩阵所需的匹配点数量减少到七个。不幸的是，在这种情况下，方程组变得非线性，最多有三个可能的解（在这种情况下，`cv::findFundamentalMat`将返回一个大小为`9x3`的基本矩阵，即三个`3x3`矩阵堆叠）。可以通过使用`CV_FM_7POINT`标志在OpenCV中调用`F`矩阵估计的七个匹配点解。这就是我们在前一个章节的例子中所做的。
- en: Lastly, we would like to mention that the choice of an appropriate set of matches
    in the image is important to obtain an accurate estimation of the fundamental
    matrix. In general, the matches should be well distributed across the image and
    include points at different depths in the scene. Otherwise, the solution will
    become unstable or degenerate configurations. In particular, the selected scene
    points should not be coplanar as the fundamental matrix (in this case) becomes
    degenerated.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想提到，在图像中选择合适的匹配集对于获得基本矩阵的准确估计非常重要。一般来说，匹配应该在整个图像中分布良好，并包括场景中不同深度的点。否则，解将变得不稳定或退化配置。特别是，所选场景点不应共面，因为在这种情况下基本矩阵会退化。
- en: See also
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*Multiple View Geometry in Computer Vision, Cambridge University Press, 2004,
    R. Hartley and A. Zisserman*, is the most complete reference on projective geometry
    in computer vision'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《计算机视觉中的多视图几何学》，剑桥大学出版社，2004年，R. Hartley 和 A. Zisserman*，是计算机视觉中投影几何最完整的参考书
- en: The next recipe explains how a fundamental matrix can be robustly estimated
    from a larger match set
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个配方解释了如何从一个更大的匹配集中稳健地估计基本矩阵
- en: The *Computing a homography between two images* recipe explains why a fundamental
    matrix cannot be computed when the matched points are coplanar or are the result
    of a pure rotation
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “在两幅图像之间计算单应性”配方解释了为什么当匹配点共面或仅是纯旋转的结果时，无法计算基本矩阵
- en: Matching images using a random sample consensus
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机样本一致性匹配图像
- en: When two cameras observe the same scene, they see the same elements but under
    different viewpoints. We have already studied the feature point matching problem
    in the previous chapter. In this recipe, we come back to this problem, and we
    will learn how to exploit the epipolar constraint between two views to match image
    features more reliably.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个相机观察同一个场景时，它们看到相同的元素，但处于不同的视点。我们已经在上一章研究了特征点匹配问题。在这个配方中，我们回到这个问题，我们将学习如何利用两个视图之间的极线约束来更可靠地匹配图像特征。
- en: 'The principle that we will follow is simple: when we match feature points between
    two images, we only accept those matches that fall on the corresponding epipolar
    lines. However, to be able to check this condition, the fundamental matrix must
    be known, but we need good matches to estimate this matrix. This seems to be a
    chicken-and-egg problem. However, in this recipe, we propose a solution in which
    the fundamental matrix and a set of good matches will be jointly computed.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循的原则很简单：当我们匹配两幅图像之间的特征点时，我们只接受落在对应极线上的匹配。然而，为了能够检查这个条件，必须知道基本矩阵，但我们需要良好的匹配来估计这个矩阵。这似乎是一个鸡生蛋的问题。然而，在这个配方中，我们提出了一种解决方案，其中基本矩阵和一组良好的匹配将共同计算。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The objective is to be able to compute a fundamental matrix and a set of good
    matches between two views. To do so, all the found feature point correspondences
    will be validated using the epipolar constraint introduced in the previous recipe.
    To this end, we have created a class that encapsulates the different steps of
    the proposed robust matching process:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是能够计算两个视图之间的基本矩阵和一组良好的匹配。为此，我们将使用在前一个配方中引入的极线约束来验证所有找到的特征点对应关系。为此，我们创建了一个类，它封装了所提出的稳健匹配过程的各个步骤：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note how we used the `create` methods of the `cv::FeatureDetector` and `cv::DescriptorExtractor`
    interfaces so that a user can select the `create` methods by their names. Note
    that the `create` methods can also be specified using the defined `setFeatureDetector`
    and `setDescriptorExtractor` setter methods.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何使用`cv::FeatureDetector`和`cv::DescriptorExtractor`接口的`create`方法，以便用户可以通过它们的名称选择`create`方法。注意，`create`方法也可以使用定义的`setFeatureDetector`和`setDescriptorExtractor`设置方法来指定。
- en: 'The main method is our `match` method that returns matches, detected keypoints,
    and the estimated fundamental matrix. The method proceeds in four distinct steps
    (explicitly identified in the comments of the following code) that we will now
    explore:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 主要方法是我们的`match`方法，它返回匹配项、检测到的关键点和估计的基本矩阵。该方法通过以下四个不同的步骤进行（在以下代码的注释中明确标识）：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first two steps simply detect the feature points and compute their descriptors.
    Next, we proceed to feature matching using the `cv::BFMatcher` class, as we did
    in the previous chapter. We use the crosscheck flag to obtain matches of better
    quality.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 前两步简单地检测特征点并计算它们的描述符。接下来，我们使用`cv::BFMatcher`类进行特征匹配，就像我们在上一章中做的那样。我们使用交叉检查标志来获得更好的匹配质量。
- en: 'The fourth step is the new concept introduced in this recipe. It consists of
    an additional filtering test that will this time use the fundamental matrix in
    order to reject matches that do not obey the epipolar constraint. This test is
    based on the `RANSAC` method that can compute the fundamental matrix even when
    outliers are still present in the match set (this method will be explained in
    the next section):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第四步是本菜谱中引入的新概念。它包括一个额外的过滤测试，这次将使用基矩阵来拒绝不遵守极线约束的匹配。这个测试基于`RANSAC`方法，即使匹配集中仍然存在异常值，该方法也能计算基矩阵（该方法将在下一节中解释）：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This code is a bit long because the keypoints need to be converted into `cv::Point2f`
    before the F matrix computation. Using this class, the robust matching of an image
    pair is then easily accomplished by the following calls:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有点长，因为在使用F矩阵计算之前，需要将关键点转换为`cv::Point2f`。使用这个类，通过以下调用就可以轻松实现图像对的鲁棒匹配：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in `62` matches that are shown in the following screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了`62`个匹配，如下面的截图所示：
- en: '![How to do it...](img/00178.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![如何做到这一点...](img/00178.jpeg)'
- en: Interestingly, almost all these matches are correct, even if a few false matches
    remain; these accidently fell on the corresponding epipolar lines of the computed
    fundamental matrix.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，几乎所有这些匹配都是正确的，即使还有一些错误的匹配存在；这些错误匹配偶然落在了计算出的基矩阵的对应极线上。
- en: How it works...
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In the preceding recipe, we learned that it is possible to estimate the fundamental
    matrix associated with an image pair from a number of feature point matches. Obviously,
    to be exact, this match set must be made up of only good matches. However, in
    a real context, it is not possible to guarantee that a match set obtained by comparing
    the descriptors of the detected feature points will be completely exact. This
    is why a fundamental matrix estimation method based on the **RANSAC** (**RANdom
    SAmpling Consensus**) strategy has been introduced.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的菜谱中，我们了解到可以从多个特征点匹配中估计与图像对相关的基矩阵。显然，为了精确，这个匹配集必须只包含好的匹配。然而，在实际情况下，无法保证通过比较检测到的特征点的描述符获得的匹配集是完全精确的。这就是为什么引入了一种基于**RANSAC**（**随机采样一致性**）策略的基矩阵估计方法。
- en: The RANSAC algorithm aims at estimating a given mathematical entity from a data
    set that may contain a number of outliers. The idea is to randomly select some
    data points from the set and perform the estimation only with these. The number
    of selected points should be the minimum number of points required to estimate
    the mathematical entity. In the case of the fundamental matrix, eight matched
    pairs is the minimum number (in fact, it could be seven matches, but the 8-point
    linear algorithm is faster to compute). Once the fundamental matrix is estimated
    from these eight random matches, all the other matches in the match set are tested
    against the epipolar constraint that derives from this matrix. All the matches
    that fulfill this constraint (that is, matches for which the corresponding feature
    is at a short distance from its epipolar line) are identified. These matches form
    the **support set** of the computed fundamental matrix.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RANSAC算法旨在从一个可能包含多个异常值的数据集中估计给定的数学实体。想法是从集合中随机选择一些数据点，并仅使用这些点进行估计。所选点的数量应该是估计数学实体所需的最小点数。在基矩阵的情况下，八个匹配对是最小数量（实际上，可能是七个匹配，但8点线性算法计算更快）。一旦从这八个随机匹配中估计出基矩阵，就将匹配集中的所有其他匹配与由此矩阵导出的极线约束进行测试。所有满足此约束的匹配（即对应特征与其极线距离较短的匹配）都被识别出来。这些匹配形成了计算出的基矩阵的**支持集**。
- en: The central idea behind the RANSAC algorithm is that the larger the support
    set, the higher the probability that the computed matrix is the right one. Conversely,
    if one (or more) of the randomly selected matches is a wrong match, then the computed
    fundamental matrix will also be incorrect, and its support set is expected to
    be small. This process is repeated a number of times, and in the end, the matrix
    with the largest support will be retained as the most probable one.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RANSAC算法背后的核心思想是支持集越大，计算出的矩阵就越可能是正确的。相反，如果随机选择的匹配中有一个（或多个）是错误的匹配，那么计算出的基础矩阵也将是错误的，其支持集预计会很小。这个过程会重复多次，最终，支持集最大的矩阵将被保留为最可能的矩阵。
- en: Therefore, our objective is to pick eight random matches several times so that
    eventually we select eight good ones, which should give us a large support set.
    Depending on the number of wrong matches in the entire data set, the probability
    of selecting a set of eight correct matches will differ. We, however, know that
    the more selections we make, the higher our confidence will be that we have at
    least one good match set among those selections. More precisely, if we assume
    that the match set is made of `w%` inliers (good matches), then the probability
    that we select eight good matches is `w%`. Consequently, the probability that
    a selection contains at least one wrong match is `(1-w)`. If we make `k` selections,
    the probability of having one random set that contains good matches only is `1-(1-w)k`.
    This is the confidence probability, `c`, and we want this probability to be as
    high as possible since we need at least one good set of matches in order to obtain
    the correct fundamental matrix. Therefore, when running the RANSAC algorithm,
    one needs to determine the number of `k` selections that need to be made in order
    to obtain a given confidence level.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的目标是多次随机选择八个匹配，最终选择出八个好的匹配，这将给我们一个大的支持集。根据整个数据集中错误匹配的数量，选择八个正确匹配的概率会有所不同。然而，我们知道，我们选择的次数越多，我们对我们至少在这些选择中有一个好的匹配集的信心就越高。更精确地说，如果我们假设匹配集由`w%`的内点（良好匹配）组成，那么我们选择八个良好匹配的概率是`w%`。因此，一个选择包含至少一个错误匹配的概率是`(1-w)`。如果我们进行`k`次选择，只有一个随机集只包含良好匹配的概率是`1-(1-w)^k`。这是置信概率，`c`，我们希望这个概率尽可能高，因为我们至少需要一个良好的匹配集来获得正确的基础矩阵。因此，在运行RANSAC算法时，需要确定需要进行多少次`k`次选择才能获得给定的置信水平。
- en: When using the `cv::findFundamentalMat` function with the `CV_FM_RANSAC` method,
    two extra parameters are provided. The first parameter is the confidence level,
    which determines the number of iterations to be made (by default, it is `0.99`).
    The second parameter is the maximum distance to the epipolar line for a point
    to be considered as an inlier. All the matched pairs in which a point is at a
    greater distance from its epipolar line than the distance specified will be reported
    as an outlier. The function also returns `std::vector` of the character value,
    indicating that the corresponding match in the input set has been identified as
    an outlier (`0`) or as an inlier (`1`).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`cv::findFundamentalMat`函数并采用`CV_FM_RANSAC`方法时，提供了两个额外的参数。第一个参数是置信水平，它决定了要进行的迭代次数（默认值为`0.99`）。第二个参数是点到极线距离的最大值，一个点如果被认为是一个内点。所有匹配对中，如果一个点与其极线距离大于指定距离的，将被报告为异常点。该函数还返回一个字符值的`std::vector`，表示输入集中相应的匹配已被识别为异常点（`0`）或内点（`1`）。
- en: The more good matches you have in your initial match set, the higher the probability
    that RANSAC will give you the correct fundamental matrix. This is why we applied
    the crosscheck filter when matching the feature points. You could have also used
    the ratio test presented in the previous recipe in order to further improve the
    quality of the final match set. It is just a question of balancing the computational
    complexity, the final number of matches, and the required level of confidence
    that the obtained match set will contain only exact matches.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的初始匹配集中有越多的良好匹配，RANSAC给出正确基础矩阵的概率就越高。这就是为什么我们在匹配特征点时应用了交叉检查过滤器。你也可以使用前一个菜谱中提到的比率测试来进一步提高最终匹配集的质量。这只是平衡计算复杂度、最终匹配数和所需置信度的问题，即所获得的匹配集只包含精确匹配。
- en: There's more...
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The result of the robust matching process presented in this recipe is an estimate
    of the fundamental matrix computed using the eight selected matches that have
    the largest support and the set matches included in this support set. Using this
    information, it is possible to refine these results in two ways.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中提出的鲁棒匹配过程的结果是使用具有最大支持集和包含在此支持集中的匹配集计算出的基本矩阵的估计。使用这些信息，我们可以以两种方式对这些结果进行精炼。
- en: Refining the fundamental matrix
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精炼基本矩阵
- en: 'Since we now have a match set of good quality, as a last step, it might be
    a good idea to use all of them to re-estimate the fundamental matrix. We already
    mentioned that there exists a linear 8-point algorithm to estimate this matrix.
    We can, therefore, obtain an over-determined system of equations that will solve
    the fundamental matrix in a least-squares sense. This step can be added at the
    end of our `ransacTest` function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在有一个高质量的匹配集，作为最后一步，使用所有这些匹配来重新估计基本矩阵可能是一个好主意。我们已经提到，存在一个线性8点算法来估计这个矩阵。因此，我们可以获得一个超定方程组，以最小二乘法求解基本矩阵。这一步可以添加到我们的`ransacTest`函数的末尾：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `cv::findFundamentalMat` function indeed accepts more than `8` matches by
    solving the linear system of equations using singular value decomposition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv::findFundamentalMat`函数确实可以通过使用奇异值分解来解决线性方程组，接受超过`8`个匹配。'
- en: Refining the matches
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精炼匹配
- en: 'We learned that in a two-view system, every point must lie on the epipolar
    line of its corresponding point. This is the epipolar constraint expressed by
    the fundamental matrix. Consequently, if you have a good estimate of a fundamental
    matrix, you can use this epipolar constraint to correct the obtained matches by
    forcing them to lie on their epipolar lines. This can be easily done by using
    the `cv::correctMatches` OpenCV function:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在双视场系统中，每个点必须位于其对应点的极线（epipolar line）上。这是由基本矩阵（fundamental matrix）表达出的极线约束。因此，如果你有一个基本矩阵的良好估计，你可以使用这个极线约束来校正获得的匹配，通过强制它们位于它们的极线上。这可以通过使用`cv::correctMatches`
    OpenCV函数轻松完成：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function proceeds by modifying the position of each corresponding point
    such that it satisfies the epipolar constraint while minimizing the cumulative
    (squared) displacement.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数通过修改每个对应点的位置来实现，使其满足极线约束，同时最小化累积（平方）位移。
- en: Computing a homography between two images
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算两张图像之间的单应性（homography）
- en: The second recipe of this chapter showed you how to compute the fundamental
    matrix of an image pair from a set of matches. In projective geometry, another
    very useful mathematical entity also exists. This one can be computed from multiview
    imagery and, as we will see, is a matrix with special properties.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二种方法向你展示了如何从一组匹配中计算图像对的基本矩阵。在射影几何中，还存在另一个非常有用的数学实体。这个实体可以从多视图图像中计算出来，正如我们将看到的，它是一个具有特殊性质的矩阵。
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Again, let''s consider the projective relation between a 3D point and its image
    on a camera, which we introduced in the first recipe of this chapter. Basically,
    we learned that this equation relates a 3D point with its image using the intrinsic
    properties of the camera and the position of this camera (specified with a rotation
    and a translation component). If we now carefully examine this equation, we realize
    that there are two special situations of particular interest. The first situation
    is when two views of a scene are separated by a pure rotation. It can then be
    observed that the fourth column of the extrinsic matrix will be made up of 0s
    (that is, the translation is null):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们考虑一个3D点和它在相机上的图像之间的射影关系，这是我们本章的第一个食谱中引入的。基本上，我们了解到这个方程通过相机的内在属性和这个相机的位置（用旋转和平移分量指定）将3D点与其图像联系起来。如果我们现在仔细检查这个方程，我们会意识到有两个特别有趣的特殊情况。第一种情况是当场景的两个视图之间只存在纯旋转时。那时可以观察到外矩阵的第四列将全部由0组成（即平移为零）：
- en: '![Getting ready](img/00179.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/00179.jpeg)'
- en: 'As a result, the projective relation in this special case becomes a `3x3` matrix.
    A similarly interesting situation also occurs when the object we observe is a
    plane. In this specific case, we can assume that the points on this plane will
    be located at `Z=0`, without the loss of generality. As a result, we obtain the
    following equation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个特殊情况下，射影关系变成了一个`3x3`矩阵。当观察到的物体是一个平面时，也会出现一个类似有趣的情况。在这种情况下，我们可以假设这个平面上的点将位于`Z=0`，这不会失去一般性。因此，我们得到以下方程：
- en: '![Getting ready](img/00180.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/00180.jpeg)'
- en: 'This zero coordinate of the scene points will then cancel the third column
    of the projective matrix, which will then again become a `3x3` matrix. This special
    matrix is called a **homography**, and it implies that, under special circumstances
    (here, a pure rotation or a planar object), a point is related to its image by
    a linear relation of the following form:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 场景点的零坐标将取消投影矩阵的第三列，这使得它再次成为一个`3x3`矩阵。这个特殊的矩阵被称为**单应性矩阵**，它意味着在特殊情况下（这里是一个纯旋转或平面物体），一个点与其图像之间通过以下形式的线性关系相关联：
- en: '![Getting ready](img/00181.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![准备中](img/00181.jpeg)'
- en: Here, `H` is a `3x3` matrix. This relation holds up to a scale factor represented
    here by the `s` scalar value. Once this matrix is estimated, all the points in
    one view can be transferred to a second view using this relation. Note that as
    a side effect of the homography relation, the fundamental matrix becomes undefined
    in these cases.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`H`是一个`3x3`矩阵。这个关系在由`s`标量值表示的尺度因子下成立。一旦估计出这个矩阵，一个视角中的所有点都可以使用这个关系转移到第二个视角。请注意，作为单应性关系的副作用，基本矩阵在这些情况下变得未定义。
- en: How to do it...
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Suppose that we have two images separated by a pure rotation. This happens,
    for example, when you take pictures of a building or a landscape by rotating yourself;
    as you are sufficiently far away from your subject, the translational component
    is negligible. These two images can be matched using the features of your choice
    and the `cv::BFMatcher` function. Then, as we did in the previous recipe, we will
    apply a RANSAC step that will this time involve the estimation of a homography
    based on a match set (which obviously contains a good number of outliers). This
    is done by using the `cv::findHomography` function, which is very similar to the
    `cv::findFundamentalMat` function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个通过纯旋转分开的图像。例如，当你通过旋转自己来拍摄建筑或风景照片时，这种情况就会发生；由于你离主题足够远，平移分量可以忽略不计。这两个图像可以使用你选择的特征和`cv::BFMatcher`函数进行匹配。然后，就像我们在前面的食谱中所做的那样，我们将应用一个RANSAC步骤，这次将涉及基于匹配集（显然包含大量异常值）的单应性估计。这是通过使用`cv::findHomography`函数完成的，该函数与`cv::findFundamentalMat`函数非常相似：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Recall that a homography exists (instead of a fundamental matrix) because our
    two images are separated by a pure rotation. The images are shown here. We also
    displayed the inlier keypoints as identified by the `inliers` argument of the
    function. Refer to the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，存在单应性（而不是基本矩阵）是因为我们的两个图像通过纯旋转分开。图像在此处显示。我们还显示了由函数的`inliers`参数确定的内点关键点。请参考以下截图：
- en: '![How to do it...](img/00182.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/00182.jpeg)'
- en: 'The second image is shown as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图像如下所示：
- en: '![How to do it...](img/00183.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/00183.jpeg)'
- en: 'The resulting inliers that comply with the found homography have been drawn
    on these images using the following loop:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下循环在这些图像上绘制了符合找到的单应性的结果内点：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The homography is a `3x3` invertible matrix; therefore, once it has been computed,
    you can transfer image points from one image to the other. In fact, you can do
    this for every pixel of an image. Consequently, you can transfer a complete image
    to the point of view of a second image. This process is called image **mosaicking**,
    and it is often used to build a large panorama from multiple images. An OpenCV
    function that does exactly this is given as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 投影变换是一个可逆的`3x3`矩阵；因此，一旦它被计算出来，你就可以将一个图像中的点转移到另一个图像中。实际上，你可以对图像的每一个像素都这样做。因此，你可以将整个图像转移到第二个图像的视角中。这个过程被称为图像**拼接**，它通常用于从多个图像构建一个大型全景图。一个执行此操作的OpenCV函数如下所示：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once this new image is obtained, it can be appended to the other image in order
    to expand the view (since the two images are now from the same point of view):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得这个新图像，它就可以附加到其他图像上以扩展视图（因为现在两个图像来自相同的视角）：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following image is the result:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像是结果：
- en: '![How to do it...](img/00184.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/00184.jpeg)'
- en: How it works...
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: When two views are related by a homography, it becomes possible to determine
    where a given scene point on one image is found on the other image. This property
    becomes particularly interesting for the points in one image that fall outside
    the image boundaries of the other. Indeed, since the second view shows a portion
    of the scene that is not visible in the first image, you can use the homography
    in order to expand the image by reading the color value of the additional pixels
    in the other image. That's how we were able to create a new image that is an expansion
    of our second image in which extra columns were added to the right-hand side.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: The homography computed by `cv::findHomography` is the one that maps the points
    in the first image to the points in the second image. This homography can be computed
    from a minimum of four matches, and the RANSAC algorithm is again used here. Once
    the homography with the best support is found, the `cv::findHomography` method
    refines it using all the identified inliers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Now, in order to transfer the points of image `1` to image `2`, what we need
    is, in fact, inverse homography. This is exactly what the `cv::warpPerspective`
    function is doing by default; that is, it uses the inverse of the homography provided
    as the input to get the color value of each point of the output image (this is
    what we called backward mapping in [Chapter 2](part0019_split_000.html#page "Chapter 2. Manipulating
    Pixels"), *Manipulating Pixels*). When an output pixel is transferred to a point
    outside the input image, a black value (`0`) is simply assigned to this pixel.
    Note that a `cv::WARP_INVERSE_MAP` flag can be specified as the optional fifth
    argument in `cv::warpPerspective` if you want to use direct homography instead
    of the inverted one during the pixel transfer process.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A homography also exists between two images of a plane. We can then make use
    of this to recognize a planar object in an image.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Detecting planar targets in an image
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you want to detect the occurrence of a planar object in an image. This
    object could be a poster, painting, signage, book cover (as in the following example),
    and so on. Based on what we learned in this chapter, the strategy would consist
    of detecting feature points on this object and to try and match them with the
    feature points in the image. These matches would then be validated using a robust
    matching scheme similar to the one we used in the previous recipe, but this time
    based on a homography.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a `TargetMatcher` class very similar to our `RobustMatcher` class:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we simply add a `target` attribute that represents the reference image
    of the planar object to be matched. The matching methods are identical to the
    ones of the `RobustMatcher` class, except that they include `cv::findHomography`
    instead of `cv::findFundamentalMat` in the `ransacTest` method. We also added
    a method to initiate target matching and find the position of the target:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once the homography has been found by the match method, we define the four
    corners of the target (that is, the four corners of its reference image). These
    are then transferred to the image using the `cv::perspectiveTransform` function.
    This function simply multiplies each point in the input vector by the homography
    matrix. This gives us the coordinates of these points in the other image. Target
    matching is then performed as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过匹配方法找到单应性，我们就定义目标（即其参考图像的四个角）。然后使用 `cv::perspectiveTransform` 函数将这些角转移到图像上。这个函数简单地用单应性矩阵乘以输入向量中的每个点。这给出了这些点在另一图像中的坐标。目标匹配随后按以下方式进行：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using the `cv::drawMatches` function, we display the results as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cv::drawMatches` 函数，我们以如下方式显示结果：
- en: '![Detecting planar targets in an image](img/00185.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![检测图像中的平面目标](img/00185.jpeg)'
- en: You can also use homographies to modify the perspectives of planar objects.
    For example, if you have several pictures from different points of view of the
    flat facade of a building, you can compute the homography between these images
    and build a large mosaic of the facade by wrapping the images and assembling them
    together, as we did in this recipe. A minimum of four matched points between two
    views are required to compute a homography. The `cv::getPerspectiveTransform`
    function allows such a transformation from four corresponding points to be computed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用单应性来修改平面物体的视角。例如，如果您有几张从不同角度拍摄的建筑物平坦立面的图片，您可以通过计算这些图片之间的单应性，通过包裹图像并将它们组装在一起来构建一个大型马赛克立面，就像我们在本配方中所做的那样。两个视角之间至少需要四个匹配点来计算单应性。`cv::getPerspectiveTransform`
    函数允许通过四个对应点进行这种变换的计算。
- en: See also
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考内容
- en: The *Remapping an image* recipe in [Chapter 2](part0019_split_000.html#page
    "Chapter 2. Manipulating Pixels"), *Manipulating Pixels*, discusses the concept
    of backward mapping
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2章](part0019_split_000.html#page "第2章. 操作像素")中的*图像重映射*配方，*操作像素*，讨论了反向映射的概念'
- en: The *Automatic panoramic image stitching using invariant features* article by
    M.Brown and D.Lowe in *International Journal of Computer Vision,74, 1, 2007*,
    describes the complete method to build panoramas from multiple images
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M.Brown 和 D.Lowe 在 *International Journal of Computer Vision,74, 1, 2007* 发表的*使用不变特征自动全景图像拼接*文章，描述了从多张图片构建全景的完整方法
