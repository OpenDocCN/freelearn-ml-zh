["```py\n        {\n            \"DatabaseInformation\": {\n              \"DatabaseName\": \"string\",\n              \"ClusterIdentifier\": \"string\"\n            },\n            \"SelectSqlQuery\": \"string\",\n            \"DatabaseCredentials\": {\n              \"Username\": \"string\",\n              \"Password\": \"string\"\n            },\n            \"S3StagingLocation\": \"string\",\n            \"DataRearrangement\": \"string\",\n            \"DataSchema\": \"string\",\n            \"DataSchemaUri\": \"string\"\n        }\n\n```", "```py\n        {\n            \"DatabaseInformation\": {\n              \"DatabaseName\": \"string\"\n              \"InstanceIdentifier\": \"string\",\n            },\n            \"SelectSqlQuery\": \"string\",\n            \"DatabaseCredentials\": {\n              \"Username\": \"string\",\n              \"Password\": \"string\"\n            },\n            \"S3StagingLocation\": \"string\",\n            \"DataRearrangement\": \"string\",\n            \"DataSchema\": \"string\",\n            \"DataSchemaUri\": \"string\",\n            \"ResourceRole\": \"string\",\n            \"ServiceRole\": \"string\",\n            \"SubnetId\": \"string\",\n            \"SecurityGroupIds\": [\"string\", ...]\n        }\n\n```", "```py\n$ psql --host=amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com --port=5439 --username=alexperrier --password --dbname=amlpacktdb\n\n```", "```py\nalexperrier@amlpacktdb=> dt\n No relations found.\n\n```", "```py\nalexperrier@amlpacktdb=> copy <table name> from '<s3 path to csv file>' CREDENTIALS 'aws_access_key_id=<aws access key id>;aws_secret_access_key=<aws secret access key>' CSV;\n\n```", "```py\nCREATE TABLE IF NOT EXISTS titanic (\n  id integer primary key,\n  pclass integer,\n  survived boolean,\n  name varchar(255),\n  sex varchar(255),\n  age real,\n  sibsp integer,\n  parch integer,\n  ticket varchar(255),\n  fare real,\n  cabin varchar(255),\n  embarked char(1),\n  boat varchar(8),\n   body varchar(8),\n  home_dest varchar(255)\n);\n\n```", "```py\nalexperrier@amlpacktdb=> dt\n List of relations\n Schema | Name | Type | Owner\n --------+---------+-------+-------------\n public | titanic | table | alexperrier\n (1 row)\n\n```", "```py\nalexperrier@amlpacktdb=> d+ titanic\n Table \"public.titanic\"\n Column | Type | Modifiers | Storage | Stats target | Description\n -----------+------------------------+------------------------------------------------------+----------+--------------+-------------\n id | integer | not null default nextval('titanic_id_seq'::regclass) |\n plain | | pclass | integer | | plain | |\n survived | boolean | | plain | |\n name | character varying(255) | | extended | |\n sex | character varying(255) | | extended | |\n age | real | | plain | |\n sibsp | integer | | plain | |\n parch | integer | | plain | |\n ticket | character varying(255) | | extended | |\n fare | real | | plain | |\n cabin | character varying(255) | | extended | |\n embarked | character(1) | | extended | |\n boat | character varying(8) | | extended | |\n body | character varying(8) | | extended | |\n home_dest | character varying(255) | | extended | |\n Indexes:\n \"titanic_pkey\" PRIMARY KEY, btree (id)\n\n```", "```py\n# Load file on S3\n$ aws s3 cp data/titanic.csv s3://aml.packt/data/ch9/\n# connect to database via psql\n$ psql --host=amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com --port=5439 --username=alexperrier --password --dbname=amlpacktdb\n# upload data from your S3 location into the titanic table\n$ copy titanic from 's3://aml.packt/data/ch9/titanic.csv' CREDENTIALS 'aws_access_key_id=<access key id>;aws_secret_access_key=<access secret key>' CSV;\n\n```", "```py\nalexperrier@amlpacktdb=> select count(*) from titanic;\n -[ RECORD 1 ]\n count | 1309\n\n```", "```py\n$ psql -h amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com -p 5439 -U alexperrier --password -d amlpacktdb\n\n```", "```py\n$ export REDSHIFT_CONNECT='-h amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com -p 5439 -U alexperrier -d amlpacktdb'\n\n```", "```py\n$ export PGPASSWORD=your_password\n\n```", "```py\n$ psql $REDSHIFT_CONNECT\n\n```", "```py\n $ psql $REDSHIFT_CONNECT\n alexperrier@amlpacktdb=> select count(*) from titanic;\n\n```", "```py\n $ psql $REDSHIFT_CONNECT -f my_file.sql\n\n```", "```py\n $ psql $REDSHIFT_CONNECT -c 'SELECT count(*) FROM my_table'\n\n```", "```py\nimport numpy as np\nn_samples = 1000\nde_linearize = lambda X: np.cos(1.5 * np.pi * X) + np.cos( 5 * np.pi * X )\nX = np.sort(np.random.rand(n_samples)) * 2\ny = de_linearize(X) + np.random.randn(n_samples) * 0.1\n\n```", "```py\nimport pandas as pd\ndf = pd.DataFrame( {'X':X, 'y': y} )\ndf = df.sample(frac=1) # shuffles the entire dataframe\ndf.to_csv('data/nonlinear.csv', index = False)\n\n```", "```py\nCREATE TABLE IF NOT EXISTS nonlinear (\n id integer primary key,\n x1 real,\n y real\n);\n\n```", "```py\ncopy nonlinear from 's3://aml.packt/data/ch9/nonlinear.csv' CREDENTIALS 'aws_access_key_id=<access key id>;aws_secret_access_key=<access secret key>' CSV;\n\n```", "```py\n$ psql $REDSHIFT_CONNECT -c \"select count(*) from nonlinear\"\n > count\n> 1000\n >(1 row)\n\n```", "```py\n{\n  \"version\" : \"1.0\",\n  \"rowId\" : null,\n  \"rowWeight\" : null,\n  \"targetAttributeName\" : \"y\",\n  \"dataFormat\" : \"CSV\",\n  \"dataFileContainsHeader\" : false,\n  \"attributes\" : [ {\n    \"attributeName\" : \"x\",\n    \"attributeType\" : \"NUMERIC\"\n  }, {\n    \"attributeName\" : \"y\",\n    \"attributeType\" : \"NUMERIC\"\n  } ],\n  \"excludedAttributeNames\" : [ ]\n}\n\n```", "```py\n{\n  \"groups\": {\n    \"NUMERIC_VARS_QB_500\": \"group('x')\"\n  },\n  \"assignments\": {},\n  \"outputs\": [\n    \"ALL_CATEGORICAL\",\n    \"quantile_bin(NUMERIC_VARS_QB_500,500)\"\n  ]\n}\n\n```", "```py\nfor each power from 2 to P:\n    write sql that extracts power 1 to P from the nonlinear table\n    do N times\n        Create training and evaluation datasource\n        Create model\n        Evaluate model\n        Get evaluation result\n        Delete datasource and model\n    Average results\n\n```", "```py\nresponse = client.create_data_source_from_redshift(\n    DataSourceId='string',\n    DataSourceName='string',\n    DataSpec={\n        'DatabaseInformation': {\n            'InstanceIdentifier': 'amlpackt',\n            'DatabaseName': 'amlpacktdb'\n        },\n        'SelectSqlQuery': 'select x, y from nonlinear order by random()',\n        'DatabaseCredentials': {\n            'Username': 'alexperrier',\n            'Password': 'my_password'\n        },\n    'S3StagingLocation': 's3://aml.packt/data/ch9/',\n    'DataRearrangement': '{\"splitting\":{\"percentBegin\":0,\"percentEnd\":70 }\n  }',\n    'DataSchemaUri': 's3://aml.packt/data/ch9/nonlinear.csv.schema'\n },\n RoleARN='arn:aws:iam::178277513911:role/service-role/AmazonMLRedshift_us-east-1_amlpackt',\n ComputeStatistics=True\n)\n\n```", "```py\nselect x, power(x,2) as x2, y from nonlinear order by random()\n\n```", "```py\nselect x, power(x,2) as x2, power(x,3) as x3, y from nonlinear order by random()\n\n```", "```py\n{\n  \"version\" : \"1.0\",\n  \"rowId\" : null,\n  \"rowWeight\" : null,\n  \"targetAttributeName\" : \"y\",\n  \"dataFormat\" : \"CSV\",\n  \"dataFileContainsHeader\" : false,\n  \"attributes\" : [ {\n    \"attributeName\" : \"x\",\n    \"attributeType\" : \"NUMERIC\"\n  }, {\n    \"attributeName\" : \"y\",\n    \"attributeType\" : \"NUMERIC\"\n  } ],\n  \"excludedAttributeNames\" : [ ]\n}\n\n```", "```py\n{ \n    \"attributeName\" : \"x{N}\", \n    \"attributeType\" : \"NUMERIC\"\n}\n\n```", "```py\n      def generate_sql(self, p):\n        powers = [ 'power(x,{0}) as x{0}'.format(i) for i in range(1,p+1) ]\n        return 'select ' + ','.join(powers) + ', y from nonlinear order by\n        random()'\n\n```", "```py\n      def generate_data_rearrangement(self,split):\n           if split == 'training':\n              pct_begin = 0\n              pct_end = 70\n           else:\n              pct_begin = 70\n              pct_end = 100\n       return json.dumps( { \"splitting\": \n       {\"percentBegin\":pct_begin,\"percentEnd\":pct_end } } )\n\n```", "```py\n      def generate_schema(self, p):\n      attributes = [ { \"attributeName\" : \"x{0}\".format(i), \"attributeType\"\n      : \"NUMERIC\" } for i in range(1,p+1) ]\n     attributes.append({ \"attributeName\" : \"y\", \"attributeType\" : \"NUMERIC\"  \n     })\n     return json.dumps({ \"version\" : \"1.0\",\n         \"rowId\" : None,\n         \"rowWeight\" : None,\n         \"targetAttributeName\" : \"y\",\n         \"dataFormat\" : \"CSV\",\n         \"dataFileContainsHeader\" : False,\n         \"attributes\" : attributes,\n         \"excludedAttributeNames\" : [ ]\n     })\n\n```", "```py\n      def create_datasource(self, p, k, split ):\n        print(\"Create datasource {0} {1} {2} {3}\".format(p,k,split, \n        self.prefix))\n        return self.client.create_data_source_from_redshift(\n        DataSourceId = \"ds_{2}_{3}_p{0}_{1}\".format(p,k,split, self.prefix),\n        DataSourceName = \"DS {2} {3} p{0} {1}\".format(p,k,split, \n         self.prefix),\n        DataSpec = {\n          'DatabaseInformation': {\n            'DatabaseName': 'amlpacktdb',\n            'ClusterIdentifier': 'amlpackt'\n           },\n           'SelectSqlQuery': self.generate_sql(p),\n           'DatabaseCredentials': {\n             'Username': 'alexperrier',\n             'Password': 'password'\n            },\n            'S3StagingLocation': 's3://aml.packt/data/ch9/',\n            'DataRearrangement': self.generate_data_rearrangement(split),\n            'DataSchema': self.generate_schema(p)\n          },\n          RoleARN='arn:aws:iam::178277513911:role/service-role\n          /AmazonMLRedshift_us-east-1_amlpackt',\n          ComputeStatistics=True\n        )\n\n```", "```py\n      def create_model(self, p, k):\n        print(\"Create model {0} {1} {2}\".format(p, k, self.prefix))\n        return self.client.create_ml_model(\n        MLModelId = \"mdl_{2}_p{0}_{1}\".format(p,k, self.prefix),\n        MLModelName = \"MDL {2} p{0} {1}\".format(p,k, self.prefix),\n        MLModelType = 'REGRESSION',\n        Parameters = self.sgd_parameters,\n        TrainingDataSourceId = self.ds_training['DataSourceId'] ,\n        Recipe = json.dumps(self.recipe)\n      )\n\n```", "```py\n      def create_evaluation(self, p, k):\n      print(\"Create evaluation {0} {1} {2}\".format(p, k, self.prefix))\n\n      return self.client.create_evaluation(\n      EvaluationId = \"eval_{2}_p{0}_{1}\".format(p,k, self.prefix),\n      EvaluationName = \"EVAL {2} p{0} {1}\".format(p,k, self.prefix),\n      MLModelId = self.model['MLModelId'],\n      EvaluationDataSourceId= self.ds_evaluation['DataSourceId']\n      )\n\n```", "```py\n# Initialize the object \nnl = NonLinear(max_p, n_crossval, prefix)\n# Run all the datasources, models and evaluations creation  \nnl.run_all_trials()\n# Wait until the evaluations are finished and get the results\nnl.get_results()\n# Export the results to a csv file\nnl.to_csv(filename)\n# Free the resources\nnl.delete_resources()\n\n```", "```py\nimport pandas as pd\nimport boto3\nimport json\nimport csv\n\nclass NonLinear():\n\n def __init__(self, max_p, n_crossval, prefix):\n self.trials = []\n self.max_p = max_p\n self.n_crossval = n_crossval\n self.prefix = prefix\n self.client = boto3.client('machinelearning')\n self.sgd_parameters = {\n \"sgd.shuffleType\": \"auto\",\n \"sgd.l2RegularizationAmount\": \"1.0E-06\",\n \"sgd.maxPasses\": \"100\"\n }\n\n self.recipe = {\n \"groups\" : {},\n \"assignments\" : { },\n \"outputs\": [\"ALL_INPUTS\"]\n # \"outputs\": [\"quantile_bin(ALL_NUMERIC,200)\"]\n }\n\n def run_all_trials(self):\n for p in range(1,self.max_p+1):\n for k in range(self.n_crossval):\n self.trials.append( self.run_trial(p,k) )\n\n def run_trial(self, p, k ):\n self.ds_training = self.create_datasource(p, k, 'training')\n self.ds_evaluation = self.create_datasource(p, k, 'evaluation')\n self.model = self.create_model(p,k)\n self.evaluation = self.create_evaluation(p,k)\n return {\n \"p\": p,\n \"k\": k,\n \"ds_training_id\": self.ds_training['DataSourceId'],\n \"ds_evaluation_id\": self.ds_evaluation['DataSourceId'],\n \"model_id\": self.model['MLModelId'],\n \"evaluation_id\": self.evaluation['EvaluationId'],\n \"rmse\": None\n }\n\n def get_results(self):\n results = []\n for trial in self.trials:\n\n waiter = self.client.get_waiter('evaluation_available')\n print(\"Waiting on evaluation {0} to finish \".format( trial['evaluation_id'] ) )\n waiter.wait(FilterVariable='DataSourceId', EQ=trial['ds_evaluation_id'] )\n\n response = self.client.get_evaluation( EvaluationId=trial['evaluation_id'] )\n rmse = float(response['PerformanceMetrics']['Properties']['RegressionRMSE'])\n trial[\"rmse\"] = rmse\n results.append(trial)\n print(\"Evaluation score {0}\".format(rmse))\n self.trials = results\n\n def delete_resources(self):\n # Now delete the resources\n print(\"Deleting datasources and model\")\n for trial in self.trials:\n response = self.client.delete_data_source(\n DataSourceId = trial['ds_training_id']\n )\n response = self.client.delete_data_source(\n DataSourceId = trial['ds_evaluation_id']\n )\n response = self.client.delete_ml_model(\n MLModelId = trial['model_id']\n )\n\n def to_csv(self, filename):\n print(\"exporting to csv {0}\".format(filename))\n keys = self.trials[0].keys()\n with open(filename, 'w') as output_file:\n dict_writer = csv.DictWriter(output_file, keys)\n dict_writer.writeheader()\n dict_writer.writerows(self.trials)\n\n```"]