- en: '*Chapter 3*: Anomaly Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Anomaly detection** was the original capability of Elastic ML and is the
    most mature, stretching its roots back to the Prelert days (before the acquisition
    by Elastic in 2016). This technology is robust, easy to use, powerful, and broadly
    applicable to all kinds of use cases for time series data.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: This jam-packed chapter will focus on using Elastic ML to detect anomalies in
    the occurrence rates of documents/events, rare occurrences of things, and numerical
    values outside of expected normal operation. We will run through some simple but
    effective examples that will highlight both the efficacy of Elastic ML and its
    ease of use.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML job types
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dissecting the detector
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting changes in event rates
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting changes in metric values
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the advanced detector functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting analysis along categorical features
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding temporal versus population analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorization analysis of unstructured messages
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Elastic ML via the API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The information in this chapter is based on the Elastic Stack as it exists
    in v7.10\. As with all of the chapters, all the example code can be found on GitHub:
    [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition.](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition
    )'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML job types
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we start using the Elastic ML UI to configure anomaly detection jobs,
    we will see that there are five different job wizards that are shown:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The Create job UI showing different configuration wizards'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – The Create job UI showing different configuration wizards
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The existence of these different configuration wizards implies that there are
    different "types" of jobs. In actuality, there is really only one job type—it
    is just that the anomaly detection job has many options, and many of these wizards
    make certain aspects of that configuration easier. Everything that you may desire
    to configure can be done via the **Advanced** wizard (or the API). In fact, when
    Elastic ML was first released as beta in v5.4, that was all that existed. Since
    then, the other wizards have been added for simplicity and usability in specific
    use cases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: An anomaly detection job has many configuration settings, but the two most important
    ones are the **analysis configuration** and the **datafeed**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The analysis configuration is the recipe for what anomalies the job will detect.
    It contains a detection configuration (called the **detector**) as well as a few
    other settings, such as the bucket span. The datafeed is the configuration of
    the query that will be executed by Elasticsearch to retrieve the data that is
    to be analyzed by the detector.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'With respect to the different job wizards, the following are true:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '*Jobs created by the Single metric wizard have only one detector*. Their datafeeds
    contain a query and aggregations, thus only sending summarized data to the ML
    algorithms. The aggregations are automatically created for you based upon your
    configuration parameters in the wizard. The job also makes use of a flag called
    `summary_count_field_name` (set with the value of `doc_count`) to signal that
    aggregated data (and not raw data from the source index) is to be expected.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单一指标向导创建的作业只有一个检测器。它们的数据源包含一个查询和聚合，因此只向机器学习算法发送汇总数据。聚合是自动为您根据向导中的配置参数创建的。作业还使用一个名为`summary_count_field_name`的标志（设置为`doc_count`的值）来表示预期的将是聚合数据（而不是来自源索引的原始数据）。
- en: '*Jobs created with the Multi-metric wizard can have one or more detectors*.
    The analysis can also be split along categorical fields by setting `partition_field_name`
    (described later in the chapter). Their datafeeds do not contain aggregations
    (because the ML code needs to see all documents for every possible instance of
    a field value and will aggregate it on its own), thus full Elasticsearch documents
    are passed to the ML algorithms.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多指标向导创建的作业可以有一个或多个检测器。分析还可以通过设置`partition_field_name`（在章节后面描述）来按分类字段拆分。它们的数据源不包含聚合（因为机器学习代码需要看到每个字段值的每个可能实例的所有文档，并将自行进行聚合），因此将完整的Elasticsearch文档传递给机器学习算法。
- en: '*Jobs created with the Population wizard can have one or more detectors*. The
    wizard also sets `over_field_name` (described later in the chapter), which signals
    that population analysis is to be used. The analysis can also be split along categorical
    fields by setting `by_field_name` (described later in the chapter). Their datafeeds
    do not contain aggregations, thus full Elasticsearch documents are passed to the
    ML algorithms.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用人口向导创建的作业可以有一个或多个检测器。向导还设置了`over_field_name`（在章节后面描述），表示将使用人口分析。分析还可以通过设置`by_field_name`（在章节后面描述）来按分类字段拆分。它们的数据源不包含聚合，因此将完整的Elasticsearch文档传递给机器学习算法。
- en: '*Jobs created with the Categorization wizard have only one detector*. The wizard
    also sets `categorization_field_name` (described later in the chapter), which
    signals that categorization analysis is to be used. Categorization analysis also
    sets `by_field_name` (described later in the chapter) to a value of `mlcategory`.
    The analysis can also be split along categorical fields by setting `partition_field_name`
    (described later in the chapter). Their datafeeds do not contain aggregations,
    thus full Elasticsearch documents are passed to the ML algorithms.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分类向导创建的作业只有一个检测器。向导还设置了`categorization_field_name`（在章节后面描述），表示将使用分类分析。分类分析还将`by_field_name`（在章节后面描述）设置为`mlcategory`的值。分析还可以通过设置`partition_field_name`（在章节后面描述）来按分类字段拆分。它们的数据源不包含聚合，因此将完整的Elasticsearch文档传递给机器学习算法。
- en: '*Jobs created with the Advanced wizard can leverage every option available*.
    The onus is on the user to know what they are doing and configure the job correctly.
    The UI does prevent the user from making most mistakes, however. An experienced
    user can exclusively use the Advanced wizard to create any anomaly detection job.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高级向导创建的作业可以利用所有可用的选项。用户需要知道自己在做什么，并正确配置作业。然而，UI可以防止用户犯大多数错误。有经验的用户可以专门使用高级向导创建任何异常检测作业。
- en: The options around job creation might seem daunting given what was just described.
    But do not fret—once we have gotten familiar with the terminology and have walked
    through some examples, you will find that the job configurations are very sensible;
    as more experience is gained, the configuration of jobs will become second nature.
    Let's take the next step and break down the components of the detector.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据刚才描述的，作业创建的选项可能看起来令人畏惧。但不要担心——一旦我们熟悉了术语并走过了几个示例，你会发现作业配置是非常合理的；随着经验的积累，作业的配置将变得自然而然。让我们继续下一步，分解检测器的组件。
- en: Dissecting the detector
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解构检测器
- en: 'At the heart of the anomaly detection job are the analysis configuration and
    the detector. The detector has several key components to it:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测作业的核心是分析配置和检测器。检测器有几个关键组件：
- en: The **function**
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**函数**'
- en: The **field**
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段**'
- en: The **partition field**
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区字段**'
- en: The **by field**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按字段**'
- en: The **over field**
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖字段**'
- en: We will go through each in turn to fully understand them all. Note that in the
    next few sections, however, we will often refer to the actual names of settings
    within the job configuration as if we were using the advanced job editor or the
    API. Although it is good to fully understand the nomenclature, as you progress
    through this chapter you will also notice that many of the details of the job
    configuration are abstracted away from the user or are given more "UI-friendly"
    labels than the real setting names.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一介绍它们，以便完全理解它们。请注意，然而，在接下来的几节中，我们经常会引用作业配置中的实际设置名称，就像我们正在使用高级作业编辑器或API一样。尽管完全理解命名法是好的，但随着你通过本章，你也会注意到许多作业配置的细节都被抽象化了，或者比实际的设置名称有更多的“UI友好”标签。
- en: The function
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 函数
- en: 'The detector **function** describes how the data will be aggregated or measured
    within the analysis interval (bucket span). There are many functions, but they
    can be classified into the following categories:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 检测器**函数**描述了数据将在分析间隔（桶跨度）内如何聚合或测量。有许多函数，但它们可以被归类为以下几类：
- en: '![Figure 3.2 – Table of detector functions'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – 检测器函数表'
- en: '](img/B17040_03_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_002.jpg)'
- en: Figure 3.2 – Table of detector functions
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 检测器函数表
- en: Items marked with an asterisk (`*`) also have high/low one-sided variants (such
    as `low_distinct_count`) that allow the detection of anomalies in only one direction.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 带有星号（`*`）的项目也有高/低单侧变体（如`low_distinct_count`），这允许仅在一个方向上检测异常。
- en: The field
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字段
- en: 'Some functions in the detector require a field within the data to operate on.
    Take the following examples:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 检测器中的一些函数需要在数据中操作一个字段。以下是一些例子：
- en: '`max(bytes)`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max(bytes)`'
- en: '`mean(products.price)`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean(products.price)`'
- en: '`high_distinct_count(destination.port)`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`high_distinct_count(destination.port)`'
- en: Therefore, the name of the field the function directly operates on is simply
    called `field_name`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该函数直接操作的字段名称简单地称为`field_name`。
- en: The partition field
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分配字段
- en: There are often cases in which the detection analysis needs to be split along
    a categorical field so the analysis can be done separately for all unique instances
    of that field. In this case, the `partition` field (the setting is called `partition_field_name`)
    defines the field to split on. For example, in e-commerce, you might want to see
    the average revenue per category (men's clothing, women's accessories, and so
    on). In this case, the `category` field would be the `partition` field. We will
    explore the splitting of the analysis later in this chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 经常会有这样的情况，检测分析需要沿着一个分类字段进行拆分，以便对该字段的所有唯一实例分别进行分析。在这种情况下，`partition`字段（设置称为`partition_field_name`）定义了要拆分的字段。例如，在电子商务中，你可能想查看每个类别的平均收入（男士服装、女士配饰等）。在这种情况下，`category`字段将是`partition`字段。我们将在本章后面探讨分析拆分。
- en: The by field
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区字段
- en: Similar to the `partition` field, the `by` field (the setting is called `by_field_name`)
    is another mechanism to split the analysis, but it behaves differently with respect
    to how the results are modeled and scored. Additionally, the `by` field is mandatory
    if `rare` or `freq_rare` is used. More details on the differences in using the
    `by` field for splitting versus using the `partition` field will be discussed
    later in the chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与`partition`字段类似，`by`字段（设置称为`by_field_name`）是另一种拆分分析的手段，但在如何建模和评分结果方面表现不同。此外，如果使用`rare`或`freq_rare`，则`by`字段是强制性的。关于使用`by`字段进行拆分与使用`partition`字段进行拆分的差异的更多细节将在本章后面讨论。
- en: The over field
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超字段
- en: The `over_field_name`) signals to the anomaly detection algorithms that **population
    analysis** is desired, where entities are compared to their peers (instead of
    against their own past behavior). Population analysis is discussed in depth later
    in this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`over_field_name`)向异常检测算法发出信号，希望进行**总体分析**，其中实体与其同伴进行比较（而不是与自己的过去行为进行比较）。总体分析将在本章后面进行深入讨论。'
- en: The "formula"
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “公式”
- en: 'If we were to document all of the possible configuration options for a detector
    and then create a flow chart-like map, it would look like the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要记录一个检测器的所有可能的配置选项，然后创建一个类似于流程图的地图，它将看起来如下：
- en: '![Figure 3.3 – The "formula" for building a detector from scratch'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3 – 从零开始构建检测器的“公式”'
- en: '](img/B17040_03_003.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_003.jpg)'
- en: Figure 3.3 – The "formula" for building a detector from scratch
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 从零开始构建检测器的“公式”
- en: 'The following are things to note about the diagram shown in *Figure 3.3*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于*图3.3*中所示图表的注意事项：
- en: Capitalized text is the explanation and italics text is the detector configuration
    settings (`by_field_name`, `partition_field_name`, and `over_field_name` are shortened
    to be simply `by`, `partition`, and `over`).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大写文本是解释，斜体文本是检测器配置设置（`by_field_name`、`partition_field_name`和`over_field_name`简化为`by`、`partition`和`over`）。
- en: Items in square brackets are optional (high, low, non-zero, non-null).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方括号中的项是可选的（高、低、非零、非空）。
- en: Choose only one exit branch (notice only one exit branch out of `rare`/`freq_rare`
    because `by` is mandatory).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只选择一个退出分支（注意`rare`/`freq_rare`中只有一个退出分支，因为`by`是强制性的）。
- en: Comparison of something versus its own history is accomplished simply by *not*
    choosing an `over` field.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不选择`over`字段，简单地比较某事物与其自身历史记录。
- en: With a comprehensive understanding of the construction of a detector, we will
    now move into practical examples of using detectors for different use cases. First,
    we will explore the count functions that allow us to detect changes in event rates
    over time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在对检测器结构有全面理解之后，我们现在将进入使用检测器针对不同用例的实用示例。首先，我们将探索允许我们检测事件率随时间变化的计数函数。
- en: Detecting changes in event rates
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 检测事件率的变化
- en: 'There are many important use cases that revolve around the idea of event change
    detection. These include the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多重要的用例围绕着事件变化检测的概念。以下是一些例子：
- en: Discovering a flood of error messages suddenly cropping up in a log file
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在日志文件中突然出现大量错误消息
- en: Detecting a sudden drop in the number of orders processed by an online system
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测在线系统处理的订单数量的突然下降
- en: Determining a sudden excessive number of attempts at accessing something (for
    example, a sudden increase in the number of login attempts on a particular user
    ID)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定对某事物进行访问尝试的突然过多（例如，特定用户 ID 上登录尝试数量的突然增加）
- en: In order for us to find the abnormal, we must first have a mechanism to understand
    the normal rate of occurrence. But relying on our fallible human observation and
    intuition is not always the easiest (or most reliable) approach.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们能够找到异常，我们首先必须有一个机制来理解正常发生率。但是，依赖我们易出错的人类观察和直觉并不总是最容易（或最可靠）的方法。
- en: Exploring the count functions
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索计数函数
- en: 'As mentioned in [*Chapter 2*](B17040_02_Epub_AM.xhtml#_idTextAnchor033), *Enabling
    and Operationalization*, Elastic ML jobs have an anomaly detection "recipe" known
    as the **detector**. The detector is key to defining what anomalies the user wants
    to detect. Within the detector is the **function**, which selects the "feature"
    of what is to be detected. In the case of the count functions, the feature is
    the occurrence rate of something over time. There are three main count functions
    that we will see:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第 2 章*](B17040_02_Epub_AM.xhtml#_idTextAnchor033)中所述，*启用和实施*，Elastic ML 作业有一个称为**检测器**的异常检测“配方”。检测器是定义用户想要检测的异常的关键。在检测器中是**函数**，它选择要检测的“特征”。在计数函数的情况下，特征是某事物随时间发生的频率。我们将看到三个主要的计数函数：
- en: '`count`: Counts the number of documents in the bucket resulting from a query
    of the raw data index'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count`：计算从原始数据索引查询中得到的桶中文档的数量'
- en: '`high_count`: The same as `count`, but will only flag an anomaly if the count
    is higher than expected'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`high_count`：与`count`相同，但只有当计数高于预期时才会标记异常'
- en: '`low_count`: The same as `count`, but will only flag an anomaly if the count
    is lower than expected'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_count`：与`count`相同，但只有当计数低于预期时才会标记异常'
- en: We will see that there are a variety of one-sided functions in Elastic ML (to
    only detect anomalies in a certain direction). Additionally, it is important to
    know that the count functions are not counting a field or even the existence of
    fields within a document; they are merely counting the documents in the index
    over time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到 Elastic ML 中有许多单侧函数（仅用于检测某一方向上的异常）。此外，重要的是要知道计数函数并不是在计数字段或文档中字段的存不存在；它们只是在索引中随时间计数文档。
- en: 'To get a more intuitive feeling for what the count functions do, let''s jump
    into a simple example using the sample data within Kibana:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地了解计数函数的作用，让我们通过 Kibana 内的示例数据来举一个简单的例子：
- en: To enable the sample data, from the Kibana home screen, click on the **Add data**
    button (in either location) as shown in *Figure 3.4*:![Figure 3.4 – The Kibana
    home screen with Add data options
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启用示例数据，从 Kibana 主屏幕点击**添加数据**按钮（任一位置），如图 3.4 所示：![图 3.4 – 带有添加数据选项的 Kibana
    主屏幕
- en: '](img/B17040_03_004.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_03_004.jpg]'
- en: Figure 3.4 – The Kibana home screen with Add data options
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.4 – 带有添加数据选项的Kibana主屏幕
- en: After clicking on **Add data**, select **Sample data** to reveal three sets
    of data:![Figure 3.5 – Adding sample data
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加数据**后，选择**样本数据**以显示三组数据：![图3.5 – 添加样本数据
- en: '](img/B17040_03_005.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_005.jpg)'
- en: Figure 3.5 – Adding sample data
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.5 – 添加样本数据
- en: Click on each of the three **Add data** buttons in each section to load that
    sample dataset into your Elastic Stack. Once the loading is complete, we will
    jump directly to ML by selecting the three-horizontal-lines menu icon (![Text
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击每个部分的三个**添加数据**按钮，将样本数据集加载到你的Elastic Stack中。一旦加载完成，我们将通过选择三个横线菜单图标（![文本
- en: Description automatically generated](img/B17040_03_032.png)) at the top left
    of Kibana to reveal the list of apps, and then select **Machine Learning**:![Figure
    3.6 – Selecting Machine Learning from the Kibana apps menu
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B17040_03_032.png)) 在Kibana的左上角以显示应用程序列表，然后选择**机器学习**：![图3.6 – 从Kibana应用程序菜单中选择机器学习
- en: '](img/B17040_03_006.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_006.jpg)'
- en: Figure 3.6 – Selecting Machine Learning from the Kibana apps menu
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.6 – 从Kibana应用程序菜单中选择机器学习
- en: Once this has been clicked, we will be on the ML overview page, where we can
    immediately see where we can create our first anomaly detection job. Click on
    the **Create job** button, as shown in *Figure 3.7*:![Figure 3.7 – Elastic Cloud
    welcome screen
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦点击，我们将进入ML概览页面，在那里我们可以立即看到我们可以创建我们的第一个异常检测作业。点击**创建作业**按钮，如图3.7所示：![图3.7 –
    Elastic Cloud欢迎屏幕
- en: '](img/B17040_03_007.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_007.jpg)'
- en: Figure 3.7 – Elastic Cloud welcome screen
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.7 – Elastic Cloud欢迎屏幕
- en: Our next task is to select the index pattern (marked with an index with shards
    icon) or a saved search (marked with a magnifying glass icon) that contains the
    data that we'd like to analyze. If a saved search is chosen, then a filtered query
    that was previously created and saved within Kibana's `kibana_sample_data_logs`
    index, as we want to pass every document in that index through Elastic ML:![Figure
    3.8 – Selecting the kibana_sample_data_logs index for analysis
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来的任务是选择索引模式（带有索引分片图标标记）或一个已保存的搜索（带有放大镜图标标记），其中包含我们想要分析的数据。如果选择了一个已保存的搜索，那么一个之前在Kibana的`kibana_sample_data_logs`索引中创建并保存的过滤查询，因为我们希望将那个索引中的每个文档都通过Elastic
    ML：![图3.8 – 选择kibana_sample_data_logs索引进行分析
- en: '](img/B17040_03_008.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_008.jpg)'
- en: Figure 3.8 – Selecting the kibana_sample_data_logs index for analysis
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.8 – 选择kibana_sample_data_logs索引进行分析
- en: 'On the next screen, we will select the **Single metric** job wizard because,
    at this point, we''re interested in analyzing only one aspect of the data: its
    count over time:![Figure 3.9 – Choosing a Single metric job'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏幕上，我们将选择**单个度量**作业向导，因为此时我们只对分析数据的单一方面感兴趣：其随时间的变化计数：![图3.9 – 选择单个度量作业
- en: '](img/B17040_03_009.jpg)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_009.jpg)'
- en: Figure 3.9 – Choosing a Single metric job
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.9 – 选择单个度量作业
- en: On the next screen, in order to follow along with this example, *you must select
    the* **Use full kibana_sample_logs_data** *button in order to include the sample
    anomaly in this dataset*:![Figure 3.10 – Selecting to use all the data within
    the index
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏幕上，为了跟随这个示例，*你必须选择* **使用完整的kibana_sample_logs_data** *按钮，以便将样本异常包含在这个数据集中*：![图3.10
    – 选择使用索引中的所有数据
- en: '](img/B17040_03_010.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_03_010.jpg)'
- en: Figure 3.10 – Selecting to use all the data within the index
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.10 – 选择使用索引中的所有数据
- en: Note
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This demo data, when installed, actually puts about half of the data in the
    past and half in the future (by dynamically modifying the timestamps on ingest).
    This is done to provide a mechanism for the static data to look "real-time" when
    dashboards are viewed on data in the "last hour," for example. As a result of
    this, we're really going to ask Elastic ML to analyze data from the past and the
    future, where normally it would be impossible to have data from the future. Suspend
    belief for now for the sake of the example as the anomaly we'd like to demonstrate
    is in the second half of the dataset.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当此演示数据安装时，实际上将大约一半的数据放在过去，另一半放在未来（通过动态修改摄取时间戳）。这样做是为了提供一个机制，使得静态数据在查看“过去一小时”的数据仪表板时看起来像是“实时”的。因此，我们实际上会要求Elastic
    ML分析过去和未来的数据，而通常情况下，未来的数据是无法获得的。为了这个示例，现在暂时放下怀疑，因为我们想展示的异常在数据集的第二部分。
- en: Now, click the **Next** button to advance to the next step in the configuration
    wizard.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，点击**下一步**按钮以进入配置向导的下一步。
- en: After clicking the **Next** button, we will need to select what we want to analyze
    from the **Pick fields** drop-down box. We will select **Count(Event rate)** to
    focus on our original goal here, which is to detect changes in the event rate
    in this index over time:![Figure 3.11 – Selecting the count of events over time
    as our detection
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮后，我们需要从**选择字段**下拉框中选择我们想要分析的内容。我们将选择**事件计数（事件率）**，以关注我们的原始目标，即检测在此索引中事件率随时间的变化：![图3.11
    – 选择事件计数随时间变化作为我们的检测
- en: '](img/B17040_03_011.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_011.jpg)'
- en: Figure 3.11 – Selecting the count of events over time as our detection
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.11 – 选择事件计数随时间变化作为我们的检测
- en: Notice that looking through this drop-down box shows that other analyses could
    be done, depending on the data type of the field in the data. We will explore
    some of these other options later on in subsequent examples.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，查看这个下拉框会显示，根据数据中字段的数据类型，可以进行其他分析。我们将在后续示例中探索这些其他选项。
- en: Click the **Next** button to proceed, leaving the other options as their defaults
    for now.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮继续，现在保留其他选项为默认值。
- en: Now, we need to name our anomaly detection job. In the `web_logs_rate` was used:![Figure
    3.12 – Naming the anomaly detection job
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要为我们的异常检测工作命名。在`web_logs_rate`中使用：![图3.12 – 为异常检测工作命名
- en: '](img/B17040_03_012.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_012.jpg)'
- en: Figure 3.12 – Naming the anomaly detection job
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.12 – 为异常检测工作命名
- en: Again, leave the other options as their defaults and click the **Next** button.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次，将其他选项保留为默认值，并点击**下一步**按钮。
- en: A validation step takes place to ensure that everything is reasonable for the
    analysis to work:![Figure 3.13 – Job validation step
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行验证步骤以确保分析合理：![图3.13 – 工作验证步骤
- en: '](img/B17040_03_013.jpg)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_013.jpg)'
- en: Figure 3.13 – Job validation step
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.13 – 工作验证步骤
- en: Click the **Next** button to proceed.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮继续。
- en: At this point, the job is ready to be created (and notice in *Figure 3.14* that
    some sensible default options, such as **Model memory limit** and **Enable model
    plot**, were chosen for you):![Figure 3.14 – Anomaly detection job ready to be
    created](img/B17040_03_014.jpg)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，工作已经准备好创建（注意在*图3.14*中，一些合理的默认选项，例如**模型内存限制**和**启用模型绘图**，已经为您选择了）：![图3.14
    – 准备创建的异常检测工作](img/B17040_03_014.jpg)
- en: Figure 3.14 – Anomaly detection job ready to be created
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.14 – 准备创建的异常检测工作
- en: After the **Create job** button is clicked, you will see an animated preview
    of the results superimposed on top of the data, as follows:![Figure 3.15 – Results
    preview of the job execution displayed
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建工作**按钮后，您将看到结果动画预览叠加在数据上方，如下所示：![图3.15 – 显示工作执行结果的预览
- en: '](img/B17040_03_015.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_015.jpg)'
- en: Figure 3.15 – Results preview of the job execution displayed
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.15 – 显示工作执行结果的预览
- en: Let's now click the **View results** button to investigate in detail what the
    anomaly detection job has found in the data.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们点击**查看结果**按钮，以详细了解异常检测工作在数据中发现了什么。
- en: 'Using the scrubber below the main graph, adjust the location and width of the
    viewing area to zoom in on the big spike:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用主图下方的刮擦器，调整查看区域的定位和宽度，以便放大查看大峰值：
- en: '![Figure 3.16 – Results shown for a critical anomaly'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.16 – 显示关键异常的结果'
- en: '](img/B17040_03_016.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_03_016.jpg)'
- en: Figure 3.16 – Results shown for a critical anomaly
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – 显示关键异常的结果
- en: Note
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As you zoom in, out, and around, just be aware of the chart aggregation interval
    as compared to the job's bucket span (as circled in *Figure 3.16*). If you are
    zoomed out to a wider view, the chart aggregation interval can be larger than
    the job's bucket span, making the position of the drawn anomaly on the chart less
    exact.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当您放大、缩小和移动时，请注意图表聚合间隔与工作桶跨度（如图3.16中圈出所示）的比较。如果您放大到一个更宽的视图，图表聚合间隔可以大于工作桶跨度，使得图表上绘制的异常位置不够精确。
- en: 'Here, in *Figure 3.16*, we can see that the very large spike in events was
    flagged as two distinct anomalies because the actual number of web requests seen
    in the logs was around 11 times higher than expected (given the learned model
    of the data up until that point in time). You may notice that the chart shows
    two anomalies next to each other because clearly, the spike in events spanned
    more than one 15-minute bucket interval. You may also notice that by default,
    there is only one anomaly shown in the table below the chart. This is because
    **Interval** defaults to **Auto** and time-adjacent anomalies are summarized together,
    with only the highest score shown. If **Interval** is changed to **Show all**,
    then both anomaly records are listed in the table:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在*图3.16*中，我们可以看到事件的大幅上升被标记为两个不同的异常，因为日志中实际看到的Web请求数量比预期的高出大约11倍（考虑到直到那个时间点之前学习到的数据模型）。你可能注意到图表显示了两个相邻的异常，因为显然事件的上升跨越了多个15分钟的桶间隔。你也可能注意到，默认情况下，图表下面的表格中只显示了一个异常。这是因为**间隔**默认设置为**自动**，时间相邻的异常被汇总在一起，只显示最高分。如果**间隔**改为**显示所有**，那么两个异常记录都会列在表格中：
- en: '![Figure 3.17 – Interval set to Show all anomalies'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.17 – 设置为显示所有异常'
- en: '](img/B17040_03_017.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_017.jpg)'
- en: Figure 3.17 – Interval set to Show all anomalies
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 设置为显示所有异常
- en: 'There''s one final thing to notice in this example, which is the other, lesser-scored
    anomalies earlier in the dataset:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，还有一点需要注意，那就是数据集中较早出现的其他低分异常：
- en: '![Figure 3.18 – Multi-bucket anomalies'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.18 – 多桶异常'
- en: '](img/B17040_03_018.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_018.jpg)'
- en: Figure 3.18 – Multi-bucket anomalies
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 – 多桶异常
- en: 'There are a few key things to recognize about these less-than-obvious anomalies:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些不太明显的异常，有几个关键点需要注意：
- en: They have lower scores than the massive spike we just investigated because relatively
    speaking, these are not as anomalous, but are interestingly significant.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们的分数低于我们刚刚调查的大幅波动，因为相对而言，这些并不那么异常，但有趣的是，它们具有显著性。
- en: The anomaly here is the "lack" of expected values. In other words, the `count`
    function interprets *no data* as 0 and that can be anomalous if normally there's
    an expectation that events should be occurring.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里的异常是“预期值”的“缺乏”。换句话说，`count`函数将“无数据”解释为0，如果通常情况下预期事件应该发生，那么这可能是异常的。
- en: These anomalies are not single-bucket anomalies, but rather **multi-bucket anomalies**.
    Multi-bucket anomalies are designated with a different symbol in the UI (a cross
    instead of a dot). They denote cases in which the actual singular value may not
    necessarily be anomalous, but there is a trend that is occurring in a sliding
    window of 12 consecutive buckets. Here, you can see that there is a noticeable
    slump spanning several adjacent buckets.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些异常不是单个桶的异常，而是**多桶异常**。在UI中，多桶异常用不同的符号表示（一个十字而不是一个点）。它们表示实际的单个值可能并不一定是异常的，但在12个连续桶的滑动窗口中存在一个趋势。在这里，你可以看到有几个相邻桶出现了一个明显的下滑。
- en: Note
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on interpreting multi-bucket anomalies, see the detailed
    blog post at [elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features](http://elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features).
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于解释多桶异常的更多信息，请参阅[elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features](http://elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features)上的详细博客文章。
- en: We have seen, through this example, how the count function allows us to easily
    detect an obvious (and not-so-obvious) set of anomalies relating to the overall
    rate of occurrence of events (documents) in an index over time. Let's continue
    our journey by looking at other count and occurrence-based functions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个例子，我们已经看到了`count`函数如何使我们能够轻松地检测到一组明显（和不那么明显）的异常，这些异常与索引中事件（文档）随时间发生的总体发生率有关。让我们继续我们的旅程，通过查看其他基于计数和发生率的函数。
- en: Other counting functions
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他计数函数
- en: In addition to the functions that we've described so far, there are several
    other counting functions that enable a broader set of use cases.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前描述的函数之外，还有几个其他计数函数，可以支持更广泛的使用场景。
- en: Non-zero count
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非零计数
- en: 'The non-zero count functions (`non_zero_count`, `low_non_zero_count`, and `high_non_zero_count`)
    allow the handling of count-based analysis, as well as allowing accurate modeling
    in cases where the data may be sparse and you would not want the non-existence
    of data to be explicitly treated as zero, but rather as null—in other words, a
    dataset in time, which looks like the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 非零计数函数（`non_zero_count`、`low_non_zero_count`和`high_non_zero_count`）允许处理基于计数的分析，同时允许在数据可能稀疏且您不希望将数据不存在明确地视为零，而是将其视为空值的情况下进行准确建模——换句话说，一个看起来像以下的时间序列数据集：
- en: '[PRE0]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Data with the `non_zero_count` functions will be interpreted as the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`non_zero_count`函数的数据将被解释为以下内容：
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The act of treating zeros as null can be useful in cases where the non-existence
    of measurements at regular intervals is expected. Some practical examples of this
    are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 将零视为空值在预期到在常规间隔内不存在测量值的情况下是有用的。以下是一些实际例子：
- en: The number of airline tickets purchased per month by an individual
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月个人购买的航空机票数量
- en: The number of times a server reboots in a day
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每天服务器重启的次数
- en: The number of login attempts on a system per hour
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每小时系统登录尝试的次数
- en: 'To select the non-zero count version of the count functions in the job wizards,
    just toggle the **Sparse data** option during the setup:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要在作业向导中选择计数函数的非零计数版本，只需在设置期间切换**稀疏数据**选项：
- en: '![Figure 3.19 – Adding the Sparse data option to select the non-zero count'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.19 – 添加稀疏数据选项以选择非零计数'
- en: '](img/B17040_03_019.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_019.jpg)'
- en: Figure 3.19 – Adding the Sparse data option to select the non-zero count
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19 – 添加稀疏数据选项以选择非零计数
- en: We will see later in the chapter, when we are configuring the jobs in the advanced
    job wizard or via the API, that we will be explicitly using function names (such
    as `high_non_zero_count`) instead of toggling options with more conceptual descriptions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面看到，当我们通过高级作业向导或API配置作业时，我们将明确使用函数名（如`high_non_zero_count`）而不是使用更具概念描述的选项切换。
- en: Distinct count
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独特计数
- en: 'The distinct count functions (`distinct_count`, `low_distinct_count`, and `high_distinct_count`)
    measure the uniqueness (`distinct_count(url.keyword)` as the detector configuration
    in the last example on the `kibana_sample_data_logs` index, we would have caught
    the same anomalous timeframe, but for a different reason—not only was the overall
    volume of requests high, as we saw back in *Figure 3.16*, but here in *Figure
    3.20*, we see that there was a high diversity of URLs being requested:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 独特计数函数（`distinct_count`、`low_distinct_count`和`high_distinct_count`）测量唯一性（在`kibana_sample_data_logs`索引的最后一个示例中，`distinct_count(url.keyword)`作为检测器配置，我们会捕捉到相同的时间段异常，但原因不同——不仅请求的整体量很高，正如我们在*图3.16*中看到的，而且在*图3.20*中，我们看到请求的URL多样性很高）：
- en: '![Figure 3.20 – Distinct count detector example'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.20 – 独特计数检测器示例'
- en: '](img/B17040_03_020.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_03_020.jpg)'
- en: Figure 3.20 – Distinct count detector example
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 – 独特计数检测器示例
- en: With an appreciation of count-based functions, let's now turn to metric-based
    functions, which allow us to analyze numerical fields in the data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解基于计数的功能之后，我们现在转向基于度量的功能，这些功能使我们能够分析数据中的数值字段。
- en: Detecting changes in metric values
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测度量值的变化
- en: Obviously, not all data being emitted from systems will be text or categorical
    in nature—a vast amount of it is numerical. Detecting changes in metric values
    over time is perfectly suited for anomaly detection because, as mentioned in [*Chapter
    1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016), *Machine Learning for IT*, the
    historical paradigm of alerting on exceptions in numerical values via static thresholds
    has been troublesome for decades. Let's explore all that Elastic ML has to offer
    with respect to the functions that help you detect changes in numerical fields
    in your data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，并非所有从系统中发出的数据都将是有文本或分类性质的——其中大量的是数值数据。检测度量值随时间的变化非常适合用于异常检测，因为正如在[*第一章*](B17040_01_Epub_AM.xhtml#_idTextAnchor016)《IT机器学习》中提到的，通过静态阈值在数值值上的异常警报的历史范式已经困扰了数十年。让我们一起来探索Elastic
    ML在帮助您检测数据中数值字段变化的功能方面所能提供的一切。
- en: Metric functions
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 度量函数
- en: Metric functions operate on numerical fields and return numerical values. They
    are perhaps the easiest of the detector functions to understand.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 度量函数在数值字段上操作并返回数值。它们可能是最容易理解的检测器函数。
- en: min, max, mean, median, and metric
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小值，最大值，平均值，中位数和度量
- en: 'These functions do exactly as you would expect: they return the minimum, maximum,
    average/mean, and median of all of the numerical observations for the field of
    interest in the bucket span.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The `metric` function is a little unique in that it is really just a shorthand
    way of specifying that `min`, `max`, and `mean` are to be used together.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that if the frequency of the data (for example, data that
    comes from a sampling source such as Metricbeat) exactly matches the bucket span,
    then there is only one sample per bucket span. This means that the minimum, maximum,
    average/mean, and median of the field of interest are all the same value (the
    value of the single observation itself). Therefore, if possible, it is usually
    better to have multiple numerical samples per bucket span if you want to have
    discrimination using these functions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Another fact to note is that these metric functions treat the lack of data as
    *null*. In other words, if your data is sparse and there are bucket spans in which
    no observations are seen, the lack of data will not "drag down" the statistics
    for the field of interest. This is why these metric-based functions have no "non-zero"
    or "non-null" counterpart.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: varp
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `varp` function measures the overall variance of a metric over time—its
    volatility. Using this function might be applicable to finding cases where the
    numerical value of a field should normally be rather consistent, but you would
    like to detect whether there was a change.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Sum and non-null sum
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `sum` function will return the sum of all of the numerical observations
    for the field of interest in the bucket span. Use the "non-null" version if you
    have sparse data and do not want the lack of data being treated as *zero*, which
    will inevitably "drag down" the value of the sum.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'If we had selected `sum(bytes)` as the detector configuration in the last example
    on the `kibana_sample_data_logs` index, we would have caught the same anomalous
    timeframe, but for a different reason—we see that the requests made also resulted
    in a higher quantity of bytes being transferred from the web server:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Sum detector example'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_021.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 – Sum detector example
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: This is totally sensible, given that an increased number of requests to a web
    server will correlate with an increase in the number of bytes being transferred.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an appreciation for the simpler detector functions, let's move
    on to the more complex, advanced functions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the advanced detector functions
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the detector functions mentioned so far, there are also a few
    other, more advanced functions that allow some very unique capabilities. Some
    of these functions are only available if the ML job is configured via the advanced
    job wizard or via the API.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: rare
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of a stream of temporal information (such as a log file), the
    notion of something being statistically rare (occurring at a low frequency) is
    paradoxically both intuitive and hard to understand. If I were asked, for example,
    to trawl through a log file and find a rare message, I might be tempted to label
    the first novel message that I saw as a rare one. But what if practically every
    message was novel? Are they all rare? Or is nothing rare?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间信息流（如日志文件）的上下文中，某事物在统计上稀有（以低频率发生）的概念既直观又难以理解。例如，如果我被要求浏览日志文件并找到一条稀有信息，我可能会倾向于将我看到的第一条新颖信息标记为稀有。但如果我们几乎每条信息都是新颖的怎么办？它们都是稀有的吗？或者什么都不是稀有的？
- en: In order to define rarity to be useful in the context of a stream of events
    in time, we need to agree that the declaration of something as being rare must
    take into account the context in which it exists. If there are lots of other routine
    things and a small number of unique things, then we can deem the unique things
    rare. If there are many unique things, then we will deem that nothing is rare.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在时间事件流的环境中定义稀有性以使其有用，我们需要同意，将某事物声明为稀有必须考虑其存在的上下文。如果有许多其他常规事物和少量独特事物，那么我们可以认为独特事物是稀有的。如果有许多独特事物，那么我们将认为没有什么事物是稀有的。
- en: 'When applying the `rare` function in an ML job, there is a requirement to declare
    which field the `rare` function is focusing on. This field is then defined as
    `by_field_name`. Configuration of the `rare` function does not have its own wizard
    in the Elastic ML UI, so you will need to define it using the advanced job wizard.
    For example, to find log entries that reference a rare country name, structure
    your detector similar to this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML作业中应用`rare`函数时，需要声明`rare`函数关注的字段。该字段随后被定义为`by_field_name`。`rare`函数的配置在Elastic
    ML UI中没有自己的向导，因此您需要使用高级作业向导来定义它。例如，为了找到引用稀有国家名称的日志条目，将探测器结构设计如下：
- en: '![Figure 3.22 – Rare detector example'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.22 – 稀有探测器示例'
- en: '](img/B17040_03_022.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_03_022.jpg]'
- en: Figure 3.22 – Rare detector example
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 – 稀有探测器示例
- en: This could be handy for finding unexpected geographical access (as in "Our admins
    usually log in from the New York and London offices almost daily, but never from
    Moscow!").
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于查找意外的地理访问（例如，“我们的管理员几乎每天都从纽约和伦敦办公室登录，但从未从莫斯科登录！”）可能很有用。
- en: Frequency rare
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频率稀有
- en: The `freq_rare` function is a specialized version of `rare`, in that it looks
    for members of a population that cause rare values of `by_field_name` to occur
    frequently. For example, you could locate a particular IP address that is attempting
    to access many rare URLs that are not generally seen across the entire population
    of all client IP addresses. This IP address could be attempting to access otherwise
    hidden sections of a website in a nefarious way, or may be attempting attacks
    such as SQL injection.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`freq_rare`函数是`rare`的一个专用版本，它寻找导致`by_field_name`出现稀有值的群体成员。例如，您可以定位一个试图访问许多罕见URL的特定IP地址，这些URL在整个客户端IP地址群体中通常看不到。这个IP地址可能正在以恶意的方式尝试访问网站的隐藏部分，或者可能正在尝试SQL注入等攻击。'
- en: Information content
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息含量
- en: The `info_content` function is perhaps the most specialized detector function
    in Elastic ML's arsenal. It was originally written as a means to measure the amount
    of **entropy** in text strings (how many and how diverse the characters are).
    This is because there are well-known techniques in malware that encrypt instructions
    and/or payload data for transmission for **command and control** (**C2**) and
    data exfiltration activity. Detecting this activity along this feature of the
    data is more reliable than looking at other features (such as the number of bytes
    sent or counting distinct entities).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`info_content`函数可能是Elastic ML工具箱中最专业的探测器函数。它最初被编写为测量文本字符串中**熵**的量（字符的数量和多样性）。这是因为已知在恶意软件中有加密指令和/或有效载荷数据以进行**命令和控制**（**C2**）和数据泄露活动的技术。通过数据的这一特征检测此活动比查看其他特征（如发送的字节数或计数不同的实体）更可靠。'
- en: 'The algorithm used will essentially do the following steps:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的算法将基本上执行以下步骤：
- en: Sort the unique strings into alphabetical order.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将独特的字符串按字母顺序排序。
- en: Concatenate those unique strings into one long string.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将那些独特的字符串连接成一个长字符串。
- en: Perform the `gzip` algorithm on that long string to compress it.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对那个长字符串执行`gzip`算法以压缩它。
- en: The information content is the length of the compressed data.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息含量是压缩数据的长度。
- en: Some of the ML jobs in the Elastic SIEM utilize the `info_content` function—stay
    tuned for [*Chapter 8*](B17040_08_Epub_AM.xhtml#_idTextAnchor146), *Anomaly Detection
    in Other Elastic Stack Apps*, for more details.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Geographic
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you find a geographic location that is unusual to a learned location area
    on Earth, then the `lat_long` function will be helpful, taking a `field_name`
    argument that is a comma-separated pair of numbers in the range of -180 to 180
    (for example, `40.75, -73.99`, the coordinates of Times Square in New York City).
    The `lat_long` function can also operate on a `geo_point` field, a `geo_shape`
    field that contains point values, or a `geo_centroid` aggregation. An example
    use case would be to flag a location that isn't normal (and potentially fraudulent
    or malicious) for a specific user, transaction, and so on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Time
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all things occur randomly in time, especially with things involving human
    behavior. We may eat, commute, or log into certain systems at predictable times
    of the day or week. Using the `time_of_day` and `time_of_week` functions, you
    can detect changes of behavior from a learned temporal routine. If a behavior
    is predictable on a 24-hour timeframe, then `time_of_day` is more appropriate.
    If the routine is day-of-the-week-dependent, then `time_of_week` should be a more
    logical choice.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse the usage of these time functions with the natural temporal learning
    of all detectors in the anomaly detection jobs. As explained in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016),
    *Machine Learning for IT*, the de-trending capability of the modeling will take
    into account the time at which something occurs. These functions simply model
    the event's timestamp within the day or week. For example, if something routinely
    happens at 2:00 A.M. every day, the function will learn that the normal time for
    this to happen is at the 7,200th second into the day.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've been through the entire catalog of detector functions, let's
    look ahead and see how we can expand the breadth of our analysis by splitting
    the modeling across entities that are represented by categorical fields.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Splitting analysis along categorical features
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the power of anomaly detection jobs in uncovering interesting anomalies
    in a single time series dataset. However, there are a few mechanisms by which
    the analysis can be split along a categorical field to invoke a parallel analysis
    across tens, hundreds, and even multiple thousands of unique entities.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Setting the split field
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using some of the job wizards (such as the Multi-metric and Population
    wizards), you will see an option to split the analysis:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Splitting on a categorical field'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_023.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 – Splitting on a categorical field
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, in *Figure 3.23*, which uses the Multi-metric wizard to build a job against
    the `kibana_sample_data_ecommerce` index, we see that the high sum function on
    the `taxful_total_price` field is being split per instance on the field called
    `category.keyword` (plus turning the **Sparse data** option on). In other words,
    the analysis will be done for every category of items in this e-commerce store
    (men''s clothing, women''s accessories, and so on). If the analysis is run and
    the results are inspected using the Anomaly Explorer UI, the result might look
    like the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Results of split analysis'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_024.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – Results of split analysis
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Notice in *Figure 3.24*, that the Anomaly Explorer view is different from what
    we've seen so far in the Single Metric Viewer. The Anomaly Explorer shows the
    top 10 most anomalous categories (the field we split on) over time. Notice that
    not every category is shown, only the ones with anomalies—and clearly, the **Men's
    Clothing** category was the most unusual with a revenue of $2,250 on November
    9th (in this version of the dataset). We will be learning more about understanding
    the results of multi-metric jobs and will use the Anomaly Explorer extensively
    in [*Chapter 5*](B17040_05_Epub_AM.xhtml#_idTextAnchor090), *Interpreting Results*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The difference between splitting using partition and by_field
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a reminder, when using the Multi-metric wizard and a split is invoked, the
    `partition_field_name` setting is set with the value of the field chosen in the
    UI.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: When splitting is chosen in the Population wizard, however, `by_field_name`
    is chosen to split the analysis. If the Advanced wizard is used, then `partition_field_name`
    and/or `by_field_name` can be defined (if both, then it's effectively a double-split).
    Therefore, it would be helpful to know how these two settings, which effectively
    split the analysis, are different from each other.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to "hard split" the analysis, use `partition_field_name`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: The field chosen should, in general, have <10,000 distinct values per job, as
    more memory is required to partition.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each instance of the field is like an independent variable.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scoring of anomalies in one partition is more independent from other partitions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want a "soft split," use `by_field_name`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The field chosen should, in general, have <100,000 distinct values per job.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More appropriate for attributes of an entity (dependent variables).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring considers the history of other `by` fields.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's dive deep into that last listed item—relating to the "history" of the
    other `by` fields. What exactly does that mean?
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there is a concept in anomaly detection job analysis relating to
    when an entity first happens, which we''ll call the `host:X` or `error_code:Y`),
    there may be one of two situations:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: That new entity is seen as "novel" and that, in itself, is notable and potentially
    worthy of being flagged as anomalous. To do that, you need to have your "dawn
    of time" be when the job starts.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That new entity is just part of the normal "expansion" of the data—perhaps a
    new server was added to the mix or a new `product_id` was added to the catalog.
    In this case, just start modeling that new entity and don't make a fuss about
    it showing up. To do that, you need to have the "dawn of time" be when that entity
    first shows up.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When analyzing splits using `by_field_name`, the dawn of time is when the ML
    job was started and when split using `partition_field_name`, the dawn of time
    is when that partition first showed up in the data. As such, you will get different
    results if you split one way versus the other for a situation in which something
    "new" comes along.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Is double-splitting the limit?
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, by using both `partition_field_name` and `by_field_name` when
    in the advanced job wizard, you can effectively get a double-split. But, if you
    need to split more, you'll have to rely on some other methods. Namely, you'll
    have to create a **scripted field** that is a concatenation of two (or more) fields.
    Using scripted fields is something that is covered in one of the examples in the
    [*Appendix*](B17040_14_Epub_AM.xhtml#_idTextAnchor248).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the concept of splitting the analysis, let's
    focus on the differences between temporal and population analysis in anomaly detection.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Understanding temporal versus population analysis
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned back in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016),
    *Machine Learning for IT*, that there are effectively two ways to consider something
    as anomalous:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Whether or not something changes drastically with respect to its own behavior
    over time
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not something is drastically different when compared to its peers
    in an otherwise homogeneous population
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the former (which we'll simply call temporal analysis) is the mode
    used *unless* the `over_field_name` setting is specified in the detector config.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Population analysis can be very useful in finding outliers in a variety of
    important use cases. For example, perhaps we want to find machines that are logging
    more (or less) than similarly configured machines in the following scenarios:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect configuration changes that have caused more errors to suddenly occur
    in the log file for the system or application.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system that might be compromised by malware may actually be instructed to
    suppress logging in certain situations, thus drastically decreasing the log volume.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system that has lost connectivity or has operationally failed, thus having
    its log volume diminished.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An otherwise harmless change to a logging-level setting (debug instead of normal),
    now annoyingly making your logs take up more disk space.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another way population analysis is often used is with respect to **User/Entity
    Behavioral Analysis** (**EUBA**), where a comparison of an entity''s or human''s
    actions compared against their peers might reveal the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated users**: Instead of the typical human behavior or usage pattern,
    an automated script may exhibit behavioral patterns that look quite different
    in terms of the speed, duration, and diversity of events they create. Whether
    it is finding a crawler trying to harvest the products and prices of an online
    catalog or detecting a bot that might be engaged in the spread of misinformation
    on social media, the automatic identification of automated users can be helpful.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distinct_count` function can help find a snooper.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Malicious/abusive users**: After the reconnaissance phase, a malicious user
    or malware will move on to actively wreaking havoc and will engage in active measures
    such as denial of service, brute-forcing, or stealing valuable information. Again,
    compared with typical users, malicious and abusive users have stark contrasts
    in their behavior regarding the volume, diversity, and intensity of activity per
    unit of time.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A practical example might be to find a customer that drastically spends a lot
    more than their peers. Whether or not you do this in the context of proactively
    investigating potential fraud, or whether you are interested in increasing the
    marketing to your most affluent customers, you still need to find those outliers.
    If we were to use the `kibana_sample_data_ecommerce` index that we added earlier
    in the chapter, we could create a population job by selecting the `customer_full_name.keyword`
    field for `taxful_total_price` field, which is the total revenue for each order
    placed by individuals:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Population analysis of revenue over users'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_025.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – Population analysis of revenue over users
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'After this job is executed, you should see the following results:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Population analysis results of the biggest spenders'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_026.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.26 – Population analysis results of the biggest spenders
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Here, in *Figure 3.26*, we see that the list of the most unusual users (in this
    case, the biggest spenders per unit of time) is dominated by a user named **Wagdi
    Shaw**, who apparently placed an order for $2,250 worth of goods. The astute among
    you will recognize this anomaly from an earlier example—except this time, we are
    orienting our analysis around the user, not around the inventory category.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, population analysis can be very powerful and is heavily used
    in use cases in which individual entities are targeted. Thus, it is very useful
    in security analytics use cases. Let's now pivot to focus on one additional, but
    powerful, capability of Elastic ML's anomaly detection—the ability to effectively
    analyze unstructured log messages via a process called categorization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Categorization analysis of unstructured messages
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that you are troubleshooting a problem by looking at a particular log
    file. You see a line in the log that looks like the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Unless you have some intimate knowledge about the inner workings of the application
    that created this log, you may not know whether the message is important. Having
    the database be `Not Updated` possibly sounds like a negative situation. However,
    if you knew that the application routinely writes this message, day in and day
    out, several hundred times per hour, then you would naturally realize that this
    message is benign and should possibly be ignored, because clearly the application
    works fine every day despite this message being written to the log file.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The problem, obviously, is one of human interpretation. Inspection of the text
    of the message and the reading of a negative phrase (`Not Updated`) potentially
    biases a person toward thinking that the message is noteworthy because of a possible
    problem. However, the frequency of the message (it happens routinely) should inform
    the person that the message must not be that important because the application
    is working (that is, there are no reported outages) despite these messages being
    written to the log.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: It can be hard for a human to process that information (assess the message content/relevance
    and also the frequency over time) for just a few types of messages in a log file.
    Imagine if there were thousands of unique message types occurring at a total rate
    of millions of log lines per day. Even the most seasoned expert in both the application
    content and search/visualizations will find this impractical, if not impossible,
    to wrangle.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML comes to the rescue with capabilities that allow the empirical assessment
    of both the uniqueness of the content of the messages and the relative frequency
    of occurrence.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Types of messages that are good candidates for categorization
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to be a little rigorous in our definition of the kinds of message-based
    log lines that are good for this kind of analysis. What we are *not* considering
    are log lines/events/documents that are completely freeform and likely the result
    of human creation (emails, tweets, comments, and so on). These kinds of messages
    are too arbitrary and variable in their construction and content.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we will focus on machine-generated messages that are obviously emitted
    when an application encounters different situations or exceptions, thus constraining
    their construction and content to a relatively discrete set of possibilities (understanding
    that there may indeed be some variable aspects of the message). For example, let''s
    look at the following few lines of an application log:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we can see that there is a variety of messages with different text in
    each, but there is some structure here. After the date/time stamp and the server
    name from which the message originates (here, `ACME6`), there is the actual meat
    of the message, where the application is informing the outside world what is happening
    at that moment—whether something is being tried or errors are occurring.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The process used by categorization
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to bring some order to the otherwise disorderly flow of the messages
    in the log file, Elastic ML will employ a technique of grouping similar messages
    together by using a string-similarity clustering algorithm. The heuristics behind
    this algorithm are roughly as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Focus on the (English) dictionary words more than `network` and `address` are
    dictionary words, but `dbmssocn` is likely a mutable/variable string).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass the immutable dictionary words through a string-similarity algorithm (similar
    to the **Levenshtein distance**) to determine how similar the log line is to past
    log lines.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the difference between the current log line and an existing category is small,
    then group the existing log line into that category. Otherwise, create a new category
    for the current log line.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a simple example, consider these three messages:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The algorithm would cluster the first two messages together in the same category,
    as they would be deemed as `Error writing file on` types of messages, whereas
    the third message would be given its own (new) category.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'The naming of these categories is simple: ML will just call them `mlcategory
    N`, where `N` is an incrementing integer. Therefore, in this example, the first
    two lines will be associated with `mlcategory 1`, and the third line will be associated
    with `mlcategory 2`. In a realistic machine log, there may be thousands (or even
    tens of thousands) of categories that are generated due to the diversity of the
    log messages, but the set of possible categories should be finite. However, if
    the number of categories starts to get into the hundreds of thousands, it may
    become obvious that the log messages are not a constrained set of possible message
    types and will not be a good candidate for this type of analysis.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the categories
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the messages are going to be categorized by the algorithm described
    previously, the next part of the process is to do the analysis (using either `count`
    or `rare`). In this case, we''re not going to be counting the log lines (and thus
    the documents of an Elasticsearch index) themselves; instead, we''re going to
    be counting the occurrence rate of the different categories that are the output
    of the algorithm. So, for example, given the example log lines in the previous
    section, if they occurred within the same bucket span, we would have the following
    output of the categorization algorithm:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In other words, there were two occurrences of the `Error writing file on` types
    of messages and one occurrence of the `Opening database on host` type in the last
    bucket span interval. It is this information that will ultimately be modeled by
    the ML job in order to determine whether it is unusual.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Categorization job example
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the categorization job wizard in the UI, the process of configuring this
    type of job is extremely easy. Let''s first assume we have an unstructured log
    file ingested (perhaps such as the `secure.log` file in the `example_data` folder
    on GitHub):'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: For more information on how to ingest data using the File Visualizer, see the
    detailed blog post at [elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer](http://elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: After picking the index of interest and choosing the Categorization wizard,
    and then selecting the appropriate time range for the analysis, we see that the
    wizard will ask us which `@timestamp` and `message`). Therefore, the `message`
    field is the field we would like Elastic ML to categorize. We will also pick the
    **Count** detector in this example:![Figure 3.27 – Categorization job configuration
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_027.jpg)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.27 – Categorization job configuration
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice in *Figure 3.27* that there is a check on the selected category field
    to make sure it will yield sensible results. Also notice that in the **Examples**
    section, you get visual confirmation of Elastic ML focusing on the non-mutable
    text of the log messages.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the configuration is confirmed and the job is started in the wizard, you
    will see a preview of the results as they are being discovered and analyzed:![Figure
    3.28 – Categorization job execution preview
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_028.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.28 – Categorization job execution preview
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that in this simple example, a total of 23 categories were discovered
    in the data. When the results are viewed in the Anomaly Explorer, we see that
    the top anomaly here is `mlcategory` number 7\.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you click on the `Received disconnect` messages.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By clicking on the gear icon, as shown in *Figure 3.29*, we can select **View
    examples** to transport us over to the Kibana Discover UI, but filtered to this
    appropriate message and zoomed into the relevant timeframe:![Figure 3.30 – Inspecting
    raw log lines from the categorization job results
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_030.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.30 – Inspecting raw log lines from the categorization job results
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that the Discover query bar has been automatically filled in with an
    appropriate KQL query to limit our view to the kind of messages that were anomalous.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we were to remove that query filter, we would see all of the messages in
    the log file at the time of this anomaly, and we would see the bigger story, which
    is that someone or something was attempting a lot of authentications:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Inspecting all log lines during the time of the anomaly'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_031.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.31 – Inspecting all log lines during the time of the anomaly
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 3.31*, there seems to be a flurry of authentication
    attempts using well-known usernames (`user`, `test`, and so on). Looks like we
    found a brute-force authentication attempt merely by using categorization!
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: When to avoid using categorization
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite categorization being quite useful, it''s not without its limitations.
    Specifically, here are some cases where attempting to use categorization will
    likely return poor results:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: With fields of text that are freeform, likely created by humans. Examples include
    tweets, comments, emails, and notes.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With log lines that should otherwise really be parsed into proper name/value
    pairs, such as a web access log.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With documents that contain a lot of multi-line text. This would include stack
    traces, XML, and so on.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that said, we can see that categorization can still be extremely useful
    in cases where analyzing unstructured text would otherwise be an increased burden
    on a human analyst.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Managing Elastic ML via the API
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with just about everything in the Elastic Stack, ML can also be completely
    automated via API calls—including job configuration, execution, and result gathering.
    Actually, all interactions you have in the Kibana UI leverage the ML API behind
    the scenes. You could, for example, completely write your own UI if there were
    specific workflows or visualizations that you wanted.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: For more in-depth information about the anomaly detection APIs, please refer
    to [elastic.co/guide/en/machine-learning/current/ml-api-quickref.html](http://elastic.co/guide/en/machine-learning/current/ml-api-quickref.html).
    The data frame analytics part of Elastic ML has a completely separate API, which
    will be discussed in *Chapters 9* to *13*.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into each API call, but we would like to highlight some parts that
    are worth a detour.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious first API to mention is the job creation API, which allows the
    creation of the ML job configuration. For example, if you wanted to recreate the
    population analysis job shown in *Figure 3.25*, the following call would create
    that job, which we will call `revenue_over_users_api`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the `job_id` field needs to be unique when creating the job.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create the companion datafeed configuration for this job, we would
    issue this separate API call:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that the default query to the index is `match_all`, which means that
    no filtering will take place. We could, of course, insert any valid Elasticsearch
    DSL in the query block to perform custom filters or aggregations. This concept
    will be covered later in the book.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: There are other APIs that can be used to extract results or modify other operational
    aspects of the ML job. Consult the online documentation for more information.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen that Elastic ML can highlight variations in volume, diversity, and
    uniqueness in metrics and log messages, including those that need some categorization
    first. Also, we've shown that population analysis can be an extremely interesting
    alternative to temporal anomaly detection when the focus is more on finding the
    most unusual entities. These techniques help solve the challenges we described
    before, where a human might struggle to recognize what is truly unusual and worthy
    of attention and investigation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: The skills learned in this chapter will be helpful in subsequent chapters, where
    we will see how ML assists in the process of getting to the root cause of complex
    IT problems, identifying application performance slowdowns, or when ML can assist
    in the identification of malware and/or malicious activity.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll see how the expressive time series models built by
    anomaly detection jobs can be leveraged to forecast trends of your data into the
    future.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
