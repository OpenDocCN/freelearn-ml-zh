- en: '*Chapter 3*: Anomaly Detection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Anomaly detection** was the original capability of Elastic ML and is the
    most mature, stretching its roots back to the Prelert days (before the acquisition
    by Elastic in 2016). This technology is robust, easy to use, powerful, and broadly
    applicable to all kinds of use cases for time series data.'
  prefs: []
  type: TYPE_NORMAL
- en: This jam-packed chapter will focus on using Elastic ML to detect anomalies in
    the occurrence rates of documents/events, rare occurrences of things, and numerical
    values outside of expected normal operation. We will run through some simple but
    effective examples that will highlight both the efficacy of Elastic ML and its
    ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML job types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dissecting the detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting changes in event rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting changes in metric values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the advanced detector functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting analysis along categorical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding temporal versus population analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorization analysis of unstructured messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Elastic ML via the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The information in this chapter is based on the Elastic Stack as it exists
    in v7.10\. As with all of the chapters, all the example code can be found on GitHub:
    [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition.](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML job types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we start using the Elastic ML UI to configure anomaly detection jobs,
    we will see that there are five different job wizards that are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – The Create job UI showing different configuration wizards'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – The Create job UI showing different configuration wizards
  prefs: []
  type: TYPE_NORMAL
- en: The existence of these different configuration wizards implies that there are
    different "types" of jobs. In actuality, there is really only one job type—it
    is just that the anomaly detection job has many options, and many of these wizards
    make certain aspects of that configuration easier. Everything that you may desire
    to configure can be done via the **Advanced** wizard (or the API). In fact, when
    Elastic ML was first released as beta in v5.4, that was all that existed. Since
    then, the other wizards have been added for simplicity and usability in specific
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: An anomaly detection job has many configuration settings, but the two most important
    ones are the **analysis configuration** and the **datafeed**.
  prefs: []
  type: TYPE_NORMAL
- en: The analysis configuration is the recipe for what anomalies the job will detect.
    It contains a detection configuration (called the **detector**) as well as a few
    other settings, such as the bucket span. The datafeed is the configuration of
    the query that will be executed by Elasticsearch to retrieve the data that is
    to be analyzed by the detector.
  prefs: []
  type: TYPE_NORMAL
- en: 'With respect to the different job wizards, the following are true:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Jobs created by the Single metric wizard have only one detector*. Their datafeeds
    contain a query and aggregations, thus only sending summarized data to the ML
    algorithms. The aggregations are automatically created for you based upon your
    configuration parameters in the wizard. The job also makes use of a flag called
    `summary_count_field_name` (set with the value of `doc_count`) to signal that
    aggregated data (and not raw data from the source index) is to be expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jobs created with the Multi-metric wizard can have one or more detectors*.
    The analysis can also be split along categorical fields by setting `partition_field_name`
    (described later in the chapter). Their datafeeds do not contain aggregations
    (because the ML code needs to see all documents for every possible instance of
    a field value and will aggregate it on its own), thus full Elasticsearch documents
    are passed to the ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jobs created with the Population wizard can have one or more detectors*. The
    wizard also sets `over_field_name` (described later in the chapter), which signals
    that population analysis is to be used. The analysis can also be split along categorical
    fields by setting `by_field_name` (described later in the chapter). Their datafeeds
    do not contain aggregations, thus full Elasticsearch documents are passed to the
    ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jobs created with the Categorization wizard have only one detector*. The wizard
    also sets `categorization_field_name` (described later in the chapter), which
    signals that categorization analysis is to be used. Categorization analysis also
    sets `by_field_name` (described later in the chapter) to a value of `mlcategory`.
    The analysis can also be split along categorical fields by setting `partition_field_name`
    (described later in the chapter). Their datafeeds do not contain aggregations,
    thus full Elasticsearch documents are passed to the ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jobs created with the Advanced wizard can leverage every option available*.
    The onus is on the user to know what they are doing and configure the job correctly.
    The UI does prevent the user from making most mistakes, however. An experienced
    user can exclusively use the Advanced wizard to create any anomaly detection job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The options around job creation might seem daunting given what was just described.
    But do not fret—once we have gotten familiar with the terminology and have walked
    through some examples, you will find that the job configurations are very sensible;
    as more experience is gained, the configuration of jobs will become second nature.
    Let's take the next step and break down the components of the detector.
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting the detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the heart of the anomaly detection job are the analysis configuration and
    the detector. The detector has several key components to it:'
  prefs: []
  type: TYPE_NORMAL
- en: The **function**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **field**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **partition field**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **by field**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **over field**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will go through each in turn to fully understand them all. Note that in the
    next few sections, however, we will often refer to the actual names of settings
    within the job configuration as if we were using the advanced job editor or the
    API. Although it is good to fully understand the nomenclature, as you progress
    through this chapter you will also notice that many of the details of the job
    configuration are abstracted away from the user or are given more "UI-friendly"
    labels than the real setting names.
  prefs: []
  type: TYPE_NORMAL
- en: The function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The detector **function** describes how the data will be aggregated or measured
    within the analysis interval (bucket span). There are many functions, but they
    can be classified into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Table of detector functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Table of detector functions
  prefs: []
  type: TYPE_NORMAL
- en: Items marked with an asterisk (`*`) also have high/low one-sided variants (such
    as `low_distinct_count`) that allow the detection of anomalies in only one direction.
  prefs: []
  type: TYPE_NORMAL
- en: The field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some functions in the detector require a field within the data to operate on.
    Take the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max(bytes)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean(products.price)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`high_distinct_count(destination.port)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the name of the field the function directly operates on is simply
    called `field_name`.
  prefs: []
  type: TYPE_NORMAL
- en: The partition field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are often cases in which the detection analysis needs to be split along
    a categorical field so the analysis can be done separately for all unique instances
    of that field. In this case, the `partition` field (the setting is called `partition_field_name`)
    defines the field to split on. For example, in e-commerce, you might want to see
    the average revenue per category (men's clothing, women's accessories, and so
    on). In this case, the `category` field would be the `partition` field. We will
    explore the splitting of the analysis later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The by field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the `partition` field, the `by` field (the setting is called `by_field_name`)
    is another mechanism to split the analysis, but it behaves differently with respect
    to how the results are modeled and scored. Additionally, the `by` field is mandatory
    if `rare` or `freq_rare` is used. More details on the differences in using the
    `by` field for splitting versus using the `partition` field will be discussed
    later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The over field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `over_field_name`) signals to the anomaly detection algorithms that **population
    analysis** is desired, where entities are compared to their peers (instead of
    against their own past behavior). Population analysis is discussed in depth later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The "formula"
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we were to document all of the possible configuration options for a detector
    and then create a flow chart-like map, it would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – The "formula" for building a detector from scratch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – The "formula" for building a detector from scratch
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are things to note about the diagram shown in *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: Capitalized text is the explanation and italics text is the detector configuration
    settings (`by_field_name`, `partition_field_name`, and `over_field_name` are shortened
    to be simply `by`, `partition`, and `over`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Items in square brackets are optional (high, low, non-zero, non-null).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose only one exit branch (notice only one exit branch out of `rare`/`freq_rare`
    because `by` is mandatory).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of something versus its own history is accomplished simply by *not*
    choosing an `over` field.
  prefs: []
  type: TYPE_NORMAL
- en: With a comprehensive understanding of the construction of a detector, we will
    now move into practical examples of using detectors for different use cases. First,
    we will explore the count functions that allow us to detect changes in event rates
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting changes in event rates
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many important use cases that revolve around the idea of event change
    detection. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering a flood of error messages suddenly cropping up in a log file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting a sudden drop in the number of orders processed by an online system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining a sudden excessive number of attempts at accessing something (for
    example, a sudden increase in the number of login attempts on a particular user
    ID)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order for us to find the abnormal, we must first have a mechanism to understand
    the normal rate of occurrence. But relying on our fallible human observation and
    intuition is not always the easiest (or most reliable) approach.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the count functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in [*Chapter 2*](B17040_02_Epub_AM.xhtml#_idTextAnchor033), *Enabling
    and Operationalization*, Elastic ML jobs have an anomaly detection "recipe" known
    as the **detector**. The detector is key to defining what anomalies the user wants
    to detect. Within the detector is the **function**, which selects the "feature"
    of what is to be detected. In the case of the count functions, the feature is
    the occurrence rate of something over time. There are three main count functions
    that we will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '`count`: Counts the number of documents in the bucket resulting from a query
    of the raw data index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`high_count`: The same as `count`, but will only flag an anomaly if the count
    is higher than expected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_count`: The same as `count`, but will only flag an anomaly if the count
    is lower than expected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see that there are a variety of one-sided functions in Elastic ML (to
    only detect anomalies in a certain direction). Additionally, it is important to
    know that the count functions are not counting a field or even the existence of
    fields within a document; they are merely counting the documents in the index
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a more intuitive feeling for what the count functions do, let''s jump
    into a simple example using the sample data within Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: To enable the sample data, from the Kibana home screen, click on the **Add data**
    button (in either location) as shown in *Figure 3.4*:![Figure 3.4 – The Kibana
    home screen with Add data options
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.4 – The Kibana home screen with Add data options
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After clicking on **Add data**, select **Sample data** to reveal three sets
    of data:![Figure 3.5 – Adding sample data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.5 – Adding sample data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on each of the three **Add data** buttons in each section to load that
    sample dataset into your Elastic Stack. Once the loading is complete, we will
    jump directly to ML by selecting the three-horizontal-lines menu icon (![Text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Description automatically generated](img/B17040_03_032.png)) at the top left
    of Kibana to reveal the list of apps, and then select **Machine Learning**:![Figure
    3.6 – Selecting Machine Learning from the Kibana apps menu
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_03_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.6 – Selecting Machine Learning from the Kibana apps menu
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once this has been clicked, we will be on the ML overview page, where we can
    immediately see where we can create our first anomaly detection job. Click on
    the **Create job** button, as shown in *Figure 3.7*:![Figure 3.7 – Elastic Cloud
    welcome screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.7 – Elastic Cloud welcome screen
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our next task is to select the index pattern (marked with an index with shards
    icon) or a saved search (marked with a magnifying glass icon) that contains the
    data that we'd like to analyze. If a saved search is chosen, then a filtered query
    that was previously created and saved within Kibana's `kibana_sample_data_logs`
    index, as we want to pass every document in that index through Elastic ML:![Figure
    3.8 – Selecting the kibana_sample_data_logs index for analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.8 – Selecting the kibana_sample_data_logs index for analysis
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On the next screen, we will select the **Single metric** job wizard because,
    at this point, we''re interested in analyzing only one aspect of the data: its
    count over time:![Figure 3.9 – Choosing a Single metric job'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.9 – Choosing a Single metric job
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the next screen, in order to follow along with this example, *you must select
    the* **Use full kibana_sample_logs_data** *button in order to include the sample
    anomaly in this dataset*:![Figure 3.10 – Selecting to use all the data within
    the index
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.10 – Selecting to use all the data within the index
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This demo data, when installed, actually puts about half of the data in the
    past and half in the future (by dynamically modifying the timestamps on ingest).
    This is done to provide a mechanism for the static data to look "real-time" when
    dashboards are viewed on data in the "last hour," for example. As a result of
    this, we're really going to ask Elastic ML to analyze data from the past and the
    future, where normally it would be impossible to have data from the future. Suspend
    belief for now for the sake of the example as the anomaly we'd like to demonstrate
    is in the second half of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, click the **Next** button to advance to the next step in the configuration
    wizard.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After clicking the **Next** button, we will need to select what we want to analyze
    from the **Pick fields** drop-down box. We will select **Count(Event rate)** to
    focus on our original goal here, which is to detect changes in the event rate
    in this index over time:![Figure 3.11 – Selecting the count of events over time
    as our detection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.11 – Selecting the count of events over time as our detection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that looking through this drop-down box shows that other analyses could
    be done, depending on the data type of the field in the data. We will explore
    some of these other options later on in subsequent examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the **Next** button to proceed, leaving the other options as their defaults
    for now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we need to name our anomaly detection job. In the `web_logs_rate` was used:![Figure
    3.12 – Naming the anomaly detection job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.12 – Naming the anomaly detection job
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Again, leave the other options as their defaults and click the **Next** button.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A validation step takes place to ensure that everything is reasonable for the
    analysis to work:![Figure 3.13 – Job validation step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.13 – Job validation step
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the **Next** button to proceed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, the job is ready to be created (and notice in *Figure 3.14* that
    some sensible default options, such as **Model memory limit** and **Enable model
    plot**, were chosen for you):![Figure 3.14 – Anomaly detection job ready to be
    created](img/B17040_03_014.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.14 – Anomaly detection job ready to be created
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After the **Create job** button is clicked, you will see an animated preview
    of the results superimposed on top of the data, as follows:![Figure 3.15 – Results
    preview of the job execution displayed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.15 – Results preview of the job execution displayed
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's now click the **View results** button to investigate in detail what the
    anomaly detection job has found in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the scrubber below the main graph, adjust the location and width of the
    viewing area to zoom in on the big spike:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Results shown for a critical anomaly'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – Results shown for a critical anomaly
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As you zoom in, out, and around, just be aware of the chart aggregation interval
    as compared to the job's bucket span (as circled in *Figure 3.16*). If you are
    zoomed out to a wider view, the chart aggregation interval can be larger than
    the job's bucket span, making the position of the drawn anomaly on the chart less
    exact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, in *Figure 3.16*, we can see that the very large spike in events was
    flagged as two distinct anomalies because the actual number of web requests seen
    in the logs was around 11 times higher than expected (given the learned model
    of the data up until that point in time). You may notice that the chart shows
    two anomalies next to each other because clearly, the spike in events spanned
    more than one 15-minute bucket interval. You may also notice that by default,
    there is only one anomaly shown in the table below the chart. This is because
    **Interval** defaults to **Auto** and time-adjacent anomalies are summarized together,
    with only the highest score shown. If **Interval** is changed to **Show all**,
    then both anomaly records are listed in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Interval set to Show all anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Interval set to Show all anomalies
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s one final thing to notice in this example, which is the other, lesser-scored
    anomalies earlier in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Multi-bucket anomalies'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – Multi-bucket anomalies
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few key things to recognize about these less-than-obvious anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: They have lower scores than the massive spike we just investigated because relatively
    speaking, these are not as anomalous, but are interestingly significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The anomaly here is the "lack" of expected values. In other words, the `count`
    function interprets *no data* as 0 and that can be anomalous if normally there's
    an expectation that events should be occurring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These anomalies are not single-bucket anomalies, but rather **multi-bucket anomalies**.
    Multi-bucket anomalies are designated with a different symbol in the UI (a cross
    instead of a dot). They denote cases in which the actual singular value may not
    necessarily be anomalous, but there is a trend that is occurring in a sliding
    window of 12 consecutive buckets. Here, you can see that there is a noticeable
    slump spanning several adjacent buckets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For more information on interpreting multi-bucket anomalies, see the detailed
    blog post at [elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features](http://elastic.co/blog/interpreting-multi-bucket-impact-anomalies-using-elastic-machine-learning-features).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have seen, through this example, how the count function allows us to easily
    detect an obvious (and not-so-obvious) set of anomalies relating to the overall
    rate of occurrence of events (documents) in an index over time. Let's continue
    our journey by looking at other count and occurrence-based functions.
  prefs: []
  type: TYPE_NORMAL
- en: Other counting functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the functions that we've described so far, there are several
    other counting functions that enable a broader set of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Non-zero count
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The non-zero count functions (`non_zero_count`, `low_non_zero_count`, and `high_non_zero_count`)
    allow the handling of count-based analysis, as well as allowing accurate modeling
    in cases where the data may be sparse and you would not want the non-existence
    of data to be explicitly treated as zero, but rather as null—in other words, a
    dataset in time, which looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Data with the `non_zero_count` functions will be interpreted as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The act of treating zeros as null can be useful in cases where the non-existence
    of measurements at regular intervals is expected. Some practical examples of this
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of airline tickets purchased per month by an individual
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of times a server reboots in a day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of login attempts on a system per hour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To select the non-zero count version of the count functions in the job wizards,
    just toggle the **Sparse data** option during the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Adding the Sparse data option to select the non-zero count'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 – Adding the Sparse data option to select the non-zero count
  prefs: []
  type: TYPE_NORMAL
- en: We will see later in the chapter, when we are configuring the jobs in the advanced
    job wizard or via the API, that we will be explicitly using function names (such
    as `high_non_zero_count`) instead of toggling options with more conceptual descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Distinct count
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The distinct count functions (`distinct_count`, `low_distinct_count`, and `high_distinct_count`)
    measure the uniqueness (`distinct_count(url.keyword)` as the detector configuration
    in the last example on the `kibana_sample_data_logs` index, we would have caught
    the same anomalous timeframe, but for a different reason—not only was the overall
    volume of requests high, as we saw back in *Figure 3.16*, but here in *Figure
    3.20*, we see that there was a high diversity of URLs being requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Distinct count detector example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 – Distinct count detector example
  prefs: []
  type: TYPE_NORMAL
- en: With an appreciation of count-based functions, let's now turn to metric-based
    functions, which allow us to analyze numerical fields in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting changes in metric values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Obviously, not all data being emitted from systems will be text or categorical
    in nature—a vast amount of it is numerical. Detecting changes in metric values
    over time is perfectly suited for anomaly detection because, as mentioned in [*Chapter
    1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016), *Machine Learning for IT*, the
    historical paradigm of alerting on exceptions in numerical values via static thresholds
    has been troublesome for decades. Let's explore all that Elastic ML has to offer
    with respect to the functions that help you detect changes in numerical fields
    in your data.
  prefs: []
  type: TYPE_NORMAL
- en: Metric functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metric functions operate on numerical fields and return numerical values. They
    are perhaps the easiest of the detector functions to understand.
  prefs: []
  type: TYPE_NORMAL
- en: min, max, mean, median, and metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These functions do exactly as you would expect: they return the minimum, maximum,
    average/mean, and median of all of the numerical observations for the field of
    interest in the bucket span.'
  prefs: []
  type: TYPE_NORMAL
- en: The `metric` function is a little unique in that it is really just a shorthand
    way of specifying that `min`, `max`, and `mean` are to be used together.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that if the frequency of the data (for example, data that
    comes from a sampling source such as Metricbeat) exactly matches the bucket span,
    then there is only one sample per bucket span. This means that the minimum, maximum,
    average/mean, and median of the field of interest are all the same value (the
    value of the single observation itself). Therefore, if possible, it is usually
    better to have multiple numerical samples per bucket span if you want to have
    discrimination using these functions.
  prefs: []
  type: TYPE_NORMAL
- en: Another fact to note is that these metric functions treat the lack of data as
    *null*. In other words, if your data is sparse and there are bucket spans in which
    no observations are seen, the lack of data will not "drag down" the statistics
    for the field of interest. This is why these metric-based functions have no "non-zero"
    or "non-null" counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: varp
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `varp` function measures the overall variance of a metric over time—its
    volatility. Using this function might be applicable to finding cases where the
    numerical value of a field should normally be rather consistent, but you would
    like to detect whether there was a change.
  prefs: []
  type: TYPE_NORMAL
- en: Sum and non-null sum
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `sum` function will return the sum of all of the numerical observations
    for the field of interest in the bucket span. Use the "non-null" version if you
    have sparse data and do not want the lack of data being treated as *zero*, which
    will inevitably "drag down" the value of the sum.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we had selected `sum(bytes)` as the detector configuration in the last example
    on the `kibana_sample_data_logs` index, we would have caught the same anomalous
    timeframe, but for a different reason—we see that the requests made also resulted
    in a higher quantity of bytes being transferred from the web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Sum detector example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 – Sum detector example
  prefs: []
  type: TYPE_NORMAL
- en: This is totally sensible, given that an increased number of requests to a web
    server will correlate with an increase in the number of bytes being transferred.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an appreciation for the simpler detector functions, let's move
    on to the more complex, advanced functions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the advanced detector functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the detector functions mentioned so far, there are also a few
    other, more advanced functions that allow some very unique capabilities. Some
    of these functions are only available if the ML job is configured via the advanced
    job wizard or via the API.
  prefs: []
  type: TYPE_NORMAL
- en: rare
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of a stream of temporal information (such as a log file), the
    notion of something being statistically rare (occurring at a low frequency) is
    paradoxically both intuitive and hard to understand. If I were asked, for example,
    to trawl through a log file and find a rare message, I might be tempted to label
    the first novel message that I saw as a rare one. But what if practically every
    message was novel? Are they all rare? Or is nothing rare?
  prefs: []
  type: TYPE_NORMAL
- en: In order to define rarity to be useful in the context of a stream of events
    in time, we need to agree that the declaration of something as being rare must
    take into account the context in which it exists. If there are lots of other routine
    things and a small number of unique things, then we can deem the unique things
    rare. If there are many unique things, then we will deem that nothing is rare.
  prefs: []
  type: TYPE_NORMAL
- en: 'When applying the `rare` function in an ML job, there is a requirement to declare
    which field the `rare` function is focusing on. This field is then defined as
    `by_field_name`. Configuration of the `rare` function does not have its own wizard
    in the Elastic ML UI, so you will need to define it using the advanced job wizard.
    For example, to find log entries that reference a rare country name, structure
    your detector similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Rare detector example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 – Rare detector example
  prefs: []
  type: TYPE_NORMAL
- en: This could be handy for finding unexpected geographical access (as in "Our admins
    usually log in from the New York and London offices almost daily, but never from
    Moscow!").
  prefs: []
  type: TYPE_NORMAL
- en: Frequency rare
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `freq_rare` function is a specialized version of `rare`, in that it looks
    for members of a population that cause rare values of `by_field_name` to occur
    frequently. For example, you could locate a particular IP address that is attempting
    to access many rare URLs that are not generally seen across the entire population
    of all client IP addresses. This IP address could be attempting to access otherwise
    hidden sections of a website in a nefarious way, or may be attempting attacks
    such as SQL injection.
  prefs: []
  type: TYPE_NORMAL
- en: Information content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `info_content` function is perhaps the most specialized detector function
    in Elastic ML's arsenal. It was originally written as a means to measure the amount
    of **entropy** in text strings (how many and how diverse the characters are).
    This is because there are well-known techniques in malware that encrypt instructions
    and/or payload data for transmission for **command and control** (**C2**) and
    data exfiltration activity. Detecting this activity along this feature of the
    data is more reliable than looking at other features (such as the number of bytes
    sent or counting distinct entities).
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm used will essentially do the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Sort the unique strings into alphabetical order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Concatenate those unique strings into one long string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the `gzip` algorithm on that long string to compress it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The information content is the length of the compressed data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Some of the ML jobs in the Elastic SIEM utilize the `info_content` function—stay
    tuned for [*Chapter 8*](B17040_08_Epub_AM.xhtml#_idTextAnchor146), *Anomaly Detection
    in Other Elastic Stack Apps*, for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Geographic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you find a geographic location that is unusual to a learned location area
    on Earth, then the `lat_long` function will be helpful, taking a `field_name`
    argument that is a comma-separated pair of numbers in the range of -180 to 180
    (for example, `40.75, -73.99`, the coordinates of Times Square in New York City).
    The `lat_long` function can also operate on a `geo_point` field, a `geo_shape`
    field that contains point values, or a `geo_centroid` aggregation. An example
    use case would be to flag a location that isn't normal (and potentially fraudulent
    or malicious) for a specific user, transaction, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all things occur randomly in time, especially with things involving human
    behavior. We may eat, commute, or log into certain systems at predictable times
    of the day or week. Using the `time_of_day` and `time_of_week` functions, you
    can detect changes of behavior from a learned temporal routine. If a behavior
    is predictable on a 24-hour timeframe, then `time_of_day` is more appropriate.
    If the routine is day-of-the-week-dependent, then `time_of_week` should be a more
    logical choice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse the usage of these time functions with the natural temporal learning
    of all detectors in the anomaly detection jobs. As explained in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016),
    *Machine Learning for IT*, the de-trending capability of the modeling will take
    into account the time at which something occurs. These functions simply model
    the event's timestamp within the day or week. For example, if something routinely
    happens at 2:00 A.M. every day, the function will learn that the normal time for
    this to happen is at the 7,200th second into the day.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've been through the entire catalog of detector functions, let's
    look ahead and see how we can expand the breadth of our analysis by splitting
    the modeling across entities that are represented by categorical fields.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting analysis along categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the power of anomaly detection jobs in uncovering interesting anomalies
    in a single time series dataset. However, there are a few mechanisms by which
    the analysis can be split along a categorical field to invoke a parallel analysis
    across tens, hundreds, and even multiple thousands of unique entities.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the split field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using some of the job wizards (such as the Multi-metric and Population
    wizards), you will see an option to split the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Splitting on a categorical field'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 – Splitting on a categorical field
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, in *Figure 3.23*, which uses the Multi-metric wizard to build a job against
    the `kibana_sample_data_ecommerce` index, we see that the high sum function on
    the `taxful_total_price` field is being split per instance on the field called
    `category.keyword` (plus turning the **Sparse data** option on). In other words,
    the analysis will be done for every category of items in this e-commerce store
    (men''s clothing, women''s accessories, and so on). If the analysis is run and
    the results are inspected using the Anomaly Explorer UI, the result might look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Results of split analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – Results of split analysis
  prefs: []
  type: TYPE_NORMAL
- en: Notice in *Figure 3.24*, that the Anomaly Explorer view is different from what
    we've seen so far in the Single Metric Viewer. The Anomaly Explorer shows the
    top 10 most anomalous categories (the field we split on) over time. Notice that
    not every category is shown, only the ones with anomalies—and clearly, the **Men's
    Clothing** category was the most unusual with a revenue of $2,250 on November
    9th (in this version of the dataset). We will be learning more about understanding
    the results of multi-metric jobs and will use the Anomaly Explorer extensively
    in [*Chapter 5*](B17040_05_Epub_AM.xhtml#_idTextAnchor090), *Interpreting Results*.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between splitting using partition and by_field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a reminder, when using the Multi-metric wizard and a split is invoked, the
    `partition_field_name` setting is set with the value of the field chosen in the
    UI.
  prefs: []
  type: TYPE_NORMAL
- en: When splitting is chosen in the Population wizard, however, `by_field_name`
    is chosen to split the analysis. If the Advanced wizard is used, then `partition_field_name`
    and/or `by_field_name` can be defined (if both, then it's effectively a double-split).
    Therefore, it would be helpful to know how these two settings, which effectively
    split the analysis, are different from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to "hard split" the analysis, use `partition_field_name`:'
  prefs: []
  type: TYPE_NORMAL
- en: The field chosen should, in general, have <10,000 distinct values per job, as
    more memory is required to partition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each instance of the field is like an independent variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scoring of anomalies in one partition is more independent from other partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want a "soft split," use `by_field_name`:'
  prefs: []
  type: TYPE_NORMAL
- en: The field chosen should, in general, have <100,000 distinct values per job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More appropriate for attributes of an entity (dependent variables).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring considers the history of other `by` fields.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's dive deep into that last listed item—relating to the "history" of the
    other `by` fields. What exactly does that mean?
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there is a concept in anomaly detection job analysis relating to
    when an entity first happens, which we''ll call the `host:X` or `error_code:Y`),
    there may be one of two situations:'
  prefs: []
  type: TYPE_NORMAL
- en: That new entity is seen as "novel" and that, in itself, is notable and potentially
    worthy of being flagged as anomalous. To do that, you need to have your "dawn
    of time" be when the job starts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That new entity is just part of the normal "expansion" of the data—perhaps a
    new server was added to the mix or a new `product_id` was added to the catalog.
    In this case, just start modeling that new entity and don't make a fuss about
    it showing up. To do that, you need to have the "dawn of time" be when that entity
    first shows up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When analyzing splits using `by_field_name`, the dawn of time is when the ML
    job was started and when split using `partition_field_name`, the dawn of time
    is when that partition first showed up in the data. As such, you will get different
    results if you split one way versus the other for a situation in which something
    "new" comes along.
  prefs: []
  type: TYPE_NORMAL
- en: Is double-splitting the limit?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, by using both `partition_field_name` and `by_field_name` when
    in the advanced job wizard, you can effectively get a double-split. But, if you
    need to split more, you'll have to rely on some other methods. Namely, you'll
    have to create a **scripted field** that is a concatenation of two (or more) fields.
    Using scripted fields is something that is covered in one of the examples in the
    [*Appendix*](B17040_14_Epub_AM.xhtml#_idTextAnchor248).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the concept of splitting the analysis, let's
    focus on the differences between temporal and population analysis in anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding temporal versus population analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned back in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016),
    *Machine Learning for IT*, that there are effectively two ways to consider something
    as anomalous:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether or not something changes drastically with respect to its own behavior
    over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether or not something is drastically different when compared to its peers
    in an otherwise homogeneous population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the former (which we'll simply call temporal analysis) is the mode
    used *unless* the `over_field_name` setting is specified in the detector config.
  prefs: []
  type: TYPE_NORMAL
- en: 'Population analysis can be very useful in finding outliers in a variety of
    important use cases. For example, perhaps we want to find machines that are logging
    more (or less) than similarly configured machines in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorrect configuration changes that have caused more errors to suddenly occur
    in the log file for the system or application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system that might be compromised by malware may actually be instructed to
    suppress logging in certain situations, thus drastically decreasing the log volume.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system that has lost connectivity or has operationally failed, thus having
    its log volume diminished.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An otherwise harmless change to a logging-level setting (debug instead of normal),
    now annoyingly making your logs take up more disk space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another way population analysis is often used is with respect to **User/Entity
    Behavioral Analysis** (**EUBA**), where a comparison of an entity''s or human''s
    actions compared against their peers might reveal the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated users**: Instead of the typical human behavior or usage pattern,
    an automated script may exhibit behavioral patterns that look quite different
    in terms of the speed, duration, and diversity of events they create. Whether
    it is finding a crawler trying to harvest the products and prices of an online
    catalog or detecting a bot that might be engaged in the spread of misinformation
    on social media, the automatic identification of automated users can be helpful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distinct_count` function can help find a snooper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Malicious/abusive users**: After the reconnaissance phase, a malicious user
    or malware will move on to actively wreaking havoc and will engage in active measures
    such as denial of service, brute-forcing, or stealing valuable information. Again,
    compared with typical users, malicious and abusive users have stark contrasts
    in their behavior regarding the volume, diversity, and intensity of activity per
    unit of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A practical example might be to find a customer that drastically spends a lot
    more than their peers. Whether or not you do this in the context of proactively
    investigating potential fraud, or whether you are interested in increasing the
    marketing to your most affluent customers, you still need to find those outliers.
    If we were to use the `kibana_sample_data_ecommerce` index that we added earlier
    in the chapter, we could create a population job by selecting the `customer_full_name.keyword`
    field for `taxful_total_price` field, which is the total revenue for each order
    placed by individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Population analysis of revenue over users'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – Population analysis of revenue over users
  prefs: []
  type: TYPE_NORMAL
- en: 'After this job is executed, you should see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Population analysis results of the biggest spenders'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.26 – Population analysis results of the biggest spenders
  prefs: []
  type: TYPE_NORMAL
- en: Here, in *Figure 3.26*, we see that the list of the most unusual users (in this
    case, the biggest spenders per unit of time) is dominated by a user named **Wagdi
    Shaw**, who apparently placed an order for $2,250 worth of goods. The astute among
    you will recognize this anomaly from an earlier example—except this time, we are
    orienting our analysis around the user, not around the inventory category.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, population analysis can be very powerful and is heavily used
    in use cases in which individual entities are targeted. Thus, it is very useful
    in security analytics use cases. Let's now pivot to focus on one additional, but
    powerful, capability of Elastic ML's anomaly detection—the ability to effectively
    analyze unstructured log messages via a process called categorization.
  prefs: []
  type: TYPE_NORMAL
- en: Categorization analysis of unstructured messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that you are troubleshooting a problem by looking at a particular log
    file. You see a line in the log that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Unless you have some intimate knowledge about the inner workings of the application
    that created this log, you may not know whether the message is important. Having
    the database be `Not Updated` possibly sounds like a negative situation. However,
    if you knew that the application routinely writes this message, day in and day
    out, several hundred times per hour, then you would naturally realize that this
    message is benign and should possibly be ignored, because clearly the application
    works fine every day despite this message being written to the log file.
  prefs: []
  type: TYPE_NORMAL
- en: The problem, obviously, is one of human interpretation. Inspection of the text
    of the message and the reading of a negative phrase (`Not Updated`) potentially
    biases a person toward thinking that the message is noteworthy because of a possible
    problem. However, the frequency of the message (it happens routinely) should inform
    the person that the message must not be that important because the application
    is working (that is, there are no reported outages) despite these messages being
    written to the log.
  prefs: []
  type: TYPE_NORMAL
- en: It can be hard for a human to process that information (assess the message content/relevance
    and also the frequency over time) for just a few types of messages in a log file.
    Imagine if there were thousands of unique message types occurring at a total rate
    of millions of log lines per day. Even the most seasoned expert in both the application
    content and search/visualizations will find this impractical, if not impossible,
    to wrangle.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic ML comes to the rescue with capabilities that allow the empirical assessment
    of both the uniqueness of the content of the messages and the relative frequency
    of occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: Types of messages that are good candidates for categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to be a little rigorous in our definition of the kinds of message-based
    log lines that are good for this kind of analysis. What we are *not* considering
    are log lines/events/documents that are completely freeform and likely the result
    of human creation (emails, tweets, comments, and so on). These kinds of messages
    are too arbitrary and variable in their construction and content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we will focus on machine-generated messages that are obviously emitted
    when an application encounters different situations or exceptions, thus constraining
    their construction and content to a relatively discrete set of possibilities (understanding
    that there may indeed be some variable aspects of the message). For example, let''s
    look at the following few lines of an application log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that there is a variety of messages with different text in
    each, but there is some structure here. After the date/time stamp and the server
    name from which the message originates (here, `ACME6`), there is the actual meat
    of the message, where the application is informing the outside world what is happening
    at that moment—whether something is being tried or errors are occurring.
  prefs: []
  type: TYPE_NORMAL
- en: The process used by categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to bring some order to the otherwise disorderly flow of the messages
    in the log file, Elastic ML will employ a technique of grouping similar messages
    together by using a string-similarity clustering algorithm. The heuristics behind
    this algorithm are roughly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Focus on the (English) dictionary words more than `network` and `address` are
    dictionary words, but `dbmssocn` is likely a mutable/variable string).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pass the immutable dictionary words through a string-similarity algorithm (similar
    to the **Levenshtein distance**) to determine how similar the log line is to past
    log lines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the difference between the current log line and an existing category is small,
    then group the existing log line into that category. Otherwise, create a new category
    for the current log line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a simple example, consider these three messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm would cluster the first two messages together in the same category,
    as they would be deemed as `Error writing file on` types of messages, whereas
    the third message would be given its own (new) category.
  prefs: []
  type: TYPE_NORMAL
- en: 'The naming of these categories is simple: ML will just call them `mlcategory
    N`, where `N` is an incrementing integer. Therefore, in this example, the first
    two lines will be associated with `mlcategory 1`, and the third line will be associated
    with `mlcategory 2`. In a realistic machine log, there may be thousands (or even
    tens of thousands) of categories that are generated due to the diversity of the
    log messages, but the set of possible categories should be finite. However, if
    the number of categories starts to get into the hundreds of thousands, it may
    become obvious that the log messages are not a constrained set of possible message
    types and will not be a good candidate for this type of analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the messages are going to be categorized by the algorithm described
    previously, the next part of the process is to do the analysis (using either `count`
    or `rare`). In this case, we''re not going to be counting the log lines (and thus
    the documents of an Elasticsearch index) themselves; instead, we''re going to
    be counting the occurrence rate of the different categories that are the output
    of the algorithm. So, for example, given the example log lines in the previous
    section, if they occurred within the same bucket span, we would have the following
    output of the categorization algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In other words, there were two occurrences of the `Error writing file on` types
    of messages and one occurrence of the `Opening database on host` type in the last
    bucket span interval. It is this information that will ultimately be modeled by
    the ML job in order to determine whether it is unusual.
  prefs: []
  type: TYPE_NORMAL
- en: Categorization job example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the categorization job wizard in the UI, the process of configuring this
    type of job is extremely easy. Let''s first assume we have an unstructured log
    file ingested (perhaps such as the `secure.log` file in the `example_data` folder
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on how to ingest data using the File Visualizer, see the
    detailed blog post at [elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer](http://elastic.co/blog/importing-csv-and-log-data-into-elasticsearch-with-file-data-visualizer).
  prefs: []
  type: TYPE_NORMAL
- en: After picking the index of interest and choosing the Categorization wizard,
    and then selecting the appropriate time range for the analysis, we see that the
    wizard will ask us which `@timestamp` and `message`). Therefore, the `message`
    field is the field we would like Elastic ML to categorize. We will also pick the
    **Count** detector in this example:![Figure 3.27 – Categorization job configuration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_027.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.27 – Categorization job configuration
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice in *Figure 3.27* that there is a check on the selected category field
    to make sure it will yield sensible results. Also notice that in the **Examples**
    section, you get visual confirmation of Elastic ML focusing on the non-mutable
    text of the log messages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the configuration is confirmed and the job is started in the wizard, you
    will see a preview of the results as they are being discovered and analyzed:![Figure
    3.28 – Categorization job execution preview
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_028.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.28 – Categorization job execution preview
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that in this simple example, a total of 23 categories were discovered
    in the data. When the results are viewed in the Anomaly Explorer, we see that
    the top anomaly here is `mlcategory` number 7\.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you click on the `Received disconnect` messages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By clicking on the gear icon, as shown in *Figure 3.29*, we can select **View
    examples** to transport us over to the Kibana Discover UI, but filtered to this
    appropriate message and zoomed into the relevant timeframe:![Figure 3.30 – Inspecting
    raw log lines from the categorization job results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_03_030.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.30 – Inspecting raw log lines from the categorization job results
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notice that the Discover query bar has been automatically filled in with an
    appropriate KQL query to limit our view to the kind of messages that were anomalous.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we were to remove that query filter, we would see all of the messages in
    the log file at the time of this anomaly, and we would see the bigger story, which
    is that someone or something was attempting a lot of authentications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Inspecting all log lines during the time of the anomaly'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_03_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.31 – Inspecting all log lines during the time of the anomaly
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 3.31*, there seems to be a flurry of authentication
    attempts using well-known usernames (`user`, `test`, and so on). Looks like we
    found a brute-force authentication attempt merely by using categorization!
  prefs: []
  type: TYPE_NORMAL
- en: When to avoid using categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite categorization being quite useful, it''s not without its limitations.
    Specifically, here are some cases where attempting to use categorization will
    likely return poor results:'
  prefs: []
  type: TYPE_NORMAL
- en: With fields of text that are freeform, likely created by humans. Examples include
    tweets, comments, emails, and notes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With log lines that should otherwise really be parsed into proper name/value
    pairs, such as a web access log.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With documents that contain a lot of multi-line text. This would include stack
    traces, XML, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that said, we can see that categorization can still be extremely useful
    in cases where analyzing unstructured text would otherwise be an increased burden
    on a human analyst.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Elastic ML via the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with just about everything in the Elastic Stack, ML can also be completely
    automated via API calls—including job configuration, execution, and result gathering.
    Actually, all interactions you have in the Kibana UI leverage the ML API behind
    the scenes. You could, for example, completely write your own UI if there were
    specific workflows or visualizations that you wanted.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more in-depth information about the anomaly detection APIs, please refer
    to [elastic.co/guide/en/machine-learning/current/ml-api-quickref.html](http://elastic.co/guide/en/machine-learning/current/ml-api-quickref.html).
    The data frame analytics part of Elastic ML has a completely separate API, which
    will be discussed in *Chapters 9* to *13*.
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into each API call, but we would like to highlight some parts that
    are worth a detour.
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious first API to mention is the job creation API, which allows the
    creation of the ML job configuration. For example, if you wanted to recreate the
    population analysis job shown in *Figure 3.25*, the following call would create
    that job, which we will call `revenue_over_users_api`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `job_id` field needs to be unique when creating the job.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create the companion datafeed configuration for this job, we would
    issue this separate API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the default query to the index is `match_all`, which means that
    no filtering will take place. We could, of course, insert any valid Elasticsearch
    DSL in the query block to perform custom filters or aggregations. This concept
    will be covered later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: There are other APIs that can be used to extract results or modify other operational
    aspects of the ML job. Consult the online documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen that Elastic ML can highlight variations in volume, diversity, and
    uniqueness in metrics and log messages, including those that need some categorization
    first. Also, we've shown that population analysis can be an extremely interesting
    alternative to temporal anomaly detection when the focus is more on finding the
    most unusual entities. These techniques help solve the challenges we described
    before, where a human might struggle to recognize what is truly unusual and worthy
    of attention and investigation.
  prefs: []
  type: TYPE_NORMAL
- en: The skills learned in this chapter will be helpful in subsequent chapters, where
    we will see how ML assists in the process of getting to the root cause of complex
    IT problems, identifying application performance slowdowns, or when ML can assist
    in the identification of malware and/or malicious activity.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll see how the expressive time series models built by
    anomaly detection jobs can be leveraged to forecast trends of your data into the
    future.
  prefs: []
  type: TYPE_NORMAL
