<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Clustering</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hierarchical clustering - World Bank</li><li class="listitem" style="list-style-type: disc">Hierarchical clustering - Amazon rainforest burned between 1999-2010</li><li class="listitem" style="list-style-type: disc">Hierarchical clustering - gene clustering</li><li class="listitem" style="list-style-type: disc">Binary clustering - math test</li><li class="listitem" style="list-style-type: disc">K-means clustering - European countries protein consumption</li><li class="listitem" style="list-style-type: disc">K-means clustering - foodstuff</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec15"/>Introduction</h1></div></div></div><p>
<span class="strong"><strong>Hierarchical clustering</strong></span>: One of the most important methods in unsupervised learning is Hierarchical clustering. In Hierarchical clustering for a given set of data points, the output is produced in the form of a binary tree (dendrogram). In the binary tree, the leaves represent the data points while internal nodes represent nested clusters of various sizes. Each object is assigned a separate cluster. Evaluation of all the clusters takes place based on a pairwise distance matrix. The distance matrix will be constructed using distance values. The pair of clusters with the shortest distance must be considered. The identified pair should then be removed from the matrix and merged together. The merged clusters' distance must be evaluated with the other clusters and the distance matrix should be updated. The process is to be repeated until the distance matrix is reduced to a single element.</p><p>An ordering of the objects is produced by hierarchical clustering. This helps with informative data display. The smaller clusters produced help in the discovery of information. The main disadvantage of hierarchical clustering is that, if the objects have been incorrectly grouped at an early stage then, there is no provision for a relocation of objects. Use of different distance metrics for measuring distances between clusters may result in the generation of different results.</p><p>
<span class="strong"><strong>K-means clustering</strong></span>: The K-means clustering algorithm is a method for estimating the mean (vectors) of a set of K-groups. The K-Means clustering method is unsupervised, non-deterministic, and iterative in nature. The method produces a specific number of disjointed, flat (non-hierarchical) clusters. K denotes the number of clusters. These clusters are based on the data at hand. Each of the clusters has at least one data point. The clusters are non-overlapping and non-hierarchical in nature. The dataset is partitioned into K number of clusters. The data points are randomly assigned to each of the clusters. This results in an almost equal distribution of data points among the clusters at the early stage. If a data point is closest to its own cluster, it is not changed. If a data point is not close to its own cluster, it is moved to the cluster to which it is closest. The steps are repeated for all the data points till no data points are moving from one cluster to another. At this point the clusters are stabilized and the clustering process ends. The choice of initial an partition can greatly affect the final clusters that result, in terms of inter-cluster and intra-cluster distances and cohesion.</p><p>The main advantage of K-means clustering is that it is relatively computationally, less expensive in terms of time compared to hierarchical clustering. The main challenge is that there is a difficulty in determining the number of clusters.</p></div></div>
<div class="section" title="Hierarchical clustering - World Bank sample dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec16"/>Hierarchical clustering - World Bank sample dataset</h1></div></div></div><p>One of the main goals for establishing the World Bank was to fight and eliminate poverty. Continuous evolution and fine-tuning its policies in the ever-evolving world has been helping the institution to achieve the goal of poverty elimination. The barometer of success in the elimination of poverty is measured in terms of improvement of each of the parameters in health, education, sanitation, infrastructure, and other services needed to improve the lives of the poor. The development gains that will ensure the goals must be pursued in an environmentally, socially, and economically sustainable manner.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec9"/>Getting ready</h2></div></div></div><p>In order to perform Hierarchical clustering, we shall be using a dataset collected from the World Bank dataset.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec24"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset titled <code class="literal">WBClust2013</code> shall be used. This is available in the CSV format titled <code class="literal">WBClust2013.csv</code>. The dataset is in standard format. There are 80 rows of data and 14 variables. The numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">new.forest</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Rural</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">log.CO2</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">log.GNI</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">log.Energy.2011</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">LifeExp</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Fertility</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">InfMort</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">log.Exports</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">log.Imports</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">CellPhone</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">RuralWater</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Pop</code></li></ul></div><p>The non-numeric variable is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Country</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec10"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec25"/>Step 2 - exploring data</h3></div></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note4"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>Let's explore the data and understand the relationships between variables. We'll begin by importing the CSV file named <code class="literal">WBClust2013.csv</code>. We will be saving the data to the <code class="literal">wbclust</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; wbclust=read.csv("d:/WBClust2013.csv",header=T)</strong></span>
</pre><p>Next, we shall print the <code class="literal">wbclust</code> data frame. The <code class="literal">head()</code> function returns the <code class="literal">wbclust</code> data frame. The <code class="literal">wbclust</code> data frame is passed as an input parameter:</p><pre class="programlisting">
<span class="strong"><strong>&gt; head(wbclust)</strong></span>
</pre><p>The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_001.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - transforming data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec26"/>Step 3 - transforming data</h3></div></div></div><p>Centering variables and creating z-scores are two common data analysis activities to standardize data. The numeric variables mentioned above need to create z-scores. The <code class="literal">scale()</code> function is a generic function whose default method centers and/or scales the columns of a numeric matrix. The data frame, <code class="literal">wbclust</code> is passed to the scale function. Only numeric fields are considered. The result is then stored in another data frame, <code class="literal">wbnorm</code>.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; wbnorm &lt;- scale(wbclust[,2:13])</strong></span>
<span class="strong"><strong>    &gt; wbnorm</strong></span>
</pre><p>The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_002.jpg" alt="Step 3 - transforming data"/></div><p>
</p><p>All data frames have a <code class="literal">rownames</code> attribute. In order to retrieve or set the row or column names of a matrix-like object, the <code class="literal">rownames()</code> function is used. The data frame <code class="literal">wbclust</code> with the first column is passed to the <code class="literal">rownames()</code> function.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; rownames(wbnorm)=wbclust[,1]</strong></span>
<span class="strong"><strong>    &gt; rownames(wbnorm)</strong></span>
</pre><p>The call to the function <code class="literal">rownames(wbnorm)</code> results in displaying of the values from the first column. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_003.jpg" alt="Step 3 - transforming data"/></div><p>
</p></div><div class="section" title="Step 4 - training and evaluating the model performance"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec27"/>Step 4 - training and evaluating the model performance</h3></div></div></div><p>The next step is about training the model. The first step is to calculate the distance matrix. The <code class="literal">dist()</code> function is used. Using the specified distance measure, distances between the rows of a data matrix are computed. The distance measure used can be Euclidean, maximum, Manhattan, Canberra, binary, or Minkowski. The distance measure used is Euclidean. The Euclidean distance calculates the distance between two vectors as <span class="emphasis"><em>sqrt(sum((x_i - y_i)^2))</em></span>. The result is then stored in a new data frame, <code class="literal">dist1</code>.</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist1 &lt;- dist(wbnorm, method="euclidean")</strong></span>
</pre><p>The next step is to perform clustering using Ward's method. The <code class="literal">hclust()</code> function is used. In order to perform cluster analysis on a set of dissimilarities of <span class="emphasis"><em>n</em></span> objects, the <code class="literal">hclust()</code> function is used. At the first stage, each of the objects is assigned to its own cluster. After this, at each stage the algorithm iterates and joins two of the most similar clusters. This process continues till there is just a single cluster left. The <code class="literal">hclust()</code> function requires that we provide the data in the form of a distance matrix. The <code class="literal">dist1</code> data frame is passed. By default, the complete linkage method is used. There are multiple agglomeration methods which can be used, some of them could be <code class="literal">ward.D</code>, <code class="literal">ward.D2</code>, <code class="literal">single</code>, <code class="literal">complete</code>, and <code class="literal">average</code>.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; clust1 &lt;- hclust(dist1,method="ward.D")</strong></span>
<span class="strong"><strong>    &gt; clust1</strong></span>
</pre><p>The call to the function <code class="literal">clust1</code> results in displaying the agglomeration methods used, the manner in which the distance is calculated, and the number of objects. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_004.jpg" alt="Step 4 - training and evaluating the model performance"/></div><p>
</p></div><div class="section" title="Step 5 - plotting the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec28"/>Step 5 - plotting the model</h3></div></div></div><p>The <code class="literal">plot()</code> function is a generic function for plotting R objects. Here, the <code class="literal">plot()</code> function is used to draw the dendrogram:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(clust1,labels= wbclust$Country, cex=0.7, xlab="",ylab="Distance",main="Clustering for 80 Most Populous Countries")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_005.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>The <code class="literal">rect.hclust()</code> function highlights the clusters and draws the rectangles around the branches of the dendrogram. The dendrogram is first cut at a certain level followed by drawing a rectangle around the selected branches.</p><p>The object <code class="literal">clust1</code> is passed as an object to the function along with the number of clusters to be formed:</p><pre class="programlisting">
<span class="strong"><strong>&gt; rect.hclust(clust1,k=5)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_006.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>The <code class="literal">cuts()</code> function shall cut the tree into multiple groups on the basis of the desired number of groups or the cut height. Here, <code class="literal">clust1</code> is passed as an object to the function along with the number of the desired group:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; cuts=cutree(clust1,k=5)</strong></span>
<span class="strong"><strong>    &gt; cuts</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_007.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>Getting the list of countries in each group:</p><pre class="programlisting">
<span class="strong"><strong>for (i in 1:5){</strong></span>
<span class="strong"><strong>     print(paste("Countries in Cluster ",i))</strong></span>
<span class="strong"><strong>     print(wbclust$Country[cuts==i])</strong></span>
<span class="strong"><strong>     print (" ")</strong></span>
<span class="strong"><strong>}</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_008.jpg" alt="Step 5 - plotting the model"/></div><p>
</p></div></div></div>
<div class="section" title="Hierarchical clustering - Amazon rainforest burned between 1999-2010"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Hierarchical clustering - Amazon rainforest burned between 1999-2010</h1></div></div></div><p>Between 1999-2010, 33,000 square miles (85,500 square kilometers), or 2.8 percent of the Amazon rainforest burned down. This was found by NASA-led research. The main purpose of the research was to measure the extent of fire smolders under the forest canopy. The research found that burning forests destroys a much larger area compared to when forest lands are cleared for agriculture and cattle pasture. Yet, no correlation could be established between the fires and deforestation.</p><p>The answer to the query of no correlation between fires and deforestation lay in humidity data from the <span class="strong"><strong>Atmospheric Infrared Sounder</strong></span> (<span class="strong"><strong>AIRS</strong></span>) instrument aboard NASA's Aqua satellite. The fire frequency coincides with low night-time humidity, which allowed the low-intensity surface fires to continue burning.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec11"/>Getting ready</h2></div></div></div><p>In order to perform hierarchical clustering, we shall be using a dataset collected on the Amazon rainforest, which burned from 1999-2010.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec29"/>Step 1 - collecting and describing data</h3></div></div></div><p>The  <code class="literal">NASAUnderstory</code> dataset shall be used. This is available in CSV format as <code class="literal">NASAUnderstory.csv</code>. The dataset is in standard format. There are 64 rows of data and 32 variables. The numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">PlotID</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">SPHA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">BLIT</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ASMA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">MOSS</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">LEGR</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">CHCA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">GRAS</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">SEDG</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">SMTR</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">PTAQ</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">COCA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">VAAN</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">GAHI</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ARNU</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">LYOB</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">PIMA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">RUBU</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">VAOX</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ACSP</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">COCO</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ACRU</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">TRBO</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">MACA</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">CLOB</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">STRO</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">FUNG</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DILO</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ERIO</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">GATR</code></li></ul></div><p>The non-numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Overstory Species</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Labels</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec12"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec30"/>Step 2 - exploring data</h3></div></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note5"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>Let's explore the data and understand the relationships among the variables. We'll begin by importing the file named <code class="literal">NASAUnderstory.csv</code>. We will be saving the data to the <code class="literal">NASA</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; NASA = read.csv("d:/NASAU   nderstory.csv",header=T)</strong></span>
</pre><p>Next, we shall obtain the long version of each of the species column labels:</p><pre class="programlisting">
<span class="strong"><strong>
<span class="emphasis"><em>&gt; </em></span>NASA.lab=NASA$Labels</strong></span>
</pre><p>Next, we shall print the <code class="literal">NASA.lab</code> data frame. This contains the complete name of each of the species as obtained.</p><p>The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_009.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>Next, we shall pass the entire data content to the <code class="literal">NASA</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; NASA=NASA[,-32]</strong></span>
</pre><p>Printing the <code class="literal">NASA</code> data frame shall results in displaying the entire data content.</p><pre class="programlisting">
<span class="strong"><strong>&gt; NASA</strong></span>
</pre><p>The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_010.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - transforming data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec31"/>Step 3 - transforming data</h3></div></div></div><p>Next, data standardization shall be carried out. The <code class="literal">scale()</code> function shall center and scale the columns of all the numeric variables as mentioned earlier:</p><pre class="programlisting">
<span class="strong"><strong>&gt; NASAscale &lt;- scale(NASA[,3:31])</strong></span>
</pre><p>This shall scale all the numeric values between columns <code class="literal">3</code> to <code class="literal">31</code> of the <code class="literal">NASA</code> data frame.</p><p>Printing the <code class="literal">NASAscale</code> data frame results in displaying all the scaled and centered values of the <code class="literal">NASAscale.</code>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; NASAscale</strong></span>
</pre><p>The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_011.jpg" alt="Step 3 - transforming data"/></div><p>
</p><p>In order to encode a vector as a factor, the function factor is used. If the argument ordered is <code class="literal">TRUE</code>, the factor levels are assumed to be ordered. Here, we are passing the <code class="literal">OverstorySpecies</code> column as a value to the factor function:</p><pre class="programlisting">
<span class="strong"><strong>&gt; rownames(NASAscale)=as.factor(NASA$Overstory.Species) </strong></span>
</pre><p>The <code class="literal">as.factor()</code> returns a data frame with the row names.</p><p>Printing the data frame <code class="literal">rownames(NASAscale)</code> results in displaying all the values of the <code class="literal">OverstorySpecies</code> column:</p><pre class="programlisting">
<span class="strong"><strong>&gt; rownames(NASAscale)</strong></span>
</pre><p>The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_012.jpg" alt="Step 3 - transforming data"/></div><p>
</p></div><div class="section" title="Step 4 - training and evaluating model performance"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec32"/>Step 4 - training and evaluating model performance</h3></div></div></div><p>The next step is about training the model. The first step is to calculate the distance matrix. The <code class="literal">dist()</code> function is used. The function computes and returns the distance matrix, using the specified distance measure to compute the distances between the rows of a data matrix. The distance measure used can be Euclidean, maximum, Manhattan, Canberra, binary, or Minkowski. The distance measure used is Euclidean. The Euclidean distance calculates the distance between two vectors as <span class="emphasis"><em>sqrt(sum((x_i - y_i)^2))</em></span>. The result is then stored in a new data frame <code class="literal">dist1</code>.</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist1 &lt;- dist(NASAscale, method="euclidean")</strong></span>
</pre><p>The next step is to perform clustering using Ward's method. The <code class="literal">hclust()</code> function is used. In order to perform cluster analysis on a set of dissimilarities of <span class="emphasis"><em>n</em></span> objects, the <code class="literal">hclust()</code> function is used. At the first stage, each of the objects is assigned to its own cluster. The algorithm then proceeds iteratively at each stage joining the two most similar clusters. This process continues till there is just a single cluster left. The <code class="literal">hclust()</code> function requires us to provide the data in the form of a distance matrix. The <code class="literal">dist1</code> data frame is passed. By default, the complete linkage method is used. There can be multiple agglomeration methods which can be used, some of them could be <code class="literal">ward.D</code>, <code class="literal">ward.D2</code>, <code class="literal">single</code>, <code class="literal">complete</code>, and <code class="literal">average</code>.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; clust1 &lt;- hclust(dist1,method="ward.D")</strong></span>
<span class="strong"><strong>    &gt; clust1</strong></span>
</pre><p>The call to the function, <code class="literal">clust1</code> results in display of the agglomeration method used, the manner in which the distance is calculated, and the number of objects. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_013.jpg" alt="Step 4 - training and evaluating model performance"/></div><p>
</p></div><div class="section" title="Step 5 - plotting the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec33"/>Step 5 - plotting the model</h3></div></div></div><p>The <code class="literal">plot()</code> function is a generic function for the plotting R objects. Here, the <code class="literal">plot()</code> function is used to draw the dendrogram:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(clust1,labels= NASA[,2], cex=0.5, xlab="",ylab="Distance",main="Clustering for NASA Understory Data")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_014.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>The <code class="literal">rect.hclust()</code> function highlights clusters and draws rectangles around the branches of the dendrogram. The dendrogram is first cut at a certain level followed by drawing a rectangle around selected branches.</p><p>The object, <code class="literal">clust1</code> is passed as an object to the function along with the number of clusters to be formed:</p><pre class="programlisting">
<span class="strong"><strong>&gt; rect.hclust(clust1,k=2)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_015.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>The <code class="literal">cuts()</code> function shall cut the tree into multiple groups on the basis of the desired number of groups or the cut height. Here, <code class="literal">clust1</code> is passed as an object to the function along with the number of desired groups:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; cuts=cutree(clust1,k=2)</strong></span>
<span class="strong"><strong>    &gt; cuts</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_016.jpg" alt="Step 5 - plotting the model"/></div><p>
</p></div><div class="section" title="Step 6 - improving model performance"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec34"/>Step 6 - improving model performance</h3></div></div></div><p>The following package needs to be loaded as a first step:</p><pre class="programlisting">
<span class="strong"><strong>&gt; library(vegan)</strong></span>
</pre><p>The <code class="literal">vegan</code> library is primarily used by community and vegetation ecologists. It contains ordination methods, diversity analysis, and other functions. Some of the popular tools are <span class="strong"><strong>diversity analysis, species abundance models, analysis of species richness, dissimilarity analyses, and so on.</strong></span>
</p><p>The next step is about improving the model by training using the distance method, <code class="literal">jaccard</code>. The first step is to calculate the distance matrix. The <code class="literal">vegdist()</code> function is used. The function calculates pairwise distance. The result is then stored in a new data frame, <code class="literal">dist1</code>. The <code class="literal">jaccard</code> coefficient measures similarity between finite sample sets. This is calculated by dividing the size of the intersection by the size of the union of the sample sets:</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist1 &lt;- vegdist(NASA[,3:31], method="jaccard", upper=T)</strong></span>
</pre><p>The next step is to perform clustering using Ward's method. The <code class="literal">hclust()</code> function is used:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; clust1 &lt;- hclust(dist1,method="ward.D")</strong></span>
<span class="strong"><strong>    &gt; clust1</strong></span>
</pre><p>The call to the function, <code class="literal">clust1</code> results in display of the agglomeration methods used, the manner in which the distance is calculated, and the number of objects. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_017.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>The <code class="literal">plot()</code> function is a generic function for the plotting R objects:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(clust1,labels= NASA[,2], cex=0.5, xlab="",ylab="Distance",main="Clustering for NASA Understory Data")</strong></span>
</pre><p>The <code class="literal">clust1</code> data frame is passed as an object to the function. <code class="literal">cex</code> gives the numerical value of the amount by which plotting text and symbols can be magnified relative to the default.</p><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_018.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>The object <code class="literal">clust1</code> is passed as an object to the function along with the number of clusters to be formed:</p><pre class="programlisting">
<span class="strong"><strong>&gt; rect.hclust(clust1,k=2)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/B04714_03_19.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>The <code class="literal">cuts()</code> function shall cut the tree into multiple groups on the basis of the desired number of groups or the cut height:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; cuts=cutree(clust1,k=2)</strong></span>
<span class="strong"><strong>    &gt; cuts</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_020.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>Using principal components lets us plot of two cluster solutions.</p><p>The <code class="literal">clusplot()</code> function shall draw a two-dimensional clustering plot. Here, the <code class="literal">NASA</code> data frame is passed as an object.</p><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_021.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>Using discriminant functions lets us plot two cluster solutions.</p><p>The <code class="literal">plotcluster()</code> function plots using projection methods in order to distinguish the given classes. Various projection methods include classical discriminant coordinates, methods to project differences in mean and covariance structure, asymmetric methods (separation of a homogeneous class from a heterogeneous one), local neighborhood-based methods and methods based on robust covariance matrices.</p><p>The <code class="literal">clusplot()</code> function shall draw a two-dimensional clustering plot. Here, the <code class="literal">NASA</code> data frame is passed as an object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; clusplot(NASA, cuts, color=TRUE, shade=TRUE, labels=2, lines=0,  main="NASA Two Cluster  Plot, Ward's Method, First two PC")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/B04714_03_22.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>Next, transposing the <code class="literal">NASAscale</code> data frame takes place using the <code class="literal">t()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; library(fpc)</strong></span>
<span class="strong"><strong>    &gt; NASAtrans=t(NASAscale)</strong></span>
</pre><p>The next step is about improving the model by training using the Minkowski distance method. The first step is to calculate the distance matrix. The <code class="literal">dist() </code>function is used.</p><p>The Minkowski distance is often used when variables are measured on ratio scales with an absolute zero value.</p><pre class="programlisting">
<span class="strong"><strong>
&gt; dist1 &lt;- dist(NASAtrans, method="minkowski", p=3)
</strong></span>
</pre><p>The next step is to perform clustering using Ward's method. The <code class="literal">hclust()</code> function is used.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; clust1 &lt;- hclust(dist1,method="ward.D")</strong></span>
<span class="strong"><strong>    &gt; clust1</strong></span>
</pre><p>The call to the <code class="literal">clust1</code> function results in display of the agglomeration method used, the manner in which the distance is calculated, and the number of objects. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_023.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>The <code class="literal">plot()</code> function is a generic function for the plotting R objects. Here, the <code class="literal">plot()</code> function is used to draw the dendrogram:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(clust1,labels= NASA.lab[1:29], cex=1, xlab="",ylab="Distance",main="Clustering for NASA Understory Data")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_024.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>The <code class="literal">rect.hclust()</code> function shall draw rectangles around the branches of the dendrogram highlighting the corresponding clusters. First, the dendrogram is cut at a certain level, and then a rectangle is drawn around selected branches.</p><p>The <code class="literal">clust1</code> object is passed as an object to the function along with the number of clusters to be formed:</p><pre class="programlisting">
<span class="strong"><strong>&gt; rect.hclust(clust1,k=3)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_025.jpg" alt="Step 6 - improving model performance"/></div><p>
</p><p>The <code class="literal">cuts()</code> function shall cut the tree into multiple groups on the basis of the desired number of groups or the cut height. Here, <code class="literal">clust1</code> is passed as an object to the function along with the number of the desired group:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; cuts=cutree(clust1,k=3)</strong></span>
<span class="strong"><strong>    &gt; cuts</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_026.jpg" alt="Step 6 - improving model performance"/></div><p>
</p></div></div></div>
<div class="section" title="Hierarchical clustering - gene clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Hierarchical clustering - gene clustering</h1></div></div></div><p>The ability to gather genome-wide expression data is a computationally complex task. The human brain with its limitations cannot solve the problem. However, data can be fine-grained to an easily comprehensible level by subdividing the genes into a smaller number of categories and then analyzing them.</p><p>The goal of clustering is to subdivide a set of genes in such a way that similar items fall into the same cluster, whereas dissimilar items fall into different clusters. The important questions to be considered are decisions on similarity and usage for the items that have been clustered. Here we shall explore clustering genes and samples using the photoreceptor time series for the two genotypes.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec13"/>Getting ready</h2></div></div></div><p>In order to perform Hierarchical clustering, we shall be using a dataset collected on mice.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec35"/>Step 1 - collecting and describing data</h3></div></div></div><p>The datasets titled <code class="literal">GSE4051_data</code> and <code class="literal">GSE4051_design</code> shall be used. These are available in the CSV format titled <code class="literal">GSE4051_data.csv</code> and <code class="literal">GSE4051_design.csv</code>. The dataset is in standard format.</p><p>In <code class="literal">GSE4051_data</code> there are 29,949 rows of data and 39 variables. The numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_21</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_22</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_23</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_16</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_17</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_6</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_24</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_25</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_26</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_27</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_14</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_3</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_5</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_8</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_28</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_29</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_30</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_31</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_1</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_10</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_4</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_7</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_32</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_33</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_34</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_35</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_13</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_15</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_18</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_19</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_36</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_37</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_38</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_39</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_11</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_12</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_2</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Sample_9</code></li></ul></div><p>In the <code class="literal">GSE4051_design</code> dataset there are 39 rows of data and 4 variables. The numeric variable is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">sidNum</code></li></ul></div><p>The non-numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">sidChar</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">devStage</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">gType</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec14"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec36"/>Step 2 - exploring data</h3></div></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note6"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>The <code class="literal">RColorBrewer</code> package is an R package from <a class="ulink" href="http://colorbrewer2.org">
http://colorbrewer2.org
</a> and provides color schemes for maps and other graphics.</p><p>The <code class="literal">pvclust</code> package is used for assessing uncertainty in hierarchical cluster analysis. In hierarchical clustering, each of the clusters calculates p-values via multi-scale bootstrap resampling. The p-value of a cluster is measured between 0 and 1. There are two types of p-value available: <span class="strong"><strong>approximately</strong></span>
<span class="strong"><strong>unbiased</strong></span> (<span class="strong"><strong>AU</strong></span>) and <span class="strong"><strong>bootstrap probability</strong></span> (<span class="strong"><strong>BP</strong></span>) value. The AU p-value is calculated using the multi-scale bootstrap resampling method, while the ordinary bootstrap resampling method is used to calculate the BP p-value. The AU p-value has superiority bias compared to the BP p-value.</p><p>LaTeX-formatted tables are produced by the <code class="literal">xtable</code> package. Using <code class="literal">xtable</code>, package-specific R objects can be turned into <code class="literal">xtables</code>. These <code class="literal">xtables</code> can then be output in either LaTeX or HTML formats.</p><p>The <code class="literal">plyr</code> package is used as a tool for carrying out <span class="strong"><strong>split-apply-combine </strong></span>(<span class="strong"><strong>SAC</strong></span>) procedures. It breaks a big problem down into manageable pieces, operates on each piece, and then puts all the pieces back together.</p><p>The following packages must be loaded:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; library(RColorBrewer)</strong></span>
<span class="strong"><strong>    &gt; library(cluster)</strong></span>
<span class="strong"><strong>    &gt; library(pvclust)</strong></span>
<span class="strong"><strong>    &gt; library(xtable)</strong></span>
<span class="strong"><strong>    &gt; library(plyr)</strong></span>
</pre><p>Let's explore the data and understand the relationships among the variables. We'll begin by importing the CSV file named <code class="literal">GSE4051_data.csv</code>. We will be saving the data to the <code class="literal">GSE4051_data</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; GSE4051_data =read.csv("d:/ GSE4051_data.csv",header=T)</strong></span>
</pre><p>Next, we shall print information about the <code class="literal">GSE4051_data</code> data frame. The <code class="literal">str()</code> function returns the provided information about the structure of the <code class="literal">GSE4051_data</code> data frame. It compactly displays the internal structure of the <code class="literal">GSE4051_data</code> data frame. <code class="literal">max.level</code> indicates the maximal level of nesting applied to display nested structures:</p><pre class="programlisting">
<span class="strong"><strong>&gt; str(GSE4051_data, max.level = 0) </strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_027.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>Next, we shall import the CSV file named <code class="literal">GSE4051_design.csv</code>. We will be saving the data to the <code class="literal">GSE4051_design</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; GSE4051_design =read.csv("d:/ GSE4051_design.csv",header=T)</strong></span>
</pre><p>The preceding line prints the internal structure of the <code class="literal">GSE4051_design</code> data frame.</p><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_028.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - transforming data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec37"/>Step 3 - transforming data</h3></div></div></div><p>In order to ease visualization at a later stage, the rows are rescaled. Since the absolute differences in the expression between genes at the currently required, rescaling of the rows is carried out.</p><p>Centering variables and creating z-scores are two common data analysis activities. The <code class="literal">scale</code> function centers and/or scales the columns of a numeric matrix.</p><p>Transposing the matrix. The <code class="literal">GSE4051_data</code> data frame is passed for transposition of the data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; trans_GSE4051_data &lt;- t(scale(t(GSE4051_data)))</strong></span>
</pre><p>Next, we shall print information about the <code class="literal">GSE4051_data</code> data frame. With <code class="literal">give.attr = FALSE</code>, attributes as sub structures are not displayed.</p><pre class="programlisting">
<span class="strong"><strong>&gt; str(trans_GSE4051_data, max.level = 0, give.attr = FALSE)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_029.jpg" alt="Step 3 - transforming data"/></div><p>
</p><p>The <code class="literal">head()</code> function returns the first part of a vector, matrix, table, data frame, or function. The <code class="literal">GSE4051_data</code> and <code class="literal">trans_GSE4051_data</code> data frames are passed as objects. The <code class="literal">rowMeans()</code> function calculates the means of rows. The <code class="literal">data.frame()</code> function creates data frames that are tightly coupled collections of variables and share many of the properties of matrices:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; round(data.frame(avgBefore = rowMeans(head(GSE4051_data)), </strong></span>
<span class="strong"><strong>                      avgAfter = rowMeans(head(trans_GSE4051_data)), </strong></span>
<span class="strong"><strong>                      varBefore = apply(head(GSE4051_data), 1, var), </strong></span>
<span class="strong"><strong>                      varAfter = apply(head(trans_GSE4051_data),                      1, var)), 2)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_030.jpg" alt="Step 3 - transforming data"/></div><p>
</p></div><div class="section" title="Step 4 - training the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec38"/>Step 4 - training the model</h3></div></div></div><p>The next step is training the model. The first step is to calculate the distance matrix. The <code class="literal">dist()</code> function is used. The function computes and returns the distance matrix, using the specified distance measure to compute the distances between the rows of a data matrix. The distance measure used can be Euclidean, maximum, Manhattan, Canberra, binary, or Minkowski. The distance measure used is Euclidean. The Euclidean distance calculates the distance between two vectors as <span class="emphasis"><em>sqrt(sum((x_i - y_i)^2))</em></span>. The transposed <code class="literal">trans_GSE4051_data</code> data frame is used to calculate the distance. The result is then stored in the <code class="literal">pair_dist_GSE4051_data</code> data frame.</p><pre class="programlisting">
<span class="strong"><strong>&gt; pair_dist_GSE4051_data &lt;- dist(t(trans_GSE4051_data), method = 'euclidean')</strong></span>
</pre><p>Next, the <code class="literal">interaction()</code> function is used, which computes and returns an unordered factor with the interaction of the <code class="literal">gType</code>, <code class="literal">devStage</code> variables. The result of unordered factors is passed to the <code class="literal">with()</code> function along with the data frame, <code class="literal">GSE4051_design</code>. This creates a new factor representing the interaction of <code class="literal">gType</code>, <code class="literal">devStage</code> variables:</p><pre class="programlisting">
<span class="strong"><strong>&gt; GSE4051_design$group &lt;- with(GSE4051_design, interaction(gType, devStage))</strong></span>
</pre><p>The <code class="literal">summary()</code> function is used to produce result summaries of the data frame, <code class="literal">GSE4051_design$group</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; summary(GSE4051_design$group)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_031.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>Next, the computing of hierarchical clustering using various linkage types is carried out.</p><p>The <code class="literal">hclust()</code> function is used. In order to perform cluster analysis on a set of dissimilarities of <span class="emphasis"><em>n</em></span> objects, the <code class="literal">hclust()</code> function is used. At the first stage, each of the objects is assigned to its own cluster. The algorithm then proceeds iteratively at each stage joining the two most similar clusters. This process continues till there is just a single cluster left. The <code class="literal">hclust()</code> function requires we provide the data in the form of a distance matrix. The <code class="literal">pair_dist_GSE4051_data</code> data frame is passed.</p><p>The agglomeration method, <code class="literal">single</code>, is used as the first case:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.single &lt;- hclust(pair_dist_GSE4051_data, method = 'single')</strong></span>
</pre><p>The call to <code class="literal">pr.hc.single</code> results in display of the agglomeration method used, the manner in which the distance is calculated and the number of objects:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.single</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_032.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>The agglomeration method, <code class="literal">complete</code> is used as the second case:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.complete &lt;- hclust(pair_dist_GSE4051_data, method = 'complete')</strong></span>
</pre><p>The call to <code class="literal">pr.hc.complete</code> results in display of the agglomeration method used, the manner in which the distance is calculated and the number of objects:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.complete</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_033.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>The agglomeration method <code class="literal">average</code> is used as the third case:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.average &lt;- hclust(pair_dist_GSE4051_data, method = 'average')</strong></span>
</pre><p>The call to <code class="literal">pr.hc.average</code> results in display of the agglomeration method used, the manner in which the distance is calculated, and the number of objects:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.average</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_034.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>The agglomeration method ward is used as the fourth case:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.ward &lt;- hclust(pair_dist_GSE4051_data, method = 'ward.D2')</strong></span>
</pre><p>The call to <code class="literal">pr.hc.ward</code> results in display of the agglomeration method used, the manner in which the distance is calculated, and the number of objects:</p><pre class="programlisting">
<span class="strong"><strong>&gt; pr.hc.ward</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_035.jpg" alt="Step 4 - training the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; op &lt;- par(mar = c(0,4,4,2), mfrow = c(2,2))</strong></span>
</pre><p>The <code class="literal">plot()</code> function is a generic function for plotting R objects.</p><p>The first call to the <code class="literal">plot()</code> function passes the <code class="literal">pr.hc.single</code> data frame as an object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(pr.hc.single, labels = FALSE, main = "Single Linkage Representation", xlab = "")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_036.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>The second call to the <code class="literal">plot()</code> function passes the <code class="literal">pr.hc.complete</code> data frame as an object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(pr.hc.complete, labels = FALSE, main = "Complete Linkage Representation", xlab = "")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_037.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>The third call to the <code class="literal">plot()</code> function passes the <code class="literal">pr.hc.average</code> data frame as an object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(pr.hc.average, labels = FALSE, main = "Average Linkage Representation", xlab = "")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_038.jpg" alt="Step 4 - training the model"/></div><p>
</p><p>The fourth call to the <code class="literal">plot()</code> function passes the <code class="literal">pr.hc.ward</code> data frame as an object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(pr.hc.ward, labels = FALSE, main = "Ward Linkage Representation", xlab = "")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_039.jpg" alt="Step 4 - training the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>    &gt; par(op)</strong></span>
<span class="strong"><strong>    &gt; op &lt;- par(mar = c(1,4,4,1))</strong></span>
</pre></div><div class="section" title="Step 5 - plotting the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec39"/>Step 5 - plotting the model</h3></div></div></div><p>The <code class="literal">plot()</code> function is a generic function for the plotting R objects. Here, the <code class="literal">plot()</code> function is used to draw the dendrogram.</p><p>The <code class="literal">rect.hclust() </code>function shall draw rectangles around the branches of the dendrogram highlighting the corresponding clusters. First the dendrogram is cut at a certain level, and then a rectangle is drawn around selected branches.</p><p>
<code class="literal">RColorBrewer</code> uses the work from <a class="ulink" href="http://colorbrewer2.org/">
http://colorbrewer2.org/
</a> to choose sensible color schemes for figures in R.</p><p>The colors are split into three groups:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sequential: Low data--light colors; high data--dark colors</li><li class="listitem" style="list-style-type: disc">Diverging: Mid-range data--light colors; low and high range data--contrasting dark colors</li><li class="listitem" style="list-style-type: disc">Qualitative: Colors have been designed to highlight the maximum visual difference between classes</li></ul></div><p>One of the important functions of <code class="literal">RColorBrewer</code> is <code class="literal">brewer.pal()</code>. This function allows one to choose from the <code class="literal">display.brewer.all()</code> function by passing the number of colors and the name of the palette.</p><p>As a first case, <code class="literal">pr.hc.single</code> is passed as an object to the <code class="literal">plot()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; plot(pr.hc.single, labels = GSE4051_design$group, cex = 0.6, main = "Single Hierarchical Cluster - 10 clusters")</strong></span>
<span class="strong"><strong>    &gt; rect.hclust(clust1,k=5)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_040.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>Next, we create the heat maps using the <code class="literal">single</code> agglomeration method. By default, the <code class="literal">heatmap()</code> function uses the agglomeration method <code class="literal">euclidean</code>:</p><pre class="programlisting">
<span class="strong"><strong>     &gt; par(op)</strong></span>
<span class="strong"><strong>     &gt; jGraysFun &lt;- colorRampPalette(brewer.pal(n = 9, "Blues"))</strong></span>
<span class="strong"><strong>     &gt; gTypeCols &lt;- brewer.pal(9, "Spectral")[c(4,7)]</strong></span>
<span class="strong"><strong>     &gt; heatmap(as.matrix(trans_GSE4051_data), Rowv = NA, col = jGraysFun(256), hclustfun = function(x) hclust(x, method = 'single'),</strong></span>
<span class="strong"><strong>     scale = "none", labCol = GSE4051_design$group, labRow = NA, margins = c(8,1), </strong></span>
<span class="strong"><strong> ColSideColor = gTypeCols[unclass(GSE4051_design$gType)])</strong></span>
<span class="strong"><strong>     &gt; legend("topright", legend = levels(GSE4051_design$gType), col = gTypeCols, lty = 1, lwd = 5, cex = 0.5)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_041.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>As a second case, <code class="literal">pr.hc.complete</code> is passed as an object to the <code class="literal">plot()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; plot(pr.hc.complete, labels = GSE4051_design$group, cex = 0.6, main = "Complete Hierarchical Cluster - 10 clusters")</strong></span>
<span class="strong"><strong>    &gt; rect.hclust(pr.hc.complete, k = 10)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_042.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>Next we create heat maps using the <code class="literal">complete</code> agglomeration method.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; par(op)</strong></span>
<span class="strong"><strong>    &gt; jGraysFun &lt;- colorRampPalette(brewer.pal(n = 9, "Greens"))</strong></span>
<span class="strong"><strong>    &gt; gTypeCols &lt;- brewer.pal(11, "PRGn")[c(4,7)]</strong></span>
<span class="strong"><strong>    &gt; heatmap(as.matrix(trans_GSE4051_data), Rowv = NA, col = jGraysFun(256), hclustfun = function(x) hclust(x, method = 'complete'), </strong></span>
<span class="strong"><strong> scale = "none", labCol = GSE4051_design$group, labRow = NA, margins = c(8,1),</strong></span>
<span class="strong"><strong> ColSideColor = gTypeCols[unclass(GSE4051_design$gType)])</strong></span>
<span class="strong"><strong>    &gt; legend("topright", legend = levels(GSE4051_design$gType), col = gTypeCols, lty = 1, lwd = 5, cex = 0.5)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_043.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>As a third case, <code class="literal">pr.hc.average</code> is passed as an object to the <code class="literal">plot() </code>function:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; plot(pr.hc.average, labels = GSE4051_design$group, cex = 0.6, main = "Average Hierarchical Cluster - 10 clusters")</strong></span>
<span class="strong"><strong>    &gt; rect.hclust(pr.hc.average, k = 10)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_044.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>Next, we create heat maps using the <code class="literal">average</code> agglomeration method:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; jGraysFun &lt;- colorRampPalette(brewer.pal(n = 9, "Oranges"))</strong></span>
<span class="strong"><strong>    &gt; gTypeCols &lt;- brewer.pal(9, "Oranges")[c(4,7)]</strong></span>
<span class="strong"><strong>    &gt; heatmap(as.matrix(trans_GSE4051_data), Rowv = NA, col = jGraysFun(256), hclustfun = function(x) hclust(x, method = 'average'), </strong></span>
<span class="strong"><strong>    scale = "none", labCol = GSE4051_design$group, labRow = NA, margins = c(8,1), </strong></span>
<span class="strong"><strong> ColSideColor = gTypeCols[unclass(GSE4051_design$gType)])</strong></span>
<span class="strong"><strong>    &gt; legend("topright", legend = levels(GSE4051_design$gType), col = gTypeCols, lty = 1, lwd = 5, cex = 0.5)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_045.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>As a fourth case, <code class="literal">pr.hc.ward</code> is passed as an object to the <code class="literal">plot()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; plot(pr.hc.ward, labels = GSE4051_design$group, cex = 0.6, main = "Ward Hierarchical Cluster - 10 clusters")</strong></span>
<span class="strong"><strong>    &gt; rect.hclust(pr.hc.ward, k = 10)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_046.jpg" alt="Step 5 - plotting the model"/></div><p>
</p><p>Next, we create heat maps using the <code class="literal">ward</code> agglomeration method:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; jGraysFun &lt;- colorRampPalette(brewer.pal(n = 9, "Reds")) </strong></span>
<span class="strong"><strong>    &gt; gTypeCols &lt;- brewer.pal(9, "Reds")[c(4,7)] </strong></span>
<span class="strong"><strong>    &gt; heatmap(as.matrix(trans_GSE4051_data), Rowv = NA, col = jGraysFun(256), hclustfun = function(x) hclust(x, method = 'ward.D2'), </strong></span>
<span class="strong"><strong>    scale = "none", labCol = GSE4051_design$group, labRow = NA, margins = c(8,1), </strong></span>
<span class="strong"><strong>    ColSideColor = gTypeCols[unclass(GSE4051_design$gType)]) </strong></span>
<span class="strong"><strong>    &gt; legend("topright", legend = levels(GSE4051_design$gType), col = gTypeCols, lty = 1, lwd = 5, cex = 0.5)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_047.jpg" alt="Step 5 - plotting the model"/></div><p>
</p></div></div></div>
<div class="section" title="Binary clustering - math test"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Binary clustering - math test</h1></div></div></div><p>In the education system tests and examinations are major features. The advantage of examination system is that it can be one of the ways to differentiate between good and poor performers. The examination system puts the onus on students to upgrade for next standard for which they should appear and pass exams. It creates responsibility on students to study on regular basis. The exam systems prepare the students to meet the challenges of future. It helps them to analyze reason and communicate their ideas effectively in a fixed time period. On the other hand few draw backs are noticed such as slow learners cannot perform well in test and this creates inferior complexity among students.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec15"/>Getting ready</h2></div></div></div><p>In order to perform binary clustering, we shall be using a dataset collected on math tests.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec40"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset titled <code class="literal">math test</code> shall be used. This is available in the TXT format titled <code class="literal">math test.txt</code>. The dataset is in standard format. There are 60 rows of data. There are 60 columns. The columns are scores on items for 55 male students.</p></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec16"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec41"/>Step 2 - exploring data</h3></div></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note7"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>Let's explore the data and understand relationships among the variables. We'll begin by importing the TXT file named <code class="literal">ACT math test.txt</code>. We will be saving the data to the <code class="literal">Mathtest</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; Mathtest = read.table("d:/math test.txt",header=T)</strong></span>
</pre></div><div class="section" title="Step 3 - training and evaluating model performance"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec42"/>Step 3 - training and evaluating model performance</h3></div></div></div><p>Next, we shall perform clustering of the items. Groups of items based on the students' scores shall be clustered together.</p><p>First, we shall count the total mismatches based on the squared Euclidean distance.</p><p>The <code class="literal">dist()</code> function is called. The <code class="literal">Mathtest</code> data frame is passed as input to the <code class="literal">dist()</code> function. Counting the total mismatches based on the squared Euclidean distance, the result shall be stored in the, <code class="literal">dist.items</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist.items &lt;- dist(Mathtest[,-1], method='euclidean')^2</strong></span>
</pre><p>Next, we shall print the <code class="literal">dist.items</code> data frame.</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist.items</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_048.jpg" alt="Step 3 - training and evaluating model performance"/></div><p>
</p><p>Next, the distance measure ignores <code class="literal">0-0</code> matches altogether. The binary method shall be used in the <code class="literal">dist()</code> function. In the binary method, the non-zero elements are on and zero-elements are off since the vectors are considered binary bits.</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist.items.2 &lt;- dist(Mathtest[,-1], method='binary')</strong></span>
</pre><p>Next, we shall print the data frame, <code class="literal">dist.items.2</code>, to observe the result.</p><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_049.jpg" alt="Step 3 - training and evaluating model performance"/></div><p>
</p><p>Next, the distance measure ignores <code class="literal">1-1</code> matches altogether. The binary method shall be used in the <code class="literal">dist()</code> function. In the binary method, the non-zero elements are on and zero-elements are off since the vectors are considered binary bits.</p><pre class="programlisting">
<span class="strong"><strong>&gt; dist.items.3 &lt;- dist(1 - Mathtest[,-1], method='binary')</strong></span>
</pre><p>Next, we shall print the data frame, <code class="literal">dist.items.3</code>, to observe the result.</p><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_050.jpg" alt="Step 3 - training and evaluating model performance"/></div><p>
</p><p>The next step is to perform clustering using the <code class="literal">complete</code> method. The <code class="literal">hclust()</code> function is used. In order to perform cluster analysis on a set of dissimilarities for <span class="emphasis"><em>n</em></span> objects, the <code class="literal">hclust()</code> function is used. At the first stage, each of the objects is assigned to its own cluster. The algorithm then proceeds iteratively at each stage joining the two most similar clusters. This process continues till there is just a single cluster left. The <code class="literal">hclust()</code> function requires us to provide the data in the form of a distance matrix. The <code class="literal">dist1</code> data frame is passed. By default, the complete linkage method is used. There can be multiple agglomeration methods which can be used, some of them could be <code class="literal">ward.D</code>, <code class="literal">ward.D2</code>, <code class="literal">single</code>, <code class="literal">complete</code>, or <code class="literal">average</code>.</p><p>The method used is complete. When the complete method is used, the cluster that is formed has the maximum distance between any object in the cluster and the other object:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; items.complete.link &lt;- hclust(dist.items, method='complete')</strong></span>
<span class="strong"><strong>    &gt; items.complete.link</strong></span>
</pre><p>The call to the <code class="literal">items.complete.link</code> function results in display of the agglomeration method used, the manner in which the distance is calculated, and the number of objects. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_051.jpg" alt="Step 3 - training and evaluating model performance"/></div><p>
</p></div><div class="section" title="Step 4 - plotting the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec43"/>Step 4 - plotting the model</h3></div></div></div><p>The <code class="literal">plot()</code> function is a generic function for the plotting R objects. Here, the <code class="literal">plot()</code> function is used to plot the complete linkage dendrogram.</p><p>Complete linkage is used for hierarchical clustering, and ensures the distance between two clusters is the maximum distance. At each step of the algorithm when using complete linkage, two of the nearest clusters are merged together. The process is iterated until the entire dataset is merged into a single cluster:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(items.complete.link, labels=Mathtest[,1], ylab="Distance")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_052.jpg" alt="Step 4 - plotting the model"/></div><p>
</p><p>Next, we shall be performing single linkage on the dendrogram. In single linkage hierarchical clustering, each step is merged into two clusters based on the smallest distance to other objects, or the smallest minimum pairwise distance between the clusters:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; items.sing.link &lt;- hclust(dist.items, method='single')</strong></span>
<span class="strong"><strong>    &gt; items.sing.link</strong></span>
</pre><p>The call to the <code class="literal">items.sing.link</code> function results in display of the agglomeration method used, the manner in which the distance is calculated, and the number of objects. The results are as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_053.jpg" alt="Step 4 - plotting the model"/></div><p>
</p><p>Here, the <code class="literal">plot()</code> function is used to plot the complete linkage dendrogram. <code class="literal">items.sing.link</code> is passed as a data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(items.sing.link, labels=Mathtest[,1], ylab="Distance")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_054.jpg" alt="Step 4 - plotting the model"/></div><p>
</p></div><div class="section" title="Step 5 - K-medoids clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec44"/>Step 5 - K-medoids clustering</h3></div></div></div><p>Loading the <code class="literal">cluster()</code> library:</p><pre class="programlisting">
<span class="strong"><strong>&gt; library(cluster)</strong></span>
</pre><p>In order to calculate the average silhouette width, we write a function.</p><p>Silhouette refers to a method for interpreting and validating consistency within clusters of data. In order to provide the position of the object in the cluster, the technique uses graphical representation. The silhouette range of is between -1 and 1, with 1 indicating the highest match and -1 indicating the poorest match of the object to its own cluster. In a cluster, if most of the objects have a high value, for instance closer to 1, the clustering configuration is appropriate.</p><pre class="programlisting">
<span class="strong"><strong>&gt; my.k.choices &lt;- 2:8</strong></span>
</pre><p>
<code class="literal">rep()</code> is a generic function that is used to replicate the values of <code class="literal">my.k.choices</code>. The result is stored in the data frame <code class="literal">avg.sil.width</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; avg.sil.width &lt;- rep(0, times=length(my.k.choices))</strong></span>
</pre><p>
<span class="strong"><strong>PAM</strong></span> stands for <span class="strong"><strong>Partitioning Around Medoids</strong></span>. PAM requires that one be aware of the number of clusters desired (like k-means clustering), but it does more computation than k-means in order to ensure that the medoids it finds are truly representative of the observations within a given cluster.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; for (ii in (1:length(my.k.choices)) ){</strong></span>
<span class="strong"><strong>    + avg.sil.width[ii] &lt;- pam(dist.items, k=my.k.choices[ii])$silinfo$avg.width</strong></span>
<span class="strong"><strong>    + }</strong></span>
</pre><p>Printing the value of choices with silhouette values calculated. 
</p><pre class="programlisting">
<span class="strong"><strong>&gt; print( cbind(my.k.choices, avg.sil.width) )</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_055.jpg" alt="Step 5 - K-medoids clustering"/></div><p>
</p><p>Performing clustering on the basis of 2 clusters:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; items.kmed.2 &lt;- pam(dist.items, k=2, diss=T)</strong></span>
<span class="strong"><strong>    &gt; items.kmed.2</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_056.jpg" alt="Step 5 - K-medoids clustering"/></div><p>
</p><p>The <code class="literal">lapply()</code> function returns a list of the same length as X, each element of which is the result of applying <code class="literal">FUN</code> to the corresponding element of X:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; items.2.clust &lt;- lapply(1:2, function(nc) Mathtest[,1][items.kmed.2$clustering==nc]) </strong></span>
<span class="strong"><strong>    &gt; items.2.clust</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_057.jpg" alt="Step 5 - K-medoids clustering"/></div><p>
</p><p>Performing clustering on the basis of 3 clusters.</p><pre class="programlisting">
<span class="strong"><strong>    &gt; items.kmed.3 &lt;- pam(dist.items, k=3, diss=T)</strong></span>
<span class="strong"><strong>    &gt; items.kmed.3</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_058.jpg" alt="Step 5 - K-medoids clustering"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>    &gt; items.3.clust &lt;- lapply(1:3, function(nc) Mathtest[,1][items.kmed.3$clustering==nc])</strong></span>
<span class="strong"><strong>    &gt; items.3.clust</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_059.jpg" alt="Step 5 - K-medoids clustering"/></div><p>
</p></div></div></div>
<div class="section" title="K-means clustering - European countries protein consumption"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>K-means clustering - European countries protein consumption</h1></div></div></div><p>A food consumption pattern is of great interest in the field of medicine and nutrition. Food consumption is correlated to the overall health of an individual, the nutritional value of the food, the economics involved in purchasing a food item, and the environment in which it is consumed. This analysis is concerned with the relationship between meat and other food items in 25 European countries. It is interesting to observe the correlation between meat and other food items. The data includes measures of red meat, white meat, eggs, milk, fish, cereals, starchy foods, nuts (including pulses and oil-seeds), fruits, and vegetables.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec17"/>Getting ready</h2></div></div></div><p>In order to perform K-means clustering, we shall be using a dataset collected on protein consumption for 25 European countries.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec45"/>Step 1 - collecting and describing data</h3></div></div></div><p>The dataset titled <code class="literal">protein</code> which is in the CSV format shall be used. The dataset is in standard format. There are 25 rows of data and are 10 variables.</p><p>The numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">RedMeat</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">WhiteMeat</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Eggs</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Milk</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Fish</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Cereals</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Starch</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Nuts</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Fr&amp;Veg</code></li></ul></div><p>The non-numeric variable is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Country</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec18"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec46"/>Step 2 - exploring data</h3></div></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note8"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10)</p></div></div><p>Let's explore the data and understand relationships among the variables. We'll begin by importing the CSV file named <code class="literal">protein.csv</code>. We will be saving the data to the <code class="literal">protein</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; protein = read.csv("d:/Europenaprotein.csv",header=T)</strong></span>
</pre><p>The <code class="literal">head()</code> returns the first or last parts of a vector, matrix, table, data frame, or function. The <code class="literal">protein</code> data frame is passed to the <code class="literal">head() </code>function.</p><pre class="programlisting">
<span class="strong"><strong>&gt; head(protein)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_060.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec47"/>Step 3 - clustering</h3></div></div></div><p>Start clustering on the basis of three clusters.</p><p>In order to find a random number of clusters at the initial stage, call the <code class="literal">set.seed()</code> function. The <code class="literal">set.seed()</code> function results in the generation of random numbers:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(123456789)</strong></span>
</pre><p>The <code class="literal">kmeans()</code> function shall carry out the K-means clustering on the data matrix. The <code class="literal">protein</code> data matrix is passed as an object that can be coerced to a numeric matrix of data. <code class="literal">centers=3</code> signifies the number of initial (distinct) cluster centers. Since, the number of clusters is denoted by a number, <code class="literal">nstart=10</code> defines the number of random sets to be chosen:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; groupMeat &lt;- kmeans(protein[,c("WhiteMeat","RedMeat")], centers=3, nstart=10)</strong></span>
<span class="strong"><strong>    &gt; groupMeat</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_061.jpg" alt="Step 3 - clustering"/></div><p>
</p><p>Next, the listing of cluster assignments takes place. The <code class="literal">order()</code> function returns a permutation that rearranges its first argument, produced in ascending or descending order. The data frame <code class="literal">groupMeat</code> is passed as a data frame object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; o=order(groupMeat$cluster)</strong></span>
</pre><p>The call to the <code class="literal">data.frame()</code> function results in displaying the countries and the clusters in which they are placed:</p><pre class="programlisting">
<span class="strong"><strong>&gt; data.frame(protein$Country[o],groupMeat$cluster[o])</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_062.jpg" alt="Step 3 - clustering"/></div><p>
</p><p>The <code class="literal">plot()</code> function is a generic function for the plotting R objects. The argument type signifies the type of plot to be drawn. The <code class="literal">xlim </code>argument means arguments should be given the extremes of the range, not a range. <code class="literal">xlab</code> and <code class="literal">ylab</code> provide the title for the <span class="emphasis"><em>x-</em></span>axis and <span class="emphasis"><em>y-</em></span>axis respectively:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; plot(protein$Red, protein$White, type="n", xlim=c(3,19), xlab="Red Meat", ylab="White Meat")</strong></span>
<span class="strong"><strong>    &gt; text(x=protein$Red, y=protein$White, labels=protein$Country,col=groupMeat$cluster+1)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_063.jpg" alt="Step 3 - clustering"/></div><p>
</p></div><div class="section" title="Step 4 - improving the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec48"/>Step 4 - improving the model</h3></div></div></div><p>Next, clustering on all the nine protein groups takes place and seven clusters are created. There is a close significance between the colored scatter-plot for white meat against red meat. Countries in close geographic proximity tend to be clustered into the same group.</p><p>The <code class="literal">set.seed()</code> function results in the generation of random numbers:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(123456789)</strong></span>
</pre><p>
<code class="literal">centers=7</code> signifies the number of initial (distinct) cluster centers:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; groupProtein &lt;- kmeans(protein[,-1], centers=7, nstart=10)</strong></span>
<span class="strong"><strong>    &gt; o=order(groupProtein$cluster)</strong></span>
<span class="strong"><strong>    &gt; data.frame(protein$Country[o],groupProtein$cluster[o])</strong></span>
</pre><p>Seven different clusters are formed. Each of the 25 countries is placed in one of the clusters.</p><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_064.jpg" alt="Step 4 - improving the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>    &gt; library(cluster)</strong></span>
</pre><p>The <code class="literal">clustplot()</code> function creates a bivariate plot that can be visualized as a partition (clustering) of the data. All observations are represented by points in the plot, using principal components. Around each cluster, an ellipse is drawn. The data frame, protein is passed as an object:</p><pre class="programlisting">
<span class="strong"><strong>&gt; clusplot(protein[,-1], groupProtein$cluster, main='2D representation of the Cluster solution', color=TRUE, shade=TRUE, labels=2, lines=0)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_065.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>Another approach is to view it in hierarchical form. The <code class="literal">agnes()</code> function is used. By putting <code class="literal">diss=FALSE</code>, the dissimilarity matrix is used for calculating from raw data. <code class="literal">metric="euclidean</code>" indicates the use of the Euclidean distance measure:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; foodagg=agnes(protein,diss=FALSE,metric="euclidean")</strong></span>
<span class="strong"><strong>    &gt; foodagg</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_066.jpg" alt="Step 4 - improving the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; plot(foodagg, main='Dendrogram')</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_067.jpg" alt="Step 4 - improving the model"/></div><p>
</p><p>The <code class="literal">cutree()</code> function cuts a tree into several groups by specifying either the desired number(s) of groups or the cut height(s):</p><pre class="programlisting">
<span class="strong"><strong>&gt; groups &lt;- cutree(foodagg, k=4)</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_03_068.jpg" alt="Step 4 - improving the model"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt; rect.hclust(foodagg, k=4, border="red")</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_069.jpg" alt="Step 4 - improving the model"/></div><p>
</p></div></div></div>
<div class="section" title="K-means clustering - foodstuff"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>K-means clustering - foodstuff</h1></div></div></div><p>
Nutrients in the food we consume can be classified by the role they play in building body mass. These nutrients can be divided into either macronutrients or essential micronutrients. Some examples of macronutrients are carbohydrates, protein, and fat while some examples of essential micronutrients are vitamins, minerals, and water.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Getting ready</h2></div></div></div><p>Let's get started with the recipe.</p><div class="section" title="Step 1 - collecting and describing data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec49"/>Step 1 - collecting and describing data</h3></div></div></div><p>In order to perform K-means clustering we shall be using a dataset collected on various food items and their respective <code class="literal">Energy</code>, <code class="literal">Protein</code>, <code class="literal">Fat</code>, <code class="literal">Calcium</code>, and <code class="literal">Iron</code> content. The numeric variables are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Energy</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Protein</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Fat</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Calcium</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Iron</code></li></ul></div><p>The non-numeric variable is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Food</code></li></ul></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec20"/>How to do it...</h2></div></div></div><p>Let's get into the details.</p><div class="section" title="Step 2 - exploring data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec50"/>Step 2 - exploring data</h3></div></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note9"/>Note</h3><p>Version info: Code for this page was tested in R version 3.2.3 (2015-12-10).</p></div></div><p>Loading the <code class="literal">cluster()</code> library.</p><pre class="programlisting">
<span class="strong"><strong>&gt; library(cluster)</strong></span>
</pre><p>Let's explore the data and understand relationships among the variables. We'll begin by importing the text file named <code class="literal">foodstuffs.txt</code>. We will be saving the data to the <code class="literal">food.energycontent</code> data frame:</p><pre class="programlisting">
<span class="strong"><strong>&gt; food.energycontent &lt;- read.table("d:/foodstuffs.txt", header=T)</strong></span>
</pre><p> The <code class="literal">head() </code>returns the first or last parts of a vector, matrix, table, data frame, or function. The <code class="literal">food.energycontent</code> data frame is passed to the <code class="literal">head()</code> function:</p><pre class="programlisting">
<span class="strong"><strong>&gt; head(food.energycontent) </strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_070.jpg" alt="Step 2 - exploring data"/></div><p>
</p><p>The <code class="literal">str()</code> function returns the provided information on the structure of the <code class="literal">food.energycontent</code> data frame. It compactly displays the internal structure:</p><pre class="programlisting">
<span class="strong"><strong>&gt; str(food.energycontent)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_071.jpg" alt="Step 2 - exploring data"/></div><p>
</p></div><div class="section" title="Step 3 - transforming data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec51"/>Step 3 - transforming data</h3></div></div></div><p>The <code class="literal">apply()</code> function carries out entry-by-entry changes to data frames and matrices. It returns a vector, array, or list of values obtained by applying a function to margins of an array or matrix. 2 indicates column subscripts the function will be applied over. <code class="literal">sd</code> is for standard deviation function, which is to be applied on the data frame:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; standard.deviation &lt;- apply(food.energycontent[,-1], 2, sd)</strong></span>
<span class="strong"><strong>    &gt; standard.deviation</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_072.jpg" alt="Step 3 - transforming data"/></div><p>
</p><p>The <code class="literal">sweep()</code> function returns an array obtained from an input array by sweeping out a summary statistic. <code class="literal">food.energycontent[,-1]</code> is passed as an array. 2 indicates column subscripts which the function will be applied over. <code class="literal">standard.deviation</code> is the summary statistic which is to be swept out:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; foodergycnt.stddev &lt;- sweep(food.energycontent[,-1],2,standard.deviation,FUN="/") </strong></span>
<span class="strong"><strong>    &gt; foodergycnt.stddev</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_073.jpg" alt="Step 3 - transforming data"/></div><p>
</p></div><div class="section" title="Step 4 - clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec52"/>Step 4 - clustering</h3></div></div></div><p>The <code class="literal">kmeans()</code> function shall carry out K-means clustering on the data matrix. The data matrix <code class="literal">foodergycnt.stddev</code> is passed as an object that can be coerced to the numeric matrix of data. <code class="literal">centers=5</code> signifies the number of initial (distinct) cluster centers. <code class="literal">iter.max=100</code> means the maximum number of iterations allowed. Since the number of clusters is denoted by a number, <code class="literal">nstart=25</code> defines the number of random sets to be chosen:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; food.5cluster &lt;- kmeans(foodergycnt.stddev, centers=5, iter.max=100, nstart=25)</strong></span>
<span class="strong"><strong>    &gt; food.5cluster</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_074.jpg" alt="Step 4 - clustering"/></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>    &gt; food.4cluster &lt;- kmeans(foodergycnt.stddev, centers=4, iter.max=100, nstart=25)</strong></span>
<span class="strong"><strong>    &gt; food.4cluster</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_075.jpg" alt="Step 4 - clustering"/></div><p>
</p><p>Printing the clustering vector for the 4-cluster solution:</p><pre class="programlisting">
<span class="strong"><strong>&gt; food.4cluster$cluster</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_076.jpg" alt="Step 4 - clustering"/></div><p>
</p><p>Next, we shall be printing the clusters for the 4-cluster solution in terms of food labels.</p><p>The <code class="literal">lapply()</code> function returns a list of the same length as X:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; food.4cluster.clust &lt;- lapply(1:4, function(nc) protein[food.4cluster$cluster==nc])</strong></span>
<span class="strong"><strong>    &gt; food.4cluster.clust</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_077.jpg" alt="Step 4 - clustering"/></div><p>
</p></div><div class="section" title="Step 5 - visualizing the clusters"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec53"/>Step 5 - visualizing the clusters</h3></div></div></div><p>Using the <code class="literal">pairs()</code> function, a matrix of scatterplots is produced. <code class="literal">food.energycontent[,-1]</code> provides the coordinates of points given as numeric columns of a matrix or data frame.</p><pre class="programlisting">
<span class="strong"><strong>&gt; pairs(food.energycontent[,-1], panel=function(x,y) text(x,y,food.4cluster$cluster))</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_078.jpg" alt="Step 5 - visualizing the clusters"/></div><p>
</p><p>The <code class="literal">princomp()</code>function performs a principal components analysis on the given numeric data matrix. The function produces an unrotated principal component analysis. <code class="literal">cor=T</code> signifies a logical value indicating that the calculation should use the correlation matrix:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; food.pc &lt;- princomp(food.energycontent[,-1],cor=T)</strong></span>
<span class="strong"><strong>    &gt; my.color.vector &lt;- rep("green", times=nrow(food.energycontent))</strong></span>
<span class="strong"><strong>    &gt; my.color.vector[food.4cluster$cluster==2] &lt;- "blue"</strong></span>
<span class="strong"><strong>    &gt; my.color.vector[food.4cluster$cluster==3] &lt;- "red"</strong></span>
<span class="strong"><strong>    &gt; my.color.vector[food.4cluster$cluster==4] &lt;- "orange"</strong></span>
</pre><p>The <code class="literal">par()</code> function combines multiple plots into one overall graph. <code class="literal">s</code> generates a square plotting region:</p><pre class="programlisting">
<span class="strong"><strong>&gt; par(pty="s")</strong></span>
</pre><p>Plotting the cluster:</p><pre class="programlisting">
<span class="strong"><strong>    &gt; plot(food.pc$scores[,1], food.pc$scores[,2], ylim=range(food.pc$scores[,1]), </strong></span>
<span class="strong"><strong>    + xlab="PC 1", ylab="PC 2", type ='n', lwd=2)</strong></span>
<span class="strong"><strong>    &gt; text(food.pc$scores[,1], food.pc$scores[,2], labels=Food, cex=0.7, lwd=2,</strong></span>
<span class="strong"><strong>    + col=my.color.vector)</strong></span>
</pre><p>The result is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_079.jpg" alt="Step 5 - visualizing the clusters"/></div><p>
</p></div></div></div></body></html>