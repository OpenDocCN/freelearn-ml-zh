<html><head></head><body>
		<div id="_idContainer327">
			<h1 id="_idParaDest-105"><em class="italic"><a id="_idTextAnchor110"/>Chapter 11</em><span class="superscript">: Understanding the Hyperparameters of Popular Algorithms</span></h1>
			<p><span class="superscript">Most </span><strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) algorithms have their own hyperparameters. Knowing how to implement a lot of fancy hyperparameter tuning methods without understanding the hyperparameters of the model is the same as a doctor writing a prescription before diagnosing the patient.</p>
			<p>In this chapter, we’ll learn about the hyperparameters of several popular ML algorithms. There will be a broad explanation for each of the algorithms, including (but not limited to) the definition of each hyperparameter, what will be impacted when the value of each hyperparameter is changed, and the priority list of hyperparameters based on the impact.</p>
			<p>By the end of this chapter, you will understand the important hyperparameters of several popular ML algorithms. Understanding the hyperparameters of ML algorithms is crucial since not all hyperparameters are equally significant when it comes to impacting the model’s performance. We do not have to perform hyperparameter tuning on all of the hyperparameters of a model; we just need to focus on the more critical hyperparameters.</p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li>Exploring Random Forest hyperparameters</li>
				<li>Exploring XGBoost hyperparameters</li>
				<li>Exploring LightGBM hyperparameters</li>
				<li>Exploring CatBoost hyperparameters</li>
				<li>Exploring SVM hyperparameters</li>
				<li>Exploring artificial neural network hyperparameters</li>
			</ul>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor111"/>Exploring Random Forest hyperparameters</h1>
			<p><strong class="bold">Random Forest</strong> is a <a id="_idIndexMarker539"/>tree-based model that is built using a collection of <strong class="bold">decision trees</strong>. It is a <a id="_idIndexMarker540"/>very powerful ensemble ML model that can be utilized for both classification and regression tasks. The way Random Forest utilizes the collection of decision trees is by performing an ensemble method called <strong class="bold">bootstrap aggregation</strong> (<strong class="bold">bagging</strong>) with <a id="_idIndexMarker541"/>some modifications. To understand how each of the Random Forest’s hyperparameters can impact the model’s performance, we need to understand how the model works in the first place.</p>
			<p>Before discussing <a id="_idIndexMarker542"/>how Random Forest ensembles a collection of decision trees, let’s discuss how a decision tree works at a high level. A decision tree can be utilized to perform a classification or regression task by constructing a series of decisions (in the form of rules and splitting points) that can be visualized in the form of a tree. These decisions are made by looking through all of the features and the feature values of the given training data. The goal of a decision tree is to have high homogeneity for each of the leaf nodes. Several methods can be used to measure homogeneity; the two most popular methods for classification tasks are to <a id="_idIndexMarker543"/>calculate the <strong class="bold">Gini</strong> or <strong class="bold">Entropy</strong> values, while the most popular method for regression <a id="_idIndexMarker544"/>tasks is to <a id="_idIndexMarker545"/>calculate the <strong class="bold">Mean Squared Error</strong> value.</p>
			<p>Random Forest utilizes the bagging method to ensemble the collection of decision trees. Bagging is an ensemble method that works by combining predictions from multiple ML models with the hope of generating a more accurate and robust prediction. In this case, Random Forest combines the prediction outputs from several decision trees so that we are not too focused on the prediction from a single tree. This is because a decision tree is very likely to overfit the training data. However, Random Forest does not just utilize the vanilla bagging ensemble method – it also ensures that it only utilizes prediction outputs from the collection of decision trees that are not highly correlated with each other. How is Random Forest able to do that? Instead of asking each decision tree to look through all the features and their values when choosing the splitting points, Random Forest customizes this procedure so that each decision tree only looks at a random sample of features.</p>
			<p>The most popular and well-maintained implementation of Random Forest in Python can be found in the scikit-learn package. It includes implementations for both regression (<strong class="source-inline">RandomForestRegressor</strong>) and classification (<strong class="source-inline">RandomForestClassifier</strong>) tasks. Both implementations have very similar hyperparameters with only a few small differences. The following are the most important hyperparameters, starting with the most important to the least based on the impact on <a id="_idIndexMarker546"/>model performance. Note that this priority list is subjective, based on our experience of developing Random Forest models in the past:</p>
			<ol>
				<li><strong class="source-inline">n_estimators</strong>: This specifies the number of decision trees to be utilized to build the Random Forest. In general, the larger the number of trees, the better the model’s performance will be, with a trade-off of having longer computational time. However, there is a threshold beyond which adding more trees will not have much additional impact on the model’s performance. It could even have a negative impact due to the problem of overfitting.</li>
				<li><strong class="source-inline">max_features</strong>: This specifies the number of randomly sampled features that are used by Random Forest to choose the best splitting point in each of the decision trees. The higher the value, the lower the reduction in variance, and hence the lower the increase in bias. A higher value also leads to having a longer computational time. scikit-learn, by default, will use all of the features for regression tasks and use only <strong class="source-inline">sqrt(n_features)</strong> number of features for classification tasks.</li>
				<li><strong class="source-inline">criterion</strong>: This is used to measure the homogeneity of each decision tree. scikit-learn implemented several methods for both regression and classification tasks. There’s <strong class="source-inline">squared_error</strong>, <strong class="source-inline">absolute_error</strong>, and <strong class="source-inline">poisson</strong> for regression tasks, while there’s <strong class="source-inline">gini</strong>, <strong class="source-inline">entropy</strong>, and <strong class="source-inline">log_loss</strong> for classification tasks. Different methods will have different impacts on model performance; there is no clear rule of thumb for this hyperparameter.</li>
				<li><strong class="source-inline">max_depth</strong>: This specifies the maximum depth of each decision tree. The default value of this hyperparameter is <strong class="source-inline">None</strong>, meaning that the nodes of each tree will keep branching until we have pure leaf nodes or until all the leaves contain less than <strong class="source-inline">min_samples_split</strong> number of samples. The lower the value, the better, since this prevents overfitting. However, a value that is too low can lead to an underfitting problem. One thing is for sure – a higher value implies a longer computational time.</li>
				<li><strong class="source-inline">min_samples_split</strong>: This specifies the minimum number of samples required <a id="_idIndexMarker547"/>for a tree to be able to further split an internal node (a node that can be split into child nodes).  The higher the value, the easier it is to prevent overfitting.</li>
				<li><strong class="source-inline">min_samples_leaf</strong>: This specifies the minimum number of samples required in the leaf nodes. A higher value can help us prevent overfitting.  </li>
			</ol>
			<p class="callout-heading">Random Forest Hyperparameters in scikit-learn</p>
			<p class="callout">For more <a id="_idIndexMarker548"/>information about each of the hyperparameters of the Random Forest implementation in scikit-learn, please visit the official documentation pages at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html</a>. </p>
			<p>Other <a id="_idIndexMarker549"/>useful boilerplate parameters can be found across different scikit-learn estimator implementations. The following are several <a id="_idIndexMarker550"/>important parameters that you need to be aware of that can help you while training a scikit-learn estimator:</p>
			<ol>
				<li value="1"><strong class="source-inline">class_weight</strong>: This specifies the weights for each class that exists in the training data. This is only available for classification tasks. This parameter is very important when you face an imbalanced class problem. We need to give higher weights to classes that have fewer samples.</li>
				<li><strong class="source-inline">n_jobs</strong>: This specifies the number of parallel processes to be utilized when training the estimator. scikit-learn utilizes the <strong class="source-inline">joblib</strong> package in the backend.</li>
				<li><strong class="source-inline">random_state</strong>: This specifies the random seed number to ensure the code is reproducible.</li>
				<li><strong class="source-inline">verbose</strong>: This parameter <a id="_idIndexMarker551"/>is used to control any logging activities. Setting <strong class="source-inline">verbose</strong> to an integer greater than zero enables us to see what happens when training an estimator. </li>
			</ol>
			<p>In this section, we <a id="_idIndexMarker552"/>learned how Random Forest works at a high level and looked at several important hyperparameters, along with an explanation of how they impact the model’s performance. We also looked at the main hyperparameters. Furthermore, we learned about several useful parameters in scikit-learn that can ease the training process. In the next section, we will discuss the XGBoost algorithm.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor112"/>Exploring XGBoost hyperparameters</h1>
			<p><strong class="bold">Extreme Gradient Boosting</strong> (<strong class="bold">XGBoost</strong>) is also a tree-based model that is built using a <a id="_idIndexMarker553"/>collection of decision trees, similar to a Random Forest. It can also be utilized for both classification and regression tasks. The difference <a id="_idIndexMarker554"/>between XGBoost and Random Forest is in how they perform the ensemble. Unlike Random Forest, which uses the bagging <a id="_idIndexMarker555"/>ensemble method, XGBoost utilizes another ensemble method called <strong class="bold">boosting</strong>. </p>
			<p>Boosting is an ensemble algorithm whose goal is to achieve higher performance through a sequence of individually weak models by overcoming the weaknesses of the predecessor models (see <em class="italic">Figure 11.1</em>). It is not a specific model; it’s just a generic ensemble algorithm. The definition of weakness may vary across different types of boosting ensemble implementation. In XGBoost, it is defined based on the error of the gradient from the previous decision tree model. Take a look at the following diagram:</p>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<img src="image/B18753_11_001.jpg" alt="Figure 11.1 – Boosting ensemble algorithm&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Boosting ensemble algorithm</p>
			<p>XGBoost is a very popular and well-adopted ML model that is built using the boosting ensemble algorithm and a collection of decision trees. Each of the decision trees is added one at a time and is fitted to the prediction errors from the previous tree to correct those errors. It is worth noting that since XGBoost is part of the gradient boosting algorithm, all of the weak models (decision trees) need to be fitted using a differentiable loss function and the gradient descent optimization method.</p>
			<p>XGBoost has <a id="_idIndexMarker556"/>its own package and can be utilized not only in Python but also in other programming languages, such as R and JVM. In Python, you can install XGBoost via <strong class="source-inline">pip install xgboost</strong>. This package also implements the scikit-learn wrappers for both regression (<strong class="source-inline">XGBRegressor</strong>) and classification (<strong class="source-inline">XGBClassifier</strong>) tasks. Numerous hyperparameters are provided by the package, but not all of them are very important in affecting the model’s performance. The following are the most important hyperparameters, starting with the most important to the least based on their impact on model performance:</p>
			<ol>
				<li value="1"><strong class="source-inline">n_estimators</strong>: This specifies the number of decision trees to be utilized to build the XGBoost model. It can also be interpreted as the number of boosting rounds, which is similar to the concept of epochs in a neural network. In general, the higher the value, the better the model’s performance will be, with the trade-off of having a longer computation time. However, we need to be careful with a value that’s too high since it can lead us to the overfitting problem.</li>
				<li><strong class="source-inline">learning_rate</strong>: This is the learning rate of the gradient descent optimization algorithm. The lower the value, the higher the chances of the model finding the optimum solution, with a trade-off of having a longer computational time. You can increase the value of this hyperparameter if there no sign of overfitting is found on the last iterations of training; you can decrease it if there is overfitting.</li>
				<li><strong class="source-inline">max_depth</strong>: This is the maximum depth of each decision tree. A lower value can help us prevent overfitting. However, a too-low value can lead to an underfitting problem. One thing is for sure – a higher value leads to a longer computational time.</li>
				<li><strong class="source-inline">min_child_weight</strong>: This is the minimum sum of instance weight, calculated using the Hessian, that’s needed in a child. This hyperparameter acts as a regularizer to <a id="_idIndexMarker557"/>ensure that each tree will stop trying to split the node once a certain degree of purity is reached. In other words, it is a regularization parameter that works by limiting the depth of the tree so that the overfitting problem can be prevented. A higher value can help us prevent overfitting. However, a too-high value can lead to an underfitting problem.</li>
				<li><strong class="source-inline">gamma</strong>: This is a pseudo-regularization parameter that is calculated based on a reduction in the loss value. The value of this hyperparameter specifies the minimum loss reduction required to make a further partition on a leaf node of the tree. You can put a high value on this hyperparameter to prevent the overfitting problem. However, please be careful and don’t use a value that’s too high; it can lead to an underfitting problem.</li>
				<li><strong class="source-inline">colsample_bytree</strong>: This is the fraction version of the <strong class="source-inline">max_features</strong> hyperparameter in the scikit-learn implementation of Random Forest. This hyperparameter is responsible for telling XGBoost how many randomly sampled features are needed to choose the best splitting point in each of the decision trees. A low value can help us prevent overfitting and lowers the computational time. However, a value that’s too low can lead to an underfitting problem.</li>
				<li><strong class="source-inline">subsample</strong>: This is the observation’s version of the <strong class="source-inline">colsample_bytree</strong> hyperparameter. This hyperparameter is responsible for telling XGBoost how many training samples need to be used while training each tree. This hyperparameter <a id="_idIndexMarker558"/>can be useful to prevent the overfitting problem. However, it can also lead us to an underfitting problem if we use a value that’s too low.</li>
			</ol>
			<p class="callout-heading">Complete List of XGBoost Hyperparameters</p>
			<p class="callout">For more <a id="_idIndexMarker559"/>information about other XGBoost’s hyperparameters, please visit the official documentation page: https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn. </p>
			<p>In this section, we discussed how XGBoost works at a high level and looked at several important hyperparameters, along with an explanation of how they impact model performance. We also looked at the main hyperparameters. In the next section, we will discuss the LightGBM algorithm.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor113"/>Exploring LightGBM hyperparameters</h1>
			<p><strong class="bold">Light Gradient Boosting Machine</strong> (<strong class="bold">LightGBM</strong>) is also a boosting algorithm built on top <a id="_idIndexMarker560"/>of a collection of decision trees, similar <a id="_idIndexMarker561"/>to XGBoost. It can also be utilized both for classification and regression tasks. However, it differs from XGBoost in the way the trees are grown. In LightGBM, trees are grown in a leaf-wise manner, while XGBoost grows trees in a level-wise manner (see <em class="italic">Figure 11.2</em>). By leaf-wise, we mean that LightGBM grows trees by prioritizing nodes whose split leads to the highest increase of homogeneity:</p>
			<div>
				<div id="_idContainer324" class="IMG---Figure">
					<img src="image/B18753_11_002.jpg" alt="Figure 11.2 – Level-wise versus leaf-wise tree growth&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Level-wise versus leaf-wise tree growth</p>
			<p>Besides the difference in how XGBoost and LightGBM grow the trees, they also have different ways of handling categorical features. In XGBoost, we need to encode the categorical features before passing them to the model. This is usually done using the one-hot <a id="_idIndexMarker562"/>encoding or integer encoding methods. In LightGBM, we can just tell which features are categorical and it will handle those features automatically by performing equality splitting. There are several other differences between XGBoost and LightGBM in terms of the way they perform optimization in distributed learning. In general, LightGBM has a much faster computation time compared to XGBoost.</p>
			<p>Similar to XGBoost, LightGBM also has its own package and can be utilized not only in Python but also in the R language. In Python, you can install LightGBM via <strong class="source-inline">pip install lightgbm</strong>. This package also implements the scikit-learn wrappers for both regression (<strong class="source-inline">LGBMRegressor</strong>) and classification (<strong class="source-inline">LGBMClassifier</strong>) tasks. The following are the most important hyperparameters for LightGBM, starting with the most important to the least based on the impact on model performance:</p>
			<ol>
				<li value="1"><strong class="source-inline">max_depth</strong>: This specifies the maximum depth of each decision tree. A lower value can help us prevent overfitting. However, a value that’s too low can lead to an underfitting problem. One thing is for sure – a higher value implies a longer computational time.</li>
				<li><strong class="source-inline">num_leaves</strong>: This specifies the maximum number of leaves in each tree. It should have a value lower than two to the power of <strong class="source-inline">max_depth</strong> since a leaf-wise tree is much deeper than a depth-wise tree for a set number of leaves. In general, the higher the value, the better the model’s performance will be, with a trade-off of having a longer computational time. However, there is a threshold where the impact of adding more leaves will not have much additional impact on the model’s performance or even have a negative impact due to overfitting.</li>
				<li><strong class="source-inline">Learning_rate</strong>: This specifies the learning rate of the gradient descent optimization algorithm. The lower the value, the higher the chances of the model finding a more optimum solution, with a trade-off of having a longer computational time. You can increase the value of this hyperparameter if no sign of overfitting is found on the last iterations of training and vice versa.</li>
				<li><strong class="source-inline">min_child_samples</strong>: This specifies the minimum number of samples required in <a id="_idIndexMarker563"/>the leaf nodes. A higher value can help us prevent overfitting. However, a value that’s too high can lead to an underfitting problem.</li>
				<li><strong class="source-inline">Feature_fraction</strong>: This is similar to <strong class="source-inline">colsample_bytree</strong> in XGBoost. This hyperparameter tells LightGBM how many randomly sampled features need to be used to choose the best splitting point in each of the decision trees. This hyperparameter can be useful for preventing overfitting. However, it can also lead to an underfitting problem if we use a value that is too low.</li>
				<li><strong class="source-inline">bagging_fraction</strong>: This is the observation’s version of the <strong class="source-inline">feature_fraction</strong> hyperparameter. This hyperparameter is responsible for telling LightGBM how many training samples need to be used during the training of each tree. Lower values can help us prevent overfitting and lower the computational time. However, a value that is too low can lead to an underfitting problem.</li>
			</ol>
			<p class="callout-heading">Complete List of LightGBM Hyperparameters</p>
			<p class="callout">For more <a id="_idIndexMarker564"/>information about other LightGBM hyperparameters, please visit the official documentation page: <a href="https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api">https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api</a>.</p>
			<p>In this section, we discussed how LightGBM works at a high level and looked at several important <a id="_idIndexMarker565"/>hyperparameters, along with an explanation of how they impact model performance. We also looked at the main hyperparameters. In the next section, we will discuss the CatBoost algorithm.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor114"/>Exploring CatBoost hyperparameters</h1>
			<p><strong class="bold">Categorical Boosting</strong> (<strong class="bold">CatBoost</strong>) is another boosting algorithm built on top of a collection <a id="_idIndexMarker566"/>of decision trees, similar to XGBoost and LightGBM. It can also be utilized both for classification and regression tasks. The main difference <a id="_idIndexMarker567"/>between CatBoost and XGBoost or LightGBM is how it grows the trees. In XGBoost and LightGBM, trees are grown asymmetrically, while in CatBoost, trees are grown symmetrically so that all of the trees are balanced. This balanced tree characteristic provides several benefits, including the ability to control overfitting problems, lower inference time, and efficient implementation in CPUs. CatBoost does this by using the same condition in every split in the nodes, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer325" class="IMG---Figure">
					<img src="image/B18753_11_003.jpg" alt="Figure 11.3 – Asymmetric versus symmetric tree&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Asymmetric versus symmetric tree</p>
			<p>The main selling point of CatBoost is its ability to handle numerous types of features automatically, including numerical, categorical, and text, especially for categorical features. We just need to tell CatBoost which features are categorical features via the <strong class="source-inline">cat_features</strong> parameter and it will handle those features automatically. By default, CatBoost will perform one-hot encoding for categorical features that only have two classes. For higher <a id="_idIndexMarker568"/>cardinality features, it will perform target encoding and combine several categorical features or even categorical and numerical features. For more information on how CatBoost handles categorical <a id="_idIndexMarker569"/>features, please refer to the official documentation page: <a href="https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic">https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic</a>.</p>
			<p>Similar to XGBoost and LightGBM, CatBoost also has its own package and can be utilized not only in Python but also in the R language. In Python, you can install CatBoost via <strong class="source-inline">pip install catboost</strong>. You can utilize the implemented scikit-learn-friendly classes for both regression (<strong class="source-inline">CatBoostRegressor</strong>) and classification (<strong class="source-inline">CatBoostClassifier</strong>) tasks. The following is a list of CatBoost’s most important hyperparameters, sorted in descending order based on the importance of each hyperparameter regarding model performance:</p>
			<ol>
				<li value="1"><strong class="source-inline">iterations</strong>: This specifies the number of decision trees to be utilized to build the CatBoost model. It can also be interpreted as the number of boosting rounds, similar to the concept of epochs in a neural network. In general, the higher the value, the better the model’s performance will be, with a trade-off of having a longer computational time. However, there is a threshold where the impact of adding more trees will not have much additional impact on the model’s performance or even have a negative impact due to overfitting.</li>
				<li><strong class="source-inline">depth</strong>: This specifies the maximum depth of each decision tree. A lower value can help us prevent overfitting. However, a value that’s too low can lead to an underfitting problem. One thing is for sure – a higher value implies a longer computational time.</li>
				<li><strong class="source-inline">learning_rate</strong>: This specifies the learning rate of the gradient descent optimization algorithm. The lower the value, the higher the chances of the model finding a more optimum solution, with a trade-off of having a longer computational time. You can increase the value of this hyperparameter if no sign of overfitting is found on the last iterations of training and vice versa.</li>
				<li><strong class="source-inline">l2_leaf_reg</strong>: This is the regularization parameter on the cost function. This hyperparameter can prevent the overfitting problem. However, it can also lead to an underfitting problem if we use a value that’s too high.</li>
				<li><strong class="source-inline">one_hot_max_size</strong>: This is the threshold that tells CatBoost when to perform one-hot encoding on the categorical features. Any categorical features that have <a id="_idIndexMarker570"/>cardinality lower than or equal to the given value will be transformed into numerical values via the one-hot encoding method.</li>
			</ol>
			<p class="callout-heading">Complete List of CatBoost Hyperparameters</p>
			<p class="callout">For more <a id="_idIndexMarker571"/>information about other CatBoost hyperparameters, please visit the official documentation page (<a href="https://catboost.ai/en/docs/concepts/parameter-tuning">https://catboost.ai/en/docs/concepts/parameter-tuning</a>).</p>
			<p>In this section, we discussed how CatBoost works at a high level and looked at several important hyperparameters, along with an explanation of how they impact model performance. We also looked at the main hyperparameters. In the next section, we will discuss the SVM algorithm.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor115"/>Exploring SVM hyperparameters</h1>
			<p><strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>) is an ML model that utilizes lines or hyperplanes, along with <a id="_idIndexMarker572"/>some linear algebra transformations, to perform a classification or regression task. All the algorithms discussed in the previous sections <a id="_idIndexMarker573"/>can be classified as tree-based algorithms, while SVM is not part of the <em class="italic">tree-based</em> group of ML algorithms. It is part of the <em class="italic">distance-based</em> group of algorithms. We usually called the linear algebra transformation in SVM a <strong class="bold">kernel</strong>. This is <a id="_idIndexMarker574"/>responsible for transforming any problem into a linear problem. </p>
			<p>The most popular and well-maintained implementation of SVM in Python can be found in the scikit-learn package. It includes implementations for both regression (<strong class="source-inline">SVR</strong>) and classification (<strong class="source-inline">SVC</strong>) tasks. Both of them have very similar hyperparameters with only a few small differences. The following are the most important hyperparameters for SVM, starting with the <a id="_idIndexMarker575"/>most important to the least based on their impact on model performance:</p>
			<ol>
				<li value="1"><strong class="source-inline">kernel</strong>: This is the linear algebra transformation, whose goal is to convert the given problem into a linear problem. There are five kernels that we can choose from, including linear (<strong class="source-inline">linear</strong>), polynomial (<strong class="source-inline">poly</strong>), radial basis function (<strong class="source-inline">rbf</strong>), and sigmoid (<strong class="source-inline">sigmoid</strong>) kernels. Different kernels will have different impacts on model performance and there is no clear rule of thumb for this hyperparameter.</li>
				<li><strong class="source-inline">C</strong>: This is the regularization parameter that controls overfitting. The lower the value, the stronger the impact that regularization will have on the model, and hence a higher chance of preventing overfitting. </li>
				<li><strong class="source-inline">degree</strong>: This hyperparameter is specific to the polynomial kernel function. The value of this hyperparameter corresponds to the degree of the polynomial function that’s used by the model.</li>
				<li><strong class="source-inline">gamma</strong>: This is the coefficient for the radial basis, polynomial, and sigmoid kernel functions. There are two options that scikit-learn provides, namely <strong class="source-inline">scale</strong> and <strong class="source-inline">auto</strong>. </li>
			</ol>
			<p class="callout-heading">SVM Hyperparameters in scikit-learn</p>
			<p class="callout">For more <a id="_idIndexMarker576"/>information about how each of the hyperparameters in SVM are implemented in scikit-learn, you can visit the official documentation pages at https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html</a>. </p>
			<p>In this section, we discussed how SVM works at a high level and looked at several important hyperparameters, along with an explanation of how they impact model performance. We also looked at the main hyperparameters. In the next section, we will discuss artificial neural networks.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor116"/>Exploring artificial neural network hyperparameters</h1>
			<p>An <strong class="bold">artificial neural network</strong>, also known <a id="_idIndexMarker577"/>as <strong class="bold">deep learning</strong>, is a kind of ML algorithm that mimics <a id="_idIndexMarker578"/>how human brains work. Deep learning can be utilized for both regression and classification tasks. One of the <a id="_idIndexMarker579"/>main selling points of this model is its ability to perform feature engineering and selection automatically from the raw data. In general, to ensure this algorithm works decently, we need a large amount of training data to be fed to the model. The simplest form of a neural network is called a <strong class="bold">perceptron</strong> (see <em class="italic">Figure 11.4</em>). A <a id="_idIndexMarker580"/>perceptron is just a linear combination that is applied on top of all of the features, with bias added at the end of the calculation:</p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<img src="image/B18753_11_004.jpg" alt="Figure 11.4 – Perceptron&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Perceptron</p>
			<p>If the output from the perceptron is passed to a non-linear function, which is usually called an <strong class="bold">activation function</strong>, and <a id="_idIndexMarker581"/>then passed to another perceptron, then <a id="_idIndexMarker582"/>we can call this a <strong class="bold">multi-layer perceptron</strong> (<strong class="bold">MLP</strong>) with one layer. The training process for a neural network consists of two big <a id="_idIndexMarker583"/>procedures, namely <strong class="bold">forward propagation</strong> and <strong class="bold">backward propagation</strong>. In forward <a id="_idIndexMarker584"/>propagation, we just let the neural network perform calculations on top of the given inputs based on the defined architecture. In backward propagation, the model will update the weights and bias parameters based on the defined loss function using a gradient-based optimization procedure.</p>
			<p>There are <a id="_idIndexMarker585"/>other variants of neural networks <a id="_idIndexMarker586"/>besides MLP, such as <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>), <strong class="bold">long short-term memory networks</strong> (<strong class="bold">LSTMs</strong>), <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>), and <strong class="bold">transformers</strong>. CNN is usually adopted <a id="_idIndexMarker587"/>when we work with image data, but we can also <a id="_idIndexMarker588"/>use a one-dimensional CNN when working with text data. RNNs and LSTMs are usually adopted when working with time series or natural language data. Transformers are mainly used for text-related projects, but recently, they have been adopted for image and voice data.</p>
			<p>Several <a id="_idIndexMarker589"/>packages provide implementations of neural networks in Python, including PyTorch, TensorFlow, and Scikit Learn. The following are the most important hyperparameters, sorted in descending order based on the importance of each hyperparameter regarding model performance. Note that this priority list is subjective based on our experience of developing Random Forest models in the past. Since the naming of the hyperparameters may differ across different packages, we will only use the general names of the hyperparameters:</p>
			<ol>
				<li value="1"><strong class="bold">Optimizer</strong>: This is <a id="_idIndexMarker590"/>the gradient-based optimization algorithm to be used. There are several optimizers for us to choose from. However, perhaps the most popular and widely adopted optimizer is <strong class="bold">Adam</strong>. There are other options, including (but not limited to) SGD and RMSProp. Different optimizers may have different impacts on model performance and there is no clear rule of thumb for choosing which one is the best. It is worth noting that each optimizer has its own hyperparameter as well.</li>
				<li><strong class="bold">Learning Rate</strong>: This hyperparameter controls how big the step will be for the optimizer to “learn” from the given training data during the optimization process. It is <a id="_idIndexMarker591"/>important to choose the best range of learning rates first before tuning other hyperparameters. The lower the value, the higher the chances of the model finding a more optimum solution, with a trade-off of having a longer computational time. You can increase the value of this hyperparameter if no sign of overfitting is found on the last iterations of training and vice versa.</li>
				<li><strong class="bold">Batch Size</strong>: This specifies the number of training samples that will be passed to the neural <a id="_idIndexMarker592"/>network within each training step. In general, the higher the value, the better the model’s performance will be. However, a batch size that’s too high will usually be constrained by the device’s memory.</li>
				<li><strong class="bold">Epochs</strong>: This is the <a id="_idIndexMarker593"/>number of training iterations. Similar to <strong class="source-inline">n_estimators</strong> in XGBoost and <strong class="source-inline">iterations</strong> in CatBoost, a high value can lead to better model performance, with a trade-off of having a longer computational time. However, we need to be careful when using a value that’s too high since it can lead to overfitting.</li>
				<li><strong class="bold">Number of Layers</strong>: The higher <a id="_idIndexMarker594"/>the value, the higher the complexity of the model, hence the higher the chance of overfitting. Usually, one or two layers is more than enough to build a good model.</li>
				<li><strong class="bold">Number of Nodes</strong>: The number <a id="_idIndexMarker595"/>of units or nodes within each of the layers. The higher the value, the higher the complexity of the model, hence a higher chance of overfitting.</li>
				<li><strong class="bold">Activation Function</strong>: The <a id="_idIndexMarker596"/>non-linear transformation <a id="_idIndexMarker597"/>function. There <a id="_idIndexMarker598"/>are many <a id="_idIndexMarker599"/>activation functions <a id="_idIndexMarker600"/>to choose <a id="_idIndexMarker601"/>from. Some of the most well-adopted activation functions in practice are <strong class="bold">Rectified Linear Activation Function</strong> (<strong class="bold">ReLU</strong>), <strong class="bold">Exponential Linear Unit</strong> (<strong class="bold">ELU</strong>), <strong class="bold">Sigmoid</strong>, <strong class="bold">Softmax</strong>, and <strong class="bold">Tanh</strong>. </li>
				<li><strong class="bold">Dropout Rate</strong>: The rate <a id="_idIndexMarker602"/>for the dropout layer. The dropout layer is a special layer in a neural network that acts as a regularizer by randomly setting the unit value to zero. This hyperparameter controls how many units are set to zero. A higher value can help us prevent overfitting. However, a value that’s too high can lead to an underfitting problem.</li>
				<li><strong class="bold">L1/L2 Regularization</strong>: These <a id="_idIndexMarker603"/>are the regularization parameters <a id="_idIndexMarker604"/>that are applied to the loss function. This hyperparameter can help prevent overfitting. However, it can also lead to an underfitting problem if its value is too high.</li>
			</ol>
			<p>In this section, we have discussed how neural network works at a high level, the variants of neural networks, and looked at several important hyperparameters, along with an explanation of how they impact model performance. We also looked at the main hyperparameters. Now, let’s summarize this chapter.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor117"/>Summary</h1>
			<p>In this chapter, we discussed how several popular algorithms work at a high level, explained their important hyperparameters and how they impact performance, and provided priority lists of the hyperparameters, sorted in descending order based on their impact on performance. At this point, you should be able to design your hyperparameter tuning experiments more effectively by focusing on the most important hyperparameters. You should also understand what impact each of the important hyperparameters has on the performance of the model. </p>
			<p>In the next chapter, we’ll summarize the hyperparameter tuning methods we’ve discussed here into a simple decision map that can help you choose which method is the most suitable for your problem. Furthermore, we will cover several study cases that show how to utilize this decision map in practice.</p>
		</div>
		<div>
			<div id="_idContainer328">
			</div>
		</div>
	</body></html>