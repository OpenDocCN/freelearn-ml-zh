- en: An Introduction to Create ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The intention of this book has been to explore ways to apply machine learning
    on the iPhone, specifically focusing on computer vision tasks. Even with this
    narrow focus, we have only scratched the surface of what is currently possible.
    But, hopefully, we've covered enough to spark your curiosity and provided enough
    intuition behind the details of machine learning models to help you on your journey
    to build intelligent apps.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is intended as a primer into continuing that journey by introducing
    **Create ML**, a tool released with Core ML 2 that provides an easy way to create
    some common models using custom data. Even though we only provide a high-level
    introduction, specifically around computer vision, it still should be enough to
    help you make use of it in your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have:'
  prefs: []
  type: TYPE_NORMAL
- en: Revised the machine learning workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appreciated the importance of splitting your data into sets for training and
    validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used Create ML to create a custom image classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seen other tools and frameworks to continue your journey
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin by reviewing a typical machine learning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: A typical workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with any project, you enter the process with some understanding of what you
    are trying to build. The better you understand this (the problem), the better
    you are able to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding what it is that you're trying to do, your next question
    (in the context of building a machine learning model) is *what data do I need?*
    This includes an exploration into what data is available and what data you may
    need to generate yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve understood what you''re trying to do and what data you need, your
    next question/task is to decide on what algorithm (or model) is needed. This is
    obviously dependent on your task and the data you have; in some instances, you
    may be required to create your own model, but more often than not, there will
    be an adequate model available for you to use, or at least an architecture you
    can use with your own data. The following table shows some typical computer vision
    tasks and their related machine learning counterparts:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Machine learning algorithm** |'
  prefs: []
  type: TYPE_TB
- en: '| Label images | Image classification |'
  prefs: []
  type: TYPE_TB
- en: '| Recognize multiple objects and their location | Object detection and semantic
    segmentation  |'
  prefs: []
  type: TYPE_TB
- en: '| Find similar images  | Image similarity  |'
  prefs: []
  type: TYPE_TB
- en: '| Creating stylized images  | Style transfer |'
  prefs: []
  type: TYPE_TB
- en: The next step is to train your model; typically, this is an iterative process
    with a lot of fine-tuning until you have a model that sufficiently achieves its
    task on data it hadn't been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, with a trained model, you can deploy and use your model in your application.
    This process is summarized in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e953380d-6b7e-4500-b2b6-46326d593c02.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram is an oversimplification of the process; typically the
    workflow is more cyclic, with multiple iterations between training and selecting
    and tuning your model. It is also common to run multiple models (and model parameters)
    concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: To make the concepts of this chapter more concrete, let's work with the hypothetical
    brief of having to build a fun application to assist toddlers to learn the names
    of fruits. You and your team have come up with the concept of a game that asks
    the toddler to find a specific fruit. The toddler earns points when they correctly
    identify the fruit using the device's camera. With our task now defined, let's
    discuss what data we need.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our task, we require a collection of labeled photos of fruits. As you may
    recall from [Chapter 1](7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml), *Introduction
    to Machine Learning*, this type of machine learning problem is known as **supervised
    learning**. We need our model to take in an image and return the label of what
    it thinks the image is, also known as **multi-class classification**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and collect photos of fruits. Create ML allows for multiple ways of
    organizing your data, but I find that ad hoc collection is easiest done by organizing
    it in folders, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6438e297-85ec-4c59-972b-01cbb68c4a05.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: http://www.image-net.org/
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have organized our data into folders, where the folder name is used
    as a label for its contents. An alternative is labeling each image, where each
    instance of a specific class has a suffix number, for example `banana.0.jpg`,
    `banana.1.jpg`, and so on. Or you can simply pass in a dictionary of labels with
    their associated list of image URLs.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, you may be wondering how many images you should get. Apple has
    suggested a minimum of 10 images per class, butyou typically want to collect as
    many as possible, to help the model generalize by ensuring that it sees a lot
    of variations during training. It's also important to, wherever possible, obtain
    images that are as close as possible to the real data the model will be used on
    (in the realworld). This is because the model is not biased according to what
    it learns. It just learns what it needs to. That is, if all your apple examples
    were of red apples with a white background, then it's likely that your model will
    learn to associate these colors with apples, and any time it sees these colors,
    it will predict that the image contains an apple.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, Apple has suggested a minimum of 10 images; this should
    have somewhat surprised you. Typically, when you talk about training deep neural
    networks, you expect the dataset to be large, very large. For example, a standard
    dataset used for training image classifiers is ImageNet. This dataset consists
    of over 14 million images; and this is part of the secret. As we've discussed
    throughout this book, layers of a CNN learn how to extract meaningful features
    from images, which they then use to infer an image's class. A common practice
    for specialized classifiers, like our fruit classifier, is to borrow these learnings
    from a model that has trained on millions of images and use the features it extracts
    to train a classifier on our smaller dataset—a technique known as **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two diagrams provide an illustrative example of this, with the
    first showing a network that has been trained on a large dataset and the second
    using what it has learnt to train on a more specialized dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7895715-bc54-4eb0-9088-3f8d269a4548.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are interested in the feature vectors that the convolutional layers learn;
    you can think of this as an encoding of its understanding of the input image.
    This is what we want to use to train our own classifier, shown in the following
    diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9486a355-fcc1-4043-ae53-a26ba3451891.png)'
  prefs: []
  type: TYPE_IMG
- en: With this approach, we forgo having to learn how to extract features and are
    left with just having to train the weights of a fully connected network for classification,
    taking advantage of the previous network's ability to extract meaningful features.
    Create ML uses this technique for its image classifiers. Using a pre-trained model
    that resides on the device and has been trained over 1,000 categories means that
    we are left just having to train a relatively small network for classification.
    This is done using the features provided by the pre-trained network. This not
    only allows us to learn from a smaller dataset but also reduces the amount of
    time required for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another feature Create ML offers, and performs on our behalf, to train effectively
    on small datasets is something called data augmentation.Data augmentation is simply
    a way of increasing the variance of our dataset by applying a number of random
    transformations to each image before the image is passed into the network during
    training, for example, horizontally flipping an image. The goal is that at training
    time, your model will see many variations of an image so as to improve your model''s
    ability to generalize, that is, learn meaningful features that work on data it
    hasn''t seen before. The following figure illustrates some of the transformations
    typically performed for data augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ea43127-3807-4955-8920-6c37ad861901.png)'
  prefs: []
  type: TYPE_IMG
- en: Another convenience offered by Create ML out of the box is that it handles the
    typical preprocessing tasks required when working with images, such as cropping
    and resizing. They typically have fixed-size inputs and outputs, requiring you
    to either explicitly preprocess the images to match the model or use the Vision
    framework to handle this for you. An extra consequence of Create ML being built
    on top of Vision is that it handles a lot of the pipeline you would typically
    need to do manually when training models.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is just one more important topic I would like to highlight before moving
    on to creating and training our model; this has to do with balanced datasets,
    or the effects of imbalanced datasets. Balanced datasets refer to having an equal
    amount of examples for each class; that is, you avoid having a large variance
    between the number of examples you have in each of your classes. Why is this important?
    To answer this, let''s remind ourselves of how a model is trained and what it
    learns. The following figure illustrates the process of training, where training
    is an iterative process of performing inference (forward pass) for a given input.
    Then, small adjustments are made to the weights of the model so that they reduce
    any discrepancies between the prediction and expected value (loss):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95712ee3-0d50-4c57-88a0-d8298e4b1a79.png)'
  prefs: []
  type: TYPE_IMG
- en: Put another way, overexposing a class will dominate this process of adjusting
    weights such that the weights will better fit their own class over others. This
    is especially true when training with batches, as the error is typically the average
    over all samples in the batch. So, if your model can effectively predict the dominant
    class, it's likely to achieve a reasonable loss and be unable to learn anything
    useful for the other classes.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we know what we are trying to achieve, have our balanced training
    set, and know what machine learning task we need; we are now ready to build and
    train our model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and training a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks to the great effort by Apple's engineers, the process of creating common
    machine learning models is incredibly easy and will no doubt spark a new wave
    of intelligent apps over the coming months.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will see just how easy it is as we walk through creating
    an image classifier for our application using Create ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create ML is accessible using Xcode Playground, so there is a good place to
    start. Open up Xcode and create a new Playground, ensuring that you select macOS
    as the platform, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0fa97a3-d516-4921-93ad-e7a2ce12dd30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once in the playground, import `CreateML` and `Foundation` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a `URL` that points to the directory that contains your training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing left to do is to create an instance of our model, passing in
    the path to our training data (I did say it was incredibly easy):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Create ML offers you the flexibility of providing a custom dictionary of labels
    and their associated files or through the convenience of a `MLImageClassifier.DataSource`.
    This can either be a hierarchical directory structure where classes are organized
    into their respective folders, `MLImageClassifier.DataSource.labeledDirectories` (as
    we have done in this example), or one where each file has been named with respect
    to their associated class, `MLImageClassifier.DataSource.labeledFiles`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as the model is instantiated, it will begin training. Once finished,
    it will output the accuracy achieved on your training set to the console, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f7c863b-0f7e-4108-8e93-96b0fb99fce8.png)'
  prefs: []
  type: TYPE_IMG
- en: We are almost done; this tells us that our model has fit our training data well,
    but it doesn't tell us how well it will generalize, that is, how well it will
    work on images it hasn't seen before. It's possible (and common) for deep neural
    networks to remember their training data, commonly referred to as overfitting.
    To avoid overfitting, and therefore make it more likely to produce something usable
    in the real world, it's a common practice to split your data into three buckets.
    The first bucket is used to train your model. The second bucket, called validation
    data, is used during training (typically at the end of each iteration/epoch) to
    see how well the model is generalizing. It also provides clues as to when the
    model starts overfitting (when the training accuracy and validation accuracy begin
    to diverge). The last bucket is only used once you are satisfied with how your
    model performs on the validation data and is the determinant of how well your
    model actually works; this bucket is known as the test data.
  prefs: []
  type: TYPE_NORMAL
- en: How much data do you reserve for validation and testing? For shallow learners,
    it was common to have a 70/20/10 (training, validation, and test) split. But deep
    learning normally implies big datasets, in which case the reserved data for validation
    and test may be excessive. So the answer really depends on how much data you have
    and what type of data it is.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, before deploying our model, we evaluate it on a dataset it hasn't
    seen during training. Once again, collect an equal amount of data for each of
    your classes and return here once you've done so.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we had done before, create a URL that points to the directory that contains
    your validation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s simply a matter of calling `evaluation` on the model, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will perform inference on each of our validation samples and report the
    accuracy, which you can access via quick looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23aa4d83-4680-49a7-9b94-9a7ba54db124.png)'
  prefs: []
  type: TYPE_IMG
- en: Satisfied with our validation accuracy, we are now ready to export our model,
    but just before we do so, let's perform a prediction on an individual image.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily do this by calling the `prediction` method of your model instance
    (or `predictions` if you have multiple samples you want to perform inference on),
    as shown in this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, then `Strawberry` should be output to your console. Now, feeling
    confident with our model, it's time to export it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In keeping with the nature of Create ML, exporting is simply a single line
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From here, it's just a matter of importing the Core ML model into your project,
    as we have seen many times throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: We have almost concluded our brief introduction to Create ML; but before we
    move on, I want to quickly highlight a few things, starting with model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, I mentioned the usefulness of data augmentation for
    small datasets. So, how do you use this during your training? The options are
    exposed to you using the  `MLImageClassifier.ModelParameters` structure, which
    you can pass an instance of when instantiating the classifier. One of the parameters
    is the `OptionSet`  `CreateML.MLImageClassifier.ImageAugmentationOptions`, which
    allows you to toggle various augmentation techniques on and off.
  prefs: []
  type: TYPE_NORMAL
- en: '`MLImageClassifier.ModelParameters` also allows you to specify the maximum
    number of iterations, version of the feature extraction, and validation data.
    You can learn more about these on the official web page at [https://developer.apple.com/documentation/create_ml/mlimageclassifier/modelparameters](https://developer.apple.com/documentation/create_ml/mlimageclassifier/modelparameters).'
  prefs: []
  type: TYPE_NORMAL
- en: Model metadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with the Core ML Tools package in [Chapters 5](a89287b3-5c90-4f77-801f-371f7a8f2d36.xhtml), *Emotion
    Detection with CNNs,* and [Chapter 6](40971e0d-b260-42e1-a9fb-5c4a56b0ebb2.xhtml), *Creating
    Art with Style Transfer,* to convert a Keras model to Core ML, we saw how we could
    explicitly set the metadata, which is shown in Xcode. Create ML provides a way
    of explicitly setting this data by passing in an instance of `MLModelMetadata`
    when exporting the model. It provides you all the metadata we had seen when working
    with the Core ML Tools package, such as name, description and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative workflow (graphical)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last point before moving on to the next section! In this chapter, we have
    walked through programmatically creating, training, and validating a model. Create
    ML offers an alternative, where, instead of using code to build your model, you
    can use a graphical interface. This is accessible via the `CreateMLUI` library,
    where you simply create an instance of `MLImageClassifierBuilder` and call its `showInLiveView`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this runs, you will see a widget in the live view, which allows you to
    train the model simply by dragging and dropping in your training and validation
    examples. The following figure shows this widget after training and validation,
    and the panel for entering metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c03a3709-578e-48fc-8b41-cb4f7284b09f.png)'
  prefs: []
  type: TYPE_IMG
- en: This concludes this section, the chapter, and the book. We will wrap up with
    some closing thoughts, including a list of some other tools to help you on your
    journey to creating more intelligent apps.
  prefs: []
  type: TYPE_NORMAL
- en: Closing thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This tool essentially democratizes machine learning by way of allowing anyone
    (who is able) to create custom models, but there is always a trade-off between
    simplicity and expressiveness. So, here is a short list of tools you may want
    to explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Turi create**: comes from a firm acquired by Apple in 2016; it provides tight
    integration with Core ML, allowing for easy deployment and custom models. It also
    provides a more comprehensive suite of machine learning models such as Style Transfer
    and segmentation. You can learn more about Turi create here: [https://github.com/apple/turicreate](https://github.com/apple/turicreate).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IBM Watson Services for Core ML**: IBM Watson is IBM''s AI platform, exposing
    an array of common machine learning models as a service. They have recently made
    available some of these services via Core ML models, allowing your application
    to leverage IBM Watson''s services even when offline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML Kit**: Google announced an ML Kit in early 2018 as a platform for common
    machine learning tasks such as image labeling and optical character recognition.
    The platform also takes care of model distribution, including custom ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlowLite**: A lightweight version of the popular machine learning framework
    TensorFlow. Like Core ML, it enables on-device inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only a few of the options available to integrate machine learning
    into your application, and all this is likely to grow significantly over the coming
    years. But, as we have seen throughout this book, the machine learning algorithm
    is (literally) only one part of the equation; data is what drives the experience,
    so I encourage you to seek out and experiment with new datasets to see what unique
    experiences you can come up with using what you have learnt here.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is evolving at an incredible pace. The website Arxiv is a popular
    repository for researchers to publish their papers; by just monitoring this site
    for over a week, you will be amazed and excited by the volume of papers being
    published and the advancements being made.
  prefs: []
  type: TYPE_NORMAL
- en: But, right now, there is a gap between the research community and industry practitioners,
    which in part motivated me to write this book. I hope that what you have read
    in the pages of this book has given you enough intuition behind deep neural networks
    and, more importantly, sparked enough curiosity and excitement for you to continue
    exploring and experimenting. As I mentioned at the start of this chapter, we have
    just scratched the surface of what is currently out and possible, never mind what
    will be around in 12 months.
  prefs: []
  type: TYPE_NORMAL
- en: So, consider this as an invite or challenge to join me in creating the next
    generation of applications. I look forward to seeing what you create!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced Create ML, a tool that makes it incredibly easy
    to train and deploy common machine learning models. We saw how easy it is to create
    an image classifier using a minimal amount of examples and minimal amount of code.
    We discussed how this was achieved through the use of transfer learning, and then
    covered some considerations to keep in mind with regard to your training data
    and the importance of splitting it for validation and testing.
  prefs: []
  type: TYPE_NORMAL
