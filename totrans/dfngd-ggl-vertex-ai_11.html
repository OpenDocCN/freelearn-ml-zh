<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer121">
			<h1 id="_idParaDest-153" class="chapter-number"><a id="_idTextAnchor153"/>11</h1>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor154"/>MLOps Governance with Vertex AI</h1>
			<p>In the rapidly evolving digital era, the successful implementation of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) solutions is <a id="_idIndexMarker712"/>not just about creating sophisticated models that can predict outcomes accurately for complex use cases. While this is undoubtedly <a id="_idIndexMarker713"/>essential, the proficient management and governance of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>)/<strong class="bold">ML operations</strong> (<strong class="bold">MLOps</strong>) is equally important. This is <a id="_idIndexMarker714"/>especially important in an enterprise setting where companies have to ensure they adhere to several internal policies and regulatory compliance requirements. This chapter delves into different MLOps governance components and how you can utilize features available within Google Cloud to <span class="No-Break">implement them.</span></p>
			<p>MLOps governance revolves around instituting a structured approach to managing and optimizing the various moving parts of ML operations. It encompasses the processes, tools, and guidelines that ensure the smooth functioning of ML projects, all while complying with required policies <span class="No-Break">and regulations.</span></p>
			<p>In this chapter, we will cover MLOps governance on Google Cloud, with a focus on the following <span class="No-Break">key areas:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">MLOps governance</span></li>
				<li>Case studies of <span class="No-Break">MLOps governance</span></li>
				<li>Implementing MLOps governance on <span class="No-Break">Google Cloud</span></li>
			</ul>
			<p>By the end of this chapter, our goal is to equip you with a comprehensive understanding of MLOps governance, its implementation on Google Cloud, and its critical role in maintaining successful, scalable, and compliant <span class="No-Break">ML operations.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor155"/>What is MLOps governance and what are its key components?</h1>
			<p>MLOps refers to the discipline that combines ML, data science, and DevOps principles to manage the life <a id="_idIndexMarker715"/>cycle of ML models efficiently. The goal of MLOps is to create a streamlined pipeline for developing, deploying, and maintaining ML models, ensuring that these models provide reliable and consistent results. However, the implementation and management of such a practice require a governing framework to ensure adherence to best practices and standards. This governing framework is what we refer to as <span class="No-Break">MLOps governance.</span></p>
			<p>MLOps governance is an essential, yet often overlooked, aspect of implementing and managing ML models within an organization. It encapsulates a comprehensive set of rules, procedures, and guidelines aimed at overseeing the ML models throughout their life cycle. This governance plays a pivotal role in ensuring that the MLOps pipeline operates smoothly and ethically, mitigating any risks associated with ML model deployment <span class="No-Break">and usage.</span></p>
			<p>The primary focus of MLOps governance is to create a reliable, transparent, and accountable ML system within an organization. This involves overseeing aspects such as data handling, model development, model deployment, model monitoring, and model auditing and can be broken down into two key facets: <strong class="bold">data governance</strong> and <span class="No-Break"><strong class="bold">model governance</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor156"/>Data governance</h2>
			<p>ML models <a id="_idIndexMarker716"/>are only as good as the data they are trained on. In MLOps governance, data handling refers to the governance <a id="_idIndexMarker717"/>of how data is collected, stored, processed, and used. It entails ensuring the quality and relevance of data, preserving data privacy, and complying with relevant regulations. This guarantees that the data that’s used for model training is not only of high quality but also ethically sourced <span class="No-Break">and used.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>Model governance</h2>
			<p>Model <a id="_idIndexMarker718"/>governance comprises <a id="_idIndexMarker719"/>the <span class="No-Break">following components:</span></p>
			<ul>
				<li><strong class="bold">Model deployment management</strong>: Overseeing model deployment involves ensuring that the model is correctly integrated into the organization’s system and <a id="_idIndexMarker720"/>that it operates as expected. It also involves checking that the model doesn’t inadvertently cause any harmful outcomes, such as biased results or <span class="No-Break">privacy violations.</span></li>
				<li><strong class="bold">Model auditing</strong>: MLOps governance ensures that there is a systematic review of the ML models in terms of their performance, ethical implications, and overall <a id="_idIndexMarker721"/>impact on the organization. Model auditing is essential to maintain transparency and accountability, particularly in scenarios where the model’s predictions significantly influence business decisions or <span class="No-Break">user experiences.</span></li>
				<li><strong class="bold">Model monitoring</strong>: Once the model has been deployed, MLOps governance requires <a id="_idIndexMarker722"/>that it be monitored continuously for any changes in its performance. This includes tracking the model’s accuracy, detecting data drift, and making sure the model continues to deliver <span class="No-Break">reliable predictions.</span></li>
			</ul>
			<p>MLOps governance is not a one-size-fits-all practice; it needs to be tailored to the specific needs and circumstances of each organization. This might involve customizing the governance <a id="_idIndexMarker723"/>based on the nature of the data being handled, the type of ML models being used, the specific applications of these models, and the broader <span class="No-Break">regulatory landscape.</span></p>
			<p>To summarize, MLOps governance is a critical component of any organization that employs ML models. By establishing robust MLOps governance, organizations can ensure that their MLOps practices are not just effective but also ethical, transparent, <span class="No-Break">and compliant.</span></p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor158"/>Enterprise scenarios that highlight the importance of MLOps governance</h1>
			<p>To understand <a id="_idIndexMarker724"/>the importance of MLOps governance, let’s go through some real-world scenarios that <span class="No-Break">highlight this.</span></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor159"/>Scenario 1 – limiting bias in AI solutions</h2>
			<p>Consider a financial services firm deploying a suite of ML models to predict credit risk. A large firm <a id="_idIndexMarker725"/>in the finance sector would have an array of internal policies around the data access, usage, and risk assessment of predictive models that its ML solutions will need to adhere to. This could range from limits on what data can be used for such purposes to who can access the model’s outputs. It would also be obligated to follow several regulatory requirements, such as preventing bias against protected classes in its decision-making models. For example, a bank would need to ensure that its decision-making process around loan approval is not biased based on race or gender. Even if the regulators can’t decipher the underlying ML models, they can conduct statistical analysis to detect whether there is a significant correlation between loan approvals and factors such as race. If a bank is found to be biased in its decision-making, besides being hit with substantial penalties by the regulators, it would also have a major public relations disaster on its hands. So the bank needs to build checks and balances in their ML development life cycle to flag any such issues in their models under development and prevent such models from ever reaching <span class="No-Break">production environments.</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor160"/>Scenario 2 – the need to constantly monitor shifts in feature distributions</h2>
			<p>Consider a scenario where an online e-commerce giant makes extensive use of AI to provide <a id="_idIndexMarker726"/>personalized recommendations to its retail customers. It has a set of models that seem to be working well in production. Now, the retailer is making a big marketing push to acquire users in additional regions that have been underrepresented in its customer base so far. As the influx of customers from new regions starts to grow, the retailer’s business development team notices a sharp decline in its click-through rate and revenue per user session based on the new monthly sales analytics report. When its analysts dig into the possible causes, they realize that the age distribution of users from the new regions is significantly different from the age distribution of the customers from the existing regions. This type <a id="_idIndexMarker727"/>of shift in feature/data distribution is known as <strong class="bold">data drift</strong> in MLOps parlance and can have a significant impact on user experience and, ultimately, the company’s bottom line. Although we are considering a hypothetical scenario where the company’s expansion into additional regions is causing a shift in data, this can happen due to several different scenarios, including, but not limited to, a shift in marketing strategy, a change in the economy, and a change in <span class="No-Break">product offerings.</span></p>
			<p>So, it’s <a id="_idIndexMarker728"/>important to have checks in place to catch such material changes in inference input data early so that the data science team can mitigate its impact by either building newer models with more recent data or building more <span class="No-Break">targeted models.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor161"/>Scenario 3 – the need to monitor costs</h2>
			<p>With great power comes great responsibility. Just like any other scalable technology in the cloud, there is <a id="_idIndexMarker729"/>a possibility of your team running up a huge bill if the resources are not planned properly, and proper budgets and limits are not set in the Google Cloud projects as safeguards. Consider a situation where a data scientist spins up a Vertex AI Workbench environment with an expensive GPU attached to the node for a quick experiment but then forgets to shut down the machine. Another similar scenario would be where someone tries to schedule an MLOps pipeline to run once a month with an extremely large GPU cluster but mistakenly configures it to run once a day, thereby making the cost 30x what it should have been. One or two such mistakes by themselves might not break the bank for a typical mid-size company but you can imagine how such costs can quickly add up, especially in large, distributed teams where no single person has full context of whether a training job running on a $10k/month cluster for last 3 days is an actual experiment being tracked or whether it’s just a mistake. So, it’s extremely important to set up cost management policies and, more importantly, automated controls that would limit the usage of specific resources on <strong class="bold">Google Cloud </strong><span class="No-Break"><strong class="bold">Platform</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GCP</strong></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor162"/>Scenario 4 – monitoring how the training data is sourced</h2>
			<p>Although <a id="_idIndexMarker730"/>data controls at the source of the data would primarily be handled by the data owners, AI product/solution leaders need to be cognizant of where they are sourcing their data from. If the data that’s being used to train the models is later discovered to be unlicensed or coming from sources with questionable data quality, it can lead to a significant amount of wasted resources, both in terms of infrastructure cost and <span class="No-Break">personnel overhead.</span></p>
			<p>Now, let’s look at the different tools and features available within Vertex AI to help you implement MLOps governance across your <span class="No-Break">ML solutions.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor163"/>Tools in Vertex AI that can help with governance</h1>
			<p>Vertex AI offers several tools to help with ML solution governance and monitoring that you can utilize <a id="_idIndexMarker731"/>to implement and track your organization’s standard governance policies and more generic governance best practices. Please keep in mind that for many of the governance policies, especially the ones around security and cost management, you will need to use tools outside of Vertex AI. For example, to set up monthly cost limits and budgets, you will need to use GCP’s native <span class="No-Break">billing tools.</span></p>
			<p>Let’s walk through the details of the different tools within Vertex AI that can be used as part of MLOps <span class="No-Break">governance processes:</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor164"/>Model Registry</h2>
			<p>Vertex AI Model Registry <a id="_idIndexMarker732"/>provides a centralized, organized, and secure location for managing all ML models within an organization. This <a id="_idIndexMarker733"/>facilitates seamless and efficient ML operations, from development and validation to deployment <span class="No-Break">and monitoring:</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B17792_11_1.jpg" alt="Figure 11.1 – Vertex AI Model Registry" width="1586" height="808"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Vertex AI Model Registry</p>
			<p>Acting as a central hub for managing your ML models’ life cycles, Vertex AI Model Registry offers a bird’s-eye view of your models, thus enabling a more organized and efficient method of tracking and training new model versions. It serves as an access point from where you can deploy your preferred model version to an endpoint, either directly or by <span class="No-Break">employing aliases.</span></p>
			<p>Vertex AI Model <a id="_idIndexMarker734"/>Registry extends its support to custom models across all AutoML data types – be it text, tabular data, images, or videos. Moreover, it can <a id="_idIndexMarker735"/>incorporate BigQuery ML models, which means that if you have models that have undergone training via BigQuery ML, you can easily register them within Vertex AI <span class="No-Break">Model Registry.</span></p>
			<p>Navigating to the model version details page, you’re provided with numerous options: you can evaluate a model, deploy it to an endpoint, set up batch prediction, and inspect specific details related to the model. With its user-friendly and streamlined interface, Vertex AI Model Registry simplifies how you can manage and deploy your optimal models to a <span class="No-Break">production environment.</span></p>
			<p>Let’s explore how Vertex AI Model Registry contributes to <span class="No-Break">ML governance:</span></p>
			<ul>
				<li><strong class="bold">Centralized repository for models</strong>: Model Registry provides a single location <a id="_idIndexMarker736"/>where all models in the organization are stored. This centralized repository makes it easy for data scientists, ML engineers, and DevOps teams to store, access, and manage models. It also fosters cross-functional visibility and collaboration, which are essential elements in maintaining a robust <span class="No-Break">governance framework.</span></li>
				<li><strong class="bold">Version control and model lineage</strong>: Every time a new model is trained or an <a id="_idIndexMarker737"/>existing model is updated, a new version is created in Model Registry. It maintains a history of all versions of a model, enabling easy tracking and comparison of different versions and ensuring that any updates or modifications are adequately logged and <span class="No-Break">accounted for:</span></li>
			</ul>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17792_11_2.jpg" alt="Figure 11.2 – Vertex AI Model Registry (version view)" width="1486" height="310"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Vertex AI Model Registry (version view)</p>
			<ul>
				<li><strong class="bold">Model metadata management</strong>: In combination with the Metadata Store, it can <a id="_idIndexMarker738"/>help record the model’s lineage, providing information about the datasets, model parameters, and training pipelines that are used to build each version of the model. This lineage information is invaluable for auditing and compliance purposes, a critical aspect of <span class="No-Break">ML governance.</span></li>
				<li><strong class="bold">Model validation and testing</strong>: Before a model is deployed into production, it needs to be validated and tested to ensure it meets the requisite <a id="_idIndexMarker739"/>performance metrics. Model Registry supports this by integrating with Vertex AI’s model evaluation tools. These tools can compare different model versions and validate them against predefined metrics, ensuring that only accurate and reliable models are deployed. You can view detailed information about your models, including performance metrics, directly from the model <span class="No-Break">details page:</span></li>
			</ul>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17792_11_3.jpg" alt="Figure 11.3 – Vertex AI model evaluation" width="900" height="1088"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Vertex AI model evaluation</p>
			<ul>
				<li><strong class="bold">Integration with other Vertex AI services</strong>: Model Registry integrates seamlessly <a id="_idIndexMarker740"/>with other Vertex AI services, including training pipelines and prediction services. Vertex AI Model Registry allows you to easily deploy your models to an endpoint with a few clicks or a few lines of code for real-time predictions. Integration with BigQuery allows you to register BQML models into Vertex AI Model Registry so that you can track all your models in one place. This integration facilitates end-to-end MLOps governance, allowing for efficient, consistent, and controlled <span class="No-Break">ML operations.</span></li>
			</ul>
			<p>Next, let's look at the <span class="No-Break">Metadata Store.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor165"/>Metadata Store</h2>
			<p>Vertex AI <a id="_idIndexMarker741"/>Metadata Store provides a robust, scalable system <a id="_idIndexMarker742"/>for tracking and managing all metadata associated with your ML workflows. Metadata, in this context, refers to information about the data used, the details of model training runs, the parameters used in these runs, the metrics generated, the artifacts created, and <span class="No-Break">much more:</span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17792_11_4.jpg" alt="Figure 11.4 – Vertex AI Metadata Store model lineage" width="1064" height="363"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Vertex AI Metadata Store model lineage</p>
			<p>By systematically <a id="_idIndexMarker743"/>collecting and organizing <a id="_idIndexMarker744"/>this metadata, Vertex AI Metadata Store enables comprehensive tracking of the entire ML life cycle, facilitating effective ML governance. <span class="No-Break">Here’s how:</span></p>
			<ul>
				<li><strong class="bold">Traceability</strong>: One of the key features of Vertex AI Metadata Store is its ability to <a id="_idIndexMarker745"/>provide end-to-end traceability for tracked ML workflows. For every model built, it can trace back the lineage of the data and the steps taken during preprocessing, feature engineering, model training, validation, <span class="No-Break">and deployment.</span></li>
				<li><strong class="bold">Model experimentation and comparison</strong>: Vertex AI Metadata Store allows <a id="_idIndexMarker746"/>you to track and compare different model versions, parameters, and metrics. This aids in governance by ensuring that the development and selection of models are systematic and transparent, making it easier to replicate and audit <span class="No-Break">your processes.</span></li>
				<li><strong class="bold">Consistency and standardization</strong>: By using Vertex AI Metadata Store, organizations <a id="_idIndexMarker747"/>can standardize metadata across different ML workflows. This promotes consistency in how ML workflows are executed and tracked, making it easier to apply governance policies <span class="No-Break">and procedures.</span></li>
				<li><strong class="bold">Compliance and regulatory adherence</strong>: In industries such as healthcare <a id="_idIndexMarker748"/>or finance, ML models must comply with strict regulatory requirements. Vertex AI Metadata Store aids in this compliance by providing a detailed lineage of the ML model that can link the final trained model to the source of data and showcase that model development best practices were followed and proper evaluation criteria were satisfied before the model was deployed <span class="No-Break">in production.</span></li>
				<li><strong class="bold">Reproducibility</strong>: Vertex AI Metadata Store also plays a significant role in ensuring <a id="_idIndexMarker749"/>the reproducibility of ML experiments, a crucial aspect of ML governance. By keeping track of all elements of an experiment, including data, configurations, parameters, and results, it ensures that the experiment can be reliably reproduced in <span class="No-Break">the future.</span></li>
				<li><strong class="bold">Collaboration and communication</strong>: Metadata Store can foster better collaboration <a id="_idIndexMarker750"/>and communication within teams. With the comprehensive tracking of ML workflows, team members can understand what others are doing, promoting transparency and <span class="No-Break">effective collaboration.</span></li>
			</ul>
			<p>Vertex AI Metadata Store serves as a comprehensive repository for the metadata associated with your ML operations, presented as <span class="No-Break">a graph:</span></p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17792_11_5.jpg" alt="Figure 11.5 – Vertex AI Metadata Store" width="1650" height="789"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Vertex AI Metadata Store</p>
			<p>Within this graph-based metadata framework, both artifacts and executions form the nodes, while events serve as the connecting edges that designate artifacts as the inputs or outputs <a id="_idIndexMarker751"/>of specific executions. <em class="italic">Contexts</em> denote logical subgroups, encompassing select sets of artifacts and executions for ease <span class="No-Break">of reference.</span></p>
			<p>Vertex AI Metadata <a id="_idIndexMarker752"/>Store permits the application of metadata as key-value pairs to the artifacts, executions, and contexts. For instance, a trained model could carry metadata that provides details about the training framework used, performance indicators such as accuracy, precision, and recall, and <span class="No-Break">so forth.</span></p>
			<p>To fully grasp shifts in the performance of your ML system, a thorough analysis of metadata produced by your ML workflows and the genealogy of its artifacts is mandatory. The lineage of an artifact encases all elements contributing to its origination, along with subsequent artifacts and metadata originating from this <span class="No-Break">root artifact.</span></p>
			<p>Take, for example, the lineage of a model, which could comprise the <span class="No-Break">following elements:</span></p>
			<ul>
				<li>The datasets that were utilized for model training, testing, <span class="No-Break">and evaluation</span></li>
				<li>The hyperparameters that were employed during the training process of <span class="No-Break">the model</span></li>
				<li>The specific code base, which is instrumental in training <span class="No-Break">the model</span></li>
				<li>The metadata that was accrued from the training and evaluation stages, such as the accuracy of <span class="No-Break">the model</span></li>
				<li>The artifacts that were derived from this parent model, such as batch <span class="No-Break">prediction results</span></li>
			</ul>
			<p>The Vertex ML Meta data system arranges resources in a hierarchical structure, necessitating <a id="_idIndexMarker753"/>that all resources belong to a Metadata Store. Therefore, establishing a MetadataStore is a prerequisite for creating <span class="No-Break">Metadata resources.</span></p>
			<p>Let’s delve <a id="_idIndexMarker754"/>into the key concepts and terminology that’s used in Vertex ML Metadata, which forms the basis for organizing resources <span class="No-Break">and components:</span></p>
			<ul>
				<li><strong class="bold">MetadataStore</strong>: This forms the top-tier container for metadata resources. A MetadataStore is region-specific and linked to a unique Google Cloud project. Conventionally, organizations employ one shared MetadataStore per project to manage <span class="No-Break">metadata resources.</span></li>
				<li><strong class="bold">Metadata resources</strong>: Vertex ML Metadata presents a graph-like data model to embody metadata originating from and utilized by ML workflows. The chief concepts under this model are artifacts, executions, events, <span class="No-Break">and contexts.</span></li>
				<li><strong class="bold">Artifact</strong>: In the context of an ML workflow, an artifact is a distinct entity or data fragment generated or consumed. It could be datasets, models, input files, training logs, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Context</strong>: A context is leveraged to group artifacts and executions under one searchable and typed category. It can be used to denote sets of metadata. For instance, a run of an ML pipeline could be designated as <span class="No-Break">a context.</span></li>
			</ul>
			<p>To illustrate, contexts can encapsulate the following <span class="No-Break">metadata sets:</span></p>
			<ul>
				<li>A single run of a Vertex AI Pipelines pipeline, where the context represents the run and each execution symbolizes a step in the ML pipeline. This demonstrates how artifacts, executions, and context meld into Vertex ML Metadata’s graph <span class="No-Break">data model.</span></li>
				<li>An experiment run from a Jupyter Notebook. Here, the context could symbolize the notebook, and each execution could denote a cell within <span class="No-Break">that notebook:</span><ul><li><strong class="bold">Event</strong>: An event is <a id="_idIndexMarker755"/>the term that’s used to describe the connection between artifacts and executions. Each artifact can be generated by an execution and consumed by others. Events aid in establishing the lineage of artifacts in ML workflows by chaining together artifacts <span class="No-Break">and executions.</span></li><li><strong class="bold">Execution</strong>: An execution is <a id="_idIndexMarker756"/>a log of a single step in the ML workflow, generally annotated with its runtime parameters. Examples of executions include model training, model evaluation, model deployment, data validation, and <span class="No-Break">data ingestion.</span></li><li><strong class="bold">Metadata Schema</strong>: A MetadataSchema provides a schema for specific types of <a id="_idIndexMarker757"/>artifacts, executions, or contexts. These schemas are employed to validate the key-value pairs at the time of creation of the corresponding Metadata resources. The schema validation only scrutinizes matching fields between the resource and the MetadataSchema. These types of schemas are depicted using OpenAPI schema objects and are generally described <span class="No-Break">using YAML.</span></li></ul></li>
			</ul>
			<h3>Exercise – using Vertex AI Metadata Store to track ML model development</h3>
			<p>Please <a id="_idIndexMarker758"/>refer to the accompanying notebook, <em class="italic">Chp11_Metadata_Store.ipynb</em>, <a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Metadata_Store.ipynb">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Metadata_Store.ipynb</a>, which walks you through the exercise to create a Metadata Store to store artifacts from a Vertex AI <span class="No-Break">Pipeline run.</span></p>
			<p>Let's talk about the Feature <span class="No-Break">Store next.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor166"/>Feature Store</h2>
			<p>Google Cloud’s Vertex AI Feature Store is a managed service that allows data scientists and ML engineers to create, manage, and share ML features. The service helps accelerate the process of turning raw data into ML models, ensuring the models are built with high-quality data that is both reliable <span class="No-Break">and consistent.</span></p>
			<p>While <a id="_idIndexMarker759"/>Vertex AI Feature Store primarily streamlines the model development process, it also plays a significant role in supporting ML governance. Let’s delve deeper into how this service assists with various facets of <span class="No-Break">ML governance:</span></p>
			<ul>
				<li><strong class="bold">Data management and traceability</strong>: A key aspect of ML governance is ensuring that the data that’s used for developing ML models is accurate, relevant, and traceable. Vertex AI Feature Store facilitates this by maintaining metadata about model lineage. This level of traceability makes it possible to audit the entire data pipeline effectively, thus promoting transparency and accountability in <span class="No-Break">ML operations.</span></li>
				<li><strong class="bold">Data consistency</strong>: Consistency in the data used for training and serving models is essential for ML governance. Discrepancies can lead to skewed results, negatively impacting the model’s performance and reliability. Vertex AI Feature Store provides unified storage for both training and online serving, ensuring that the same data features are used across <span class="No-Break">these stages.</span></li>
				<li><strong class="bold">Data quality monitoring</strong>: Maintaining the quality of data is another important aspect of ML governance. Poor data quality can lead to biased or inaccurate model predictions. Vertex AI Feature Store helps manage this by providing functionalities to monitor and validate the data ingested into the feature store. It can help identify anomalies or changes in data distribution over time, allowing timely intervention <span class="No-Break">and rectification.</span></li>
				<li><strong class="bold">Data versioning and reproducibility</strong>: In the context of ML governance, managing different versions of features is essential to track changes over time and enable reusability. Vertex AI Feature Store automatically tracks data updates and supports point-in-time lookup, which helps with consistency in training experiments and <span class="No-Break">model reproducibility.</span></li>
				<li><strong class="bold">Privacy and security</strong>: ML governance also involves ensuring that data privacy and security regulations are adhered to. Vertex AI Feature Store is built on Google Cloud’s robust security model, ensuring that sensitive data is encrypted <a id="_idIndexMarker760"/>both at rest and in transit. With Google Cloud’s <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>), organizations can also enforce fine-grained access controls to the feature store, ensuring that only authorized individuals have access to sensitive <span class="No-Break">data features.</span></li>
			</ul>
			<p>The <a id="_idIndexMarker761"/>following are best practices when using <span class="No-Break">Feature Store:</span></p>
			<ul>
				<li><strong class="bold">Modeling features for multiple entities</strong>: There can be scenarios where some features apply to more than one type of entity. Consider, for instance, a computed value that tracks the clicks on a product by a user. Such a feature jointly characterizes the product-user duo. In these cases, it’s advisable to form a new entity type such as <strong class="bold">product-user</strong> to group the shared features. Entity IDs can be formed by combining the IDs of the individual entities involved, given that the IDs are strings. These collectively formed entity types are known as composite <span class="No-Break">entity types.</span></li>
				<li><strong class="bold">Regulating access with IAM policies</strong>: IAM roles and policies provide a powerful way to govern access across multiple teams with diverse needs. For example, you might have ML researchers, data scientists, DevOps, and site reliability engineers who all need to access the same feature store, but the extent of their access can vary. Resource-level IAM policies can be employed to control access to a specific feature store or entity type. This allows for each role or persona within your organization to have a predefined IAM role tailored to the specific level of <span class="No-Break">access required.</span></li>
				<li><strong class="bold">Optimizing batch ingestion with resource monitoring and tuning</strong>: Batch ingestion jobs can intensify the CPU utilization of your feature store, thereby affecting online serving performance. To strike a balance, consider starting with one worker for every 10 online serving nodes and then monitoring the CPU usage during ingestion. The number of workers can be adjusted for future batch ingestion jobs based on your monitoring results to optimize throughput and <span class="No-Break">CPU usage.</span></li>
				<li><strong class="bold">Managing historical data with the disableOnlineServing field</strong>: During the process of backfilling – that is, ingesting historical feature values – you can disable online serving, which effectively bypasses any modifications to the <span class="No-Break">online store.</span></li>
				<li><strong class="bold">Adopting autoscaling for cost optimization</strong>: For users facing frequent fluctuations in load, autoscaling can help in cost optimization. This enables Vertex <a id="_idIndexMarker762"/>AI Feature Store to auto-adjust the number of nodes according to CPU utilization. However, it’s worth noting that autoscaling might not be the best solution for managing sudden surges <span class="No-Break">in traffic.</span></li>
				<li><strong class="bold">Testing online serving nodes for real-time serving performance</strong>: It’s essential to test the performance of your online serving nodes to ensure the real-time performance of your feature store. This can be accomplished by benchmarking parameters such as QPS, latency, and API. Remember to run these tests from the same region, use the gRPC API in the SDK for better performance, and conduct long-duration tests for more <span class="No-Break">accurate metrics.</span></li>
				<li><strong class="bold">Optimizing offline storage costs during batch serving and batch export</strong>: During batch serving and batch export, offline storage costs can be optimized by specifying a start time in your <strong class="source-inline">batchReadFeatureValues</strong> or <strong class="source-inline">exportFeatureValues</strong> request. This ensures the request runs a query over a subset of available feature data, which can result in significant savings on offline storage <span class="No-Break">usage costs.</span></li>
			</ul>
			<h3>Exercise – using Vertex AI Feature Store to catalog and monitor features</h3>
			<p>Please <a id="_idIndexMarker763"/>refer to the accompanying notebook, <em class="italic">Chp11_feature_store.ipynb</em>, <a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_feature_store.ipynb">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_feature_store.ipynb</a>, which walks you through the exercise of enabling model monitoring in <span class="No-Break">Vertex AI</span></p>
			<p>We'll discuss Kubeflow Pipelines in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor167"/>Vertex AI pipelines</h2>
			<p>This topic is covered in detail in <a href="B17792_10.xhtml#_idTextAnchor136"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>,<em class="italic"> Vertex AI Deployment and </em><span class="No-Break"><em class="italic">Automation Tools</em></span><span class="No-Break">.</span></p>
			<p>Vertex AI Pipelines is designed to help manage and orchestrate ML workflows, and it plays a significant role in ML governance. By providing a platform for building, deploying, and managing ML workflows, this tool enables organizations to implement effective governance processes for their <span class="No-Break">ML operations:</span></p>
			<ul>
				<li><strong class="bold">Defining and reusing ML pipelines</strong>: Vertex AI Pipelines and Kubeflow Pipelines support defining pipelines as a series of componentized steps. These steps can encapsulate data preprocessing, model training, evaluation, deployment, and more. By defining these steps, you can enforce best practices, ensure that every step of the pipeline is traceable, and guarantee that all models are <span class="No-Break">developed consistently.</span></li>
				<li>The reuse of pipelines and components across multiple workflows is another significant advantage. This allows for standardization across different ML projects, which is a crucial aspect of ML governance. Standardization not only promotes code and process reuse but also reduces the risk of errors and ensures consistency in how ML models are built <span class="No-Break">and deployed.</span></li>
				<li><strong class="bold">Versioning and experiment tracking</strong>: Both Vertex AI Pipelines and Kubeflow Pipelines offer capabilities for versioning and experiment tracking. With ML model versioning, different versions of models can be managed, and older versions can be rolled back <span class="No-Break">when necessary.</span></li>
				<li>Experiment tracking is also critical for governance. It provides visibility into how different model parameters and datasets impact the performance of a model. The ability to record and compare experiments also facilitates auditability, allowing you to understand the decision-making process behind <span class="No-Break">each model.</span></li>
				<li><strong class="bold">Automated and reproducible pipelines</strong>: Automating ML workflows ensures that all steps are executed consistently and reliably, which is an essential aspect of ML governance. Both Vertex AI Pipelines and Kubeflow Pipelines allow for the creation of automated pipelines, which means each step in the ML process <span class="No-Break">is reproducible.</span></li>
				<li>Reproducibility is an often-understated aspect of ML governance. Reproducible pipelines mean you can track the data, code, configurations, and results <a id="_idIndexMarker764"/>at every step of the pipeline, which is crucial for debugging and auditing purposes. This is particularly important when your models need to comply with certain regulations that require transparent and explainable model <span class="No-Break">development processes.</span></li>
				<li><strong class="bold">Integration with other Google Cloud services</strong>: Vertex AI Pipelines and Kubeflow Pipelines are designed to work seamlessly with other Google Cloud services, such as BigQuery for data management, Cloud Storage for storing models and data, and AI Platform for model deployment. This integration makes it easier to implement governance processes across your entire ML workflow. For example, you can ensure data privacy and security by using BigQuery’s data governance features, or you can manage access control and monitor model performance using the capabilities of AI Platform. Vertex AI Pipelines and Kubeflow Pipelines offer various features that support ML governance, including pipeline definition and reuse, versioning, experiment tracking, automation, reproducibility, and integration with other Google Cloud services. By leveraging these features, organizations can effectively manage their ML operations, ensure compliance with best practices and regulations, and create a transparent, accountable, and efficient <span class="No-Break">ML workflow.</span></li>
			</ul>
			<p>Now, let's talk about Monitoring <span class="No-Break">in detail!</span></p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor168"/>Model Monitoring</h2>
			<p>Vertex AI Monitoring plays a critical role in the MLOps governance process by offering tools for the real-time monitoring and management of ML models. It enables organizations to establish transparency, accountability, and reliability in their ML <a id="_idIndexMarker765"/>processes. Here’s an overview of how Vertex AI Monitoring helps with <span class="No-Break">ML governance:</span></p>
			<ul>
				<li><strong class="bold">Model monitoring</strong>: Vertex AI Monitoring offers automated monitoring of models deployed in production. This means the system tracks the model’s performance continuously, identifying any potential drift in the data and degradation in the model’s performance. If the model’s performance dips below a predefined threshold, it alerts the appropriate stakeholders. This continuous monitoring is vital for maintaining the model’s accuracy and relevance, which are fundamental aspects of <span class="No-Break">MLOps governance.</span></li>
				<li><strong class="bold">Data skew and drift detection</strong>: One of the main features of Vertex AI Monitoring is its ability to detect data skew and drift. Data skew is the difference between the data used for training a model and the data used for serving predictions. Drift, on the other hand, is the change in data over time. Both can lead to a decline in the model’s performance. Vertex AI Monitoring automatically detects these discrepancies and provides timely alerts, allowing for rapid remediation. Ensuring the consistency and reliability of data aligns with the principle of data governance, a critical component of <span class="No-Break">MLOps governance.</span></li>
				<li><strong class="bold">Automated alerts</strong>: Automated alerts from Vertex AI Monitoring provide an early warning system for any potential issues with the models in production. Timely alerts ensure that any problems are identified and remediated promptly, preventing any long-term impact on the model’s performance or the business operations. This feature is vital for risk management, a crucial aspect of <span class="No-Break">MLOps governance.</span></li>
				<li><strong class="bold">Integration with other Google Cloud tools</strong>: Vertex AI Monitoring seamlessly integrates with other Google Cloud tools such as Cloud Logging and Cloud Monitoring. This allows you to create comprehensive dashboards for visualizing your ML model’s health and performance, and to receive alerts for any detected issues. These features enable more robust monitoring and troubleshooting capabilities, improving the overall governance of <span class="No-Break">ML models.</span></li>
			</ul>
			<p>Now, let’s <a id="_idIndexMarker766"/>look at the details of how a monitoring solution calculates training-serving skew and <span class="No-Break">prediction drift.</span></p>
			<p>Vertex AI <a id="_idIndexMarker767"/>Monitoring uses <strong class="bold">TensorFlow Data Validation</strong> (<strong class="bold">TFDV</strong>) to detect training-serving skew and prediction drift by calculating distributions and distance scores. The process involves <span class="No-Break">two steps:</span></p>
			<ol>
				<li>Calculating the baseline <span class="No-Break">statistical distribution</span><p class="list-inset">In the context of Vertex AI Monitoring, skew detection and drift detection hinge critically on the accurate definition of a baseline statistical distribution. The distinction between the baselines for these two facets lies in the data used to <span class="No-Break">compute them:</span></p><ul><li><strong class="bold">Skew detection</strong>: The baseline <a id="_idIndexMarker768"/>is derived from the statistical distribution of the feature values present in the <span class="No-Break">training data</span></li><li><strong class="bold">Drift detection</strong>: Conversely, for drift detection, the baseline is formulated from the <a id="_idIndexMarker769"/>statistical distribution of the observed feature values from the recent <span class="No-Break">production data</span></li></ul><p class="list-inset">The process of calculating these distributions unfolds <span class="No-Break">as follows:</span></p><ul><li><strong class="bold">Categorical features</strong>: The distribution for categorical features is determined by computing the quantity or proportion of occurrences for each potential value of <span class="No-Break">the feature.</span></li><li><strong class="bold">Numerical features</strong>: When dealing with numerical features, Vertex AI Monitoring segregates the entire range of possible feature values into uniform intervals. Subsequently, the number or percentage of feature values residing within each interval <span class="No-Break">is computed.</span></li></ul><p class="list-inset">It is important to note that the baseline is initially set at the time of creating a model monitoring job and is subject to recalculation only if there are updates to the training dataset allocated for <span class="No-Break">the job.</span></p></li>
				<li>Calculating the statistical distribution of recent feature values seen <span class="No-Break">in production</span><p class="list-inset">The process <a id="_idIndexMarker770"/>initiates by contrasting the distribution of the most recent feature values, observed in a production environment, with a baseline distribution, through the computation of a distance score. Different methods are utilized for different types <span class="No-Break">of features:</span></p><ul><li><strong class="bold">Categorical features</strong>: The L-infinity distance method is employed to compute the <span class="No-Break">distance score</span></li><li><strong class="bold">Numerical features</strong>: The Jensen-Shannon divergence method is used to calculate the <span class="No-Break">distance score</span></li></ul><p class="list-inset">When the computed distance score exceeds a predefined threshold, indicating a significant disparity between the two statistical distributions, Vertex AI Monitoring identifies and flags the inconsistency, labeling it as skew <span class="No-Break">or drift.</span></p></li>
			</ol>
			<p>The following <a id="_idIndexMarker771"/>are best practices for utilizing Vertex <span class="No-Break">AI Monitoring:</span></p>
			<ul>
				<li><strong class="bold">Prediction request sampling rate</strong>: To enhance cost efficiency, a prediction request sampling rate can be configured. This feature enables monitoring a portion of the production inputs to a model instead of the <span class="No-Break">entire dataset.</span></li>
				<li><strong class="bold">Monitoring frequency</strong>: It’s possible to define the frequency at which the recently logged inputs of a deployed model are scrutinized for skew or drift. This frequency, also known as the monitoring window size, dictates the time frame of logged data evaluated in each <span class="No-Break">monitoring run.</span></li>
				<li><strong class="bold">Alerting thresholds</strong>: You can set alerting thresholds for each feature that is monitored. If the statistical distance between the input feature distribution and its respective baseline surpasses this threshold, an alert is generated. By default, both categorical and numerical features are monitored, each with a threshold value <span class="No-Break">of 0.3.</span></li>
				<li><strong class="bold">Shared configuration parameters across multiple models</strong>: An online prediction endpoint can host more than one model. When skew or drift <a id="_idIndexMarker772"/>detection is enabled on an endpoint, certain configuration parameters, including detection type, monitoring frequency, and the fraction of input requests monitored, are shared across all models hosted on <span class="No-Break">that endpoint.</span></li>
				<li><strong class="bold">Model-specific configuration parameters</strong>: Apart from the shared parameters, it is also possible to specify different values for other configuration parameters for each model. This flexibility allows you to tailor the monitoring settings according to the unique needs and behavior of <span class="No-Break">each model.</span></li>
			</ul>
			<h3>Exercise – [notebook] using Vertex AI Monitoring features to track the performance of deployed models in production environments</h3>
			<p>Please <a id="_idIndexMarker773"/>refer to the accompanying notebook, <strong class="source-inline">Chp11_Model_Monitoring.ipynb</strong>, <a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Model_Monitoring.ipynb">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Model_Monitoring.ipynb</a>, which walks you through the exercise of enabling model monitoring in <span class="No-Break">Vertex AI.</span></p>
			<p>We'll look at billing monitoring in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor169"/>Billing monitoring</h2>
			<p><strong class="bold">GCP</strong> offers a <a id="_idIndexMarker774"/>suite of robust <a id="_idIndexMarker775"/>billing and cost management tools that can play a crucial role in MLOps governance. These tools provide fine-grained visibility into how resources are being utilized, helping organizations effectively manage costs associated with their ML workflows. <span class="No-Break">Here’s how:</span></p>
			<ul>
				<li><strong class="bold">Budgets and alerts</strong>: GCP’s budget and alerts feature allows organizations to establish custom budgets for their GCP projects or billing accounts, and configure alerts when the actual spending exceeds the defined thresholds. This tool is instrumental in tracking and controlling the costs associated with training, deploying, and running ML models. When integrated into the MLOps governance framework, it ensures that the expenses related to ML workflows do not exceed their allocated budgets, preventing cost overruns and promoting <span class="No-Break">financial responsibility.</span></li>
				<li><strong class="bold">Detailed billing reports</strong>: GCP’s detailed billing reports offer insights into the specific costs associated with each service. For instance, an organization can view detailed <a id="_idIndexMarker776"/>reports about the expenses incurred for services such as Vertex AI, Cloud Storage, BigQuery, and Compute Engine. These reports allow organizations to understand which ML workflows or components are more cost-intensive and need optimization. This granular visibility is essential for cost governance in MLOps, enabling organizations to strategically plan their resource usage and <span class="No-Break">manage costs.</span></li>
				<li><strong class="bold">Billing export to BigQuery</strong>: GCP allows you to export detailed billing data to BigQuery, Google’s highly scalable and cost-effective data warehouse. This feature enables organizations to analyze their GCP billing data programmatically and build custom dashboards using data visualization tools such as Data Studio. With this, MLOps teams can better understand and manage the costs associated with various ML projects, and identify opportunities for savings <span class="No-Break">and optimization.</span></li>
				<li><strong class="bold">Cost management tools</strong>: GCP’s cost management tools, such as the Pricing Calculator <a id="_idIndexMarker777"/>and the <strong class="bold">Total Cost of Ownership</strong> (<strong class="bold">TCO</strong>) tool, help organizations forecast their cloud expenses and compare them with the costs of running the same infrastructure on-premises or on other cloud platforms. These tools are especially valuable in the planning and budgeting stages of ML projects, enabling MLOps teams to make more informed decisions about resource allocation and <span class="No-Break">cost optimization.</span></li>
				<li><strong class="bold">Cloud Functions for automating cost controls</strong>: GCP’s serverless execution environment, Cloud Functions, can be used to create functions that automatically stop or start services based on custom logic. For example, you can write a <a id="_idIndexMarker778"/>function that automatically stops a Compute Engine instance when it’s not being used, thereby saving costs. This level of automated cost control can be invaluable in managing the costs associated with running ML models, a crucial aspect of <span class="No-Break">MLOps governance.</span></li>
			</ul>
			<p>Since billing and budget monitoring is a much broader topic than Vertex AI, it is outside the scope <a id="_idIndexMarker779"/>of this book, but you can refer to the GCP Billing documentation (<a href="https://cloud.google.com/billing/docs/how-to">https://cloud.google.com/billing/docs/how-to</a>) to dive deeper into <span class="No-Break">the topic.</span></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor170"/>Summary</h1>
			<p>In this chapter, we went over the fundamentals of MLOps governance, detailing its key role in maintaining ML systems’ efficiency, accuracy, and reliability. To emphasize the importance of MLOps governance in real-world scenarios, we explored case studies from various sectors, showcasing how this governance model can dramatically impact the success of <span class="No-Break">AI/ML implementations.</span></p>
			<p>As we dove deeper into the topic, we clarified the core components of MLOps governance – data governance and model governance – offering an overview of their function and necessity within the ML model life cycle. Additionally, we went through some real-world scenarios that effectively underscored the relevance and importance of <span class="No-Break">MLOps governance.</span></p>
			<p>On the technical side, we enumerated and discussed several tools available within Vertex AI that aid in ML solution governance and monitoring. We touched upon the functionalities of Model Registry, Metadata Store, Feature Store, Vertex AI Pipelines, Model Monitoring, and GCP’s cost management tools. Through their combined use, we illustrated how you can establish robust, transparent, and compliant <span class="No-Break">ML operations.</span></p>
			<p>We supplemented this chapter with examples and exercises on implementing ML governance using Vertex AI to cement these concepts. These practical exercises offered hands-on experience with Vertex AI’s Model Registry, Metadata Store, and Model <span class="No-Break">Monitoring functionalities.</span></p>
			<p>In the next section of this book, <em class="italic">Part 3</em>, <em class="italic">Prebuilt/Turnkey ML Solutions Available in GCP</em>, we will cover different out-of-the-box ML models and solutions such as GenAI/LLM models, Document AI, Vision APIs, and NLP APIs, which you can utilize to build ML solutions for different <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor171"/>References</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<p>GCP Vertex AI Metadata Store <span class="No-Break">documentation: </span><a href="https://cloud.google.com/vertex-ai/docs/ml-metadata"><span class="No-Break">https://cloud.google.com/vertex-ai/docs/ml-metadata</span></a></p>
			<p>GCP Vertex AI billing and budgeting <span class="No-Break">features: </span><a href="https://cloud.google.com/billing/docs/how-to"><span class="No-Break">https://cloud.google.com/billing/docs/how-to</span></a></p>
			<p>Practitioner’s guide to <span class="No-Break">MLOps: </span><a href="https://cloud.google.com/resources/mlops-whitepaper"><span class="No-Break">https://cloud.google.com/resources/mlops-whitepaper</span></a></p>
		</div>
	</div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer122" class="Content">
			<h1 id="_idParaDest-172" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor172"/>Part 3: Prebuilt/Turnkey  ML Solutions Available in GCP</h1>
		</div>
		<div id="_idContainer123">
			<p>In this part, you will learn about some of the most commonly used prebuilt ML solution offerings available in Google Cloud. Many of these solutions are ready to use and can be integrated with real-world use cases in no time. Most importantly, this part also covers the recently launched generative AI offerings within <span class="No-Break">Vertex AI.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B17792_12.xhtml#_idTextAnchor173"><em class="italic">Chapter 12</em></a>, <em class="italic">Vertex AI – Generative AI Tools</em></li>
				<li><a href="B17792_13.xhtml#_idTextAnchor194"><em class="italic">Chapter 13</em></a>, <em class="italic">Document AI – an End-to-End Solution for Processing Documents</em></li>
				<li><a href="B17792_14.xhtml#_idTextAnchor203"><em class="italic">Chapter 14</em></a>, <em class="italic">ML APIs for Vision, NLP, and Speech</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer124">
			</div>
		</div>
		<div>
			<div id="_idContainer125" class="Basic-Graphics-Frame">
			</div>
		</div>
	</div>
</div>
</body></html>