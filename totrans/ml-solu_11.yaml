- en: Chapter 11. Building Gaming Bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we covered applications that belong to the computer vision
    domain. In this chapter, we will be making a gaming bot. We will cover different
    approaches to build the gaming bot. These gaming bots can be used to play a variety
    of Atari games.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a quick recap of the past two years. Let''s begin with 2015\. A small
    London-based company called DeepMind published a research paper titled Playing
    Atari with Deep Reinforcement Learning, available at [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
    In this paper, they demonstrated how a computer can learn and play Atari 2600
    video games. A computer can play the game just by observing the screen pixels.
    Our computer game agent (the computer game player) will receive rewards when the
    game score increases. The result presented in this paper is remarkable. The paper
    created a lot of buzz, and that was because each game has different scoring mechanisms
    and these games are designed in such a way that humans find it difficult to achieve
    the highest score. The beauty of this research paper is that we can use the concept
    and given model architecture without any changes to learn different games. This
    model architecture and algorithm are applied to seven games, and in three of them,
    the algorithm performed way better than a human! This is a big leap in the field
    of AI because the hope is that we can build a single algorithm that can master
    many tasks, as well as build a General Artificial Intelligence or Artificial General
    Intelligence (AGI) system at some point in the next few decades. You can read
    more about AGI at: [https://en.wikipedia.org/wiki/Artificial_general_intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence).
    We all know that DeepMind was immediately acquired by Google.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2017, Google DeepMind and OpenAI achieved a major milestone, which gives
    us hope that AGI will happen soon. Let''s start with Google DeepMind first; you
    must have heard that Google DeepMind ''s AlphaGo AI (a gaming bot) won a three-match
    series against the world''s best Go player. Go is a complex game because it has
    a huge number of permutations and combinations for a single move. You can watch
    the video for this game by clicking on this YouTube video: [https://www.youtube.com/watch?v=vFr3K2DORc8](https://www.youtube.com/watch?v=vFr3K2DORc8).
    Now let''s talk about OpenAI. If this is the first time you have heard about OpenAI,
    this is a short introduction. OpenAI is a non-profit AI research organization,
    cofounded by Elon Musk, which is trying to build AI that will be safe and ensure
    that the benefits of Artificial Intelligence (AI) systems are widely and evenly
    distributed as far as possible. In 2017, OpenAI''s gaming bot beat the world''s
    best Dota 2 players. You can watch this YouTube video for reference: [https://www.youtube.com/watch?v=7U4-wvhgx0w](https://www.youtube.com/watch?v=7U4-wvhgx0w).
    All this was achieved by AGI system environments created by tech giants. The goal
    of making an AGI system is that a single system can perform a variety of complex
    tasks. The ideal AGI system can help us solve lots of complex tasks in the fields
    of healthcare, agriculture, robotics, and so on without any changes to its algorithm.
    So, it is better for us if we can understand the basic concepts in order to develop
    the AGI system.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, just to start with, we will be trying to make a gaming bot
    that can play simple Atari games. We will achieve this by using reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, we will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the coding environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding reinforcement learning (RL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Atari gaming bot for pathfinder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the key concepts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the basic version of the gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the Space Invaders gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the key concepts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Space Invaders gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the Pong gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the key concepts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Pong gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just for fun - implementing the Flappy Bird gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know we are trying to develop a gaming bot: a program that can play simple
    Atari games. If we provide enough time and computation resources, then it can
    outperform humans who are experts at playing certain games. I will list down some
    famous Atari games so that you can see which types of games I''m talking about.
    You must have played one of these games for sure. Some of the famous Atari games
    are Casino, Space Invaders, Pac-man, Space War, Pong (ping-pong), and so on. In
    short, the problem statement that we are trying to solve is how can we build a
    bot that can learn to play Atari games?'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be using already built-in gaming environments using
    `gym` and `dqn` libraries. So, we don't need to create a gaming visual environment
    and we can focus on the approach of making the best possible gaming bot. First,
    we need to set up the coding environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the coding environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover how to set up a coding environment that can
    help us implement our applications. We need to install the gym library. These
    are the steps that you can follow. I''m using `Ubuntu 16.04 LTS` as my operating
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Clone the gym repository from GitHub by executing this command: `$
    sudo git clone` `https://github.com/openai/gym.gi``t`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Jump to the gym directory by executing this command: `$ cd gym`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Execute this command to install the minimum number of required libraries
    for `gym`: `$ sudo pip install -e`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: Install the gaming environment for Atari games by executing this command:
    `$ sudo pip install gym[atari]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 5: This step is optional. If you want to install all the gaming environments,
    then you can execute the following commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$ sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev
    xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$ sudo pip install gym[all]`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how you can install the `gym` machine learning library, which we will
    be using to develop the gaming bot. We will be using the TensorFlowimplementation
    of the `dqn` library, so there will be no need to install `dqn` separately, but
    you can definitely refer to this installation note: [https://github.com/deepmind/dqn](https://github.com/deepmind/dqn).'
  prefs: []
  type: TYPE_NORMAL
- en: As we are ready with the environment setup, we need to move on to our next section,
    which will help us understand the techniques that will be useful in order to develop
    the gaming bots. So let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Reinforcement Learning (RL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are making a gaming bot with the help of reinforcement learning
    techniques. The motivation behind reinforcement learning is simple. RL gives the
    machine or any software agent a chance to learn its behavior based on the feedback
    this agent receives from the environment. This behavior can be learned once, or
    you can keep on adapting with time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand RL with a fun example of a child learning to speak. These
    are the steps a child will take when they are learning how to speak:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: The first thing is that the child starts to observe you; how you are
    speaking and how you are interacting with him or her. The child listens to the
    basic words and sentences from you and learns that they can make a similar sound
    too. So, the child tries to imitate you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: The child wants to speak full sentences or words but they may not understand
    that even before speaking sentences, they need to learn simple words! This is
    a challenge that comes while they are trying to speak. Now the child attempts
    to make sounds, some sounds are funny or weird, but they are still determined
    to speak words and sentences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: There is another challenge that the child faces, which is that they
    need to understand and remember the meaning behind the words they are trying to
    speak. But the child manages to overcome this challenge and learns to speak their
    first few words, which are very simple words, such as *mama, papa, dadda, paa,
    maa*, and so on. They learn this task by constantly observing their surroundings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: The real challenge begins with how to use a particular word, when to
    use which word, and remembering all the words they hear for the first time. Try
    to feed the meaning of all the words and the context in which the child needs
    to use them. Sounds like a challenging task, doesn''t it?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a child, it is a difficult task, but once it starts understanding the language
    and practices the sentences, then it will become a part of the child's life. Within
    2-3 years, the child could have enough practice to start interacting easily. If
    we think of ourselves speaking, it is an easy task for us because we have learned
    enough about how to interact within our environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to connect the dots. With the help of the preceding example,
    we will try to understand the concept of Reinforcement Learning. The problem statement
    of the given example is speaking, where the child is the agent who is trying to
    manipulate the environment (which word the child speaks first) by taking an action
    (here, the action is speaking), and they try to speak one word or the other. The
    child gets a reward—say, a chocolate—when they accomplish a submodule of the task,
    which means speaking some words in a day, and will not receive any chocolate when
    they are not able to speak anything. This is a simplified description of reinforcement
    learning. You can refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Reinforcement Learning (RL)](img/B08394_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Pictorial representation of the basic concept of RL'
  prefs: []
  type: TYPE_NORMAL
- en: So basically, RL allows machines and software agents to automatically determine
    the ideal and best possible behavior within a specific task or within a specific
    context in order to maximize the performance of software agents. Simple reward
    feedback is required for the agent to learn its behavior, and this is known as
    the reinforcement signal. Every time the software agent tries to take the kind
    of actions that lead it to gain maximum rewards. Eventually, it learns all the
    actions or moves that lead the agent to the optimum solution of the task so that
    it becomes the master of it. The algorithms for RL learn to react to an environment.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build the gaming bot, RL algorithms are the perfect choice, and
    there is a reason behind it. Suppose there are many slot machines with random
    payouts and you want to win the maximum amount of money. How do you win the maximum
    amount of money? One naive approach is to just select a single machine and pull
    its lever all day long, and it might give you some payouts. If you are lucky enough,
    then you may hit the jackpot. There are chances that in order to try this approach,
    you may lose some money. This approach is called a *pure exploitation* *approach*.
    It is not an optimal approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take another approach. In this approach, we will pull the lever of every
    single slot machine and pray that at least one of them hits the jackpot. This
    too is a naive approach. In this approach, we need to keep pulling the lever all
    day long. This approach is called a *pure exploration approach*. This approach
    is not optimal as well, so we need to find a proper balance between these two
    approaches in order to get maximum rewards. This is referred to as the exploration
    versus exploitation dilemma of RL. Now we need to solve this issue. Well, for
    that, we need a mathematical framework that can help us achieve the optimal solution,
    and that mathematical approach is *Markov Decision Process (MDP)*. Let's explore
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Markov Decision Process (MDP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Markov Decision Process uses the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Set of states, *S*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set of actions, *A*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reward function, *R*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy, *π*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value, *V*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to perform transition for one state to the end state *(S)*, we have
    to take an action *(A)* or a series of actions. We will get rewards *(R)* for
    each action we take. Our action can provide us either a positive reward or a negative
    reward. The set of actions that we take define our policy *(π)*. The rewards that
    we get in return after performing each action define our value *(V).* Our goal
    is to maximize the rewards by choosing the correct policy. We can do that by performing
    the best possible action. Mathematically, we can express this as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov Decision Process (MDP)](img/B08394_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Mathematical representation of Markov Decision Process'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be applying the preceding equation for all possible values of *S* for
    a time t. We have a set of states and actions. We need to consider these states,
    actions, and rules for transitioning the agent from one state to another. When
    we perform an action that changes the state of the gaming agent, the the agent
    will get rewards for doing that. This entire process of state, action, and getting
    rewards makes up Markov Decision Process (MDP). One round of a game is considered
    *one episode* of MDP. This process includes a finite sequence of states, actions,
    and rewards. Take a look at the following equation for a representation of the
    process: :'
  prefs: []
  type: TYPE_NORMAL
- en: S0, a0, r1, s1, a1, r2, s2, a2, r3 ,…, sn-1, an-1, rn, sn
  prefs: []
  type: TYPE_NORMAL
- en: Here, s[i] represents the state, ai is the action, and r[i+1] is the reward
    that we will get after performing the action. sn indicates that a particular episode
    ends with a terminal state, and this happens when the **game over** screen appears.
    A Markov Decision Process is based on the Markov assumption, the probability of
    the next state s[i+1] depends on the current state s[i] and the performed action
    ai and does not depend on the preceding states or actions.
  prefs: []
  type: TYPE_NORMAL
- en: Discounted Future Reward
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the long term, if we want our gaming agent to do well, then we need to take
    into account the immediate rewards, but we also need to consider the future awards
    that our agent will get. How should we approach this scenario? Well, the answer
    lies in the concept of discounted future rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given one run of MDP, we can calculate the *total rewards* for one episode
    by using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: R = r1 + r2 + r3 + … + rn
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the preceding equation, we can calculate the *total future rewards*
    from time stamp *t* onward, and that can be expressed by the given equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Rt = rt + rt+1 + rt+2 + rt+3 + … + rn
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are dealing with a gaming environment that is random, and we cannot
    be sure whether we will get the same rewards the next time we perform the same
    actions to play a specific game. The more you think about the future, the more
    it will get diverged. For that reason, it is better that we use *discounted future
    rewards* instead of total rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: Rt = rt+ γrt+1 + γ2rt+2+ … + γn-1 rn
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, γ is the discount factor. Its value is between *0 to 1*. It is easy to
    understand that the discounted future reward at particular time step t can be
    expressed with the help of the rewards of the current stare plus rewards at time
    step *t+1*:'
  prefs: []
  type: TYPE_NORMAL
- en: Rt = rt + γ (rt+1 + γ (rt+2 + …)) = rt + γRt+1
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let me tell you what the practical meaning of tuning this discount factor
    is: if we set the value of the discount factor γ = 0, then our strategy of plying
    will be short-sighted and we take our gaming decision just based on the immediate
    rewards. We need to find a balance between immediate rewards and future rewards,
    so we should set the value of the discount factor to something more than 0.7.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can set the value as γ = 0.9\. If our gaming environment is
    deterministic and we know that the same actions always lead us to the same reward,
    then we can set the value of the discount factor γ =1\. A good strategy for a
    gaming agent would be to always *choose an action that maximizes the discounted
    future reward.*
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the basics of RL. From now onward, we will start implementing
    our gaming bot. So let's get ready for some fun!
  prefs: []
  type: TYPE_NORMAL
- en: Basic Atari gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are trying a hands-on approach to building some basic gaming
    bots. We are choosing some famous Atari games that nearly everybody has played
    at some point in their lives. We choose Atari games because we know how to play
    them, and that makes our life easy because we can understand what kind of action
    our bot should perform in order to get better over a period of time.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are building our own game. This game is simple, so we can
    look at how we can apply the Q-Learning algorithms. Here, we will be designing
    the game world on our own. Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be looking at a lot of important aspects that will
    help us while coding, so here, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Rules for the game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Q-Learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules for the game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we begin with the basic concepts or algorithms, we need to understand
    the rules of the game that we are building. The game is simple and easy to play.
    The rules for this game are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Rules of the game:* The gaming agent means a yellow box has to reach one of
    the goals to end the game: it can be either a green cell or a red cell. This means
    the yellow box should reach either the green cell or the red cell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rewards:* Each step gives us a negative reward of - 0.04\. If our gaming agent
    reaches the red cell, then the red cell gives us a negative reward of - 1\. If
    our gaming agent reaches the green cell, then the green cell gives us a positive
    reward of +1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*States:* Each cell is a state for the agent that it takes to find its goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Actions:* There are only four actions for this game: Up direction, Down direction,
    Right direction, Left direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need the `tkinter` library to implement this approach. I have already provided
    a description about how to install it at this GitHub link: [https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the Q learning algorithm that we will use during this chapter
    to build the gaming bot.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Q-Learning algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This algorithm was originally published by DeepMind in two papers. The first
    one was published with the title *Playing Atari with Deep Reinforcement Learning*
    on NIPS 2013\. The link for the paper is [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf).
    The second one was published with the title *Human-level control through deep
    reinforcement Learning* on Nature in 2015\. The link for this paper is [http://www.davidqiu.com:8888/research/nature14236.pdf](http://www.davidqiu.com:8888/research/nature14236.pdf).
    You should definitely read these papers. I have simplified the main concepts of
    these papers for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Q-learning, we need to define a *Q (s, a)* function that represents the
    discount factor reward when we perform action *a* in state *s*, and it continues
    optimally from that point onward. You can see the equation that helps us choose
    the maximum reward in the followingscreenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Equation for Q-function'
  prefs: []
  type: TYPE_NORMAL
- en: We can think of the Q (s, a) function as giving us the best possible score at
    the end of the game after performing action *a* in the particular state *s*. This
    function is the Q function because it indicates the *quality* of a certain action
    in a certain given state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me simplify this for you. Suppose you are in state *s* and are thinking
    about whether you should perform action *a* or *b*. You really want to win the
    game with a high score. So, in order to achieve your goal, you want to select
    the action that gives you the highest score at the end of the game. If you have
    this Q-function with you, then the selection of actions become quite easy because
    you just need to pick the action that has the highest Q-value. You can see the
    equation that you can use to obtain the highest Q-value in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Equation for choosing the maximum rewards using the Q-function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, π represents the policy. The policy indicates the rules of the game and
    the action. With the help of the policy, we can choose what kind of typical actions
    are available in each state. Our next step is to obtain this Q-function. For that,
    we need to concentrate on just one transition. This transition is made of four
    states: *< s, a, r, s'' >*. Remember the discount factor reward, where we can
    express the Q-value of the current state *s* and the current action *a* in terms
    of the Q-value of the next state *s''*. The equation for calculating rewards is
    provided in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: The bellman equation for calculation rewards'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding equation is called the Bellman equation, and it is the main idea
    behind the Q-learning algorithm. This equation is quite logical, and it indicates
    that the maximum future rewards for this state and action are the summation of
    the immediate rewards and the maximum future reward for the next state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main intuition is that with the help of the *n number of iterations followed
    by approximation* step, we can generate the values for the Q-function. We will
    achieve this by using the *Bellman equation*. In the simplest case, the Q-function
    is implemented in the form of a table where states are its rows and actions are
    its columns. The pseudo steps of this Q-learning algorithm are simple. You can
    take a look at them, as follows: at them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Initialize Q [number of states, number of actions] arbitrarily'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Observe initial states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Repeat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Until terminated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to follow these steps, where α is the learning rate. The learning rate
    verifies the difference between the previous Q-value and the newly proposed Q-value.
    This difference value is taken into account so that we can check when our model
    will converge. With the help of the learning rate, we can regulate the speed of
    training in such a way that our model won''t become too slow to converge or too
    fast to converge in a way that it cannot learn anything. We will be using *maxa''Q
    [s'', a'']* to update *Q [s, a]*In order to maximize the reward. This is the only
    operation that we need to perform. This estimation operation will give us the
    updated Q-value. In the early stages of training, when our agent is learning,
    there could be a situation where our estimations may go completely wrong, but
    the estimations and updated Q-values get more and more accurate with every iteration.
    If we perform this process enough times, then the Q-function will converge. It
    represents the true and optimized Q-value. For better understanding, we will implement
    the preceding algorithm. Refer to the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Code snippet for building and updating the Q-table'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output in the form of a Q-table in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the Q-Learning algorithm](img/B08394_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: Q-table value'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the implementation of the preceding algorithm by referring to this
    GitHub link: [https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's start implementing the game.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the basic version of the gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be implementing a simple game. I have already defined
    the rules of this game. Just to remind you quickly, our agent, yellow block tries
    to reach either the red block or the green block. If the agent reaches the green
    block, we will receive + 1 as a reward. If it reaches the red block, we get -1\.
    Each step the agent will take will be considered a - 0.04 reward. You can turn
    back the pages and refer to the section Rules for the game if you want. You can
    refer to the code for this basic version of a gaming bot by referring to this
    GitHub link: [https://github.com/jalajthanaki/Q_learning_for_simple_atari_game](https://github.com/jalajthanaki/Q_learning_for_simple_atari_game).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this game, the gaming world or the gaming environment is already built,
    so we do not need to worry about it. We need to include this gaming world by just
    using the import statement. The main script that we are running is `Lerner.py`.
    The code snippet for this code is given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: Code snippet for the basic version of the gaming bot - I'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code, we are keeping track of the agent''s
    states and actions with the help of the code given in loops. After that, we will
    define the four possible actions for this game, and based on that, we will calculate
    the reward values. We have also defined the `max_Q` function, which calculates
    the maximum Q value for us. You can also refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Code snippet for basic version of gaming bot - II'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding code snippet, the helper function uses the
    `inc_Q` method in order to update Q. By using the `run` function, we can update
    Q values so that our bot will learn how to achieve the best solution. You can
    run this script by executing this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the script, you can see the following output window, and within
    1-2 minutes, this bot will find the optimal solution. You can find the bot''s
    initial state and final state output in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: Output of the basic version of the gaming bot'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can track the progress of the bot by using the reward score. You can refer
    to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the basic version of the gaming bot](img/B08394_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: Tracking the progress of the gaming bot'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, during the initial iteration, the gaming bot didn't perform
    well after some iterations bot started learning how to take action based on the
    experience it gained. We stopped the code when there was no significant improvement
    in the reward scores. That is because our gaming bot was able to achieve the best
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's build a more complex gaming bot; we will be using a deep Q-network
    for training. So let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Space Invaders gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to build a gaming bot that can play Space Invaders. Most of you
    may have played this game or at least heard of it. If you haven''t played it or
    you can''t remember it at this moment, then take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the Space Invaders gaming bot](img/B08394_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12: Snippet of the Space Invaders game'
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully you remember the game now and how it was played. First, we will look
    at the concepts that we will be using to build this version of the gaming bot.
    Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this version of the gaming bot, we will be using the deep Q-network and
    training our bot. So before implementing this algorithm, we need to understand
    the concepts. Take a look at the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a deep Q-network (DQN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Experience Replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding a deep Q-network (DQN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep Q-network algorithm is basically a combination of two concepts. It
    uses the Q-learning logic for a deep neural network. That is the reason why it
    is called a deep Q-network (DQN).
  prefs: []
  type: TYPE_NORMAL
- en: Every gaming world has a different environment. So, say, Super Mario looks different
    from Space Invaders. We can't feed the entire gaming environment for an individual
    game every time, so first of all, we need to decide on the universal representation
    of all games so that we use them as input for the DQN algorithm. The screen pixels
    are the obvious choice for input because clearly they contain all the relevant
    information about the game world and its situation. Without the help of the screen
    pixels we cannot capture the speed and direction of the gaming agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we apply the same preprocessing steps to the game screens as mentioned in
    the DeepMind paper, then we need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We need to consider the last four screen images of the game as the
    input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: We need to resize them to 84 x 84 and convert them into grayscale with
    256 gray levels. That means we would have 256^(84x84x4), which is approximately
    10^(67970) possible gaming states. This means we have 10^(67970) rows in our imaginary
    Q-table, and that is a big number. You could argue that many pixel combinations
    or states never occur so we can possibly represent it as a sparse matrix. This
    sparse matrix contains only visited states. However, most of the states are rarely
    visited. So, it would take a long time for the Q-table to converge. Honestly,
    we would also like to take a good guess for Q-values for states we have never
    seen before by the agent so that we can generate a reasonably good action for
    the gaming agent. This is the point where deep learning enters the picture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Neural networks are quite good for generating good features for highly
    structured data. With the help of the neural network, we can represent our Q-function.
    This neural network takes the states, which means four game screens and actions,
    as input and generates the corresponding Q-value as output. *Alternatively, we
    could take only game screens as the input and generate the Q-value for each possible
    action as output.* This approach has a great advantage. Let me explain. There
    are two major things that we are doing here. First, we need to obtain the updated
    Q-value. Second, we need to pick up the action with the highest Q-value.'
  prefs: []
  type: TYPE_NORMAL
- en: So if we have Q-values for all possible actions, then we can update the Q-value
    easily. We can also pick the action with the highest Q-value with a lot of ease.
    The interesting part is that we can generate the Q-values for all actions by performing
    a forward pass through the network. After a single forward pass, we can have a
    list of Q-values for all possible actions with us. This forward pass will save
    a lot of time and give the gaming agent good rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of DQN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the optimal architecture of a deep Q-network represented in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of DQN](img/B08394_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.13: Architecture of DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding architecture is used and published in a DeepMind paper. The architecture
    for the neural network is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of DQN](img/B08394_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: The DQN architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The provided architecture uses a classic convolutional neural network (CNN).
    There are three convolutional layers followed by two fully connected layers that
    we have seen in the CNN architecture for object detection and face recognition
    CNN with pooling layers. Here, there are no pooling layers. That is because the
    main motive behind using pooling layers is that they make the neural network insensitive
    to the location. This means that if we use the pooling layer, then the placement
    of the objects in the image is not considered by the neural network. This kind
    of location insensitivity makes sense for a classification task, but for games,
    the location of the objects in a gaming environment is important. They help us
    determine the action as well as potential rewards, and we wouldn't want to discard
    this information. So, we are not using pooling layers here.
  prefs: []
  type: TYPE_NORMAL
- en: Steps for the DQN algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s see the steps for DQN algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input of network:* Four 84 x 84 grayscale game screen pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output of network:* As output, we will generate Q-values for each possible
    action. Q- values take any real number, which means it can be any real number
    you can possibly imagine, and that makes it a regression task. We know we can
    optimize the regression function with a simple squared error loss. The equation
    of the error loss is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Steps for the DQN algorithm](img/B08394_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.15: Equation for the error loss function'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q-table update step:* There''s the transition *< s, a, r, s'' >*, but this
    time, the rules for updating the Q-table are not the same as Q-learning. There
    are some changes. So, the steps for updating the Q-table are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We need to perform a feedforward pass for the current state *s* in
    order to get predicted Q-values for all actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Perform a feedforward pass for the next sate *s''* and calculate the
    maximum over all network output *maxa''Q(s'', a'').*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Set a Q-value target for action *a* to *r + γmaxa''Q(s'', a'')*. Here,
    we can use the *maxa''Q(s'', a'')* value that we have already calculated in step
    2\. For all other actions, set the Q-values that are originally from step 1, making
    the error zero for those outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: We need to update the weights of the neural network using backpropagation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's look at the concept of experience replay.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Experience Replay
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are estimating the future rewards in each state using two concepts. We use
    Q-learning and approximate the Q-function using a convolutional neural network.
    Here, the approximation of Q-values is done using a nonlinear function, and this
    function is not very stable for converging the model. So, we need to experiment
    with various hyperparameters. This takes a long time: almost a week on a single
    GPU to train the gaming bot.'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a concept called experience replay. During the training, all
    the experiences *< s, a, r, s' >* are stored in a replay memory. When we perform
    training, the network will use random samples from the replay memory instead of
    the most recent transition. This way, the training time will be less, plus there
    is another advantage. With the help of the experience replay, our training task
    will become more similar to the usual supervised learning. Now we can easily perform
    debugging and testing operations for the algorithm. With the help of the replay
    memory, we can store all our human experiences of gameplay and then train the
    model based on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the steps for the final Q-learning algorithm used in DQN will be as follows.
    This algorithm takes from the original DQN paper, which is available at [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: We need to initialize the replay memory D'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: We need to initialize the action-value function Q with random weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Observe value of the initial states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 4: Repeat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are using Q-learning and DQN to implement the Space Invaders gaming bot.
    So let's start coding.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Space Invaders gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be coding the Space Invaders game using DQN and Q-learning.
    For coding, we will be using the `gym`, `TensorFlow`, and `virtualenv` libraries.
    You can refer to the entire code by using this GitHub link: [https://github.com/jalajthanaki/SpaceInvaders_gamingbot](https://github.com/jalajthanaki/SpaceInvaders_gamingbot).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using a convolutional neural network (CNN). Here, we have defined the
    CNN in a separate file. The name of this file is `convnet.py`. Take a look at
    the following screenshot: at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.16: Code snippet for Convnrt.py'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code using this GitHub link: [https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py](https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are defining the DQN algorithm in the `dqn.py` script. You can refer to
    the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.17: Code snippet for dqn.py'
  prefs: []
  type: TYPE_NORMAL
- en: 'For training, we have defined our training logic in `train.py`. You can refer
    to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.18: Code snippet for train.py'
  prefs: []
  type: TYPE_NORMAL
- en: 'At last, we import all these separate scripts to the main `atari.py` script,
    and in that script, we define all the parameter values. You can refer to the code
    snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.19: Code snippet for atari.py'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start training by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ python atari.py --game SpaceInvaders-v0 --display true`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training this bot to pass human level performance requires at least 3-4 days
    of training. I have not provided that amount of training, but you can definitely
    do that. You can take a look at the output of the training in the following screenshot:
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.20: Output snippet of training step – I'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet for the gaming environment initial score
    by referring to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.21: Code snippet of the score for the initial few games from the
    gaming bot'
  prefs: []
  type: TYPE_NORMAL
- en: 'To stop the training, there will be two parameters: either we can end our training
    when our loss function value becomes constant for a few iterations, or we complete
    all the training steps. Here, we have defined 50,000 training steps. You can refer
    to the code snippet of the output of training in the following screenshot :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.22: Code snippet for the training log'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the score of the gaming bot after 1,000 iterations by taking a
    look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the Space Invaders gaming bot](img/B08394_11_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.23: Code snippet for the gaming bot after 1,000 iterations'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have already upload the pre-trained model for you. You can download it by
    using this GitHub link: [https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model](https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model).'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to build the gaming bot for the Pong game. If you train this bot
    for a week using a single GPU, it can beat the AI rules that are written by the
    gaming manufacture team. So, our agent will surely act better than the computer
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Pong gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be looking at how we can build a gaming bot that can
    learn the game of Pong. Before we start, we will look at the approach and concepts
    that we will be using for building the Pong gaming bot.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will be covering some aspects of building the Pong game
    bot, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approach for the gaming bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of the gaming bot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to develop the Pong gaming bot, we are choosing a neural-network-based
    approach. The architecture of our neural network is crucial. Let''s look at the
    architectural components step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: We take the gaming screen as the input and preprocess it as per the DQN algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We pass this preprocessed screen to an neural network (NN.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a gradient descent to update the weights of the NN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Weight [1]: This matrix holds the weights of pixels passing into the hidden
    layer. The dimension will be [200 x 80 x 80] – [200 x 6400].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Weight [2]: This matrix holds the weights of the hidden layer passing into
    the output. The dimension will be [1 x 200].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can refer to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of the gaming bot](img/B08394_11_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.24: Architecture of NN for the Pong gaming bot'
  prefs: []
  type: TYPE_NORMAL
- en: The tasks for each component of the NN make more sense when we see the detailed
    approach for this gaming bot.
  prefs: []
  type: TYPE_NORMAL
- en: Approach for the gaming bot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to build the Pong gaming bot, we will be using the following approach:'
  prefs: []
  type: TYPE_NORMAL
- en: For implementation, we are using the preprocessed image vector, which is a [6400
    x 1] dimension array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of NN, we can compute a probability of moving up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of that probability distribution, we will decide whether the agent
    is moving up or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the gaming round is over, it means that the gaming agent as well as the opponent
    missed the ball. In this case, we need to find out whether our gaming agent won
    or lost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the episode finishes, which means if either of the players scores 21 points,
    we need to pass the result. With the help of the loss function, we can find out
    the error values. We applied the gradient descent algorithm to find out the direction
    in which our neural network's weight should be updated. Based on the backpropagation
    algorithm, we propagate the error back to the network so that our network can
    update the weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once 10 episodes have finished, we need to sum up the gradient, and after that,
    we update the weights in the direction of the gradient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat this process until our networks weights are tuned and we can beat the
    computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's cover the coding steps.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Pong gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These are the implementation steps that we need to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of the parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights stored in the form of matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to move the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the process using NN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the entire code by using this GitHub link: [https://github.com/jalajthanaki/Atari_Pong_gaming_bot](https://github.com/jalajthanaki/Atari_Pong_gaming_bot).'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of the parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we define and initialize our parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size:` This parameter indicates how many rounds of games we should play
    before updating the weights of our network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma:` This is the discount factor. We use this to discount the effect of
    old actions of the game on the final result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decay_rate:` This parameter is used to update the weight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layer_neurons:` This parameter indicates how many neurons we should
    put in the hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate:` This is the speed at which our gaming agent learns from the
    results so that we can compute new weights. A higher learning rate means we react
    more strongly to results, and a lower rate means we don''t react much to each
    result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Initialization of the parameters](img/B08394_11_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.25: Initialization of parameters'
  prefs: []
  type: TYPE_NORMAL
- en: Weights stored in the form of matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The weights of the neural network are stored in the form of matrices. The first
    layer of NN is a 200 x 6400 matrix that represents the weights for our hidden
    layer. If we use the notation *w1_ij*, then that would mean that we are representing
    the weight of the *i^(th)* neuron for the input pixel *j* in layer 1\. The second
    layer is a 200 x 1 matrix representing the weights. These weights are the output
    of the hidden layer. For layer 2, element *w2_i* indicates the weights placed
    on the activation of the *i^(th)* neuron in the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weights stored in the form of matrices](img/B08394_11_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.26: Weight matrices'
  prefs: []
  type: TYPE_NORMAL
- en: Updating weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For updating the weight, we will be using RMSprop. You can refer to this paper
    in order to understand more details about this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop](http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop).
    Refer to the following figure for the following figure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Updating weights](img/B08394_11_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.27: Equation for RMSprop'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Updating weights](img/B08394_11_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.28: Code snippet for updating weights'
  prefs: []
  type: TYPE_NORMAL
- en: How to move the agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the help of the preprocessed input, we pass the weight matrix to the neural
    network. We need to generate the probability of telling our agent to move up.
    You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to move the agent](img/B08394_11_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.29: Code snippet to move the agent'
  prefs: []
  type: TYPE_NORMAL
- en: We are done with all the major helper functions. We need to apply all this logic
    to the neural network so that it can take the observation and generate the probability
    of our gaming agent for going in upward direction.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the process using NN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are the steps that can help us generate the probability for our agent
    so that it can decide when they should move in upward direction
  prefs: []
  type: TYPE_NORMAL
- en: We need to compute hidden layer values by applying the dot product between weights
    [1] and `observation_matrix`. Weight [1] is a 200 x 6400 matrix and `observation_matrix`
    is a 6400 x 1 matrix. The dimension of the output matrix is 200 x 1\. Here, we
    are using 200 neurons. Each row of Q-function represents the output of one neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We apply a nonlinear function ReLU to the hidden layer values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are using hidden layer activation values in order to calculate the values
    for the output layer. Again, we performed dot product between `hidden_layer_values`
    [200 x 1] and weights [2] [1 x 200]. This dot product gives us single value [1
    x 1].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we apply the sigmoid function to the output value. This will give us
    the answer in terms of probability. The value of the output is between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the process using NN](img/B08394_11_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.30: Code snippet for the process happens using NN'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this code, you need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to build a bot that can beat the computer, then you need to train
    it for at least three to four days on a single GPU. You can refer to the output
    of the bot in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the process using NN](img/B08394_11_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.31: The Pong gaming bot output'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the training log in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the process using NN](img/B08394_11_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.32: Training log for Pong gaming bot'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's build a gaming bot just for fun. This bot uses the Flappy Bird gaming
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Just for fun - implementing the Flappy Bird gaming bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be building the Flappy Bird gaming bot. This gaming
    bot has been built using DQN. You can find the entire code at this GitHub link:
    [https://github.com/jalajthanaki/DQN_FlappyBird](https://github.com/jalajthanaki/DQN_FlappyBird).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This bot has a pre-trained model, so you test it using the pre-trained model.
    In order to run this bot, you need to execute this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the output in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Just for fun - implementing the Flappy Bird gaming bot](img/B08394_11_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.33: Output of the Flappy Bird gaming bot'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the combination of all the concepts that we have studied so far
    in this implementation, so make sure you explore this code. Consider this your
    exercise for the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Congratulations, readers; you have made it to the end! We covered basic concepts
    related to reinforcement learning in this chapter. You learned about the various
    concepts and algorithms of building the gaming bot. You also learned how the Deep
    Q Learner algorithm works. Using the `gym` library, we loaded the gaming world.
    By using the `dqn` library, we will be able to train the model. Training a gaming
    bot that can defeat human level experts takes a lot of time. So, I trained it
    for a few hours only. If you want to train for more hours, you can definitely
    do that. We tried to build a variety of simple Atari games, such as a simple pathfinder
    gaming bot, Space Invaders, Pong, and Flappy Bird. You can expand this basic approach
    to the bigger gaming environment. If you want to get yourself updated and contribute,
    then you can take a look at the OpenAI GitHub repository at: [https://github.com/openai](https://github.com/openai).
    Deep Mind news and the blog section are at this link: [https://deepmind.com/blog/](https://deepmind.com/blog/)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, you will find an appendix that can help you gain some
    extra information. This extra information will help you when you are building
    Machine Learning (ML) applications or taking part in a hackathon or other competitions.
    I have also provided some cheat sheets that can help you when you are building
    ML applications.
  prefs: []
  type: TYPE_NORMAL
