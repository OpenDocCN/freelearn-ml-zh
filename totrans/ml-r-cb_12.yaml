- en: Chapter 12. Big Data Analysis (R and Hadoop)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the RHadoop environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing rmr2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing rhdfs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating HDFS with rhdfs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a word count problem with RHadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the performance between an R MapReduce program and a standard R program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and debugging the rmr2 program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing plyrmr
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating data with plyrmr
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting machine learning with RHadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring RHadoop clusters on Amazon EMR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RHadoop is a collection of R packages that enables users to process and analyze
    big data with Hadoop. Before understanding how to set up RHadoop and put it in
    to practice, we have to know why we need to use machine learning to big-data scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we have mentioned how useful R is when performing
    data analysis and machine learning. In traditional statistical analysis, the focus
    is to perform analysis on historical samples (small data), which may ignore rarely
    occurring but valuable events and results to uncertain conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: The emergence of Cloud technology has made real-time interaction between customers
    and businesses much more frequent; therefore, the focus of machine learning has
    now shifted to the development of accurate predictions for various customers.
    For example, businesses can provide real-time personal recommendations or online
    advertisements based on personal behavior via the use of a real-time prediction
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the data (for example, behaviors of all online users) is too large
    to fit in the memory of a single machine, you have no choice but to use a supercomputer
    or some other scalable solution. The most popular scalable big-data solution is
    Hadoop, which is an open source framework able to store and perform parallel computations
    across clusters. As a result, you can use RHadoop, which allows R to leverage
    the scalability of Hadoop, helping to process and analyze big data. In RHadoop,
    there are five main packages, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rmr`: This is an interface between R and Hadoop MapReduce, which calls the
    Hadoop streaming MapReduce API to perform MapReduce jobs across Hadoop clusters.
    To develop an R MapReduce program, you only need to focus on the design of the
    map and reduce functions, and the remaining scalability issues will be taken care
    of by Hadoop itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rhdfs`: This is an interface between R and HDFS, which calls the HDFS API
    to access the data stored in HDFS. The use of `rhdfs` is very similar to the use
    of the Hadoop shell, which allows users to manipulate HDFS easily from the R console.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rhbase`: This is an interface between R and HBase, which accesses Hbase and
    is distributed in clusters through a Thrift server. You can use `rhbase` to read/write
    data and manipulate tables stored within HBase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plyrmr`: This is a higher-level abstraction of MapReduce, which allows users
    to perform common data manipulation in a plyr-like syntax. This package greatly
    lowers the learning curve of big-data manipulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ravro`: This allows users to read `avro` files in R, or write `avro` files.
    It allows R to exchange data with HDFS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will start by preparing the Hadoop environment, so that
    you can install RHadoop. We then cover the installation of three main packages:
    `rmr`, `rhdfs`, and `plyrmr`. Next, we will introduce how to use `rmr` to perform
    MapReduce from R, operate an HDFS file through `rhdfs`, and perform a common data
    operation using `plyrmr`. Further, we will explore how to perform machine learning
    using RHadoop. Lastly, we will introduce how to deploy multiple RHadoop clusters
    on Amazon EC2.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the RHadoop environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As RHadoop requires an R and Hadoop integrated environment, we must first prepare
    an environment with both R and Hadoop installed. Instead of building a new Hadoop
    system, we can use the **Cloudera QuickStart VM** (the VM is free), which contains
    a single node Apache Hadoop Cluster and R. In this recipe, we will demonstrate
    how to download the Cloudera QuickStart VM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use the Cloudera QuickStart VM, it is suggested that you should prepare a
    64-bit guest OS with either VMWare or VirtualBox, or the KVM installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you choose to use VMWare, you should prepare a player compatible with WorkStation
    8.x or higher: Player 4.x or higher, ESXi 5.x or higher, or Fusion 4.x or higher.'
  prefs: []
  type: TYPE_NORMAL
- en: Note, 4 GB of RAM is required to start VM, with an available disk space of at
    least 3 GB.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to set up a Hadoop environment using the Cloudera
    QuickStart VM:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit the Cloudera QuickStart VM download site (you may need to update the link
    as Cloudera upgrades its VMs , the current version of CDH is 5.3) at [http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html](http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html).![How
    to do it...](img/00267.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A screenshot of the Cloudera QuickStart VM download site
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Depending on the virtual machine platform installed on your OS, choose the
    appropriate link (you may need to update the link as Cloudera upgrades its VMs)
    to download the VM file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**To download VMWare**: You can visit [https://downloads.cloudera.com/demo_vm/vmware/cloudera-quickstart-vm-5.2.0-0-vmware.7z](https://downloads.cloudera.com/demo_vm/vmware/cloudera-quickstart-vm-5.2.0-0-vmware.7z)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To download KVM**: You can visit [https://downloads.cloudera.com/demo_vm/kvm/cloudera-quickstart-vm-5.2.0-0-kvm.7z](https://downloads.cloudera.com/demo_vm/kvm/cloudera-quickstart-vm-5.2.0-0-kvm.7z)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**To download VirtualBox**: You can visit [https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.2.0-0-virtualbox.7z](https://downloads.cloudera.com/demo_vm/virtualbox/cloudera-quickstart-vm-5.2.0-0-virtualbox.7z)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you can start the QuickStart VM using the virtual machine platform installed
    on your OS. You should see the desktop of Centos 6.2 in a few minutes.![How to
    do it...](img/00268.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The screenshot of Cloudera QuickStart VM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can then open a terminal and type `hadoop`, which will display a list of
    functions that can operate a Hadoop cluster.![How to do it...](img/00269.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The terminal screenshot after typing `hadoop`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open a terminal and type `R`. Access an R session and check whether version
    3.1.1 is already installed in the Cloudera QuickStart VM. If you cannot find R
    installed in the VM, please use the following command to install R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of building a Hadoop system on your own, you can use the Hadoop VM application
    provided by Cloudera (the VM is free). The QuickStart VM runs on CentOS 6.2 with
    a single node Apache Hadoop cluster, Hadoop Ecosystem module, and R installed.
    This helps you to save time, instead of requiring you to learn how to install
    and use Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The QuickStart VM requires you to have a computer with a 64-bit guest OS, at
    least 4 GB of RAM, 3 GB of disk space, and either VMWare, VirtualBox, or KVM installed.
    As a result, you may not be able to use this version of VM on some computers.
    As an alternative, you could consider using Amazon's Elastic MapReduce instead.
    We will illustrate how to prepare a RHadoop environment in EMR in the last recipe
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Cloudera QuickStart VM is simple. Download the VM from the download
    site and then open the built image with either VMWare, VirtualBox, or KVM. Once
    you can see the desktop of CentOS, you can then access the terminal and type `hadoop`
    to see whether Hadoop is working; then, type `R` to see whether R works in the
    QuickStart VM.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides using the Cloudera QuickStart VM, you may consider using a Sandbox VM
    provided by Hontonworks or MapR. You can find Hontonworks Sandbox at [http://hortonworks.com/products/hortonworks-sandbox/#install](http://hortonworks.com/products/hortonworks-sandbox/#install)
    and mapR Sandbox at [https://www.mapr.com/products/mapr-sandbox-hadoop/download](https://www.mapr.com/products/mapr-sandbox-hadoop/download).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing rmr2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `rmr2` package allows you to perform big data processing and analysis via
    MapReduce on a Hadoop cluster. To perform MapReduce on a Hadoop cluster, you have
    to install R and `rmr2` on every task node. In this recipe, we will illustrate
    how to install `rmr2` on a single node of a Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have completed the previous recipe by starting the Cloudera
    QuickStart VM and connecting the VM to the Internet, so that you can proceed with
    downloading and installing the `rmr2` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to install `rmr2` on the QuickStart VM:'
  prefs: []
  type: TYPE_NORMAL
- en: First, open the terminal within the Cloudera QuickStart VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the permission of the root to enter an R session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then install dependent packages before installing `rmr2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Quit the R session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can download `rmr-3.3.0` to the QuickStart VM. You may need to update
    the link if Revolution Analytics upgrades the version of `rmr2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then install `rmr-3.3.0` to the QuickStart VM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can enter an R session and use the `library` function to test whether
    the library has been successfully installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to perform MapReduce on a Hadoop cluster, you have to install R and
    RHadoop on every task node. Here, we illustrate how to install `rmr2` on a single
    node of a Hadoop cluster. First, open the terminal of the Cloudera QuickStart
    VM. Before installing `rmr2`, we first access an R session with root privileges
    and install dependent R packages.
  prefs: []
  type: TYPE_NORMAL
- en: Next, after all the dependent packages are installed, quit the R session and
    use the `wget` command in the Linux shell to download `rmr-3.3.0` from GitHub
    to the local filesystem. You can then begin the installation of `rmr2`. Lastly,
    you can access an R session and use the library function to validate whether the
    package has been installed.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To see more information and read updates about RHadoop, you can refer to the
    RHadoop wiki page hosted on GitHub: [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing rhdfs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `rhdfs` package is the interface between R and HDFS, which allows users
    to access HDFS from an R console. Similar to `rmr2`, one should install `rhdfs`
    on every task node, so that one can access HDFS resources through R. In this recipe,
    we will introduce how to install `rhdfs` on the Cloudera QuickStart VM.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have completed the previous recipe by starting the Cloudera
    QuickStart VM and connecting the VM to the Internet, so that you can proceed with
    downloading and installing the `rhdfs` package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to install `rhdfs`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can download `rhdfs 1.0.8` from GitHub. You may need to update the
    link if Revolution Analytics upgrades the version of `rhdfs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can install `rhdfs` under the command-line mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then set up `JAVA_HOME`. The configuration of `JAVA_HOME` depends on
    the installed Java version within the VM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Last, you can set up the system environment and initialize `rhdfs`. You may
    need to update the environment setup if you use a different version of QuickStart
    VM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The package, `rhdfs`, provides functions so that users can manage HDFS using
    R. Similar to `rmr2`, you should install `rhdfs` on every task node, so that one
    can access HDFS through the R console.
  prefs: []
  type: TYPE_NORMAL
- en: To install `rhdfs`, you should first download `rhdfs` from GitHub. You can then
    install `rhdfs` in R by specifying where the `HADOOP_CMD` is located. You must
    configure R with Java support through the command, `javareconf`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can access R and configure where `HADOOP_CMD` and `HADOOP_STREAMING`
    are located. Lastly, you can initialize `rhdfs` via the `rhdfs.init` function,
    which allows you to begin operating HDFS through `rhdfs`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To find where `HADOOP_CMD` is located, you can use the `which hadoop` command
    in the Linux shell. In most Hadoop systems, `HADOOP_CMD` is located at `/usr/bin/hadoop`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for the location of `HADOOP_STREAMING`, the streaming JAR file is often
    located in `/usr/lib/hadoop-mapreduce/`. However, if you cannot find the directory,
    `/usr/lib/Hadoop-mapreduce`, in your Linux system, you can search the streaming
    JAR by using the `locate` command. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Operating HDFS with rhdfs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `rhdfs` package is an interface between Hadoop and R, which can call an
    HDFS API in the backend to operate HDFS. As a result, you can easily operate HDFS
    from the R console through the use of the `rhdfs` package. In the following recipe,
    we will demonstrate how to use the `rhdfs` function to manipulate HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To proceed with this recipe, you need to have completed the previous recipe
    by installing `rhdfs` into R, and validate that you can initial HDFS via the `hdfs.init`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to operate files stored on HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the `rhdfs` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then manipulate files stored on HDFS, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`hdfs.put`: Copy a file from the local filesystem to HDFS:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.ls`: Read the list of directory from HDFS:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.copy`: Copy a file from one HDFS directory to another:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.move` : Move a file from one HDFS directory to another:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.delete`: Delete an HDFS directory from R:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.rm`: Delete an HDFS directory from R:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.get`: Download a file from HDFS to a local filesystem:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.rename`: Rename a file stored on HDFS:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.chmod`: Change the permissions of a file or directory:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`hdfs.file.info`: Read the meta information of the HDFS file:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, you can write stream to the HDFS file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can read stream from the HDFS file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to manipulate HDFS using the `rhdfs` package.
    Normally, you can use the Hadoop shell to manipulate HDFS, but if you would like
    to access HDFS from R, you can use the `rhdfs` package.
  prefs: []
  type: TYPE_NORMAL
- en: Before you start using `rhdfs`, you have to initialize `rhdfs` with `hdfs.init()`.
    After initialization, you can operate HDFS through the functions provided in the
    `rhdfs` package.
  prefs: []
  type: TYPE_NORMAL
- en: Besides manipulating HDFS files, you can exchange streams to HDFS through `hdfs.read`
    and `hdfs.write`. We, therefore, demonstrate how to write a data frame in R to
    an HDFS file, `iris.txt`, using `hdfs.write`. Lastly, you can recover the written
    file back to the data frame using the `hdfs.read` function and the `unserialize`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To initialize `rhdfs`, you have to set `HADOOP_CMD` and `HADOOP_STREAMING` in
    the system environment. Instead of setting the configuration each time you're
    using `rhdfs`, you can put the configurations in the `.rprofile` file. Therefore,
    every time you start an R session, the configuration will be automatically loaded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a word count problem with RHadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate how MapReduce works, we illustrate the example of a word count,
    which counts the number of occurrences of each word in a given input set. In this
    recipe, we will demonstrate how to use `rmr2` to implement a word count problem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will need an input file as our word count program input.
    You can download the example input from [https://github.com/ywchiu/ml_R_cookbook/tree/master/CH12](https://github.com/ywchiu/ml_R_cookbook/tree/master/CH12).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement the word count program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to configure the system environment, and then load `rmr2` and
    `rhdfs` into an R session. You may need to update the use of the JAR file if you
    use a different version of QuickStart VM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then create a directory on HDFS and put the input file into the newly
    created directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can create a `map` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `reduce` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `MapReduce` program to count the words within a document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can retrieve the top 10 occurring words within the document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to implement a word count using the `rmr2`
    package. First, we need to configure the system environment and load `rhdfs` and
    `rmr2` into R. Then, we specify the input of our word count program from the local
    filesystem into the HDFS directory, `/user/cloudera/wordcount/data`, via the `hdfs.put`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we begin implementing the MapReduce program. Normally, we can divide the
    MapReduce program into the `map` and `reduce` functions. In the `map` function,
    we first use the `strsplit` function to split each line into words. Then, as the
    `strsplit` function returns a list of words, we can use the `unlist` function
    to character vectors. Lastly, we can return key-value pairs with each word as
    a key and the value as one. As the `reduce` function receives the key-value pair
    generated from the `map` function, the `reduce` function sums the count and returns
    the number of occurrences of each word (or key).
  prefs: []
  type: TYPE_NORMAL
- en: After we have implemented the `map` and `reduce` functions, we can submit our
    job via the `mapreduce` function. Normally, the `mapreduce` function requires
    four inputs, which are the HDFS input path, the HDFS output path, the map function,
    and the reduce function. In this case, we specify the input as `wordcount/data`,
    output as `wordcount/out`, mapfunction as `map`, reduce function as `reduce`,
    and wrap the `mapreduce` call in function, `wordcount`. Lastly, we call the function,
    `wordcount` and store the output path in the variable, `out`.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `from.dfs` function to load the HDFS data into the `results`
    variable, which contains the mapping of words and number of occurrences. We can
    then generate the top 10 occurring words from the `results` variable.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to write an R MapReduce program to solve
    a word count problem. However, if you are interested in how to write a native
    Java MapReduce program, you can refer to [http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html](http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the performance between an R MapReduce program and a standard R program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Those not familiar with how Hadoop works may often see Hadoop as a remedy for
    big data processing. Some might believe that Hadoop can return the processed results
    for any size of data within a few milliseconds. In this recipe, we will compare
    the performance between an R MapReduce program and a standard R program to demonstrate
    that Hadoop does not perform as quickly as some may believe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you should have completed the previous recipe by installing
    `rmr2` into the R environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to compare the performance of a standard R program
    and an R MapReduce program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can implement a standard R program to have all numbers squared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To compare the performance, you can implement an R MapReduce program to have
    all numbers squared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implement two programs to square all the numbers. In the
    first program, we use a standard R function, `sapply`, to square the sequence
    from 1 to 100,000\. To record the program execution time, we first record the
    processing time before the execution in `a.time`, and then subtract `a.time` from
    the current processing time after the execution. Normally, the execution takes
    no more than 10 seconds. In the second program, we use the `rmr2` package to implement
    a program in the R MapReduce version. In this program, we also record the execution
    time. Normally, this program takes a few minutes to complete a task.
  prefs: []
  type: TYPE_NORMAL
- en: The performance comparison shows that a standard R program outperforms the MapReduce
    program when processing small amounts of data. This is because a Hadoop system
    often requires time to spawn daemons, job coordination between daemons, and fetching
    data from data nodes. Therefore, a MapReduce program often takes a few minutes
    to a couple of hours to finish the execution. As a result, if you can fit your
    data in the memory, you should write a standard R program to solve the problem.
    Otherwise, if the data is too large to fit in the memory, you can implement a
    MapReduce solution.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to check whether a job will run smoothly and efficiently in Hadoop,
    you can run a MapReduce benchmark, MRBench, to evaluate the performance of the
    job:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Testing and debugging the rmr2 program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since running a MapReduce program will require a considerable amount of time,
    varying from a few minutes to several hours, testing and debugging become very
    important. In this recipe, we will illustrate some techniques you can use to troubleshoot
    an R MapReduce program.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you should have completed the previous recipe by installing
    `rmr2` into an R environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to test and debug an R MapReduce program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you can configure the backend as local in `rmr.options`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, you can execute the number squared MapReduce program mentioned in the
    previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition to this, if you want to print the structure information of any
    variable in the MapReduce program, you can use the `rmr.str` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we introduced some debugging and testing techniques you can
    use while implementing the MapReduce program. First, we introduced the technique
    to test a MapReduce program in a local mode. If you would like to run the MapReduce
    program in a pseudo distributed or fully distributed mode, it would take you a
    few minutes to several hours to complete the task, which would involve a lot of
    wastage of time while troubleshooting your MapReduce program. Therefore, you can
    set the backend to the local mode in `rmr.options` so that the program will be
    executed in the local mode, which takes lesser time to execute.
  prefs: []
  type: TYPE_NORMAL
- en: Another debugging technique is to list the content of the variable within the
    `map` or `reduce` function. In an R program, you can use the `str` function to
    display the compact structure of a single variable. In `rmr2`, the package also
    provides a function named `rmr.str`, which allows you to print out the content
    of a single variable onto the console. In this example, we use `rmr.str` to print
    the content of variables within a MapReduce program.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For those who are interested in the `option` settings for the `rmr2` package,
    you can refer to the help document of `rmr.options`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Installing plyrmr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `plyrmr` package provides common operations (as found in `plyr` or `reshape2`)
    for users to easily perform data manipulation through the MapReduce framework.
    In this recipe, we will introduce how to install `plyrmr` on the Hadoop system.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have completed the previous recipe by starting the Cloudera
    QuickStart VM and connecting the VM to the Internet. Also, you need to have the
    `rmr2` package installed beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to install `plyrmr` on the Hadoop system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you should install `libxml2-devel` and `curl-devel` in the Linux shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then access R and install the dependent packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can download `plyrmr 0.5.0` and install it on Hadoop VM. You may
    need to update the link if Revolution Analytics upgrades the version of `plyrmr`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, validate the installation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides writing an R MapReduce program using the `rmr2` package, you can use
    the `plyrmr` to manipulate data. The `plyrmr` package is similar to hive and pig
    in the Hadoop ecosystem, which is the abstraction of the MapReduce program. Therefore,
    we can implement an R MapReduce program in `plyr` style instead of implementing
    the `map` f and `reduce` functions.
  prefs: []
  type: TYPE_NORMAL
- en: To install `plyrmr`, first install the package of `libxml2-devel` and `curl-devel`,
    using the `yum install` command. Then, access R and install the dependent packages.
    Lastly, download the file from GitHub and install `plyrmr` in R.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To read more information about `plyrmr`, you can use the `help` function to
    refer to the following document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Manipulating data with plyrmr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While writing a MapReduce program with `rmr2` is much easier than writing a
    native Java version, it is still hard for nondevelopers to write a MapReduce program.
    Therefore, you can use `plyrmr`, a high-level abstraction of the MapReduce program,
    so that you can use plyr-like operations to manipulate big data. In this recipe,
    we will introduce some operations you can use to manipulate data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you should have completed the previous recipes by installing
    `plyrmr` and `rmr2` in R.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to manipulate data with `plyrmr`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to load both `plyrmr` and `rmr2` into R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then set the execution mode to the local mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, load the Titanic dataset into R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Begin the operation by filtering the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also use a pipe operator to filter the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Put the Titanic data into HDFS and load the path of the data to the variable,
    `tidata`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you can generate a summation of the frequency from the Titanic data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also group the frequency by sex:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then sample 10 records out of the population:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition to this, you can use plyrmr to join two datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we introduce how to use `plyrmr` to manipulate data. First,
    we need to load the `plyrmr` package into R. Then, similar to `rmr2`, you have
    to set the backend option of `plyrmr` as the local mode. Otherwise, you will have
    to wait anywhere between a few minutes to several hours if `plyrmr` is running
    on Hadoop mode (the default setting).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can begin the data manipulation with data filtering. You can choose
    to call the function nested inside the other function call in step 4\. On the
    other hand, you can use the pipe operator, `%|%`, to chain multiple operations.
    Therefore, we can filter data similar to step 4, using pipe operators in step
    5.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can input the dataset into either the HDFS or local filesystem, using
    `to.dfs` in accordance with the current running mode. The function will generate
    the path of the dataset and save it in the variable, `tidata`. By knowing the
    path, you can access the data using the `input` function. Next, we illustrate
    how to generate a summation of the frequency from the Titanic dataset with the
    `transmute` and `sum` functions. Also, `plyrmr` allows users to sum up the frequency
    by gender.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in order to sample data from a population, you can also use the
    `sample` function to select 10 records out of the Titanic dataset. Lastly, we
    demonstrate how to join two datasets using the `merge` function from `plyrmr`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we list some functions that can be used to manipulate data with `plyrmr`.
    You may refer to the `help` function for further details on their usage and functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data manipulation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bind.cols`: This adds new columns'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select`: This is used to select columns'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`where`: This is used to select rows'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transmute`: This uses all of the above plus their summaries'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From `reshape2`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`melt` and `dcast`: It converts long and wide data frames'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Summary:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quantile`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extract:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top.k`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bottom.k`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting machine learning with RHadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we have demonstrated how powerful R is when used to
    solve machine learning problems. Also, we have shown that the use of Hadoop allows
    R to process big data in parallel. At this point, some may believe that the use
    of RHadoop can easily solve machine learning problems of big data via numerous
    existing machine learning packages. However, you cannot use most of these to solve
    machine learning problems as they cannot be executed in the MapReduce mode. In
    the following recipe, we will demonstrate how to implement a MapReduce version
    of linear regression and compare this version with the one using the `lm` function.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you should have completed the previous recipe by installing
    `rmr2` into the R environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to implement a linear regression in MapReduce:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the `cats` dataset from the `MASS` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then generate a linear regression model by calling the `lm` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now make a regression plot with the given data points and model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![How to do it...](img/00270.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Linear regression plot of cats dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load `rmr2` into R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then set up `X` and `y` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a `Sum` function to sum up the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute `Xtx` in MapReduce, `Job1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then compute `Xty` in MapReduce, `Job2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can derive the coefficient from `XtX` and `Xty`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to implement linear logistic regression in
    a MapReduce fashion in R. Before we start the implementation, we review how traditional
    linear models work. We first retrieve the `cats` dataset from the `MASS` package.
    We then load `X` as the body weight (`Bwt`) and `y` as the heart weight (`Hwt`).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we begin to fit the data into a linear regression model using the `lm`
    function. We can then compute the fitted model and obtain the summary of the model.
    The summary shows that the coefficient is 4.0341 and the intercept is -0.3567\.
    Furthermore, we draw a scatter plot in accordance with the given data points and
    then draw a regression line on the plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we cannot perform linear regression using the `lm` function in the MapReduce
    form, we have to rewrite the regression model in a MapReduce fashion. Here, we
    would like to implement a MapReduce version of linear regression in three steps,
    which are: calculate the `Xtx` value with the MapReduce, job1, calculate the `Xty`
    value with MapReduce, `job2`, and then derive the coefficient value:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, we pass the matrix, `X`, as the input to the `map` function.
    The `map` function then calculates the cross product of the transposed matrix,
    `X`, and, `X`. The `reduce` function then performs the sum operation defined in
    the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second step, the procedure of calculating `Xty` is similar to calculating
    `XtX`. The procedure calculates the cross product of the transposed matrix, `X,`
    and, `y`. The `reduce` function then performs the sum operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, we use the `solve` function to derive the coefficient, which is 3.907113.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the results show, the coefficients computed by `lm` and MapReduce differ
    slightly. Generally speaking, the coefficient computed by the `lm` model is more
    accurate than the one calculated by MapReduce. However, if your data is too large
    to fit in the memory, you have no choice but to implement linear regression in
    the MapReduce version.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can access more information on machine learning algorithms at: [https://github.com/RevolutionAnalytics/rmr2/tree/master/pkg/tests](https://github.com/RevolutionAnalytics/rmr2/tree/master/pkg/tests)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring RHadoop clusters on Amazon EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have only demonstrated how to run a RHadoop program in a single
    Hadoop node. In order to test our RHadoop program on a multi-node cluster, the
    only thing you need to do is to install RHadoop on all the task nodes (nodes with
    either task tracker for mapreduce version 1 or node manager for map reduce version
    2) of Hadoop clusters. However, the deployment and installation is time consuming.
    On the other hand, you can choose to deploy your RHadoop program on Amazon EMR,
    so that you can deploy multi-node clusters and RHadoop on every task node in only
    a few minutes. In the following recipe, we will demonstrate how to configure RHadoop
    cluster on an Amazon EMR service.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, you must register and create an account on AWS, and you also
    must know how to generate a EC2 key-pair before using Amazon EMR.
  prefs: []
  type: TYPE_NORMAL
- en: For those who seek more information on how to start using AWS, please refer
    to the tutorial provided by Amazon at [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to configure RHadoop on Amazon EMR:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you can access the console of the Amazon Web Service (refer to [https://us-west-2.console.aws.amazon.com/console/](https://us-west-2.console.aws.amazon.com/console/))
    and find EMR in the analytics section. Then, click on **EMR**.![How to do it...](img/00271.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access EMR service from AWS console.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should find yourself in the cluster list of the EMR dashboard (refer to
    [https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list::](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list::));
    click on **Create cluster**.![How to do it...](img/00272.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster list of EMR
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, you should find yourself on the **Create Cluster** page (refer to [https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#create-cluster:](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#create-cluster:)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you should specify **Cluster name** and **Log folder S3 location** in
    the cluster configuration.![How to do it...](img/00273.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster configuration in the create cluster page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can then configure the Hadoop distribution on **Software Configuration**.![How
    to do it...](img/00274.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the software and applications
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, you can configure the number of nodes within the Hadoop cluster.![How
    to do it...](img/00275.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the hardware within Hadoop cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can then specify the EC2 key-pair for the master node login.![How to do
    it...](img/00276.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Security and access to the master node of the EMR cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To set up RHadoop, one has to perform bootstrap actions to install RHadoop
    on every task node. Please write a file named `bootstrapRHadoop.sh`, and insert
    the following lines within the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should upload `bootstrapRHadoop.sh` to `S3`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You now need to add the bootstrap action with `Custom action`, and add `s3://<location>/bootstrapRHadoop.sh`
    within the S3 location.![How to do it...](img/00277.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the bootstrap action
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, you can click on **Create cluster** to launch the Hadoop cluster.![How
    to do it...](img/00278.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lastly, you should see the master public DNS when the cluster is ready. You
    can now access the terminal of the master node with your EC2-key pair:![How to
    do it...](img/00279.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A screenshot of the created cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to set up RHadoop on Amazon EMR. The benefit
    of this is that you can quickly create a scalable, on demand Hadoop with just
    a few clicks within a few minutes. This helps save you time from building and
    deploying a Hadoop application. However, you have to pay for the number of running
    hours for each instance. Before using Amazon EMR, you should create an AWS account
    and know how to set up the EC2 key-pair and the S3\. You can then start installing
    RHadoop on Amazon EMR.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, access the EMR cluster list and click on **Create cluster**.
    You can see a list of configurations on the **Create cluster** page. You should
    then set up the cluster name and log folder in the S3 location in the cluster
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can set up the software configuration and choose the Hadoop distribution
    you would like to install. Amazon provides both its own distribution and the MapR
    distribution. Normally, you would skip this section unless you have concerns about
    the default Hadoop distribution.
  prefs: []
  type: TYPE_NORMAL
- en: You can then configure the hardware by specifying the master, core, and task
    node. By default, there is only one master node, and two core nodes. You can add
    more core and task nodes if you like. You should then set up the key-pair to login
    to the master node.
  prefs: []
  type: TYPE_NORMAL
- en: You should next make a file containing all the start scripts named `bootstrapRHadoop.sh`.
    After the file is created, you should save the file in the S3 storage. You can
    then specify `custom action` in **Bootstrap Action** with `bootstrapRHadoop.sh`
    as the Bootstrap script. Lastly, you can click on `Create cluster` and wait until
    the cluster is ready. Once the cluster is ready, one can see the master public
    DNS and can use the EC2 key-pair to access the terminal of the master node.
  prefs: []
  type: TYPE_NORMAL
- en: Beware! Terminate the running instance if you do not want to continue using
    the EMR service. Otherwise, you will be charged per instance for every hour you
    use.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google also provides its own cloud solution, the Google compute engine. For
    those who would like to know more, please refer to [https://cloud.google.com/compute/](https://cloud.google.com/compute/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A. Resources for R and Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table lists all the resources for R and machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '| R introduction |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Title** | **Link** | **Author** |'
  prefs: []
  type: TYPE_TB
- en: '| R in Action | [http://www.amazon.com/R-Action-Robert-Kabacoff/dp/1935182390](http://www.amazon.com/R-Action-Robert-Kabacoff/dp/1935182390)
    | Robert Kabacoff |'
  prefs: []
  type: TYPE_TB
- en: '| The Art of R Programming: A Tour of Statistical Software Design | [http://www.amazon.com/The-Art-Programming-Statistical-Software/dp/1593273843](http://www.amazon.com/The-Art-Programming-Statistical-Software/dp/1593273843)
    | Norman Matloff |'
  prefs: []
  type: TYPE_TB
- en: '| An Introduction to R | [http://cran.r-project.org/doc/manuals/R-intro.pdf](http://cran.r-project.org/doc/manuals/R-intro.pdf)
    | W. N. Venables, D. M. Smith, and the R Core Team |'
  prefs: []
  type: TYPE_TB
- en: '| Quick-R | [http://www.statmethods.net/](http://www.statmethods.net/) | Robert
    I. Kabacoff, PhD |'
  prefs: []
  type: TYPE_TB
- en: '| **Online courses** |'
  prefs: []
  type: TYPE_TB
- en: '| **Title** | **Link** | **Instructor** |'
  prefs: []
  type: TYPE_TB
- en: '| Computing for Data Analysis (with R) | [https://www.coursera.org/course/compdata](https://www.coursera.org/course/compdata)
    | Roger D. Peng, Johns Hopkins University |'
  prefs: []
  type: TYPE_TB
- en: '| Data Analysis | [https://www.coursera.org/course/dataanalysis](https://www.coursera.org/course/dataanalysis)
    | Jeff Leek, Johns Hopkins University |'
  prefs: []
  type: TYPE_TB
- en: '| Data Analysis and Statistical Inference | [https://www.coursera.org/course/statistics](https://www.coursera.org/course/statistics)
    | Mine Çetinkaya-Rundel, Duke University |'
  prefs: []
  type: TYPE_TB
- en: '| **Machine learning** |'
  prefs: []
  type: TYPE_TB
- en: '| **Title** | **Link** | **Author** |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Learning for Hackers | [http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20](http://www.amazon.com/dp/1449303714?tag=inspiredalgor-20)
    | Drew Conway and John Myles White |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Learning with R | [http://www.packtpub.com/machine-learning-with-r/book](http://www.packtpub.com/machine-learning-with-r/book)
    | Brett Lantz |'
  prefs: []
  type: TYPE_TB
- en: '| **Online blog** |'
  prefs: []
  type: TYPE_TB
- en: '| **Title** | **Link** |'
  prefs: []
  type: TYPE_TB
- en: '| R-bloggers | [http://www.r-bloggers.com/](http://www.r-bloggers.com/) |'
  prefs: []
  type: TYPE_TB
- en: '| The R Journal | [http://journal.r-project.org/](http://journal.r-project.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **CRAN task view** |'
  prefs: []
  type: TYPE_TB
- en: '| **Title** | **Link** |'
  prefs: []
  type: TYPE_TB
- en: '| CRAN Task View: Machine Learning and Statistical Learning | [http://cran.r-project.org/web/views/MachineLearning.html](http://cran.r-project.org/web/views/MachineLearning.html)
    |'
  prefs: []
  type: TYPE_TB
- en: Appendix B. Dataset – Survival of Passengers on the Titanic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the exploration process, we would like to introduce the example adopted
    here. It is the demographic information on passengers aboard the RMS Titanic,
    provided by Kaggle ([https://www.kaggle.com/](https://www.kaggle.com/), a platform
    for data prediction competitions). The result we are examining is whether passengers
    on board would survive the shipwreck or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two reasons to apply this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: RMS Titanic is considered as the most infamous shipwreck in history, with a
    death toll of up to 1,502 out of 2,224 passengers and crew. However, after the
    ship sank, the passengers' chance of survival was not by chance only; actually,
    the cabin class, sex, age, and other factors might also have affected their chance
    of survival.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset is relatively simple; you do not need to spend most of your time
    on data munging (except when dealing with some missing values), but you can focus
    on the application of exploratory analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following chart is the variables'' descriptions of the target dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset – Survival of Passengers on the Titanic](img/00280.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Judging from the description of the variables, one might have some questions
    in mind, such as, "Are there any missing values in this dataset?", "What was the
    average age of the passengers on the Titanic?", "What proportion of the passengers
    survived the disaster?", "What social class did most passengers on board belong
    to?". All these questions presented here will be answered in [Chapter 2](part0024_split_000.html#page
    "Chapter 2. Data Exploration with RMS Titanic"), *Data Exploration with RMS Titanic*.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond questions relating to descriptive statistics, the eventual object of
    [Chapter 2](part0024_split_000.html#page "Chapter 2. Data Exploration with RMS
    Titanic"), *Data Exploration with RMS Titanic*, is to generate a model to predict
    the chance of survival given by the input parameters. In addition to this, we
    will assess the performance of the generated model to determine whether the model
    is suited for the problem.
  prefs: []
  type: TYPE_NORMAL
