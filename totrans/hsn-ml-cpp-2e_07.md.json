["```py\nusing namespace mlpack;\nsize_t num_classes;\narma::mat train_input;\narma::Row<size_t> train_labels;\n```", "```py\nSoftmaxRegression smr(train_input.n_cols, num_classes);\n```", "```py\nsmr.Train(train_input, train_labels, num_classes);\narma::Row<size_t> predictions;\nsmr.Classify(test_input, predictions);\n```", "```py\nmlpack::LinearSVM<> lsvm;\n```", "```py\nlsvm.Train(train_input, train_labels, num_classes);\narma::Row<size_t> predictions;\nlsvm.Classify(test_input, predictions);\n```", "```py\nusing namespace mlpack;\nLogisticRegression<> lr;\nlr.Train(train_input, train_labels);\narma::Row<size_t> predictions;\nlr.Classify(test_input, predictions);\n```", "```py\nvoid KRRClassification(const Samples& samples,\n                       const Labels& labels,\n                       const Samples& test_samples,\n                       const Labels& test_labels) {\n  using OVOtrainer = one_vs_one_trainer<\n    any_trainer<SampleType>>;\n  using KernelType = radial_basis_kernel<SampleType>;\n  krr_trainer<KernelType> krr_trainer;\n  krr_trainer.set_kernel(KernelType(0.1));\n  OVOtrainer trainer;\n  trainer.set_trainer(krr_trainer);\n  one_vs_one_decision_function<OVOtrainer> df =\n      trainer.train(samples, labels);\n  // process results and estimate accuracy\n  DataType accuracy = 0;\n  for (size_t i = 0; i != test_samples.size(); i++) {\n    auto vec = test_samples[i];\n    auto class_idx = static_cast<size_t>(df(vec));\n    if (static_cast<size_t>(test_labels[i]) == class_idx)\n        ++accuracy;\n    // ...\n  }\n  accuracy /= test_samples.size();\n}\n```", "```py\n        auto vec = test_samples[i];\n        auto class_idx = static_cast<size_t>(df(vec));\n```", "```py\nvoid SVMClassification(const Samples& samples,\n                       const Labels& labels,\n                       const Samples& test_samples,\n                       const Labels& test_labels) {\n  using OVOtrainer = one_vs_one_trainer<\n    any_trainer<SampleType>>;\n  using KernelType = radial_basis_kernel<SampleType>;\n  svm_nu_trainer<KernelType> svm_trainer;\n  svm_trainer.set_kernel(KernelType(0.1));\n  OVOtrainer trainer;\n  trainer.set_trainer(svm_trainer);\n  one_vs_one_decision_function<OVOtrainer> df =\n    trainer.train(samples, labels);\n  // process results and estimate accuracy\n  DataType accuracy = 0;\n  for (size_t i = 0; i != test_samples.size(); i++) {\n    auto vec = test_samples[i];\n    auto class_idx = static_cast<size_t>(df(vec));\n    if (static_cast<size_t>(test_labels[i]) == class_idx)\n        ++accuracy;\n    // ...\n  }\n  accuracy /= test_samples.size();\n}\n```", "```py\nfl::Tensor train_linear_classifier(\n  const fl::Tensor& train_x,\n  const fl::Tensor& train_y, float learning_rate);\n```", "```py\nstd::vector<fl::Tensor> fields{train_x, train_y};\nauto dataset = std::make_shared<fl::TensorDataset>(fields);\nint batch_size = 8;\nauto batch_dataset = std::make_shared<fl::BatchDataset>(\n  dataset, batch_size);\n```", "```py\nauto weights = fl::Variable(fl::rand({\n  train_x.shape().dim(0), 1}), /*calcGrad=*/true);\n```", "```py\nint num_epochs = 100;\nfor (int e = 1; e <= num_epochs; ++e) {\n  fl::Tensor epoch_error = fl::fromScalar(0);\n  for (auto& batch : *batch_dataset) {\n    auto x = fl::Variable(batch[0], /*calcGrad=*/false);\n    auto y = fl::Variable(\n      fl::reshape(batch[1], {1, batch[1].shape().dim(0)}), \n      /*calcGrad=*/false);\n  }\n}\n```", "```py\nauto z = fl::matmul(fl::transpose(weights), x);\nauto loss = fl::sum(fl::log(1 + fl::exp(-1 * y * z)), /*axes=*/{1});\n```", "```py\nloss.backward();\nweights.tensor() -= learning_rate *weights.grad().tensor();\nweights.zeroGrad();\n```", "```py\nreturn weights.tensor();\n```", "```py\nfl::Tensor train_x;\nfl::Tensor train_y;\nauto weights = train_linear_classifier(\n  train_x, train_y, /*learning_rate=*/0.1f);\n```", "```py\nfl::Tensor sample;\nconstexpr float threshold = 0.5;\nauto p = fl::sigmoid(fl::matmul(fl::transpose(weights), sample));\nif (p.scalar<float>() > threshold)\n  return 1;\nelse\n  return 0;\n```", "```py\nfl::Tensor make_kernel_matrix(const fl::Tensor& x,\n                              const fl::Tensor& y, float gamma) {\n  auto x_norm = fl::sum(fl::power(x, 2), /*axes=*/{-1});\n  x_norm = fl::reshape(x_norm, {x_norm.dim(0), 1});\n  auto y_norm = fl::sum(fl::power(y, 2), /*axes=*/{-1});\n  y_norm = fl::reshape(y_norm, {1, y_norm.dim(0)});\n  auto k = fl::exp(-gamma * (x_norm + y_norm -\n                             2 * fl::matmul(fl::transpose(x), y)));\n  return k;\n}\n```", "```py\nconstexpr float rbf_gamma = 100.f;\nauto kx = make_kernel_matrix(train_x, train_x, rbf_gamma);\n```", "```py\nauto kweights = train_linear_classifier(kx, train_y, learning_rate);\n```", "```py\nfl::Tensor sample;\nauto k_sample = make_kernel_matrix(fl::reshape(sample, {\n  sample.dim(0), 1}), train_x, rbf_gamma);\n```", "```py\nconstexpr float threshold = 0.5;\nauto p = fl::sigmoid(fl::matmul(fl::transpose(kweights),\n         fl::transpose(k_sample)));\nif (p.scalar<float>() > threshold)\n  return 1;\nelse\n  return 0;\n```"]