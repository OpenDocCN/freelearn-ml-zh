["```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n```", "```py\ntext = [\" Machine translation automatically translate text from one human language to another text\"]\n```", "```py\nvectorizer.fit(text)\n```", "```py\nprint(vectorizer.vocabulary_)\n```", "```py\nvector = vectorizer.transform(text)\n```", "```py\nprint(type(vector))\nprint(vector.toarray())\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```", "```py\ncorpus = ['First document', 'Second document','Third document','First and second document' ]\n```", "```py\nvectorizer = TfidfVectorizer()\n```", "```py\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.shape)\n```", "```py\nX.toarray()\n```", "```py\nimport numpy as np\n import pandas as pd\n import seaborn as sns\n import matplotlib.pyplot as plt\n sns.set(color_codes=True)\n import os\n print(os.listdir())\n```", "```py\ndata= pd.read_csv(\"imdb_master.csv\",encoding = \"ISO-8859-1\")\n```", "```py\nprint(data.head())\nprint(data.shape)\n```", "```py\nNewdata= data[[\"review\",\"label\"]]\nNewdata.shape\n```", "```py\ng= Newdata.groupby(\"label\")\ng.count()\n```", "```py\nsent=[\"neg\",\"pos\"]\n\nNewdata = Newdata[Newdata.label.isin(sent)]\nNewdata.head()\n```", "```py\nprint(len(Newdata))\nNewdata=Newdata.reset_index(drop=True) Newdata.head()\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n labelencoder = LabelEncoder()\n Newdata[\"label\"] = labelencoder.fit_transform(Newdata[\"label\"])\n```", "```py\nNewdata[\"Clean_review\"]= Newdata['review'].str.replace(\"[^a-zA-Z#]\", \" \")\n\nNewdata.head()\n```", "```py\nNewdata['Clean_review'] = Newdata['Clean_review'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n Newdata.shape\n```", "```py\ntokenized_data = Newdata['Clean_review'].apply(lambda x: x.split())\n tokenized_data.shape\n```", "```py\nfrom nltk.stem.porter import *\n stemmer = PorterStemmer()\n tokenized_data = tokenized_data.apply(lambda x: [stemmer.stem(i) for i in x])\n tokenized_data.head()\n```", "```py\nfor i in range(len(tokenized_data)):\n tokenized_data[i] = ' '.join(tokenized_data[i])\n\ntokenized_data.head()\n```", "```py\nNewdata[\"Clean_review2\"]= tokenized_data\n Newdata.head()\n```", "```py\nall_words = ' '.join([str(text) for text in Newdata['Clean_review2']])\n from wordcloud import WordCloud\n wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n plt.figure(figsize=(10, 7))\n plt.imshow(wordcloud, interpolation=\"bilinear\")\n plt.axis('off')\n plt.show()\n```", "```py\nNegative =' '.join([text for text in Newdata['Clean_review2'][Newdata['label'] == 0]])\n wordcloud1= WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Negative)\n plt.figure(figsize=(10, 7))\n plt.imshow(wordcloud1, interpolation=\"bilinear\")\n plt.title(\"Word Cloud- Negative\")\n plt.axis('off')\n plt.show()\n```", "```py\nPositive=' '.join([text for text in Newdata['Clean_review2'][Newdata['label'] == 1]])\n wordcloud2 = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Positive)\n plt.figure(figsize=(10, 7))\n plt.imshow(wordcloud, interpolation=\"bilinear\")\n plt.title(\"Word Cloud-Positive\")\n plt.axis('off')\n plt.show()\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf= TfidfVectorizer(max_df=0.9,min_df= 2, max_features=1000,\n                        stop_words=\"english\")\ntfidfV = tfidf.fit_transform(Newdata['Clean_review2'])\n\ntfidf.vocabulary_\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n # bag-of-words\n bow = bow_vectorizer.fit_transform(Newdata['Clean_review2'])\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n from sklearn.model_selection import train_test_split\n from sklearn.metrics import f1_score,accuracy_score\n # splitting data into training and validation set\n xtrain, xtest, ytrain, ytest = train_test_split(bow, Newdata['label'], random_state=42, test_size=0.3)\n lreg = LogisticRegression()\n lreg.fit(xtrain, ytrain) # training the model\n prediction = lreg.predict_proba(xtest) # predicting on the validation set\n prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n prediction_int = prediction_int.astype(np.int)\n print(\"F1 Score-\",f1_score(ytest, prediction_int))\n print(\"Accuracy-\",accuracy_score(ytest,prediction_int))\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n # splitting data into training and validation set\n xtraintf, xtesttf, ytraintf, ytesttf = train_test_split(tfidfV, Newdata['label'], random_state=42, test_size=0.3)\n lreg = LogisticRegression()\n lreg.fit(xtraintf, ytraintf) # training the model\n prediction = lreg.predict_proba(xtesttf) # predicting on the test set\n prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n prediction_int = prediction_int.astype(np.int)\n print(\"F1 Score-\",f1_score(ytest, prediction_int))\n print(\"Accuracy-\",accuracy_score(ytest,prediction_int))\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups\n dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n documents = dataset.data\n```", "```py\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n```", "```py\nimport nltk\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n```", "```py\nstop = set(stopwords.words('english'))\npunc = set(string.punctuation)\nlemma = WordNetLemmatizer()\ndef clean(doc):\n     stopw_free = \" \".join([i for i in doc.lower().split() if i not in stop and len(i)>3])\n     punc_free = ''.join(ch for ch in stop_free if ch not in punc)\n     lemmatized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n     return lemmatized\n doc_clean = [clean(doc).split() for doc in documents]\n```", "```py\nimport gensim\nfrom gensim import corpora\n```", "```py\ncorpus = corpora.Dictionary(doc_clean)\n doc_term_matrix = [corpus.doc2bow(doc) for doc in doc_clean]\n```", "```py\nfrom gensim import models\ntfidf = models.TfidfModel(doc_term_matrix)\ncorpus_tfidf = tfidf[doc_term_matrix]\n```", "```py\nlda_model1 = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=corpus, passes=2, workers=2)\n```", "```py\nprint(lda_model1.print_topics(num_topics=5, num_words=5))\n```", "```py\nlda_model2 = gensim.models.LdaMulticore(doc_term_matrix, num_topics=10, id2word=corpus, passes=2, workers=2)\n\nprint(lda_model2.print_topics(num_topics=5, num_words=5))\n```", "```py\nprint(\"lda_model 1- Perplexity:-\",lda_model.log_perplexity(corpus_tfidf))\nprint(\"lda_model 2- Perplexity:-\",lda_model2.log_perplexity(doc_term_matrix))\n```", "```py\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npyLDAvis.enable_notebook()\nvisual1= pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, corpus)\nvisual1\n```", "```py\npyLDAvis.enable_notebook()\n visual2= pyLDAvis.gensim.prepare(lda_model2, doc_term_matrix, corpus)\n visual2\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB\n# splitting data into training and validation set\nxtraintf, xtesttf, ytraintf, ytesttf = train_test_split(tfidfV, Newdata['label'], random_state=42, test_size=0.3)\nNB= MultinomialNB()\nNB.fit(xtraintf, ytraintf)\nprediction = NB.predict_proba(xtesttf) # predicting on the test set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction_int.astype(np.int)\nprint(\"F1 Score-\",f1_score(ytest, prediction_int))\nprint(\"Accuracy-\",accuracy_score(ytest,prediction_int))\n```"]