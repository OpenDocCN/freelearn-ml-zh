- en: '*Chapter 5*: Practical Exposure to Using LIME in ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading the last chapter, you should now have a good conceptual understanding
    of **Local Interpretable Model-agnostic Explanations (LIME)**. We saw how the
    LIME Python framework can explain black-box models for classification problems.
    We also discussed some of the pros and cons of the LIME framework. In practice,
    LIME is still one of the most popular XAI frameworks as it can be easily applied
    to tabular datasets and text and image datasets. LIME can provide model-agnostic
    local explanations for solving both regression and classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will get much more in-depth practical exposure to using
    LIME in ML. These are the main topics of discussion for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using LIME on tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining image classifiers with LIME
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LIME on text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME for production-level systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the previous chapter, this chapter is very technical with code walk-throughs
    in Python and Jupyter notebooks. For this chapter, the code and dataset resources
    can be downloaded or cloned from the GitHub repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05).
    Like the previous chapters, we will be using Python and Jupyter notebooks to run
    the code and generate the necessary outputs. All other relevant details are provided
    in the notebooks, and I recommend that you all run the notebooks while going through
    the chapter content to get a better understanding of the topics covered.'
  prefs: []
  type: TYPE_NORMAL
- en: Using LIME on tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Practical example of using LIME for classification problems* section
    of [*Chapter 4*](B18216_04_ePub.xhtml#_idTextAnchor076), *LIME for Model Interpretability*,
    we discussed how to set up LIME in Python and how to use LIME to explain classification
    ML models. The dataset used for the tutorial in [*Chapter 4*](B18216_04_ePub.xhtml#_idTextAnchor076)*,
    LIME for Model Interpretability* ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb))
    was a tabular structured data. In this section, we will discuss using LIME to
    explain regression models that are built on tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up LIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting the code walk-through, I would ask you to check the following
    notebook, [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_tabular_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_tabular_data.ipynb),
    which already contains the steps needed to understand the concept that we are
    going to discuss now in more depth. I assume that most of the Python libraries
    that we will use for this tutorial are already installed on your system. But if
    not, please run the following command to install the upgraded versions of the
    Python libraries that we are going to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Discussion about the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this tutorial, we will use the *Diabetes dataset* from *scikit-learn datasets*
    ([https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)).
    This dataset is used to predict the *disease progression level* of diabetes. It
    contains around *442 samples* with *10 baseline features* – *age*, *sex*, *body
    mass index (bmi)*, *average blood pressure (bp)*, *total serum cholesterol (s1)*,
    *low-density lipoproteins (s2)*, *high-density lipoproteins (s3)*, *total cholesterol
    / HDL (s4)*, *possibly log of serum triglycerides level (s5)*, and *blood sugar
    level (s6)*. The dataset is quite interesting and relevant, considering that the
    underlying problem of monitoring diabetes progression is an important practical
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature variables provided in the dataset are already normalized by centering
    the feature values around the mean and scaling by the standard deviation times
    the number of samples (N):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: More information about the original dataset can be found at [https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the dataset, just execute the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can perform the necessary EDA steps if needed, but since our main focus
    is to use LIME for explaining black-box models, we will not spend too much effort
    on EDA for the purpose of this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Discussions about the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As demonstrated in the notebook tutorial, we have used a **Gradient Boosting
    Regressor** (**GBR**) ([https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html))
    model to train our predictive model. However, any regression ML algorithm can
    be used instead of GBR as the model itself is regarded as any black-box model
    by the LIME algorithm. Also, when we evaluated the trained model on the unseen
    data, we observed a **Mean Absolute Percentage Error (MAPE)** of 0.37, a **Mean
    Square Error (MSE)** of 2,538, and an **R-squared coefficient** score of 0.6\.
    All these results indicate that our model is not very good and definitely has
    room for improvement. So, if such a model is deployed in production-level systems,
    there can be many questions asked by the end stakeholders as it is always difficult
    for them to trust models that are not accurate. Also, algorithms such as GBR are
    not inherently interpretable and the complexity of the algorithm depends on hyperparameters
    including the number of estimators and the depth of the tree. Thus, model explainability
    frameworks such as LIME are not just an add-on step, but a necessary part of the
    process of building ML models. Next, we will see how easily LIME can be applied
    to explain black-box regression models with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Application of LIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have seen in the previous chapter, we can easily support the LIME framework
    for tabular data with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the LIME module is successfully imported, we will need to create an explainer
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we just need to take the data instance and provide local explainability
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output from the preceding lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Output visualization from the LIME framework when applied to
    a regression model trained on a tabular dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_05_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Output visualization from the LIME framework when applied to a
    regression model trained on a tabular dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.1* illustrates the visualization-based explanations provided by the
    LIME framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s try to understand what the output visualization in *Figure 5.1*
    is telling us:'
  prefs: []
  type: TYPE_NORMAL
- en: The left-most visualization from *Figure 5.1* shows a range of possible values
    and the position of the model's predicted outcome. Intuitively speaking, all model
    predictions should lie within the minimum and the maximum possible value as this
    indicates to the user to compare the current forecast with the best-case and the
    worst-case values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middle visualization shows which features contribute to the prediction being
    on the higher side or the lower side. Considering our prior knowledge of diabetes,
    a higher BMI, as well as raised blood pressure and serum triglyceride levels,
    do indicate increasing progression of the disease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right-most visualization in *Figure 5.1* shows us the actual local data
    values for the most important features identified, arranged in descending order
    of their relevance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The explanations provided by the LIME framework are human-interpretable to a
    great extent and do give us an indication of the feature-value pairs used by the
    black-box model to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: So, this is how we can use LIME to explain black-box regression models trained
    on tabular data with just a few lines of code. But, as we discussed in [*Chapter
    4*](B18216_04_ePub.xhtml#_idTextAnchor076), *LIME for Model Interpretability*,
    under the *Potential pitfalls* section, explanations provided by LIME are not
    always holistic and may have some inconsistencies. This is something we all need
    to be mindful of. However, LIME explanations, coupled with a thorough EDA, data-centric
    XAI, counterfactual explanations, and other model explainability methods, can
    provide a powerful, holistic explainability to black-box models trained on tabular
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore how to use LIME for classifiers trained on unstructured data
    such as images in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining image classifiers with LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen how we can easily apply LIME to explain
    models trained on tabular data. However, the main challenge always comes while
    explaining complex deep learning models trained on unstructured data such as images.
    Generally, deep learning models are much more efficient than conventional ML models
    on image data as these models have the ability to perform *auto feature extraction*.
    They can extract complex *low-level features* such as *stripes*, *edges*, *contours*,
    *corners*, and *motifs*, and even *higher-level features* such as *larger shapes*
    and *certain parts of the object*. These higher-level features are usually referred
    to as **Regions of Interest** **(RoI)** in the image, or **superpixels**, as they
    are collections of pixels of the image that cover a particular area of the image.
    Now, the low-level features are not human-interpretable, but the high-level features
    are human-interpretable, as any non-technical end user will relate to the images
    with respect to the higher-level features. LIME also works in a similar fashion.
    The algorithm tries to highlight the superpixels in images that contribute positively
    or negatively to the model's decision-making process. So, let's see how LIME can
    be used to explain image classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the required Python modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we begin the code walk-through, please check the notebook provided in
    the code repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data.ipynb).
    The notebook contains the necessary details required for the practical application
    of the concepts. In this section, I will give you a walk-through of the code and
    explain all the steps covered in the notebook tutorial. Use the following command
    to install the upgraded versions of the Python libraries if not already installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's discuss the model used in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pre-trained TensorFlow model as our black-box model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this tutorial, we have used a *pre-trained TensorFlow Keras Xception model*
    as our black-box model. The model is pre-trained on the ImageNet dataset ([https://www.image-net.org/](https://www.image-net.org/)),
    which is one of the most popular benchmarking datasets for image classification.
    The pre-trained model can be loaded with the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In order to use any inference data for image classification, we will also need
    to perform the necessary preprocessing steps. Please refer to the notebook at
    [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_image_data).ipynb
    for the necessary pre-processing methods.
  prefs: []
  type: TYPE_NORMAL
- en: Application of LIME Image Explainers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will see how the LIME framework can be used to identify
    *super-pixels* or regions from the image used by the model to predict the specific
    outcome. We will first need to define an image `explainer` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will need to pass the inference data (`normalized_img[0]`) to the
    `explainer` object and use the LIME framework to highlight superpixels that have
    the maximum positive and negative influence on the model''s prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As an output to the preceding lines of code, we will get certain highlighted
    portions of the image that contribute to the model''s prediction, in both a positive
    and negative manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – (Left) Original inference image. (Middle) Most important image
    superpixel. (Right) Image with superposed mask superpixel on the original data
    highlighted in green'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_05_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – (Left) Original inference image. (Middle) Most important image
    superpixel. (Right) Image with superposed mask superpixel on the original data
    highlighted in green
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.2*, the left-most image was used as the inference image. When the
    trained model was applied to the inference image, the top prediction of the model
    was a *tiger shark*.
  prefs: []
  type: TYPE_NORMAL
- en: The prediction was actually correct. However, in order to explain the model,
    the LIME algorithm can highlight the superpixel, which has the maximum influence
    on the prediction. From the middle and the right-most images in *Figure 5.2*,
    we can see that the black-box model was actually good and trustworthy as the relevant
    superpixel captured by the LIME algorithm indicates the presence of a tiger shark.
  prefs: []
  type: TYPE_NORMAL
- en: 'The superpixel estimated by the LIME algorithm can be displayed using the following
    lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also form a heatmap highlighting the importance of each of the superpixels,
    which gives us further insight into the functioning of the black-box model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output obtained is shown in *Figure 5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – (Left) Image showing all the superpixels picked up by the LIME
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: (Right) Heatmap of the superpixels based on their importance in terms of the
    model's prediction
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_05_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – (Left) Image showing all the superpixels picked up by the LIME
    algorithm. (Right) Heatmap of the superpixels based on their importance in terms
    of the model's prediction
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap from *Figure 5.3* provides us with some insight into important superpixels,
    which is also easy when it comes to any non-technical user interpreting any black-box
    model.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have seen how LIME can explain even complicated deep learning models
    trained on image data in just a few lines of code. I found LIME to be one of the
    most effective algorithms to visually explain deep learning-based image classifiers
    without presenting any complicated statistical or numerical values or complicated
    graphical visualizations. Unlike tabular data, I felt explanations provided to
    image classifiers are more robust, stable, and human-interpretable. It is definitely
    one of my favorite methods for interpreting image classifiers, and before moving
    any image classification model to production, I strongly recommend applying LIME
    as an additional evaluation step to gain more confidence in the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's explore LIME for models trained on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Using LIME on text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed how LIME is an effective approach to explaining
    complicated black-box models trained on image datasets. Like images, text is also
    a form of unstructured data, which is very much different from structured tabular
    data. Explaining such black-box models trained on unstructured data is always
    very challenging. But LIME can also be applied to models trained on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Using the LIME algorithm, we can analyze whether the presence of a particular
    word or group of words increases the probability of predicting a specific outcome.
    In other words, LIME helps to highlight the importance of text tokens or words
    that can influence the model's outcome toward a particular class. In this section,
    we will see how LIME can be used to interpret text classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required Python modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the previous tutorials, the complete notebook tutorial is available at
    [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_text_data.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter05/LIME_with_text_data.ipynb).
    Although the necessary instructions needed to run the notebook are clearly documented
    in the notebook itself, similar to the previous tutorials, I will provide the
    necessary details to walk you through the implementation. Using the following
    commands, you can install the modules required to run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'For text-related operations on the underlying dataset, I will be mainly using
    the NLTK Python framework. So, you will need to download certain `nltk` modules
    by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the tutorial, we will try to explain a text classifier designed to perform
    sentiment analysis by classifying the text data into positive and negative classes.
  prefs: []
  type: TYPE_NORMAL
- en: Discussions about the dataset used for training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have used the **Sentiment Polarity dataset v2.0** consisting of *Movie reviews*
    for this tutorial used for sentiment analysis from text data. The dataset consists
    of about 1,000 samples of positive and negative movie reviews. More information
    about the dataset can be found on the source website: [https://www.cs.cornell.edu/people/pabo/movie-review-data/](https://www.cs.cornell.edu/people/pabo/movie-review-data/).
    The dataset is also provided in the GitHub repository of this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter05).'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Polarity dataset v2.0
  prefs: []
  type: TYPE_NORMAL
- en: 'This data was first used in Bo Pang and Lillian Lee, "A Sentimental Education:
    Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts", Proceedings
    of the ACL, 2004.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussions about the text classification model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike the previous image classifier tutorial, we have not used a pre-trained
    model. We have trained an **XGBoost Classifier** ([https://xgboost.readthedocs.io/en/stable/](https://xgboost.readthedocs.io/en/stable/))
    from scratch with the necessary data preprocessing, preparation, and feature extraction
    steps as covered in the notebook. XGBoost is an ensemble learning boosting algorithm
    that is not inherently interpretable. So, we will consider this as our black-box
    text classification model. We are not focused on improving the model''s accuracy
    with necessary hyperparameter tuning, as LIME is completely model-agnostic. For
    this tutorial, we have created a scikit-learn pipeline to apply feature extraction
    first using **TFIDF Vectorizer** (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html),
    and then the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the next subsection, we will see how the LIME framework can be easily applied
    with text data as well.
  prefs: []
  type: TYPE_NORMAL
- en: Applying LIME Text Explainers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like the previous tutorials with image and tabular data, applying LIME is simple
    with text data in a few lines of code. We will now define the LIME `explainer`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use the inference data instance to provide local explainability
    for that particular data instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: And that's it! In just a few lines of code, we can explain the text classifier
    that actually relies on TFIDF numerical features, but the explainability is provided
    with a human interpretable perspective as words that can positively or negatively
    influence the model outcome are highlighted. It is easier for any non-technical
    user to understand the working of the text model in this way, rather than providing
    explanations using numerically encoded features.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at the output visualization provided by LIME when applied
    to text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Output visualization when LIME is applied to a text classifier'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_05_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Output visualization when LIME is applied to a text classifier
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.4*, we can see the output visualization of the LIME framework when
    applied to text data.
  prefs: []
  type: TYPE_NORMAL
- en: The output visualization is very similar to what we have observed with tabular
    data. It shows us the *prediction probability*, which can be used as a *model
    confidence* score. The algorithm highlights the most influential words that determine
    the model outcome, with a feature importance score. For example, from *Figure
    5.4*, we can see that the inference data instance is predicted as negative by
    the model (which is predicted correctly as demonstrated in the notebook). The
    presence of words such as *waste*, *bad*, and *ridiculous* does indicate a *negative
    review*. This is human-interpretable as well, since if you ask a non-technical
    user to justify why the review is classified as negative, the user might refer
    to the usage of frequently used words in negative reviews or words used in sentences
    with a negative tone.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can see that LIME can be easily applied with text classifiers as well.
    Even with text data, the algorithm is simple, yet effective in providing a human-interpretable
    explanation. I would definitely recommend using LIME to explain black-box text
    classifiers as an additional model evaluation or quality inspection step.
  prefs: []
  type: TYPE_NORMAL
- en: But so far, we have seen the application of LIME Python frameworks in the Jupyter
    notebook environment. The immediate question that you might have is – *Can we
    scale LIME for use in production-level systems?* Let's find out in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: LIME for production-level systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The short answer to the question posted toward the end of the last section
    is *Yes*. LIME can definitely be scaled for use in production-level systems due
    to the following main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimal implementation complexity**: The API structure of the LIME Python
    framework is concise and well structured. This allows us to add model explainability
    in just a few lines of code. For providing local explainability to inference data
    instances, the runtime complexity of the LIME algorithm is very low and, hence,
    this approach can also work for real-time applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy integration with other software applications**: The API structure of
    the framework is modular. For consuming the explainability results, we do not
    need to solely depend on the in-built visualizations provided by the framework.
    We can utilize the raw explainability results and create our own custom visualization
    dashboards or reports. Also, we can create custom web API methods and host the
    web APIs on remote cloud servers, creating our own model explainability cloud-based
    service that can be integrated easily with other software applications. We will
    cover this in more detail in [*Chapter 10*](B18216_10_ePub.xhtml#_idTextAnchor209),
    *XAI Industry Best Practices*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Does not require heavy computational resources**: The LIME framework works
    well with low computational resources. For real-time applications, the algorithms
    used need to be very fast and should have the ability to run on low computational
    resources, as otherwise, the user experience is affected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy to set up and package**: As we have already seen before running the
    tutorial notebooks, LIME is very easy to set up and does not have a dependency
    on packages that are difficult to install. Similarly, any Python program using
    LIME is easy to package or **containerize**. Most production-level systems have
    automated CI/CD pipelines to create **Docker containers** (https://www.docker.com/resources/what-container),
    which are deployed on production-level systems. The engineering effort needed
    to containerize a Python program using a LIME framework is low and hence it is
    easy to productionalize such software applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the key reasons why LIME is the preferred model explainability method
    used in industrial applications, despite some of its well-known pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed the practical applications of the LIME Python
    framework on different types of datasets. The tutorials covered in the chapter
    are just the starting point and I strongly recommend you try out LIME explainability
    on other datasets. We have also discussed why LIME is a good fit for production-level
    ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss another very popular explainable AI Python
    framework called **SHAP**, which even considers the collective contribution of
    multiple features in influencing the model's outcome.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following resources to gain additional information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Why Should I Trust You?" Explaining the Predictions of Any Classifier*, by
    *Ribeiro et al.*: [https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Local Interpretable Model-Agnostic Explanations (LIME): An Introduction*:
    [https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LIME GitHub Project*: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Docker Blog*: [https://www.docker.com/blog/](https://www.docker.com/blog/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
