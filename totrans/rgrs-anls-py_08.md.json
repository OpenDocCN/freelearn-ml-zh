["```py\nIn:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndiabetes = datasets.load_diabetes()\nX = StandardScaler().fit_transform(diabetes.data)\ny = StandardScaler(with_mean=True, with_std=False) \\\n        .fit_transform(diabetes.target)\n\nalphas, _, coefs = linear_model.lars_path(X, y, verbose=2)\n\nxx = np.sum(np.abs(coefs.T), axis=1)\nxx /= xx[-1]\n\nplt.plot(xx, coefs.T)\nymin, ymax = plt.ylim()\nplt.vlines(xx, ymin, ymax, linestyle='dashed')\nplt.xlabel('|coef| / max|coef|')\nplt.ylabel('Coefficients')\nplt.axis('tight')\nplt.show()\n\nOut:\n```", "```py\nIn:\nregr = linear_model.Lars()\n\nregr.fit(X, y)\n\nprint(\"Coefficients are:\", regr.coef_)\nOut:\nCoefficients are: \n[ -0.47623169 -11.40703082  24.72625713  15.42967916 -37.68035801\n  22.67648701   4.80620008   8.422084    35.73471316   3.21661161]\n```", "```py\nIn:\nprint(\"R2 score is\", regr.score(X,y))â€©Out:\nR2 score is 0.517749425413\n```", "```py\nIn:\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=10, n_features=1, n_informative=1, noise=3, random_state=1)\n```", "```py\nIn:\nregr = linear_model.LinearRegression()\nregr.fit(X, y)\n\ntest_x = 2*np.max(X)\npred_test_x = regr.predict(test_x)\npred_test_x\nOut:\narray([ 10.79983753])\n```", "```py\nIn:\nplt.scatter(X, y)\nx_bounds = np.array([1.2*np.min(X), 1.2*np.max(X)]).reshape(-1, 1)\nplt.plot(x_bounds, regr.predict(x_bounds) , 'r-')\nplt.plot(test_x, pred_test_x, 'g*')\nplt.show()\nOut:\n```", "```py\nIn:\nregr = linear_model.BayesianRidge()\nregr.fit(X, y)\nOut:\nBayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, \n              copy_X=True, fit_intercept=True, lambda_1=1e-06, \n              lambda_2=1e-06, n_iter=300, normalize=False, \n              tol=0.001, verbose=False)\nIn:\nfrom matplotlib.mlab import normpdf\n\nmean = regr.predict(test_x)\nstddev = regr.alpha_\nplt_x = np.linspace(mean-3*stddev, mean+3*stddev,100)\nplt.plot(plt_x, normpdf(plt_x, mean, stddev))\nplt.show()\nOut:\n```", "```py\nIn:\nfrom sklearn.linear_model import SGDClassifier\n\n# we create 50 separable points\nX, y = make_classification(n_samples=100, n_features=2, \n                           n_informative=2, n_redundant=0,\n                           n_clusters_per_class=1, class_sep=2, \n                           random_state=101)\n\n# fit the model\nclf = SGDClassifier(loss=\"hinge\", n_iter=500, random_state=101, \n                    alpha=0.001)\nclf.fit(X, y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 10)\nyy = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 10)\n\nX1, X2 = np.meshgrid(xx, yy)\nZ = np.empty(X1.shape)\nfor (i, j), val in np.ndenumerate(X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\nplt.contour(X1, X2, Z, levels, colors='k', linestyles=linestyles)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n\nplt.show()\n\nOut:\n```", "```py\nIn:\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = make_classification(n_samples=10000, n_features=20, \n                           n_informative=5, n_redundant=5,\n                           n_clusters_per_class=2, class_sep=1,\n                           random_state=101)\n\nX_train, X_test, y_train, y_test =  train_test_split(\n    X, y, test_size=0.3, random_state=101)\n\nclf_1 = SGDClassifier(loss=\"hinge\", random_state=101)\nclf_1.fit(X_train, y_train)\n\nclf_2 = SGDClassifier(loss=\"log\", random_state=101)\nclf_2.fit(X_train, y_train)\n\nprint('SVD            : ', accuracy_score(y_test, clf_1.predict(X_test)))\nprint('Log. Regression: ', accuracy_score(y_test, clf_2.predict(X_test)))\nOut:\nSVD            :  0.814333333333\nLog. Regression:  0.756666666667\n```", "```py\nIn:\n%timeit clf_1.fit(X_train, y_train)  \nOut:\n100 loops, best of 3: 3.16 ms per loop\nIn:\n%timeit clf_2.fit(X_train, y_train)\nOut:\n100 loops, best of 3: 4.86 ms per loop\n```", "```py\nIn:\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\nX = StandardScaler().fit_transform(boston['data'])\ny = boston['target']\n\nX_train, X_test, y_train, y_test =  train_test_split(\n    X, y, test_size=0.3, random_state=101)\n\nregr_1 = SVR(kernel='linear')\nregr_1.fit(X_train, y_train)\n\nregr_2 = SGDRegressor(random_state=101)\nregr_2.fit(X_train, y_train)\n\nprint('SVR            : ', mean_absolute_error(y_test, regr_1.predict(X_test)))\nprint('Lin. Regression: ', mean_absolute_error(y_test, regr_2.predict(X_test)))\nOut:\nSVR            :  3.67434988716\nLin. Regression:  3.7487663498\n```", "```py\nIn:\nfrom sklearn.tree import DecisionTreeRegressor\n\nregr = DecisionTreeRegressor(random_state=101)\nregr.fit(X_train, y_train)\n\nmean_absolute_error(y_test, regr.predict(X_test))\nOut:\n3.2842105263157895\n```", "```py\nIn:\nfrom sklearn.ensemble import BaggingRegressor\nbagging = BaggingRegressor(SGDRegressor(), n_jobs=-1,\n                           n_estimators=1000, random_state=101,\n                           max_features=0.8)\nbagging.fit(X_train, y_train)\nmean_absolute_error(y_test, bagging.predict(X_test))\nOut:\n3.8345485952100629\n```", "```py\nIn:\nfrom sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(n_estimators=100, \n                             n_jobs=-1, random_state=101)\nregr.fit(X_train, y_train)\nmean_absolute_error(y_test, regr.predict(X_test))\nOut:\n2.6412236842105261\n```", "```py\nIn:\nsorted(zip(regr.feature_importances_, boston['feature_names']),\n       key=lambda x: -x[0])\nOut:\n```", "```py\nIn:\nfrom sklearn.ensemble import AdaBoostRegressor\nbooster = AdaBoostRegressor(SGDRegressor(), random_state=101,\n                            n_estimators=100, learning_rate=0.01)\n\nbooster.fit(X_train, y_train)\nmean_absolute_error(y_test, booster.predict(X_test))\nOut:\n3.8621128094354349\n```", "```py\nIn:\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nregr = GradientBoostingRegressor(n_estimators=500, \n                                 learning_rate=0.01, \n                                 random_state=101)\nregr.fit(X_train, y_train)\nmean_absolute_error(y_test, regr.predict(X_test))\nOut:\n2.6148878419996806\n```", "```py\nIn:\nsorted(zip(regr.feature_importances_, boston['feature_names']),\n       key=lambda x: -x[0])\nOut:\n```", "```py\nIn:\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nregr = GradientBoostingRegressor('lad',\n                                 n_estimators=500, \n                                 learning_rate=0.1, \n                                 random_state=101)\nregr.fit(X_train, y_train)\nmean_absolute_error(y_test, regr.predict(X_test))\nOut:\n2.6216986613160258\n```"]