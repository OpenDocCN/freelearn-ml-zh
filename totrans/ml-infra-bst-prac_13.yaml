- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Training and Evaluating Classical Machine Learning Systems and Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估经典机器学习系统和神经网络
- en: Modern machine learning frameworks are designed to be user-friendly for programmers.
    The popularity of the Python programming environment (and R) has shown that designing,
    developing, and testing machine learning models can be focused on the machine
    learning task and not on the programming tasks. The developers of the machine
    learning models can focus on developing the entire system and not on programming
    the internals of the algorithms. However, this bears a darker side – a lack of
    understanding of the internals of the models and how they are trained, evaluated,
    and validated.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习框架被设计成对程序员友好。Python编程环境（以及R）的流行表明，设计、开发和测试机器学习模型可以专注于机器学习任务，而不是编程任务。机器学习模型的开发者可以专注于开发整个系统，而不是算法内部的编程。然而，这也带来了一些负面影响——对模型内部结构和它们是如何被训练、评估和验证的缺乏理解。
- en: In this chapter, I’ll dive a bit deeper into the process of training and evaluation.
    We’ll start with the basic theory behind different algorithms before learning
    how they are trained. We’ll start with the classical machine learning models,
    exemplified by decision trees. Then, we’ll gradually move toward deep learning,
    where we’ll explore both dense neural networks and more advanced types of networks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将更深入地探讨训练和评估的过程。我们将从不同算法背后的基本理论开始，然后学习它们是如何被训练的。我们将从经典的机器学习模型开始，以决策树为例。然后，我们将逐步转向深度学习，在那里我们将探索密集神经网络和更高级的网络类型。
- en: The most important part of this chapter is understanding the difference between
    training/evaluating algorithms and testing/validating the entire machine learning
    software system. I’ll explain this by describing how machine learning algorithms
    are used as part of a production machine learning system (or what the entire machine
    learning system looks like).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最重要的部分是理解训练/评估算法与测试/验证整个机器学习软件系统之间的区别。我将通过描述机器学习算法作为生产机器学习系统的一部分（或整个机器学习系统的样子）来解释这一点。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Training and test processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和测试过程
- en: Training classical machine learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练经典机器学习模型
- en: Training deep learning models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度学习模型
- en: Misleading results – problems with data leaks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误导性结果——数据泄露问题
- en: Training and testing processes
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试过程
- en: Machine learning has revolutionized the way we solve complex problems by enabling
    computers to learn from data and make predictions or decisions without being explicitly
    programmed. One crucial aspect of machine learning is training models, which involves
    teaching algorithms to recognize patterns and relationships in data. Two fundamental
    methods for training machine learning models are `model.fit()` and `model.predict()`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通过使计算机能够从数据中学习并做出预测或决策，而无需明确编程，从而彻底改变了我们解决复杂问题的方式。机器学习的一个关键方面是训练模型，这涉及到教算法识别数据中的模式和关系。训练机器学习模型的两种基本方法是
    `model.fit()` 和 `model.predict()`。
- en: The `model.fit()` function lies at the heart of training a machine learning
    model. It is the process by which a model learns from a labeled dataset to make
    accurate predictions. During training, the model adjusts its internal parameters
    to minimize the discrepancy between its predictions and the true labels in the
    training data. This iterative optimization process, often referred to as “learning,”
    allows the model to generalize its knowledge and perform well on unseen data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.fit()` 函数是训练机器学习模型的核心。它是模型从标记数据集中学习以做出准确预测的过程。在训练过程中，模型调整其内部参数以最小化其预测与训练数据中真实标签之间的差异。这种迭代优化过程，通常被称为“学习”，允许模型推广其知识并在未见过的数据上表现良好。'
- en: In addition to the training data and labels, the `model.fit()` function also
    takes various hyperparameters as arguments. These hyperparameters include the
    number of epochs (that is, the number of times the model will iterate over the
    entire dataset), the batch size (the number of samples processed before updating
    the parameters), and the learning rate (determining the step size for parameter
    updates). Properly tuning these hyperparameters is crucial to ensure effective
    training and prevent issues such as overfitting or underfitting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练数据和标签之外，`model.fit()`函数还接受各种超参数作为参数。这些超参数包括周期数（即模型将遍历整个数据集的次数）、批量大小（在更新参数之前处理的样本数量）和学习率（确定参数更新的步长）。正确调整这些超参数对于确保有效的训练和防止诸如过拟合或欠拟合等问题至关重要。
- en: Once the training process is complete, the trained model can be used to make
    predictions on new, unseen data. This is where the `model.predict()` method comes
    into play. Given a trained model and a set of input data, the `model.predict()`
    function applies the learned weights and biases to generate predictions or class
    probabilities. The predicted outputs can then be used for various purposes, such
    as classification, regression, or anomaly detection, depending on the nature of
    the problem at hand.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，训练好的模型就可以用于对新数据做出预测。这就是`model.predict()`方法发挥作用的地方。给定一个训练好的模型和一组输入数据，`model.predict()`函数将应用学习到的权重和偏差来生成预测或类别概率。预测输出可以用于各种目的，如分类、回归或异常检测，具体取决于手头问题的性质。
- en: 'We saw examples of this interface in previous chapters. Now, it is time to
    understand what’s under the hood of this interface and how the process of training
    works. In the previous chapter, we looked at this process as a black box, where
    the process was done once the program moved past the line with `model.fit()`.
    This is the basics of the process, but it is not only that. The process is iterative
    and depends on the algorithm/model that is being trained. As every model has different
    parameters, the fit function can take more parameters. The additional parameters
    can also be added when we instantiate the model, even before the training process.
    *Figure 10**.1* presents this process as a gray box:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中看到了这个界面的例子。现在，是时候了解这个界面底下的内容以及训练过程是如何进行的了。在上一章中，我们将这个过程视为一个黑盒，即程序跳过`model.fit()`行之后，这个过程就完成了。这是这个过程的基本原理，但不仅如此。这个过程是迭代的，并且取决于正在训练的算法/模型。由于每个模型都有不同的参数，拟合函数可以接受更多的参数。我们甚至可以在实例化模型时添加额外的参数，甚至在训练过程之前。*图10.1*
    将这个过程呈现为一个灰色框：
- en: '![Figure 10.1 – Gray box for training a machine learning model](img/B19548_10_1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 训练机器学习模型的灰色框](img/B19548_10_1.jpg)'
- en: Figure 10.1 – Gray box for training a machine learning model
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 训练机器学习模型的灰色框
- en: Before we start the training process, we split the data into training and test
    sets (which we discussed previously). At the same time, we select the parameters
    for the machine learning model that we use. These can be anything from the number
    of trees (for random forest) to the number of iterations and batch size for neural
    networks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练过程之前，我们将数据分为训练集和测试集（我们之前已经讨论过）。同时，我们选择我们使用的机器学习模型的参数。这些参数可以是任何东西，从随机森林中的树的数量到神经网络中的迭代次数和批量大小。
- en: The training process is iterative, where a model is trained on the data, evaluated
    internally, and then re-trained to find a better fit for the data. In this chapter,
    we’ll explore how this internal training works.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程是迭代的，其中模型在数据上训练，内部评估，然后重新训练以找到更适合数据的拟合。在本章中，我们将探讨这种内部训练是如何工作的。
- en: Finally, once the model has been trained, it is set for the testing process.
    In the testing process, we use pre-defined performance measures to check how well
    the model’s learned pattern can be reproduced for new data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦模型经过训练，它就准备好进行测试过程。在测试过程中，我们使用预定义的性能指标来检查模型学习到的模式对于新数据能否得到良好的重现。
- en: Training classical machine learning models
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练经典机器学习模型
- en: 'We’ll start by training a model that lets us look inside it. We’ll use the
    CART decision tree classifier, where we can visualize the actual decision tree
    that is trained. We’ll use the same numerical data we used in the previous chapter.
    First, let’s read the data and create the train/test split:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先训练一个模型，让我们能够查看其内部。我们将使用CART决策树分类器，我们可以可视化训练的实际决策树。我们将使用与上一章相同的数值数据。首先，让我们读取数据并创建训练/测试分割：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding code reads an Excel file named `'chapter_6_dataset_numerical.xlsx'`
    using the `pd.read_excel()` function from pandas. The file is read into a DataFrame
    called `dfDataAnt13`. The `sheet_name` parameter specifies the sheet within the
    Excel file to read, while the `index_col` parameter sets the first column as the
    index of the DataFrame.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用pandas库中的`pd.read_excel()`函数读取名为`'chapter_6_dataset_numerical.xlsx'`的Excel文件。文件被读取到一个名为`dfDataAnt13`的DataFrame中。`sheet_name`参数指定了要读取的Excel文件中的工作表，而`index_col`参数将第一列设置为DataFrame的索引。
- en: The code prepares the dataset for training a machine learning model. It assigns
    the independent variables (features) to the `X` variable by dropping the `'Defect'`
    column from the `dfDataAnt13` DataFrame using the `drop()` method. The dependent
    variable (target) is assigned to the `y` variable by selecting the `'Defect'`
    column from the `dfDataAnt13` DataFrame.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 代码为训练机器学习模型准备数据集。通过使用`drop()`方法从`dfDataAnt13` DataFrame中删除`'Defect'`列，将独立变量（特征）分配给`X`变量。通过从`dfDataAnt13`
    DataFrame中选择`'Defect'`列，将因变量（目标）分配给`y`变量。
- en: The `sklearn.model_selection.train_test_split()` function is used to split the
    dataset into training and testing sets. The `X` and `y` variables are split into
    `X_train`, `X_test`, `y_train`, and `y_test` variables. The `train_size` parameter
    is set to `0.9`, indicating that 90% of the data will be used for training and
    the remaining 10% will be used for testing. The `random_state` parameter is set
    to `42` to ensure reproducibility of the split.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn.model_selection.train_test_split()`函数将数据集分为训练集和测试集。`X`和`y`变量被分为`X_train`、`X_test`、`y_train`和`y_test`变量。`train_size`参数设置为`0.9`，表示90%的数据将用于训练，剩余的10%将用于测试。`random_state`参数设置为`42`以确保分割的可重复性。
- en: 'Once the data has been prepared, we can import the decision tree library and
    train the model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备就绪，我们可以导入决策树库并训练模型：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding code fragment imports the `DecisionTreeClassifier` class from
    the `sklearn.tree` module. An empty decision tree classifier object is created
    and assigned to the `decisionTreeModel` variable. This object will be trained
    on the dataset that was prepared in the previous fragment. The `fit()` method
    is called on the `decisionTreeModel` object to train the classifier. The `fit()`
    method takes the training data (`X_train`) and the corresponding target values
    (`y_train`) as input. The classifier will learn patterns and relationships in
    the training data to make predictions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段从`sklearn.tree`模块导入`DecisionTreeClassifier`类。创建了一个空的决策树分类器对象，并将其分配给`decisionTreeModel`变量。该对象将在之前片段中准备好的数据集上进行训练。在`decisionTreeModel`对象上调用`fit()`方法来训练分类器。`fit()`方法接受训练数据（`X_train`）和相应的目标值（`y_train`）作为输入。分类器将学习训练数据中的模式和关系以进行预测。
- en: 'The trained decision tree classifier is used to predict the target values for
    the test dataset (`X_test`). The `predict()` method is called on the `decisionTreeModel`
    object, passing `X_test` as the input. The predicted target values are stored
    in the `y_pred_cart` variable. The predicted model needs to be evaluated, so let’s
    evaluate the accuracy, precision, and recall of the model:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的决策树分类器用于预测测试数据集（`X_test`）的目标值。在`decisionTreeModel`对象上调用`predict()`方法，并将`X_test`作为输入。预测的目标值存储在`y_pred_cart`变量中。预测的模型需要评估，因此让我们评估模型的准确率、精确率和召回率：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code fragment results in the following output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段生成了以下输出：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The metrics show that the model is not that bad. It classified 83% of the data
    in the test set correctly. It is a bit more sensitive to the true positives (higher
    precision) than to true negatives (lower recall). This means that it tends to
    miss some of the defect-prone modules in its predictions. However, the decision
    tree model lets us take a look inside the model and explore the pattern that it
    learned from the data. The following code fragment does this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 指标显示模型表现不错。它正确地将测试集中的83%的数据分类。它对真实正例（更高的精确率）比对真实负例（较低的召回率）更敏感。这意味着它在预测中可能会错过一些缺陷易发模块。然而，决策树模型让我们能够查看模型内部，并探索它从数据中学到的模式。以下代码片段就是这样做的：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code fragment exports the decision tree in the form of text that
    we print. The `export_text()` function takes two arguments – the first one is
    the decision tree to visualize and the next one is the list of features. In our
    case, the list of features is the list of columns in the dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段以文本形式导出决策树，我们将其打印出来。`export_text()` 函数接受两个参数——第一个是要可视化的决策树，下一个是特征列表。在我们的情况下，特征列表是数据集的列列表。
- en: 'The entire decision tree is quite complex in this case, but the first decision
    path looks like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，整个决策树相当复杂，但第一个决策路径看起来是这样的：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This decision path looks very similar to a large `if-then` statement, which
    we could write ourselves if we knew the patterns in the data. This pattern is
    not simple, which means that the data is quite complex. It can be non-linear and
    requires complex models to capture the dependencies. It can also require a lot
    of effort to find the right balance between the performance of the model and its
    ability to generalize the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策路径看起来非常类似于一个大的 `if-then` 语句，如果我们知道数据中的模式，我们就可以自己编写它。这个模式并不简单，这意味着数据相当复杂。它可能是非线性的，需要复杂的模型来捕捉依赖关系。它也可能需要大量的努力来找到模型性能和其泛化数据能力之间的正确平衡。
- en: So, here is my best practice for working with this kind of model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是我处理这类模型的最佳实践。
- en: 'Best practice #54'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #54'
- en: If you want to understand your numerical data, use models that provide explainability.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要理解你的数值数据，请使用提供可解释性的模型。
- en: In the previous chapters, I advocated for using AutoML models as they are robust
    and save us a lot of trouble finding the right module. However, if we want to
    understand our data a bit better and understand the patterns, we can start with
    models such as decision trees. Their insight into the data provides us with a
    good overview of what we can get out of the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我提倡使用 AutoML 模型，因为它们稳健且能为我们节省大量寻找正确模块的麻烦。然而，如果我们想更好地理解我们的数据并了解模式，我们可以从决策树等模型开始。它们对数据的洞察为我们提供了关于我们可以从数据中获得什么的良好概述。
- en: 'As a counter-example, let’s look at the data from another module from the same
    dataset. Let’s read it and perform the split:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为反例，让我们看看来自同一数据集的另一个模块的数据。让我们读取它并执行分割：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let’s train a new model for that data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为这些数据训练一个新的模型：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So far, so good – no errors, no problems. Let’s check the performance of the
    model:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利——没有错误，没有问题。让我们检查一下模型的表现：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The performance, however, is not as high as it was previously:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，性能并不像之前那么高：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let’s print the tree:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印出树：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see, the results are also quite complex:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，结果也相当复杂：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If we look at the very first decision in this tree and the previous one, it
    is based on the WMC feature. **WMC** means **weighted method per class** and is
    one of the classical software metrics that was introduced in the 1990s by Chidamber
    and Kamerer. The metric captures both the complexity and the size of the class
    (in a way) and it is quite logical that large classes are more defect-prone –
    simply because there is more chance to make a mistake if there is more source
    code. In the case of this model, this is a bit more complicated as the model recognizes
    that the classes with WMC over 36 are more prone to errors than others, apart
    from classes that are over 64.5, which are less prone to errors. The latter is
    also a known phenomenon that large classes are also more difficult to test and
    therefore can contain undiscovered defects.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看看这个树中的第一个决策和上一个决策，它基于 WMC 特征。**WMC** 代表 **加权方法每类**，是20世纪90年代由 Chidamber
    和 Kamerer 提出的经典软件度量之一。该度量捕捉了类的复杂性和大小（以某种方式），因此大类的缺陷倾向性更强——简单地说，如果源代码更多，犯错误的机会就更大。在这个模型的情况下，这要复杂一些，因为模型认识到WMC超过36的类比其他类更容易出错，除了超过64.5的类，这些类不太容易出错。后者也是一个已知现象，即大类的测试也更为困难，因此可能包含未发现的缺陷。
- en: Here is my next best practice, which is about the explainability of models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我的下一个最佳实践，关于模型的可解释性。
- en: 'Best practice #55'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #55'
- en: The best models are those that capture the empirical phenomena in the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的模型是那些能够捕捉数据中经验现象的模型。
- en: Although machine learning models can capture any kind of dependencies, the best
    models are the ones that can capture logical, empirical observations. In the previous
    examples, the model could capture the software engineering empirical observations
    related to the size of the classes and their defect-proneness. Having a model
    that captures empirical relations leads to better products and explainable AI.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习模型可以捕捉任何类型的依赖关系，但最佳模型是那些能够捕捉逻辑和经验观察的模型。在先前的例子中，模型可以捕捉与类的大小及其易出错性相关的软件工程经验观察。拥有能够捕捉经验关系的模型可以带来更好的产品和可解释的人工智能。
- en: Understanding the training process
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解训练过程
- en: From the software engineer’s perspective, the training process is rather simple
    – we fit the model, validate it, and use it. We check how good the model is in
    terms of the performance metrics. If the model is good enough, and we can explain
    it, then we develop the entire product around it, or we use it in a larger software
    product.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件工程师的角度来看，训练过程相当简单——我们拟合模型，验证它，并使用它。我们检查模型在性能指标方面的好坏。如果模型足够好，并且我们可以解释它，那么我们就围绕它开发整个产品，或者将其用于更大的软件产品中。
- en: When the model does not learn anything useful, we need to understand why this
    is the case and whether there could be another model that can. We can use the
    visualization techniques we learned about in [*Chapter 6*](B19548_06.xhtml#_idTextAnchor074)
    to explore the data and clear it from noise using the techniques from [*Chapter
    4*](B19548_04.xhtml#_idTextAnchor049).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型没有学习到任何有用的东西时，我们需要了解为什么会出现这种情况，以及是否可能存在另一个可以做到的模型。我们可以使用我们在[*第6章*](B19548_06.xhtml#_idTextAnchor074)中学到的可视化技术来探索数据，并使用[*第4章*](B19548_04.xhtml#_idTextAnchor049)中的技术清除噪声。
- en: Now, let’s explore the process of how the decision tree model learns from the
    data. The `DecisionTree` classifier learns from the provided data by recursively
    partitioning the feature space based on the values of the features in the training
    dataset. It constructs a binary tree where each internal node represents a feature
    and a decision rule based on a threshold value, and each leaf node represents
    a predicted class or outcome.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索决策树模型如何从数据中学习的流程。`DecisionTree`分类器通过递归地根据训练数据集中特征的值对特征空间进行分区来从提供的数据中学习。它构建一个二叉树，其中每个内部节点代表一个特征和一个基于阈值值的决策规则，每个叶节点代表一个预测的类别或结果。
- en: 'The training is done in steps:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程分为以下步骤：
- en: '**Selecting the best feature**: The classifier evaluates different features
    and determines the one that best separates the data into different classes. This
    is typically done using a measure of impurity or information gain, such as Gini
    impurity or entropy.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择最佳特征**：分类器评估不同的特征，并确定最佳分离数据为不同类别的特征。这通常是通过不纯度度量或信息增益来完成的，例如基尼不纯度或熵。'
- en: '**Splitting the dataset**: Once the best feature has been selected, the classifier
    splits the dataset into two or more subsets based on the values of that feature.
    Each subset represents a different branch or path in the decision tree.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分割数据集**：一旦选定了最佳特征，分类器将数据集根据该特征的值分割成两个或更多子集。每个子集代表决策树中的不同分支或路径。'
- en: '**Repeating the process recursively**: The preceding steps are repeated for
    each subset or branch of the decision tree, treating them as separate datasets.
    The process continues until a stopping condition is met, such as reaching a maximum
    depth, a minimum number of samples at a node, or other predefined criteria.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**递归重复过程**：上述步骤对决策树的每个子集或分支重复进行，将它们视为单独的数据集。这个过程会继续进行，直到满足停止条件，例如达到最大深度、节点上的最小样本数或其他预定义标准。'
- en: '**Assigning class labels**: At the leaf nodes of the decision tree, the classifier
    assigns class labels based on the majority class of the samples in that region.
    This means that when making predictions, the classifier assigns the most frequent
    class in the leaf node to the unseen samples that fall into that region.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配类别标签**：在决策树的叶节点处，分类器根据该区域样本的多数类别分配类别标签。这意味着在做出预测时，分类器将叶节点中最频繁的类别分配给落入该区域的未见样本。'
- en: During the learning process, the `DecisionTree` classifier aims to find the
    best splits that maximize the separation of classes and minimize the impurity
    within each resulting subset. By recursively partitioning the feature space based
    on the provided training data, the classifier learns decision rules that allow
    it to make predictions for unseen data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，`DecisionTree`分类器旨在找到最佳分割，以最大化类别的分离并最小化每个结果子集中的不纯度。通过根据提供的训练数据递归地根据特征空间进行分区，分类器学习决策规则，使其能够对未见数据做出预测。
- en: It’s important to note that decision trees are prone to overfitting, meaning
    they can memorize the training data too well and not generalize well to new data.
    Techniques such as pruning, limiting the maximum depth, or using ensemble methods
    such as random forest can help mitigate overfitting and improve the performance
    of decision tree models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，决策树容易过拟合，这意味着它们可以过度记住训练数据，并且对新数据泛化能力不强。例如剪枝、限制最大深度或使用随机森林等集成方法可以帮助减轻过拟合并提高决策树模型的表现。
- en: We used the random forest classifier previously in this book, so we won’t dive
    into the details here. Although random forests are better at generalizing data,
    they are opaque compared to decision trees. We cannot explore what the model learned
    – we can only explore which features contribute the most to the verdict.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中已经使用过随机森林分类器，所以这里不会深入细节。尽管随机森林在泛化数据方面表现更好，但与决策树相比，它们是不透明的。我们无法探索模型学到了什么——我们只能探索哪些特征对判决贡献最大。
- en: Random forest and opaque models
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林和不透明模型
- en: Let’s train the random forest classifier based on the same data as in the counter-example
    and check whether the model performs better and whether the model uses similar
    features as the `DecisionTree` classifier in the original counter-example.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于与反例中相同的数据训练随机森林分类器，并检查模型是否表现更好，以及模型是否使用与原始反例中`DecisionTree`分类器相似的特征。
- en: 'Let’s instantiate, train, and validate the model on the same data using the
    following fragment of code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码片段实例化、训练和验证模型：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After evaluating the model, we obtain the following performance metrics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型后，我们获得了以下性能指标：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Admittedly, these metrics are different than the metrics in the decision trees,
    but the overall performance is not that much different. The difference in accuracy
    of 0.03 is negligible. First, we can extract the important features, reusing the
    same techniques that were presented in [*Chapter 5*](B19548_05.xhtml#_idTextAnchor060):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，这些指标与决策树中的指标不同，但整体性能并没有太大的差异。0.03的准确度差异是可以忽略不计的。首先，我们可以提取重要特征，重复使用在[*第5章*](B19548_05.xhtml#_idTextAnchor060)中介绍过的相同技术：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can visualize the set of features used in the decision by executing the
    following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行以下代码来可视化决策中使用的特征集：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This code helps us understand the importance chart shown in *Figure 10**.2*.
    Here, again, the WMC is the most important feature. This means that a lot of trees
    in the forest are using this metric to make decisions. However, we do not know
    the algorithm since the forest is an ensemble classifier – it uses voting for
    the decisions – meaning that always more than one tree is used when making the
    final call/prediction:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码帮助我们理解*图10*.*2*中显示的重要性图表。在这里，WMC（加权方法计数）是最重要的特征。这意味着森林中有许多树使用此指标来做出决策。然而，由于森林是一个集成分类器——它使用投票来做出决策——这意味着在做出最终调用/预测时总是使用多棵树：
- en: '![Figure 10.2 – Feature importance chart for the random forest classifier.](img/B19548_10_2.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 随机森林分类器的特征重要性图表。](img/B19548_10_2.jpg)'
- en: Figure 10.2 – Feature importance chart for the random forest classifier.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 随机森林分类器的特征重要性图表。
- en: Please note that the model is more complex than just a linear combination of
    these features. This chart illustrates something that is not a best practice,
    but a best experience. So, I will use it as a best practice to illustrate the
    importance of it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该模型比这些特征的线性组合更复杂。此图表展示了不是最佳实践，而是一种最佳经验。因此，我将将其用作最佳实践来展示其重要性。
- en: 'Best practice #56'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #56'
- en: Simple, but explainable, models can often capture data in a good way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 简单但可解释的模型通常可以很好地捕捉数据。
- en: What I’ve learned throughout my experiments with different types of data is
    that if there is a pattern, a simple model will capture it. If there is no pattern,
    or if the data has a lot of exceptions from rules, then even the most complex
    models will have problems in finding the patterns. Therefore, if you cannot explain
    your results, do not use them in your product, as there is a chance that these
    results will make the products quite useless.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我使用不同类型数据的实验过程中，我所学到的经验是，如果有模式，一个简单的模型就能捕捉到它。如果没有模式，或者数据有很多不符合规则的情况，那么即使是最复杂的模型在寻找模式时也会遇到问题。因此，如果你不能解释你的结果，不要将它们用于你的产品中，因为这些结果可能会使产品变得相当无用。
- en: However, there is a light at the end of the tunnel here. Some models can capture
    very complex patterns, but they are opaque – neural networks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这个隧道尽头有一线光明。一些模型可以捕捉非常复杂的模式，但它们是透明的——神经网络。
- en: Training deep learning models
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练深度学习模型
- en: Training a dense neural network involves various steps. First, we prepare the
    data. This typically involves tasks such as feature scaling, handling missing
    values, encoding categorical variables, and splitting the data into training and
    validation sets.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 训练密集神经网络涉及多个步骤。首先，我们准备数据。这通常涉及特征缩放、处理缺失值、编码分类变量以及将数据分为训练集和验证集。
- en: Then, we define the architecture of the dense neural network. This includes
    specifying the number of layers, the number of neurons in each layer, the activation
    functions to be used, and any regularization techniques, such as dropout or batch
    normalization.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义密集神经网络的架构。这包括指定层数、每层的神经元数量、要使用的激活函数以及任何正则化技术，如dropout或批量归一化。
- en: Once the model has been defined, we need to initialize it. We create an instance
    of the neural network model based on the defined architecture. This involves creating
    an instance of the neural network class or using a predefined model architecture
    available in a deep learning library. We also need to define a loss function that
    quantifies the error between the predicted output of the model and the actual
    target values. The choice of loss function depends on the nature of the problem,
    such as classification (cross-entropy) or regression (mean squared error).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了模型，我们就需要初始化它。我们根据定义的架构创建神经网络模型的一个实例。这涉及到创建神经网络类的一个实例或使用深度学习库中可用的预定义模型架构。我们还需要定义一个损失函数，该函数量化模型预测输出与实际目标值之间的误差。损失函数的选择取决于问题的性质，例如分类（交叉熵）或回归（均方误差）。
- en: In addition to the loss function, we need an optimizer. The optimizer algorithm
    will update the weights of the neural network during training. Common optimizers
    include **stochastic gradient descent** (**SGD**), Adam, and RMSprop.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了损失函数之外，我们还需要一个优化器。优化器算法将在训练过程中更新神经网络的权重。常见的优化器包括**随机梯度下降（SGD**）、Adam和RMSprop。
- en: 'Then, we can train the model. Here, we iterate over the training data for multiple
    epochs (passes through the entire dataset). In each epoch, perform the following
    steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以训练模型。在这里，我们遍历训练数据多次（整个数据集的遍历）。在每个epoch（遍历整个数据集）中，执行以下步骤：
- en: '**Forward pass**: We feed a batch of input data into the model and compute
    the predicted output.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**前向传播**：我们将一批输入数据输入到模型中，并计算预测输出。'
- en: '**Compute loss**: We compare the predicted output with the actual target values
    using the defined loss function to calculate the loss.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算损失**：我们使用定义的损失函数将预测输出与实际目标值进行比较，以计算损失。'
- en: '**Backward pass**: We propagate the loss backward through the network to compute
    the gradients of the weights concerning the loss using backpropagation.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**反向传播**：我们通过反向传播将损失反向传播到网络中，以计算权重相对于损失的梯度。'
- en: '**Update the weights**: We use the optimizer to update the weights of the neural
    network based on the computed gradients, adjusting the network parameters to minimize
    the loss.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新权重**：我们使用优化器根据计算出的梯度来更新神经网络的权重，调整网络参数以最小化损失。'
- en: We repeat these steps for each batch in the training data until all batches
    have been processed.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对训练数据中的每个批次重复这些步骤，直到所有批次都已被处理。
- en: Finally, we need to perform the validation process, just like in the previous
    models. Here, we compute a validation metric (such as accuracy or mean squared
    error) to assess how well the model is generalizing to unseen data. This helps
    us monitor the model’s progress and detect overfitting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要执行验证过程，就像在之前的模型中一样。在这里，我们计算一个验证指标（例如准确度或均方误差）来评估模型对未见数据的泛化能力。这有助于我们监控模型的进展并检测过拟合。
- en: Once the model has been trained and validated, we can evaluate its performance
    on a separate test dataset that was not used during training or validation. Here,
    we calculate relevant evaluation metrics to assess the model’s accuracy, precision,
    recall, or other desired metrics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过训练和验证，我们就可以在未用于训练或验证的单独测试数据集上评估其性能。在这里，我们计算相关的评估指标来评估模型的准确度、精确度、召回率或其他所需的指标。
- en: 'So, let’s do this for our dataset. First, we must define the architecture of
    the model using the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们为我们的数据集做这件事。首先，我们必须使用以下代码定义模型的架构：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we define a class called `NeuralNetwork`, which is a subclass of `nn.Module`.
    This class represents our neural network model. It has two fully connected layers
    (`fc1` and `fc2`) with a `ReLU` activation function in between. The network looks
    something like the one shown in *Figure 10**.3*:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个名为 `NeuralNetwork` 的类，它是 `nn.Module` 的子类。这个类代表我们的神经网络模型。它有两个全连接层（`fc1`
    和 `fc2`），层间使用 `ReLU` 激活函数。网络看起来就像 *图10.3* 中所示的那样：
- en: '![Figure 10.3 – Neural network used for predicting defects.](img/B19548_10_3.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 用于预测缺陷的神经网络](img/B19548_10_3.jpg)'
- en: Figure 10.3 – Neural network used for predicting defects.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 用于预测缺陷的神经网络。
- en: This visualization was created using [http://alexlenail.me/NN-SVG/index.html](http://alexlenail.me/NN-SVG/index.html).
    The number of neurons in the hidden layer is 64, but in this figure, only 16 are
    shown to make it more readable. The network starts with 6 input neurons, then
    64 neurons in the hidden layer (middle), and two neurons for the decision classes
    at the end.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可视化是使用[http://alexlenail.me/NN-SVG/index.html](http://alexlenail.me/NN-SVG/index.html)创建的。隐藏层中的神经元数量是64，但在这个图中，只显示了16个，以便使其更易于阅读。网络从6个输入神经元开始，然后是隐藏层（中间）的64个神经元，最后是两个用于决策类别的神经元。
- en: 'Now, we can define the hyperparameters for training the network and instantiate
    it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义训练网络的超参数并实例化它：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we create an instance of the `NeuralNetwork` class called `model` with
    the specified input size, hidden size, and number of output classes, as we defined
    in the first code fragment. We define the loss function (cross-entropy loss) and
    the optimizer (Adam optimizer) to train the model. The data is then converted
    into PyTorch tensors using `torch.Tensor()` and `torch.LongTensor()`. Finally,
    we say that we want to train the model in 10,000 epochs (iterations) with 32 elements
    (data points) in each iteration:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个名为 `model` 的 `NeuralNetwork` 类实例，具有指定的输入大小、隐藏大小和输出类数量，正如我们在第一个代码片段中定义的那样。我们定义了损失函数（交叉熵损失）和优化器（Adam优化器）来训练模型。然后，使用
    `torch.Tensor()` 和 `torch.LongTensor()` 将数据转换为PyTorch张量。最后，我们表示我们希望在10,000个epoch（迭代）中训练模型，每个迭代包含32个元素（数据点）：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can get the predictions for the test data and obtain the performance
    metrics:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获取测试数据的预测并获取性能指标：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The performance metrics are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标如下：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So, this is a bit better than the previous models, but it’s not great. The patterns
    are just not there. We could make the network larger by increasing the number
    of hidden layers, but this does not make the predictions better.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这比之前的模型要好一些，但并不出色。模式根本不存在。我们可以通过增加隐藏层的数量来使网络更大，但这并不会使预测变得更好。
- en: Misleading results – data leaking
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 误导性结果 – 数据泄露
- en: In the training process, we use one set of data and in the test set, we use
    another set. The best training process is when these two datasets are separate.
    If they are not, we get into something that is called a data leak problem. This
    problem is when we have the same data points in both the train and test sets.
    Let’s illustrate this with an example.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们使用一组数据，在测试集中使用另一组数据。最佳的训练过程是当这两个数据集是分开的时候。如果它们不是分开的，我们就会遇到一个称为数据泄露问题的情况。这个问题是指我们在训练集和测试集中有相同的数据点。让我们用一个例子来说明这一点。
- en: 'First, we need to create a new split, where we have some data points in both
    sets. We can do that by using the split function and setting 20% of the data points
    to the test set. This means that at least 10% of the data points are in both sets:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个新的分割，其中两个集合中都有一些数据点。我们可以通过使用split函数并将20%的数据点设置为测试集来实现这一点。这意味着至少有10%的数据点在两个集合中：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we can use the same code to make predictions on this data and then calculate
    the performance metrics:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用相同的代码对这组数据进行预测，然后计算性能指标：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The results are as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The results are better than before. However, they are only better because 10%
    of the data points were used in both the training and the test sets. This means
    that the model performs much worse than the metrics suggest. Hence, we’ve come
    to my next best practice.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果比之前更好。然而，它们之所以更好，仅仅是因为10%的数据点被用于训练和测试集。这意味着模型的性能比指标所暗示的要差得多。因此，我们得出了我的下一个最佳实践。
- en: 'Best practice #56'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #56'
- en: Always make sure that the data points in both the train and test sets are separate.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要确保训练集和测试集中的数据点是分开的。
- en: Although we made this mistake on purpose here, it is quite easy to make it in
    practice. Please note the `random_state=42` parameter in the split function. Setting
    it explicitly ensures that the split is repeatable. However, if we do not do this,
    we can end up with different splits every time we make them and thus we can end
    up with the data leak problem.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这里故意犯了这个错误，但在实践中很容易犯这个错误。请注意split函数中的`random_state=42`参数。显式设置它确保了分割的可重复性。然而，如果我们不这样做，我们每次进行分割时都可能会得到不同的分割，从而可能导致数据泄露问题。
- en: The data leak problem is even more difficult to discover when we’re working
    with images or text. Just the fact that an image comes from two different files
    does not guarantee that it is different. For example, images taken one after another
    during highway driving will be different but will not be too different, and if
    they end up in test and train sets, we get a whole new dimension of the data leak
    problem.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理图像或文本时，数据泄露问题甚至更难发现。仅仅因为一个图像来自两个不同的文件，并不能保证它是不同的。例如，在高速公路上连续拍摄的图像将不同，但不会太不同，如果它们最终出现在测试集和训练集中，我们就会得到数据泄露问题的一个全新维度。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed various topics related to machine learning and
    neural networks. We explained how to read data from an Excel file using the pandas
    library and prepare the dataset for training a machine learning model. We explored
    the use of decision tree classifiers and demonstrated how to train a decision
    tree model using scikit-learn. We also showed how to make predictions using the
    trained model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了与机器学习和神经网络相关的各种主题。我们解释了如何使用pandas库从Excel文件中读取数据，并为训练机器学习模型准备数据集。我们探讨了决策树分类器的使用，并展示了如何使用scikit-learn训练决策树模型。我们还展示了如何使用训练好的模型进行预测。
- en: Then, we discussed how to switch from a decision tree classifier to a random
    forest classifier, which is an ensemble of decision trees. We explained the necessary
    code modifications and provided an example. Next, we shifted our focus to using
    a dense neural network in PyTorch. We described the process of creating the neural
    network architecture, training the model, and making predictions using the trained
    model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了如何从决策树分类器切换到随机森林分类器，后者是决策树的集成。我们解释了必要的代码修改，并提供了示例。接下来，我们将重点转向在PyTorch中使用密集神经网络。我们描述了创建神经网络架构、训练模型以及使用训练好的模型进行预测的过程。
- en: Lastly, we explained the steps involved in training a dense neural network,
    including data preparation, model architecture, initializing the model, defining
    a loss function and optimizer, the training loop, validation, hyperparameter tuning,
    and evaluation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们解释了训练密集神经网络所涉及的步骤，包括数据准备、模型架构、初始化模型、定义损失函数和优化器、训练循环、验证、超参数调整和评估。
- en: Overall, we covered a range of topics related to machine learning algorithms,
    including decision trees, random forests, and dense neural networks, along with
    their respective training processes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，我们涵盖了与机器学习算法相关的一系列主题，包括决策树、随机森林和密集神经网络，以及它们各自的训练过程。
- en: In the next chapter we explore how to train more advanced machine learning models
    - for example AutoEncoders.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何训练更先进的机器学习模型——例如自编码器。
- en: References
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Chidamber, S.R. and C.F. Kemerer, A metrics suite for object oriented design.
    IEEE Transactions on Software Engineering, 1994\. 20(6):* *p. 476–493.*'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chidamber, S.R. 和 C.F. Kemerer, 对面向对象设计的度量集。IEEE 软件工程 Transactions, 1994\.
    20(6):* *p. 476–493.*'
