- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training Machine Learning Models in AMLS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training **Machine Learning** (**ML**) models in **Azure Machine Learning**
    (**AML**) is key to enabling your data science workload. Typically, during the
    model creation process, data is split into test and training datasets. Models
    are then built with the training data and evaluated using the test dataset. During
    this process, many algorithms are selected and used to answer the question: what
    model will provide the best results on an unseen dataset? AML has the capability
    to log metrics, taking snapshots of the code that produced a given model performance
    to enable answering this question. AML comes with a variety of accelerating capacities.
    During this chapter, we will focus on the creation of experiments to train models
    and on the basic functionality of AML experiments to unlock using compute instances,
    compute clusters, and registered datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Model training can be established through the AML Python SDK or the designer
    for a low-code experience. During the model training process, different compute
    resources can be used to accomplish this task. In this chapter, we will explore
    using compute instances for basic model training and compute clusters for a repeatable
    training environment, as well as through the designer for a low-code experience.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use datasets to build ML models with AML
    compute and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Training code-free models with the Designer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training on a compute instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training on a compute cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to [*Chapter 1*](B18003_01.xhtml#_idTextAnchor020), *Introducing the Azure
    Machine Learning Service*, to create the environment workspace to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Have access to the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a web browser, preferably Google Chrome or Microsoft Edge Chromium.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To access the Azure Machine Learning Service workspace, please go to this address:
    [https://ml.azure.com](https://ml.azure.com). Select the workspace from the drop-down
    list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training code-free models with the designer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let’s see what options are available for code-free modeling
    using the **designer**. The Designer enables data scientists to create models
    without the need to write code. It makes it easy for any data scientist to build
    and compare models. Within the designer, the development environment is a graphical
    user interface that allows right-clicking to change the settings for a given step.
    The interface allows not only the development of the model but also one-click
    deployment to deploy to various styles of APIs, including real-time or batch endpoints,
    which are REST APIs that can be consumed by other business applications or downstream
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, drag a task to connect to a data source and configure the properties
    to connect to the data source. These properties include the dataset name and connection
    string parameters, including the username and password.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset using the user interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s go through the steps to create a dataset using the user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the left side of the workspace, select **Designer**, as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Designer icon ](img/B18003_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Designer icon
  prefs: []
  type: TYPE_NORMAL
- en: Once you are on the **Designer** screen, you’ll see that the screen is divided
    into two parts. The top part shows some examples or samples available for anyone
    to test. All samples use open source datasets, so anyone can test, develop, and
    learn from them. The bottom parts show the experiments you created, and if you
    click on an experiment, the graph will be shown in the canvas to the right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The designer experience is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Designer experience ](img/B18003_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Designer experience
  prefs: []
  type: TYPE_NORMAL
- en: Before creating an experiment, make sure a compute cluster is created, as shown
    in [*Chapter 1*](B18003_01.xhtml#_idTextAnchor020), *Introducing the Azure Machine
    Learning Service*, or you can also find it in this chapter where we show how to
    create a cluster for remote jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the **+** button to create a new experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Create a new designer experiment ](img/B18003_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Create a new designer experiment
  prefs: []
  type: TYPE_NORMAL
- en: Clicking `Classificationsample1` as the name. Now, we also have to provide a
    compute target. From the dropdown, select **Compute cluster** and then select
    the cluster that you have created. The cluster can be CPU-based or GPU-based.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: A compute cluster provides a way to scale compute across multiple VMs to scale
    horizontally. Select CPU or GPU based on the use case. For example, most tabular
    datasets with less-than-TB size can use CPU-based clusters. For vision-based workloads,
    please choose GPU clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the default data store, which usually would be the Azure Machine Learning
    service’s underlying data store. Select **workspaceblobstore** as your choice
    and provide experiment details in the **Draft details** section. Then, click on
    the **X** icon on the right side to see the entire canvas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Create a new designer experiment canvas ](img/B18003_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Create a new designer experiment canvas
  prefs: []
  type: TYPE_NORMAL
- en: Note, from *Figure 3**.4*, that on the left side you have the menu options.
    This is where we have all the tasks that can be dragged and dropped onto the canvas
    to build a model. The menu is categorized for different operations, such as datasets,
    data input and output, data transformation, and ML algorithms. Consider reading
    the product documentation online for each task and its properties and functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build a model. The first thing you are going to bring is the dataset that
    you have it registered from [*Chapter 2*](B18003_02.xhtml#_idTextAnchor038), *Working
    with Data in AMLS*. In the left menu, expand **Datasets**, select the one to use,
    and drag and drop it to the canvas.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I am going to walk you through a basic ML modeling step.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, I have created a Titanic open source data sample and configured
    as a dataset. Here is how it looks when you drag and drop it on the canvas. Right-click
    on the dataset and the property sheet will show on the right side of the screen.
    Select **Output** and view the dataset sample rows. An option to view the data
    profile will also be provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Dataset on canvas ](img/B18003_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Dataset on canvas
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in the search box of the left menu, type `Select Columns in Dataset` and
    drag and drop it on the canvas, then connect the previous dataset to **Select
    Columns in Dataset**. Now, click **Select Columns in Dataset**, go to **Select
    columns**, and select the **Edit columns** to use for modeling. This task allows
    us to choose which columns we can use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Select Columns in Dataset ](img/B18003_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Select Columns in Dataset
  prefs: []
  type: TYPE_NORMAL
- en: In the **Edit Columns** section, choose the columns we need. There are options
    to select all columns, all features, or all labels or choose by name. For the
    preceding sample, we chose to select by column name and selected columns from
    the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next transformation is going to be for cleaning data. This is a common
    task to perform to get rid of unwanted rows that have no values, null values,
    and so on. Type `Clean Missing Data` in the search box and drop it on the canvas,
    then connect it to the previous task, in this case, **Select Columns** **in Dataset**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Clean Missing Data ](img/B18003_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Clean Missing Data
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Edit column** and select the columns or features to clean. Feature
    or column selection can be either by rule or by name. When you click on the text
    box, a list will pop up, as shown in *Figure 3**.8*. Select the columns or features
    you need to model. In this example, select all the columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Edit column ](img/B18003_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Edit column
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have done some feature engineering, it’s time to split the data
    for training and testing. It’s a very common practice during ML to split the dataset
    into training and testing. Usually, the split is 70% training and 30% testing.
    Type `Split Data` in the search box and drag and drop it on the canvas to connect
    it to the previous task. The property sheet will show up on the right. For `0.7`.
    This setting allows us to split the dataset into 70% for training and the remaining
    30% for the test dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Split Data** activity enables you to choose the split, whether the split
    should be randomized, the seed used for splitting, and whether the split should
    be stratified, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Split Data ](img/B18003_03_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Split Data
  prefs: []
  type: TYPE_NORMAL
- en: After `Two-Class Boosted Decision Tree` and drag and drop it onto the canvas.
    Since this is a simple survived ML task, we are choosing two-class boosted decision
    tree. You can select another two-class algorithm as needed. Once the algorithm
    is selected, we need to configure its parameters. Typically, you can leave most
    as the default values, as these parameters are chosen based on statistics of various
    model runs. You should ensure that you set your random seed as shown in *Figure
    3**.10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot showcases the parameters to set for **Two-Class Boosted**
    **Decision Tree**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Two-Class Boosted Decision Tree ](img/B18003_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Two-Class Boosted Decision Tree
  prefs: []
  type: TYPE_NORMAL
- en: Now, on the left menu, in the search box, type in `Train Model` and drop it
    onto the canvas. For **Train Model**, we need to connect to two inputs, which
    are one from the model output and the other from the first output of **Split Data**.
    Make sure the algorithm and training data are connected. Then, select **Train
    Model** and the label column to predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **Edit Column** and then set **Label column** to **Survived**. **Model
    explanations** is a feature that allows us to enable model expandability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Train Model ](img/B18003_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Train Model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can score the model. Go to the left menu and in the search box, type
    in `Score Model` and drag and drop it onto the canvas. **Score Model** must be
    connected to two outputs. The first input should be connected to the output of
    **Train Model** and the second input has to be connected to **Split Data**’s second
    output. **Score Model** takes the training output model and then consumes the
    test dataset to score the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Score Model ](img/B18003_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Score Model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the last step here is adding the **Evaluate Model** output and getting
    the model accuracy scores. For this, from the left menu, drag and drop **Evaluate
    Model** onto the canvas. For **Evaluate Model**, the first input should be connected
    to the **Score Model** output. There are no settings for this task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Evaluate Model ](img/B18003_03_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Evaluate Model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, click **Save** on the canvas. We have now completed our development of
    the model. In the upper-right corner, you will see a button for **Submit**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Submit experiment ](img/B18003_03_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Submit experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Submit** and provide a name for the experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.15 – New experiment ](img/B18003_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – New experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click `Classificationsample1`. Provide a job description for the experiment,
    which is optional. Leave **Compute target** as **Default** as we selected at the
    beginning of the experiment. Wait for the experiment to complete. Once the experiment
    is started, you should see the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Experiment status ](img/B18003_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Experiment status
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, you can also see the run details by clicking on **Jobs**, and then
    for **Classificationsample1**, click on its latest job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the experiment is completed, you should see all the tasks in green, as
    per the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Experiment completed ](img/B18003_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Experiment completed
  prefs: []
  type: TYPE_NORMAL
- en: 'By clicking on the **Evaluate Model** task, we can review the results of the
    **Evaluate Model** task. In our case, I am only going to show **Evaluate Model**
    to see the confusion matrix and the **Receiver Operating Characteristic** (**ROC**)
    curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – ROC curve ](img/B18003_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to reviewing the ROC curve, we can also review the confusion matrix,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Confusion matrix ](img/B18003_03_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how to train a model using the Designer in AMLS. Using
    the Designer, we can train the model using the drag-and-drop interface, as well
    as carrying out feature engineering to provide an improved model. As a follow-up,
    take a look at each task in the user interface.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to train a model by writing Python code
    and running it on a compute instance.
  prefs: []
  type: TYPE_NORMAL
- en: Training on a compute instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can train a model on a compute instance or on a compute cluster. In this
    section, we will use our existing compute instance before continuing with training
    on a compute cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To begin working with a compute instance, we will need to turn on the compute
    instance that was created in [*Chapter 1*](B18003_01.xhtml#_idTextAnchor020),
    *Introducing the Azure Machine* *Learning Service*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to train a model on a compute instance within AMLS:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the left side of the workspace, click **Compute**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Compute instance icon ](img/B18003_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Compute instance icon
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Compute** screen, select your compute instance and select **Start**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Start compute ](img/B18003_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Start compute
  prefs: []
  type: TYPE_NORMAL
- en: Your compute instance will change from **Stopped** to **Starting** status. Once
    the compute instance moves from **Starting** to **Running** status, it is ready
    for use.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the Terminal blue hyperlink under applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will open the terminal on your compute instance. Note your user will be
    included in the directory path. Type the following into the terminal to clone
    the sample notebooks into your working directory: [https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering.git](https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering.git).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clicking on the refresh icon shown in *Figure 3**.22* will display the repository
    in your working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Refresh ](img/B18003_03_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Refresh
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking the refresh icon within AML, the cloned repository will be displayed
    in your working directory, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Azure-Machine-Learning-Engineering ](img/B18003_03_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Azure-Machine-Learning-Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on the **train-on-compute-instance.ipynb** notebook will bring up the
    notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 3**.24* enables you to connect to the Azure
    ML workspace in your compute instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Connecting to the workspace ](img/B18003_03_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – Connecting to the workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 3**.25* uses pandas to read in the dataset
    that is located in the `my_data` folder and then view it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Data exploration: Reading in the dataset and viewing the data
    ](img/B18003_03_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25 – Data exploration: Reading in the dataset and viewing the data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next cell checks to see what columns have null data have so that we can
    cleanse the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Data exploration: View null values in the dataset ](img/B18003_03_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26 – Data exploration: View null values in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Pclass` and `Sex` when the value is `null`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Data cleansing: Populating the Age column’s null values with
    the medians for Pclass and Sex ](img/B18003_03_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27 – Data cleansing: Populating the Age column’s null values with
    the medians for Pclass and Sex'
  prefs: []
  type: TYPE_NORMAL
- en: 'For `Cabin`, we can use the first character and treat it as a categorical variable
    during model creation to represent the location in the ship. If the value is `null`,
    we will replace it with `X`, representing an unknown `Cabin` type, as shown in
    *Figure 3**.28*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.28 – Feature engineering: Populating the Loc column ](img/B18003_03_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.28 – Feature engineering: Populating the Loc column'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will drop the `Cabin` column, given we have retrieved the useful information
    from it, and also remove the `Ticket` column, as the `Loc` column will now represent
    the location within the ship, as shown in *Figure 3**.29*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.29 – Data cleansing: Dropping unnecessary columns ](img/B18003_03_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.29 – Data cleansing: Dropping unnecessary columns'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with feature engineering, we will create a column called `GroupSize`,
    as groups of passengers that were family members may have had a greater chance
    at survival by working together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.30 – Feature engineering: Group size ](img/B18003_03_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.30 – Feature engineering: Group size'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the values for the `''Embarked''` column are limited, and we only have
    two rows with null values, we will arbitrarily set them to the value of `''S''`,
    as shown in *Figure 3**.31*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31 – Data cleansing: Populating null value with a default value
    ](img/B18003_03_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.31 – Data cleansing: Populating null value with a default value'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 3**.32*, we will drop the columns that we won’t be using
    during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32 – Data cleansing: Dropping columns not needed for model training
    ](img/B18003_03_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.32 – Data cleansing: Dropping columns not needed for model training'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a folder to hold our training data, as shown in *Figure 3**.33*.
    This is not required but will keep our directories clear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.34 – View the clean_training_data folder in the directory](img/B18003_03_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.33 – Folder creation
  prefs: []
  type: TYPE_NORMAL
- en: 'The directory can be seen on the left pane in the *Notebooks* section after
    clicking on the refresh button, as shown in *Figure 3**.34*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18003_03_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.34 – View the clean_training_data folder in the directory
  prefs: []
  type: TYPE_NORMAL
- en: 'With the newly created directory, we can save the dataset to the `clean_training_data`
    directory, as shown in *Figure 3**.35*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.35 – Saving the engineered dataset ](img/B18003_03_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.35 – Saving the engineered dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin with importing the needed libraries to create a logistic regression
    model, as shown in *Figure 3**.36*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.36 – Import libraries ](img/B18003_03_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.36 – Import libraries
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing the libraries required for our experiment, we will focus on
    the main method, which will be called as the code is executed on the compute instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.37 – Main module of the experiment ](img/B18003_03_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.37 – Main module of the experiment
  prefs: []
  type: TYPE_NORMAL
- en: Using MLflow, which is an open source ML framework to manage an end-to-end ML
    life cycle and is fully integrated with AMLS, we will create an experiment named
    `'titanic_local_compute'`, which can be seen in *Figure 3**.37* inside the main
    method. If it does not exist the first time the notebook is run, it will be created
    in the AML workspace. This experiment can be viewed on the `mlflow.start_run()`
    method, which will begin a new run within the experiment. Using the `mlflow.sklearn.autolog()`
    and `mlflow.log_metric()` methods, we can log parameters and ML metrics for the
    current run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will discuss the `model_train` function, as shown in *Figure 3**.38*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.38 – Model _train ](img/B18003_03_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.38 – Model _train
  prefs: []
  type: TYPE_NORMAL
- en: A logistic regression model is created in the `model_train` function, as shown
    in *Figure 3**.38*. The inputs and the response variable are separated and a train-test
    split is applied using the `sklearn` library. The model is fitted and the AUC
    and accuracy are printed as outputs in the notebook, but also leveraging the MLflow
    framework, they are logged to the experiment as output. In addition to logging
    key metrics and parameters as part of an experiment, figures associated with these
    metrics will be automatically logged using MLflow, which we will look at later
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the training has been handled, we also want to include a preprocessor
    pipeline to handle data transformation as part of our pipeline, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.39 – Preprocessor ](img/B18003_03_039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.39 – Preprocessor
  prefs: []
  type: TYPE_NORMAL
- en: We have incorporated a preprocessor pipeline to handle data transformation,
    as shown in *Figure 3**.39*. Creating a pipeline for data transformation ensures
    that when we score unseen data, the same transformations that were applied to
    our training dataset will be applied to the testing data, and to the new data
    the model hasn’t seen before. This enables model deployment to use the same transformations
    in a deployed model when we get to that step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the `model_train` method, after plotting the ROC curve, we use the `confusion_Matrix()`
    method to calculate the confusion matrix for the run of the experiment, which
    will also be logged automatically within AML, as shown later in the section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This experiment was run directly on the AML compute instance as cells are executed
    in the notebook. Print statements and `plt.show` provide output directly in the
    notebook, as shown in *Figure 3**.40*, but the Azure Machine Learning service
    has also captured this information within an experiment run, so when a notebook
    is cleared out, key metrics for model evaluation are not lost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.40 – Notebook output ](img/B18003_03_040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.40 – Notebook output
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left pane, by clicking on the **Jobs** icon, we can see **titanic_local_compute**
    under **Experiment** as a blue hyperlink. By clicking on this experiment, we can
    drill into an overview of the different runs of our experiment, as shown in *Figure
    3**.41*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.41 – Experiment run output ](img/B18003_03_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.41 – Experiment run output
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the display name of a given run in the experiment provides details
    regarding the experiment run, as shown in *Figure 3**.42*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.42 – Run details ](img/B18003_03_042.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.42 – Run details
  prefs: []
  type: TYPE_NORMAL
- en: 'By clicking on the **Metrics** tab, we can see our **AUC** and **Accuracy**
    values, as logged for the run of the experiment in *Figure 3**.43*. This enables
    a data scientist to easily compare models to understand how changes to the model
    impact performance, accelerating model development:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.43 – Run metrics ](img/B18003_03_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.43 – Run metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Images** tab captures the figures associated with model metrics, such
    as the ROC curve, precision-recall curve, and confusion matrix, as shown in *Figure
    3**.44*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.44 – Images tab ](img/B18003_03_044.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.44 – Images tab
  prefs: []
  type: TYPE_NORMAL
- en: This section showed how to train a model using an AML compute instance. The
    objective is to show how we can train a model using a code-first approach with
    common open source libraries to tackle the task of data engineering, feature selection,
    and model development.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use a compute cluster to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Training on a compute cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we showed how to train your model on a compute instance.
    In this section, we will show you how to submit your training job to a compute
    cluster when the training job needs to scale out. AML has made it extremely easy
    to run your training code on various compute targets without the need to change
    the training script. You need to create an AML pipeline that handles the data
    processing, the model training, and registering the trained model, as explained
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to train your model on a compute cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the left side of the workspace user interface, click **Compute**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.45 – Compute icon ](img/B18003_03_045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.45 – Compute icon
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Compute** screen, click on the **Compute clusters** tab and then click
    on **+ New**, as shown in *Figure 3**.46*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.46 – Creating a new compute cluster ](img/B18003_03_046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.46 – Creating a new compute cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few screens allow you to create and configure a compute cluster based
    on your need. Once done, click on **Create**, as shown in *Figure 3**.47*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.47 – Configuring a compute cluster ](img/B18003_03_047.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.47 – Configuring a compute cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few seconds, your new compute cluster will be running, as shown in
    *Figure 3**.48*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.48 – List of compute clusters ](img/B18003_03_048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.48 – List of compute clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to your working directory, as shown in *Figure 3**.49*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.49 – Working directory ](img/B18003_03_049.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.49 – Working directory
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on **train-on-compute-cluster.ipynb** will bring up the notebook, which
    is very similar to the notebook in the previous section. We will not repeat all
    the steps here again and will only go through the steps needed to run the training
    on the remote cluster that you have just created. To learn more about AML pipelines
    and submitting remote jobs, please refer to [https://github.com/Azure/azureml-examples](https://github.com/Azure/azureml-examples).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code snippet shown in *Figure 3**.50* is used to create an object to access
    your AML workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.50 – Working directory ](img/B18003_03_050.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.50 – Working directory
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create an AML environment, which is an encapsulated Python environment
    where different steps of an ML pipeline, such as the training step, happen. *Figure
    3**.51* shows how to create a `conda` environment for your jobs, using a `conda`.`yaml`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.51 – conda environment ](img/B18003_03_051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.51 – conda environment
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding `.yaml` specification file contains Python packages, which are
    used in the pipeline. You can then use this .`yaml` file to create and register
    this custom environment in your AML workspace, as shown in *Figure 3**.52*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.52 – Creating and registering a custom environment in AML ](img/B18003_03_052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.52 – Creating and registering a custom environment in AML
  prefs: []
  type: TYPE_NORMAL
- en: To create an AML component, you need to write a Python script to implement the
    ML task. This script can take input data and input parameters, which are used
    within the code, and output the results (for example, a transformed dataset or
    a trained model) to a persistent place, such as a mounted folder. An output of
    an ML component can be used as an input to the next step in the pipeline via its
    ML component’s input. Once you have the Python script, you can create the component
    either programmatically or using a .`yaml` definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to write a Python script for the data processing step of the
    pipeline, as shown in *Figure 3**.53*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.53 – Python script for the data preparation step of the pipeline
    ](img/B18003_03_053.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.53 – Python script for the data preparation step of the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a script to perform the data processing task, we can create
    an AML component from it using the programmatic definition. To create an ML component,
    we will use the `command` class to specify the inputs and outputs of the Python
    script, the source code directory of the script, the command line to run the script,
    and the Python environment where this job will run, as shown in *Figure 3**.54*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.54 – Creating a data processing component using the command class
    ](img/B18003_03_054.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.54 – Creating a data processing component using the command class
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to write a Python script for the model training step of the pipeline
    and then create an AML component from it. Please refer to our GitHub to use the
    Python code for the model training. To create an AML component from it, we will
    be using the .`yaml` definition this time, as shown in *Figure 3**.55*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.55 – Creating a model training component using the yaml definition
    ](img/B18003_03_055.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.55 – Creating a model training component using the yaml definition
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the training component from the .`yaml` file and then register
    it to the workspace using the code snippet shown in *Figure 3**.56*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.56 – Loading and registering the model training component ](img/B18003_03_056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.56 – Loading and registering the model training component
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that both the data processing and model training components have been created,
    we can start coding the pipeline definition, which will be called in a later step.
    We will use the `@dsl.pipeline` decorator to tell the SDK that we are defining
    an AML pipeline, as shown in *Figure 3**.57*. As you can see, the compute target
    is specified here, which is our compute cluster that we created earlier in this
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.57 – Defining an AML pipeline using the @dsl.pipeline decorator
    ](img/B18003_03_057.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.57 – Defining an AML pipeline using the @dsl.pipeline decorator
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined our pipeline, let’s instantiate it by passing our
    dataset, the split rate, and the name for the trained model, as shown in *Figure
    3**.58*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.58 – Instantiating the pipeline by passing data and parameters ](img/B18003_03_058.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.58 – Instantiating the pipeline by passing data and parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have instantiated the pipeline object, we can submit the job to
    run in AML, which uses the compute cluster to run the steps within the pipeline,
    as shown in *Figure 3**.59*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.59 – Submitting a pipeline job to run in AML ](img/B18003_03_059.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.59 – Submitting a pipeline job to run in AML
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the pipeline progress and details about each step of the pipeline, you
    can either click on the link shown in *Figure 3**.61* or click on the **Jobs**
    tab, which will show you a list of experiments, including **titanic_remote_cluster**,
    as shown in *Figure 3**.60*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.60 – Submitting a pipeline job to run in AML ](img/B18003_03_060.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.60 – Submitting a pipeline job to run in AML
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and click on the **titanic_remote_cluster** experiment to see a list
    of all the runs associated with this pipeline job. You should only see one run
    for this job, which we just ran in the notebook. Clicking on that will take you
    to a screen with a graphical representation of the pipeline, as shown in *Figure
    3**.61*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.61 – Graphical view of an AML pipeline ](img/B18003_03_061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.61 – Graphical view of an AML pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and click on different steps of the pipeline to see detailed information
    regarding each step. For instance, by clicking on the **Train Titanic Survival
    Model** step, you can see model metrics such as accuracy, precision, and recall,
    as shown in *Figure 3**.62*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.62 – Detailed view of pipeline steps ](img/B18003_03_062.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.62 – Detailed view of pipeline steps
  prefs: []
  type: TYPE_NORMAL
- en: You have successfully trained a model and reviewed the metrics within your AML
    workspace using an AML compute cluster. Now, let’s review the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot of topics in this chapter. We have shown you how to train
    an ML model using the AML Designer, which requires no coding. It is a great fit
    for citizen data scientists or advanced data scientists who would like to explore
    different algorithms quickly and evaluate their performance without having to
    write a lot of code to do so. Next, you learned how to train an ML model through
    the AML Python SDK, running on both a compute instance and a compute cluster for
    more compute-intensive ML jobs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to tune hyperparameters for your ML
    models using AML HyperDrive.
  prefs: []
  type: TYPE_NORMAL
