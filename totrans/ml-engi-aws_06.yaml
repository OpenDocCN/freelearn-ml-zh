- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker Training and Debugging Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041), *Deep Learning AMIs*, and
    [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060), *Deep Learning Containers*, we
    performed our initial ML training experiments inside EC2 instances. We took note
    of the cost per hour of running these EC2 instances as there are some cases where
    we would need to use the more expensive instance types (such as the `p2.8xlarge`
    instance at approximately *$7.20 per hour*) to run our ML training jobs and workloads.
    To manage and reduce the overall cost of running ML workloads using these EC2
    instances, we discussed a few cost optimization strategies, including manually
    turning off these instances after the training job has finished.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you might be wondering if it is possible to automate the following
    processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Launching the EC2 instances that will run the ML training jobs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Uploading the model artifacts of the trained ML model to a storage location
    (such as an S3 bucket) after model training*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deleting the EC2 instances once the training job has been completed*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The good news is that this is possible using automated scripts! Once a major
    portion of this process has been automated, we can focus more on preparing the
    scripts used to train our ML model. We can write our own set of automation scripts;
    however, I would recommend that you do *NOT* reinvent the wheel since AWS has
    already automated this process for us in **Amazon SageMaker**!
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker has a lot of capabilities and features that help data scientists and
    ML practitioners perform ML experiments and deployments in the AWS cloud with
    ease. In the previous chapters, we were able to take a quick look at some of these
    capabilities, including **SageMaker Canvas**, **SageMaker Autopilot**, and **SageMaker
    Data Wrangler**. In this chapter, we will dive deeper into its capabilities and
    features that focus on training ML models inside the managed infrastructure resources
    in AWS. You would be surprised that it only takes a few additional configuration
    parameters to enable certain training techniques and solutions such as **Network
    Isolation**, **Distributed Training**, **Managed Spot Training**, **Checkpointing**,
    and **Incremental Training**. If this is your first time encountering these concepts
    and techniques, do not worry as we will discuss these in more detail in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the SageMaker Python SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an image classification model with the SageMaker Python SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Debugger Insights Dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing Managed Spot Training and checkpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we proceed with the hands-on solutions in this chapter, we’ll start by
    having a quick discussion on how we will use the **SageMaker Python SDK** to help
    us utilize and work with the different capabilities and features of the SageMaker
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we must have the following ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (preferably Chrome or Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account that was used in the first few chapters of this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Jupyter notebooks, source code, and other files used for each chapter are
    available in this book’s GitHub repository: https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to use an IAM user with limited permissions instead of the
    root account when running the examples in this book. We will discuss this, along
    with other security best practices, in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*. If you are just starting to
    use AWS, you may proceed with using the root account in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the SageMaker Python SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **SageMaker Python SDK** is a library that allows ML practitioners to train
    and deploy ML models using the different features and capabilities of SageMaker.
    It provides several high-level abstractions such as **Estimators**, **Models**,
    **Predictors**, **Sessions**, **Transformers**, and **Processors**, all of which
    encapsulate and map to specific ML processes and entities. These abstractions
    allow data scientists and ML engineers to manage ML experiments and deployments
    with just a few lines of code. At the same time, infrastructure management is
    handled by SageMaker already, so all we need to do is configure these high-level
    abstractions with the correct set of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is also possible to use the different capabilities and features
    of SageMaker using the **boto3** library. Compared to using the SageMaker Python
    SDK, we would be working with significantly more lines of code with boto3 since
    we would have to take care of the little details when using the low-level clients
    and functions available in this library. It is recommended to use the SageMaker
    Python SDK whenever possible and just use the boto3 library for the more advanced
    scenarios not directly supported by the SageMaker Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about how to use both libraries together
    when handling more advanced use cases, check out [*Chapter 8*](B18638_08.xhtml#_idTextAnchor172),
    *Model Monitoring and Management Solutions*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows that training and deploying an ML model using the
    SageMaker Python SDK involves only a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – SageMaker Python SDK ](img/B18638_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – SageMaker Python SDK
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use the **SageMaker Python SDK** to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by initializing an `Estimator` object and then using its `set_hyperparameters()`
    method to specify the desired combination of hyperparameter values. Here, we can
    specify whether to use a built-in algorithm or a custom one (using scripts and
    custom Docker container images) by providing the corresponding configuration parameter
    values while initializing the `Estimator` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we call the `fit()` method, which runs a training job with the desired
    set of properties, as defined in the `Estimator` object configuration. This training
    job would run inside dedicated instances and once the training job completes,
    these instances would be terminated automatically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use the `deploy()` method to deploy the trained model to a dedicated
    real-time inference endpoint prepared for us automatically by SageMaker. Then,
    we use the `predict()` method to perform sample predictions on the inference endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is just one of the ways to use the **SageMaker Python SDK** when training
    and deploying our ML models in the AWS cloud. If we already have a pre-trained
    model available for use (for example, after downloading a prebuilt ML model from
    a repository of models), we may skip the training step altogether and deploy the
    model right away using the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the preceding block of code assumes that the model artifacts have
    been uploaded into an S3 bucket already and that the `model_data` variable points
    to where these model artifacts or files are stored.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about how to perform deployments directly
    in SageMaker using pre-trained models, check out [*Chapter 7*](B18638_07.xhtml#_idTextAnchor151),
    *SageMaker Deployment Solutions*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to utilize the **Automatic Model Tuning** capability of SageMaker
    and run multiple training jobs using different combinations of hyperparameters
    automatically when looking for the “best model,” then we just need to run a couple
    of lines of code, similar to what we have in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, SageMaker does all the heavy lifting for us and all we need to worry about
    are the configuration parameters needed to run the hyperparameter tuning job.
    It would have taken us a few weeks (or maybe even a few months!) if we were to
    build this ourselves using custom automation scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about how to utilize the Automatic Model
    Tuning capability of SageMaker using the **SageMaker Python SDK**, then check
    out [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and Debugging
    Solutions,* of the book *Machine Learning with Amazon SageMaker Cookbook*.
  prefs: []
  type: TYPE_NORMAL
- en: There are several options and features available when training models using
    Amazon SageMaker. These include network isolation, distributed training, managed
    spot training, checkpointing, incremental training, and more. Similar to the automatic
    model tuning capability discussed earlier, utilizing and enabling these would
    simply involve just a few additional lines of code. If you are wondering what
    these are, do not worry –we will discuss each of these in detail as we work on
    the hands-on solutions in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of how the **SageMaker Python SDK**
    helps us train and deploy ML models in the cloud, let’s proceed with creating
    a service limit request!
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will ensure that the following prerequisites are ready
    before proceeding with the hands-on solutions of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a service limit increase to run SageMaker training jobs using the `ml.p2.xlarge`
    instance (SageMaker Training)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a service limit increase to run SageMaker training jobs using the `ml.p2.xlarge`
    instance (SageMaker Managed Spot Training)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are wondering why we are using `ml.p2.xlarge` instances in this chapter,
    that’s because we are required to use one of the supported instance types for
    the **Image Classification Algorithm**, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – EC2 Instance Recommendation for the image classification algorithm
    ](img/B18638_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – EC2 Instance Recommendation for the image classification algorithm
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can use `ml.p2.xlarge`, `ml.p2.8xlarge`, `ml.p2.16xlarge`,
    `ml.p3.2xlarge`, `ml.p3.8xlarge`, and `ml.p3.16xlarge` (at the time of writing)
    when running training jobs using the Image Classification Algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.xhtml)
    for more information about this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service limit increase request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will train an Image Classification model using multiple
    `ml.p2.xlarge` instances. Before we can use this type of instance to train ML
    models, we will need to request for the service quota (or service limit) to be
    increased through the `0`; we would encounter a `ResourceLimitExceeded` error
    if we were to run training jobs using `ml.p2.xlarge` instances.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to make any adjustments needed in case certain resources need to
    be transferred to the region of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create a support case and request for the SageMaker training
    instance count limits to be increased:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the `support` in the search bar of the AWS Management Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting **Support** from the list of results under **Services**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate and click the **Create case** button.*   On the **Create case** page,
    select **Service limit increase** from the list of options.*   Specify the following
    configuration under `SageMaker Training Jobs`*   Under `US West (Oregon)`*   `SageMaker
    Training`*   `ml.p2.xlarge`*   `2`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that increasing the service limit for the SageMaker training resource type
    does not automatically increase the service limit for the SageMaker Managed Spot
    Training resource type.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Add another request**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `US West (Oregon)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SageMaker Managed Spot Training`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ml.p2.xlarge`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`2`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under **Case description**, specify the following use case description in the
    text area provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure that you replace `Oregon (us-west-2)` with the appropriate region
    if you are planning to run your ML experiments in another region.
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down to **Contact options** and select **Web** (or **Chat** if available)
    from the list of options under **Contact methods**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, click the **Submit** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that it may take around 24 to 48 hours for the limit increase request to
    get approved by the **AWS Support team**. While waiting, you may browse through
    the contents and concepts explained in this chapter. This will help you have a
    better idea of the capabilities of SageMaker before you work on the hands-on solutions.
    You may also skip this chapter and proceed with [*Chapter 7*](B18638_07.xhtml#_idTextAnchor151),
    *SageMaker Deployment Solutions*, while waiting for the limit increase to be approved.
  prefs: []
  type: TYPE_NORMAL
- en: Training an image classification model with the SageMaker Python SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the *Getting started with the SageMaker Python SDK* section,
    we can use built-in algorithms or custom algorithms (using scripts and custom
    Docker container images) when performing training experiments in SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data scientists and ML practitioners can get started with training and deploying
    models in SageMaker quickly using one or more of the built-in algorithms prepared
    by the AWS team. There are a variety of built-in algorithms to choose from and
    each of these algorithms has been provided to help ML practitioners solve specific
    business and ML problems. Here are some of the built-in algorithms available,
    along with some of the use cases and problems these can solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DeepAR Forecasting**: Time-series forecasting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis**: Dimensionality reduction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP Insights**: IP anomaly detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation (LDA)**: Topic modeling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence-to-Sequence**: Text summarization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic Segmentation**: Computer vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option involves using SageMaker **script mode**, where we import
    a custom training script, which makes use of a deep learning framework (such as
    **TensorFlow**, **PyTorch**, or **MXNet**) to train a model. Here, the custom
    training script will run inside one of the pre-built containers, which includes
    **AWS Deep Learning Containers**, as discussed in [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060),
    *Deep Learning Containers*. That said, all we need to worry about when choosing
    this option is preparing the training script since most of the dependencies are
    already installed inside the container environment where these scripts will run.
  prefs: []
  type: TYPE_NORMAL
- en: The third option involves building and using a custom container image for training
    ML models in SageMaker. This option gives us the highest level of flexibility
    as we have full control over the environment where our custom training scripts
    will run.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Which option is best for us?* If we want to proceed with training an ML model
    without having to prepare a custom script along with a custom container image,
    the best option would be to use SageMaker’s built-in algorithms. If we are trying
    to port our custom script to SageMaker, which makes use of open source ML libraries
    and frameworks (such as scikit-learn, PyTorch, and TensorFlow) to train a model,
    then the best option would be to use SageMaker’s script mode. If we need a bit
    more flexibility, then we may choose the option where we use our own custom container
    image instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better idea of what options are available when training ML
    models in SageMaker, let’s proceed with discussing what we will do in the hands-on
    portion of this section. In this section, we will use the built-in `ml.p2.xlarge`
    instances. To test the model we trained, we will deploy the model and launch an
    inference endpoint inside an `ml.m5.xlarge` instance. This inference endpoint
    is then used to perform sample predictions using several test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, we can utilize **distributed training**
    when performing the training steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Training and deploying an image classification model ](img/B18638_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Training and deploying an image classification model
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training can help reduce the training time through the use of multiple
    instances instead of one. Since we are using a built-in algorithm, all we need
    to do is configure the training job to use two or more instances to enable distributed
    training.
  prefs: []
  type: TYPE_NORMAL
- en: With these aspects in mind, let’s proceed with creating a new notebook in **SageMaker
    Studio**. We will use this to run the blocks of code to train our Image Classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new Notebook in SageMaker Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Start by opening SageMaker Studio and creating a new directory named `CH06`.
    Then, create a new **Jupyter notebook** and save it inside this directory.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you have completed the hands-on solutions in the *Getting started
    with SageMaker and SageMaker Studio* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to launch SageMaker Studio and create the new notebook that
    will be used to run the Python scripts in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to SageMaker Studio by doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Typing `sagemaker studio` in the search bar of the AWS Management Console.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting **SageMaker Studio** from the list of results under **Features**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to make any adjustments needed if certain resources need to be transferred
    to the region of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, click **Studio** under **SageMaker Domain** in the sidebar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Launch app**, as highlighted in the following screenshot. Select **Studio**
    from the list of dropdown options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Opening SageMaker Studio ](img/B18638_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Opening SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: This will redirect you to SageMaker Studio. Wait a few seconds for the interface
    to load.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the empty space in the **File Browser** sidebar pane to open
    a context menu similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Creating a new folder ](img/B18638_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Creating a new folder
  prefs: []
  type: TYPE_NORMAL
- en: Select `CH06`.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the `CH06` directory by double-clicking the corresponding folder
    name in the sidebar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new notebook by clicking the **File** menu and choosing **notebook**
    from the list of options under the **New** submenu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Creating a new Notebook ](img/B18638_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Creating a new Notebook
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see other options as well, including creating a new **Console**,
    **Data Wrangler Flow**, **Terminal**, **Text File**, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Data Science` (option found under the Sagemaker image)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Python 3`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`No script`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Select** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the kernel to start. This step may take around 3 to 5 minutes while
    an ML instance is being provisioned to run the Jupyter notebook cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the tab’s name, as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Renaming a notebook ](img/B18638_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Renaming a notebook
  prefs: []
  type: TYPE_NORMAL
- en: Select **Rename Notebook…** from the list of options in the context menu.
  prefs: []
  type: TYPE_NORMAL
- en: In the `PART01.ipynb` under **New Name**. Then, click the **Rename** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following in the first cell of the Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Click the **Run the selected cell and advance** button, as highlighted in the
    following screenshot. Alternatively, you can hold **SHIFT** and press **ENTER**
    to run the selected cell and create a new cell automatically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Running a selected cell ](img/B18638_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Running a selected cell
  prefs: []
  type: TYPE_NORMAL
- en: This should yield an output of `Hello`, which should show up under the cell.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If no output is displayed, this means that either no kernel is running, or the
    kernel is still starting. Once the kernel is ready, you can run the cells again.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our notebook is ready, we will create a new cell for each block of
    code in the succeeding sections.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the training, validation, and test datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, you might be wondering what dataset we will use to train our
    ML model. To answer your question, we will use the **MNIST dataset**, which is
    a large collection of images of handwritten digits. An example of this can be
    seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – MNIST dataset ](img/B18638_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that each image in the MNIST dataset has a class that corresponds
    to a number between `0` and `9`. That said, there are a total of 10 classes and
    each image in this dataset falls under exactly one class.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST dataset contains thousands of images of handwritten numbers. The usual
    challenge involves correctly identifying which number from 0 to 9 maps to the
    handwritten number displayed (in the image). It might be trivial for us humans
    to classify these handwritten numbers correctly. However, it’s not straightforward
    for machines since they would have to process the pixel data of the images and
    establish patterns of how the numbers are represented in the images. To get machines
    to properly classify these images, we’ll use deep learning (using the image classification
    algorithm of SageMaker)!
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our lives easier, we have already prepared the training, validation,
    and test sets and stored these inside a ZIP file. Follow these steps to download
    this ZIP file and extract the files inside a specified directory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following statement to ensure that you have an empty `tmp` directory
    ready:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use an exclamation point (`!`) before the command so that we can run
    Terminal commands inside the Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `batch1.zip` file using the `wget` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, run the following block of code to extract the contents of the `batch1.zip`
    file inside the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a set of logs showing the files extracted from the ZIP file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Enabling scrolling for the output logs ](img/B18638_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Enabling scrolling for the output logs
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the empty space near the generated log messages. This should
    open a context menu similar to what’s shown in the preceding screenshot. Select
    **Enable Scrolling for Outputs** from the list of options available in the context
    popup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `ls` command to check the extracted files in the current directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set two flags when using the `ls` command. The first one is the `-R` flag,
    which lists the directory tree recursively. The second flag is the `-F` flag,
    which adds a specific character, depending on the type of file: `/` for directories,
    `*` for executables, `@` for symbolic links, and `|` for FIFO special files.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the `ls` command should give us a set of logs similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Listing the extracted files and folders ](img/B18638_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Listing the extracted files and folders
  prefs: []
  type: TYPE_NORMAL
- en: 'You should find five directories inside the `tmp` directory – `test`, `train`,
    `train_lst`, `validation`, and `validation_lst`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Files and directories extracted from the batch1.zip file ](img/B18638_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Files and directories extracted from the batch1.zip file
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, we should find 10 directories inside the
    `train` directory. Each of these directories contains several *PNG* files with
    a label corresponding to the name of the directory where these files are stored.
    For example, the PNG files stored inside the `0` directory have a label of `0`.
    Inside the `train_lst` directory is the `train.lst` file, which contains a mapping
    of the labels and the images from the `train` directory (given the specified paths
    and filenames). We should find a similar set of directories and files inside `validation`
    and `validation_lst`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s install `IPyPlot`, which we will use to inspect the images we have
    extracted from the ZIP file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With `IPyPlot` installed, let’s have a quick look at what our labeled set of
    images looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should plot a series of images, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Using IPyPlot to display a selected number of images ](img/B18638_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Using IPyPlot to display a selected number of images
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the differences and variations in the images of the same group.
    For one thing, the zeros do not look alike!
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to tweak and change the parameter value for `max_images` when calling
    the `plot_images()` function before proceeding to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the training, validation, and test datasets ready, let’s proceed
    with uploading these to an **Amazon S3** bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the data to S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note that we will be working with two different S3 buckets in this chapter,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – S3 buckets ](img/B18638_06_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – S3 buckets
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the first S3 bucket will contain the input and output files for
    the training job in this section. Similarly, the second S3 bucket will contain
    the input and output files for the training job we’ll run later in the *Utilizing
    Managed Spot Training and Checkpoints* section toward the end of this chapter.
    In addition to this, we will use a technique called incremental training, where
    we will use the model generated in this section as a starting point to train a
    more accurate model. For now, let’s focus on the first S3 bucket and upload the
    data that will be used to train our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create an S3 bucket and then upload all the files and
    folders from the `tmp` directory to the new S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify a unique S3 bucket name and prefix. Make sure that you replace the
    value of `<INSERT S3 BUCKET NAME HERE>` with a unique S3 bucket name before running
    the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is recommended not to use any of the S3 buckets created in the previous chapters.
    So, the S3 bucket name here should be for a bucket that doesn’t exist yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the `glob()` function to prepare a list containing all the images
    inside the `tmp/train` directory. Then, use the `len()` function to count the
    number of items in the list generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us a value of `4000`, which is the total number of `.png` files
    inside the `tmp/train` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `aws s3 mb` command to create a new Amazon S3 bucket. Here, `{s3_bucket}`
    automatically gets replaced with the value of `s3_bucket` from the previous code
    cells written in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see a success log message similar to `make_bucket: <S3 bucket name>`
    if the S3 bucket creation step is successful. Note that this step may fail if
    the bucket already exists before using this command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, use the AWS CLI to upload the contents of the `tmp` directory to the
    target S3 path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first parameter of the `aws s3 cp` command is the source (`tmp/.`), while
    the second parameter is the target destination (S3 path). Here, we use the `--recursive`
    flag to copy all the files from the source to the destination recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Copying the files and directories from the tmp directory to
    the S3 bucket ](img/B18638_06_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Copying the files and directories from the tmp directory to the
    S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, the `aws s3 cp` command will copy all the
    contents from the `tmp` directory of the SageMaker Studio notebook to the new
    S3 bucket. This includes all the files and directories inside the `train`, `train_lst`,
    `validation`, `validation_lst`, and `test` directories.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step should take about 1 to 2 minutes to complete. Feel free to grab a
    cup of coffee or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: Once the upload operation has been completed, we can start training an ML model!
  prefs: []
  type: TYPE_NORMAL
- en: Using the SageMaker Python SDK to train an ML model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we uploaded the training and validation datasets to
    an Amazon S3 bucket. These datasets will be used as input when running the training
    job in this section. Of course, there are a few more input parameters we need
    to prepare before we can configure and run a SageMaker training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Requirements when initializing an Estimator object ](img/B18638_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Requirements when initializing an Estimator object
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, we need to have a few configuration parameter
    values, along with the hyperparameter configuration values, ready when initializing
    and configuring an `Estimator` object. When the `Estimator` object’s `fit()` method
    is called, SageMaker uses the parameter values used to configure the `Estimator`
    object when running the training job. For example, the instance type used to train
    the ML model depends on the parameter value for `instance_type` when initializing
    the estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to use the **SageMaker Python SDK** to train an image classification
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the SageMaker Python SDK and the **Boto AWS Python SDK**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize a few prerequisites, such as `session`, `role`, and `region_name`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `retrieve()` function to prepare the image URI for the image classification
    algorithm. Note that the `retrieve()` function returns the Amazon ECR URI of the
    built-in algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us a value similar to `'433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:1'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `map_path()` and `map_input()` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the `data_channels` dictionary by running the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These data channels correspond to each of the directories we have uploaded to
    the Amazon S3 bucket (except for the `test` directory).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the S3 URL for the output path using the `map_path()` function we
    defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us an S3 path similar to `'s3://<S3 BUCKET NAME>/ch06/output'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we initialize the `Estimator` object, let’s quickly review what we have
    so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Data channels and the output path ](img/B18638_06_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Data channels and the output path
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the data channels we have prepared in the previous steps
    will be used as input later when we run the training job. Once the training job
    has been completed, the output file(s) will be stored in the S3 location specified
    in `output_path`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything ready, let’s initialize the `Estimator` object. When initializing
    an `Estimator` object, we pass several arguments, such as the container image
    URI, the IAM role ARN, and the SageMaker `session` object. We also specify the
    number and type of ML instances used when performing the training job, along with
    the parameter values for `output_path` and `enable_network_isolation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that initializing the `Estimator` object does not run the training job
    yet. When we run the training job using the `fit()` method in a later step, SageMaker
    will launch and provision two `ml.p2.xlarge` instances to run the image classification
    algorithm to train a model. Then, the results get uploaded to the S3 location
    in `output_path`. Since we set `enable_network_isolation` to `True`, we have configured
    the containers inside the SageMaker ML instances so that they don’t have external
    network access while the training jobs are running. This helps secure the setup
    since this configuration prevents the running container from downloading malicious
    code or accessing external services.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We should be fine since we are using a container image prepared by AWS. If we
    were to use a custom container image instead, we can set `enable_network_isolation`
    to `True`, especially if we are not expecting the container to access external
    services or download resources. This will help safeguard our ML environments and
    resources against attacks requiring network connectivity. For more information
    about this topic, check out [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security,
    Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the hyperparameter configuration values with the following block
    of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The configurable hyperparameter values depend on the algorithm used. These are
    just some of the hyperparameters we can configure with the image classification
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `set_hyperparameters()` method to configure the `Estimator` object
    with the hyperparameters prepared in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can see that we used `**` to pass multiple arguments to a function
    or method directly using a dictionary. Note that this is equivalent to calling
    the `set_hyperparameters()` method, similar to what we have in the following block
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, we may inspect the properties of the `Estimator` object using the
    `__dict__` attribute. Feel free to run `estimator.__dict__` in a separate cell
    before proceeding with the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `fit()` method to start the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the training job has finished, we should see a set of logs similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Logs generated after the training job has been completed ](img/B18638_06_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Logs generated after the training job has been completed
  prefs: []
  type: TYPE_NORMAL
- en: 'Several operations and steps are performed behind the scenes when the `fit()`
    method is called. After SageMaker provisions the desired number of ML instances,
    the input data and the training container image are downloaded to each of the
    instances. A container is run from the downloaded container image, and an ML model
    is trained using the input data. The resulting model files are stored inside a
    `model.tar.gz` file. This `model.tar.gz` file is then uploaded to the configured
    output S3 location. Finally, SageMaker terminates the instances after the training
    job has finished:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.19 – What happens after calling the fit() method ](img/B18638_06_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – What happens after calling the fit() method
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, each of the relevant steps performed inside
    the ML instance generates logs that automatically get stored in **CloudWatch Logs**.
    This includes the metric values, along with different types of log messages that
    were generated while the training job was running.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take around 5 to 10 minutes to complete. If you encounter a **ResourceLimitExceeded**
    error, it means that you have exceeded the quota when using a certain type of
    ML instance when running a training job. Make sure that you have completed the
    steps specified in the *Preparing the essential prerequisites* section of this
    chapter. For more information on this topic, check out [https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/](https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/).
  prefs: []
  type: TYPE_NORMAL
- en: There’s a lot of information we can get from the logs stored in CloudWatch Logs.
    If you encounter an error when running a training job, you can check the logs
    stored in CloudWatch Logs (for example, `/aws/sagemaker/TrainingJob`) to troubleshoot
    the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Using the %store magic to store data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we deploy and test our model, let’s quickly store a backup copy of the
    values of some variables used in our first notebook (for example, `PART01.ipynb`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – %store magic ](img/B18638_06_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – %store magic
  prefs: []
  type: TYPE_NORMAL
- en: We will do this using the `%store` magic from IPython and make these variable
    values available in other notebooks as well. We will load these variable values
    later in the *Utilizing Managed Spot Training and Checkpoints* section, where
    we will create a new notebook named `PART02.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to use the `%store` magic to save a copy of some of the
    variable values used in `PART01.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the value of `model_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should return the S3 path where the training job output file (`model.tar.gz`)
    is stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the value of `estimator.model_data` to a new variable named `model_data`.
    Similarly, copy the value of the name of the latest training job to a variable
    named `job_name`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `%store` magic to store data in memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the `%store` magic helps us divide a long Jupyter notebook into
    several smaller notebooks. Later, in the *Utilizing Managed Spot Training and
    Checkpoints* section, we will use `%store -r <variable name>` to load the variable
    values stored in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the SageMaker Python SDK to deploy an ML model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s time we deploy the model to an inference endpoint. Deploying an ML model
    using the SageMaker Python SDK is straightforward. All we need to do is call the
    `deploy()` method; an inference endpoint will automatically be provisioned and
    configured for us in just a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to deploy our ML model using the SageMaker Python SDK and
    then perform some test predictions afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `deploy()` method to deploy the trained Image Classification model
    to a real-time inference endpoint. Model deployment should take around 5 to 10
    minutes to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we specify that we are using an `ml.m5.xlarge` instance to host the trained
    ML model. At this point, you might be wondering why several different instance
    types are involved when training or deploying a model. The first thing you need
    to know is that the SageMaker Studio notebook instance where the Jupyter notebook
    scripts are running is different and completely separate from the instances that
    are used when training or deploying a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Different instances used to train and deploy a model ](img/B18638_06_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Different instances used to train and deploy a model
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the instance(s) used to train the model are different
    from the instances used during deployment as well. In most cases, the ML instances
    used to train a model are more powerful (and more expensive per hour) compared
    to the instances used when deploying the trained model. In our case, we used two
    `ml.p2.xlarge` instances (*GPU-powered | 4 vCPU | 61 GiB | $1.125 per hour per
    instance*) during training and a single `ml.m5.xlarge` instance (*4 vCPU | 16
    GiB | $0.23 per hour per instance*) to host our real-time inference endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Looking at these numbers alone, we may incorrectly assume that the overall cost
    of running the `ml.p2.xlarge` training instances is higher than the overall cost
    of running the `ml.m5.xlarge` instance used to host the deployed model. In reality,
    the overall cost of running the `ml.m5.xlarge` inference instance will exceed
    the overall cost of running the `ml.p2.xlarge` training instances if we do not
    delete the inference instances right away. ML instances used during training are
    automatically terminated after the training job has been completed. Since we only
    pay for what we use, we would pay approximately `$1.125 x 2 x 0.1 = $0.225` if
    we were to run two `ml.p2.xlarge` training instances for 6 minutes each. On the
    other hand, an `ml.m5.xlarge` inference instance would cost around `$0.23 x 24
    = $5.52` if we kept it running for 24 hours. To manage costs, make sure to delete
    instances used for real-time inference during periods of inactivity. If the expected
    traffic to be received by the inference endpoint is unpredictable or intermittent,
    you may want to check the **SageMaker Serverless Inference** option. For more
    information, check out [https://aws.amazon.com/about-aws/whats-new/2021/12/amazon-sagemaker-serverless-inference/](https://aws.amazon.com/about-aws/whats-new/2021/12/amazon-sagemaker-serverless-inference/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we use the inference endpoint to perform test predictions, let’s quickly
    update the `serializer` property of the endpoint to accept the specified content
    type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s define the `get_class_from_results()` function, which accepts the raw
    output data from the SageMaker real-time inference endpoint and returns the corresponding
    class as a string (for example, `"ONE",` `"TWO",` `"THREE"`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s define a custom `predict()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This custom `predict()` function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Opens the test image, given a filename.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displays the test image in the Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses the `predict()` method of the endpoint object to get the predicted class
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prints the predicted class value right after the rendered image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Performing test predictions ](img/B18638_06_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Performing test predictions
  prefs: []
  type: TYPE_NORMAL
- en: Note that there’s an extra processing step after the `endpoint.predict()` method
    is called. As shown in the preceding diagram, the custom `predict()` function
    uses the `get_class_from_results()` function to convert the raw output data from
    the inference endpoint into a human-friendly string representation of the predicted
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the custom `predict()` function we defined in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a set of results similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Performing test predictions ](img/B18638_06_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Performing test predictions
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see three sample images, along with their corresponding predicted
    class values. Our ML model seems to be doing just fine!
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s delete the inference endpoint using the `delete_endpoint()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Wasn’t that easy?* The deployment we performed in this section is just one
    of the many possible scenarios when performing model deployments on AWS. We will
    look at other deployment strategies and techniques in [*Chapter 7*](B18638_07.xhtml#_idTextAnchor151),
    *SageMaker Deployment Solutions*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll take a closer look at how we can use the **Debugger
    Insights Dashboard** to check the utilization of the resources that were used
    to train our Image Classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Debugger Insights Dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working on ML requirements, ML practitioners may encounter a variety of
    issues before coming up with a high-performing ML model. Like software development
    and programming, building ML models requires a bit of trial and error. Developers
    generally make use of a variety of debugging tools to help them troubleshoot issues
    and implementation errors when writing software applications. Similarly, ML practitioners
    need a way to monitor and debug training jobs when building ML models. Luckily
    for us, Amazon SageMaker has a capability called **SageMaker Debugger** that allows
    us to troubleshoot different issues and bottlenecks when training ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – SageMaker Debugger features ](img/B18638_06_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – SageMaker Debugger features
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows the features that are available when we use SageMaker
    Debugger to monitor, debug, and troubleshoot a variety of issues that affect an
    ML model’s performance. This includes the **data capture** capability across a
    variety of ML frameworks, **Debugger Interactive Reports**, the **SMDebug client
    library**, automated error detection with **Debugger built-in rules** and **custom
    rules**, and the **Debugger Insights Dashboard**.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on using the **Debugger Insights Dashboard**
    to review and monitor the hardware system resource utilization rate of the instances
    used to train our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that an `ml.m5.4xlarge` instance is provisioned whenever we use the Debugger
    Insights Dashboard. This `ml.m5.4xlarge` instance needs to be turned off manually
    since it is not automatically turned off during periods of inactivity. We will
    make sure to turn off this instance toward the end of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, let’s use the Debugger Insights Dashboard to monitor the hardware
    system resource utilization rate of the instances we used in the previous sections:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to **SageMaker resources** by clicking the left sidebar icon shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Navigating to SageMaker resources ](img/B18638_06_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Navigating to SageMaker resources
  prefs: []
  type: TYPE_NORMAL
- en: Select **Experiments and trials** from the list of options available in the
    first dropdown. Double-click on **Unassigned trial components**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right-click on the first result in the list. It should have a name that starts
    with `image-classification`, followed by a timestamp. This should open a context
    menu, similar to the following. Select **Open Debugger for insights** from the
    list of options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.26 – Open Debugger for insights ](img/B18638_06_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – Open Debugger for insights
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see another option called **Open in trial details**. If you selected
    this option instead, you will see several charts, which help you analyze the metrics
    and results of the training job.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to turn off the `ml.m5.4xlarge` instance after using the Debugger
    Insights Dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Overview** tab, scroll down and locate the **Resource utilization
    summary** report, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Resource utilization summary ](img/B18638_06_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Resource utilization summary
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the hardware system resource utilization statistics such as
    the total CPU and GPU utilization, total CPU and GPU memory utilization, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **Nodes** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down and locate the different reports and charts, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.28 – Debugger insights – nodes ](img/B18638_06_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – Debugger insights – nodes
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see graphs that help us review and analyze the different utilization
    metrics over time. This includes reports such as **CPU utilization over time**,
    **Network utilization over time**, **GPU utilization over time**, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These reports can help ML engineers determine whether the resources used to
    train the model are “right-sized.” This can help optimize costs and identify performance
    bottlenecks during the training steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Running Instances and Kernels** icon in the sidebar, as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.29 – Turning off the running instances ](img/B18638_06_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – Turning off the running instances
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Running Instances and Kernels** icon should open and show the
    running instances, apps, and terminals in SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Turn off the `ml.m5.4xlarge` running instance under **RUNNING INSTANCES** by
    clicking the **Shutdown** button, as highlighted in the preceding screenshot.
    Clicking the **Shutdown** button will open a pop-up window verifying the instance
    shutdown operation. Click the **Shut down all** button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we should have a better overall understanding of how to train
    and deploy ML models in Amazon SageMaker. Note that we’re just scratching the
    surface as there are a lot more features and capabilities available for us to
    use to manage, analyze, and troubleshoot ML experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about the other features of **SageMaker
    Debugger**, then feel free to check out [*Chapter 5*](B18638_05.xhtml#_idTextAnchor105),
    *Pragmatic Data Processing and Analysis,* of the book *Machine Learning with Amazon
    SageMaker Cookbook*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a few more capabilities and features available
    in SageMaker when training ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Managed Spot Training and Checkpoints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding of how to use the SageMaker Python
    SDK to train and deploy ML models, let’s proceed with using a few additional options
    that allow us to reduce costs significantly when running training jobs. In this
    section, we will utilize the following SageMaker features and capabilities when
    training a second Image Classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: Managed Spot Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpointing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incremental Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041), *Deep Learning AMIs*, we
    mentioned that spot instances can be used to reduce the cost of running training
    jobs. Using spot instances instead of on-demand instances can help reduce the
    overall cost by up to 70% to 90%. So, why are spot instances cheaper? The downside
    of using spot instances is that these instances can be interrupted, which will
    restart the training job from the start. If we were to train our models outside
    of SageMaker, we would have to prepare our own set of custom automation scripts
    that will utilize and manage spot instances to train our model. Again, there’s
    no need for us to prepare a custom solution as SageMaker already supports the
    ability to automatically manage spot instances for us through its **Managed Spot
    Training** capability! In addition to this, if we configure our SageMaker training
    jobs to use **checkpoints**, we will be able to resume training from the last
    saved checkpoint even if there has been an interruption while we are using spot
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will also use a technique called **Incremental Training**,
    where we will use the model that was generated in the *Training an Image Classification
    model with the SageMaker Python SDK* section as a starting point to train a more
    accurate model. Here, we will be using a pre-trained model we have provided instead
    of training a new model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that Incremental Training can only be used when using the **Image Classification
    Algorithm**, **Object Detection Algorithm**, and **Semantic Segmentation Algorithm**
    built-in algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to use the **SageMaker Python SDK** to run a training job
    that utilizes checkpointing, Managed Spot Training, and Incremental Training:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new notebook by clicking the **File** menu and choosing **notebook**
    from the list of options under the **New** submenu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `Data Science` (option found under the Sagemaker image)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Python 3`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`No script`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Select** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rename the notebook `PART02.ipynb`. Now that we have our new Jupyter notebook
    ready, let’s run the blocks of code in the succeeding set of steps inside this
    Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the S3 bucket name and prefix. Make sure that you replace the value
    of `<INSERT S3 BUCKET NAME HERE>` with a unique S3 bucket name before running
    the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that this should be different from the name of the S3 bucket you created
    in the *Training an Image Classification model with the SageMaker Python SDK*
    section. In this chapter, we will work with two different S3 buckets, similar
    to what’s shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30 – Working with two S3 buckets ](img/B18638_06_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – Working with two S3 buckets
  prefs: []
  type: TYPE_NORMAL
- en: The first bucket should contain the model output files stored in a `model.tar.gz`
    file after running the first training job. Later in this section, we will use
    this `model.tar.gz` file as an input parameter for a new training job that utilizes
    Incremental Training when building a new model. The output of this training job
    will be stored in an output folder inside the second S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `%store` magic from IPython to load the values of the stored variables
    from the *Training an Image Classification model with the SageMaker Python SDK*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the value of the loaded `job_name` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should return a value similar to `'image-classification-2022-04-11-16-22-24-589'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize and import some of the training prerequisites:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, load an `Estimator` object using the name of the previous training job
    using `Estimator.attach()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `logs()` method to inspect the logs of the training job we loaded in
    the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that this should yield a set of logs similar to the logs that were generated
    when we ran the training job in the *Training an Image Classification model with
    the SageMaker Python SDK* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the location where the ML model ZIP file of the previous training job is
    stored. Store this value inside the `model_data` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `model_data` variable should have a value with a format similar to `'s3://<S3
    BUCKET NAME>/ch06/output/image-classification-<DATETIME>/output/model.tar.gz'`.
    We will use this value later when initializing and configuring a new `Estimator`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `generate_random_string()` function, which will be used to generate
    a unique base job name for the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a unique base job name using `generate_random_string()` and store
    it in the `base_job_name` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should get a 12-character string similar to `'FTMHLGKYVOAC'` after using
    the `generate_random_string()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Where will we use this? In a later step, we will specify a base job name of
    our choice when initializing a new `Estimator` object. If a base job name is not
    specified when initializing an `Estimator` object, SageMaker generally uses the
    algorithm image name (for example, `image-classification`) as the default base
    job name when running a training job. The base job name is then appended with
    a string representation of the current timestamp to produce the complete training
    job name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the different configuration parameters when enabling checkpointing
    support:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following block of code to ensure that an empty `tmp2` directory exists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download `batch2.zip` using the `wget` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, run the following block of code to extract the contents of the `batch1.zip`
    file inside the `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the `glob()` function to get a list containing all the images inside
    the `tmp2/train` directory. After that, we will use the `len()` function to count
    the number of items in the list that was generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us a value of `7200`, which is the total number of `.png` files
    inside the `tmp2/train` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new S3 bucket using the `aws s3 mb` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `aws s3 cp` command to copy the contents of the `tmp2` directory to
    the S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `map_path()` and `map_input()` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the `data_channels` dictionary by running the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the S3 output path using the `map_path()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `Estimator` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This initialization step should be similar to what we did in the *Training an
    Image Classification model with the SageMaker Python SDK* section. In addition
    to the original set of parameter values we set when initializing the `Estimator`
    object, we have also set a few additional arguments, including `model_uri`, `use_spot_instances`,
    `max_run`, `max_wait`, `checkpoint_s3_uri`, and `checkpoint_local_path`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.31 – Initializing the Estimator object with Checkpointing and Managed
    Spot Training ](img/B18638_06_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – Initializing the Estimator object with Checkpointing and Managed
    Spot Training
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, enabling checkpointing and Managed Spot Training
    is straightforward and easy. These are disabled by default when running SageMaker
    training jobs, so all we need to do is update the parameter values for `use_spot_instances`,
    `max_run`, `max_wait`, `checkpoint_s3_uri`, and `checkpoint_local_path` with the
    appropriate values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When using built-in algorithms, the `Estimator` classes such as `RandomCutForest`,
    `FactorizationMachines`, and `PCA` that can be used instead of the “generic” `Estimator`
    class. Using these has its own set of benefits and a lot of the configuration
    parameters already have good default starting values during initialization (which
    makes the code much shorter as well). In this chapter, we will use the “generic”
    `Estimator` class when performing the training experiments, but if you’re interested
    in learning more about the other classes available in the SageMaker Python SDK,
    then feel free to check out [https://sagemaker.readthedocs.io/en/stable/algorithms/index.xhtml](https://sagemaker.readthedocs.io/en/stable/algorithms/index.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the hyperparameter configuration and store it inside the `hyperparameters`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should be similar to the hyperparameter configuration we did in the *Training
    an Image Classification model with the SageMaker Python SDK* section, except for
    the value of `num_training_samples`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Nothing is stopping us from changing the values of some of the configuration
    parameters here, such as `mini_batch_size`, `epochs`, and `learning_rate`. Once
    you are comfortable testing different combinations of hyperparameter values, you
    may also try configuring and using other hyperparameters, such as `optimizer`,
    `num_layers`, and `momentum`. For more details on this topic, check out [https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `set_hyperparameters()` method to specify the hyperparameter configuration
    values for the training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Incremental Training job using the `fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a set of logs similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.32 – A portion of the logs after running the training job ](img/B18638_06_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – A portion of the logs after running the training job
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the savings we get when using Managed Spot Training – approximately
    `70%` savings! Note that all we did was make some additional tweaks in the configuration
    of the `Estimator` object. By doing so, we were able to significantly reduce the
    cost of running the training job.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter an `fit()` method, you may stop the current training job and
    try again in an hour or so. Alternatively, you may run the experiment in another
    region. If you encounter a **ResourceLimitExceeded** error, this means that you
    have exceeded the quota when using a certain type of ML spot training instance
    when running a training job. Make sure that you have completed the steps specified
    in the *Preparing the essential prerequisites* section of this chapter. For more
    information on this topic, check out [https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/](https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspect the output location of the trained model using the `model_data` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We should get a value similar to `'s3://<S3 BUCKET NAME>/ch06/output/<BASE JOB
    NAME>-<DATE AND TIME>/output/model.tar.gz'.`
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If we decide to deploy the model outside of SageMaker (for example, in `estimator.model_data`
    points to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the generated checkpoint files using the `aws s3 ls` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a set of results similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Generated checkpoint files ](img/B18638_06_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – Generated checkpoint files
  prefs: []
  type: TYPE_NORMAL
- en: These saved checkpoint files can be used to restart and continue a training
    job from the last saved checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use the last saved checkpoint and continue a previous training
    job, you simply need to specify the same `checkpoint_s3_uri` when initializing
    the `Estimator` object. This will automatically download the checkpoint files
    from S3 to the training instance(s) and continue the training job from there.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing works well with the **Managed Spot Training** capability of SageMaker
    since we can easily resume model training, even if there’s an unexpected interruption
    to the training instance or training job. In addition to this, we can use checkpoints
    to analyze our model at different stages of the training step (since we have multiple
    *snapshots* of the model at various intermediate stages).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss a few other strategies we can use when training and tuning ML
    models in SageMaker. The first one, **early stopping**, involves configuring a
    hyperparameter tuning job to stop training jobs earlier if the objective metric
    value is not improving significantly over a specified amount of time. This helps
    reduce costs (since the training job ends earlier), as well as prevent the model
    from overfitting. The second one, **local mode**, involves running and testing
    custom scripts inside SageMaker notebook instances before running them in dedicated
    ML instances. This helps speed up the development and debugging of custom training
    (and deployment) scripts since the feedback loop when using local mode is much
    faster. The third one, **heterogeneous cluster training**, involves running a
    training job over several different instance groups. This helps improve the scaling
    and utilization of resources by using a combination of GPU and CPU instances when
    processing ML workloads. The fourth one, **Fast File Mode**, helps significantly
    speed up training jobs by enabling high-performance data access from Amazon S3
    (when downloading the training data). There are more best practices and strategies
    outside of this list, but these should do for now!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished working on the hands-on solutions of this chapter,
    it is time we clean up and turn off any resources we will no longer use.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these steps to locate and turn off any remaining running instances in
    SageMaker Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Running Instances and Kernels** icon in the sidebar of **Amazon
    SageMaker Studio**, as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Turning off any remaining running instances ](img/B18638_06_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.34 – Turning off any remaining running instances
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the **Running Instances and Kernels** icon should open and show the
    running instances, apps, and terminals in SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Turn off any remaining running instances under **RUNNING INSTANCES** by clicking
    the **Shutdown** button for each of the instances as highlighted in the preceding
    screenshot. Clicking the **Shutdown** button will open a pop-up window verifying
    the instance shutdown operation. Click the **Shut down all** button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that this cleanup operation needs to be performed after using SageMaker
    Studio. These resources are not turned off automatically by SageMaker, even during
    periods of inactivity. Turning off unused resources and performing regular cleanup
    operations will help reduce and manage costs.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should be comfortable using the **SageMaker Python SDK** when
    performing ML experiments in the AWS cloud. We are just scratching the surface
    here as SageMaker has a lot more capabilities and features, all of which we will
    discuss over the next few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we trained and deployed ML models using the **SageMaker Python
    SDK**. We started by using the MNIST dataset (training dataset) and SageMaker’s
    built-in **Image Classification Algorithm** to train an image classifier model.
    After that, we took a closer look at the resources used during the training step
    by using the **Debugger Insights Dashboard** available in SageMaker Studio. Finally,
    we performed a second training experiment that made use of several features and
    options available in SageMaker, such as **managed spot training**, **checkpointing**,
    and **incremental training**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into the different deployment options
    and strategies when performing model deployments using SageMaker. We will be deploying
    a pre-trained model into a variety of inference endpoint types, including the
    **real-time**, **serverless**, and **asynchronous** inference endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that were covered in this chapter, feel
    free to check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Amazon SageMaker Debugger* ([https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use Checkpoints in Amazon SageMaker* ([https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Incremental Training in Amazon SageMaker* ([https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Managed Spot Training in Amazon SageMaker* ([https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
