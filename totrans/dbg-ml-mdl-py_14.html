<html><head></head><body>
<div id="_idContainer135">
<h1 class="chapter-number" id="_idParaDest-229"><a id="_idTextAnchor379"/><span class="koboSpan" id="kobo.1.1">14</span></h1>
<h1 id="_idParaDest-230"><a id="_idTextAnchor380"/><span class="koboSpan" id="kobo.2.1">Introduction to Recent Advancements in Machine Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Supervised learning was the focus of the majority of successful applications of machine learning across different industries and application domains until 2020. </span><span class="koboSpan" id="kobo.3.2">However, other techniques, such as generative modeling, later caught the attention of developers and users of machine learning. </span><span class="koboSpan" id="kobo.3.3">So, an understanding of such techniques will help you to broaden your understanding of machine learning capabilities beyond </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">supervised learning.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">following topics:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.7.1">Generative modeling</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.8.1">Reinforcement learning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.9.1">Self-supervised learning</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.10.1">By the end of this chapter, you will have learned about the meaning, widely used techniques, and benefits of generative modeling, </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.13.1">RL</span></strong><span class="koboSpan" id="kobo.14.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">self-supervised learning</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.17.1">SSL</span></strong><span class="koboSpan" id="kobo.18.1">). </span><span class="koboSpan" id="kobo.18.2">You will also practice some of these techniques using Python </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">and PyTorch.</span></span></p>
<h1 id="_idParaDest-231"><a id="_idTextAnchor381"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.21.1">The following requirements are applicable to this chapter as they will help you better understand the concepts, be able to use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.23.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.25.1">torch</span></strong><span class="koboSpan" id="kobo.26.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">2.0.0</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">torchvision</span></strong><span class="koboSpan" id="kobo.29.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">0.15.1</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">matplotlib</span></strong><span class="koboSpan" id="kobo.32.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">3.7.1</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.34.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter14"><span class="No-Break"><span class="koboSpan" id="kobo.36.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter14</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.37.1">.</span></span></p>
<h1 id="_idParaDest-232"><a id="_idTextAnchor382"/><span class="koboSpan" id="kobo.38.1">Generative modeling</span></h1>
<p><span class="koboSpan" id="kobo.39.1">Generative modeling, or </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.40.1">more generally Generative AI, provides you with the opportunity to generate data that is close to an expected or reference set of data points or distributions, commonly referred to as realistic data. </span><span class="koboSpan" id="kobo.40.2">One of the most successful applications of </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.41.1">generative modeling has been in language modeling. </span><span class="koboSpan" id="kobo.41.2">The success story of </span><strong class="bold"><span class="koboSpan" id="kobo.42.1">Generative Pre-trained Transformer</span></strong><span class="koboSpan" id="kobo.43.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.44.1">GPT</span></strong><span class="koboSpan" id="kobo.45.1">)-4 </span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.46.1">and ChatGPT (</span><a href="https://openai.com/blog/chatgpt"><span class="koboSpan" id="kobo.47.1">https://openai.com/blog/chatgpt</span></a><span class="koboSpan" id="kobo.48.1">), a chatbot built on top of GPT-4 and GPT-3.5, and similar </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.49.1">tools such as Perplexity (</span><a href="https://www.perplexity.ai/"><span class="koboSpan" id="kobo.50.1">https://www.perplexity.ai/</span></a><span class="koboSpan" id="kobo.51.1">), resulted in the rise in interest among engineers, scientists, people in different businesses such as finance and healthcare, and many other job roles in generative modeling. </span><span class="koboSpan" id="kobo.51.2">When using Chat-GPT or GPT-4, you can ask a question or provide the description of an ask, called a prompt, and then these tools generate a series of statements or data to provide you with the answer, information, or text you </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">asked for.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">In addition to the successful application of generative modeling in text generation, many other applications of generative modeling can help you in your work or studies. </span><span class="koboSpan" id="kobo.53.2">For example, GPT-4 and its previous versions or other similar models, such as LLaMA (Touvron et al., 2023), can be used for code generation and completion (</span><a href="https://github.com/features/copilot/"><span class="koboSpan" id="kobo.54.1">https://github.com/features/copilot/</span></a><span class="koboSpan" id="kobo.55.1"> and </span><a href="https://github.com/sahil280114/codealpaca"><span class="koboSpan" id="kobo.56.1">https://github.com/sahil280114/codealpaca</span></a><span class="koboSpan" id="kobo.57.1">). </span><span class="koboSpan" id="kobo.57.2">You can write the code you are interested in generating and it generates the corresponding code for you. </span><span class="koboSpan" id="kobo.57.3">Although the generated code might not work as expected all the time, it is usually close to what is expected, at least after a couple </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">of trials.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">There have also been many other successful applications of generative modeling, such as in image generation (</span><a href="https://openai.com/product/dall-e-2"><span class="koboSpan" id="kobo.60.1">https://openai.com/product/dall-e-2</span></a><span class="koboSpan" id="kobo.61.1">), drug discovery (Cheng et al., 2021), fashion design (Davis et al., 2023), manufacturing (Zhao et al., 2023), and </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">Beginning in 2023, many traditional commercial tools and services started integrating Generative AI capabilities. </span><span class="koboSpan" id="kobo.63.2">For example, you can now edit photos using Generative AI in Adobe Photoshop simply by explaining what you need in plain English (</span><a href="https://www.adobe.com/ca/products/photoshop/generative-fill.html"><span class="koboSpan" id="kobo.64.1">https://www.adobe.com/ca/products/photoshop/generative-fill.html</span></a><span class="koboSpan" id="kobo.65.1">). </span><span class="koboSpan" id="kobo.65.2">WolframAlpha also combined its power of symbolic computation with Generative AI, which you can use to ask for specific symbolic processes in plain English (</span><a href="https://www.wolframalpha.com/input?i=Generative+Adversarial+Networks"><span class="koboSpan" id="kobo.66.1">https://www.wolframalpha.com/input?i=Generative+Adversarial+Networks</span></a><span class="koboSpan" id="kobo.67.1">). </span><span class="koboSpan" id="kobo.67.2">Khan Academy (</span><a href="https://www.khanacademy.org/"><span class="koboSpan" id="kobo.68.1">https://www.khanacademy.org/</span></a><span class="koboSpan" id="kobo.69.1">) designed a strategy to help teachers and students benefit </span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.70.1">from Generative AI, specifically ChatGPT, instead of it being harmful to the education </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">of students.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">These success stories have been achieved by relying on different deep learning techniques designed for generative modeling, which we will briefly </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">review n</span><a id="_idTextAnchor383"/><span class="koboSpan" id="kobo.74.1">ext.</span></span></p>
<h2 id="_idParaDest-233"><a id="_idTextAnchor384"/><span class="koboSpan" id="kobo.75.1">Generative deep learning techniques</span></h2>
<p><span class="koboSpan" id="kobo.76.1">There are </span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.77.1">multiple generative modeling approaches with available the APIs available in PyTorch or other deep learning frameworks, such as TensorFlow. </span><span class="koboSpan" id="kobo.77.2">Here, we will review some of them to help you start learning more about how they work and how you can use them </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">in Python.</span></span></p>
<h3><span class="koboSpan" id="kobo.79.1">Transformer-based text generation</span></h3>
<p><span class="koboSpan" id="kobo.80.1">You already </span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.81.1">learned that transformers, introduced in 2017 (Vaswani et al., 2017), are used to generate </span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.82.1">the most successful recent language models in </span><a href="B16369_13.xhtml#_idTextAnchor342"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.83.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.84.1">, </span><em class="italic"><span class="koboSpan" id="kobo.85.1">Advanced Deep Learning Techniques</span></em><span class="koboSpan" id="kobo.86.1">. </span><span class="koboSpan" id="kobo.86.2">However, these models are not useful only for tasks such as translation, which is traditional in natural language processing, but can be used in generative modeling to help us generate meaningful text, for example, in response to a question we ask. </span><span class="koboSpan" id="kobo.86.3">This is the approach behind GPT models, Chat-GPT, and many other generative language models. </span><span class="koboSpan" id="kobo.86.4">The process of providing a short text, as an ask or a question, is also called prompting, in which we need to provide a good prompt to get a good answer. </span><span class="koboSpan" id="kobo.86.5">We will talk about optimal prompting in the </span><em class="italic"><span class="koboSpan" id="kobo.87.1">Prompt engineering for text-based generative </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.88.1">models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.89.1"> section.</span></span></p>
<h3><span class="koboSpan" id="kobo.90.1">Variational autoencoders (VAEs)</span></h3>
<p><span class="koboSpan" id="kobo.91.1">Autoencoders are </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.92.1">techniques </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.93.1">with which you can reduce the number of features to an information-rich set of embeddings, which you can consider a more complicated version of </span><strong class="bold"><span class="koboSpan" id="kobo.94.1">principal component analysis</span></strong><span class="koboSpan" id="kobo.95.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.96.1">PCA</span></strong><span class="koboSpan" id="kobo.97.1">) to better </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.98.1">understand it. </span><span class="koboSpan" id="kobo.98.2">It does that by first attempting to encode the original space to the new embedding (called encoding), then decode the embeddings, and regenerate the original features for each data point (called decoding). </span><span class="koboSpan" id="kobo.98.3">In a VAE (Kingma and Welling, 2013), instead of one set of features (embeddings), it generates a distribution for each new feature. </span><span class="koboSpan" id="kobo.98.4">For example, instead of reducing the original 1,000 features to 100 features, each having one float value, you get 100 new variables, each being a normal (or Gaussian) distribution. </span><span class="koboSpan" id="kobo.98.5">The beauty of this process is that then you can select different values from these distributions for each variable and </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.99.1">generate a new set of 100 embeddings. </span><span class="koboSpan" id="kobo.99.2">In the </span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.100.1">process of decoding them, these embeddings get decoded and a new set of features with the original size (1,000) gets generated. </span><span class="koboSpan" id="kobo.100.2">This process can be used for different types of data such as images (Vahdat et al., 2020) and graphs (Simonovsky et al., 2018; Wengong et al., 2018). </span><span class="koboSpan" id="kobo.100.3">You can find a collection of VAEs implemented in PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">at </span></span><a href="https://github.com/AntixK/PyTorch-VAE"><span class="No-Break"><span class="koboSpan" id="kobo.102.1">https://github.com/AntixK/PyTorch-VAE</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.103.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.104.1">Generative Adversarial Networks (GANs)</span></h3>
<p><span class="koboSpan" id="kobo.105.1">In this technique introduced in 2014 (Goodfellow et al., 2020), a discriminator that works like a </span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.106.1">supervised </span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.107.1">classification model and a generator work alongside each other. </span><span class="koboSpan" id="kobo.107.2">The generator, which could be a neural network architecture for generating the desired data types, such as images, generates images aiming to fool the discriminator into recognizing the generated data as real data. </span><span class="koboSpan" id="kobo.107.3">The discriminator learns to remain good at distinguishing generated data from real data. </span><span class="koboSpan" id="kobo.107.4">The generated data in some cases is called fake data, as in technologies and models </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.108.1">such as deepfakes (</span><a href="https://www.businessinsider.com/guides/tech/what-is-deepfake"><span class="koboSpan" id="kobo.109.1">https://www.businessinsider.com/guides/tech/what-is-deepfake</span></a><span class="koboSpan" id="kobo.110.1">). </span><span class="koboSpan" id="kobo.110.2">However, the generated data can be used as opportunities for new data points to be used in different applications, </span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.111.1">such as drug discovery (Prykhodko et al., 2019). </span><span class="koboSpan" id="kobo.111.2">You can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">torchgan</span></strong><span class="koboSpan" id="kobo.113.1"> to implement </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">GANs (</span></span><a href="https://torchgan.readthedocs.io/en/latest/"><span class="No-Break"><span class="koboSpan" id="kobo.115.1">https://torchgan.readthedocs.io/en/latest/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.116.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">As there has been an emerging generation of prompt-based technologies built on top of generative models, we will provide a better understanding of how to optimally desi</span><a id="_idTextAnchor385"/><span class="koboSpan" id="kobo.118.1">gn </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">prompts next.</span></span></p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor386"/><span class="koboSpan" id="kobo.120.1">Prompt engineering for text-based generative models</span></h2>
<p><span class="koboSpan" id="kobo.121.1">Prompt engineering is not only a recent topic in machine learning but has also become a highly paid job title. </span><span class="koboSpan" id="kobo.121.2">In prompt engineering, we aim to provide optimal prompts to generate the </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.122.1">best possible result (for example, text, code, and images) and identify issues with the generative models as opportunities for improving them. </span><span class="koboSpan" id="kobo.122.2">A basic understanding of large language and generative models, your language proficiency, and domain knowledge for domain-specific data generation can help you in better prompting. </span><span class="koboSpan" id="kobo.122.3">There are free resources that you can use to learn about prompt engineering, such as a course by Andrew Ng and OpenAI (</span><a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/"><span class="koboSpan" id="kobo.123.1">https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/</span></a><span class="koboSpan" id="kobo.124.1">) and some introductory content about prompt engineering released by Microsoft (</span><a href="https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/prompt-engineering"><span class="koboSpan" id="kobo.125.1">https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/prompt-engineering</span></a><span class="koboSpan" id="kobo.126.1">). </span><span class="koboSpan" id="kobo.126.2">However, we will not leave you to learn this topic from scratch by yourself. </span><span class="koboSpan" id="kobo.126.3">We will provide you with some guidance for optimal prompting here that will help you improve your </span><a id="_idTextAnchor387"/><span class="No-Break"><span class="koboSpan" id="kobo.127.1">prompting skills.</span></span></p>
<h3><span class="koboSpan" id="kobo.128.1">Targeted prompting</span></h3>
<p><span class="koboSpan" id="kobo.129.1">In our daily conversations, either at work, university, or home, there are ways we try to make sure the person across from us better understands what we mean, and as a result, we get a better </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.130.1">response. </span><span class="koboSpan" id="kobo.130.2">For example, if you tell your friend, “Give me that” instead of “Give me that bottle of water on the desk,” there is a chance that your friend won’t give you the bottle of water or get confused about what exactly you are referring to. </span><span class="koboSpan" id="kobo.130.3">In prompting, you can get better responses and data generated, such as images, if you clearly explain what you want for a very specific task. </span><span class="koboSpan" id="kobo.130.4">Here are a few techniques to use for </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">better prompting:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Be specific about the ask</span></strong><span class="koboSpan" id="kobo.133.1">: You </span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.134.1">can provide specific information such as the format of the data you would like to be generated, such as bullet points or code, and the task you are referring to, such as writing an email versus a </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">business plan.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.136.1">Specify who the data is getting generated for</span></strong><span class="koboSpan" id="kobo.137.1">: You can even specify an expertise or job title for whom the data is getting generated, such as generating a piece of text for a machine learning engineer, business manager, or </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">software developer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Specify time</span></strong><span class="koboSpan" id="kobo.140.1">: You can specify whether you want information about the date when technology got released, the first time something was announced, the chronological order of events, the change in something such as the net worth of a famous rich person such as Elon Musk over time, and </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Simplify the concepts</span></strong><span class="koboSpan" id="kobo.143.1">: You can </span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.144.1">provide a simplified version of what you ask to make sure the model doesn’t get confused by the complexity of </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">your prompt.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.146.1">Although these techniques will help you in better prompting, there is still a chance of getting false </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.147.1">answers with high confidence if you ask for a text response or unrelated data generation. </span><span class="koboSpan" id="kobo.147.2">This is what is usually referred to as a hallucination. </span><span class="koboSpan" id="kobo.147.3">One of the ways to decrease the chance of irrelevant or wrong responses or data generation is to provide tests for the model to use. </span><span class="koboSpan" id="kobo.147.4">When we write functions and classes in Python, we can design unit tests to make sure their output meets the expectation, as discussed in </span><a href="B16369_08.xhtml#_idTextAnchor243"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.148.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.149.1">, </span><em class="italic"><span class="koboSpan" id="kobo.150.1">Controlling Risks Using </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Tes</span><a id="_idTextAnchor388"/><span class="koboSpan" id="kobo.152.1">t-Driven Development</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">.</span></span></p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor389"/><span class="koboSpan" id="kobo.154.1">Generative modeling using PyTorch</span></h2>
<p><span class="koboSpan" id="kobo.155.1">You can develop generative models based on different techniques discussed earlier in this chapter </span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.156.1">using PyTorch. </span><span class="koboSpan" id="kobo.156.2">We want to practice with </span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.157.1">VAEs here. </span><span class="koboSpan" id="kobo.157.2">With VAEs, the aim is to identify a probability distribution for a lower-dimensional representation of data. </span><span class="koboSpan" id="kobo.157.3">For example, the model learns about the mean and variance (or log variance) for the representations of the input parameters, assuming normal or Gaussian distribution for the latent space (that is, the space of the latent variables </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">or representations).</span></span></p>
<p><span class="koboSpan" id="kobo.159.1">We first import the required libraries and modules and load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">Flowers102</span></strong><span class="koboSpan" id="kobo.161.1"> dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">from PyTorch:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.163.1">
transform = transforms.Compose([    transforms.Resize((32, 32)),
    transforms.ToTensor()
])
train_dataset = datasets.Flowers102(root='./data',
    download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32,
    shuffle=True)</span></pre>
<p><span class="koboSpan" id="kobo.164.1">Then, we define a class for the VAE as follows in which two linear layers are defined to encode the input pixels of images. </span><span class="koboSpan" id="kobo.164.2">Then, the mean and variance of the probability distribution of latent space are also defined by two linear layers for decoding the latent variables back </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.165.1">to the original number of inputs to generate </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.166.1">images similar to the input data. </span><span class="koboSpan" id="kobo.166.2">The learned mean and variance of the distribution in latent space will be then used to generate new latent variables and potentially generate </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">new data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.168.1">
class VAE(nn.Module):    def __init__(self):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(32 * 32 * 3, 512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.ReLU(),
        )self.fc_mean = nn.Linear(128, 32)
         self.fc_var = nn.Linear(128, 32)
         self.decoder = nn.Sequential(
             nn.Linear(32, 128),
             nn.ReLU(),
             nn.Linear(128, 512),
             nn.ReLU(),
             nn.Linear(512, 32 * 32 * 3),
             nn.Sigmoid(),
         )
    def forward(self, x):
        h = self.encoder(x.view(-1, 32 * 32 * 3))
        mean, logvar = self.fc_mean(h), self.fc_var(h)
        std = torch.exp(0.5*logvar)
        q = torch.distributions.Normal(mean, std)
        z = q.rsample()
        return self.decoder(z), mean, logvar</span></pre>
<p><span class="koboSpan" id="kobo.169.1">We now initialize the defined </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">VAE</span></strong><span class="koboSpan" id="kobo.171.1"> class and determine the </span><strong class="source-inline"><span class="koboSpan" id="kobo.172.1">Adam</span></strong><span class="koboSpan" id="kobo.173.1"> optimizer as the optimization algorithm with </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">0.002</span></strong><span class="koboSpan" id="kobo.175.1"> as the </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">learning rate:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.177.1">
model = VAE()optimizer = optim.Adam(model.parameters(), lr=2e-3)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)</span></pre>
<p><span class="koboSpan" id="kobo.178.1">We then define </span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.179.1">a loss function using </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">binary_cross_entropy</span></strong><span class="koboSpan" id="kobo.181.1"> as follows </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.182.1">to compare the regenerated pixels with the </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">input pixels:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.184.1">
def loss_function(recon_x, x, mu, logvar):    BCE = nn.functional.binary_cross_entropy(recon_x,
        x.view(-1, 32 * 32 * 3), reduction='sum')
    KLD = -0.5 * torch.sum(
        1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD</span></pre>
<p><span class="koboSpan" id="kobo.185.1">Now we are ready to train the model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.186.1">Flowers102</span></strong><span class="koboSpan" id="kobo.187.1"> dataset we </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">loaded before:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.189.1">
n_epoch = 400for epoch in range(n_epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mean, logvar = model(data)
        loss = loss_function(recon_batch, data, mean,
            logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
    print(f'Epoch: {epoch} Average loss: {
        train_loss / len(train_loader.dataset):.4f}')</span></pre>
<p><span class="koboSpan" id="kobo.190.1">We can then use this </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.191.1">trained model to generate images </span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.192.1">that almost look like flowers (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.193.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.194.1">.1</span></em><span class="koboSpan" id="kobo.195.1">). </span><span class="koboSpan" id="kobo.195.2">Upon hyperparameter optimization, such as changing the model's architecture, you can achieve better results. </span><span class="koboSpan" id="kobo.195.3">You can review hyperparameter optimization in deep learning in </span><a href="B16369_12.xhtml#_idTextAnchor320"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.196.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.197.1">, </span><em class="italic"><span class="koboSpan" id="kobo.198.1">Going Beyond ML Debugging with </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.199.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<span class="koboSpan" id="kobo.201.1"><img alt="Figure 14.1 – Example images generated by the simple VAE we developed earlier" src="image/B16369_14_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.202.1">Figure 14.1 – Example images generated by the simple VAE we developed earlier</span></p>
<p><span class="koboSpan" id="kobo.203.1">This was a simple </span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.204.1">example of generative modeling </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.205.1">using PyTorch. </span><span class="koboSpan" id="kobo.205.2">In spite of the success of generative modeling, part of the recent success of tools developed using generative models, such as Chat-GPT, is due to the smart use of reinforcement learnin</span><a id="_idTextAnchor390"/><span class="koboSpan" id="kobo.206.1">g, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">discuss next.</span></span></p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor391"/><span class="koboSpan" id="kobo.208.1">Reinforcement learning</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.209.1">Reinforcement learning</span></strong><span class="koboSpan" id="kobo.210.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.211.1">RL</span></strong><span class="koboSpan" id="kobo.212.1">) is not a new idea or technique. </span><span class="koboSpan" id="kobo.212.2">The initial idea dates back to the 1950s, when </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.213.1">it was introduced by Richard Bellman with the concept of the Bellman equation (Sutton and Barto, 2018). </span><span class="koboSpan" id="kobo.213.2">However, its recent combination with human feedback, which we will explain in the next section, provided a new opportunity for its utility in developing machine learning technologies. </span><span class="koboSpan" id="kobo.213.3">The general idea of RL is to learn by experience, or interaction with a specified environment, instead of using a collected set of data points for training, as in supervised learning. </span><span class="koboSpan" id="kobo.213.4">An agent is considered in RL, which learns how to improve actions to get a greater reward (Kaelbling et al., 1996). </span><span class="koboSpan" id="kobo.213.5">The agent learns to improve its approach to taking action, or policy in more technical terminology, iteratively after receiving the reward of the action taken in the </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">previous step.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">In the history of RL, two important developments and utilities resulted in an increase in its popularity including the development of Q-learning (Watkins, 1989) and combining RL and deep learning (Mnih et al., 2013) using Q-learning. </span><span class="koboSpan" id="kobo.215.2">In spite of the success stories behind RL and the intuition that it mimics learning by experience as humans do, it has been shown that deep reinforcement learning is not data efficient and requires large amounts of data or iterative experience, which makes it fundamentally different from human learning (Botvinick et </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">al., 2019).</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">More recently, </span><strong class="bold"><span class="koboSpan" id="kobo.218.1">reinforcement learning with human feedback</span></strong><span class="koboSpan" id="kobo.219.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.220.1">RLHF</span></strong><span class="koboSpan" id="kobo.221.1">) was used as a successful </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.222.1">application of reinforcement learning to improve the </span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.223.1">results of generative mo</span><a id="_idTextAnchor392"/><span class="koboSpan" id="kobo.224.1">dels, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">discuss next.</span></span></p>
<h2 id="_idParaDest-237"><a id="_idTextAnchor393"/><span class="koboSpan" id="kobo.226.1">Reinforcement learning with human feedback (RLHF)</span></h2>
<p><span class="koboSpan" id="kobo.227.1">With reinforcement learning with human feedback, the reward is calculated based on the feedback of </span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.228.1">humans, either experts or non-experts, depending on the problem. </span><span class="koboSpan" id="kobo.228.2">However, the reward is not like a predefined mathematical formula considering the complexity of the problems such as language modeling. </span><span class="koboSpan" id="kobo.228.3">The feedback provided by humans results in improving the model step by step. </span><span class="koboSpan" id="kobo.228.4">For example, the training process of a RLHF language model can be summarized as </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">follows (</span></span><a href="https://huggingface.co/blog/rlhf"><span class="No-Break"><span class="koboSpan" id="kobo.230.1">https://huggingface.co/blog/rlhf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.231.1">):</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.232.1">Training a language model, which is referred to </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">as pretraining.</span></span></li>
<li><span class="koboSpan" id="kobo.234.1">Data collection and training the </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">reward model.</span></span></li>
<li><span class="koboSpan" id="kobo.236.1">Fine-tuning the language model with reinforcement learning using the </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">reward model.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.238.1">However, learning how to use PyTorch to design RLHF-based models could be helpful </span><a id="_idTextAnchor394"/><span class="koboSpan" id="kobo.239.1">to better understand </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">this concept.</span></span></p>
<h3><span class="koboSpan" id="kobo.241.1">RLHF with PyTorch</span></h3>
<p><span class="koboSpan" id="kobo.242.1">One of the major challenges in benefitting from RLHF is designing an infrastructure for human feedback </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.243.1">collection and curation, then providing them to calculate the reward, and then improving </span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.244.1">the main pre-trained model. </span><span class="koboSpan" id="kobo.244.2">Here, we don’t want to get into that aspect of RLHF but rather go through a simple code example to understand how such feedback can be incorporated into a machine learning model. </span><span class="koboSpan" id="kobo.244.3">There are good resources, such as </span><a href="https://github.com/lucidrains/PaLM-rlhf-pytorch"><span class="koboSpan" id="kobo.245.1">https://github.com/lucidrains/PaLM-rlhf-pytorch</span></a><span class="koboSpan" id="kobo.246.1">, that can help you to improve your understanding of RLHF and how to implement it using Python </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">and PyTorch.</span></span></p>
<p><span class="koboSpan" id="kobo.248.1">Here, we will use GPT-2 (</span><a href="https://huggingface.co/transformers/v1.2.0/_modules/pytorch_transformers/modeling_gpt2.html"><span class="koboSpan" id="kobo.249.1">https://huggingface.co/transformers/v1.2.0/_modules/pytorch_transformers/modeling_gpt2.html</span></a><span class="koboSpan" id="kobo.250.1">) as the pre-trained model. </span><span class="koboSpan" id="kobo.250.2">First, we import the necessary libraries and modules and initialize the model, tokenizer, and optimizer, which is chosen to </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">be </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">Adam</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.254.1">
import torchfrom transformers import GPT2LMHeadModel, GPT2Tokenizer
from torch import optim
from torch.utils.data import DataLoader
# Pretrain a GPT-2 language model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
optimizer = optim.Adam(model.parameters(), lr=1e-3)</span></pre>
<p><span class="koboSpan" id="kobo.255.1">Now, assuming </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.256.1">we collected the </span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.257.1">human feedback and formatted it properly, we can use it to create a DataLoader </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">from PyTorch:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.259.1">
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)</span></pre> <p><span class="koboSpan" id="kobo.260.1">The next step is to design a reward model, for which we use a two-layer fully connected </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">neural network:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.262.1">
class Reward_Model(torch.nn.Module):    def __init__(self, input_size, hidden_size, output_size):
        super(RewardModel, self).__init__()
        self.fc_layer1 = torch.nn.Linear(input_size,
            hidden_size)
        self.fc_layer2 = torch.nn.Linear(hidden_size,
            output_size)
    def forward(self, x):
        x = torch.relu(self.fc_layer1(x))
        x = self.fc_layer2(x)
        return x</span></pre>
<p><span class="koboSpan" id="kobo.263.1">We then initialize the reward model using the previously </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">defined class:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.265.1">
reward_model = Reward_Model(input_size, hidden_size, output_size)</span></pre> <p><span class="koboSpan" id="kobo.266.1">We are now ready to improve our pre-trained model using the collected human feedback and the reward model. </span><span class="koboSpan" id="kobo.266.2">If you pay attention to the following code, the main difference </span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.267.1">between this simple loop over epochs and batches for model training compared to neural networks </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.268.1">without a reward model is the reward calculation and then using it for </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">loss calculation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.270.1">
for epoch in range(n_epochs):    for batch in dataloader:
        input_ids = tokenizer.encode(batch['input'],
            return_tensors='pt')
        output_ids = tokenizer.encode(batch['output'],
            return_tensors='pt')
        reward = reward_model(batch['input'])
        loss = model(input_ids, labels=output_ids).loss * reward
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()</span></pre>
<p><span class="koboSpan" id="kobo.271.1">This was a very simple example of designing RLHF-based model improvement, used to help you better understand the concept. </span><span class="koboSpan" id="kobo.271.2">Resources such as </span><a href="https://github.com/lucidrains/PaLM-rlhf-pytorch"><span class="koboSpan" id="kobo.272.1">https://github.com/lucidrains/PaLM-rlhf-pytorch</span></a><span class="koboSpan" id="kobo.273.1"> will help you to implement more complex ways </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.274.1">of incorporating such human </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.275.1">feedback for improving </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">your models.</span></span></p>
<p><span class="koboSpan" id="kobo.277.1">Next, let’s go through another interesting topic in machine </span><a id="_idTextAnchor395"/><span class="koboSpan" id="kobo.278.1">learning, called </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">self-supervised learning.</span></span></p>
<h1 id="_idParaDest-238"><a id="_idTextAnchor396"/><span class="koboSpan" id="kobo.280.1">Self-supervised learning (SSL)</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.281.1">Self-supervised learning</span></strong><span class="koboSpan" id="kobo.282.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.283.1">SSL</span></strong><span class="koboSpan" id="kobo.284.1">) is not a new concept. </span><span class="koboSpan" id="kobo.284.2">It's similar to RL, but it gained attention after its</span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.285.1"> combination with deep learning due to its effectiveness in learning data representations. </span><span class="koboSpan" id="kobo.285.2">Examples of such models are Word2vec for language modeling (Mikolov et al., 2013) and Meta’s RoBERTa models trained using SSL, which achieved state-of-the-art performance on several language modeling tasks. </span><span class="koboSpan" id="kobo.285.3">The idea of SSL is to define an objective for the machine learning model that doesn’t rely on pre-labeling or the quantification of data points – for example, predicting the positions of objects or people in videos for each time step using previous time steps, masking parts of images or sequence data, and aiming to refill those masked sections. </span><span class="koboSpan" id="kobo.285.4">One of the widely used applications of such models is in RL to learn representations of images and text, and then use those representations in other contexts, for example, in supervised modeling of smaller datasets with data labels (Kolesnikov et al., 2019, Wang et </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">al., 2020).</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">There are multiple techniques under the umbrella of SSL, three of which are </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.289.1">Contrastive learning</span></strong><span class="koboSpan" id="kobo.290.1">: The idea </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.291.1">of contrastive learning is to learn representations that result in similar </span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.292.1">data points being closer to each other compared to dissimilar data points (Jaiswal et </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">al., 2020).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.294.1">Autoregressive models</span></strong><span class="koboSpan" id="kobo.295.1">: In autoregressive modeling, the model aims to predict the </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.296.1">next data points, either based on time or a specific sequence order, given the previous ones. </span><span class="koboSpan" id="kobo.296.2">This is a very popular technique in language modeling, where models such as GPT predict the next word in a sentence (Radford et </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">al., 2019).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.298.1">Self-supervision via inpainting</span></strong><span class="koboSpan" id="kobo.299.1">: In this approach, we mask parts of the data and train the models to fill in the missing parts. </span><span class="koboSpan" id="kobo.299.2">For example, a portion of an image might be </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.300.1">masked, and the model is </span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.301.1">trained to predict the masked portion. </span><span class="koboSpan" id="kobo.301.2">Masked autoencoder is an example of such a technique in which the masked portions of images are refilled in the decoding process of the autoencoder (Zhang et </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">al., 2022).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.303.1">Next, we will practice with a simple example of se</span><a id="_idTextAnchor397"/><span class="koboSpan" id="kobo.304.1">lf-supervised modeling using Python </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">and PyTorch.</span></span></p>
<h2 id="_idParaDest-239"><a id="_idTextAnchor398"/><span class="koboSpan" id="kobo.306.1">Self-supervised learning with PyTorch</span></h2>
<p><span class="koboSpan" id="kobo.307.1">From a programming perspective, the main difference between deep learning for SSL compared to </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.308.1">supervised learning is in defining the </span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.309.1">objectives and data for training and testing. </span><span class="koboSpan" id="kobo.309.2">Here, we </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.310.1">want to practice with </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">self-supervision via inpainting</span></strong><span class="koboSpan" id="kobo.312.1"> using a masked image autoencoder based on convolutional layers. </span><span class="koboSpan" id="kobo.312.2">We also use the same </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">Flowers102</span></strong><span class="koboSpan" id="kobo.314.1"> dataset we used to practice </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">with RLHF.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">We first define the neural network class using two encoding and decoding </span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">torch.nn.Conv2d()</span></strong><span class="koboSpan" id="kobo.318.1"> layers </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.320.1">
class Conv_AE(nn.Module):    def __init__(self):
        super(Conv_AE, self).__init__()
        # Encoding data
        self.encoding_conv1 = nn.Conv2d(3, 8, 3, padding=1)
        self.encoding_conv2 = nn.Conv2d(8, 32, 3,padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        # Decoding data
        self.decoding_conv1 = nn.ConvTranspose2d(32, 8, 2,
            stride=2)
        self.decoding_conv2 = nn.ConvTranspose2d(8, 3, 2,
            stride=2)
    def forward(self, x):
        # Encoding data
        x = torch.relu(self.encoding_conv1(x))
        x = self.pool(x)
        x = torch.relu(self.encoding_conv2(x))
        x = self.pool(x)
        # Decoding data
        x = torch.relu(self.decoding_conv1(x))
        x = self.decoding_conv2(x)
        x = torch.sigmoid(x)
        return x</span></pre>
<p><span class="koboSpan" id="kobo.321.1">We then initialize the model, specify </span><strong class="source-inline"><span class="koboSpan" id="kobo.322.1">torch.nn.MSELoss()</span></strong><span class="koboSpan" id="kobo.323.1"> as the criterion for comparison </span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.324.1">of predicted and true images, </span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.325.1">and </span><strong class="source-inline"><span class="koboSpan" id="kobo.326.1">torch.optim.Adam()</span></strong><span class="koboSpan" id="kobo.327.1"> as the optimizer with a learning rate </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">0.001</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.331.1">
model = Conv_AE().to(device)criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)</span></pre>
<p><span class="koboSpan" id="kobo.332.1">The following function helps us to implement masking on random 8x8 portions of each image, which then the autoencoder learns </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">to fill:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.334.1">
def create_mask(size=(32, 32), mask_size=8):    mask = np.ones((3, size[0], size[1]), dtype=np.float32)
    height, width = size
    m_height, m_width = mask_size, mask_size
    top = np.random.randint(0, height - m_height)
    left = np.random.randint(0, width - m_width)
    mask[:, top:top+m_height, left:left+m_width] = 0
    return torch.from_numpy(mask)</span></pre>
<p><span class="koboSpan" id="kobo.335.1">Then, we train the model for 200 epochs as follows. </span><span class="koboSpan" id="kobo.335.2">As you can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.336.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.337.1">.2</span></em><span class="koboSpan" id="kobo.338.1">, the images first </span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.339.1">get masked, and then in the decoding step, the </span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.340.1">autoencoder attempts to rebuild the full image, including the </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">masked portions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.342.1">
n_epoch = 200for epoch in range(n_epoch):
    for data in train_loader:
        img, _ = data
        # Creating mask for small part in training images
        mask = create_mask().to(device)
        img_masked = img * mask
        img = img.to(device)
        img_masked = img_masked.to(device)
        optimizer.zero_grad()
        outputs = model(img_masked)
        loss = criterion(outputs, img)
        loss.backward()
        optimizer.step()</span></pre>
<p><span class="koboSpan" id="kobo.343.1">As you can see in the examples of the resulting refilled images shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.344.1">Figure 14</span></em></span><em class="italic"><span class="koboSpan" id="kobo.345.1">.2</span></em><span class="koboSpan" id="kobo.346.1">, the model could find the patterns correctly. </span><span class="koboSpan" id="kobo.346.2">However, with proper hyperparameter optimization and designing models with better neural network architectures, you can achieve higher performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">better models.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<span class="koboSpan" id="kobo.348.1"><img alt="Figure 14.2 – Example images (first row), their masked versions (second row), and regenerated versions (third row) using the convolutional autoencoder model" src="image/B16369_14_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.349.1">Figure 14.2 – Example images (first row), their masked versions (second row), and regenerated versions (third row) using the convolutional autoencoder model</span></p>
<p><span class="koboSpan" id="kobo.350.1">You can read </span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.351.1">more about SSL and the other techniques </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.352.1">provided in this chapter using the provided resou</span><a id="_idTextAnchor399"/><span class="koboSpan" id="kobo.353.1">rces and references to better understand </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">these concepts.</span></span></p>
<h1 id="_idParaDest-240"><a id="_idTextAnchor400"/><span class="koboSpan" id="kobo.355.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.356.1">In this chapter, you gained a high-level understanding of recent advancements in machine learning modeling beyond supervised learning, including generative modeling, reinforcement learning, and self-supervised learning. </span><span class="koboSpan" id="kobo.356.2">You also learned about optimal prompting and prompt engineering to benefit from tools and applications built on top of generative models that accept text prompts as input from users. </span><span class="koboSpan" id="kobo.356.3">You were provided with the relevant code repositories and functionalities available in Python and PyTorch that will help you to start learning more about these advanced techniques. </span><span class="koboSpan" id="kobo.356.4">This knowledge helps you not only better understand how they work if you come across them but also start building models of your own using these </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">advanced techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.358.1">In the next chapter, you will learn about the benefits of identifying causal relationships in machine learning modeling and practice with Python </span><a id="_idTextAnchor401"/><span class="koboSpan" id="kobo.359.1">libraries that help you in implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">causal modeling.</span></span></p>
<h1 id="_idParaDest-241"><a id="_idTextAnchor402"/><span class="koboSpan" id="kobo.361.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.362.1">What are examples of generative deep </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">learning techniques?</span></span></li>
<li><span class="koboSpan" id="kobo.364.1">What are examples of generative text models that </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">use transformers?</span></span></li>
<li><span class="koboSpan" id="kobo.366.1">What are generators and discriminators </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">in GANs?</span></span></li>
<li><span class="koboSpan" id="kobo.368.1">What are some of the techniques you can use for </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">better prompting?</span></span></li>
<li><span class="koboSpan" id="kobo.370.1">Could you explain how RL could be helpful in importing the results of </span><a id="_idTextAnchor403"/><span class="No-Break"><span class="koboSpan" id="kobo.371.1">generative models?</span></span></li>
<li><span class="koboSpan" id="kobo.372.1">Briefly explain </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">contrastive learning.</span></span></li>
</ol>
<h1 id="_idParaDest-242"><a id="_idTextAnchor404"/><span class="koboSpan" id="kobo.374.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.375.1">Cheng, Yu, et al. </span><span class="koboSpan" id="kobo.375.2">“</span><em class="italic"><span class="koboSpan" id="kobo.376.1">Molecular design in drug discovery: a comprehensive review of deep generative models</span></em><span class="koboSpan" id="kobo.377.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.378.1">Briefings in bioinformatics</span></em><span class="koboSpan" id="kobo.379.1"> 22.6 (</span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">2021): bbab344.</span></span></li>
<li><span class="koboSpan" id="kobo.381.1">Davis, Richard Lee, et al. </span><span class="koboSpan" id="kobo.381.2">“</span><em class="italic"><span class="koboSpan" id="kobo.382.1">Fashioning the Future: Unlocking the Creative Potential of Deep Generative Models for Design Space Exploration</span></em><span class="koboSpan" id="kobo.383.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.384.1">Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.385.1">Systems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.386.1"> (2023).</span></span></li>
<li><span class="koboSpan" id="kobo.387.1">Zhao, Yaoyao Fiona, et al., eds. </span><span class="koboSpan" id="kobo.387.2">“</span><em class="italic"><span class="koboSpan" id="kobo.388.1">Design for Advanced Manufacturing</span></em><span class="koboSpan" id="kobo.389.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.390.1">Journal of Mechanical Design</span></em><span class="koboSpan" id="kobo.391.1"> 145.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">2023): 010301.</span></span></li>
<li><span class="koboSpan" id="kobo.393.1">Touvron, Hugo, et al. </span><span class="koboSpan" id="kobo.393.2">“</span><em class="italic"><span class="koboSpan" id="kobo.394.1">Llama: Open and efficient foundation language models</span></em><span class="koboSpan" id="kobo.395.1">.” </span><span class="koboSpan" id="kobo.395.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">arXiv:2302.13971 (2023).</span></span></li>
<li><span class="koboSpan" id="kobo.397.1">Vaswani, Ashish, et al. </span><span class="koboSpan" id="kobo.397.2">“</span><em class="italic"><span class="koboSpan" id="kobo.398.1">Attention is all you need</span></em><span class="koboSpan" id="kobo.399.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.400.1">Advances in neural information processing systems</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.401.1">30 (2017).</span></span></li>
<li><span class="koboSpan" id="kobo.402.1">Kingma, Diederik P., and Max Welling. </span><span class="koboSpan" id="kobo.402.2">“</span><em class="italic"><span class="koboSpan" id="kobo.403.1">Auto-encoding variational bayes</span></em><span class="koboSpan" id="kobo.404.1">.” </span><span class="koboSpan" id="kobo.404.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">arXiv:1312.6114 (2013).</span></span></li>
<li><span class="koboSpan" id="kobo.406.1">Vahdat, Arash, and Jan Kautz. </span><span class="koboSpan" id="kobo.406.2">“</span><em class="italic"><span class="koboSpan" id="kobo.407.1">NVAE: A deep hierarchical variational autoencoder</span></em><span class="koboSpan" id="kobo.408.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.409.1">Advances in neural information processing systems</span></em><span class="koboSpan" id="kobo.410.1"> 33 (</span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">2020): 19667-19679.</span></span></li>
<li><span class="koboSpan" id="kobo.412.1">Simonovsky, Martin, and Nikos Komodakis. </span><span class="koboSpan" id="kobo.412.2">“</span><em class="italic"><span class="koboSpan" id="kobo.413.1">Graphvae: Towards generation of small graphs using variational autoencoders</span></em><span class="koboSpan" id="kobo.414.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.415.1">Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural Networks</span></em><span class="koboSpan" id="kobo.416.1">, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I 27. </span><span class="koboSpan" id="kobo.416.2">Springer International </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">Publishing (2018).</span></span></li>
<li><span class="koboSpan" id="kobo.418.1">Jin, Wengong, Regina Barzilay, and Tommi Jaakkola. </span><span class="koboSpan" id="kobo.418.2">“</span><em class="italic"><span class="koboSpan" id="kobo.419.1">Junction tree variational autoencoder for molecular graph generation</span></em><span class="koboSpan" id="kobo.420.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.421.1">International conference on machine learning</span></em><span class="koboSpan" id="kobo.422.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">PMLR (2018).</span></span></li>
<li><span class="koboSpan" id="kobo.424.1">Goodfellow, Ian, et al. </span><span class="koboSpan" id="kobo.424.2">“</span><em class="italic"><span class="koboSpan" id="kobo.425.1">Generative adversarial networks</span></em><span class="koboSpan" id="kobo.426.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.427.1">Communications of the ACM</span></em><span class="koboSpan" id="kobo.428.1"> 63.11 (</span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">2020): 139-144.</span></span></li>
<li><span class="koboSpan" id="kobo.430.1">Karras, Tero, Samuli Laine, and Timo Aila. </span><span class="koboSpan" id="kobo.430.2">“</span><em class="italic"><span class="koboSpan" id="kobo.431.1">A style-based generator architecture for generative adversarial networks</span></em><span class="koboSpan" id="kobo.432.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.433.1">Proceedings of the IEEE/CVF conference on computer vision and pattern </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.434.1">recognition</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.435.1"> (2019).</span></span></li>
<li><span class="koboSpan" id="kobo.436.1">Prykhodko, Oleksii, et al. </span><span class="koboSpan" id="kobo.436.2">“</span><em class="italic"><span class="koboSpan" id="kobo.437.1">A de novo molecular generation method using latent vector based generative adversarial network</span></em><span class="koboSpan" id="kobo.438.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.439.1">Journal of Cheminformatics </span></em><span class="koboSpan" id="kobo.440.1">11.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">2019): 1-13.</span></span></li>
<li><span class="koboSpan" id="kobo.442.1">Sutton, Richard S., and Andrew G. </span><span class="koboSpan" id="kobo.442.2">Barto. </span><em class="italic"><span class="koboSpan" id="kobo.443.1">Reinforcement learning: An introduction</span></em><span class="koboSpan" id="kobo.444.1">. </span><span class="koboSpan" id="kobo.444.2">MIT </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">Press (2018).</span></span></li>
<li><span class="koboSpan" id="kobo.446.1">Kaelbling, Leslie Pack, Michael L. </span><span class="koboSpan" id="kobo.446.2">Littman, and Andrew W. </span><span class="koboSpan" id="kobo.446.3">Moore. </span><span class="koboSpan" id="kobo.446.4">“</span><em class="italic"><span class="koboSpan" id="kobo.447.1">Reinforcement learning: A survey</span></em><span class="koboSpan" id="kobo.448.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.449.1">Journal of artificial intelligence research</span></em><span class="koboSpan" id="kobo.450.1"> 4 (</span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">1996): 237-285.</span></span></li>
<li><span class="koboSpan" id="kobo.452.1">Watkins, Christopher John Cornish Hellaby. </span><em class="italic"><span class="koboSpan" id="kobo.453.1">Learning from delayed </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.454.1">rewards</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">. </span><span class="koboSpan" id="kobo.455.2">(1989).</span></span></li>
<li><span class="koboSpan" id="kobo.456.1">Mnih, Volodymyr, et al. </span><span class="koboSpan" id="kobo.456.2">“</span><em class="italic"><span class="koboSpan" id="kobo.457.1">Playing atari with deep reinforcement learning</span></em><span class="koboSpan" id="kobo.458.1">.” </span><span class="koboSpan" id="kobo.458.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">arXiv:1312.5602 (2013).</span></span></li>
<li><span class="koboSpan" id="kobo.460.1">Botvinick, Matthew, et al. </span><span class="koboSpan" id="kobo.460.2">“</span><em class="italic"><span class="koboSpan" id="kobo.461.1">Reinforcement learning, fast and slow</span></em><span class="koboSpan" id="kobo.462.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.463.1">Trends in cognitive sciences</span></em><span class="koboSpan" id="kobo.464.1"> 23.5 (</span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">2019): 408-422.</span></span></li>
<li><span class="koboSpan" id="kobo.466.1">Kolesnikov, Alexander, Xiaohua Zhai, and Lucas Beyer. </span><span class="koboSpan" id="kobo.466.2">“</span><em class="italic"><span class="koboSpan" id="kobo.467.1">Revisiting self-supervised visual representation learning</span></em><span class="koboSpan" id="kobo.468.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.469.1">Proceedings of the IEEE/CVF conference on computer vision and pattern </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.470.1">recognition</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.471.1"> (2019).</span></span></li>
<li><span class="koboSpan" id="kobo.472.1">Wang, Jiangliu, Jianbo Jiao, and Yun-Hui Liu. </span><span class="koboSpan" id="kobo.472.2">“</span><em class="italic"><span class="koboSpan" id="kobo.473.1">Self-supervised video representation learning by pace prediction</span></em><span class="koboSpan" id="kobo.474.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.475.1">Computer Vision–ECCV 2020: 16th European Conference</span></em><span class="koboSpan" id="kobo.476.1">, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16. </span><span class="koboSpan" id="kobo.476.2">Springer International </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">Publishing (2020).</span></span></li>
<li><span class="koboSpan" id="kobo.478.1">Jaiswal, Ashish, et al. </span><span class="koboSpan" id="kobo.478.2">“</span><em class="italic"><span class="koboSpan" id="kobo.479.1">A survey on contrastive self-supervised learning</span></em><span class="koboSpan" id="kobo.480.1">.” </span><em class="italic"><span class="koboSpan" id="kobo.481.1">Technologies</span></em><span class="koboSpan" id="kobo.482.1"> 9.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">2020): 2.</span></span></li>
<li><span class="koboSpan" id="kobo.484.1">Radford, Alec, et al. </span><span class="koboSpan" id="kobo.484.2">“</span><em class="italic"><span class="koboSpan" id="kobo.485.1">Language models are unsupervised multitask learners</span></em><span class="koboSpan" id="kobo.486.1">.” </span><span class="koboSpan" id="kobo.486.2">OpenAI blog 1.8 (</span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">2019): 9.</span></span></li>
<li><span class="koboSpan" id="kobo.488.1">Zhang, Chaoning, et al. </span><span class="koboSpan" id="kobo.488.2">“</span><em class="italic"><span class="koboSpan" id="kobo.489.1">A survey on masked autoencoder for self-supervised learning in vision and beyond</span></em><span class="koboSpan" id="kobo.490.1">.” </span><span class="koboSpan" id="kobo.490.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">arXiv:2208.00173 (2022).</span></span></li>
</ul>
</div>


<div class="Content" id="_idContainer136">
<h1 id="_idParaDest-243" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor405"/><span class="koboSpan" id="kobo.1.1">Part 5:Advanced Topics in Model Debugging</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In the concluding part of this book, we will address some of the most pivotal topics in machine learning. </span><span class="koboSpan" id="kobo.2.2">We will begin by explaining differences between correlation and causality, shedding light on their distinct implications in model development. </span><span class="koboSpan" id="kobo.2.3">Transitioning to the topic of security and privacy, we will discuss the pressing concerns, challenges, and techniques that ensure our models are both robust and respectful of user data. </span><span class="koboSpan" id="kobo.2.4">We will wrap up the book with an explanation of human-in-the -loop machine learning, emphasizing the synergy between human expertise and automated systems, and how this collaboration paves the way for more </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">effective solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B16369_15.xhtml#_idTextAnchor406"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 15</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Correlation versus Causality</span></em></li>
<li><a href="B16369_16.xhtml#_idTextAnchor429"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 16</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Security and Privacy in Machine Learning</span></em></li>
<li><a href="B16369_17.xhtml#_idTextAnchor447"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 17</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Human-in-the-Loop Machine Learning</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer137">
</div>
</div>
<div>
<div id="_idContainer138">
</div>
</div>
<div>
<div id="_idContainer139">
</div>
</div>
<div>
<div id="_idContainer140">
</div>
</div>
<div>
<div id="_idContainer141">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer142">
</div>
</div>
<div>
<div id="_idContainer143">
</div>
</div>
<div>
<div id="_idContainer144">
</div>
</div>
</body></html>