<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Libraries, Development Platform, and Datasets</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Libraries, Development Platform, and Datasets</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be&#160;setting up a development environment to help run codes for the book as well as for generic development and also introduce various datasets for computer vision. Since there are several standard libraries which are used both for studying computer vision and in the industry for deployment, it becomes trivial to also use them in learning path. As we study the various sub-topics of computer vision in further chapters, we will be able to directly implement the codes introduced then rather than getting stuck in installations and other library dependencies.</p>
<p>This chapter is divided into two major sections:</p>
<ul>
<li>Firstly we will be setting up python based environment such Anaconda</li>
<li>We will then setup OpenCV and various forms of its installations</li>
<li>For deep learning, we will also setup Keras and TensorFlow&#160;</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Libraries and installation</h1>
                </header>
            
            <article>
                
<p>Before we begin, it is required that we install each library. There are two major methods of installing a library:</p>
<ul>
<li>We download the source code and build binaries by compiling the code</li>
<li>We can directly download binaries and put them in relevant directories</li>
</ul>
<p>While downloading pre-built binaries is a faster method, however, due to the difference of platforms or non-availability of binaries may force to build a&#160;library from source. If readers are using different OS then the mentioned in the following sections, they might come across such a situation. Once installed a library, it can be used with programs or other libraries.&#160;</p>
<p>Since it <span>is crucial to have libraries that are not affected by other installations,</span>&#160;we will be using Python-based environments in most of the book. This helps in keeping track of libraries installed and also separates different environment if we would like to have multiple. Here environment refers to installed libraries with particular versions and their dependencies.</p>
<p>For building a library from source, we will use&#160;<kbd>CMake</kbd> tool. The instructions to install are as shown in further sections. This helps in building cross-platform software by linking to relevant compilers on each platform as well as to their dependencies. This comes with GUI too but for convenience, we will be using command-line <kbd>cmake</kbd>.</p>
<p>For deep learning, which we will see later in this book, a GPU is highly recommended. To run our programs using GPUs, we need to install both CUDA and cuDNN binaries provided by Nvidia. Further details of installation for each of the platforms, such as Linux, Mac OS, or Windows, are available from Nvidia.</p>
<p>Let's begin by installing the required packages in order.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing Anaconda</h1>
                </header>
            
            <article>
                
<p>The first thing we need to do is set up our Python environment such that rest of the libraries will be easily accessible through Python. Anaconda is a popular data science platform with a Python interface and is available here: <a href="https://www.anaconda.com/" target="_blank">https://www.anaconda.com/</a>. It has <kbd>conda</kbd> as a package manager, which can install, delete, and manage versions of Python libraries while keeping it isolated from other Python environments. In this book, we will use <kbd>conda</kbd> from Anaconda. Let's go ahead and set this up.&#160;</p>
<p>First, download and install&#160;<span>Anaconda:</span></p>
<ul>
<li>On Linux:</li>
</ul>
<pre style="padding-left: 60px"><strong>wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh</strong><br/><strong>bash Anaconda3-5.0.1-MacOSX-x86_64.sh</strong></pre>
<ul>
<li>On macOS, <kbd>wget</kbd> is not directly available; use brew to install <kbd>wget</kbd>:</li>
</ul>
<pre style="padding-left: 60px"><strong>wget https://repo.continuum.io/archive/Anaconda3-5.0.1-MacOSX-x86_64.sh</strong><br/><strong>bash Anaconda3-5.0.1-MacOSX-x86_64.sh</strong></pre>
<p>This will install the Python libraries in the folder <kbd>$HOME/anaconda3</kbd>, since we are using Python 3. A Python 2 version is also available and the installation process is similar. To use Anaconda, the newly installed libraries need to be added in <kbd>$PATH</kbd>, this can be done every time a new shell is launched by running:&#160;</p>
<pre><strong>export PATH="$PATH_TO_ANACONDA3/anaconda3/bin:$PATH"</strong> </pre>
<p><kbd>$PATH_TO_ANACONDA3</kbd> is the location path to the <kbd>Anaconda3</kbd> folder. For more convenience, add this to <kbd>.bashrc</kbd> or <kbd>.bash_profile</kbd> depending on if you are using Linux or macOS respectively.&#160;</p>
<p>Once conda is installed, many other scientific packages will also be installed. Some of these packages are:</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">NumPy</h1>
                </header>
            
            <article>
                
<p>NumPy package is used for performing operations on images as N-dimensional arrays. An example to create and transpose a two-dimensional array is as follows:</p>
<pre><strong>import numpy as np </strong><br/><br/><strong>A = [[1, 2],[3, 4]]</strong><br/><br/><strong># transpose A </strong><br/><strong>np.transpose(A)</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">
Matplotlib</h1>
                </header>
            
            <article>
                
<p>This is a popular Python package for plotting and displaying data and images. To use in Python, the scripts is as follows:</p>
<pre><strong>import matplotlib.pyplot as plt</strong> </pre>
<p>If we want to plot inside Jupyter notebook, add the following command:</p>
<pre>%matplotlib inline </pre>
<p>An example function to display an image is as follows:</p>
<pre>def plot_img(input_image): <br/>    """ <br/>    Takes in image <br/>    Plots image using matplotlib<br/>    """ <br/>    plt.figure(figsize=(12,8))<br/>    <br/>    # change color channels order for matplotlib <br/>    plt.imshow(input_image)<br/><br/>    # For easier view, turn off axis around image <br/>    plt.axis('off')<br/>    plt.show()</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">SciPy</h1>
                </header>
            
            <article>
                
<p>This is a Python based scientific computing library and contains several advanced algorithms for optimization, linear algebra, signal processing, statistics, and so on. &#160;</p>
<p>An example to compute eigen values and eigen vectors of a two-dimensional array is as follows:</p>
<pre><span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">linalg<br/><br/>A = [[5, 6], [7, 8]]<br/>eig_vals, eig_vectors = linalg.eig(A)</span></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Jupyter notebook</h1>
                </header>
            
            <article>
                
<p>Jupyter notebook is popularly used for creating step by step live codes with visualizations and texts. In <a href="prac-cv_ch03.html" target="_blank">Chapter 3</a>,&#160;<em>Image Filtering and Transformations in OpenCV</em> and <a href="prac-cv_ch04.html" target="_blank">Chapter 4</a>,&#160;<em>What is a Feature?,&#160;</em>the codes for image filtering and feature extraction can be used with Jupyter notebook.</p>
<p>To launch a notebook server, run the following in shell:</p>
<pre>jupyter notebook</pre>
<p>&#160;</p>
<p>This will start the browser and we can see the files inside the folder from where it is launched. After launching, click on <span class="packt_screen">New</span> on top left side on the browser page and select the notebook with desired Python. A new tab in the browser will open with Python interpreter format.</p>
<p>Other packages such as scikit-learn, pandas, seaborn, and so on.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing OpenCV</h1>
                </header>
            
            <article>
                
<p>OpenCV (available at <a href="https://opencv.org/" target="_blank">https://opencv.org/</a> ) is the most popular computer vision open source library and can be installed on all major platforms including Linux, macOS, Windows, Android, iOS, and so on. It contains optimized code written in C++ and has binding for Python and Java. Considering the versatility of OpenCV, we will be using it to explain computer vision algorithms. Most of the code in this book is in Python, except for external repositories. OpenCV can be set up in two ways depending on how we will use it. We will begin with the easy way.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">OpenCV Anaconda installation&#160;</h1>
                </header>
            
            <article>
                
<p>Using Anaconda, which we installed in the previous section, OpenCV can be installed on both Linux and macOS as follows (this is OpenCV with only the Python library):</p>
<pre><strong>conda install -c conda-forge opencv</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">OpenCV build from source</h1>
                </header>
            
            <article>
                
<p>Building OpenCV from source is quite a long process, depending on the hardware you are using:&#160;</p>
<ul>
<li>Requirements on Linux (here Ubuntu):</li>
</ul>
<pre style="padding-left: 60px"><strong>sudo apt-get install build-essential</strong><br/><strong>sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev</strong><br/><strong>sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</strong></pre>
<ul>
<li>Requirements on macOS:
<ul>
<li>Install CMake from&#160;<a href="http://www.cmake.org/download/">http://www.cmake.org/download/&#160;</a></li>
</ul>
</li>
</ul>
<p>The following is an install script; copy the following snippet to install the&#160;<kbd>install.sh</kbd> file, and run <kbd>bash install.sh</kbd><em>&#160;</em>to install OpenCV.</p>
<p>In the following code, replace <kbd>$PATH_TO_ANACONDA</kbd> with the absolute path to Anaconda, such as&#160;<span><kbd>/Users/mac</kbd>:</span></p>
<pre><strong># download opencv </strong><br/><strong>wget https://github.com/opencv/opencv/archive/3.3.0.zip</strong><br/><strong>unzip 3.3.0.zip</strong><br/><strong>mv opencv-3.3.0 opencv</strong><br/><strong>rm -rf 3.3.0.zip</strong><br/><br/><br/><strong># install opencv </strong><br/><strong>cd opencv </strong><br/><strong>mkdir build &amp;&amp; cd build</strong><br/><strong>cmake -D -DPYTHON_INCLUDE_DIR=$PATH_TO_ANACONDA/anaconda3/include/python3.6m/ \</strong><br/><strong>    -DPYTHON_EXECUTABLE=$PATH_TO_ANACONDA/anaconda3/bin/python \</strong><br/><strong>    -DPYTHON_PACKAGES_PATH=$PATH_TO_ANACONDA/anaconda3/lib/python3.6/site-packages \</strong><br/><strong>    -DINSTALL_PYTHON_EXAMPLES=ON \</strong><br/><strong>    -DCMAKE_INSTALL_PREFIX=$PATH_TO_ANACONDA/anaconda3 \</strong><br/><strong>    -DWITH_QT=ON \</strong><br/><strong>    -DFORCE_VTK=ON \</strong><br/><strong>    -DWITH_GDAL=ON \</strong><br/><strong>    -DWITH_FFMPEG=ON \</strong><br/><strong>    -DWITH_TBB=ON \</strong><br/><strong>    -DWITH_XINE=ON \</strong><br/><strong>    -DWITH_OPENCL=OFF \</strong><br/><strong>    -DBUILD_EXAMPLES=ON ..</strong><br/><br/><strong>make -j4</strong><br/><strong>make install</strong><br/><br/></pre>
<p class="mce-root">Since there are significant changes between OpenCV2 and OpenCV3, the code in this book is written using only OpenCV3.</p>
<p class="mce-root">In <kbd>OpenCV</kbd>, extra contributed modules are moved to a separate repository under the name <kbd>opencv_contrib</kbd>. In order to build <kbd>OpenCV</kbd> including with <kbd>opencv_contrib</kbd> , the steps are as follows:</p>
<ul>
<li>Download OpenCV as :</li>
</ul>
<pre style="padding-left: 60px"><strong># download opencv </strong><br/><strong>wget https://github.com/opencv/opencv/archive/3.3.0.zip</strong><br/><strong>unzip 3.3.0.zip</strong><br/><strong>mv opencv-3.3.0 opencv</strong><br/><strong>rm -rf 3.3.0.zip</strong></pre>
<ul>
<li>Download the extra module here, and note the path to this folder:</li>
</ul>
<pre style="padding-left: 60px"><strong># opencv contrib code </strong><br/><strong>wget https://github.com/opencv/opencv_contrib/archive/3.3.0.zip</strong><br/><strong>unzip 3.3.0.zip</strong><br/><strong>mv opencv_contrib-3.3.0 opencv_contrib</strong><br/><strong>rm -rf 3.3.0.zip</strong></pre>
<ul>
<li>Build a complete OpenCV again, as follows, where <kbd>PATH_TO_CONTRIB</kbd> is the path to the previously downloaded <kbd>opencv_contrib</kbd> path:</li>
</ul>
<pre style="padding-left: 60px" class="mce-root"><strong>cd opencv </strong><br/><strong>mkdir build &amp;&amp; cd build</strong><br/><strong>cmake -D -DOPENCV_EXTRA_MODULES_PATH=$PATH_TO_CONTRIB/opencv_contrib/modules \</strong><br/><strong>    -DPYTHON_INCLUDE_DIR=$PATH_TO_ANACONDA/anaconda3/include/python3.6m/ \</strong><br/><strong>    -DPYTHON_EXECUTABLE=$PATH_TO_ANACONDA/anaconda3/bin/python \</strong><br/><strong>    -DPYTHON_PACKAGES_PATH=$PATH_TO_ANACONDA/anaconda3/lib/python3.6/site-packages \</strong><br/><strong>    -DINSTALL_PYTHON_EXAMPLES=ON \</strong><br/><strong>    -DCMAKE_INSTALL_PREFIX=$PATH_TO_ANACONDA/anaconda3 \</strong><br/><strong>    -DWITH_QT=ON \</strong><br/><strong>    -DFORCE_VTK=ON \</strong><br/><strong>    -DWITH_GDAL=ON \</strong><br/><strong>    -DWITH_FFMPEG=ON \</strong><br/><strong>    -DWITH_TBB=ON \</strong><br/><strong>    -DWITH_XINE=ON \</strong><br/><strong>    -DWITH_OPENCL=OFF \</strong><br/><strong>    -DBUILD_EXAMPLES=ON ..</strong><br/><br/><strong>make -j4</strong><br/><strong>make install</strong></pre>
<p>Here, we see that there are several options which are set on or off. The choice of these operations depends on the availability of the dependencies. These can be set to on if all of the dependencies are available.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Opencv FAQs</h1>
                </header>
            
            <article>
                
<p>Though we saw an introductory OpenCV programs in previous chapter, we will see some more frequently used code snippets that will be used throughout this book.</p>
<ul>
<li class="mce-root">Let's begin with importing OpenCV and will print the version of OpenCV used:</li>
</ul>
<pre style="padding-left: 60px"><strong>import cv2</strong><br/><strong>print(cv2.__version__)</strong></pre>
<ul>
<li>We can read an image from a file as:&#160;</li>
</ul>
<pre style="padding-left: 60px"><strong> img = cv2.imread('flower.png')</strong></pre>
<p>The previous snippet will decode an image stored in common formats such as <kbd>.jpg</kbd>, &#160;<kbd>.png</kbd> , <kbd>.jpeg</kbd> , <kbd>.tiff</kbd> , <kbd>.pgm</kbd>, and so on. using image codecs either installed with OpenCV or available on the platform. If there are no codecs available, then OpenCV will not be able to read image or write image to a file. So, it is necessary for the user to install codecs on a non-supported platforms such as embedded devices.</p>
<p>We can write an image to file as:</p>
<pre><strong>cv2.imwrite('image.png', img)</strong></pre>
<p>In writing a file also there is need for image codecs which are generally installed with OpenCV. We can write the image with file formats such as JPG, PNG, JPEG, TIFF, and so on.&#160;</p>
<p>Processing a video includes opening a video file and applying algorithms on each frame. We will first initialize the source of frames which can be a video file or an attached USB camera as:&#160;</p>
<pre><strong># to use default usb camera set the value to 0</strong><br/><strong>video_capture = cv2.VideoCapture(0)</strong></pre>
<p>Or we can also write it as follows:</p>
<pre><strong># to use video file, set filename </strong><br/><strong>video_capture = cv2.VideoCapture('video.avi')</strong></pre>
<p>Similar to image reading and writing, video reading will also require codecs which are installed with OpenCV or available from the OS. Once the source is setup we can continue processing each frame as:</p>
<pre><strong>while(True):</strong><br/><strong>    # get each frame</strong><br/><strong>    ret, frame = video_capture.read()</strong><br/>    <br/><strong>    # if no frame available then quit</strong><br/><strong>    if not ret:</strong><br/><strong>        print("Frame not available")</strong><br/><strong>        break</strong><br/>    <br/><strong>    # show read frame in window</strong><br/><strong>    cv2.imshow('frame', frame)</strong><br/><br/><strong>    # escape the loop on pressing 'q'</strong><br/><strong>    if cv2.waitKey(1) &amp; 0xFF == ord('q'):</strong><br/><strong>        break</strong></pre>
<p>Here <kbd>cv2.imshow</kbd> is to display image and <kbd>cv2.waitKey()</kbd> is time delay in the execution.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">TensorFlow for deep learning</h1>
                </header>
            
            <article>
                
<p>TensorFlow is one of the popular deep learning libraries available and has APIs for Python, C++, Java, and so on. In this book, we will use the Python API&#160;<span>1.4.0</span>. Explaining TensorFlow in detail is beyond the scope of this book; the official documentation is a better starting place to get acquainted with it.&#160;</p>
<p>In order to install, we will use the&#160;<kbd>pip</kbd> based method, as follows:</p>
<pre><strong>pip install tensorflow=1.4.0</strong></pre>
<p>If there is GPU available with CUDA and cuDNN:</p>
<pre><strong>pip install tensorflow-gpu=1.4.0</strong></pre>
<p>For more information on TensorFlow and its use, please follow the tutorials here:&#160;</p>
<p><a href="https://www.tensorflow.org/get_started/get_started">https://www.tensorflow.org/get_started/get_started</a>.</p>
<p>Once installed, TensorFlow version can be checked by running:</p>
<pre><strong>python -c "import tensorflow as tf;print(tf.__version__)"</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Keras for deep learning&#160;</h1>
                </header>
            
            <article>
                
<p>Keras is a Python based API that uses TensorFlow, CNTK, or Theano as backend for deep learning. Due to its high level API and simplified abstraction, it has been quite popular in the deep learning&#160;community. We will be using this library to study CNNs. To install this, first install TensorFlow as described in previous section, and use the following:</p>
<pre><strong>pip install keras</strong></pre>
<p>There is no separate version for GPU. For installing specific versions of Keras, such as Version 2.1.2, use following:&#160;</p>
<pre><strong>pip install keras==2.1.2</strong></pre>
<p>The latest version of Keras at the time of writing this book is 2.1.2. To check the version of installed Keras, use:</p>
<pre><strong>python -c "import keras;print(keras.__version__)"</strong></pre>
<p>If TensorFlow is installed from previous sections, it will use it as backend.</p>
<p>To use Keras, one of the prerequisites is basic knowledge of deep learning. In this book, we will see it in <a href="prac-cv_ch05.html" target="_blank">Chapter 5</a>, <em>Convolutional Neural Networks</em>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets</h1>
                </header>
            
            <article>
                
<p>In computer vision, datasets play a key role in developing efficient applications. Also, now, with the availability of large open source datasets, it has become much easier to create best performing models for computer vision tasks. In this section, we will see several datasets for computer vision.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">ImageNet</h1>
                </header>
            
            <article>
                
<p>ImageNet is one of the largest annotated datasets for computer vision. The data is arranged according to a hierarchical order. There are 1,000 classes with 1.4 million images overall. Though the images are for non-commercial use, ImageNet is still one of the most popular datasets when it comes to learning computer vision. Especially in deep learning, the dataset is used to create image classification models due to availability of large number of varied images.&#160;</p>
<p>The following website provides links and resources to download image URLs or other attributes about images:</p>
<p><a href="http://image-net.org/download">http://image-net.org/download</a></p>
<p>In this book, ImageNet is not used explicitly, but we will be using a pre-trained model on it. There is no requirement to download this dataset for this book.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MNIST</h1>
                </header>
            
            <article>
                
<p><kbd>MNIST</kbd> is a dataset for handwritten digits with the numbers 0-9 with 60,000 images of size 28 x 28 as the training set and 10,000 images of size 28 x 28 as the test set. This has become the go to dataset for starting machine learning or deep learning. It is provided in most of the frameworks and there is no need to download it separately. In <kbd>Keras</kbd>, this can be used as follows:</p>
<pre><strong>from __future__ import print_function</strong><br/><br/><strong>from keras.datasets import mnist</strong><br/><strong>import matplotlib.pyplot as plt </strong><br/><br/><strong># Download and load dataset </strong><br/><strong>(x_train, y_train), (x_test, y_test) = mnist.load_data()</strong><br/><br/><strong># to know the size of data</strong><br/><strong>print("Train data shape:", x_train.shape, "Test data shape:", x_test.shape)</strong><br/><br/><strong># plot sample image</strong><br/><strong>idx = 0</strong><br/><strong>print("Label:",y_train[idx])</strong><br/><strong>plt.imshow(x_train[idx], cmap='gray')</strong><br/><strong>plt.axis('off')</strong><br/><strong>plt.show()</strong></pre>
<p>Some of the sample images from this dataset are as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="116" width="321" class="alignnone size-full wp-image-375 image-border" src="images/523e4aa9-ab14-4cc0-ba57-32a23a0f0c80.png"/></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CIFAR-10</h1>
                </header>
            
            <article>
                
<p>Though <kbd>MNIST</kbd> is one of the easiest datasets to get started, the lack of color images makes it less appealing for tasks that require a colored dataset. A slight more complex dataset is <kbd>CIFAR-10</kbd>&#160;by Alex and others[1], which consists of 10 categories of images with 60,000 training images and 10,000 test images, uniformly from each category. The size of each image is 32 x 32 and each has three color channels. This dataset can also be easily loaded in Keras, as follows:</p>
<pre><strong>from __future__ import print_function</strong><br/><br/><strong>from keras.datasets import cifar10</strong><br/><strong>import matplotlib.pyplot as plt </strong><br/><br/><strong># Download and load dataset </strong><br/><strong>(x_train, y_train), (x_test, y_test) = cifar10.load_data()</strong><br/><strong>labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']</strong><br/><strong># to know the size of data</strong><br/><strong>print("Train data shape:", x_train.shape, "Test data shape:", x_test.shape)</strong><br/><br/><strong># plot sample image</strong><br/><strong>idx = 1500</strong><br/><strong>print("Label:",labels[y_train[idx][0]])</strong><br/><strong>plt.imshow(x_train[idx])</strong><br/><strong>plt.axis('off')</strong><br/><strong>plt.show()</strong></pre>
<p>The labels, in order, are:&#160;<kbd>airplane</kbd>, <kbd>automobile</kbd>, <kbd>bird</kbd>, <kbd>cat</kbd>, <kbd>deer</kbd>, <kbd>dog</kbd>, <kbd>frog</kbd>, <kbd>horse</kbd>, <kbd>ship</kbd>, and&#160;<kbd>truck</kbd>.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pascal VOC</h1>
                </header>
            
            <article>
                
<p>As previous datasets like <kbd>MNIST</kbd> and <kbd>CIFAR</kbd> are limited in representation, we cannot use them for tasks like people detection or segmentation. Pascal VOC[4] has gained in popularity for such tasks as one of the major datasets for object recognition. During 2005-2012, there were competitions conducted that used this dataset and achieved the best possible accuracy on test data. The dataset is also usually referred to by year; for example, VOC2012 refers to the dataset available for the 2012 competition. In VOC2012, there are three competition categories. The first is the classification and detection dataset, which has 20 categories of objects along with rectangular region annotations around the objects. The second category is Segmentation with instance boundaries around objects. The third competition category is for action recognition from images.&#160;</p>
<p>This dataset can be downloaded from the following link:</p>
<p><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html" target="_blank">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</a>.</p>
<p>In this dataset, a sample annotation file (in XML format) for an image is in the following code, where the tags represent properties of that field:&#160;</p>
<pre>&lt;annotation&gt;<br/>  &lt;folder&gt;VOC2012&lt;/folder&gt;<br/>  &lt;filename&gt;2007_000033.jpg&lt;/filename&gt;<br/>  &lt;source&gt;<br/>    &lt;database&gt;The VOC2007 Database&lt;/database&gt;<br/>    &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;<br/>    &lt;image&gt;flickr&lt;/image&gt;<br/>  &lt;/source&gt;<br/>  &lt;size&gt;<br/>    &lt;width&gt;500&lt;/width&gt;<br/>    &lt;height&gt;366&lt;/height&gt;<br/>    &lt;depth&gt;3&lt;/depth&gt;<br/>  &lt;/size&gt;<br/>  &lt;segmented&gt;1&lt;/segmented&gt;<br/>  &lt;object&gt;<br/>    &lt;name&gt;aeroplane&lt;/name&gt;<br/>    &lt;pose&gt;Unspecified&lt;/pose&gt;<br/>    &lt;truncated&gt;0&lt;/truncated&gt;<br/>    &lt;difficult&gt;0&lt;/difficult&gt;<br/>    &lt;bndbox&gt;<br/>      &lt;xmin&gt;9&lt;/xmin&gt;<br/>      &lt;ymin&gt;107&lt;/ymin&gt;<br/>      &lt;xmax&gt;499&lt;/xmax&gt;<br/>      &lt;ymax&gt;263&lt;/ymax&gt;<br/>    &lt;/bndbox&gt;<br/>  &lt;/object&gt;<br/>  &lt;object&gt;<br/>    &lt;name&gt;aeroplane&lt;/name&gt;<br/>    &lt;pose&gt;Left&lt;/pose&gt;<br/>    &lt;truncated&gt;0&lt;/truncated&gt;<br/>    &lt;difficult&gt;0&lt;/difficult&gt;<br/>    &lt;bndbox&gt;<br/>      &lt;xmin&gt;421&lt;/xmin&gt;<br/>      &lt;ymin&gt;200&lt;/ymin&gt;<br/>      &lt;xmax&gt;482&lt;/xmax&gt;<br/>      &lt;ymax&gt;226&lt;/ymax&gt;<br/>    &lt;/bndbox&gt;<br/>  &lt;/object&gt;<br/>  &lt;object&gt;<br/>    &lt;name&gt;aeroplane&lt;/name&gt;<br/>    &lt;pose&gt;Left&lt;/pose&gt;<br/>    &lt;truncated&gt;1&lt;/truncated&gt;<br/>    &lt;difficult&gt;0&lt;/difficult&gt;<br/>    &lt;bndbox&gt;<br/>      &lt;xmin&gt;325&lt;/xmin&gt;<br/>      &lt;ymin&gt;188&lt;/ymin&gt;<br/>      &lt;xmax&gt;411&lt;/xmax&gt;<br/>      &lt;ymax&amp;amp;gt;223&lt;/ymax&gt;<br/>    &lt;/bndbox&gt;<br/>  &lt;/object&gt;<br/>&lt;/annotation&gt;</pre>
<p>The corresponding image is as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="323" width="441" src="images/ac8e0f1a-4309-4168-9600-e7ca110d227c.jpg"/></div>
<p class="mce-root">The available categories in this dataset are&#160;aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, and TV.&#160;</p>
<p>The number of categories is, however, limited. In the next section, we will see a more elaborate dataset with 80 categories. Having a higher number of generic object categories will help in creating applications that can be used easily in more generic scenarios.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MSCOCO</h1>
                </header>
            
            <article>
                
<p>COCO[2] refers to a common object in context and is a dataset for object recognition, with 80 categories and 330K images. After Pascal VOC'12, this became a popular benchmark for training and evaluating the system. The dataset can be downloaded from&#160;<a href="http://cocodataset.org/#download" target="_blank">http://cocodataset.org/#download</a>.&#160;</p>
<p>In order to read the data and use it for applications, there is an API available at&#160;<a href="https://github.com/cocodataset/cocoapi" target="_blank">https://github.com/cocodataset/cocoapi</a> which needs to be downloaded. &#160;To get started, we can use the API provided, as follows:</p>
<pre><strong>git clone https://github.com/cocodataset/cocoapi.git</strong><br/><strong>cd cocoapi/PythonAPI</strong><br/><strong>make</strong></pre>
<p>This will install the Python API to read the&#160;<kbd>coco</kbd> dataset.&#160;</p>
<p><span>Many models available online for object detection or image segmentation are first trained on this dataset. If we have specific data that has different object categories than in the MSCOCO dataset, a more common approach that we will see in <a href="prac-cv_ch05.html" target="_blank">Chapter 5</a>, <em>Convolution Neural Networks</em> and in <a href="prac-cv_ch06.html" target="_blank">Chapter 6</a>, <em>Feature- Based Object Detection</em>, is to first train a model on an MSCOCO dataset and use a part of the trained model and re-train on a new dataset.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">TUM RGB-D dataset</h1>
                </header>
            
            <article>
                
<p>While previous datasets were used for object recognition, this dataset is used to understand the geometry of a scene. The&#160;RGB-D dataset[3] has been popular in SLAM research and was a benchmark for comparison too. Here, RGB-D refers to a dataset with both <strong>RGB</strong> (color) images and <strong>Depth</strong> images. The depth here refers to distance of pixel from camera and are taken using a depth camera. Since there is also depth information available, this dataset can also be used to evaluate depth based SLAM algorithms and three-dimensional reconstructions from RGB image and its corresponding depth image.</p>
<p>To download this dataset, visit&#160;<a href="https://vision.in.tum.de/data/datasets/rgbd-dataset/download" target="_blank">https://vision.in.tum.de/data/datasets/rgbd-dataset/download</a>&#160;and choose the type of sequence to use.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to install the different library files of Python, Keras, and TensorFlow. In order to use several code snippets in further chapters, these libraries will be sufficient. We also had a look at different datasets like ImageNet, <kbd>MNIST</kbd>, <kbd>CIFAR-10</kbd>, MSCOCO and TUM RGBD datasets. These datasets are the backbone for computer vision applications since the ability of several software that we develop directly depends on the availability of these datasets.&#160;</p>
<p>In next chapter, we will begin with more in-depth image analysis by introducing different types of filters and also learn transformations on image such as translation, rotation or affine.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Krizhevsky, Alex, and Geoffrey Hinton. <em>Learning multiple layers of features from tiny images</em>. (2009).</span></li>
<li><span>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. <em>Microsoft coco: Common objects in context</em>. In&#160;</span>European conference on computer vision<span>, pp. 740-755. Springer, Cham, 2014.</span></li>
<li><span>Sturm, Jürgen, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. <em>A benchmark for the evaluation of RGB-D SLAM systems</em>.</span> In&#160;Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 573-580. IEEE, 2012.</li>
<li>Everingham Mark, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. <em>The pascal visual object classes (voc) challenge</em>. International journal of computer vision 88, no. 2 (2010): 303-338.</li>
</ul>


            </article>

            
        </section>
    </div>
</body>
</html>