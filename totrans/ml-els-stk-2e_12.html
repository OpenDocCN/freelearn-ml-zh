<html><head></head><body>
		<div id="_idContainer239">
			<h1 id="_idParaDest-160"><em class="italic"><a id="_idTextAnchor162"/>Chapter 9</em>: Introducing Data Frame Analytics</h1>
			<p>In the first section of this book, we took an in-depth tour of anomaly detection, the first machine learning capability to be directly integrated into the Elastic Stack. In this chapter and the following one, we will take a dive into the new machine learning features integrated into the stack. These include outlier detection, a novel unsupervised learning technique for detecting unusual data points in non-timeseries indices, as well as two supervised learning features, classification and regression. </p>
			<p>Supervised learning algorithms use labeled datasets – for example, a dataset describing various aspects of tissue samples along with whether or not the tissue is malignant – to learn a model. This model can then be used to make predictions on previously unseen data points (or tissue samples, to continue our example). When the target of prediction is a discrete variable or a category such as a malignant or non-malignant tissue sample, the supervised learning technique is called classification. When the target is a continuous numeric variable, such as the sale price of an apartment or the hourly price of electricity, the supervised learning technique is known as regression. Collectively, these three new machine learning features are known as <strong class="bold">Data Frame Analytics</strong>. We will discuss each of these in more depth in the following chapters.</p>
			<p>Although each of these solves a different problem and has a different purpose, they are all powered under the hood by a common data transformation technology, that of transforms, which enables us to transform and aggregate data from a transaction- or stream-based format into an entity-based format. This entity-centric format is required by many of the algorithms we use in Data Frame Analytics and thus, before we dive deeper into each of the new machine learning features, we are going to dedicate this chapter to understanding in depth how to use transforms to transform our data into a format that is more amenable for downstream machine learning technologies. While on this journey, we are also going to take a brief tour of Painless, the scripting language embedded into Elasticsearch, which is a good tool for any data scientists or engineers working with machine learning in the Elastic Stack.</p>
			<p>A rich ecosystem of libraries, both for data manipulation and machine learning, exists outside of the Elastic Stack as well. One of the main drivers powering these applications is Python. Because of its ubiquity in the data science and data engineering communities, we are going to focus, in the second part of this chapter, on using Python with the Elastic Stack, with a particular focus on the new data-science native Elasticsearch client, Eland. We'll check out the following topics in this chapter:</p>
			<ul>
				<li>Learning to use transforms </li>
				<li>Using Painless for advanced transform configurations</li>
				<li>Working with Python and Elasticsearch</li>
			</ul>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor163"/>Technical requirements</h1>
			<p>The material in this chapter requires Elasticsearch version 7.9 or above and Python 3.7 or above. Code samples and snippets required for this chapter will be added under the folder <strong class="source-inline">Chapter 9 - Introduction to Data Frame Analytics</strong> in the book's GitHub repository (<a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics</a>). In such cases where some examples require a specific newer release of Elasticsearch, this will be mentioned before the example is presented.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor164"/>Learning how to use transforms</h1>
			<p>In this<a id="_idIndexMarker583"/> section, we are going to dive right into the world of transforming stream or event-based data, such as logs, into an entity-centric index. </p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor165"/>Why are transforms useful?</h2>
			<p>Think about the <a id="_idIndexMarker584"/>most common data types that are ingested into Elasticsearch. These will often be documents recording some kind of time-based or sequential event, for example, logs from a web server, customer purchases from a web store, comments published on a social media platform, and so forth. </p>
			<p>While this kind of data is useful for understanding the behavior of our systems over time and is perfect for use with technologies such as anomaly detection, it is harder to make stream- or event-based <a id="_idIndexMarker585"/>datasets work with <strong class="bold">Data Frame Analytics</strong> features without first aggregating or transforming them in some way. For example, consider an e-commerce store that records purchases made by customers. Over a year, there may be tens or hundreds of transactions for each customer. If the e-commerce store then wants to find a way to use outlier detection to detect unusual customers, they will have to transform all of the transaction data points for each customer and summarize certain key metrics such as the average amount of money spent per purchase or number of purchases per calendar month. In <em class="italic">Figure 9.1</em>, we have a simplified illustration that depicts the process of taking transaction records from e-commerce purchases made by two customers and converting them into an entity-centric index that describes the total quantity of items <a id="_idIndexMarker586"/>purchased by these customers, as well as the average price paid per order. </p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B17040_09_001.jpg" alt="Figure 9.1 – A diagram illustrating the process of taking e-commerce transactions and transforming them into an entity-centric index &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – A diagram illustrating the process of taking e-commerce transactions and transforming them into an entity-centric index </p>
			<p>In order to perform the transformation depicted in <em class="italic">Figure 9.1</em>, we have to group each of the documents in the transaction index by the name of the customer and then perform two computations: sum up the quantity of items in each transaction document to get a total sum and also compute the average price of purchases for each customer. Doing this manually for all of the transactions for each of the thousands of potential customers would be extremely<a id="_idIndexMarker587"/> arduous, which is where <strong class="bold">transforms</strong> come in. </p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor166"/>The anatomy of a transform</h2>
			<p>Although we<a id="_idIndexMarker588"/> are going to start off our journey into transforms with simple examples, many real-life use cases can very quickly get complicated. It is useful to keep in mind two things that will help you keep your bearing as you apply<a id="_idIndexMarker589"/> transforms <a id="_idIndexMarker590"/>to your own data projects: the <strong class="bold">pivot</strong> and the <strong class="bold">aggregations</strong>. </p>
			<p>Let's examine how these two entities complement each other to help us transform a stream-based document into an entity-centric index. In our customer analytics use case, we have many different features describing each customer: the name of the customer, the total price they paid for each of their products at checkout, the list of items they purchased, the date of the purchase, the location of the customer, and so forth. </p>
			<p>The first thing we want to pick is the entity for which we are going to construct our entity-centric index. Let's start with a very simple example and say that our goal is to find out how much each customer spent on average per purchase during our time period and how much they spent in total. Thus, the entity we want to construct the index for – our pivot – is the name of the customer. </p>
			<p>Most of the customers in our source index have more than one transaction associated with them. Therefore, if we try to group our index by customer name, for each customer we will have multiple documents. In order to pivot successfully using this entity, we need to decide which aggregate quantities (for example, the average price per order paid by the customer) we want to bring into our entity-centric index. This will, in turn, determine which aggregations we will define in our transform configuration. Let's take a look at how this works out with a practical example. </p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor167"/>Using transforms to analyze e-commerce orders</h2>
			<p>In this <a id="_idIndexMarker591"/>section, we will use the Kibana E-Commerce <a id="_idIndexMarker592"/>sample dataset to illustrate some of the basic transformation concepts outlined in the preceding section: </p>
			<ol>
				<li>Import the <strong class="bold">Sample eCommerce orders</strong> dataset from the Kibana <strong class="bold">Sample data</strong> panel displayed in <em class="italic">Figure 9.2</em> by clicking the <strong class="bold">Add data</strong> button. This will create a new index called <strong class="source-inline">kibana_sample_data_ecommerce</strong> and populate it with the dataset. <div id="_idContainer209" class="IMG---Figure"><img src="image/B17040_09_002.jpg" alt="Figure 9.2 – Import the Sample eCommerce orders dataset from the Kibana Sample data panel&#13;&#10;"/></div><p class="figure-caption">Figure 9.2 – Import the Sample eCommerce orders dataset from the Kibana Sample data panel</p></li>
				<li>Navigate <a id="_idIndexMarker593"/>to the <strong class="bold">Transforms</strong> wizard by <a id="_idIndexMarker594"/>bringing up the Kibana slide-out panel menu from the hamburger button in the top left-hand corner, navigating to <strong class="bold">Stack Management</strong>, and then clicking <strong class="bold">Transforms</strong> under the <strong class="bold">Data</strong> menu. </li>
				<li>In the <strong class="bold">Transforms</strong> view, click <strong class="bold">Create your first transform</strong> to bring up the <strong class="bold">Transforms</strong> wizard. This will prompt you to choose a source index – this is the index that the transform will use to create your pivot and aggregations. In our case, we are interested in the <strong class="source-inline">kibana_sample_data_ecommerce</strong> index, which you should select in the panel shown in <em class="italic">Figure 9.3</em>. The source indices displayed in your Kibana may look a bit different depending on the indices currently available in your Elasticsearch cluster.<div id="_idContainer210" class="IMG---Figure"><img src="image/B17040_09_003.jpg" alt="Figure 9.3 – For this tutorial, please select kibana_sample_data_ecommerce &#13;&#10;"/></div><p class="figure-caption">Figure 9.3 – For this tutorial, please select kibana_sample_data_ecommerce </p></li>
				<li>After selecting<a id="_idIndexMarker595"/> our source index, the <strong class="bold">Transform</strong> wizard <a id="_idIndexMarker596"/>will open a dialog that shows us a preview of our source data (<em class="italic">Figure 9.4</em>), as well as allowing us to select our pivot entity using the drop-down selector under <strong class="bold">Group by</strong>. In this case, we want to pivot on the field named <strong class="source-inline">customer_full_name</strong>.<div id="_idContainer211" class="IMG---Figure"><img src="image/B17040_09_4.jpg" alt="Figure 9.4 – Select the entity you want to pivot your source index by in the Group by menu &#13;&#10;"/></div><p class="figure-caption">Figure 9.4 – Select the entity you want to pivot your source index by in the Group by menu </p></li>
				<li>Now that we have defined the entity to pivot our index by, we will move on to the next part in the construction of a transform: the aggregations. In this case, we are interested in figuring out the average amount of money the customer spent in the e-commerce store per order. During each transaction, which is recorded in a document in the source index, the total amount paid by the customer is stored in the field <strong class="source-inline">taxful_total_price</strong>. <p>Therefore, the aggregation that we define will operate on this field. In the <strong class="bold">Aggregations</strong> menu, select <strong class="bold">taxful_total_price.avg</strong>. Once you have clicked on this <a id="_idIndexMarker597"/>selection, the field will appear in<a id="_idIndexMarker598"/> the box under <strong class="bold">Aggregations</strong> and you will see a preview of the pivoted index as shown in <em class="italic">Figure 9.5</em>. </p><div id="_idContainer212" class="IMG---Figure"><img src="image/B17040_09_5.jpg" alt="Figure 9.5 – A preview of the transformed data is displayed to allow a quick check that everything is configured as desired.&#13;&#10;"/></div><p class="figure-caption">Figure 9.5 – A preview of the transformed data is displayed to allow a quick check that everything is configured as desired.</p></li>
				<li>Finally, we will configure the last two items: an ID for the transform job, and the name of the destination index that will contain the documents that describe our pivoted entities. It is a good idea to leave the <strong class="bold">Create index pattern</strong> checkbox checked as shown in <em class="italic">Figure 9.6</em> so that you can easily navigate to the destination index in the <strong class="bold">Discover</strong> tab to view the results. <div id="_idContainer213" class="IMG---Figure"><img src="image/B17040_09_006.jpg" alt="Figure 9.6 – Each transform needs a transform ID &#13;&#10;"/></div><p class="figure-caption">Figure 9.6 – Each transform needs a transform ID </p><p>The transform<a id="_idIndexMarker599"/> ID will be used to identify the<a id="_idIndexMarker600"/> transform job and a destination index that will contain the documents of the entity-centric index that is produced as a result of the transform job. </p></li>
				<li>To start the transform job, remember to click <strong class="bold">Next</strong> in the <strong class="bold">Transform</strong> wizard after completing the instructions described in <em class="italic">step 6</em>, followed by <strong class="bold">Create and start</strong>. This will launch the transform job and create the pivoted, entity-centric index. </li>
				<li>After the transform has completed (you will see the progress bar reach 100% if all goes well), you can click on the <strong class="bold">Discover</strong> button at the bottom of the <strong class="bold">Transform</strong> wizard and view your transformed documents. <p>As we discussed at the beginning of this section, we see from a sample document in <em class="italic">Figure 9.7</em> that the transform job has taken a transaction-centric index, which recorded each purchase made by a customer in our e-commerce store and transformed it into an entity-centric index that describes a specific analytical transformation (the calculation of the average price paid by the customer) grouped by the customer's full name. </p></li>
			</ol>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/B17040_09_007.jpg" alt="Figure 9.7 – The result of the transform job is a destination index where each document describes the aggregation per each pivoted entity. In this case, the average taxful_total_price paid by each customer"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – The result of the transform job is a destination index where each document describes the aggregation per each pivoted entity. In this case, the average taxful_total_price paid by each customer </p>
			<p>Congratulations – you<a id="_idIndexMarker601"/> have now created and started<a id="_idIndexMarker602"/> your first transform job! Although it was fairly simple in nature, this basic job configuration is a good building block to use for more complicated transformations, which we will take a look at in the following sections. </p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor168"/>Exploring more advanced pivot and aggregation configurations</h2>
			<p>In the <a id="_idIndexMarker603"/>previous section, we explored the <a id="_idIndexMarker604"/>two <a id="_idIndexMarker605"/>parts<a id="_idIndexMarker606"/> of a transform: the pivot and the aggregations. In the subsequent example, our goal was to use transforms on the Kibana sample eCommerce dataset to find out the average amount of money our customers spent per order. To solve this problem, we figured out that each document that recorded a transaction had a field called <strong class="source-inline">customer.full_name</strong> and we used this field to pivot our source index. Our aggregation was an average of the field that recorded the total amount of money spent by the customer on the order. </p>
			<p>However, not all questions that we might want to ask of our e-commerce data lend themselves to simple pivot or group by configurations like the one discussed previously. Let's explore some more advanced pivot configurations that are possible with transforms, with the help of some sample investigations we might want to carry out on the e-commerce dataset. If you want to discover all of the available pivot configurations, take a look at the API documentation for the pivot object at this URL: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/put-transform.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/put-transform.html</a>. </p>
			<p>Suppose that <a id="_idIndexMarker607"/>we would like to find out the average <a id="_idIndexMarker608"/>amount of money spent <a id="_idIndexMarker609"/>per <a id="_idIndexMarker610"/>order per week in our dataset, and how many unique customers made purchases. In order to answer these questions, we will need to construct a new transform configuration: </p>
			<ol>
				<li value="1">Instead of pivoting <a id="_idIndexMarker611"/>by the name of the customer, we want to construct a <strong class="bold">date histogram</strong> from the field <strong class="source-inline">order_date</strong>, which, as the name suggests, records when the order was placed. The <strong class="bold">Transform</strong> wizard makes this simple since <strong class="bold">date_histogram(order_date)</strong> will be one of the pre-configured options displayed in the <strong class="bold">Group by</strong> dropdown. </li>
				<li>Once you have selected <strong class="bold">date_histogram(order_date)</strong> in the <strong class="bold">Group by</strong> dropdown, direct your attention to the right-hand side of the panel as shown in <em class="italic">Figure 9.8</em>. The right-hand side should contain an abbreviation for the length of the grouping interval used in the date histogram (for example <strong class="bold">1m</strong> for an interval of 1 minute). In our case, we are interested in pivoting our index by weeks, so we need to choose <strong class="bold">1w</strong> from the dropdown. <div id="_idContainer215" class="IMG---Figure"><img src="image/B17040_09_008.jpg" alt="Figure 9.8 – Adjust the frequency of the date histogram from the dropdown&#13;&#10;"/></div><p class="figure-caption">Figure 9.8 – Adjust the frequency of the date histogram from the dropdown</p></li>
				<li>Next, for <a id="_idIndexMarker612"/>our<a id="_idIndexMarker613"/> aggregation, let's choose<a id="_idIndexMarker614"/> the<a id="_idIndexMarker615"/> familiar <strong class="bold">avg(total_taxful_price)</strong>. After we have made our selection, the <strong class="bold">Transform</strong> wizard will display a preview, which will show the average price paid by a customer per order, grouped by different weeks for a few sample rows. The purpose of the preview is to act as a checkpoint. Since transform jobs can be resource-intensive, at this stage it is good to pause and examine the preview to make sure the data is transformed into a format that you desire.</li>
				<li>Sometimes we might want to interrogate our data in ways that do not lend themselves to simple one-tiered group-by configurations like the one we explored in the preceding steps. It is possible to nest group-by configurations, as we will see in just a moment. Suppose that in our hypothetical e-commerce store example, we would also be interested in seeing the average amount of money spent by week and by geographic region. <p>To solve this, let's go back to the <strong class="bold">Transform</strong> wizard and add a second group-by field. In this case, we want to group by <strong class="bold">geoip.region_name</strong>. As before, the wizard shows us a preview of the transform once we select the group-by field. As in the previous case, it is good to take a moment to look at the rows displayed in the <a id="_idIndexMarker616"/>preview to make sure the data has <a id="_idIndexMarker617"/>been transformed<a id="_idIndexMarker618"/> in<a id="_idIndexMarker619"/> the desired way. </p><p class="callout-heading">Tip </p><p class="callout">Click on the <strong class="bold">Columns</strong> toggle above the transform preview table to rearrange the order of the columns.</p><p>In addition to creating multiple group-by configurations, we can also add multiple aggregations to our transform. Suppose that in addition to the average amount of money spent per customer per week and per region, we would also be interested in finding out the number of unique customers who placed orders in our store. Let's see how we can add this aggregation to our transform.</p></li>
				<li>In the <strong class="bold">Aggregations</strong> drop-down menu in the wizard, scroll down until you find the entity cardinality (<strong class="source-inline">customer.full_name.keyword</strong>) and click on it to select it. The resulting aggregation will be added to your transform configuration and the preview should now display one additional column.<p>You can now follow the steps outlined in the tutorial of the previous section to assign an ID and a destination index for the transform, as well as to create and start the job. These will be left as exercises for you. </p></li>
			</ol>
			<p>In the previous two sections, we examined the two key components of transforms: the pivot and aggregations, and did two different walk-throughs to show how both simple and advanced pivot and aggregation combinations can be used to interrogate our data for various insights. </p>
			<p>While <a id="_idIndexMarker620"/>following the first transform, you <a id="_idIndexMarker621"/>may<a id="_idIndexMarker622"/> have <a id="_idIndexMarker623"/>noticed that in <em class="italic">Figure 9.6</em>, we left the <strong class="bold">Continuous mode</strong> checkbox unchecked. We will take a deeper look at what it means to run a transform in continuous mode in the next section.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor169"/>Discovering the difference between batch and continuous transforms</h2>
			<p>The first <a id="_idIndexMarker624"/>transform we created in the previous<a id="_idIndexMarker625"/> section was simple and ran only once. The transform job read the source index <strong class="source-inline">kibana_sample_data_ecommerce</strong>, which we configured in the <strong class="bold">Transform</strong> wizard, performed the numerical calculations required to compute the average price paid by each customer, and then wrote the resulting documents into a destination index. Because our transform runs only once, any changes to our source index <strong class="source-inline">kibana_sample_data_ecommerce</strong> that occur after the transform job runs will no longer be reflected in the data in the destination index. This kind of transform that runs only once is <a id="_idIndexMarker626"/>known as a <strong class="bold">batch transform</strong>. </p>
			<p>In many real-world use cases that produce records of transactions (like in our fictitious e-commerce store example), new documents are being constantly added to the source index. This means that our pivoted entity-centric index that we obtained as a result of running the transform job would be almost immediately out of date. One solution to keep the destination index in sync with the source index is to keep deleting the destination index and rerunning the batch transform job at regular intervals. This, however, is not practical and <a id="_idIndexMarker627"/>requires a lot of manual effort. This is where <strong class="bold">continuous transforms</strong> step in. </p>
			<p>If we have a source index that is being updated and we want to use that to create a pivoted entity-centric index, then we have to use a continuous transform instead of a batch transform. Let's explore continuous transforms in a bit more detail to understand how they differ from batch transforms and what important configuration parameters should be considered when running a continuous transform. </p>
			<p>First, let's set the <a id="_idIndexMarker628"/>stage for the problem we are<a id="_idIndexMarker629"/> trying to solve. Suppose we have a fictitious microblogging social media platform, where users post short updates, assign categories to the updates and interact with other users as well as predefined topics. It is possible to share a post and like a post. Statistics for each post are recorded as well. We have written some Python code to help generate this dataset. This code and accompanying instructions for how to run this code are available under the <strong class="source-inline">Chapter 9 - Introduction to Data Frame Analytics</strong> folder in the GitHub repository for this book (<a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics</a>). After running the generator, you will have an index called <strong class="source-inline">social-media-feed</strong> that will contain a number of documents.</p>
			<p>Each document in the dataset records a post that the user has made on the social media platform. For the sake of brevity, we have excluded the text of the post from the document. <em class="italic">Figure 9.9</em> shows a sample document in the <strong class="source-inline">social-media-feed</strong> index. </p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/B17040_09_009.jpg" alt="Figure 9.9 – A sample document in the social-media-feed index records the username, the time the post was submitted to the platform, as well as some basic statistics about the engagement the post received &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – A sample document in the social-media-feed index records the username, the time the post was submitted to the platform, as well as some basic statistics about the engagement the post received </p>
			<p>In the next <a id="_idIndexMarker630"/>section, we will see how to use this<a id="_idIndexMarker631"/> fictional social media platform dataset to learn about continuous transforms. </p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor170"/>Analyzing social media feeds using continuous transforms</h2>
			<p>In this <a id="_idIndexMarker632"/>section, we will be using <a id="_idIndexMarker633"/>the dataset introduced previously to explore the concept of continuous transforms. As we discussed in the previous section, batch transforms are useful for one-off analyses where we are either happy to analyze a snapshot of the dataset at a particular point in time or we do not have a dataset that is changing. In most real-world applications, this is not the case. Log files are continuously ingested, many social media platforms have around the clock activity, and e-commerce platforms serve customers across all time zones and thus generate a stream of transaction data. This is where continuous transforms step in. </p>
			<p>Let's see how we can analyze the average level of engagement (likes and shares) received by a social media user using continuous transforms:</p>
			<ol>
				<li value="1">Navigate to the <strong class="bold">Transforms</strong> wizard. On the <strong class="bold">Stack Management</strong> page, look to the left under the <strong class="bold">Data</strong> section and select <strong class="bold">Transforms</strong>. </li>
				<li>Just as we did in the previous sections, let's start by creating the transform. For the source index, select the <strong class="source-inline">social-media-feed</strong> index pattern. This should give <a id="_idIndexMarker634"/>you a view similar <a id="_idIndexMarker635"/>to the one in <em class="italic">Figure 9.10</em>.<em class="italic"> </em><div id="_idContainer217" class="IMG---Figure"><img src="image/B17040_09_010.jpg" alt="Figure 9.10 – The Transforms wizard shows a sample of the social-media-feed index&#13;&#10;"/></div><p class="figure-caption">Figure 9.10 – The Transforms wizard shows a sample of the social-media-feed index</p></li>
				<li>In this case, we will be interested in computing aggregations of the engagement metrics of each post per username. Therefore, our <strong class="bold">Group by</strong> configuration will include the username, while our aggregations will compute the total likes and shares per user, the average likes and shares per user as well as the total number of posts each user has made. The final <strong class="bold">Group by</strong> and <strong class="bold">Aggregations</strong> configurations <a id="_idIndexMarker636"/>should<a id="_idIndexMarker637"/> look something like <em class="italic">Figure 9.11</em>.<div id="_idContainer218" class="IMG---Figure"><img src="image/B17040_09_011.jpg" alt="Figure 9.11 – Group by and Aggregations configuration for our continuous transform&#13;&#10;"/></div><p class="figure-caption">Figure 9.11 – Group by and Aggregations configuration for our continuous transform</p></li>
				<li>Finally, tick the <strong class="bold">Continuous mode</strong> selector and confirm that <strong class="bold">Date field</strong> is selected correctly as <strong class="source-inline">timestamp</strong> as shown in <em class="italic">Figure 9.12</em>.<div id="_idContainer219" class="IMG---Figure"><img src="image/B17040_09_012.jpg" alt="Figure 9.12 – Select Continuous mode to make sure the transform process periodically checks &#13;&#10;the source index and incorporates new documents into the destination index&#13;&#10;"/></div><p class="figure-caption">Figure 9.12 – Select Continuous mode to make sure the transform process periodically checks the source index and incorporates new documents into the destination index</p></li>
				<li>Once <a id="_idIndexMarker638"/>you click <strong class="bold">Create and start</strong>, you<a id="_idIndexMarker639"/> can return to the <strong class="bold">Transforms</strong> page and you will see the continuous transforms job for the <strong class="source-inline">social-media-feed</strong> index running. Note the continuous tag in the job description.<div id="_idContainer220" class="IMG---Figure"><img src="image/B17040_09_013.jpg" alt="Figure 9.13 – Continuous transforms shown on the Transforms page. Note that the mode is tagged as continuous&#13;&#10;"/></div><p class="figure-caption">Figure 9.13 – Continuous transforms shown on the Transforms page. Note that the mode is tagged as continuous</p></li>
				<li>Let's insert <a id="_idIndexMarker640"/>some new<a id="_idIndexMarker641"/> posts into our index <strong class="source-inline">social-media-feed</strong> and see how the statistics for the user Carl change after a new document is added to the source index for the transform. To insert a new post, open the Kibana <strong class="bold">Dev Console</strong> and run the following REST API command (see <strong class="source-inline">chapter9</strong> in the book's GitHub repository for a version that you can easily copy and paste into your own Kibana Dev Console if you are following along):  <p class="source-code">POST social-media-feed/_doc</p><p class="source-code">{</p><p class="source-code">    "username": "Carl",</p><p class="source-code">    "statistics": {</p><p class="source-code">      "likes": 320,</p><p class="source-code">      "shares": 8000</p><p class="source-code">    },</p><p class="source-code">    "timestamp": "2021-01-18T23:19:06"</p><p class="source-code">  }</p></li>
				<li>Now, that we have added a new document into the source index <strong class="source-inline">social-media-feed</strong>, we expect that this document will be picked up by the continuous transform job and incorporated into our transform's destination index, <strong class="source-inline">social-media-feed-engagement</strong>. <em class="italic">Figure 9.14</em> showcases the transformed <a id="_idIndexMarker642"/>entry for<a id="_idIndexMarker643"/> the username Carl.</li>
			</ol>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/B17040_09_014.jpg" alt="Figure 9.14 – The destination index of the continuous transform job holds an entry for the new username Carl, which we added manually through the Kibana Dev Console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – The destination index of the continuous transform job holds an entry for the new username Carl, which we added manually through the Kibana Dev Console</p>
			<p>The preceding example gives a very simplified walk-through of how continuous transforms work and how you can create your own continuous transform using the <strong class="bold">Transforms</strong> wizard available in Kibana. In <a href="B17040_13_Epub_AM.xhtml#_idTextAnchor236"><em class="italic">Chapter 13</em></a>, <em class="italic">Inference</em>, we will return to the topic of continuous transforms when we showcase how to combine trained machine learning models, inference, and transforms. </p>
			<p>For now, we will take a<a id="_idIndexMarker644"/> brief detour into the world of the <strong class="bold">scripting language Painless</strong>. While the Transforms wizard and the many pre-built <strong class="bold">Group by</strong> and <strong class="bold">Aggregations</strong> configurations that it offers suffice for many of the most common data analysis use cases, more advanced users will wish to define their own aggregations. A common way<a id="_idIndexMarker645"/> to do this is with the aid<a id="_idIndexMarker646"/> of the Elasticsearch embedded scripting language, Painless. </p>
			<p>In the next section, we will take a little tour of the Painless world, which will prepare you for creating your own advanced transform configurations. </p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor171"/>Using Painless for advanced transform configurations</h1>
			<p>As we have<a id="_idIndexMarker647"/> seen in many of the previous sections, the built-in pivot and aggregation options allow us to analyze and interrogate our data in various ways. However, for more custom or advanced use cases, the built-in functions may not be flexible enough. For these use cases, we will need to write custom pivot and aggregation configurations. The flexible scripting language that is built into Elasticsearch, <strong class="bold">Painless</strong>, allows us to do this. </p>
			<p>In this section, we will introduce Painless, illustrate some tools that are useful when working with Painless, and then show how Painless can be applied to create custom Transform configurations.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor172"/>Introducing Painless</h2>
			<p>Painless is<a id="_idIndexMarker648"/> a scripting language that is built into Elasticsearch. We will take a look at Painless in terms of variables, control flow constructs, operations, and functions. These are the basic building blocks that will help you develop your own custom scripts to use with transforms. Without further ado, let's dive into the introduction. </p>
			<p>It is likely that many readers of this book come from a sort of programming language background. You may have written data cleaning scripts with Python, programmed a Linux machine with bash scripts, or developed enterprise software with Java. Although these languages have many differences and are useful for different purposes, they all have shared building blocks that help human readers of the language understand them. Although there is an almost infinite number of approaches to teaching a programming language, the approach we will take here will be based on understanding the following basic topics about Painless: variables, operations (such as addition, subtraction, and various Boolean tests), control flow (if-else constructs and for loops) and functions. These are analogous concepts that users familiar with another programming language should be able to<a id="_idIndexMarker649"/> relate to. In addition to these concepts, we will be looking at some aspects that are particular to Painless, such as different execution contexts. </p>
			<p>When learning a new programming language, it is important to have a playground that can be used to experiment with syntax. Luckily, with the 7.10 version of Elasticsearch, the <strong class="bold">Dev Tools</strong> app now contains the new <strong class="bold">Painless Lab</strong> playground, where you can try out the code samples presented in this chapter as well as any code samples you write on your own. </p>
			<p>The Painless Lab can be accessed by navigating to <strong class="bold">Dev Tools</strong> as shown in <em class="italic">Figure 9.15</em> and then, in the top menu of the <strong class="bold">Dev Tools</strong> page, selecting <strong class="bold">Painless Lab</strong>.</p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/B17040_09_015.jpg" alt="Figure 9.15 – The link to the Dev Tools page is located in the lower section of the Kibana side menu. Select it to access the interactive Painless lab environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.15 – The link to the Dev Tools page is located in the lower section of the Kibana side menu. Select it to access the interactive Painless lab environment</p>
			<p>This will <a id="_idIndexMarker650"/>open an embedded Painless code editor as shown in <em class="italic">Figure 9.16</em>. </p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/B17040_09_016.jpg" alt="Figure 9.16 – The Painless Lab in Dev Tools features an embedded code editor. The Output window shows the evaluation result of the code in the code editor &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – The Painless Lab in Dev Tools features an embedded code editor. The Output window shows the evaluation result of the code in the code editor </p>
			<p>The code editor<a id="_idIndexMarker651"/> in the Painless Lab is preconfigured with some sample functions and variable declarations to illustrate how one might draw the figure in the <strong class="bold">Output</strong> window in <em class="italic">Figure 9.16</em> using Painless. For now, you can delete this code to make space for your own experiments that you will carry out as you read the rest of this chapter. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The full Painless language specification is available online here: <a href="https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-lang-spec.html">https://www.elastic.co/guide/en/elasticsearch/painless/master/painless-lang-spec.html</a>. You can use it as a reference and resource for further information about the topics covered later.</p>
			<h3>Variables, operators, and control flow</h3>
			<p>One of <a id="_idIndexMarker652"/>the<a id="_idIndexMarker653"/> first<a id="_idIndexMarker654"/> things we usually want to do in a programming language is to manipulate values. In order to do this effectively, we assign those values names or variables. Painless has types and before a variable can be assigned, it must be declared along with its type. The syntax for declaring a variable is as follows: <strong class="bold">type_identifier variable_name ;</strong>.</p>
			<p>How to use this syntax in practice is illustrated in the following code block, where we declare variables <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> to hold integer values, the variable <strong class="source-inline">my_string</strong> to hold a string value, and the variable <strong class="source-inline">my_float_array</strong> to hold an array of floating values: </p>
			<p class="source-code">int a;</p>
			<p class="source-code">int b;</p>
			<p class="source-code">String my_string;</p>
			<p class="source-code">float[] my_float_array;</p>
			<p>At this<a id="_idIndexMarker655"/> point, the<a id="_idIndexMarker656"/> variables do not yet hold any non-null values. They have <a id="_idIndexMarker657"/>just been initialized in preparation for an assignment statement, which will assign to each a value of the appropriate type. Thus, if you try copying the preceding code block into the Painless Lab code editor, you will see an output of <strong class="bold">null</strong> in the <strong class="bold">Output</strong> window as shown in <em class="italic">Figure 9.17</em>. </p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/B17040_09_017.jpg" alt="Figure 9.17 – On the left, Painless variables of various types are initialized. On the right, the Output panel shows null, because these variables have not yet been assigned a value &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – On the left, Painless variables of various types are initialized. On the right, the Output panel shows null, because these variables have not yet been assigned a value </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Painless Lab code editor only displays the result of the last statement.</p>
			<p>Next, let's assign some values to these variables so that we can do some interesting things with them. The assignments are shown in the following code block. In the first two lines, we assign integer values to our integer variables <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong>. In the third line, we assign a string <strong class="source-inline">"hello world"</strong> to the string variable <strong class="source-inline">my_string</strong>, and in the final line, we initialize a new array with floating-point values: </p>
			<p class="source-code">a = 1;</p>
			<p class="source-code">b = 5;</p>
			<p class="source-code">my_string = "hello world";</p>
			<p class="source-code">my_double_array = new double[] {1.0, 2.0, 2.5};</p>
			<p>Let's do some interesting things with these variables to illustrate what operators are available<a id="_idIndexMarker658"/> in <a id="_idIndexMarker659"/>Painless. We will only be able to cover a few of the available<a id="_idIndexMarker660"/> operators. For the full list of available operators, please see the Painless language specification (<a href="https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators.html">https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators.html</a>). The following code blocks illustrate basic mathematical operations: addition, subtraction, division, and multiplication as well as the modulus operation or taking the remainder:</p>
			<p class="source-code">int a;</p>
			<p class="source-code">int b;</p>
			<p class="source-code">a = 1;</p>
			<p class="source-code">b = 5;</p>
			<p class="source-code">// Addition </p>
			<p class="source-code">int addition;</p>
			<p class="source-code">addition = a+b;</p>
			<p class="source-code">// Subtraction </p>
			<p class="source-code">int subtraction;</p>
			<p class="source-code">subtraction = a-b;</p>
			<p class="source-code">// Multiplication</p>
			<p class="source-code">int multiplication;</p>
			<p class="source-code">multiplication = a*b;</p>
			<p class="source-code">// Integer Division </p>
			<p class="source-code">int int_division;</p>
			<p class="source-code">int_division = a/b;</p>
			<p class="source-code">// Remainder</p>
			<p class="source-code">int remainder;</p>
			<p class="source-code">remainder = a%b;</p>
			<p>Try <a id="_idIndexMarker661"/>out<a id="_idIndexMarker662"/> these code examples on your own in<a id="_idIndexMarker663"/> the Painless Lab and you should be able to see the results of your evaluation, as illustrated in the case of addition in <em class="italic">Figure 9.18</em>. </p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/B17040_09_018.jpg" alt="Figure 9.18 – Using the Painless Lab code editor and console for addition in Painless. The result stored in a variable called &quot;addition&quot; in the code editor on the left is displayed in the Output tab on the right&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18 – Using the Painless Lab code editor and console for addition in Painless. The result stored in a variable called "addition" in the code editor on the left is displayed in the Output tab on the right</p>
			<p>In addition to mathematical operations, we will also take a look at <strong class="bold">Boolean operators</strong>. These <a id="_idIndexMarker664"/>are vital for many Painless scripts and configurations as well <a id="_idIndexMarker665"/>as for <strong class="bold">control flow statements</strong>, which we will take a look at afterward. </p>
			<p>The code snippets that follow illustrate how to declare a variable to hold a Boolean (true/false) value and how to use comparison operators to determine whether values are less, greater, less than or equal, or greater than or equal. For a full list of Boolean operators in Painless, please consult the Painless specification available here: <a href="https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-boolean.html">https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-operators-boolean.html</a>:</p>
			<p class="source-code">boolean less_than = 4&lt;5;</p>
			<p class="source-code">boolean greater_than = 4&gt;5;</p>
			<p class="source-code">boolean less_than_or_equal = 4 &lt;=5;</p>
			<p class="source-code">boolean greater_than_or_equal = 4 &gt;= 5;</p>
			<p>As <a id="_idIndexMarker666"/>an<a id="_idIndexMarker667"/> exercise, copy the preceding code block into <a id="_idIndexMarker668"/>the Painless Lab code editor. If you wish, you can view the contents of each of these variables by typing its name followed by a semicolon into the last line of the Painless Lab code editor and the value stored in the variable will be printed in the <strong class="bold">Output</strong> window on the right, as shown in <em class="italic">Figure 9.19</em>.</p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/B17040_09_019.jpg" alt="Figure 9.19 – Typing in the variable name followed by a semicolon in the Painless Lab code editor will output the contents of the variable in the Output tab &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19 – Typing in the variable name followed by a semicolon in the Painless Lab code editor will output the contents of the variable in the Output tab </p>
			<p>While the Boolean operator illustrated here is useful in many numerical computations, we probably could not write effective control flow statements without the equality operators <strong class="source-inline">==</strong> and <strong class="source-inline">!=</strong>, which check whether or not two variables are equal. The following code block illustrates how to use these operators with a few practical examples: </p>
			<p class="source-code">// boolean operator for testing for equality</p>
			<p class="source-code">boolean two_equal_strings = "hello" == "hello";</p>
			<p class="source-code">two_equal_strings;</p>
			<p class="source-code">// boolean operator for testing for inequality</p>
			<p class="source-code">boolean not_equal = 5!=6;</p>
			<p class="source-code">not_equal;</p>
			<p>Last<a id="_idIndexMarker669"/> but <a id="_idIndexMarker670"/>not least in our tour of the Boolean operators<a id="_idIndexMarker671"/> in Painless, we will look at a code block that showcases how to use the <strong class="source-inline">instanceof</strong> operator, which checks whether a given variable is an instance of a type and returns <strong class="source-inline">true</strong> or <strong class="source-inline">false</strong>. This is a useful operator to have when you are writing Painless code that you only want to operate on variables of a specified type: </p>
			<p class="source-code">// boolean operator instanceof tests if a variable is an instance of a type</p>
			<p class="source-code">// the variable is_integer evaluates to true </p>
			<p class="source-code">int int_number = 5;</p>
			<p class="source-code">boolean is_integer = int_number instanceof int;</p>
			<p class="source-code">is_integer; </p>
			<p>In the final <a id="_idIndexMarker672"/>part of this section, let's take a look at one of the most important building blocks in our Painless script: <strong class="bold">if-else statements</strong> and <strong class="bold">for loops</strong>. The <a id="_idIndexMarker673"/>syntax for <strong class="source-inline">if-else</strong> statements is shown in the following code block with the help of an example:</p>
			<p class="source-code">int a = 5;</p>
			<p class="source-code">int sum;</p>
			<p class="source-code">if (a &lt; 6){</p>
			<p class="source-code">   sum = a+5;</p>
			<p class="source-code">    }</p>
			<p class="source-code">else {</p>
			<p class="source-code">    sum = a-5;</p>
			<p class="source-code">    }</p>
			<p class="source-code">sum;</p>
			<p>In the <a id="_idIndexMarker674"/>preceding<a id="_idIndexMarker675"/> code block, we declare an<a id="_idIndexMarker676"/> integer variable, <strong class="source-inline">a</strong>, and assign it to contain the integer value <strong class="source-inline">5</strong>. We then declare another integer variable, <strong class="source-inline">sum</strong>. This variable will change according to the execution branch that is taken in the <strong class="source-inline">if-else</strong> statement. Finally, we see that the <strong class="source-inline">if-else</strong> statement first checks whether the integer variable <strong class="source-inline">a</strong> is less than 6, and if it is, stores the result of adding a and the integer 5 in the variable sum. If not, the amount stored in the variable <strong class="source-inline">sum</strong> is the result of subtracting 5 from <strong class="source-inline">a</strong>.</p>
			<p>If you type this code in the Painless Lab code editor, the <strong class="bold">Output</strong> console will print out the value of <strong class="source-inline">sum</strong> as 10 (as shown in <em class="italic">Figure 9.20</em>), which is what we expect based on the previous analysis. </p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/B17040_09_020.jpg" alt="Figure 9.20 – The if-else statement results in the sum variable being set to the value 10&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20 – The if-else statement results in the sum variable being set to the value 10</p>
			<p>Finally, we will take a look at how to write a <strong class="source-inline">for</strong> loop, which is useful for various data analysis and data processing tasks with Painless. In our <strong class="source-inline">for</strong> loop, we will be iterating over a string variable and calculating how many occurrences of the letter <strong class="source-inline">a</strong> occur in the string. This<a id="_idIndexMarker677"/> is<a id="_idIndexMarker678"/> a very simple example, but will <a id="_idIndexMarker679"/>hopefully help you to understand the syntax so that you can apply it in your own examples: </p>
			<p class="source-code">// initialize the string and the counter variable</p>
			<p class="source-code">String sample_string = "a beautiful day";</p>
			<p class="source-code">int counter = 0;</p>
			<p class="source-code">for (int i=0;i&lt;sample_string.length();i++){</p>
			<p class="source-code"> // get a letter from the string using the substring method</p>
			<p class="source-code"> String letter = sample_string.substring(i, i+1);</p>
			<p class="source-code"> //use an if-statement to check if the current letter being processed is an a</p>
			<p class="source-code"> if (letter=="a") {</p>
			<p class="source-code"> // is yes, increment the counter</p>
			<p class="source-code"> counter++</p>
			<p class="source-code">   }</p>
			<p class="source-code"> }</p>
			<p class="source-code">  </p>
			<p class="source-code">counter;</p>
			<p>Copy and paste this code sample (a copy can be found in the GitHub repository <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition</a> for this book under the folder <strong class="source-inline">Chapter 9 - Introduction to Data Frame Analytics</strong>) into your Painless Lab and you will see that the <strong class="source-inline">counter</strong> variable<a id="_idIndexMarker680"/> in<a id="_idIndexMarker681"/> the <strong class="bold">Output</strong> panel will print out <strong class="source-inline">3</strong>, as we<a id="_idIndexMarker682"/> expect, since there are three occurrences of the letter "a" in the string "a beautiful day." </p>
			<h3>Functions</h3>
			<p>Now that<a id="_idIndexMarker683"/> we <a id="_idIndexMarker684"/>have covered variables, operators, and control flow statements, let's turn our attention for a moment to <strong class="bold">functions</strong>. Sometimes, we might notice ourselves writing the same lines of code over and over again, across multiple different scripts and configurations. At this point, it might be more economical to package up the lines that we find ourselves reaching for over and over again into a reusable piece of code that we can reference with a name from our Painless scripts. </p>
			<p>Let's return to the example where we wrote a <strong class="source-inline">for</strong> loop with an <strong class="source-inline">if</strong> statement to calculate the instances of the letter "a" in a given string. Suppose we want to reuse this functionality and make it slightly more generic. This is a perfect opportunity to package up this piece of code as a Painless function. </p>
			<p>There are three parts to writing a function in Painless. First, we write the function header, which specifies the type of the value the function returns, as well as the name of the function. This is what we will use to refer to the function when we use it in our scripts or <strong class="bold">Ingest Pipelines</strong>, which <a id="_idIndexMarker685"/>we will discuss in more detail when<a id="_idIndexMarker686"/> we discuss <strong class="bold">Inference</strong> in <a href="B17040_13_Epub_AM.xhtml#_idTextAnchor236"><em class="italic">Chapter 13</em></a>, <em class="italic">Inference</em>. The skeleton of our function, which we will call <strong class="source-inline">letterCounter</strong>, is shown in the following code block: </p>
			<p class="source-code">int letterCounter(){</p>
			<p class="source-code">}</p>
			<p>The <strong class="source-inline">int</strong> in front of the function name determines the type of value returned by this function. Since we are interested in the number of occurrences of a particular letter in a particular string, we will be returning an integer count. The parentheses after the name <strong class="source-inline">letterCounter</strong> will hold the parameters that the function accepts. For now, we have not specified any parameters, so there is nothing between the parentheses. Finally, the two curly braces signify the place for the function body – this is where all of the logic of the function will reside.</p>
			<p>Now that we <a id="_idIndexMarker687"/>have investigated the elements that go into creating a basic function header, let's populate the function body (the space between the curly braces with the code we wrote in the previous section while learning about <strong class="source-inline">for</strong> loops). Our function now should look something like the following code block: </p>
			<p class="source-code">int letterCounter(){</p>
			<p class="source-code">    // initialize the string and the counter variable</p>
			<p class="source-code">    String sample_string = "a beautiful day";</p>
			<p class="source-code">    int counter = 0;</p>
			<p class="source-code">    for (int i=0;i&lt;sample_string.length();i++){</p>
			<p class="source-code">    // get a letter from the string using the substring method</p>
			<p class="source-code">    String letter = sample_string.substring(i, i+1);</p>
			<p class="source-code">    //use an if-statement to check if the current letter being processed is an a</p>
			<p class="source-code">    if (letter=="a") {</p>
			<p class="source-code">    // is yes, increment the counter</p>
			<p class="source-code">    counter++</p>
			<p class="source-code">    }</p>
			<p class="source-code">    }</p>
			<p class="source-code">    return counter;</p>
			<p class="source-code">}</p>
			<p class="source-code">letterCounter();</p>
			<p>If you look <a id="_idIndexMarker688"/>toward the end of the function body, you will notice that the only difference between our <strong class="source-inline">for</strong> loop residing inside the function body is that now we have added a <strong class="source-inline">return</strong> statement. This allows our value of interest stored in the variable <strong class="source-inline">counter</strong> to be returned to the code calling the function, which brings us to our next question. Now that we have written our first Painless function and it does something interesting, how do we go about calling this function?</p>
			<p>In the Painless Lab environment, we would simply type out <strong class="source-inline">letterCounter();</strong> as shown in <em class="italic">Figure 9.21</em>. The result returned by this function is, as we expect based on our previous dissection of this code sample, <strong class="source-inline">3</strong>. </p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/B17040_09_021.jpg" alt="Figure 9.21 – The definition of the sample function letterCounter displayed alongside an example of how to call a function in the Painless Lab environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21 – The definition of the sample function letterCounter displayed alongside an example of how to call a function in the Painless Lab environment </p>
			<p>Now that we<a id="_idIndexMarker689"/> have a working function, let's talk about making this function a bit more generic, which is what you will often need to do when working with either transforms, which we have been discussing in this chapter, or various ingest pipelines and script processors, which we will be discussing in <a href="B17040_13_Epub_AM.xhtml#_idTextAnchor236"><em class="italic">Chapter 13</em></a>, <em class="italic">Inference</em>. As it currently stands, our function <strong class="source-inline">letterCounter</strong> is very specific. It only computes the number of occurrences of a very specific letter – the letter a – in a very specific string, the phrase "a beautiful day." </p>
			<p>Suppose that to make this snippet of code really useful, we would like to vary both the phrase and the letter that is counted. With functions, we can do this with minimal code duplication, by<a id="_idIndexMarker690"/> configuring <strong class="bold">function parameters</strong>. Since we want to vary both the letter and the phrase, let's make those two into function parameters. After the change, our function definition will look as in the following code block:</p>
			<p class="source-code">int letterCounter(String sample_string, String count_letter){</p>
			<p class="source-code">    // initialize the string and the counter variable</p>
			<p class="source-code">    int counter = 0;</p>
			<p class="source-code">    for (int i=0;i&lt;sample_string.length();i++){</p>
			<p class="source-code">    // get a letter from the string using the substring method</p>
			<p class="source-code">    String letter = sample_string.substring(i, i+1);</p>
			<p class="source-code">    //use an if-statement to check if the current letter being processed is an a</p>
			<p class="source-code">    if (letter==count_letter) {</p>
			<p class="source-code">    // is yes, increment the counter</p>
			<p class="source-code">    counter++</p>
			<p class="source-code">    }</p>
			<p class="source-code">    }</p>
			<p class="source-code">    return counter;</p>
			<p class="source-code">}</p>
			<p>Notice <a id="_idIndexMarker691"/>that now, in between the parentheses in the function header, we have defined two parameters: one <strong class="source-inline">sample_string</strong> parameter representing the phrase in which we want to count the occurrences of the second parameter, <strong class="source-inline">count_letter</strong>. </p>
			<p>To call this function, we will first define new variables to hold our phrase ("a beautiful day," once again) and our letter of interest – this time the letter "b" instead of "a." Following this, we will pass both of these variables into the function call as shown in the following code block: </p>
			<p class="source-code">String phrase = "a beautiful day";</p>
			<p class="source-code">String letter_of_interest = "b";</p>
			<p class="source-code">letterCounter(phrase, letter_of_interest);</p>
			<p>Since there is only one occurrence of the letter "b" in our phrase of interest, we will expect the result of executing this function to be <strong class="source-inline">1</strong>, as is the case in <em class="italic">Figure 9.22</em>. </p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/B17040_09_022.jpg" alt="Figure 9.22 – The result of calling the function is shown in the Output panel on the right&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22 – The result of calling the function is shown in the Output panel on the right</p>
			<p>Now you should<a id="_idIndexMarker692"/> be equipped to write your own Painless code! This will come in handy in <a href="B17040_13_Epub_AM.xhtml#_idTextAnchor236"><em class="italic">Chapter 13</em></a>, <em class="italic">Inference</em>, where we will use advanced Painless features to perform feature extractions and to write script processors.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor173"/>Working with Python and Elasticsearch </h1>
			<p>In recent <a id="_idIndexMarker693"/>years, Python<a id="_idIndexMarker694"/> has become the dominant language for many data-intensive projects. Fueled by its easy-to-use machine learning and data analysis libraries, many data scientists and data engineers are now heavily relying on Python for most of their daily operations. Therefore, no discussions of machine learning in the Elastic Stack would be complete without exploring how a data analysis professional can work with the Elastic Stack in Python. </p>
			<p>In this section, we will take a look at the three official Python Elasticsearch clients, understand the differences between them, and discuss when one might want to use one over the others. We will demonstrate how usage of Elastic Stack ML can be automated by using Elasticsearch clients. In addition, we will take a<a id="_idIndexMarker695"/> deeper look at <strong class="bold">Eland</strong>, the new data science native client that enables efficient in-memory data analysis backed by Elasticsearch. After<a id="_idIndexMarker696"/> exploring how Eland works, we will illustrate how<a id="_idIndexMarker697"/> Eland can be combined with Jupyter notebooks, an open source interactive data analysis environment to analyze data stored in Elasticsearch. </p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor174"/>A brief tour of the Python Elasticsearch clients</h2>
			<p>Anyone who<a id="_idIndexMarker698"/> has used the Kibana Dev Tools Console to communicate with Elasticsearch knows that most things are carried out via the REST API. You can insert, update, and delete documents by calling the right endpoint with the right parameters. Not surprisingly, there are several levels of abstraction at which a client program calling these REST API endpoints can be written. The low-level client <strong class="bold">elasticsearch-py</strong> (<a href="https://elasticsearch-py.readthedocs.io/en/v7.10.1/">https://elasticsearch-py.readthedocs.io/en/v7.10.1/</a>) provides<a id="_idIndexMarker699"/> a thin Python wrapper on the REST API calls that one would normally execute through the Kibana Dev Console or through some application capable of sending HTTP requests. The next level of abstraction is captured by the Elasticsearch DSL client (<a href="https://elasticsearch-dsl.readthedocs.io/en/latest/">https://elasticsearch-dsl.readthedocs.io/en/latest/</a>). Finally, the most <a id="_idIndexMarker700"/>abstracted client is <strong class="bold">Eland</strong> (<a href="https://eland.readthedocs.io/en/7.10.1b1/">https://eland.readthedocs.io/en/7.10.1b1/</a>), where the data frame, a tabular representation of data, is a first-class citizen. We will see in subsequent examples what this means for data scientists wishing to use Eland with Elasticsearch. </p>
			<p>In addition to the available Elasticsearch clients, we are going to take a moment to discuss the various execution environments that are available to any data engineers or data scientists wishing to work with Python and Elasticsearch. This, in turn, will lead us to an introduction to Jupyter notebooks and the whole Jupyter ecosystem, which is a tool to know for anyone wishing to work with machine learning and the Elastic Stack.  </p>
			<p>The first and probably most familiar way to execute a Python program is to write our program's logic in a text file – or a script – save it with a <strong class="source-inline">.py</strong> extension to signify that it contains Python code, and then invoke the script using the command line as shown in <em class="italic">Figure 9.23</em>. </p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/B17040_09_023.jpg" alt="Figure 9.23 – One way to work with Python is to store our program in a text file &#13;&#10;or a script and then execute it from the command line&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23 – One way to work with Python is to store our program in a text file or a script and then execute it from the command line</p>
			<p>The second way <a id="_idIndexMarker701"/>is to use the interactive Python REPL, which is shown in <em class="italic">Figure 9.24</em>. Invoking <strong class="source-inline">python</strong> (or <strong class="source-inline">python3</strong>) on our command line will launch an interactive Python environment where we can write functions, define variables, and execute all sorts of Python code. While this environment is useful for small-scale or quick experiments, in practice it would be hard to work on a long-term or larger collaborative data analysis project in the REPL environment. Therefore, for most projects involving Python, data analysis, and the Elastic Stack, the environment of choice is some sort of integrated development environment that provides a code editor along with various tools to support both programming and execution. </p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/B17040_09_024.jpg" alt="Figure 9.24 – A screenshot depicting a sample Python interactive shell or REPL, which is perfect for quick experiments in the Python programming language&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.24 – A screenshot depicting a sample Python interactive shell or REPL, which is perfect for quick experiments in the Python programming language</p>
			<p>A development environment that is specifically designed with data analysis in mind is a Jupyter notebook, which is illustrated in <em class="italic">Figure 9.25</em>. </p>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/B17040_09_25.jpg" alt="Figure 9.25 – A sample Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.25 – A sample Jupyter notebook</p>
			<p>The notebook<a id="_idIndexMarker702"/> can be installed in a Python installation through a central package management service such as <strong class="source-inline">pip</strong> and is launched by typing <strong class="source-inline">jupyter notebook</strong> on the command line. The launched environment runs in a browser such as Chrome or Firefox and provides an environment where code snippets, text paragraphs, graphs, and visualizations (both interactive and static) can live side by side. Many authors have covered the Jupyter Notebook and the library ecosystem that exists around it much better than what we have space or time for in this chapter and thus we encourage those readers who want to or anticipate working more at the intersection of data analysis, Elasticsearch, and Python to take a look at the materials listed in the <em class="italic">Further reading</em> section at the end of this chapter. </p>
			<h3>Understanding the motivation behind Eland</h3>
			<p>Readers of<a id="_idIndexMarker703"/> the previous section might wonder, <em class="italic">what was the motivation of building yet another Elasticsearch Python client when the community already has two clients to choose from?</em> Moreover, <em class="italic">why build an entire software library around the idea of a</em> <strong class="bold">Data Frame</strong> <em class="italic">object?</em> The answers to both of these questions could <a id="_idIndexMarker704"/>probably fill an entire book, so the answers presented here will necessarily leave some subtleties unexplored. Nevertheless, we hope that for the interested reader, the discussion in this section will give some interesting context into how Eland came to be and why it was designed around the idea of the Data Frame. </p>
			<p>Although Python appears to be the dominant force for many domains of data analysis and machine learning today, this was not always the case. In particular, in the early 2010s, the ecosystem was dominated by the statistical processing language R, which had a very useful construct – a dataframe object that allowed one to analyze rows of data in a table-like structure (a concept that is no doubt familiar to users of Excel). At about the same time, Wes McKinney, then working at the New York financial firm AQR Capital, started working on a library to make the lives of Python data analysts easier. This work culminated in the release of <strong class="bold">pandas</strong>, an open source data analysis library that is used by thousands of data scientists and data engineers.<strong class="bold"> </strong></p>
			<p>One of the key features that made pandas useful and easy to use was the <strong class="source-inline">DataFrame</strong> object. Analogous to the R object, this object made it straightforward to manipulate and carry out analyses on data in a tabular manner. Although pandas is very powerful and contains a multitude of built-in functions and methods, it begins to hit limitations when the dataset one wishes to analyze is too large to fit in memory.</p>
			<p>In these cases, data analysts often resort to sampling data from the various databases, for example, Elasticsearch, where it is stored, exporting it into flat files, and then reading it into their Python process so that it can be analyzed with pandas or another library. While this approach certainly works, the workflow would become more seamless if pandas were to interface directly with the database. What if we could transparently interface a <strong class="source-inline">DataFrame</strong> object with Elasticsearch so that the data analyst could focus on analyzing data instead of having to worry about managing the connection and<a id="_idIndexMarker705"/> exporting data from Elasticsearch? This is the very grounding idea of Eland. Hopefully, the next sections will demonstrate how this philosophy has materialized concretely in the design of this library. </p>
			<h3>Taking your first steps with Eland</h3>
			<p>Since Eland is <a id="_idIndexMarker706"/>a third-party library, you will first have to install it so that your Python installation is able to use it. The instructions for this across various operating systems are given under <strong class="source-inline">Chapter 9 - Introduction to Data Frame Analytics</strong> in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%209%20-%20Introduction%20to%20Data%20Frame%20Analytics</a>. We will assume that readers wishing to follow along with the material in this section of the book will have followed the instructions linked to complete the installation of the library. The examples and screenshots in this chapter will be illustrated using a Jupyter notebook environment, but it is also possible to run the code samples presented in this chapter in a standalone environment (for example, from a Python REPL or from a Python script). Examples that specifically require the Jupyter notebook environment will be clearly indicated: </p>
			<ol>
				<li value="1">The first step that we have to do to make use of Eland is to import it into our environment. In Python, this is done using the <strong class="source-inline">import</strong> statement as shown in <em class="italic">Figure 9.26</em>. Note that when using a library such as Eland, it is common to assign an alias to the library. In the code snippet shown in <em class="italic">Figure 9.26</em>, we have assigned <strong class="source-inline">eland</strong> the alias <strong class="source-inline">ed</strong> using the keyword <strong class="source-inline">as</strong>. This will save us some typing in the future as we will be invoking the library name many times as we access its<a id="_idIndexMarker707"/> objects and methods. <div id="_idContainer233" class="IMG---Figure"><img src="image/B17040_09_26.jpg" alt="Figure 9.26 – Importing Eland into the notebook&#13;&#10;"/></div><p class="figure-caption">Figure 9.26 – Importing Eland into the notebook</p></li>
				<li>After importing the code into our Jupyter notebook, we are free to start exploring. Let's start with the most basic things one can do with Eland: creating an Eland <strong class="source-inline">DataFrame</strong>. To create the <strong class="source-inline">DataFrame</strong>, we need to specify two things: the URL of our Elasticsearch cluster (for example, <strong class="source-inline">localhost</strong> if we are running Elasticsearch locally on the default port <strong class="source-inline">9200</strong>) and the name of the Elasticsearch index. These two parameters are passed into the <strong class="source-inline">DataFrame</strong> constructor as shown in <em class="italic">Figure 9.27</em>. <div id="_idContainer234" class="IMG---Figure"><img src="image/B17040_09_27.jpg" alt="Figure 9.27 – Creating a DataFrame in Eland involves the URL of the Elasticsearch cluster and the name of the index that contains the data we wish to analyze &#13;&#10;"/></div><p class="figure-caption">Figure 9.27 – Creating a DataFrame in Eland involves the URL of the Elasticsearch cluster and the name of the index that contains the data we wish to analyze </p></li>
				<li>One of the first tasks we are interested in doing when we start examining a new dataset is<a id="_idIndexMarker708"/> learning what the data looks like (usually, it is enough to see a few example rows to get the general gist of the data) and some of the general statistical properties of the dataset. We can learn the former by calling the <strong class="source-inline">head</strong> method on our Eland <strong class="source-inline">DataFrame</strong> object as is shown in <em class="italic">Figure 9.28</em>. <div id="_idContainer235" class="IMG---Figure"><img src="image/B17040_09_28.jpg" alt="Figure 9.28 – Calling the head method on the Eland DataFrame object &#13;&#10;will show us the first 5 rows in the dataset &#13;&#10;"/></div><p class="figure-caption">Figure 9.28 – Calling the head method on the Eland DataFrame object will show us the first 5 rows in the dataset </p></li>
				<li>Knowledge of the latter, on the other hand, is obtained by calling the <strong class="source-inline">describe</strong> method, which will be familiar to pandas users and is shown in <em class="italic">Figure 9.29</em>.<div id="_idContainer236" class="IMG---Figure"><img src="image/B17040_09_29.jpg" alt="Figure 9.29 – &quot;describe&quot; summarizes the statistical properties of the numerical columns in the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 9.29 – "describe" summarizes the statistical properties of the numerical columns in the dataset</p></li>
				<li>In addition <a id="_idIndexMarker709"/>to obtaining a basic overview of the dataset, we can easily access individual values of a given field in the index by using the <strong class="source-inline">get</strong> command in conjunction with the name of the field as shown in <em class="italic">Figure 9.30</em>.<em class="italic"> </em><div id="_idContainer237" class="IMG---Figure"><img src="image/B17040_09_30.jpg" alt="Figure 9.30 – We can work with the individual field values in the index by using the get method&#13;&#10;"/></div><p class="figure-caption">Figure 9.30 – We can work with the individual field values in the index by using the get method</p></li>
				<li>Finally, we can compute aggregations on our numerical columns using the <strong class="source-inline">aggregate</strong> method. In the example illustrated in <em class="italic">Figure 9.31</em>, we select two numerical columns <strong class="source-inline">total_unique_products</strong> and <strong class="source-inline">taxful_total_price</strong>, and compute the sum, the minimum, and the maximum of the values in these fields across all documents in the index. </li>
			</ol>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B17040_09_31.jpg" alt="Figure 9.31 – Computing aggregations on selected columns is possible &#13;&#10;in Eland using the aggregate method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.31 – Computing aggregations on selected columns is possible in Eland using the aggregate method</p>
			<p>While the steps illustrated here are relatively simple, we hope that they've showcased how seamlessly it is possible to integrate Elasticsearch, working in Python, and a data analysis environment <a id="_idIndexMarker710"/>such as the Jupyter Notebook into one, seamless data analysis workflow. We will build upon this foundation with Eland further in <a href="B17040_13_Epub_AM.xhtml#_idTextAnchor236"><em class="italic">Chapter 13</em></a>, <em class="italic">Inference</em>, when we will take a look at more advanced use cases. </p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor175"/>Summary</h1>
			<p>In this section, we have dipped our toes into the world of Data Frame Analytics, a whole new branch of machine learning and data transformation tools that unlock powerful ways to use the data you have stored in Elasticsearch to solve problems. In addition to giving an overview of the new unsupervised and supervised machine learning techniques that we will cover in future chapters, we have studied three important topics: transforms, using the Painless scripting language, and the integration between Python and Elasticsearch. These topics will form the foundation of our future work in the following chapters. </p>
			<p>In our exposition on transforms, we studied the two components – the pivot and aggregations – that make up a transform, as well as the two possible modes in which to run a transform: batch and continuous. A batch transform runs only once and generates a transformation on a snapshot of the source index at a particular point in time. This works perfectly for datasets that do not change much or when the data transformation needs to be carried out only at a specific point in time. For many real-world use cases, such as logging or our familiar e-commerce store example, the system being monitored and analyzed is constantly changing. An application is constantly logging the activities of its users, an e-commerce store is constantly logging new transactions. Continuous transforms are the tools of choice for analyzing such streaming datasets. </p>
			<p>While the pre-configured options available in the <strong class="bold">Transform</strong> wizard we showcased in our examples are suitable for most situations, more advanced users may wish to configure their own aggregations. In order to do this (and in general, in order to be able to perform many of the more advanced configurations we will discuss in later chapters), users need to be familiar with Painless, the scripting language that is embedded in Elasticsearch. In particular, we took a look at how to declare variables in Painless, how to manipulate those variables with operations, how to construct more advanced programs using control flow statements, and finally how to package up useful code snippets as functions. All of these will be useful in our explorations in later chapters!</p>
			<p>Last but not least, we took a whirlwind tour of how to use Python when analyzing data stored in Elasticsearch. We took a look at the two existing Python clients for Elasticsearch, <strong class="source-inline">elasticsearch-py</strong> and <strong class="source-inline">elasticsearch-dsl</strong> and laid the motivation behind the development of the third and newest client, Eland. </p>
			<p>In the next section, we will dive into the first of the three new machine learning methods added into the Elastic Stack: outlier detection. </p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor176"/>Further reading </h1>
			<p>For more information on the Jupyter ecosystem and, in particular, the Jupyter Notebook, have a look at the comprehensive documentation of Project Jupyter, here: <a href="https://jupyter.org/documentation">https://jupyter.org/documentation</a>. </p>
			<p>If you are new to Python development and would like to have an overview of the language ecosystem and the various tools that are available, have a look at the Hitchiker's Guide to Python, here: <a href="https://docs.python-guide.org/">https://docs.python-guide.org/</a>. </p>
			<p>To learn more about the pandas project, please see the official documentation here: <a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a>. </p>
			<p>For more information on the Painless embedded scripting language, please see the official Painless language specification, here: <a href="https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-lang-spec.html">https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-lang-spec.html</a>. </p>
		</div>
	</body></html>