["```py\n> rpm -ivh jdk-8u181-linux-x64.rpm\n> vi /etc/profile.d/java.sh\n\n      $ export PATH=/usr/java/default/bin:$PATH\n      $ export JAVA_HOME=/usr/java/default\n\n> source /etc/profile.d/java.sh\n> echo $PATH\n> echo $JAVA_HOME\n```", "```py\n> java –version\n $ java version \"1.8.0_181\"\n $ Java(TM) SE Runtime Environment (build 1.8.0_181-b13)\n $ Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)\n```", "```py\n> rpm -ivh scala-2.11.12.rpm\n```", "```py\n> scala –version\n      $ Scala code runner version 2.11.12\n```", "```py\n> scala\n>> 1+2\n $ res0: Int = 3\n>> :q\n```", "```py\n> bash Anaconda3-5.2.0-Linux-x86_64.sh\n> Do you accept the license terms? [yes|no]\n>>> yes\n> Do you wish the installer to prepend the Anaconda3 install location to PATH in your .bashrc ? [yes|no]\n>>> yes\n The last command will add the location of the Anaconda, and hence Python, binaries to your local PATH variable, allowing your local Linux user to run both Python-based programs (overriding any existing Python interpreters already installed on the operating system) and conda commands. Note that you may need to open a new Linux shell in order for the local PATH updates to take effect.\n```", "```py\n> conda --version\n $ conda 4.5.4\n```", "```py\n> python\n $ Python 3.6.5 | Anaconda, Inc.\n>>> import sys\n>>> sys.path\n>>> quit()\n```", "```py\n> conda update conda\n> conda update anaconda\n```", "```py\n> conda install <name of Python package>\n> conda update <name of Python package>\n```", "```py\n> conda list\n```", "```py\n> conda install -c conda-forge findspark\n> conda install -c conda-forge pykafka\n> conda install -c conda-forge tweepy\n> conda install -c conda-forge tensorflow\n> conda install -c conda-forge keras\n```", "```py\n> jupyter notebook --generate-config\n $ Writing default config to: /home/packt/.jupyter/jupyter_notebook_config.py\n> vi /home/packt/.jupyter/jupyter_notebook_config.py\n Line 174: c.NotebookApp.ip = '192.168.56.10'\n Line 214: c.NotebookApp.notebook_dir = '/data/workspaces/packt/jupyter/notebooks/'\n Line 240: c.NotebookApp.port = 8888\n```", "```py\n> jupyter notebook\n```", "```py\n> jupyter notebook --no-browser\n```", "```py\n> firewall-cmd --get-active-zones\n> firewall-cmd --zone=public --add-port=8888/tcp --permanent\n> firewall-cmd --reload\n> firewall-cmd --list-all\n```", "```py\n> tar -xzf spark-2.3.2-bin-hadoop2.7.tgz -C /opt\n```", "```py\n> mkdir -p /data/spark/local/data\n> mkdir -p /data/spark/local/logs\n> mkdir -p /data/spark/local/pid\n> mkdir -p /data/spark/local/worker\n```", "```py\n> cp conf/spark-defaults.conf.template conf/spark-defaults.conf\n> vi conf/spark-defaults.conf\n $ spark.master spark://192.168.56.10:7077\n $ spark.driver.cores 1\n $ spark.driver.maxResultSize 0\n $ spark.driver.memory 2g\n $ spark.executor.memory 2g\n $ spark.executor.cores 2\n $ spark.serializer org.apache.spark.serializer.KryoSerializer\n $ spark.rdd.compress true\n $ spark.kryoserializer.buffer.max 128m\n```", "```py\n> cp conf/spark-env.sh.template conf/spark-env.sh\n> vi conf/spark-env.sh\n $ export SPARK_LOCAL_IP=192.168.56.10\n $ export SPARK_LOCAL_DIRS=/data/spark/local/data\n $ export SPARK_MASTER_HOST=192.168.56.10\n $ export SPARK_WORKER_DIR=/data/spark/local/worker\n $ export SPARK_CONF_DIR=/opt/spark-2.3.2-bin-hadoop2.7/conf\n $ export SPARK_LOG_DIR=/data/spark/local/logs\n $ export SPARK_PID_DIR=/data/spark/local/pid\n```", "```py\n> sbin/start-master.sh\n```", "```py\n> sbin/start-slave.sh spark://192.168.56.10:7077\n```", "```py\n> cd /etc/profile.d\n> vi spark.sh\n $ export SPARK_HOME=/opt/spark-2.3.2-bin-hadoop2.7\n> source spark.sh\n```", "```py\n# (1) Import required Python dependencies\nimport findspark\nfindspark.init()\nfrom pyspark import SparkContext, SparkConf\nimport random\n\n# (2) Instantiate the Spark Context\nconf = SparkConf()\n   .setMaster(\"spark://192.168.56.10:7077\")\n   .setAppName(\"Calculate Pi\")\nsc = SparkContext(conf=conf)\n\n# (3) Calculate the value of Pi i.e. 3.14...\ndef inside(p):\n    x, y = random.random(), random.random()\n    return x*x + y*y < 1\n\nnum_samples = 100\ncount = sc.parallelize(range(0, num_samples)).filter(inside).count()\npi = 4 * count / num_samples\n\n# (4) Print the value of Pi\nprint(pi)\n\n# (5) Stop the Spark Context\nsc.stop()\n```", "```py\n> tar -xzf kafka_2.11-2.0.0.tgz -C /opt\n> cd /opt/kafka_2.11-2.0.0\n```", "```py\n> mkdir -p /data/zookeeper/local/data\n> mkdir -p /data/kafka/local/logs\n```", "```py\n> vi config/zookeeper.properties\n $ dataDir=/data/zookeeper/local/data\n> vi config/server.properties\n $ listeners=PLAINTEXT://192.168.56.10:9092\n $ log.dirs=/data/kafka/local/logs\n $ zookeeper.connect=192.168.56.10:2181\n```", "```py\n> bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n> bin/kafka-server-start.sh -daemon config/server.properties\n```", "```py\n> bin/kafka-topics.sh --create --zookeeper 192.168.56.10:2181 --replication-factor 1 --partitions 1 --topic our-first-topic\n $ Created topic \"our-first-topic\".\n> bin/kafka-topics.sh --list --zookeeper 192.168.56.10:2181\n $ our-first-topic\n```", "```py\n> bin/kafka-console-producer.sh --broker-list 192.168.56.10:9092 --topic our-first-topic\n > This is my 1st test message\n > This is my 2nd test message\n > This is my 3rd test message\n```", "```py\n> bin/kafka-console-consumer.sh --bootstrap-server 192.168.56.10:9092 --topic our-first-topic --from-beginning\n $ This is my 1st test message\n $ This is my 2nd test message\n $ This is my 3rd test message\n```"]